corpusid,title,domain,url,section,section_title,paragraph,para_listed_answer,answer,indexed_answer,filtered_refids,filtered_refids_qualified,num_reference,segmented_answer,question_wholesec_input,question_wholesec,itemized_question_wholesec,QA_pair_wholesec,question_firstpara_input,question_firstpara,itemized_question_firstpara,question_firstsenteachpara_input,question_firstsenteachpara,itemized_question_firstsenteachpara,QA_pair_firstsenteachpara,question_firstpara_firstsenteachpara_input,question_firstpara_firstsenteachpara,itemized_question_firstpara_firstsenteachpara,QA_pair_firstpara_firstsenteachpara,answer_rephrase_sentence_filter_input,answer_rephrase_filtered_sentences-v2.1.0,answer_rephrase_filtered_sentences-v3.0.0,answer_rephrase_filtered_sentences-v3.0.1,answer_rephrase_filtered_sentences-v3.0.2,answer_rephrase_filtered_sentences-v2.1.1,answer_rephrase_filtered_sentences-v2.1.2,answer_rephrase_filtered_sentences-v2.1.3,answer_rephrase_filtered_sentences-v3.1.2,answer_rephrase_filtered_sentences-v3.1.3,answer_rephrase_filtered_sentences,answer_rephrase_sentence_iterative_rephrasing_input-v2.1.1,answer_rephrase_sentence_iterative_rephrasing-v2.1.1,QA_pair_rephrased-v2.1.1,answer_rephrase_direct_sentence_filter_input,answer_rephrase_filtered_direct_sentences-v1.1,answer_rephrase_filtered_direct_sentences,QA_pair_rephrased-2stage-v1.1,answer_rephrase_filtered_direct_sentences-v1.2,QA_pair_rephrased-2stage-v1.2,answer_rephrase_filtered_direct_sentences-v1.1.1,QA_pair_rephrased-2stage-v1.1.1,answer_rephrase_filtered_direct_sentences-v1.2.1,QA_pair_rephrased-2stage-v1.2.1,answer_rephrase_filtered_sentences-v2.1.1.1,answer_rephrase_sentence_iterative_rephrasing_input-v2.1.1.1,answer_rephrase_sentence_iterative_rephrasing-v2.1.1.1,QA_pair_rephrased-v2.1.1.1,answer_rephrase_filtered_sentences-v2.2.1,answer_rephrase_sentence_iterative_rephrasing_input-v2.2.1,answer_rephrase_sentence_iterative_rephrasing-v2.2.1,QA_pair_rephrased-v2.2.1,answer_rephrase_filtered_sentences-v2.2.0,answer_rephrase_sentence_iterative_rephrasing_input-v2.2.0,answer_rephrase_sentence_iterative_rephrasing-v2.2.0,QA_pair_rephrased-v2.2.0,answer_rephrase_filtered_direct_sentences-v2.2,QA_pair_rephrased-2stage-v2.2,answer_rephrase_filtered_direct_sentences-v2.2-zero,QA_pair_rephrased-2stage-v2.2-zero,answer_rephrase_filtered_direct_sentences-v2.2.1,QA_pair_rephrased-2stage-v2.2.1
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s1,BERT embeddings,"['p1.0', 'p1.1']","[""Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence."", ""In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).""]","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","[['b22', 'b24'], ['b21', 'b65']]","[['b22', 'b24'], ['b21', 'b65']]",4,"1. Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.
2. In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.
3. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
4. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.
5. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.
6. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.
7. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","A Primer in BERTology: What we know about how BERT works##
BERT embeddings##
Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).",How do BERT's contextualized embeddings differ from static embeddings in representing word senses?,1. How do BERT's contextualized embeddings differ from static embeddings in representing word senses?,"Questions:

1. How do BERT's contextualized embeddings differ from static embeddings in representing word senses?

Answer:

(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","A Primer in BERTology: What we know about how BERT works##
BERT embeddings##
Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.",How do BERT's embeddings differ from traditional static word embeddings?,1. How do BERT's embeddings differ from traditional static word embeddings?,"A Primer in BERTology: What we know about how BERT works##
BERT embeddings##
Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.",How do BERT's embeddings differ from conventional static embeddings?,1. How do BERT's embeddings differ from conventional static embeddings?,"Questions:

1. How do BERT's embeddings differ from conventional static embeddings?

Answer:

(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","A Primer in BERTology: What we know about how BERT works##
BERT embeddings##
Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.##
In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.",How do BERT's embeddings differ from conventional static embeddings?,1. How do BERT's embeddings differ from conventional static embeddings?,"Questions:

1. How do BERT's embeddings differ from conventional static embeddings?

Answer:

(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","How do BERT's embeddings differ from conventional static embeddings?
##
Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.
In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.
Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.
Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.
They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.
That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.","How do BERT's embeddings differ from conventional static embeddings?
##
Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","BERT's representations differ from conventional static embeddings (Mikolov et al., 2013a; Pennington et al., 2014) by being contextualized, meaning that every input token is represented by a vector that depends on the specific context of occurrence.
Wiedemann et al. (2019) discovered that BERT's contextualized embeddings create distinct and clear clusters that correspond to different word senses, demonstrating that the basic distributional hypothesis applies to these representations.
Ethayarajh (2019) found that the similarity of embeddings for identical words varies across different layers, with later layers of BERT producing more context-specific representations.
They also discovered that BERT embeddings are concentrated within a narrow region of the vector space, and this concentration intensifies from the lower to the higher layers.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

BERT's representations differ from conventional static embeddings (Mikolov et al., 2013a; Pennington et al., 2014) by being contextualized, meaning that every input token is represented by a vector that depends on the specific context of occurrence.
Wiedemann et al. (2019) discovered that BERT's contextualized embeddings create distinct and clear clusters that correspond to different word senses, demonstrating that the basic distributional hypothesis applies to these representations.
Ethayarajh (2019) found that the similarity of embeddings for identical words varies across different layers, with later layers of BERT producing more context-specific representations.
They also discovered that BERT embeddings are concentrated within a narrow region of the vector space, and this concentration intensifies from the lower to the higher layers.","How do BERT's embeddings differ from conventional static embeddings?
##
Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.
In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.
Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.
Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.
They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.
That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","How do BERT's embeddings differ from conventional static embeddings?
##
Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Unlike the conventional static embeddings (Mikolov et al., 2013a; Pennington et al., 2014), BERT's representations are contextualized, meaning every input token is represented by a vector dependent on the particular context of occurrence.
Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more context-specific representations.
They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

Unlike the conventional static embeddings (Mikolov et al., 2013a; Pennington et al., 2014), BERT's representations are contextualized, meaning every input token is represented by a vector dependent on the particular context of occurrence.
Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more context-specific representations.
They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","How do BERT's embeddings differ from conventional static embeddings?
##
Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Unlike the conventional static embeddings (Mikolov et al., 2013a; Pennington et al., 2014), BERT's representations are contextualized, meaning every input token is represented by a vector dependent on the particular context of occurrence.
Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more context-specific representations.
They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

Unlike the conventional static embeddings (Mikolov et al., 2013a; Pennington et al., 2014), BERT's representations are contextualized, meaning every input token is represented by a vector dependent on the particular context of occurrence.
Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more context-specific representations.
They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.","How do BERT's embeddings differ from conventional static embeddings?
##
Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.","Unlike the conventional static embeddings (Mikolov et al., 2013a; Pennington et al., 2014), BERT's representations are contextualized, meaning every input token is represented by a vector dependent on the particular context of occurrence.
Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more context-specific representations.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

Unlike the conventional static embeddings (Mikolov et al., 2013a; Pennington et al., 2014), BERT's representations are contextualized, meaning every input token is represented by a vector dependent on the particular context of occurrence.
Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more context-specific representations.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,s7,Distant & Weak Supervision,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4']","['In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).', 'While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.', 'Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.', 'Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.', 'While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).']","In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","(p7.0) In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

(p7.1) While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

(p7.2) Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

(p7.3) Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

(p7.4) While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","[['b20', 'b23', 'b22', 'b38', None, 'b67', 'b1', 'b2', 'b61'], ['b5', None], ['b54', None], [None], [None, 'b14']]","[['b20', 'b23', 'b22', 'b38', None, 'b67', 'b1', 'b2', 'b61'], ['b5', None], ['b54', None], [None], [None, 'b14']]",16,"1. In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified.
2. The corresponding labels are obtained through a (semi-)automatic process from an external source of information.
3. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations.
4. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012).
5. It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a).
6. The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019).
7. This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.
8. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).
9. While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP.
10. Nevertheless, distant supervision has also been successfully em-
11. (NER)3 build a discourse-structure dataset using guidance from sentiment annotations.
12. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (NER)0 or from entity lists (NER)1.
13. For some classification tasks, the labels can be rephrased with simple rules into sentences.
14. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (NER)2.
15. An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.
16. Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited.
17. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules.
18. It is an open question whether a task needs to have specific properties to be suitable for this approach.
19. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.
20. Distant supervision methods heavily rely on auxiliary data.
21. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.
22. Kann et al. (NER)3 find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages.
23. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.
24. While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.
25. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme.
26. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches.
27. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (NER)4.","A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios##
Distant & Weak Supervision##
In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).",What are the limitations and challenges of employing distant supervision in NLP tasks?,1. What are the limitations and challenges of employing distant supervision in NLP tasks?,"Questions:

1. What are the limitations and challenges of employing distant supervision in NLP tasks?

Answer:

(p7.0) In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

(p7.1) While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

(p7.2) Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

(p7.3) Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

(p7.4) While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios##
Distant & Weak Supervision##
In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).",How does distant supervision enhance information extraction in low-resource NLP tasks?,1. How does distant supervision enhance information extraction in low-resource NLP tasks?,"A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios##
Distant & Weak Supervision##
In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified.

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP.

Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited.

Distant supervision methods heavily rely on auxiliary data.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.",What are the limitations and applications of distant supervision in NLP tasks?,1. What are the limitations and applications of distant supervision in NLP tasks?,"Questions:

1. What are the limitations and applications of distant supervision in NLP tasks?

Answer:

(p7.0) In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

(p7.1) While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

(p7.2) Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

(p7.3) Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

(p7.4) While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios##
Distant & Weak Supervision##
In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).##
While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP.

Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited.

Distant supervision methods heavily rely on auxiliary data.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.",What are the limitations and requirements of distant supervision in NLP tasks?,1. What are the limitations and requirements of distant supervision in NLP tasks?,"Questions:

1. What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

(p7.0) In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

(p7.1) While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

(p7.2) Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

(p7.3) Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

(p7.4) While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","What are the limitations and requirements of distant supervision in NLP tasks?
##
In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified.
The corresponding labels are obtained through a (semi-)automatic process from an external source of information.
For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations.
Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012).
It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a).
The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019).
This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.
These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).
While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP.
Nevertheless, distant supervision has also been successfully em-
(2020) build a discourse-structure dataset using guidance from sentiment annotations.
For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020).
For some classification tasks, the labels can be rephrased with simple rules into sentences.
A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;).
An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.
Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited.
There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules.
It is an open question whether a task needs to have specific properties to be suitable for this approach.
The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.
Distant supervision methods heavily rely on auxiliary data.
In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.
Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages.
This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.
While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.
This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme.
Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches.
Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.","While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.",,,"In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified.

The corresponding labels are obtained through a (semi-)automatic process from an external source of information.

Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012).

It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a).

This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP.

Distant supervision methods heavily rely on auxiliary data.

In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.

This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.","Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012).

It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a).

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP.

Distant supervision methods heavily rely on auxiliary data.

In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.

Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.","Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012).

It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a).

Distant supervision methods heavily rely on auxiliary data.

In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.

Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages.

This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.","While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.

Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.","Distant supervision methods heavily rely on auxiliary data.

In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.

This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme.","What are the limitations and requirements of distant supervision in NLP tasks?
##
In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified.

The corresponding labels are obtained through a (semi-)automatic process from an external source of information.

Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012).

It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a).

This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP.

Distant supervision methods heavily rely on auxiliary data.

In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.

This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.","Distant or weak supervision in NLP tasks uses unlabeled text without modifying it.
The corresponding labels are obtained through a semi-automatic process from an external source of information.
Distant supervision was introduced by Mintz et al. (2009) for relation extraction, with extensions on multi-instance learning (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012).
This approach remains popular for tasks like Named Entity Recognition (NER) and Relation Extraction (RE), where external information is sourced from knowledge bases, gazetteers, dictionaries, and other structured knowledge sources (Luo et al., 2017; Hedderich and Klakow, 2018; Deng and Sun, 2019; Alt et al., 2019; Ye et al., 2019; Lange et al., 2019a; Nooralahzadeh et al., 2019; Le and Titov, 2019; Cao et al., 2019; Lison et al., 2020; Hedderich et al., 2021a).
Using information from external knowledge sources for distant supervision is considered a subset of the broader strategy of applying labeling rules.
While distant supervision is widely used for information extraction tasks such as Named Entity Recognition (NER) and Relation Extraction (RE), its application is less common in other areas of NLP.
Distant supervision methods heavily depend on auxiliary data.
In a low-resource setting, obtaining both labeled data and auxiliary data can be challenging.
This highlights the importance of testing these methods in practical scenarios and not merely simulating limited access to labeled data in languages with abundant resources.
While distant supervision enables faster acquisition of labeled data compared to manual annotation of each dataset instance, it necessitates human involvement to develop automatic annotation techniques or to establish labeling rules.","Questions:

What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

Distant or weak supervision in NLP tasks uses unlabeled text without modifying it.
The corresponding labels are obtained through a semi-automatic process from an external source of information.
Distant supervision was introduced by Mintz et al. (2009) for relation extraction, with extensions on multi-instance learning (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012).
This approach remains popular for tasks like Named Entity Recognition (NER) and Relation Extraction (RE), where external information is sourced from knowledge bases, gazetteers, dictionaries, and other structured knowledge sources (Luo et al., 2017; Hedderich and Klakow, 2018; Deng and Sun, 2019; Alt et al., 2019; Ye et al., 2019; Lange et al., 2019a; Nooralahzadeh et al., 2019; Le and Titov, 2019; Cao et al., 2019; Lison et al., 2020; Hedderich et al., 2021a).
Using information from external knowledge sources for distant supervision is considered a subset of the broader strategy of applying labeling rules.
While distant supervision is widely used for information extraction tasks such as Named Entity Recognition (NER) and Relation Extraction (RE), its application is less common in other areas of NLP.
Distant supervision methods heavily depend on auxiliary data.
In a low-resource setting, obtaining both labeled data and auxiliary data can be challenging.
This highlights the importance of testing these methods in practical scenarios and not merely simulating limited access to labeled data in languages with abundant resources.
While distant supervision enables faster acquisition of labeled data compared to manual annotation of each dataset instance, it necessitates human involvement to develop automatic annotation techniques or to establish labeling rules.","What are the limitations and requirements of distant supervision in NLP tasks?
##
In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified.
The corresponding labels are obtained through a (semi-)automatic process from an external source of information.
For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations.
Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012).
It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a).
The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019).
This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.
These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).
While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP.
Nevertheless, distant supervision has also been successfully em-
(2020) build a discourse-structure dataset using guidance from sentiment annotations.
For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020).
For some classification tasks, the labels can be rephrased with simple rules into sentences.
A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;).
An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.
Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited.
There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules.
It is an open question whether a task needs to have specific properties to be suitable for this approach.
The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.
Distant supervision methods heavily rely on auxiliary data.
In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.
Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages.
This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.
While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.
This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme.
Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches.
Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","Distant supervision methods heavily rely on auxiliary data.

In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.

Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.",None,"Questions:

What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

Distant supervision methods heavily rely on auxiliary data.

In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.

Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.","Distant supervision methods heavily rely on auxiliary data.

In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.

This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.","Questions:

What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

Distant supervision methods heavily rely on auxiliary data.

In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.

This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.",,"Questions:

What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

None",,"Questions:

What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

None",,"What are the limitations and requirements of distant supervision in NLP tasks?
##
None",Distant supervision in NLP tasks often faces limitations and requirements that need to be addressed for effective implementation.,"Questions:

What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

Distant supervision in NLP tasks often faces limitations and requirements that need to be addressed for effective implementation.",,"What are the limitations and requirements of distant supervision in NLP tasks?
##
None",Distant supervision in NLP tasks often faces limitations and requirements that need to be addressed for effective implementation.,"Questions:

What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

Distant supervision in NLP tasks often faces limitations and requirements that need to be addressed for effective implementation.","Distant supervision methods heavily rely on auxiliary data.

In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.

This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme.","What are the limitations and requirements of distant supervision in NLP tasks?
##
Distant supervision methods heavily rely on auxiliary data.

In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.

This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme.","Distant supervision methods heavily rely on auxiliary data.
In a low-resource setting, it might be difficult to obtain not only labeled data but also the necessary auxiliary data.
While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.
This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme.","Questions:

What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

Distant supervision methods heavily rely on auxiliary data.
In a low-resource setting, it might be difficult to obtain not only labeled data but also the necessary auxiliary data.
While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.
This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme.",,"Questions:

What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

None","Distant supervision methods heavily rely on auxiliary data.

In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.

Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.

This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme.","Questions:

What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

Distant supervision methods heavily rely on auxiliary data.

In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.

Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.

This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme.",None,"Questions:

What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

None"
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s2,Source(s) Target,"['p2.0', 'p2.1', 'p2.2', 'p2.3']","[""Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016)  Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017)  Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER    Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter,  News, ikipedia, Reddit. Evidence: Single, Multiple, Thread."", '2 What is Stance?', 'In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker\'s standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Kk and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).', 'Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.']","Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016)  Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017)  Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER    Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter,  News, ikipedia, Reddit. Evidence: Single, Multiple, Thread.

2 What is Stance?

In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Kk and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","(p2.0) Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016)  Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017)  Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER    Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter,  News, ikipedia, Reddit. Evidence: Single, Multiple, Thread.

(p2.1) 2 What is Stance?

(p2.2) In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Kk and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

(p2.3) Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","[['b12', None, 'b57', 'b17'], [], [None], []]","[['b12', None, 'b57', 'b17'], [], [None], []]",5,"1. Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011)
2. Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b)
3. Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016)  Headline Article *
4. 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017)  Headline Article 75K Fake news RumourEval '
5. 17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER    Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection.
6. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds.
7. * the article's body is summarised.
8. Sources: Twitter,  News, ikipedia, Reddit.
9. Evidence: Single, Multiple, Thread.
10. 2 What is Stance? In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process.
11. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition.
12. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Kk and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither.
13. Occasionally, the category label of Neutral is also added to the set of stance categories (Zubiaga et al., 2016b)0, and the target may or may not be explicitly mentioned in the text"" (Zubiaga et al., 2016b)1.
14. Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (Zubiaga et al., 2016b)2.
15. Finally, stance detection can be distinguished from several other closely related NLP tasks: (Zubiaga et al., 2016b)3 biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (Zubiaga et al., 2016b)4 emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (Zubiaga et al., 2016b)5 perspective identification, which aims to find the pointof-view of the author (Zubiaga et al., 2016b)6 and the target is always explicit, (Zubiaga et al., 2016b)7 sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (Zubiaga et al., 2016b)8 sentiment analysis, which checks the polarity of a piece of text.","A Survey on Stance Detection for Mis-and Disinformation Identification##
Source(s) Target##
Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016)  Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017)  Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER    Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter,  News, ikipedia, Reddit. Evidence: Single, Multiple, Thread.

2 What is Stance?

In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Kk and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.",What distinguishes stance detection from related NLP tasks like sentiment analysis or sarcasm detection?,1. What distinguishes stance detection from related NLP tasks like sentiment analysis or sarcasm detection?,"Questions:

1. What distinguishes stance detection from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

(p2.0) Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016)  Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017)  Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER    Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter,  News, ikipedia, Reddit. Evidence: Single, Multiple, Thread.

(p2.1) 2 What is Stance?

(p2.2) In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Kk and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

(p2.3) Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","A Survey on Stance Detection for Mis-and Disinformation Identification##
Source(s) Target##
Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016)  Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017)  Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER    Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter,  News, ikipedia, Reddit. Evidence: Single, Multiple, Thread.",What are the key characteristics of datasets used in stance detection for misinformation?,1. What are the key characteristics of datasets used in stance detection for misinformation?,"A Survey on Stance Detection for Mis-and Disinformation Identification##
Source(s) Target##
Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011)

2 What is Stance?

In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process.

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.",How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?,1. How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?,"Questions:

1. How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

(p2.0) Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016)  Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017)  Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER    Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter,  News, ikipedia, Reddit. Evidence: Single, Multiple, Thread.

(p2.1) 2 What is Stance?

(p2.2) In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Kk and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

(p2.3) Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","A Survey on Stance Detection for Mis-and Disinformation Identification##
Source(s) Target##
Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016)  Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017)  Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER    Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter,  News, ikipedia, Reddit. Evidence: Single, Multiple, Thread.##
2 What is Stance?

In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process.

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.",How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?,1. How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?,"Questions:

1. How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

(p2.0) Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016)  Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017)  Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER    Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter,  News, ikipedia, Reddit. Evidence: Single, Multiple, Thread.

(p2.1) 2 What is Stance?

(p2.2) In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Kk and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

(p2.3) Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?
##
Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011)
Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b)
Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016)  Headline Article *
2.6K Rumours FNC-1 (Pomerleau and Rao, 2017)  Headline Article 75K Fake news RumourEval '
17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER    Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection.
#Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds.
* the article's body is summarised.
Sources: Twitter,  News, ikipedia, Reddit.
Evidence: Single, Multiple, Thread.
2 What is Stance? In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process.
Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition.
Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc.
Here, we adopt the general definition of stance detection by Kk and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither.
Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016).
Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).
Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Here, we adopt the general definition of stance detection by Kk and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither.

Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016).

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Here, we adopt the general definition of stance detection by Kk and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither.

Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016).

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?
##
Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Stance detection differs from several closely related NLP tasks such as biased language detection, emotion recognition, perspective identification, sarcasm detection, and sentiment analysis by focusing on identifying the author's position towards a specific target, which is always explicit, unlike the objectives of these other tasks.","Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Stance detection differs from several closely related NLP tasks such as biased language detection, emotion recognition, perspective identification, sarcasm detection, and sentiment analysis by focusing on identifying the author's position towards a specific target, which is always explicit, unlike the objectives of these other tasks.","How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?
##
Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011)
Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b)
Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016)  Headline Article *
2.6K Rumours FNC-1 (Pomerleau and Rao, 2017)  Headline Article 75K Fake news RumourEval '
17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER    Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection.
#Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds.
* the article's body is summarised.
Sources: Twitter,  News, ikipedia, Reddit.
Evidence: Single, Multiple, Thread.
2 What is Stance? In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process.
Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition.
Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc.
Here, we adopt the general definition of stance detection by Kk and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither.
Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016).
Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).
Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.",None,"Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.",,"How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?
##
None",Stance detection differs from related NLP tasks like sentiment analysis or sarcasm detection in its focus on identifying the position or attitude expressed in a text towards a particular target or topic.,"Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Stance detection differs from related NLP tasks like sentiment analysis or sarcasm detection in its focus on identifying the position or attitude expressed in a text towards a particular target or topic.",,"How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?
##
None",Stance detection differs from related NLP tasks like sentiment analysis or sarcasm detection in its focus on identifying the position or attitude expressed towards a particular target or topic.,"Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Stance detection differs from related NLP tasks like sentiment analysis or sarcasm detection in its focus on identifying the position or attitude expressed towards a particular target or topic.","Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?
##
Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, which explores the existence of an inclination or tendency towards a particular perspective within a text, (ii) emotion recognition, which aims to recognize emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the point of view of the author (e.g., Democrat vs. Republican) with the target always being explicit, (iv) sarcasm detection, which focuses on satirical or ironic pieces of text often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, which explores the existence of an inclination or tendency towards a particular perspective within a text, (ii) emotion recognition, which aims to recognize emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the point of view of the author (e.g., Democrat vs. Republican) with the target always being explicit, (iv) sarcasm detection, which focuses on satirical or ironic pieces of text often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.",,"Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

None","Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.",None,"Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

None"
234093015,A Survey of Data Augmentation Approaches for NLP,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/63d8426ba1f51a8525dd19fd8ec92934ec71aea5,s1,Background,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5']","['What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.', 'What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.', 'Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.', 'Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.', 'Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.', 'Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in 6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.']","What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in 6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","(p1.0) What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

(p1.1) What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

(p1.2) Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

(p1.3) Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

(p1.4) Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

(p1.5) Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in 6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","[[None], ['b23', None], [None, 'b1'], [], [], [None]]","[[None], ['b23', None], [None, 'b1'], [], [], [None]]",6,"1. What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.
2. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020).
3. DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training.
4. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.
5. What are the goals and trade-offs?
6. Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).
7. As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance.
8. Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).
9. Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.
10. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.
11. Further, the distribution of augmented data should neither be too similar nor too different from the original.
12. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.
13. Effective DA approaches should aim for a balance.
14. Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.
15. Interpretation of DA Dao et al. (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020)0 note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.
16. Overall, there indeed appears to be a lack of research on why exactly DA works.
17. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles.
18. We discuss this challenge more in 6, and highlight some of the existing work below.
19. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2).
20. Rajput et al. (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020)0 show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods.
21. Dao et al. (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020)0 think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization.
22. Chen et al. (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020)1 show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","A Survey of Data Augmentation Approaches for NLP##
Background##
What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in 6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.",What are the theoretical explanations for the effectiveness of data augmentation in NLP?,1. What are the theoretical explanations for the effectiveness of data augmentation in NLP?,"Questions:

1. What are the theoretical explanations for the effectiveness of data augmentation in NLP?

Answer:

(p1.0) What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

(p1.1) What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

(p1.2) Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

(p1.3) Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

(p1.4) Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

(p1.5) Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in 6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","A Survey of Data Augmentation Approaches for NLP##
Background##
What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.",How does data augmentation contribute to reducing overfitting in machine learning models?,1. How does data augmentation contribute to reducing overfitting in machine learning models?,"A Survey of Data Augmentation Approaches for NLP##
Background##
What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.

What are the goals and trade-offs?

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Further, the distribution of augmented data should neither be too similar nor too different from the original.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.",What are the primary goals and challenges of data augmentation in NLP?,1. What are the primary goals and challenges of data augmentation in NLP?,"Questions:

1. What are the primary goals and challenges of data augmentation in NLP?

Answer:

(p1.0) What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

(p1.1) What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

(p1.2) Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

(p1.3) Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

(p1.4) Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

(p1.5) Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in 6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","A Survey of Data Augmentation Approaches for NLP##
Background##
What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.##
What are the goals and trade-offs?

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Further, the distribution of augmented data should neither be too similar nor too different from the original.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.",What are the primary challenges and considerations in applying data augmentation techniques in NLP?,1. What are the primary challenges and considerations in applying data augmentation techniques in NLP?,"Questions:

1. What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

(p1.0) What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

(p1.1) What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

(p1.2) Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

(p1.3) Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

(p1.4) Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

(p1.5) Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in 6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","What are the primary challenges and considerations in applying data augmentation techniques in NLP?
##
What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.
Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020).
DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training.
In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.
What are the goals and trade-offs?
Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).
As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance.
Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).
Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.
Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.
Further, the distribution of augmented data should neither be too similar nor too different from the original.
This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.
Effective DA approaches should aim for a balance.
Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.
Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.
Overall, there indeed appears to be a lack of research on why exactly DA works.
Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles.
We discuss this challenge more in 6, and highlight some of the existing work below.
Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2).
Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods.
Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization.
Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020).

In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles.

Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization.",,,,"What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.

Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020).

In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.","In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.","In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.","Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020).

In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance.

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).","In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective DA approaches should aim for a balance.","What are the primary challenges and considerations in applying data augmentation techniques in NLP?
##
What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.

Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020).

In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.","Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.
Most strategies involve adding slightly modified copies of existing data or creating synthetic data, with the goal for the augmented data to serve as a regularizer and help reduce overfitting in machine learning models (Shorten and Khoshgoftaar, 2019; Hernndez-Garca and Knig, 2020).
In NLP, generating effective augmented examples that capture the desired invariances is less obvious due to the discrete nature of the input space.
Despite these challenges, a variety of data augmentation techniques for NLP have been proposed, including rule-based manipulations (Zhang et al., 2015) and more complex generative approaches (Liu et al., 2020b).
Additionally, the distribution of augmented data needs to maintain a balance, ensuring it is neither too similar nor too divergent from the original data.
This could result in either increased overfitting or diminished performance due to training on examples that do not accurately represent the specific domain.
Dao et al. (2019) note that data augmentation is often conducted in an ad hoc manner without a deep understanding of the underlying theoretical principles, suggesting that the common explanation of data augmentation as a form of regularization may be insufficient.","Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.
Most strategies involve adding slightly modified copies of existing data or creating synthetic data, with the goal for the augmented data to serve as a regularizer and help reduce overfitting in machine learning models (Shorten and Khoshgoftaar, 2019; Hernndez-Garca and Knig, 2020).
In NLP, generating effective augmented examples that capture the desired invariances is less obvious due to the discrete nature of the input space.
Despite these challenges, a variety of data augmentation techniques for NLP have been proposed, including rule-based manipulations (Zhang et al., 2015) and more complex generative approaches (Liu et al., 2020b).
Additionally, the distribution of augmented data needs to maintain a balance, ensuring it is neither too similar nor too divergent from the original data.
This could result in either increased overfitting or diminished performance due to training on examples that do not accurately represent the specific domain.
Dao et al. (2019) note that data augmentation is often conducted in an ad hoc manner without a deep understanding of the underlying theoretical principles, suggesting that the common explanation of data augmentation as a form of regularization may be insufficient.","What are the primary challenges and considerations in applying data augmentation techniques in NLP?
##
What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.
Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020).
DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training.
In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.
What are the goals and trade-offs?
Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).
As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance.
Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).
Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.
Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.
Further, the distribution of augmented data should neither be too similar nor too different from the original.
This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.
Effective DA approaches should aim for a balance.
Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.
Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.
Overall, there indeed appears to be a lack of research on why exactly DA works.
Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles.
We discuss this challenge more in 6, and highlight some of the existing work below.
Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2).
Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods.
Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization.
Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.

In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.",None,"Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.

In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.","In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements.

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.","Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements.

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.",,"Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

None",,"Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

None",,"What are the primary challenges and considerations in applying data augmentation techniques in NLP?
##
None",The primary challenges and considerations in applying data augmentation techniques in NLP include ensuring the augmented data maintains semantic consistency and relevance to the original data.,"Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

The primary challenges and considerations in applying data augmentation techniques in NLP include ensuring the augmented data maintains semantic consistency and relevance to the original data.","What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.

Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020).

In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs?

Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance.

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.

Overall, there indeed appears to be a lack of research on why exactly DA works.

Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles.

We discuss this challenge more in 6, and highlight some of the existing work below.

Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2).

Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods.

Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization.

Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","What are the primary challenges and considerations in applying data augmentation techniques in NLP?
##
What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.

Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernndez-Garca and Knig, 2020).

In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs?

Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance.

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.

Overall, there indeed appears to be a lack of research on why exactly DA works.

Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles.

We discuss this challenge more in 6, and highlight some of the existing work below.

Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2).

Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods.

Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization.

Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.
Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019; Hernndez-Garca and Knig, 2020).
In NLP, where the input space is discrete, generating effective augmented examples that capture the desired invariances is less obvious.
The goals and trade-offs of data augmentation in NLP involve balancing the creation of diverse and representative augmented examples while avoiding the introduction of noise or artifacts that could mislead the model.
Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).
An ideal DA technique should be both easy to implement and capable of improving model performance.
Most offer trade-offs between these two. Rule-based techniques are easy to implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b).
Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.
Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.
Further, the distribution of augmented data should neither be too similar nor too different from the original.
This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.
Effective DA approaches should aim for a balance.
Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.
Dao et al. (2019) note that ""data augmentation is typically performed in an ad hoc manner with little understanding of the underlying theoretical principles"" and claim the typical explanation of DA as regularization to be insufficient.
Overall, there indeed appears to be a lack of research on why exactly DA works.
Existing work on this topic is mainly surface-level and rarely investigates the theoretical underpinnings and principles.
This challenge is further discussed in existing work, which highlights the need for deeper investigation into the theoretical principles underlying data augmentation.
Bishop (1995) show that training with noised examples is reducible to Tikhonov regularization, which subsumes L2 regularization.
Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods.
Dao et al. (2019) think of DA transformations as kernels and find two ways DA helps: averaging of features and variance regularization.
Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.
Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019; Hernndez-Garca and Knig, 2020).
In NLP, where the input space is discrete, generating effective augmented examples that capture the desired invariances is less obvious.
The goals and trade-offs of data augmentation in NLP involve balancing the creation of diverse and representative augmented examples while avoiding the introduction of noise or artifacts that could mislead the model.
Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).
An ideal DA technique should be both easy to implement and capable of improving model performance.
Most offer trade-offs between these two. Rule-based techniques are easy to implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b).
Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.
Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.
Further, the distribution of augmented data should neither be too similar nor too different from the original.
This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.
Effective DA approaches should aim for a balance.
Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.
Dao et al. (2019) note that ""data augmentation is typically performed in an ad hoc manner with little understanding of the underlying theoretical principles"" and claim the typical explanation of DA as regularization to be insufficient.
Overall, there indeed appears to be a lack of research on why exactly DA works.
Existing work on this topic is mainly surface-level and rarely investigates the theoretical underpinnings and principles.
This challenge is further discussed in existing work, which highlights the need for deeper investigation into the theoretical principles underlying data augmentation.
Bishop (1995) show that training with noised examples is reducible to Tikhonov regularization, which subsumes L2 regularization.
Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods.
Dao et al. (2019) think of DA transformations as kernels and find two ways DA helps: averaging of features and variance regularization.
Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective DA approaches should aim for a balance.","What are the primary challenges and considerations in applying data augmentation techniques in NLP?
##
In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective DA approaches should aim for a balance.","In NLP, where the input space is discrete, generating effective augmented examples that capture the desired invariances is less obvious.
Despite challenges associated with text, many data augmentation techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).
Most offer trade-offs between simplicity and performance. Rule-based techniques are easy to implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b).
Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.
Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.
Further, the distribution of augmented data should neither be too similar nor too different from the original.
This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.
Effective data augmentation (DA) approaches should aim for a balance.","Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

In NLP, where the input space is discrete, generating effective augmented examples that capture the desired invariances is less obvious.
Despite challenges associated with text, many data augmentation techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).
Most offer trade-offs between simplicity and performance. Rule-based techniques are easy to implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b).
Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.
Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.
Further, the distribution of augmented data should neither be too similar nor too different from the original.
This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.
Effective data augmentation (DA) approaches should aim for a balance.",,"Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

None","In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective DA approaches should aim for a balance.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.","Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective DA approaches should aim for a balance.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.",None,"Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

None"
236460206,Towards Argument Mining for Social Good: A Survey,"Political Science, Linguistics, Computer Science",https://www.semanticscholar.org/paper/dcb0b23685c9c116d8d53fe47e5157753659d3bd,s2,Framework,"['p2.0', 'p2.1', 'p2.2', 'p2.3', 'p2.4', 'p2.5', 'p2.6', 'p2.7', 'p2.8', 'p2.9', 'p2.10', 'p2.11', 'p2.12']","['Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Nive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).', 'Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.', 'To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.', 'Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.', 'A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.', ""Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation."", 'A1: Marvel Universe is better than DC Universe.', ""A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses."", 'A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.', 'A4: This is especially true due to his unfortunate passing.', ""A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics."", ""The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise)."", ""Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).""]","Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Nive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

A1: Marvel Universe is better than DC Universe.

A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

A4: This is especially true due to his unfortunate passing.

A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","(p2.0) Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Nive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

(p2.1) Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

(p2.2) To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

(p2.3) Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

(p2.4) A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

(p2.5) Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

(p2.6) A1: Marvel Universe is better than DC Universe.

(p2.7) A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

(p2.8) A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

(p2.9) A4: This is especially true due to his unfortunate passing.

(p2.10) A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

(p2.11) The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

(p2.12) Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","[['b29', None, 'b28', 'b26'], ['b49', 'b14'], ['b35', 'b44', None, 'b31'], [], [], [], [], [], [], [], [], [], ['b44']]","[['b29', None, 'b28', 'b26'], ['b49', 'b14'], ['b35', 'b44', None, 'b31'], [], [], [], [], [], [], [], [], [], ['b44']]",11,"1. Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM.
2. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.
3. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).
4. A variety of models were used for this including Nive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).Relation assignment
5. The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.
6. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).
7. Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (e.g. claims, premises, rebuttal, etc.)0.
8. Detecting these relations is necessary to model the overall structure of the argumentation (e.g. claims, premises, rebuttal, etc.)1.
9. As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues.
10. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.
11. To simplify the problem, some approaches reduce the graph to a tree-structure representation (e.g. claims, premises, rebuttal, etc.)2.
12. Different methods to generate the structure have been investigated, e.g. SVMs (e.g. claims, premises, rebuttal, etc.)3 or textual entailment (e.g. claims, premises, rebuttal, etc.)4.
13. Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly.
14. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?""
15. (e.g. claims, premises, rebuttal, etc.)5 A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects.
16. Vaccinated people become immune to a certain pathogen and do not develop a disease.
17. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.
18. A2: Many vaccines have serious and sometimes deadly side effects.
19. With many vaccines the immunity is not lifelong.
20. Sometimes the vaccines itself can cause a serious disease to develop as a side effect.
21. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.
22. Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (e.g. claims, premises, rebuttal, etc.)6 are extracted.
23. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it.
24. However, consider another example, extracted from an online debate platform Kialo 4 .
25. Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.
26. A1: Marvel Universe is better than DC Universe.
27. A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.
28. A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.
29. A4: This is especially true due to his unfortunate passing.
30. A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.
31. The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components.
32. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level.
33. Note, however, that the relations between arguments and claim are still relatively clear (e.g. claims, premises, rebuttal, etc.)7.
34. Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict.
35. Working in the realm of overtly argumentative text (e.g. claims, premises, rebuttal, etc.)8), while challenging of course, can be quite standardized.
36. The language use is generally in line with natural language expectations and often standard (e.g. claims, premises, rebuttal, etc.)9, the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (Moens et al., 2007)0.","Towards Argument Mining for Social Good: A Survey##
Framework##
Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Nive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

A1: Marvel Universe is better than DC Universe.

A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

A4: This is especially true due to his unfortunate passing.

A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).",What are the stages involved in the argument mining framework and how do they function?,1. What are the stages involved in the argument mining framework and how do they function?,"Questions:

1. What are the stages involved in the argument mining framework and how do they function?

Answer:

(p2.0) Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Nive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

(p2.1) Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

(p2.2) To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

(p2.3) Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

(p2.4) A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

(p2.5) Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

(p2.6) A1: Marvel Universe is better than DC Universe.

(p2.7) A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

(p2.8) A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

(p2.9) A4: This is especially true due to his unfortunate passing.

(p2.10) A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

(p2.11) The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

(p2.12) Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","Towards Argument Mining for Social Good: A Survey##
Framework##
Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Nive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).",What methodologies have been applied in the extraction of argumentative structures from texts?,1. What methodologies have been applied in the extraction of argumentative structures from texts?,"Towards Argument Mining for Social Good: A Survey##
Framework##
Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM.

Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017).

Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly.

A2: Many vaccines have serious and sometimes deadly side effects.

Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted.

A1: Marvel Universe is better than DC Universe.

A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

A4: This is especially true due to his unfortunate passing.

A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components.

Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict.",How does the argument mining framework handle the modeling of relations between argumentative components?,1. How does the argument mining framework handle the modeling of relations between argumentative components?,"Questions:

1. How does the argument mining framework handle the modeling of relations between argumentative components?

Answer:

(p2.0) Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Nive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

(p2.1) Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

(p2.2) To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

(p2.3) Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

(p2.4) A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

(p2.5) Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

(p2.6) A1: Marvel Universe is better than DC Universe.

(p2.7) A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

(p2.8) A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

(p2.9) A4: This is especially true due to his unfortunate passing.

(p2.10) A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

(p2.11) The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

(p2.12) Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","Towards Argument Mining for Social Good: A Survey##
Framework##
Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Nive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).##
Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017).

Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly.

A2: Many vaccines have serious and sometimes deadly side effects.

Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted.

A1: Marvel Universe is better than DC Universe.

A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

A4: This is especially true due to his unfortunate passing.

A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components.

Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict.",What are the stages involved in the argument mining framework and how are they implemented?,1. What are the stages involved in the argument mining framework and how are they implemented?,"Questions:

1. What are the stages involved in the argument mining framework and how are they implemented?

Answer:

(p2.0) Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Nive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

(p2.1) Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

(p2.2) To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

(p2.3) Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

(p2.4) A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

(p2.5) Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

(p2.6) A1: Marvel Universe is better than DC Universe.

(p2.7) A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

(p2.8) A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

(p2.9) A4: This is especially true due to his unfortunate passing.

(p2.10) A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

(p2.11) The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

(p2.12) Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","What are the stages involved in the argument mining framework and how are they implemented?
##
Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM.
Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.
Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).
A variety of models were used for this including Nive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).Relation assignment
The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.
These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).
Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019).
Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate).
As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues.
After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.
To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017).
Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020).
Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly.
Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?""
(Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects.
Vaccinated people become immune to a certain pathogen and do not develop a disease.
Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.
A2: Many vaccines have serious and sometimes deadly side effects.
With many vaccines the immunity is not lifelong.
Sometimes the vaccines itself can cause a serious disease to develop as a side effect.
If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.
Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted.
Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it.
However, consider another example, extracted from an online debate platform Kialo 4 .
Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.
A1: Marvel Universe is better than DC Universe.
A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.
A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.
A4: This is especially true due to his unfortunate passing.
A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.
The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components.
Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level.
Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).
Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict.
Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized.
The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).

After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted.",,,,"Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM.

Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

A variety of models were used for this including Nive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).

Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019).

Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate).

As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues.

After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017).

Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020).","Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).

After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017).","Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).

After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017).","Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).

Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019).

Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate).

As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues.

After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017).

Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020).

Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted.

Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it.","Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).

Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019).

Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate).

After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020).

Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted.","Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).

After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.","What are the stages involved in the argument mining framework and how are they implemented?
##
Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM.

Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

A variety of models were used for this including Nive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).

Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019).

Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate).

As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues.

After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017).

Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020).","Cabrio and Villata (2018) provide an elaborate overview of the argument mining (AM) framework, detailing significant developments in the field over the past five years.
The process is typically carried out in two stages: initially, the system identifies documents that contain argumentative structures and then pinpoints the specific textual spans where the argumentation is encoded.
Once the textual boundaries are defined, subportions of the argumentative spans are categorized into pre-established argument components such as claims, premises, and rebuttals.
A variety of models have been employed, including Nive Bayes (Moens et al., 2007), SVMs (Mochales and Moens, 2011), RNNs (Niculae et al., 2017; Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019; Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).
The second stage aims to model the relations between the argumentative spans identified in the first stage.
These relations can be of two types: those that exist between different arguments, such as support or attack, and those that exist within an argument, connecting the premises with the claim.
Recent approaches to classifying argumentative relations include using relational models (Trautmann et al., 2020) or enhancing models by incorporating features from various knowledge bases (Kobbe et al., 2019).
Detecting these relations is crucial for modeling the overall structure of the argumentation.
This complexity arises from the need for sophisticated knowledge representation and reasoning capabilities.
After detecting the relations, the discourse structure is represented as an argumentation graph, where arguments form the nodes and the relations between them are the edges.
To address the complexity, some methods simplify the argumentation graph into a tree-structure representation (Peldszus and Stede, 2015; Stab and Gurevych, 2017).
Different methods to generate the argumentation structure include using SVMs (Habernal and Gurevych, 2017; Niculae et al., 2017) or textual entailment approaches (Cabrio and Villata, 2013; Cocarascu et al., 2020).","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

Cabrio and Villata (2018) provide an elaborate overview of the argument mining (AM) framework, detailing significant developments in the field over the past five years.
The process is typically carried out in two stages: initially, the system identifies documents that contain argumentative structures and then pinpoints the specific textual spans where the argumentation is encoded.
Once the textual boundaries are defined, subportions of the argumentative spans are categorized into pre-established argument components such as claims, premises, and rebuttals.
A variety of models have been employed, including Nive Bayes (Moens et al., 2007), SVMs (Mochales and Moens, 2011), RNNs (Niculae et al., 2017; Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019; Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).
The second stage aims to model the relations between the argumentative spans identified in the first stage.
These relations can be of two types: those that exist between different arguments, such as support or attack, and those that exist within an argument, connecting the premises with the claim.
Recent approaches to classifying argumentative relations include using relational models (Trautmann et al., 2020) or enhancing models by incorporating features from various knowledge bases (Kobbe et al., 2019).
Detecting these relations is crucial for modeling the overall structure of the argumentation.
This complexity arises from the need for sophisticated knowledge representation and reasoning capabilities.
After detecting the relations, the discourse structure is represented as an argumentation graph, where arguments form the nodes and the relations between them are the edges.
To address the complexity, some methods simplify the argumentation graph into a tree-structure representation (Peldszus and Stede, 2015; Stab and Gurevych, 2017).
Different methods to generate the argumentation structure include using SVMs (Habernal and Gurevych, 2017; Niculae et al., 2017) or textual entailment approaches (Cabrio and Villata, 2013; Cocarascu et al., 2020).","What are the stages involved in the argument mining framework and how are they implemented?
##
Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM.
Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.
Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).
A variety of models were used for this including Nive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).Relation assignment
The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.
These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).
Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019).
Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate).
As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues.
After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.
To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017).
Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020).
Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly.
Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?""
(Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects.
Vaccinated people become immune to a certain pathogen and do not develop a disease.
Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.
A2: Many vaccines have serious and sometimes deadly side effects.
With many vaccines the immunity is not lifelong.
Sometimes the vaccines itself can cause a serious disease to develop as a side effect.
If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.
Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted.
Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it.
However, consider another example, extracted from an online debate platform Kialo 4 .
Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.
A1: Marvel Universe is better than DC Universe.
A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.
A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.
A4: This is especially true due to his unfortunate passing.
A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.
The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components.
Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level.
Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).
Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict.
Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized.
The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM.

Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).

After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.","Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM.

Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).

After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.","Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.",,"Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

None","Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.","Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).

Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate).

After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017).","What are the stages involved in the argument mining framework and how are they implemented?
##
Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).

Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate).

After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017).","Given a collection of natural language texts, the task is implemented in two stages: argument extraction, where the system identifies the documents containing the argumentative structure and the specific textual spans in which argumentation is encoded.
Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g., claims, premises, rebuttal, etc.).
The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.
These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).
Detecting these relations is necessary to model the overall structure of the argumentation.
After the relations are detected, the discourse structure can then be mapped to a graph representation, called an argumentation graph, with the arguments as nodes and relations as edges.
To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015; Stab and Gurevych, 2017).","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

Given a collection of natural language texts, the task is implemented in two stages: argument extraction, where the system identifies the documents containing the argumentative structure and the specific textual spans in which argumentation is encoded.
Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g., claims, premises, rebuttal, etc.).
The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.
These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).
Detecting these relations is necessary to model the overall structure of the argumentation.
After the relations are detected, the discourse structure can then be mapped to a graph representation, called an argumentation graph, with the arguments as nodes and relations as edges.
To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015; Stab and Gurevych, 2017).","Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

Relation assignment

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).

After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.","What are the stages involved in the argument mining framework and how are they implemented?
##
Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

Relation assignment

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).

After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.","The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.
Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g., claims, premises, rebuttal, etc.).
The next stage involves relation assignment.
The goal of this stage is to model the relations between the argumentative spans identified in the first stage.
These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).
After the relations are detected, the discourse structure can then be mapped to a graph representation, called an argumentation graph, with the arguments as nodes and relations as edges.","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.
Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g., claims, premises, rebuttal, etc.).
The next stage involves relation assignment.
The goal of this stage is to model the relations between the argumentative spans identified in the first stage.
These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).
After the relations are detected, the discourse structure can then be mapped to a graph representation, called an argumentation graph, with the arguments as nodes and relations as edges.","Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).

After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.","What are the stages involved in the argument mining framework and how are they implemented?
##
Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).

After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.","Given a collection of natural language texts, the task is implemented in two stages: argument extraction, where the system identifies the documents containing the argumentative structure and the specific textual spans in which argumentation is encoded.
Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g., claims, premises, rebuttal, etc.).
The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.
These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).
After the relations are detected, the discourse structure can then be mapped to a graph representation, called an argumentation graph, with the arguments as nodes and relations as edges.","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

Given a collection of natural language texts, the task is implemented in two stages: argument extraction, where the system identifies the documents containing the argumentative structure and the specific textual spans in which argumentation is encoded.
Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g., claims, premises, rebuttal, etc.).
The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.
These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).
After the relations are detected, the discourse structure can then be mapped to a graph representation, called an argumentation graph, with the arguments as nodes and relations as edges.","Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.","Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.","Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.

Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).

Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim)."
236460241,A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/ee6d66efc86746d42ace14db30fcbaf9d3380e25,s2,"Code-switching, Borrowing, Transfer, Loan Translation","['p2.0', 'p2.1', 'p2.2']","[""While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruz and Backus, 2009). 4.lkokul-ustanbul-da yap-t-m."", ""primary.school-ACCstanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'"", ""In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher aprs 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.""]","While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruz and Backus, 2009). 4.lkokul-ustanbul-da yap-t-m.

primary.school-ACCstanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher aprs 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","(p2.0) While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruz and Backus, 2009). 4.lkokul-ustanbul-da yap-t-m.

(p2.1) primary.school-ACCstanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'

(p2.2) In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher aprs 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","[['b37', 'b58', None, 'b64', 'b2'], [], []]","[['b37', 'b58', None, 'b64', 'b2'], [], []]",5,"1. While C-S implies active alternation between grammatical systems, borrowing does not.
2. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).
3. When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).
4. Similarly, what looks like complex C-S may not be perceived as switching at all.
5. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.
6. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two.
7. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate.
8. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.
9. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruz and Backus, 2009).
10. 4.lkokul-ustanbul-da yap-t-m.primary.school-ACCstanbul-LOC do-past-1sg.
11. 'I finished primary school in Istanbul.'
12. In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.
13. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French.
14. In Brussels French (Poplack et al., 1988)0, the construction chercher aprs 'look after' (Poplack et al., 1988)1 is a translation of the Dutch equivalent and, in Ontario French (Poplack et al., 1988)2, chercher pour is the translation equivalent of English 'look for'.
15. In reference French (Poplack et al., 1988)3, there is normally no particle following the verb.
16. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies##
Code-switching, Borrowing, Transfer, Loan Translation##
While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruz and Backus, 2009). 4.lkokul-ustanbul-da yap-t-m.

primary.school-ACCstanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher aprs 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","How do code-switching, borrowing, and transfer differ in bilingual language use?","1. How do code-switching, borrowing, and transfer differ in bilingual language use?","Questions:

1. How do code-switching, borrowing, and transfer differ in bilingual language use?

Answer:

(p2.0) While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruz and Backus, 2009). 4.lkokul-ustanbul-da yap-t-m.

(p2.1) primary.school-ACCstanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'

(p2.2) In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher aprs 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies##
Code-switching, Borrowing, Transfer, Loan Translation##
While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruz and Backus, 2009). 4.lkokul-ustanbul-da yap-t-m.",How do linguistic phenomena like code-switching differ from borrowing and loan translation?,1. How do linguistic phenomena like code-switching differ from borrowing and loan translation?,"A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies##
Code-switching, Borrowing, Transfer, Loan Translation##
While C-S implies active alternation between grammatical systems, borrowing does not.

primary.school-ACCstanbul-LOC do-past-1sg.

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.",What distinguishes code-switching from borrowing and transfer in multilingual communication?,1. What distinguishes code-switching from borrowing and transfer in multilingual communication?,"Questions:

1. What distinguishes code-switching from borrowing and transfer in multilingual communication?

Answer:

(p2.0) While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruz and Backus, 2009). 4.lkokul-ustanbul-da yap-t-m.

(p2.1) primary.school-ACCstanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'

(p2.2) In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher aprs 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies##
Code-switching, Borrowing, Transfer, Loan Translation##
While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruz and Backus, 2009). 4.lkokul-ustanbul-da yap-t-m.##
primary.school-ACCstanbul-LOC do-past-1sg.

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.",How do code-switching and borrowing differ in linguistic integration?,1. How do code-switching and borrowing differ in linguistic integration?,"Questions:

1. How do code-switching and borrowing differ in linguistic integration?

Answer:

(p2.0) While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruz and Backus, 2009). 4.lkokul-ustanbul-da yap-t-m.

(p2.1) primary.school-ACCstanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'

(p2.2) In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher aprs 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","How do code-switching and borrowing differ in linguistic integration?
##
While C-S implies active alternation between grammatical systems, borrowing does not.
It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).
When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).
Similarly, what looks like complex C-S may not be perceived as switching at all.
Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.
These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two.
Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate.
Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.
In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruz and Backus, 2009).
4.lkokul-ustanbul-da yap-t-m.primary.school-ACCstanbul-LOC do-past-1sg.
'I finished primary school in Istanbul.'
In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.
Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French.
In Brussels French (Belgium), the construction chercher aprs 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'.
In reference French (France), there is normally no particle following the verb.
The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","While C-S implies active alternation between grammatical systems, borrowing does not.

It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).

Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.

These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two.","While C-S implies active alternation between grammatical systems, borrowing does not.","While C-S implies active alternation between grammatical systems, borrowing does not.

It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).","While C-S implies active alternation between grammatical systems, borrowing does not.

It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).","While C-S implies active alternation between grammatical systems, borrowing does not.

It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).

When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).

Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.

Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.","While C-S implies active alternation between grammatical systems, borrowing does not.

It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).

When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).

Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.

Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.","While C-S implies active alternation between grammatical systems, borrowing does not.

It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).

When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).

Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.

Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.","While C-S implies active alternation between grammatical systems, borrowing does not.

It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).

When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).

Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.

These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two.

Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate.

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.","While C-S implies active alternation between grammatical systems, borrowing does not.

It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).

When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).","While C-S implies active alternation between grammatical systems, borrowing does not.

Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.","How do code-switching and borrowing differ in linguistic integration?
##
While C-S implies active alternation between grammatical systems, borrowing does not.

It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).

When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).

Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.

Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.","Code-switching implies active alternation between grammatical systems, whereas borrowing does not.
Determining whether a single word insertion constitutes borrowing or code-switching requires examining how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).
When analyzed, most lone-item insertions are categorized as one-time borrowings, known as nonce borrowings (Sankoff et al., 1990).
Auer (1999) identifies a spectrum of mixing types: prototypical code-switching is pragmatic and intentional, Language Mixing lacks a pragmatic purpose, and Mixed Languages represent the unified code of a community.
Loan translation and transfer involve using words from one language while adopting the semantics and grammatical constructions from another.
In transfer, the borrowing of grammatical constructions from one language to another occurs without the actual borrowing of words.","Questions:

How do code-switching and borrowing differ in linguistic integration?

Answer:

Code-switching implies active alternation between grammatical systems, whereas borrowing does not.
Determining whether a single word insertion constitutes borrowing or code-switching requires examining how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).
When analyzed, most lone-item insertions are categorized as one-time borrowings, known as nonce borrowings (Sankoff et al., 1990).
Auer (1999) identifies a spectrum of mixing types: prototypical code-switching is pragmatic and intentional, Language Mixing lacks a pragmatic purpose, and Mixed Languages represent the unified code of a community.
Loan translation and transfer involve using words from one language while adopting the semantics and grammatical constructions from another.
In transfer, the borrowing of grammatical constructions from one language to another occurs without the actual borrowing of words.","How do code-switching and borrowing differ in linguistic integration?
##
While C-S implies active alternation between grammatical systems, borrowing does not.
It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).
When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).
Similarly, what looks like complex C-S may not be perceived as switching at all.
Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.
These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two.
Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate.
Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.
In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruz and Backus, 2009).
4.lkokul-ustanbul-da yap-t-m.primary.school-ACCstanbul-LOC do-past-1sg.
'I finished primary school in Istanbul.'
In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.
Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French.
In Brussels French (Belgium), the construction chercher aprs 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'.
In reference French (France), there is normally no particle following the verb.
The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","While C-S implies active alternation between grammatical systems, borrowing does not.

It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).

When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).

Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.","While C-S implies active alternation between grammatical systems, borrowing does not.

When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.","Questions:

How do code-switching and borrowing differ in linguistic integration?

Answer:

While C-S implies active alternation between grammatical systems, borrowing does not.

It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).

When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).

Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.","While C-S implies active alternation between grammatical systems, borrowing does not.

It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).

When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).

Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.

Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.","Questions:

How do code-switching and borrowing differ in linguistic integration?

Answer:

While C-S implies active alternation between grammatical systems, borrowing does not.

It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).

When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).

Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.

Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.",,"Questions:

How do code-switching and borrowing differ in linguistic integration?

Answer:

None",,"Questions:

How do code-switching and borrowing differ in linguistic integration?

Answer:

None",,"How do code-switching and borrowing differ in linguistic integration?
##
None","I'm sorry, but the provided sentence does not contain any information that can be used to answer the question. Could you please provide a different sentence or additional context?","Questions:

How do code-switching and borrowing differ in linguistic integration?

Answer:

I'm sorry, but the provided sentence does not contain any information that can be used to answer the question. Could you please provide a different sentence or additional context?","While C-S implies active alternation between grammatical systems, borrowing does not.

It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).

When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).

Similarly, what looks like complex C-S may not be perceived as switching at all.

Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.

These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two.

Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.","How do code-switching and borrowing differ in linguistic integration?
##
While C-S implies active alternation between grammatical systems, borrowing does not.

It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).

When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).

Similarly, what looks like complex C-S may not be perceived as switching at all.

Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.

These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two.

Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.","Code-switching implies active alternation between grammatical systems, whereas borrowing does not.
Determining whether a lone word insertion constitutes borrowing or code-switching requires considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).
When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).
Similarly, what appears to be complex code-switching may not be perceived as switching at all.
Auer (1999) distinguishes a continuum of mixing types: prototypical code-switching is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.
These can look structurally identical, but the latter can be modeled as a single language, such as Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012), rather than the intertwining of two.
Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.
In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.","Questions:

How do code-switching and borrowing differ in linguistic integration?

Answer:

Code-switching implies active alternation between grammatical systems, whereas borrowing does not.
Determining whether a lone word insertion constitutes borrowing or code-switching requires considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).
When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).
Similarly, what appears to be complex code-switching may not be perceived as switching at all.
Auer (1999) distinguishes a continuum of mixing types: prototypical code-switching is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.
These can look structurally identical, but the latter can be modeled as a single language, such as Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012), rather than the intertwining of two.
Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.
In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.","While C-S implies active alternation between grammatical systems, borrowing does not.

Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.","How do code-switching and borrowing differ in linguistic integration?
##
While C-S implies active alternation between grammatical systems, borrowing does not.

Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.","While code-switching implies active alternation between grammatical systems, borrowing does not.
Auer (1999) distinguishes a continuum of mixing types: prototypical code-switching is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.","Questions:

How do code-switching and borrowing differ in linguistic integration?

Answer:

While code-switching implies active alternation between grammatical systems, borrowing does not.
Auer (1999) distinguishes a continuum of mixing types: prototypical code-switching is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.",,"Questions:

How do code-switching and borrowing differ in linguistic integration?

Answer:

None","While C-S implies active alternation between grammatical systems, borrowing does not.","Questions:

How do code-switching and borrowing differ in linguistic integration?

Answer:

While C-S implies active alternation between grammatical systems, borrowing does not.","While C-S implies active alternation between grammatical systems, borrowing does not.

When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.","Questions:

How do code-switching and borrowing differ in linguistic integration?

Answer:

While C-S implies active alternation between grammatical systems, borrowing does not.

When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s3,Robustness under Distribution Shift,['p3.0'],"[""Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-juss, 2019).""]","Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-juss, 2019).","(p3.0) Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-juss, 2019).","[['b26', 'b21', 'b33', None, 'b67', 'b83']]","[['b26', 'b21', 'b33', None, 'b67', 'b83']]",6,"1. Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.
2. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation.
3. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).
4. Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area.
5. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Hendrycks et al., 2021)0, occupation classification (Hendrycks et al., 2021)1, and neural machine translation (Hendrycks et al., 2021)2.","Measure and Improve Robustness in NLP Models: A Survey##
Robustness under Distribution Shift##
Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-juss, 2019).",What defines robustness in NLP models under natural distribution shifts?,1. What defines robustness in NLP models under natural distribution shifts?,"Questions:

1. What defines robustness in NLP models under natural distribution shifts?

Answer:

(p3.0) Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-juss, 2019).","Measure and Improve Robustness in NLP Models: A Survey##
Robustness under Distribution Shift##
Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-juss, 2019).",How do NLP models maintain robustness under natural distribution shifts?,1. How do NLP models maintain robustness under natural distribution shifts?,"Measure and Improve Robustness in NLP Models: A Survey##
Robustness under Distribution Shift##
Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.",What defines robustness in NLP models under natural distribution shifts?,1. What defines robustness in NLP models under natural distribution shifts?,"Questions:

1. What defines robustness in NLP models under natural distribution shifts?

Answer:

(p3.0) Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-juss, 2019).","Measure and Improve Robustness in NLP Models: A Survey##
Robustness under Distribution Shift##
Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-juss, 2019).##
",How do NLP models maintain robustness under natural distribution shifts?,1. How do NLP models maintain robustness under natural distribution shifts?,"Questions:

1. How do NLP models maintain robustness under natural distribution shifts?

Answer:

(p3.0) Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-juss, 2019).","How do NLP models maintain robustness under natural distribution shifts?
##
Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.
Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation.
In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).
Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area.
For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-juss, 2019).","Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.

Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation.

In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.

In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.

Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation.

In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.

Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation.

In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.

Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation.

In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.

Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation.

In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.

Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation.

In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.

Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation.

In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.

Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation.

In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","How do NLP models maintain robustness under natural distribution shifts?
##
Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.

Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation.

In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","Another line of research focuses on data drawn from a naturally occurring different distribution (Hendrycks et al., 2021), where robustness is defined by the model's performance under distribution shift.
Existing definitions of robustness in this context are more aligned with the concept of domain generalization or out-of-distribution generalization to unforeseen distribution shifts, where the model must generalize without adaptation since the test data is not available during training.
In the context of NLP, robustness to natural distribution shifts also encompasses the idea that models' performance should remain stable despite variations in grammar errors, dialects, speakers, languages (Craig and Washington, 2002; Blodgett et al., 2016; Demszky et al., 2021), or when faced with newly collected datasets for the same task but from different domains (Miller et al., 2020).","Questions:

How do NLP models maintain robustness under natural distribution shifts?

Answer:

Another line of research focuses on data drawn from a naturally occurring different distribution (Hendrycks et al., 2021), where robustness is defined by the model's performance under distribution shift.
Existing definitions of robustness in this context are more aligned with the concept of domain generalization or out-of-distribution generalization to unforeseen distribution shifts, where the model must generalize without adaptation since the test data is not available during training.
In the context of NLP, robustness to natural distribution shifts also encompasses the idea that models' performance should remain stable despite variations in grammar errors, dialects, speakers, languages (Craig and Washington, 2002; Blodgett et al., 2016; Demszky et al., 2021), or when faced with newly collected datasets for the same task but from different domains (Miller et al., 2020).","How do NLP models maintain robustness under natural distribution shifts?
##
Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.
Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation.
In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).
Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area.
For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-juss, 2019).","Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.

Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation.

In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).",None,"Questions:

How do NLP models maintain robustness under natural distribution shifts?

Answer:

Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.

Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation.

In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.

In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","Questions:

How do NLP models maintain robustness under natural distribution shifts?

Answer:

Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.

In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).",,"Questions:

How do NLP models maintain robustness under natural distribution shifts?

Answer:

None",,"Questions:

How do NLP models maintain robustness under natural distribution shifts?

Answer:

None",,"How do NLP models maintain robustness under natural distribution shifts?
##
None","I'm sorry, but the provided sentence does not contain any information that can be used to answer the question.","Questions:

How do NLP models maintain robustness under natural distribution shifts?

Answer:

I'm sorry, but the provided sentence does not contain any information that can be used to answer the question.",,"How do NLP models maintain robustness under natural distribution shifts?
##
None","I'm sorry, but the provided sentence does not contain any information that can be used to answer the question.","Questions:

How do NLP models maintain robustness under natural distribution shifts?

Answer:

I'm sorry, but the provided sentence does not contain any information that can be used to answer the question.","In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","How do NLP models maintain robustness under natural distribution shifts?
##
In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","In the context of NLP, robustness to natural distribution shifts means that models' performance should not degrade due to differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002; Blodgett et al., 2016; Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","Questions:

How do NLP models maintain robustness under natural distribution shifts?

Answer:

In the context of NLP, robustness to natural distribution shifts means that models' performance should not degrade due to differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002; Blodgett et al., 2016; Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).",,"Questions:

How do NLP models maintain robustness under natural distribution shifts?

Answer:

None",,"Questions:

How do NLP models maintain robustness under natural distribution shifts?

Answer:

None",None,"Questions:

How do NLP models maintain robustness under natural distribution shifts?

Answer:

None"
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Linguistics, Computer Science",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s9,Semantic Understanding,"['p9.0', 'p9.1', 'p9.2']","['Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.', 'Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b). Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019). Different types of tokens within the instruction also function differently (Zhu et al., 2021b). Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).', 'Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021). The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).']","Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b). Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019). Different types of tokens within the instruction also function differently (Zhu et al., 2021b). Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021). The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","(p9.0) Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

(p9.1) Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b). Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019). Different types of tokens within the instruction also function differently (Zhu et al., 2021b). Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).

(p9.2) Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021). The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","[[], ['b22', 'b21', None, 'b4', 'b28'], ['b21', None]]","[[], ['b22', 'b21', None, 'b4', 'b28'], ['b21', None]]",7,"1. Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.
2. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.
3. Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN.
4. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b).
5. Therefore, it is important to find the feature(s) that best improve performance.
6. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).
7. Different types of tokens within the instruction also function differently (Zhu et al., 2021b).
8. Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).
9. Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).
10. The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions##
Semantic Understanding##
Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b). Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019). Different types of tokens within the instruction also function differently (Zhu et al., 2021b). Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021). The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).",How does semantic understanding enhance performance in Vision-and-Language Navigation tasks?,1. How does semantic understanding enhance performance in Vision-and-Language Navigation tasks?,"Questions:

1. How does semantic understanding enhance performance in Vision-and-Language Navigation tasks?

Answer:

(p9.0) Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

(p9.1) Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b). Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019). Different types of tokens within the instruction also function differently (Zhu et al., 2021b). Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).

(p9.2) Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021). The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions##
Semantic Understanding##
Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.",How does semantic understanding enhance performance in vision-and-language navigation tasks?,1. How does semantic understanding enhance performance in vision-and-language navigation tasks?,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions##
Semantic Understanding##
Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.

Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN.

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).",How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?,1. How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?,"Questions:

1. How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

(p9.0) Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

(p9.1) Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b). Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019). Different types of tokens within the instruction also function differently (Zhu et al., 2021b). Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).

(p9.2) Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021). The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions##
Semantic Understanding##
Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.##
Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN.

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).",How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?,1. How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?,"Questions:

1. How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

(p9.0) Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

(p9.1) Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b). Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019). Different types of tokens within the instruction also function differently (Zhu et al., 2021b). Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).

(p9.2) Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021). The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?
##
Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.
In addition to the raw features, high-level semantic representations also improve performance in unseen environments.
Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN.
The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b).
Therefore, it is important to find the feature(s) that best improve performance.
High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).
Different types of tokens within the instruction also function differently (Zhu et al., 2021b).
Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).
Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).
The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).",,,,"Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.

In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Different types of tokens within the instruction also function differently (Zhu et al., 2021b).

Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).

The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.

In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).","In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).","In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).

The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).

The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.

In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).","How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?
##
Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.

In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Different types of tokens within the instruction also function differently (Zhu et al., 2021b).

Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).

The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","Semantic understanding enhances Vision-and-Language Navigation (VLN) tasks by incorporating knowledge about important features in VLN.
High-level semantic representations also improve performance in unseen environments.
High-level features such as visual appearance, route structure, and detected objects are more effective than the low-level visual features extracted by CNN (Hu et al., 2019).
Different types of tokens within the instruction play distinct roles (Zhu et al., 2021b).
Extracting and encoding the object tokens and direction tokens are crucial steps in this process (Qi et al., 2020a; Zhu et al., 2021b).
Semantic connections between different modalities such as actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned using an attention mechanism (Qi et al., 2020a; Gao et al., 2021).
The soft alignment technique emphasizes the parts of the instruction that are relevant to the current step (Landi et al., 2019; Zhang et al., 2020a).","Questions:

How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

Semantic understanding enhances Vision-and-Language Navigation (VLN) tasks by incorporating knowledge about important features in VLN.
High-level semantic representations also improve performance in unseen environments.
High-level features such as visual appearance, route structure, and detected objects are more effective than the low-level visual features extracted by CNN (Hu et al., 2019).
Different types of tokens within the instruction play distinct roles (Zhu et al., 2021b).
Extracting and encoding the object tokens and direction tokens are crucial steps in this process (Qi et al., 2020a; Zhu et al., 2021b).
Semantic connections between different modalities such as actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned using an attention mechanism (Qi et al., 2020a; Gao et al., 2021).
The soft alignment technique emphasizes the parts of the instruction that are relevant to the current step (Landi et al., 2019; Zhang et al., 2020a).","How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?
##
Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.
In addition to the raw features, high-level semantic representations also improve performance in unseen environments.
Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN.
The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b).
Therefore, it is important to find the feature(s) that best improve performance.
High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).
Different types of tokens within the instruction also function differently (Zhu et al., 2021b).
Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).
Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).
The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.

In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).",None,"Questions:

How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.

In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).","Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.

In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).","Questions:

How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.

In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).",,"Questions:

How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

None","High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).","Questions:

How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).","Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.

In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Different types of tokens within the instruction also function differently (Zhu et al., 2021b).

Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).

The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?
##
Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.

In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Different types of tokens within the instruction also function differently (Zhu et al., 2021b).

Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).

The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.
In addition to the raw features, high-level semantic representations also improve performance in unseen environments.
High-level features such as visual appearance, route structure, and detected objects outperform the low-level visual features extracted by CNN (Hu et al., 2019).
Different types of tokens within the instruction also function differently (Zhu et al., 2021b).
Extracting these tokens and encoding the object tokens and direction tokens are crucial (Qi et al., 2020a; Zhu et al., 2021b).
Inter-modality semantic connections between actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with an attention mechanism (Qi et al., 2020a; Gao et al., 2021).
The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019; Zhang et al., 2020a).","Questions:

How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.
In addition to the raw features, high-level semantic representations also improve performance in unseen environments.
High-level features such as visual appearance, route structure, and detected objects outperform the low-level visual features extracted by CNN (Hu et al., 2019).
Different types of tokens within the instruction also function differently (Zhu et al., 2021b).
Extracting these tokens and encoding the object tokens and direction tokens are crucial (Qi et al., 2020a; Zhu et al., 2021b).
Inter-modality semantic connections between actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with an attention mechanism (Qi et al., 2020a; Gao et al., 2021).
The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019; Zhang et al., 2020a).","Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.

In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).","How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?
##
Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.

In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).","Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.
High-level semantic representations, in addition to raw features, also improve performance in unseen environments.
High-level features such as visual appearance, route structure, and detected objects outperform the low-level visual features extracted by CNN (Hu et al., 2019).
Inter-modality semantic connections between actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with an attention mechanism (Qi et al., 2020a; Gao et al., 2021).","Questions:

How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.
High-level semantic representations, in addition to raw features, also improve performance in unseen environments.
High-level features such as visual appearance, route structure, and detected objects outperform the low-level visual features extracted by CNN (Hu et al., 2019).
Inter-modality semantic connections between actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with an attention mechanism (Qi et al., 2020a; Gao et al., 2021).","Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.

In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).","How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?
##
Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.

In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).","Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.
In addition to the raw features, high-level semantic representations also improve performance in unseen environments.
High-level features such as visual appearance, route structure, and detected objects outperform the low-level visual features extracted by CNN (Hu et al., 2019).
Inter-modality semantic connections between actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with an attention mechanism (Qi et al., 2020a; Gao et al., 2021).","Questions:

How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.
In addition to the raw features, high-level semantic representations also improve performance in unseen environments.
High-level features such as visual appearance, route structure, and detected objects outperform the low-level visual features extracted by CNN (Hu et al., 2019).
Inter-modality semantic connections between actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with an attention mechanism (Qi et al., 2020a; Gao et al., 2021).",,"Questions:

How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

None","In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).","Questions:

How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).",None,"Questions:

How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

None"
248426721,What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,Computer Science,https://www.semanticscholar.org/paper/29a369d83a7d6a49f6a3259bee23bd4d95db0b16,s2,The Relation Extraction Task,"['p2.0', 'p2.1', 'p2.2']","['Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.', 'One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.', 'Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.']","Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","(p2.0) Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

(p2.1) One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

(p2.2) Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","[[], ['b32', 'b36', 'b7', None, 'b3'], ['b36', 'b56', None, 'b43', 'b44', 'b54']]","[[], ['b32', 'b36', 'b7', None, 'b3'], ['b36', 'b56', None, 'b43', 'b44', 'b54']]",11,"1. Conceptually, RE involves a pipeline of steps (see Figure 2).
2. Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.
3. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).
4. 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.
5. One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.
6. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).
7. As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered).
8. The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.
9. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (NER)0 label.
10. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (NER)1.
11. Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.
12. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NER)2 relation (NER)3.
13. Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set.
14. 6 What Do You Mean by Relation Extraction?
15. RE studies rarely address the whole pipeline.
16. We 5 Some studies divide the entity extraction into two substeps: identification (NER)4, and subsequent classification into entity types.
17. 6 Some studies name such relation Other (NER)5. analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER)6.
18. Table 2 shows such investigation.
19. We leave out from this analysis (NER)7 papers which make use of distant supervision or which somehow involve knowledge bases, (NER)8 shared task papers, (NER)9 the bioNLP field, (MD)0 temporal RE, and (MD)1 Open RE.
20. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.
21. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (MD)2.
22. Our analysis further shows that it is difficult to determine the RI setup.
23. While RC is always performed, the situation is different for RI (MD)3.
24. Sometimes RI is clearly not done (MD)4, but most of the times it is either not clear from the paper, or done in a simplified scenario (MD)5.
25. As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (MD)6 are handled.
26. These details are utterly important as they impact both model estimation and evaluation.
27. Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.
28. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (MD)7.
29. However, Taill et al. (MD)8 recently discussed the challenge of properly evaluating such complex models.
30. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (MD)9.
31. They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.
32. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.
33. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.
34. Since the output label space is different, separate encoders could better capture distinct contextual information.
35. At the moment it is not clear if one approach is more suitable than the other for RE.
36. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification##
The Relation Extraction Task##
Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.",What are the current challenges and considerations in evaluating relation extraction models?,1. What are the current challenges and considerations in evaluating relation extraction models?,"Questions:

1. What are the current challenges and considerations in evaluating relation extraction models?

Answer:

(p2.0) Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

(p2.1) One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

(p2.2) Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification##
The Relation Extraction Task##
Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.",What are the initial steps in the relation extraction process from raw text?,1. What are the initial steps in the relation extraction process from raw text?,"What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification##
The Relation Extraction Task##
Conceptually, RE involves a pipeline of steps (see Figure 2).

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.",What are the main components of the relation extraction process in NLP?,1. What are the main components of the relation extraction process in NLP?,"Questions:

1. What are the main components of the relation extraction process in NLP?

Answer:

(p2.0) Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

(p2.1) One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

(p2.2) Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification##
The Relation Extraction Task##
Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.##
One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.",What are the primary steps involved in the relation extraction process from text?,1. What are the primary steps involved in the relation extraction process from text?,"Questions:

1. What are the primary steps involved in the relation extraction process from text?

Answer:

(p2.0) Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

(p2.1) One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

(p2.2) Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","What are the primary steps involved in the relation extraction process from text?
##
Conceptually, RE involves a pipeline of steps (see Figure 2).
Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.
Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).
5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.
One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.
This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).
As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered).
The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.
Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.
Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).
Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.
Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).
Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set.
6 What Do You Mean by Relation Extraction?
RE studies rarely address the whole pipeline.
We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.
6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC).
Table 2 shows such investigation.
We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE.
The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.
Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020).
Our analysis further shows that it is difficult to determine the RI setup.
While RC is always performed, the situation is different for RI (or no-rel).
Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances).
As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled.
These details are utterly important as they impact both model estimation and evaluation.
Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.
Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021).
However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models.
They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a).
They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.
While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.
Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.
Since the output label space is different, separate encoders could better capture distinct contextual information.
At the moment it is not clear if one approach is more suitable than the other for RE.
For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.","One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.","One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.",,"Conceptually, RE involves a pipeline of steps (see Figure 2).

Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.

Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.

Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.

Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021).","Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.

Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.

Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).

Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.

Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).","Conceptually, RE involves a pipeline of steps (see Figure 2).

Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.

Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.

Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.","Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).

5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.

Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.

Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).

Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.

As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled.","Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.

Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.

Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.

As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled.","Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.","What are the primary steps involved in the relation extraction process from text?
##
Conceptually, RE involves a pipeline of steps (see Figure 2).

Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.

Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.

Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.

Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021).","Relation extraction involves a conceptual pipeline of steps.
The first step involves identifying the entities from the raw text and possibly categorizing them into types.
One approach involves two primary steps: Relation Identification (RI) and subsequent Relation Classification (RC).
The final step, Relation Classification (RC), typically involves multi-class classification to assign a specific relation type to the positively identified samples from the previous step.
Some studies combine Relation Identification and Relation Classification into a single step by introducing a no-relation label (Seganti et al., 2021).
Other studies simplify the process by focusing solely on Relation Classification, operating under the assumption that a relation exists between two entities and the task is to identify its type, without considering a no-relation option.
The traditional relation extraction pipeline is prone to error propagation by sub-tasks due to its sequential nature.
To address this issue, some researchers have proposed joint entity and relation extraction approaches (Miwa and Bansal, 2016; Zhang et al., 2017a; Bekoulis et al., 2018a,b; Wang and Lu, 2020; Wang et al., 2021).","Questions:

What are the primary steps involved in the relation extraction process from text?

Answer:

Relation extraction involves a conceptual pipeline of steps.
The first step involves identifying the entities from the raw text and possibly categorizing them into types.
One approach involves two primary steps: Relation Identification (RI) and subsequent Relation Classification (RC).
The final step, Relation Classification (RC), typically involves multi-class classification to assign a specific relation type to the positively identified samples from the previous step.
Some studies combine Relation Identification and Relation Classification into a single step by introducing a no-relation label (Seganti et al., 2021).
Other studies simplify the process by focusing solely on Relation Classification, operating under the assumption that a relation exists between two entities and the task is to identify its type, without considering a no-relation option.
The traditional relation extraction pipeline is prone to error propagation by sub-tasks due to its sequential nature.
To address this issue, some researchers have proposed joint entity and relation extraction approaches (Miwa and Bansal, 2016; Zhang et al., 2017a; Bekoulis et al., 2018a,b; Wang and Lu, 2020; Wang et al., 2021).","What are the primary steps involved in the relation extraction process from text?
##
Conceptually, RE involves a pipeline of steps (see Figure 2).
Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.
Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).
5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.
One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.
This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).
As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered).
The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.
Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.
Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).
Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.
Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).
Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set.
6 What Do You Mean by Relation Extraction?
RE studies rarely address the whole pipeline.
We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.
6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC).
Table 2 shows such investigation.
We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE.
The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.
Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020).
Our analysis further shows that it is difficult to determine the RI setup.
While RC is always performed, the situation is different for RI (or no-rel).
Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances).
As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled.
These details are utterly important as they impact both model estimation and evaluation.
Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.
Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021).
However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models.
They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a).
They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.
While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.
Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.
Since the output label space is different, separate encoders could better capture distinct contextual information.
At the moment it is not clear if one approach is more suitable than the other for RE.
For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.","Conceptually, RE involves a pipeline of steps (see Figure 2).

Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.","Questions:

What are the primary steps involved in the relation extraction process from text?

Answer:

Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.","Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.","Questions:

What are the primary steps involved in the relation extraction process from text?

Answer:

Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.","Conceptually, RE involves a pipeline of steps (see Figure 2).

Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.","Questions:

What are the primary steps involved in the relation extraction process from text?

Answer:

Conceptually, RE involves a pipeline of steps (see Figure 2).

Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.",,"Questions:

What are the primary steps involved in the relation extraction process from text?

Answer:

None","Conceptually, RE involves a pipeline of steps (see Figure 2).



Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.



Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).



One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC), as illustrated in Figure 2.



This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).



The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.



Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.



Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).



Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.



Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).



RE studies rarely address the whole pipeline.



We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.



The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.



Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020).



Our analysis further shows that it is difficult to determine the RI setup.



While RC is always performed, the situation is different for RI (or no-rel).



Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances).



As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled.



These details are utterly important as they impact both model estimation and evaluation.



Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.



Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016; Zhang et al., 2017a; Bekoulis et al., 2018a,b; Wang and Lu, 2020; Wang et al., 2021).



However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models.



They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting proposed by Bekoulis et al. (2018a).



They observe unfair comparisons and overestimations of end-to-end models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.



While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.



Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.



Since the output label space is different, separate encoders could better capture distinct contextual information.



At the moment it is not clear if one approach is more suitable than the other for RE.



For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","What are the primary steps involved in the relation extraction process from text?
##
Conceptually, RE involves a pipeline of steps (see Figure 2).



Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.



Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).



One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC), as illustrated in Figure 2.



This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).



The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.



Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.



Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).



Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.



Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).



RE studies rarely address the whole pipeline.



We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.



The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.



Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020).



Our analysis further shows that it is difficult to determine the RI setup.



While RC is always performed, the situation is different for RI (or no-rel).



Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances).



As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled.



These details are utterly important as they impact both model estimation and evaluation.



Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.



Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016; Zhang et al., 2017a; Bekoulis et al., 2018a,b; Wang and Lu, 2020; Wang et al., 2021).



However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models.



They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting proposed by Bekoulis et al. (2018a).



They observe unfair comparisons and overestimations of end-to-end models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.



While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.



Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.



Since the output label space is different, separate encoders could better capture distinct contextual information.



At the moment it is not clear if one approach is more suitable than the other for RE.



For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","Conceptually, relation extraction (RE) involves a pipeline of steps.
The primary steps typically include identifying entities within the text, classifying these entities into predefined categories, and then determining the relationships between these entities.
The first step consists of identifying the entities and assigning them a type.
The next step involves classifying these entities into predefined categories.
Entities can be either nominals or named entities, which means the process involves either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).
The final step is to determine the relationships between these identified and classified entities.
One approach is to take two steps: Relation Identification (RI) and subsequent Relation Classification (RC).
One approach is to take two steps: Relation Identification (RI) and subsequent Relation Classification (RC).
This involves first identifying from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).
This involves first identifying from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).
The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.
The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.
Some studies merge RI and RC (Seganti et al., 2021) into one step by adding a no-relation (no-rel) label.
Some studies merge RI and RC (Seganti et al., 2021) into one step by adding a no-relation (no-rel) label.
Other studies instead reduce the task to RC, assuming there exists a relation between two entities and the task is to determine the type (without a no-rel label).
Other studies instead reduce the task to RC, assuming there exists a relation between two entities and the task is to determine the type (without a no-rel label).
Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.
Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.
Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).
Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).
RE studies rarely address the whole pipeline.
RE studies rarely address the entire pipeline.
Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.
Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.
The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.
The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.
Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020).
Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020).
Our analysis further shows that it is difficult to determine the RI setup.
Our analysis further shows that it is difficult to determine the RI setup.
While RC is always performed, the situation is different for RI (or no-rel).
While RC is always performed, the situation is different for RI (or no-rel).
Sometimes RI is clearly not performed, as the paper assumes that every instance contains at least one relation. However, most of the time, it is either not explicitly mentioned in the paper or conducted in a simplified scenario, such as using datasets that already exclude most of the no-rel entity pair instances.
Sometimes RI is clearly not performed, as the paper assumes that every instance contains at least one relation. However, most of the time, it is either not explicitly mentioned in the paper or conducted in a simplified scenario, such as using datasets that already exclude most of the no-rel entity pair instances.
As this blurriness hampers fair evaluation, it is proposed that studies clearly state which step they include, i.e., whether the work focuses on RC, RI+RC, or the full RE pipeline, and how special cases (no-rel and NOTA) are handled.
As this blurriness hampers fair evaluation, it is proposed that studies clearly state which step they include, i.e., whether the work focuses on RC, RI+RC, or the full RE pipeline, and how special cases (no-rel and NOTA) are handled.
These details are utterly important as they impact both model estimation and evaluation.
These details are utterly important as they impact both model estimation and evaluation.
The traditional RE pipeline is, by definition, prone to error propagation by sub-tasks.
The traditional RE pipeline is, by definition, prone to error propagation by sub-tasks.
Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016; Zhang et al., 2017a; Bekoulis et al., 2018a,b; Wang and Lu, 2020; Wang et al., 2021).
Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016; Zhang et al., 2017a; Bekoulis et al., 2018a,b; Wang and Lu, 2020; Wang et al., 2021).
However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models.
However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models.
Recent studies have surveyed the evaluation metrics of end-to-end RE, referring to the Strict, Boundaries, and Relaxed evaluation settings proposed by Bekoulis et al. (2018a).
Recent studies have surveyed the evaluation metrics of end-to-end RE, referring to the Strict, Boundaries, and Relaxed evaluation settings proposed by Bekoulis et al. (2018a).
They observe unfair comparisons and overestimations of end-to-end models and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.
They observe unfair comparisons and overestimations of end-to-end models and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.
While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.
While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.
Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT), using separate encoders can surpass existing joint models.
Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT), using separate encoders can surpass existing joint models.
Since the output label space is different, separate encoders could better capture distinct contextual information.
Since the output label space is different, separate encoders could better capture distinct contextual information.
At the moment, it is not clear if one approach is more suitable than the other for RE.
At the moment, it is not clear if one approach is more suitable than the other for RE.
For this reason and to achieve a closer examination of sub-domains in the scientific field, the pipeline approach is followed, with a focus on the RC task.","Questions:

What are the primary steps involved in the relation extraction process from text?

Answer:

Conceptually, relation extraction (RE) involves a pipeline of steps.
The primary steps typically include identifying entities within the text, classifying these entities into predefined categories, and then determining the relationships between these entities.
The first step consists of identifying the entities and assigning them a type.
The next step involves classifying these entities into predefined categories.
Entities can be either nominals or named entities, which means the process involves either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).
The final step is to determine the relationships between these identified and classified entities.
One approach is to take two steps: Relation Identification (RI) and subsequent Relation Classification (RC).
One approach is to take two steps: Relation Identification (RI) and subsequent Relation Classification (RC).
This involves first identifying from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).
This involves first identifying from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).
The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.
The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.
Some studies merge RI and RC (Seganti et al., 2021) into one step by adding a no-relation (no-rel) label.
Some studies merge RI and RC (Seganti et al., 2021) into one step by adding a no-relation (no-rel) label.
Other studies instead reduce the task to RC, assuming there exists a relation between two entities and the task is to determine the type (without a no-rel label).
Other studies instead reduce the task to RC, assuming there exists a relation between two entities and the task is to determine the type (without a no-rel label).
Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.
Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.
Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).
Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).
RE studies rarely address the whole pipeline.
RE studies rarely address the entire pipeline.
Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.
Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.
The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.
The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.
Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020).
Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020).
Our analysis further shows that it is difficult to determine the RI setup.
Our analysis further shows that it is difficult to determine the RI setup.
While RC is always performed, the situation is different for RI (or no-rel).
While RC is always performed, the situation is different for RI (or no-rel).
Sometimes RI is clearly not performed, as the paper assumes that every instance contains at least one relation. However, most of the time, it is either not explicitly mentioned in the paper or conducted in a simplified scenario, such as using datasets that already exclude most of the no-rel entity pair instances.
Sometimes RI is clearly not performed, as the paper assumes that every instance contains at least one relation. However, most of the time, it is either not explicitly mentioned in the paper or conducted in a simplified scenario, such as using datasets that already exclude most of the no-rel entity pair instances.
As this blurriness hampers fair evaluation, it is proposed that studies clearly state which step they include, i.e., whether the work focuses on RC, RI+RC, or the full RE pipeline, and how special cases (no-rel and NOTA) are handled.
As this blurriness hampers fair evaluation, it is proposed that studies clearly state which step they include, i.e., whether the work focuses on RC, RI+RC, or the full RE pipeline, and how special cases (no-rel and NOTA) are handled.
These details are utterly important as they impact both model estimation and evaluation.
These details are utterly important as they impact both model estimation and evaluation.
The traditional RE pipeline is, by definition, prone to error propagation by sub-tasks.
The traditional RE pipeline is, by definition, prone to error propagation by sub-tasks.
Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016; Zhang et al., 2017a; Bekoulis et al., 2018a,b; Wang and Lu, 2020; Wang et al., 2021).
Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016; Zhang et al., 2017a; Bekoulis et al., 2018a,b; Wang and Lu, 2020; Wang et al., 2021).
However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models.
However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models.
Recent studies have surveyed the evaluation metrics of end-to-end RE, referring to the Strict, Boundaries, and Relaxed evaluation settings proposed by Bekoulis et al. (2018a).
Recent studies have surveyed the evaluation metrics of end-to-end RE, referring to the Strict, Boundaries, and Relaxed evaluation settings proposed by Bekoulis et al. (2018a).
They observe unfair comparisons and overestimations of end-to-end models and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.
They observe unfair comparisons and overestimations of end-to-end models and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.
While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.
While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.
Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT), using separate encoders can surpass existing joint models.
Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT), using separate encoders can surpass existing joint models.
Since the output label space is different, separate encoders could better capture distinct contextual information.
Since the output label space is different, separate encoders could better capture distinct contextual information.
At the moment, it is not clear if one approach is more suitable than the other for RE.
At the moment, it is not clear if one approach is more suitable than the other for RE.
For this reason and to achieve a closer examination of sub-domains in the scientific field, the pipeline approach is followed, with a focus on the RC task.","Conceptually, RE involves a pipeline of steps (see Figure 2).

Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.

Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.

Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).

Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.

Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).

RE studies rarely address the whole pipeline.

We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.

Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020).

Our analysis further shows that it is difficult to determine the RI setup.

While RC is always performed, the situation is different for RI (or no-rel).

Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances).

As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled.

These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.

Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021).

However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models.

They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a).

They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.

While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.

Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.

Since the output label space is different, separate encoders could better capture distinct contextual information.

At the moment it is not clear if one approach is more suitable than the other for RE.

For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","What are the primary steps involved in the relation extraction process from text?
##
Conceptually, RE involves a pipeline of steps (see Figure 2).

Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.

Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.

Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).

Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.

Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).

RE studies rarely address the whole pipeline.

We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.

Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020).

Our analysis further shows that it is difficult to determine the RI setup.

While RC is always performed, the situation is different for RI (or no-rel).

Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances).

As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled.

These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.

Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021).

However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models.

They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a).

They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.

While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.

Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.

Since the output label space is different, separate encoders could better capture distinct contextual information.

At the moment it is not clear if one approach is more suitable than the other for RE.

For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","Conceptually, relation extraction (RE) involves a pipeline of steps.
The first step involves identifying the entities in the raw text and assigning them a type.
Entities can be either nominals or named entities, and hence the process involves either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).
One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC).
This involves first identifying from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).
The last step, Relation Classification (RC), is usually a multi-class classification to assign a relation type \( r \) to the positive samples from the previous step.
Some studies merge RI and RC (Seganti et al., 2021) into one step by adding a no-relation (no-rel) label.
Other studies instead reduce the task to RC, assuming there exists a relation between two entities and the task is to determine the type (without a no-rel label).
Regardless, RI is influenced by the RC setup: relations that are not in the RC label set are considered as negative samples in the RI phase.
Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).
RE studies rarely address the whole pipeline.
Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.
Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020).
Our analysis further shows that it is difficult to determine the RI setup.
While RC is always performed, the situation is different for RI (or no-rel).
Sometimes RI is clearly not performed, as the paper assumes a scenario in which every instance contains at least one relation. However, most of the time, it is either not clear from the paper or done in a simplified scenario, such as using datasets that already exclude most of the no-rel entity pair instances.
As this blurriness hampers fair evaluation, it is proposed that studies clearly state which step they include, i.e., whether the work focuses on RC, RI+RC, or the full RE pipeline, and how special cases (no-rel and NOTA) are handled.
These details are utterly important as they impact both model estimation and evaluation.
The traditional RE pipeline is, by definition, prone to error propagation by sub-tasks.
Joint entity and relation extraction approaches have been proposed to alleviate the problem of error propagation in the traditional RE pipeline (Miwa and Bansal, 2016; Zhang et al., 2017a; Bekoulis et al., 2018a,b; Wang and Lu, 2020; Wang et al., 2021).
However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models.
Recent studies have surveyed the evaluation metrics of end-to-end RE, referring to the Strict, Boundaries, and Relaxed evaluation settings proposed by Bekoulis et al. (2018a).
They observe unfair comparisons and overestimations of end-to-end models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.
While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.
Zhong and Chen (2021) found that incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.
Since the output label space is different, separate encoders could better capture distinct contextual information.
At the moment, it is not clear if one approach is more suitable than the other for RE.
For this reason and to closely examine sub-domains in the scientific field, the pipeline approach is followed, with a focus on the RC task.","Questions:

What are the primary steps involved in the relation extraction process from text?

Answer:

Conceptually, relation extraction (RE) involves a pipeline of steps.
The first step involves identifying the entities in the raw text and assigning them a type.
Entities can be either nominals or named entities, and hence the process involves either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).
One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC).
This involves first identifying from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).
The last step, Relation Classification (RC), is usually a multi-class classification to assign a relation type \( r \) to the positive samples from the previous step.
Some studies merge RI and RC (Seganti et al., 2021) into one step by adding a no-relation (no-rel) label.
Other studies instead reduce the task to RC, assuming there exists a relation between two entities and the task is to determine the type (without a no-rel label).
Regardless, RI is influenced by the RC setup: relations that are not in the RC label set are considered as negative samples in the RI phase.
Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).
RE studies rarely address the whole pipeline.
Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types.
Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill et al., 2020).
Our analysis further shows that it is difficult to determine the RI setup.
While RC is always performed, the situation is different for RI (or no-rel).
Sometimes RI is clearly not performed, as the paper assumes a scenario in which every instance contains at least one relation. However, most of the time, it is either not clear from the paper or done in a simplified scenario, such as using datasets that already exclude most of the no-rel entity pair instances.
As this blurriness hampers fair evaluation, it is proposed that studies clearly state which step they include, i.e., whether the work focuses on RC, RI+RC, or the full RE pipeline, and how special cases (no-rel and NOTA) are handled.
These details are utterly important as they impact both model estimation and evaluation.
The traditional RE pipeline is, by definition, prone to error propagation by sub-tasks.
Joint entity and relation extraction approaches have been proposed to alleviate the problem of error propagation in the traditional RE pipeline (Miwa and Bansal, 2016; Zhang et al., 2017a; Bekoulis et al., 2018a,b; Wang and Lu, 2020; Wang et al., 2021).
However, Taill et al. (2020) recently discussed the challenge of properly evaluating such complex models.
Recent studies have surveyed the evaluation metrics of end-to-end RE, referring to the Strict, Boundaries, and Relaxed evaluation settings proposed by Bekoulis et al. (2018a).
They observe unfair comparisons and overestimations of end-to-end models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.
While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.
Zhong and Chen (2021) found that incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.
Since the output label space is different, separate encoders could better capture distinct contextual information.
At the moment, it is not clear if one approach is more suitable than the other for RE.
For this reason and to closely examine sub-domains in the scientific field, the pipeline approach is followed, with a focus on the RC task.","Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.","What are the primary steps involved in the relation extraction process from text?
##
Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.","The first step in the relation extraction process from text involves identifying the entities and eventually assigning them a type.
One approach is to take two steps: Relation Identification (RI) and subsequent Relation Classification (RC).
The last step, Relation Classification (RC), is usually a multi-class classification to assign a relation type to the positive samples from the previous step.","Questions:

What are the primary steps involved in the relation extraction process from text?

Answer:

The first step in the relation extraction process from text involves identifying the entities and eventually assigning them a type.
One approach is to take two steps: Relation Identification (RI) and subsequent Relation Classification (RC).
The last step, Relation Classification (RC), is usually a multi-class classification to assign a relation type to the positive samples from the previous step.","Conceptually, RE involves a pipeline of steps (see Figure 2).

Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.

Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.

Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).","Questions:

What are the primary steps involved in the relation extraction process from text?

Answer:

Conceptually, RE involves a pipeline of steps (see Figure 2).

Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.

Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.

Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).","Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.","Questions:

What are the primary steps involved in the relation extraction process from text?

Answer:

Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.","Conceptually, RE involves a pipeline of steps (see Figure 2).

Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.","Questions:

What are the primary steps involved in the relation extraction process from text?

Answer:

Conceptually, RE involves a pipeline of steps (see Figure 2).

Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s3,Output Uncertainty,"['p3.0', 'p3.1']","['Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).', ""Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).""]","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","(p3.0) Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

(p3.1) Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","[['b11', 'b91', 'b49', 'b25', None, 'b10', 'b12', 'b14', 'b19'], [None, 'b72']]","[['b11', 'b91', 'b49', 'b25', None, 'b10', 'b12', 'b14', 'b19'], [None, 'b72']]",11,"1. Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.
2. It prefers the most uncertain instances judged by the model outputs.
3. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).
4. Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.
5. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).
6. Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.
7. If an instance is near the decision boundary, the model's outputs may be different within its local region.
8. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","A Survey of Active Learning for Natural Language Processing##
Output Uncertainty##
Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).",What are the primary strategies for selecting instances in uncertainty-based active learning?,1. What are the primary strategies for selecting instances in uncertainty-based active learning?,"Questions:

1. What are the primary strategies for selecting instances in uncertainty-based active learning?

Answer:

(p3.0) Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

(p3.1) Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","A Survey of Active Learning for Natural Language Processing##
Output Uncertainty##
Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).",What are the primary strategies for uncertainty sampling in active learning for NLP?,1. What are the primary strategies for uncertainty sampling in active learning for NLP?,"A Survey of Active Learning for Natural Language Processing##
Output Uncertainty##
Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.",What are common strategies for measuring output uncertainty in active learning for NLP?,1. What are common strategies for measuring output uncertainty in active learning for NLP?,"Questions:

1. What are common strategies for measuring output uncertainty in active learning for NLP?

Answer:

(p3.0) Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

(p3.1) Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","A Survey of Active Learning for Natural Language Processing##
Output Uncertainty##
Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).##
Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.",What are the primary strategies for uncertainty sampling in active learning for NLP?,1. What are the primary strategies for uncertainty sampling in active learning for NLP?,"Questions:

1. What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

(p3.0) Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

(p3.1) Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","What are the primary strategies for uncertainty sampling in active learning for NLP?
##
Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.
It prefers the most uncertain instances judged by the model outputs.
For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).
Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.
For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).
Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.
If an instance is near the decision boundary, the model's outputs may be different within its local region.
In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

It prefers the most uncertain instances judged by the model outputs.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.","For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).","For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).","For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

It prefers the most uncertain instances judged by the model outputs.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.

If an instance is near the decision boundary, the model's outputs may be different within its local region.

In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

It prefers the most uncertain instances judged by the model outputs.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.

If an instance is near the decision boundary, the model's outputs may be different within its local region.

In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).","What are the primary strategies for uncertainty sampling in active learning for NLP?
##
Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.","Uncertainty sampling (Lewis and Gale, 1994) is the simplest and most commonly utilized query strategy in active learning for NLP.
For probabilistic models, three typical uncertainty sampling strategies are entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005), and margin-sampling (Scheffer et al., 2001; Schein and Ungar, 2007) (Settles, 2009).
Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.
For non-probabilistic models, strategies like selecting instances close to the decision boundary in an SVM (Schohn and Cohn, 2000; Tong and Koller, 2001) can be applied.
Another strategy involves measuring the output uncertainty by examining the divergence of a model's predictions within an instance's local region.","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

Uncertainty sampling (Lewis and Gale, 1994) is the simplest and most commonly utilized query strategy in active learning for NLP.
For probabilistic models, three typical uncertainty sampling strategies are entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005), and margin-sampling (Scheffer et al., 2001; Schein and Ungar, 2007) (Settles, 2009).
Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.
For non-probabilistic models, strategies like selecting instances close to the decision boundary in an SVM (Schohn and Cohn, 2000; Tong and Koller, 2001) can be applied.
Another strategy involves measuring the output uncertainty by examining the divergence of a model's predictions within an instance's local region.","What are the primary strategies for uncertainty sampling in active learning for NLP?
##
Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.
It prefers the most uncertain instances judged by the model outputs.
For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).
Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.
For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).
Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.
If an instance is near the decision boundary, the model's outputs may be different within its local region.
In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

It prefers the most uncertain instances judged by the model outputs.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.

If an instance is near the decision boundary, the model's outputs may be different within its local region.

In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","What are the primary strategies for uncertainty sampling in active learning for NLP?
##
Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

It prefers the most uncertain instances judged by the model outputs.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.

If an instance is near the decision boundary, the model's outputs may be different within its local region.

In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and most commonly utilized query strategy.
It prefers the most uncertain instances judged by the model outputs.
For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005), and margin-sampling (Scheffer et al., 2001; Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).
Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.
For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000; Tong and Koller, 2001).
Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.
If an instance is near the decision boundary, the model's outputs may vary within its local region.
In this spirit, recent works examine different ways to check instances' local divergence, such as nearest-neighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b), and data augmentation (Jiang et al., 2020).","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and most commonly utilized query strategy.
It prefers the most uncertain instances judged by the model outputs.
For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005), and margin-sampling (Scheffer et al., 2001; Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).
Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.
For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000; Tong and Koller, 2001).
Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.
If an instance is near the decision boundary, the model's outputs may vary within its local region.
In this spirit, recent works examine different ways to check instances' local divergence, such as nearest-neighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b), and data augmentation (Jiang et al., 2020).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

It prefers the most uncertain instances judged by the model outputs.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.

If an instance is near the decision boundary, the model's outputs may be different within its local region.

In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","What are the primary strategies for uncertainty sampling in active learning for NLP?
##
Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

It prefers the most uncertain instances judged by the model outputs.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.

If an instance is near the decision boundary, the model's outputs may be different within its local region.

In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.
It prefers the most uncertain instances judged by the model outputs.
For probabilistic models, three typical uncertainty sampling strategies are entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005), and margin-sampling (Scheffer et al., 2001; Schein and Ungar, 2007) (Settles, 2009).
Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.
For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000; Tong and Koller, 2001).
Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.
If an instance is near the decision boundary, the model's outputs may be different within its local region.
Recent works examine different ways to check instances' local divergence, such as nearest-neighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b), and data augmentation (Jiang et al., 2020).","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.
It prefers the most uncertain instances judged by the model outputs.
For probabilistic models, three typical uncertainty sampling strategies are entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005), and margin-sampling (Scheffer et al., 2001; Schein and Ungar, 2007) (Settles, 2009).
Schrder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.
For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000; Tong and Koller, 2001).
Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.
If an instance is near the decision boundary, the model's outputs may be different within its local region.
Recent works examine different ways to check instances' local divergence, such as nearest-neighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b), and data augmentation (Jiang et al., 2020).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).","What are the primary strategies for uncertainty sampling in active learning for NLP?
##
Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and most commonly utilized query strategy.
For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005), and margin-sampling (Scheffer et al., 2001; Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).
For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000; Tong and Koller, 2001).","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and most commonly utilized query strategy.
For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005), and margin-sampling (Scheffer et al., 2001; Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).
For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000; Tong and Koller, 2001).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.","For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).

For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001)."
