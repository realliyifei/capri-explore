# Multidimensional Scaling, Sammon Mapping, and Isomap: Tutorial and Survey

CorpusID: 221761207
 
tags: #Mathematics, #Computer_Science

URL: [https://www.semanticscholar.org/paper/b83db63020d9750758059a919435bf5631d0bcc9](https://www.semanticscholar.org/paper/b83db63020d9750758059a919435bf5631d0bcc9)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Multidimensional Scaling, Sammon Mapping, and Isomap: Tutorial and Survey


Benyamin Ghojogh 
Department of Electrical and Computer Engineering
Department of Statistics and Actuarial Science & David R. Cheriton School of Computer Science
Data Analytics Laboratory
Machine Learning Laboratory
University of Waterloo
WaterlooONCanada

Bghojogh@uwaterloo Ca 
Department of Electrical and Computer Engineering
Department of Statistics and Actuarial Science & David R. Cheriton School of Computer Science
Data Analytics Laboratory
Machine Learning Laboratory
University of Waterloo
WaterlooONCanada

Ali Ghodsi 
Department of Electrical and Computer Engineering
Centre for Pattern Analysis and Machine Intelligence
University of Waterloo
WaterlooONCanada

Ali Ghodsi@uwaterloo 
Department of Electrical and Computer Engineering
Centre for Pattern Analysis and Machine Intelligence
University of Waterloo
WaterlooONCanada

Ca 
Department of Electrical and Computer Engineering
Centre for Pattern Analysis and Machine Intelligence
University of Waterloo
WaterlooONCanada

Fakhri Karray 
Department of Electrical and Computer Engineering
Machine Learning Laboratory
University of Waterloo
WaterlooONCanada

Karray@uwaterloo Ca 
Department of Electrical and Computer Engineering
Machine Learning Laboratory
University of Waterloo
WaterlooONCanada

Mark Crowley 
University of Waterloo
WaterlooONCanada

Mcrowley@uwaterloo Ca 
University of Waterloo
WaterlooONCanada

Multidimensional Scaling, Sammon Mapping, and Isomap: Tutorial and Survey
To appear as a part of an upcoming academic book on dimensionality reduction and manifold learning.
Multidimensional Scaling (MDS) is one of the first fundamental manifold learning methods. It can be categorized into several methods, i.e., classical MDS, kernel classical MDS, metric MDS, and non-metric MDS. Sammon mapping and Isomap can be considered as special cases of metric MDS and kernel classical MDS, respectively. In this tutorial and survey paper, we review the theory of MDS, Sammon mapping, and Isomap in detail. We explain all the mentioned categories of MDS. Then, Sammon mapping, Isomap, and kernel Isomap are explained. Out-of-sample embedding for MDS and Isomap using eigenfunctions and kernel mapping are introduced. Then, Nystrom approximation and its use in landmark MDS and landmark Isomap are introduced for big data embedding. We also provide some simulations for illustrating the embedding by these methods.

## Introduction

Multidimensional Scaling (MDS) (Cox & Cox, 2008), first proposed in (Torgerson, 1952), is one of the earliest proposed manifold learning methods. It can be used for man-ifold learning, dimensionality reduction, and feature extraction (Ghojogh et al., 2019c). The idea of MDS is to preserve the similarity (Torgerson, 1965) or dissimilarity/distances (Beals et al., 1968) of points in the lowdimensional embedding space. Hence, it fits the data locally to capture the global structure of data (Saul & Roweis, 2003). MDS can be categorized into classical MDS, metric MDS, and non-metric MDS. In later approaches, Sammon mapping (Sammon, 1969) was proposed which is a special case of the distance-based metric MDS. One can consider Sammon mapping as the first proposed nonlinear manifold learning method (Ghojogh et al., 2019b). The disadvantage of Sammon mapping is its iterative solution of optimization, which makes this method a little slow. The classical MDS can be generalized to have kernel classical MDS in which any valid kernel can be used. Isomap (Tenenbaum et al., 2000) is a special case of the kernel classical MDS which uses a kernel constructed from geodesic distances between points. Because of the nonlinearity of geodesic distance, Isomap is also a nonlinear manifold learning method. MDS and its special cases, Sammon mapping, and Isomap have had different applications (Young, 2013). For example, MDS has been used for facial expression recognition (Russell & Bullock, 1985;Katsikitis, 1997). Kernel Isomap has also been used for this application (Zhao & Zhang, 2011). The goal is to embed the high-dimensional input data
{x i } n i=1 into the lower dimensional embedded data {y i } n i=1
where n is the number of data points. We denote the dimensionality of input and embedding spaces by d and p ≤ d, respectively, i.e. x i ∈ R d and y i ∈ R p . We denote R d×n X := [x 1 , . . . , x n ] and R p×n Y := [y 1 , . . . , y n ]. The remainder of this paper is organized as follows. Section 2 explains MDS and its different categories, i.e., classical MDS, generalized classical MDS (kernel classical MDS), metric MDS, and non-metric MDS. Sammon mapping and Isomap are introduced in Sections 3 and 4, respectively. Section 5 introduced the methods for out-ofsample extensions of MDS and Isomap methods. Landmark MDS and landmark Isomap, for big data embedding, are explained in Section 6. Some simulations for illustrating the results of embedding are provided in Section 7. Finally, Section 8 concludes the paper.


## Multidimensional Scaling

MDS, first proposed in (Torgerson, 1952), can be divided into several different categories (Cox & Cox, 2008;Borg & Groenen, 2005), i.e., classical MDS, metric MDS, and non-metric MDS. Note that the results of these are different (Jung, 2013). In the following, we explain all three categories.


### Classical Multidimensional Scaling


#### CLASSICAL MDS WITH EUCLIDEAN DISTANCE

The classical MDS is also referred to as Principal Coordinates Analysis (PCoA), or Torgerson Scaling, or Torg-ersonGower scaling (Gower, 1966). The goal of classical MDS is to preserve the similarity of data points in the embedding space as it was in the input space (Torgerson, 1965). One way to measure similarity is inner product. Hence, we can minimize the difference of similarities in the input and embedding spaces:
minimize {y i } n i=1 c 1 := n i=1 n j=1 (x i x j − y i y j ) 2 ,(1)
whose matrix form is:
minimize Y c 1 = ||X X − Y Y || 2 F ,(2)
where · F denotes the Frobenius norm, and X X and Y Y are the Gram matrices of the original data X and the embedded data Y , respectively. The objective function, in Eq.

(2), is simplified as:
||X X − Y Y || 2 F = tr (X X − Y Y ) (X X − Y Y ) = tr (X X − Y Y )(X X − Y Y ) = tr (X X − Y Y ) 2 ,
where tr(.) denotes the trace of matrix. If we decompose X X and Y Y using eigenvalue decomposition (Ghojogh et al., 2019a), we have:
X X = V ∆V ,(3)Y Y = QΨQ ,(4)
where eigenvectors are sorted from leading (largest eigenvalue) to trailing (smallest eigenvalue). Note that, rather than eigenvalue decomposition of X X and Y Y , one can decompose X and Y using Singular Value Decomposition (SVD) and take the right singular vectors of X and Y as V and Q, respectively. The matrices ∆ and Ψ are the obtained by squaring the singular values (to power 2). See (Ghojogh & Crowley, 2019, Proposition 1) for proof. The objective function can be further simplified as:
∴ ||X X − Y Y || 2 F = tr (X X − Y Y ) 2 = tr (V ∆V − QΨQ ) 2 (a) = tr (V ∆V − V V QΨQ V V ) 2 = tr V (∆ − V QΨQ V )V 2 = tr V 2 (∆ − V QΨQ V ) 2 (V ) 2 (b) = tr (V ) 2 V 2 (∆ − V QΨQ V ) 2 = tr (V V I ) 2 (∆ − V QΨQ V ) 2 (c) = tr (∆ − V QΨQ V ) 2 ,
where (a) and (c) are for V V = V V = I because V is a non-truncated (square) orthogonal matrix (where I denotes the identity matrix). The reason of (b) is the cyclic property of trace. Let R n×n M := V Q, so:
||X X − Y Y || 2 F = tr (∆ − M ΨM ) 2 .
Therefore:
∴ minimize Y ||X X − Y Y || 2 F ≡ minimize M ,Ψ tr (∆ − M ΨM ) 2 .
The objective function is:
c 1 = tr (∆ − M ΨM ) 2 = tr(∆ 2 + (M ΨM ) 2 − 2∆M ΨM ) = tr(∆ 2 ) + tr((M ΨM ) 2 ) − 2 tr(∆M ΨM ).
As the optimization problem is unconstrained and the objective function is the trace of a quadratic function, the minimum is non-negative. If we take derivative with respect to the first objective variable, i.e., M , we have:
R n×n ∂c 1 ∂M = 2(M ΨM )M Ψ − 2∆M Ψ set = 0 =⇒ (M ΨM )(M Ψ) = (∆)(M Ψ) (a) =⇒ M ΨM = ∆,(5)
where (a) is because M Ψ = 0.

For the derivative with respect to the second objective variable, i.e., Ψ, we simplify the objective function a little bit:
c 1 = tr(∆ 2 ) + tr((M ΨM ) 2 ) − 2 tr(∆M ΨM ) = tr(∆ 2 ) + tr(M 2 Ψ 2 M 2 ) − 2 tr(∆M ΨM ) (a) = tr(∆ 2 ) + tr(M 2 M 2 Ψ 2 ) − 2 tr(M ∆M Ψ) = tr(∆ 2 ) + tr((M M Ψ) 2 ) − 2 tr(M ∆M Ψ),
where (a) is because of the cyclic property of trace.

Taking derivative with respect to the second objective variable, i.e., Ψ, gives:
R n×n ∂c 1 ∂Ψ = 2M (M ΨM )M − 2M ∆M set = 0 =⇒ M (M ΨM )M = M (∆)M (a) =⇒ M ΨM = ∆,(6)
where (a) is because M = 0. Both Eqs. (5) and (6) are:

M ΨM = ∆, whose one possible solution is:
M = I,(7)Ψ = ∆.(8)
which means that the minimum value of the non-negative objective function tr((∆ − M ΨM ) 2 ) is zero. We had M = V Q. Therefore, according to Eq. (7), we have:
∴ V Q = I =⇒ Q = V .(9)
According to Eq. (4), we have:
Y Y = QΨQ (a) = QΨ 1 2 Ψ 1 2 Q =⇒ Y = Ψ 1 2 Q (8),(9) =⇒ Y = ∆ 1 2 V ,(10)
where (a) can be done because Ψ does not include negative entry as the gram matrix Y Y is positive semi-definite by definition.

In summary, for embedding X using classical MDS, the eigenvalue decomposition of X X is obtained as in Eq.

(3). Then, using Eq. (10), Y ∈ R n×n is obtained. Truncating this Y to have Y ∈ R p×n , with the first (top) p rows, gives us the p-dimensional embedding of the n points. Note that the leading p columns are used because singular values are sorted from largest to smallest in SVD which can be used for Eq. (3).
2.1.2. GENERALIZED CLASSICAL MDS (KERNEL CLASSICAL MDS) If d 2 ij = ||x i − x j || 2 2
is the squared Euclidean distance between x i and x j , we have:
d 2 ij = ||x i − x j || 2 2 = (x i − x j ) (x i − x j ) = x i x i − x i x j − x j x i + x j x j = x i x i − 2x i x j + x j x j = G ii − 2G ij + G jj ,
where R n×n G := X X is the Gram matrix. If R n g := [g 1 , . . . , g n ] = [G 11 , . . . , G nn ] = diag(G), we have:
d 2 ij = g i − 2G ij + g j , D = g1 − 2G + 1g = 1g − 2G + g1 ,
where 1 is the vector of ones and D is the distance matrix with squared Euclidean distance (d 2 ij as its elements). Let R n×n H := I − 1 n 11 denote the centering matrix. We double-center the matrix D as follows (Oldford, 2018):
HDH = (I − 1 n 11 )D(I − 1 n 11 ) = (I − 1 n 11 )(1g − 2G + g1 )(I − 1 n 11 ) = (I − 1 n 11 )1 = 0 g − 2(I − 1 n 11 )G + (I − 1 n 11 )g1 (I − 1 n 11 ) = −2(I − 1 n 11 )G(I − 1 n 11 ) + (I − 1 n 11 )g 1 (I − 1 n 11 ) = 0 = −2(I − 1 n 11 )G(I − 1 n 11 ) = −2 HGH ∴ HGH = HX XH = − 1 2 HDH.(11)
Note that (I − 1 n 11 )1 = 0 and 1 (I − 1 n 11 ) = 0 because removing the row mean of 1 and column mean of of 1 results in the zero vectors, respectively.

If data X are already centered, i.e., the mean has been removed (X ← XH), Eq. (11) becomes:
X X = − 1 2 HDH.(12)
Corollary 1. If using Eq. (3) as Gram matrix, the classical MDS uses the Euclidean distance as its metric. Because of using Euclidean distance, the classical MDS using Gram matrix is a linear subspace learning method.

Proof. The Eq.

(3) in classical MDS is the eigenvalue decomposition of the Gram matrix X X. According to Eq. (12), this Gram matrix can be restated to an expression based on squared Euclidean distance. Hence, the classical MDS with Eq. (3) uses Euclidean distance and is linear, consequently.

In Eq. (11) or (12), we can write a general kernel matrix (Hofmann et al., 2008) rather than the double-centered Gram matrix, to have (Cox & Cox, 2008):
R n×n K = − 1 2 HDH.(13)
Note that the classical MDS with Eq. (3) is using a linear kernel X X for its kernel. This is another reason for why classical MDS with Eq. (3) is a linear method. It is also noteworthy that Eq. (13) can be used for unifying the spectral dimensionality reduction methods as special cases of kernel principal component analysis with different kernels. See (Ham et al., 2004;Bengio et al., 2004a) and (Strange & Zwiggelaar, 2014, 
K = V ∆V .(14)
Then, using Eq. (10), Y ∈ R n×n is obtained. It is noteworthy that in this case, we are replacing X X with the kernel K = Φ(X) Φ(X) and then, according to Eqs.

(10) and (14), we have:
K = Y Y .(15)
Truncating the Y , obtained from Eq. (10), to have Y ∈ R p×n , with the first (top) p rows, gives us the pdimensional embedding of the n points. It is noteworthy that, because of using kernel in the generalized classical MDS, one can name it the kernel classical MDS.


#### EQUIVALENCE OF PCA AND KERNEL PCA WITH CLASSICAL MDS AND GENERALIZED

CLASSICAL MDS, RESPECTIVELY Proposition 1. Classical MDS with Euclidean distance is equivalent to Principal Component Analysis (PCA). Moreover, the generalized classical MDS is equivalent to kernel PCA.

Proof. On one hand, the Eq. (3) can be obtained by the SVD of X. The projected data onto classical MDS subspace is obtained by Eq. (10) which is ∆V . On the other hand, according to (Ghojogh & Crowley, 2019, Eq. 42), the projected data onto PCA subspace is ∆V where ∆ and V are from the SVD of X. Comparing these shows that classical MDS is equivalent to PCA. Moreover, Eq. (14) is the eigenvalue decomposition of the kernel matrix. The projected data onto the generalized classical MDS subspace is obtained by Eq. (10) which is ∆V . According to (Ghojogh & Crowley, 2019, Eq. 62), the projected data onto the kernel PCA subspace is ∆V where ∆ and V are from the eigenvalue decomposition of the kernel matrix; see (Ghojogh & Crowley, 2019, Eq. 61). Comparing these shows that the generalized classical MDS is equivalent to kernel PCA.


### Metric Multidimensional Scaling

Recall that the classical MDS tries to preserve the similarities of points in the embedding space. In later approaches after classical MDS, the cost function was changed to preserve the distances rather than the similarities (Lee & Verleysen, 2007;Bunte et al., 2012). Metric MDS has this opposite view and tries to preserve the distances of points in the embedding space (Beals et al., 1968). For this, it minimizes the difference of distances of points in the input and embedding spaces (Ghodsi, 2006). The cost function in metric MDS is usually referred to as the stress function (Mardia, 1978;De Leeuw, 2011). This method is named metric MDS because it uses distance metric in its optimization. The optimization in metric MDS is:
minimize {y i } n i=1 c 2 := n i=1 n j=1,j<i d x (x i , x j ) − d y (y i , y j ) 2 n i=1 n j=1,j<i d 2 x (x i , x j ) 1 2 ,(16)
or, without the normalization factor:
minimize {y i } n i=1 c 2 := n i=1 n j=1,j<i d x (x i , x j ) − d y (y i , y j ) 2 1 2 ,(17)
where d x (., .) and d y (., .) denote the distance metrics in the input and the embedded spaces, respectively. The Eqs. (16) and (17) use indices j < i rather than j = i because the distance metric is symmetric and it is not necessary to consider the distance of the j-th point from the i-th point when we already have considered the distance of the i-th point from the j-th point. Note that in Eq. (16) and (17), d y is usually the Euclidean distance, i.e. d y = y i −y j 2 , while d x can be any valid metric distance such as the Euclidean distance. The optimization problem (16) can be solved using either gradient descent or Newton's method. Note that the classical MDS is a linear method and has a closed-form solution; however, the metric and non-metric MDS methods are nonlinear but do not have closed-form solutions and should be solved iteratively. Note that in mathematics, whenever you get something, you lose something. Likewise, here, the method has become nonlinear but lost its closed form solution and became iterative. Inspired by (Sammon, 1969), we can use diagonal quasi-Newton's method for solving this optimization problem. If we consider the vectors component-wise, the diagonal quasi-Newton's method updates the solution as (Lee & Verleysen, 2007):
y (ν+1) i,k := y (ν) i,k − η ∂ 2 c 2 ∂y 2 i,k −1 ∂c 2 ∂y i,k ,(18)
where η is the learning rate, y i,k is the k-th element of the i-th embedded point R p y i = [y i,1 , . . . , y i,p ] , and | · | is the absolute value guaranteeing that we move toward the minimum and not maximum in the Newton's method. If using gradient descent for solving the optimization, we update the solution as:
y (ν+1) i,k := y (ν) i,k − η ∂c 2 ∂y i,k .(19)

### Non-Metric Multidimensional Scaling

In non-metric MDS, rather than using a distance metric, d y (x i , x j ), for the distances between points in the embedding space, we use
f (d y (x i , x j )) where f (.)
is a nonparametric monotonic function. In other words, only the order of dissimilarities is important rather than the amount of dissimilarities (Agarwal et al., 2007;Jung, 2013):
d y (y i , y j ) ≤ d y (y k , y ) ⇐⇒ f (d y (y i , y j )) ≤ f (d y (x k , y )).(20)
The optimization in non-metric MDS is (Agarwal et al., 2007):
minimize {y i } n i=1 c 3 := n i=1 n j=1,j<i d x (x i , x j ) − f (d y (y i , y j )) 2 n i=1 n j=1,j<i d 2 x (x i , x j ) 1 2 .(21)
An examples of non-metric MDS is Smallest Space Analysis (Schlesinger & Guttman, 1969). Another example is Kruskal's non-metric MDS or Shepard-Kruskal Scaling (SKS) (Kruskal, 1964a;b). In Kruskal's non-metric MDS, the function f (.) is the regression, where f (d y (y i , y j )) is predicted from regression which preserves the order of dissimilarities (Holland, 2008;Agarwal et al., 2007). The Eq.

(21) with f (.) as the regression function, which is used in Kruskal's non-metric MDS, is called Stress-1 formula (Agarwal et al., 2007;Holland, 2008;Jung, 2013).


## Sammon Mapping

Sammon mapping (Sammon, 1969) is a special case of metric MDS; hence, it is a nonlinear method. It is probably correct to call this method the first proposed nonlinear method for manifold learning (Ghojogh et al., 2019b). This method has different names in the literature such as Sammon's nonlinear mapping, Sammon mapping, and Nonlinear Mapping (NLM) (Lee & Verleysen, 2007). Sammon originally named it NLM (Sammon, 1969). Its most well-known name is Sammon mapping. The optimization problem in Sammon mapping is almost a weighted version of Eq. (16), formulated as:
minimize {y i } n i=1 1 a n i=1 n j=1,j<i w ij d x (x i , x j ) − d y (y i , y j ) 2 ,(22)
where w ij is the weight and a is the normalizing factor. The d x (., .) can be any metric but usually is considered to be Euclidean distance for simplicity (Lee & Verleysen, 2007). The d y (., .), however, is Euclidean distance metric. In Sammon mapping, the weights and the normalizing factor in Eq. (22) are:
w ij = 1 d x (x i , x j ) ,(23)a = n i=1 n j=1,j<i d x (x i , x j ).(24)
The weight w ij in Sammon mapping is giving more credit to the small distances (neighbor points) focusing on preserving the "local" structure of the manifold; hence it fits the manifold locally (Saul & Roweis, 2003). Substituting Eqs. (23) and (24) in Eq. (22) gives:
minimize Y c 4 := 1 n i=1 n j=1,j<i d x (x i , x j ) × n i=1 n j=1,j<i d x (x i , x j ) − d y (y i , y j ) 2 d x (x i , x j ) .
(25) Sammon used diagonal quasi-Newton's method for solving this optimization problem (Sammon, 1969). Hence, Eq.

(18) is utilized. The learning rate η is named the magic factor in (Sammon, 1969). For solving optimization, both gradient and second derivative are required. In the following, we derive these two. Note that, in practice, the classical MDS or PCA is used for initialization of points in Sammon mapping optimization. Proposition 2. The gradient of the cost function c with respect to y i,k is (Sammon, 1969;Lee & Verleysen, 2007):
∂c 4 ∂y i,k = −2 a n i=1 n j=1,j<i d x (x i , x j ) − d y (y i , y j ) d x (x i , x j ) d y (x i , x j ) (y i,k − y j,k ).(26)
Proof. Proof is according to (Lee & Verleysen, 2007). According to chain rule, we have:
∂c 4 ∂y i,k = ∂c 4 ∂d y (y i , y j ) × ∂d y (y i , y j ) ∂y i,k .
The first derivative is:
∂c 4 ∂d y (y i , y j ) = −2 a n i=1 n j=1,j<i d x (x i , x j ) − d y (y i , y j ) d x (x i , x j ) ,
and using the chain rule, the second derivative is:
∂d y (y i , y j ) ∂y i,k = ∂d y (y i , y j ) ∂d 2 y (y i , y j ) × ∂d 2 y (y i , y j ) ∂y i,k .
We have:

∂d y (y i , y j ) ∂d 2 y (y i , y j ) = 1/ ∂d 2 y (y i , y j ) ∂d y (y i , y j ) = 1/(2d y (y i , y j )).

Also we have:
d 2 y (y i , y j ) = ||y i − y j || 2 2 = p k=1 (y i,k − y j,k ) 2 .
Therefore:
∂d 2 y (y i , y j ) ∂y i,k = 2 (y i,k − y j,k ),
Therefore:
∴ ∂d y (y i , y j ) ∂y i,k = y i,k − y j,k d y (y i , y j ) .(27)
Finally, we have:
∴ ∂c 4 ∂y i,k = −2 a n i=1 n j=1,j<i d x (x i , x j ) − d y (y i , y j ) d x (x i , x j ) d y (x i , x j ) (y i,k − y j,k ),
which is the gradient mentioned in the proposition. Q.E.D.

Proposition 3. The second derivative of the cost function c with respect to y i,k is (Sammon, 1969;Lee & Verleysen, 2007):
∂ 2 c 4 ∂y 2 i,k = −2 a n i=1 n j=1,j<i d x (x i , x j ) − d y (y i , y j ) d x (x i , x j ) d y (x i , x j ) − (y i,k − y j,k ) 2 d 3 y (y i , y j ) .(28)
Proof. We have:
∂ 2 c 4 ∂y 2 i,k = ∂ ∂y i,k ∂c 4 ∂y i,k ,
where ∂c 4 /∂y i,k is Eq. (26). Therefore:
∂ 2 c 4 ∂y 2 i,k = −2 a n i=1 n j=1,j<i ∂ ∂y i,k d x (x i , x j ) − d y (y i , y j ) d x (x i , x j ) d y (x i , x j ) (y i,k − y j,k ) .
We have:
∂ ∂y i,k d x (x i , x j ) − d y (y i , y j ) d x (x i , x j ) d y (x i , x j ) (y i,k − y j,k ) = (y i,k − y j,k ) ∂ ∂y i,k d x (x i , x j ) − d y (y i , y j ) d x (x i , x j ) d y (x i , x j ) + d x (x i , x j ) − d y (y i , y j ) d x (x i , x j ) d y (x i , x j ) ∂ ∂y i,k (y i,k − y j,k ) =1 .
Note that:
∂ ∂y i,k d x (x i , x j ) − d y (y i , y j ) d x (x i , x j ) d y (x i , x j ) = 1 d x (x i , x j ) ∂ ∂y i,k d x (x i , x j ) − d y (y i , y j ) d y (x i , x j ) = 1 d x (x i , x j ) ∂ ∂y i,k d x (x i , x j ) d y (x i , x j ) − 1 = d x (x i , x j ) d x (x i , x j ) =1 ∂ ∂y i,k 1 d y (x i , x j ) − ∂ ∂y i,k(1)=0 = −1 d 2 y (x i , x j ) ∂ ∂y i,k (d y (x i , x j )) (27) = −1 d 2 y (x i , x j ) y i,k − y j,k d y (y i , y j ).
Therefore:
∴ ∂ ∂y i,k d x (x i , x j ) − d y (y i , y j ) d x (x i , x j ) d y (x i , x j ) (y i,k − y j,k ) = −(y i,k − y j,k ) 2 d 3 y (y i , y j ) + d x (x i , x j ) − d y (y i , y j ) d x (x i , x j ) d y (x i , x j ).
Therefore:
∴ ∂ 2 c 4 ∂y 2 i,k = −2 a n i=1 n j=1,j<i d x (x i , x j ) − d y (y i , y j ) d x (x i , x j ) d y (x i , x j ) − (y i,k − y j,k ) 2 d 3 y (y i , y j )
, which is the derivative mentioned in the proposition. Q.E.D.

It is noteworthy that for better time complexity of the Sammon mapping, one can use the k-Nearest Neighbors (kNN) rather than the whole data (Ghojogh et al., 2020): 
minimize {y i } n i=1 1 a n i=1 n j∈Ni w ij d x (x i , x j ) − d y (y i , y j ) 2 ,(29)

## Isomap


### Isomap

Isomap (Tenenbaum et al., 2000) is a special case of the generalized classical MDS, explained in Section 2.1.2. Rather than the Euclidean distance, Isomap uses an approximation of the geodesic distance. As was explained, the classical MDS is linear; hence, it cannot capture the nonlinearity of the manifold. Isomap makes use of the geodesic distance to make the generalized classical MDS nonlinear.


#### GEODESIC DISTANCE

The geodesic distance is the length of shortest path between two points on the possibly curvy manifold. It is ideal to use the geodesic distance; however, calculation of the geodesic distance is very difficult because it requires traversing from a point to another point on the manifold. This calculation requires differential geometry and Riemannian manifold calculations (Aubin, 2001). Therefore, Isomap approximates the geodesic distance by piecewise Euclidean distances. It finds the k-Nearest Neighbors (kNN) graph of dataset. Then, the shortest path between two points, through their neighbors, is found using a shortest-path algorithm such as the Dijkstra algorithm or the Floyd-Warshal algorithm (Cormen et al., 2009). A sklearn function in python for this is "graph shortest path" from the package "sklearn.utils.graph shortest path". Note that the approximated geodesic distance is also refered to as the curvilinear distance (Lee et al., 2002). The approximated geodesic distance can be formulated as (Bengio et al., 2004b): Figure 1. An example of the Euclidean distance, geodesic distance, and approximated geodesic distance using piece-wise Euclidean distances.
D (g) ij := min r l i=2 r i − r i+1 2 ,(30)
where l ≥ 2 is the length of sequence of points r i ∈ {x i } n i=1 and D (g)

ij denotes the (i, j)-th element of the geodesic distance matrix D (g) ∈ R n×n . An example of the Euclidean distance, geodesic distance, and the approximated geodesic distance using piece-wise Euclidean distances can be seen in Fig. 1. A real-world example is the distance between Toronto and Athens. The Euclidean distance is to dig the Earth from Toronto to reach Athens directly. The geodesic distance is to move from Toronto to Athens on the curvy Earth by the shortest path between two cities. The approximated geodesic distance is to dig the Earth from Toronto to London in UK, then dig from London to Frankfurt in Germany, then dig from Frankfurt to Rome in Italy, then dig from Rome to Athens. Calculations of lengths of paths in the approximated geodesic distance is much easier than the geodesic distance.


#### ISOMAP FORMULATION

As was mentioned before, Isomap is a special case of the generalized classical MDS with the geodesic distance used. Hence, Isomap uses Eq. (13) as:
R n×n K = − 1 2 HD (g) H.(31)
It then uses Eqs. (14) and (10) to embed the data. As Isomap uses the nonlinear geodesic distance in its kernel calculation, it is a nonlinear method.


### Kernel Isomap

Consider K(D) to be Eq. (13). Consequently, we have:
R n×n K(D 2 ) = − 1 2 HD 2 H,(32)
where D is the geodesic distance matrix, defined by Eq. (30).

Define the following equation (Cox & Cox, 2008, Section 2.2.8):
R n×n K := K(D 2 ) + 2cK(D) + 1 2 c 2 H. (33)
According to (Cailliez, 1983), K is guaranteed to be positive semi-definite for c ≥ c * where c * is the largest eigenvalue of the following matrix:
0 2K(D 2 ) −I −4K(D) ∈ R 2n×2n .(34)
Kernel Isomap (Choi & Choi, 2004) chooses a value c ≥ c * and uses K in Eq. (14) and then uses Eq. (10) for embedding the data.


## Out-of-sample Extensions for MDS and Isomap

So far, we embedded the training dataset
{x i ∈ R d } n i=1 or X = [x 1 , . . . , x n ] ∈ R d×n to have their embedding {y i ∈ R p } n i=1 or Y = [y 1 , . . . , y n ] ∈ R p×n . Assume we have some out-of-sample (test data), denoted by {x (t) i ∈ R d } nt i=1 or X t = [x (t) 1 , . . . , x (t) n ] ∈ R d×nt . We want to find their embedding {y (t) i ∈ R p } nt i=1 or Y t = [y (t) 1 , . . . , y (t)
n ] ∈ R p×nt after the training phase.


### Out of Sample for Isomap and MDS Using Eigenfunctions

5.1.1. EIGENFUNCTIONS Consider a Hilbert space H p of functions with the inner product f, g = f (x)g(x)p(x)dx with density function p(x). In this space, we can consider the kernel function K p :
(K p f )(x) = K(x, y) f (y) p(y) dy,(35)
where the density function can be approximated empirically. The eigenfunction decomposition is defined to be (Bengio et al., 2004a;b):
(K p f k )(x) = δ k f k (x),(36)
where f k (x) is the k-th eigenfunction and δ k is the corresponding eigenvalue. If we have the eigenvalue decomposition (Ghojogh et al., 2019a) for the kernel matrix K, we have Kv k = δ k v k (see Eq. (14)) where v k is the k-th eigenvector and δ k is the corresponding eigenvalue. According to (Bengio et al., 2004b, Proposition 1), we have δ k = (1/n)δ k .


#### EMBEDDING USING EIGENFUNCTIONS Proposition 4.

If v ki is the i-th element of the ndimensional vector v k and k(x, x i ) is the kernel between vectors x and x i , the eigenfunction for the point x and the i-th training point x i are:
f k (x) = √ n δ k n i=1 v kikt (x i , x),(37)f k (x i ) = √ n v ki ,(38)
respectively, wherek t (x i , x) is the centered kernel between training set and the out-of-sample point x.

Let the MDS or Isomap embedding of the point x be R p y(x) = [y 1 (x), . . . , y p (x)] . The k-th dimension of this embedding is:
y k (x) = δ k f k (x) √ n = 1 √ δ k n i=1 v kikt (x i , x). (39)
Proof. This proposition is taken from (Bengio et al., 2004b, Proposition 1). For proof, refer to (Bengio et al., 2004a, Proposition 1), (Bengio et al., 2006, Proposition 1), and (Bengio et al., 2003b, Proposition 1 and Theorem 1). More complete proofs can be found in (Bengio et al., 2003a).

If we have a set of n t out-of-sample data points,k t (x i , x) is an element of the centered out-of-sample kernel (see (Ghojogh & Crowley, 2019, Appendix C)):
R n×nt K t = K t − 1 n 1 n×n K t − 1 n K1 n×nt + 1 n 2 1 n×n K1 n×nt ,(40)
where 1 := [1, 1, . . . , 1] , K t ∈ R n×nt is the not necessarily centered out-of-sample kernel, and K ∈ R n×n is the training kernel. i should be used in place of x in Eq. (39). Note that Eq. (39) requires Eq. (40). In MDS and Isomap, K is obtained by the linear kernel, X X, and Eq. (31), respectively. Also, the out-of-sample kernel K t in MDS is obtained by the linear kernel between the training and outof-sample data, i.e., X X t . In Isomap, the kernel K t is obtained by centering the geodesic distance matrix (see Eq. (31)) where the geodesic distance matrix between the training and out-of-sample data is used. In calculation of this geodesic distance matrix, merely the training data points, and not the test points, should be used as the intermediate points in paths (Bengio et al., 2004b). It is shown in (Bengio et al., 2004b, Corollary 1) that using the geodesic distance with only training data as intermediate points, for teh sake of out-of-sample embedding in Isomap, is equivalent to the landmark Isomap method (De Silva & Tenenbaum, 2003):


#### OUT-OF-SAMPLE EMBEDDING
y k (x) = 1 2 √ δ k n i=1 v ki (D (g) avg − D (g) t (x i , x)),(41)
where D (g) avg denotes the average geodesic distance between the training points and D (g) t is the geodesic distance between the i-th training point x i and the out-of-sample point x, in which the training set is used for intermediate points.

Hence, one can use Eq. (41) for out-of-sample embedding in Isomap. It is noteworthy that in addition to the out-of-sample extension using eigenfunctions (Bengio et al., 2004b), there exist some other methods for out-of-sample extension of MDS and Isomap (Bunte et al., 2012;Strange & Zwiggelaar, 2011), which we pass by in this paper.


### Out of Sample for Isomap, Kernel Isomap, and MDS Using Kernel Mapping

There is a kernel mapping method (Gisbrecht et al., 2012;2015) to embed the out-of-sample data in Isomap, kernel Isomap, and MDS. We introduce this method here. We define a map which maps any data point as x → y(x), where:
R p y(x) := n j=1 α j k(x, x j ) n =1 k(x, x ) ,(42)
and α j ∈ R p , and x j and x denote the j-th and -th training data point. The k(x, x j ) is a kernel such as the Gaussian kernel:
k(x, x j ) = exp( −||x − x j || 2 2 2 σ 2 j ),(43)
where σ j is calculated as (Gisbrecht et al., 2015):
σ j := γ × min i (||x j − x i || 2 ),(44)
where γ is a small positive number. Assume we have already embedded the training data points using MDS (see Section 2), Isomap (see Section 4), or kernel Isomap (see Section 4.2); therefore, the set {y i } n i=1 is available. If we map the training data points, we want to minimize the following least-squares cost function in order to get y(x i ) close to y i for the i-th training point:
minimize αj 's n i=1 ||y i − y(x i )|| 2 2 ,(45)
where the summation is over the training data points. We can write this cost function in matrix form as below:
minimize A ||Y − K A|| 2 F ,(46)
where R n×p Y := [y 1 , . . . , y n ] and R n×p A := [α 1 , . . . , α n ] . The K ∈ R n×n is the kernel matrix whose (i, j)-th element is defined to be:
K (i, j) := k(x i , x j ) n =1 k(x i , x ) .(47)
The Eq. (46) is always non-negative; thus, its smallest value is zero. Therefore, the solution to this equation is:
Y − K A = 0 =⇒ Y = K A (a) =⇒ A = K † Y ,(48)
where K † is the pseudo-inverse of K :
K † = (K K ) −1 K ,(49)
and (a) is because K † K = I. Finally, the mapping of Eq. (42) for the n t out-of-sample data points is:
Y t = K t A,(50)
where the (i, j)-th element of the out-of-sample kernel matrix K t ∈ R nt×n is:
K t (i, j) := k(x (t) i , x j ) n =1 k(x (t) i , x ) ,(51)
where x (t)

i is the i-th out-of-sample data point, and x j and x are the j-th and -th training data points.


## Landmark MDS and Landmark Isomap for Big Data Embedding

Nystrom approximation, introduced below, can be used to make the spectral methods such as MDS and Isomap scalable and suitable for big data embedding.


### Nystrom Approximation

Nystrom approximation is a technique used to approximate a positive semi-definite matrix using merely a subset of its columns (or rows) (Williams & Seeger, 2001). Consider a positive semi-definite matrix R n×n K 0 whose parts are:
R n×n K = A B B C ,(52)
where A ∈ R m×m , B ∈ R m×(n−m) , and C ∈ R (n−m)×(n−m) in which m n. The Nystrom approximation says if we have the small parts of this matrix, i.e. A and B, we can approximate C and thus the whole matrix K. The intuition is as follows. Assume m = 2 (containing two points, a and b) and n = 5 (containing three other points, c, d, and e). If we know the similarity (or distance) of points a and b from one another, resulting in matrix A, as well as the similarity (or distance) of points c, d, and e from a and b, resulting in matrix B, we cannot have much freedom on the location of c, d, and e, which is the matrix C. This is because of the positive semidefiniteness of the matrix K. The points selected in submatrix A are named landmarks. Note that the landmarks can be selected randomly from the columns/rows of matrix K and, without loss of generality, they can be put together to form a submatrix at the top-left corner of matrix. As the matrix K is positive semi-definite, by definition, it can be written as
K = O O. If we take O = [R, S]
where R are the selected columns (landmarks) of O and S are the other columns of O. We have:
K = O O = R S [R, S] (53) = R R R S S R S S (52) = A B B C .(54)
Hence, we have A = R R. The eigenvalue decomposition (Ghojogh et al., 2019a) of A gives:
A = U ΣU (55) =⇒ R R = U ΣU =⇒ R = Σ (1/2) U . (56)
Moreover, we have B = R S so we have:
B = (Σ (1/2) U ) S = U Σ (1/2) S (a) =⇒ U B = Σ (1/2) S =⇒ S = Σ (−1/2) U B,(57)
where (a) is because U is orthogonal (in the eigenvalue decomposition). Finally, we have:
C = S S = B U Σ (−1/2) Σ (−1/2) U B = B U Σ −1 U B (55) = B A −1 B.(58)
Therefore, Eq. (52) becomes:
K ≈ A B B B A −1 B .(59)
Proposition 5. By increasing m, the approximation of Eq.

(59) becomes more accurate. If rank of K is at most m, this approximation is exact.

Proof. In Eq. (58), we have the inverse of A. In order to have this inverse, the matrix A must not be singular. For having a full-rank A ∈ R m×m , the rank of A should be m. This results in m to be an upper bound on the rank of K and a lower bound on the number of landmarks. In practice, it is recommended to use more number of landmarks for more accurate approximation but there is a trade-off with the speed.

Corollary 2. As we usually have m n, the Nystrom approximation works well especially for the low-rank matrices (Kishore Kumar & Schneider, 2017). Usually, because of the manifold hypothesis, data fall on a submanifold; hence, usually, the kernel (similarity) matrix or the distance matrix has a low rank. Therefore, the Nystrom approximation works well for many kernel-based or distancebased manifold learning methods.


### Using Kernel Approximation in Landmark MDS

Consider Eq. (52) or (59) as the partitions of the kernel matrix K. Note that the (Mercer) kernel matrix is positive semi-definite so the Nystrom approximation can be applied for kernels. Recall that Eq. (14) decomposes the kernel matrix into eigenvectors and then Eq. (10) embeds data. However, for big data, the eigenvalue decomposition of kernel matrix is intractable. Therefore, using Eq. (55), we decompose an m × m submatrix of kernel. Comparing Eqs. (15) and (53) shows that:
R n×n Y = [R, S] (a) = [Σ (1/2) U , Σ (−1/2) U B],(60)
where (a) is because of Eqs. (56) and (57) and the terms U and Σ are obtained from Eq. (55). The Eq. (60) gives the approximately embedded data, with a good approximation. This is the embedding in landmark MDS (De Silva & Tenenbaum, 2003;. Truncating this matrix to have Y ∈ R p×n , with top p rows, gives the p-dimensional embedding of the n points. Comparing Eq. (60) with Eq. (10) shows that the formulae for embedding of landmarks, R, and the whole data (without Nystrom approximation) are similar to each other but one is with only landmarks and the other is with the whole data.


### Using Distance Matrix in Landmark MDS

If D ij denotes the (i, j)-th element of the distance matrix and v j is the j-th element of a vector v, Eq. (13) can be restated as (Platt, 2005):
K = −1 2 D 2 ij − 1 j i c i D 2 ij − 1 i j c j D 2 ij + i,j c i c j D 2 ij ,(61)
where i c i = 1. Let the partitions of the distance matrix be:
R n×n D = E F F G ,(62)
where E ∈ R m×m , F ∈ R m×(n−m) , and G ∈ R (n−m)×(n−m) in which m n. Comparing Eqs. (52) and (62) shows that the partitions of the kernel matrix can be obtained from the partitions of the distance matrix as (Platt, 2005):
A ij = −1 2 E 2 ij − 1 i 1 m p E 2 pj − 1 j 1 m q E 2 iq + 1 m 2 p,q E 2 pq ,(63)B ij = −1 2 F 2 ij − 1 i 1 m p F 2 qj − 1 j 1 m q E 2 iq ,(64)
and C ij can be obtained from Eq. (58). In landmark MDS and landmark Isomap, the partitions (submatrices) E and F of the Euclidean and geodesic distance matrices are calculated, respectively (see Eq. (62)). Then, Eqs. (63), (64), and (58) give us the partitions of the kernel matrix. Eqs. (55) and (60) provide the embedded data. It is noteworthy that the paper (Platt, 2005) shows that different landmark MDS methods, such as Landmark MDS (LMDS) (De Silva & Tenenbaum, 2003;, FastMap (Faloutsos & Lin, 1995), and MetricMap (Wang et al., 1999) are reduced to landmark MDS introduced here. The landmark MDS is also referred to as the sparse MDS (De Silva & Tenenbaum, 2004). Moreover, the Landmark Isomap (L-Isomap) (De Silva & Tenenbaum, 2003) is reduced to the landmark Isomap method explained here (see (Bengio et al., 2004b, Corollary 1) for proof). In other words, the large-scale manifold learning methods make use of the Nystrom approximation (Talwalkar et al., 2008).


## Simulations


### Dataset

For simulations, we used the MNIST dataset (LeCun et al.) includes 60,000 training images and 10,000 test images of size 28 × 28 pixels. It includes 10 classes for the 10 digits, 0 to 9. Because of tractability of the eigenvalue problem, we used a subset of 2000 training points (200 per class) and 500 test points (50 per class).


### Training Embedding


#### CLASSICAL MDS AND COMPARISON TO PCA

The embedding of training data by classical MDS is shown in Fig. 2. As can be seen, this embedding is interpretable because, for example, the digits (7 and 9), (6 and 8), and (5 and 6), which can be converted to each other by slight changes, are embedded close to one another. Figure 2 also depicts the embedding of training data by PCA. As can be seen, the embedding of PCA is equivalent to the embedding of classical MDS because rotation and flipping does not matter in manifold learning. This vali-  (Ghojogh et al., 2019c). In this embedding, you can see two legs one of which is bigger than the other. Figure 2 also shows the embedding by kernel Isomap. Note that kernel Isomap still uses the kernel calculated using the geodesic distance. Finally, the embedding by Sammon mapping, with 1000 iterations, is also illustrated in Fig. 2. The embeddings by all these methods are meaningful because the more similar digits have been embedded close to each other. An important fact about the embeddings is that the mean is zero in the embeddigns by classical MDS, kernel classical MDS, Isomap, and kernel Isomap. This is because of double centering the distance matrices in these methods (see Eqs. (12), (13), (31), and (32)).


### Out-of-sample Embedding

The out-of-sample embedding of the classical MDS and Isomap can be seen in Fig. 3. For the out-of-sample embeddings by classical MDS and Isomap, we used Eqs. (39) and (41), respectively. In the Isomap method, as it is difficult to implement the geodesic distance matrix calculated from only the training points as the intermediate points, we used an approximation in which the test points can also be used as intermediate points. A slight shift in the mean of out-of-sample embedding in the Isomap result is because of this approximation.


### Code Implementations

The Python code implementations of simulations can be found in the repositories of the following github profile: https://github.com/bghojogh


## Conclusion

This tutorial and survey paper was on MDS, Sammon mapping, and Isomap. Classical MDS, kernel classical MDS, metric MDS, and non-metric MDS were explained as categories of MDS. Sammon mapping and Isomap were also explained as special cases of metric MDS and kernel classical MDS. Kernel Isomap was also introduced. Out-ofsample extensions of these methods using eigenfunctions and kernel mapping were also provided. Landmark MDS and landmark Isomap using Nystron approximation were also covered in this paper. Finally, some simulations were provided to show the embeddings. Some specific methods, based on MDS and Isomap were not covered in this paper for the sake of brevity. some examples of these methods are supervised Isomap (Wu & Chan, 2004), robust kernel Isomap (Choi & Choi, 2007) and kernel Isomap for noisy data (Choi & Choi, 2005).


where N i denotes the set of indices of kNN of the i-th point.


One can use Eq. (39) to embed the i-th out-of-sample data point x (t) i . For this purpose, x (t)

## Figure 2 .
2Embedding of the training data in (a) classical MDS, (b) PCA, (c) kernel classical MDS (with cosine kernel), (d) Isomap, (e) kernel Isomap, and (f) Sammon mapping.

## Figure 3 .
3Embedding of the out-of-sample data in (a) classical MDS, and (b) Isomap. The transparent points indicate the embedding of training data. dates the claim of equivalence of PCA and classical MDS, stated in Section 2.1.3. 7.2.2. KERNEL CLASSICAL MDS, ISOMAP, KERNEL ISOMAP, AND SAMMON MAPPING The embedding of kernel classical MDS or the generalized classical MDS (with cosine kernel) is also shown in Fig. 2. This figure also includes the embedding by Isomap. It is empirically observed that the embeddings by Isomap are usually like the legs of an octopus

## Table 2 .
21) for more details. 
Comparing Eqs. (11), (12), and (13) with Eq. (3) shows 
that we can use a general kernel matrix, like Radial Basis 
Function (RBF) kernel, in classical MDS to have general-
ized classical MDS. In summary, for embedding X using 
classical MDS, the eigenvalue decomposition of the kernel 
matrix K is obtained similar to Eq. (3): 


Multidimensional Scaling, Sammon Mapping, and Isomap: Tutorial and Survey

Generalized non-metric multidimensional scaling. Agarwal, Sameer, Josh Wills, Cayton, Lawrence, Lanckriet, Gert, David Kriegman, Serge Belongie, In Artificial Intelligence and Statistics. Agarwal, Sameer, Wills, Josh, Cayton, Lawrence, Lanck- riet, Gert, Kriegman, David, and Belongie, Serge. Gen- eralized non-metric multidimensional scaling. In Artifi- cial Intelligence and Statistics, pp. 11-18, 2007.

A course in differential geometry. Thierry Aubin, Graduate Studies in Mathematics. American Mathematical SocietyAubin, Thierry. A course in differential geometry, vol- ume 27. American Mathematical Society, Graduate Studies in Mathematics, 2001.

Foundations of multidimensional scaling. Richard Beals, Krantz, H David, Amos Tversky, Psychological review. 752127Beals, Richard, Krantz, David H, and Tversky, Amos. Foundations of multidimensional scaling. Psychological review, 75(2):127, 1968.

Learning eigenfunctions of similarity: linking spectral clustering and kernel PCA. Yoshua Bengio, Vincent, Pascal, Paiement, Jean-François, O Delalleau, M Ouimet, N Leroux, 1232Departement dInformatique et Recherche Oprationnelle. Technical ReportBengio, Yoshua, Vincent, Pascal, Paiement, Jean-François, Delalleau, O, Ouimet, M, and LeRoux, N. Learning eigenfunctions of similarity: linking spectral clustering and kernel PCA. Technical report, Technical Report 1232, Departement dInformatique et Recherche Opra- tionnelle , 2003a.

Nicolas. Spectral clustering and kernel PCA are learning eigenfunctions. Yoshua Bengio, Vincent, Pascal, Paiement, Jean-François, Delalleau, Olivier, Marie Ouimet, Le Roux, Citeseer1239Bengio, Yoshua, Vincent, Pascal, Paiement, Jean-François, Delalleau, Olivier, Ouimet, Marie, and Le Roux, Nico- las. Spectral clustering and kernel PCA are learning eigenfunctions, volume 1239. Citeseer, 2003b.

Learning eigenfunctions links spectral embedding and kernel PCA. Yoshua Bengio, Delalleau, Olivier, Nicolas Roux, Le, Paiement, Jean-François, Pascal Vincent, Marie Ouimet, Neural computation. 1610Bengio, Yoshua, Delalleau, Olivier, Roux, Nicolas Le, Paiement, Jean-François, Vincent, Pascal, and Ouimet, Marie. Learning eigenfunctions links spectral embed- ding and kernel PCA. Neural computation, 16(10): 2197-2219, 2004a.

Out-of-sample extensions for LLE, Isomap, MDS, eigenmaps, and spectral clustering. Yoshua Bengio, Jean - Paiement, Vincent, Pascal, Delalleau, Olivier, Roux, L Nicolas, Marie Ouimet, Advances in neural information processing systems. Bengio, Yoshua, Paiement, Jean-françcois, Vincent, Pas- cal, Delalleau, Olivier, Roux, Nicolas L, and Ouimet, Marie. Out-of-sample extensions for LLE, Isomap, MDS, eigenmaps, and spectral clustering. In Advances in neural information processing systems, pp. 177-184, 2004b.

Spectral dimensionality reduction. Yoshua Bengio, Delalleau, Le Olivier, Roux, Nicolas, Paiement, Jean-François, Pascal Vincent, Marie Ouimet, Feature Extraction. SpringerBengio, Yoshua, Delalleau, Olivier, Le Roux, Nicolas, Paiement, Jean-François, Vincent, Pascal, and Ouimet, Marie. Spectral dimensionality reduction. In Feature Extraction, pp. 519-550. Springer, 2006.

Modern multidimensional scaling: Theory and applications. Ingwer Borg, Groenen, J F Patrick, Springer Science & Business MediaBorg, Ingwer and Groenen, Patrick JF. Modern multidi- mensional scaling: Theory and applications. Springer Science & Business Media, 2005.

A general framework for dimensionality-reducing data visualization mapping. Kerstin Bunte, Michael Biehl, Barbara Hammer, Neural Computation. 243Bunte, Kerstin, Biehl, Michael, and Hammer, Barbara. A general framework for dimensionality-reducing data vi- sualization mapping. Neural Computation, 24(3):771- 804, 2012.

The analytical solution of the additive constant problem. Francis Cailliez, Psychometrika. 482Cailliez, Francis. The analytical solution of the additive constant problem. Psychometrika, 48(2):305-308, 1983.

. Heeyoul Choi, Choi , Seungjin. Kernel Isomap. Electronics letters. 4025Choi, Heeyoul and Choi, Seungjin. Kernel Isomap. Elec- tronics letters, 40(25):1612-1613, 2004.

Kernel Isomap on noisy manifold. Heeyoul Choi, Seungjin Choi, Proceedings. The 4th International Conference on Development and Learning. The 4th International Conference on Development and LearningIEEEChoi, Heeyoul and Choi, Seungjin. Kernel Isomap on noisy manifold. In Proceedings. The 4th International Confer- ence on Development and Learning, 2005, pp. 208-213. IEEE, 2005.

. Heeyoul Choi, Choi , Seungjin. Robust kernel Isomap. Pattern Recognition. 403Choi, Heeyoul and Choi, Seungjin. Robust kernel Isomap. Pattern Recognition, 40(3):853-862, 2007.

Introduction to algorithms. Thomas H Cormen, Charles E Leiserson, Rivest, L Ronald, Clifford Stein, MIT pressCormen, Thomas H, Leiserson, Charles E, Rivest, Ronald L, and Stein, Clifford. Introduction to algo- rithms. MIT press, 2009.

Multidimensional scaling. Michael Cox, Aa, Trevor F Cox, Handbook of data visualization. SpringerCox, Michael AA and Cox, Trevor F. Multidimensional scaling. In Handbook of data visualization, pp. 315-347. Springer, 2008.

Multidimensional scaling. Technical report. De Leeuw, University of California Los AngelesDe Leeuw, Jan. Multidimensional scaling. Technical re- port, University of California Los Angeles, 2011.

Global versus local methods in nonlinear dimensionality reduction. De Silva, Vin, Joshua B Tenenbaum, Advances in neural information processing systems. De Silva, Vin and Tenenbaum, Joshua B. Global versus local methods in nonlinear dimensionality reduction. In Advances in neural information processing systems, pp. 721-728, 2003.

Sparse multidimensional scaling using landmark points. De Silva, Vin, Joshua B Tenenbaum, Stanford UniversityTechnical reportDe Silva, Vin and Tenenbaum, Joshua B. Sparse multi- dimensional scaling using landmark points. Technical report, Technical report, Stanford University, 2004.

Fastmap: A fast algorithm for indexing, data-mining and visualization of traditional and multimedia datasets. Christos Faloutsos, Lin, King-Ip, Proceedings of the 1995 ACM SIGMOD international conference on Management of data. the 1995 ACM SIGMOD international conference on Management of dataFaloutsos, Christos and Lin, King-Ip. Fastmap: A fast algo- rithm for indexing, data-mining and visualization of tra- ditional and multimedia datasets. In Proceedings of the 1995 ACM SIGMOD international conference on Man- agement of data, pp. 163-174, 1995.

Dimensionality reduction a short tutorial. Ali Ghodsi, Ontario, CanadaDepartment of Statistics and Actuarial Science, Univ. of WaterlooTechnical reportGhodsi, Ali. Dimensionality reduction a short tutorial. Technical report, Department of Statistics and Actuarial Science, Univ. of Waterloo, Ontario, Canada, 2006.

Unsupervised and supervised principal component analysis: Tutorial. Benyamin Ghojogh, Mark Crowley, arXiv:1906.03148arXiv preprintGhojogh, Benyamin and Crowley, Mark. Unsupervised and supervised principal component analysis: Tutorial. arXiv preprint arXiv:1906.03148, 2019.

Eigenvalue and generalized eigenvalue problems. Ghojogh, Benyamin, Fakhri Karray, Mark Crowley, arXiv:1903.11240Tutorial. arXiv preprintGhojogh, Benyamin, Karray, Fakhri, and Crowley, Mark. Eigenvalue and generalized eigenvalue problems: Tuto- rial. arXiv preprint arXiv:1903.11240, 2019a.

Roweis discriminant analysis: A generalized subspace learning method. Ghojogh, Benyamin, Fakhri Karray, Mark Crowley, arXiv:1910.05437arXiv preprintGhojogh, Benyamin, Karray, Fakhri, and Crowley, Mark. Roweis discriminant analysis: A generalized subspace learning method. arXiv preprint arXiv:1910.05437, 2019b.

Feature selection and feature extraction in pattern analysis: A literature review. Ghojogh, Benyamin, Maria N Samad, Sayema Mashhadi, Asif, Kapoor, Tania, Ali, Wahab, Fakhri Karray, Mark Crowley, arXiv:1905.02845arXiv preprintGhojogh, Benyamin, Samad, Maria N, Mashhadi, Sayema Asif, Kapoor, Tania, Ali, Wahab, Karray, Fakhri, and Crowley, Mark. Feature selection and fea- ture extraction in pattern analysis: A literature review. arXiv preprint arXiv:1905.02845, 2019c.

Quantile-quantile embedding for distribution transformation, manifold embedding, and image embedding with choice of embedding distribution. Ghojogh, Benyamin, Fakhri Karray, Mark Crowley, arXiv:2006.11385arXiv preprintGhojogh, Benyamin, Karray, Fakhri, and Crowley, Mark. Quantile-quantile embedding for distribution transfor- mation, manifold embedding, and image embedding with choice of embedding distribution. arXiv preprint arXiv:2006.11385, 2020.

Out-of-sample kernel extensions for nonparametric dimensionality reduction. Andrej Gisbrecht, Lueks, Wouter, Bassam Mokbel, Barbara Hammer, European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning. 2012Gisbrecht, Andrej, Lueks, Wouter, Mokbel, Bassam, and Hammer, Barbara. Out-of-sample kernel extensions for nonparametric dimensionality reduction. In European Symposium on Artificial Neural Networks, Computa- tional Intelligence and Machine Learning, volume 2012, pp. 531-536, 2012.

Barbara. Parametric nonlinear dimensionality reduction using kernel t-sne. Andrej Gisbrecht, Alexander Schulz, Hammer, Neurocomputing. 147Gisbrecht, Andrej, Schulz, Alexander, and Hammer, Bar- bara. Parametric nonlinear dimensionality reduction us- ing kernel t-sne. Neurocomputing, 147:71-82, 2015.

Some distance properties of latent root and vector methods used in multivariate analysis. John C Gower, Biometrika. 533-4Gower, John C. Some distance properties of latent root and vector methods used in multivariate analysis. Biometrika, 53(3-4):325-338, 1966.

A kernel view of the dimensionality reduction of manifolds. Jihun Ham, Daniel D Lee, Sebastian Mika, Bernhard Schölkopf, Proceedings of the twentyfirst international conference on Machine learning. the twentyfirst international conference on Machine learning47Ham, Jihun, Lee, Daniel D, Mika, Sebastian, and Schölkopf, Bernhard. A kernel view of the dimensional- ity reduction of manifolds. In Proceedings of the twenty- first international conference on Machine learning, pp. 47, 2004.

Kernel methods in machine learning. The annals of statistics. Thomas Hofmann, Bernhard Schölkopf, Alexander J Smola, Hofmann, Thomas, Schölkopf, Bernhard, and Smola, Alexander J. Kernel methods in machine learning. The annals of statistics, pp. 1171-1220, 2008.

Non-metric multidimensional scaling (mds). Steven M Holland, Department of Geology, University of GeorgiaTechnical reportHolland, Steven M. Non-metric multidimensional scaling (mds). Technical report, Department of Geology, Uni- versity of Georgia, 2008.

Lecture: Multidimensional scaling, advanced applied multivariate analysis. Lecture notes, Department of Statistics. Sungkyu Jung, University of PittsburghJung, Sungkyu. Lecture: Multidimensional scaling, ad- vanced applied multivariate analysis. Lecture notes, De- partment of Statistics, University of Pittsburgh, 2013.

The classification of facial expressions of emotion: A multidimensional-scaling approach. Mary Katsikitis, Perception. 265Katsikitis, Mary. The classification of facial expressions of emotion: A multidimensional-scaling approach. Percep- tion, 26(5):613-626, 1997.

Literature survey on low rank approximation of matrices. Linear and Multilinear Algebra. Kishore Kumar, Jan Schneider, 65Kishore Kumar, N and Schneider, Jan. Literature survey on low rank approximation of matrices. Linear and Multi- linear Algebra, 65(11):2212-2244, 2017.

Non-metric multidimensional scaling. a numerical method. J Kruskal, Psychometrika. 2911Kruskal, J. Non-metric multidimensional scaling. a numer- ical method. Psychometrika, 29(1):1, 1964a.

Multidimensional scaling by optimising goodness-of-fit to non-metric hypotheses. Joseph B Kruskal, Psychometrika. 291Kruskal, Joseph B. Multidimensional scaling by optimising goodness-of-fit to non-metric hypotheses. Psychome- trika, 29(1):115-29, 1964b.

Yann Lecun, Corinna Cortes, Christopher J Burges, MNIST handwritten digits dataset. LeCun, Yann, Cortes, Corinna, and Burges, Christo- pher J.C. MNIST handwritten digits dataset. http: //yann.lecun.com/exdb/mnist/. Accessed: 2019.

Nonlinear dimensionality reduction. John A Lee, Michel Verleysen, Springer Science & Business MediaLee, John A and Verleysen, Michel. Nonlinear dimension- ality reduction. Springer Science & Business Media, 2007.

Curvilinear distance analysis versus isomap. John Lee, Aldo, Lendasse, Amaury, Verleysen, Michel, European Symposium on Artificial Neural Networks. Lee, John Aldo, Lendasse, Amaury, Verleysen, Michel, et al. Curvilinear distance analysis versus isomap. In Eu- ropean Symposium on Artificial Neural Networks, vol- ume 2, pp. 185-192, 2002.

Some properties of clasical multidimesional scaling. Kanti V Mardia, Communications in Statistics-Theory and Methods. 713Mardia, Kanti V. Some properties of clasical multi- dimesional scaling. Communications in Statistics- Theory and Methods, 7(13):1233-1241, 1978.

Lecture: Recasting principal components. Lecture notes for Data Visualization. Wayne Oldford, Department of Statistics and Actuarial Science, University of WaterlooOldford, Wayne. Lecture: Recasting principal compo- nents. Lecture notes for Data Visualization, Department of Statistics and Actuarial Science, University of Water- loo, 2018.

Fastmap, metricmap, and landmark mds are all nystrom algorithms. John Platt, AISTATS. Platt, John. Fastmap, metricmap, and landmark mds are all nystrom algorithms. In AISTATS, 2005.

Multidimensional scaling of emotional facial expressions: similarity from preschoolers to adults. James A Russell, Merry Bullock, Journal of personality and social psychology. 4851290Russell, James A and Bullock, Merry. Multidimensional scaling of emotional facial expressions: similarity from preschoolers to adults. Journal of personality and social psychology, 48(5):1290, 1985.

A nonlinear mapping for data structure analysis. John W Sammon, IEEE Transactions on computers. 1005Sammon, John W. A nonlinear mapping for data structure analysis. IEEE Transactions on computers, 100(5):401- 409, 1969.

Think globally, fit locally: unsupervised learning of low dimensional manifolds. Lawrence K Saul, Sam T Roweis, Journal of machine learning research. 4Saul, Lawrence K and Roweis, Sam T. Think globally, fit locally: unsupervised learning of low dimensional man- ifolds. Journal of machine learning research, 4(Jun): 119-155, 2003.

Smallest space analysis of intelligence and achievement tests. Itzchakm Schlesinger, Louis Guttman, Psychological Bulletin. 71295Schlesinger, ItzchakM and Guttman, Louis. Smallest space analysis of intelligence and achievement tests. Psycho- logical Bulletin, 71(2):95, 1969.

A generalised solution to the out-of-sample extension problem in manifold learning. Harry Strange, Reyer Zwiggelaar, Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence. the Twenty-Fifth AAAI Conference on Artificial IntelligenceStrange, Harry and Zwiggelaar, Reyer. A generalised solu- tion to the out-of-sample extension problem in manifold learning. In Proceedings of the Twenty-Fifth AAAI Con- ference on Artificial Intelligence, pp. 471-476, 2011.

Open Problems in Spectral Dimensionality Reduction. Harry Strange, Reyer Zwiggelaar, SpringerStrange, Harry and Zwiggelaar, Reyer. Open Problems in Spectral Dimensionality Reduction. Springer, 2014.

Large-scale manifold learning. Ameet Talwalkar, Sanjiv Kumar, Henry Rowley, IEEE Conference on Computer Vision and Pattern Recognition. IEEETalwalkar, Ameet, Kumar, Sanjiv, and Rowley, Henry. Large-scale manifold learning. In 2008 IEEE Confer- ence on Computer Vision and Pattern Recognition, pp. 1-8. IEEE, 2008.

A global geometric framework for nonlinear dimensionality reduction. Joshua B Tenenbaum, De Silva, Vin Langford, John C , Science. 2905500Tenenbaum, Joshua B, De Silva, Vin, and Langford, John C. A global geometric framework for nonlinear di- mensionality reduction. Science, 290(5500):2319-2323, 2000.

Multidimensional scaling: I. theory and method. Warren S Torgerson, Psychometrika. 174Torgerson, Warren S. Multidimensional scaling: I. theory and method. Psychometrika, 17(4):401-419, 1952.

Multidimensional scaling of similarity. Warren S Torgerson, Psychometrika. 304Torgerson, Warren S. Multidimensional scaling of similar- ity. Psychometrika, 30(4):379-393, 1965.

Evaluating a class of distance-mapping algorithms for data mining and clustering. Jason Wang, Tsong-Li, Wang, Xiong, Lin, King-Ip, Shasha, Dennis, Shapiro, A Bruce, Kaizhong Zhang, Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining. the fifth ACM SIGKDD international conference on Knowledge discovery and data miningWang, Jason Tsong-Li, Wang, Xiong, Lin, King- Ip, Shasha, Dennis, Shapiro, Bruce A, and Zhang, Kaizhong. Evaluating a class of distance-mapping al- gorithms for data mining and clustering. In Proceed- ings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 307-311, 1999.

Using the Nyström method to speed up kernel machines. Christopher Ki Williams, Matthias Seeger, Advances in neural information processing systems. Williams, Christopher KI and Seeger, Matthias. Using the Nyström method to speed up kernel machines. In Advances in neural information processing systems, pp. 682-688, 2001.

An extended Isomap algorithm for learning multi-class manifold. Yiming Wu, Chan , Kap Luk, Proceedings of 2004 International Conference on Machine Learning and Cybernetics. 2004 International Conference on Machine Learning and CyberneticsIEEE6Wu, Yiming and Chan, Kap Luk. An extended Isomap algorithm for learning multi-class manifold. In Pro- ceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No. 04EX826), volume 6, pp. 3429-3433. IEEE, 2004.

Multidimensional scaling: History, theory, and applications. Forrest W Young, Psychology PressYoung, Forrest W. Multidimensional scaling: History, the- ory, and applications. Psychology Press, 2013.

Facial expression recognition based on local binary patterns and kernel discriminant Isomap. Xiaoming Zhao, Shiqing Zhang, Sensors. 1110Zhao, Xiaoming and Zhang, Shiqing. Facial expression recognition based on local binary patterns and kernel dis- criminant Isomap. Sensors, 11(10):9573-9588, 2011.