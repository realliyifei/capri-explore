# Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. A Survey of Text Representation Methods and their Genealogy

CorpusID: 252239868 - [https://www.semanticscholar.org/paper/e541fb54b8b9f2b4d8253f678a28830cd4f52d86](https://www.semanticscholar.org/paper/e541fb54b8b9f2b4d8253f678a28830cd4f52d86)

Fields: Computer Science, Linguistics

## (s4) C. ARTIFICIAL NEURAL NETWORKS
(p4.0) The application of artificial neural networks for NLP can be divided into the creation of distributional representations, that is the contextualization of textual units, and the resolution of downstream tasks, for example text classification. A wide variety of different models can be employed for either objective (for an introduction to machine learning and deep learning cf. also [25]). Nonetheless, a smaller subset of algorithms dominates TR and NLP today, which we present in the following:
## (s9) B. GENEALOGY
(p9.0) In order to trace the history of the presented TR methods we place them in a genealogy that comprises temporal information along one axis and identifies different evolutionary branches on the other axis. The latter describes four paradigms that can be traced throughout the history of TR: size, context, efficiency, and multi-tasking. Hence, any TR method can be assigned to at least one of these evolutionary branches.

(p9.1) Size. TR methods that fall into the size branch value more data and larger models over highly curated corpora and custom-built training tasks. These models consistently achieve state-of-the-art results on NLP tasks but require extensive computational resources and distributed architectures, making them inaccessible for most researchers.

(p9.2) Context. The representatives of the context branch aim at increasing the distance of textual dependencies that can be modeled. This is achieved either by allowing for longer text sequences to be processed by a model or by adapting the memory mechanism of a model, for example storing more network activations. Context information is especially valuable if either the input or the output of a model is expected to be long in a given NLP task, for example for summarization.

(p9.3) Efficiency. The efficiency branch tries to achieve high performance on a small scale. It has gained traction due to the trend of continuously creating larger, more potent models, which, however, remain inaccessible for most researchers due to immense resource costs. Common approaches are the optimization of the operations of a TR method or the transferal of the abilities of larger models to versions with a smaller footprint.

(p9.4) Multi-tasking. Finally, the multi-tasking branch aims at explicitly capturing many facets of language by simultaneously optimizing a model on various carefully crafted training tasks. Hence, finding an effective combination of training tasks is the principal objective of this branch. For instance, it could include semantic, syntactic, and orthographic tasks. Figure 2 provides a schematic overview of the relations of the different TR methods.

(p9.5) An analysis of the temporal axis of the genealogy indicates that the evolution of TR can be split into two phases. The first phase ranges from 2013 to 2017 and the second phase ranges from 2019 until today. 2018 can be seen as the transition between the two phases. At the beginning of each phase stands a novel approach that changed the landscape of TR and inspired the development of manifold TR methods. This is reflected in the high number of outgoing connections of such a TR method in the genealogy. Consequently, the first phase was initiated by the word2vec models and the second phase by BERT and, in part, GPT. During the first phase, new TR methods were inspired by word2vec models, yet they scarcely built on top of each other and rather branched out separately to address the four paradigms size, context, efficiency, and multi-tasking. The high amount of leaf nodes in the genealogy illustrates that the first phase was characterized  by radical changes concerning model architecture and other conceptional choices. The second phase stands in stark contrast. While the TR models still branched out, the tendency was to refine and extend previous methods, in particular BERT and GPT. This emphasizes that a robust architecture was found with the Transformer [36]. An analysis of the transition period reveals two mostly disjoint streams that clearly show the adoption of LM in favor of previous contextualization approaches. Moreover, a split can be identified between TR methods that employ autoregressive approaches (left) and those that use autoencoder approaches (right). Taking another look at the second period, however, the two split streams are in the process of reuniting due to an increasing number of new sequence-to-sequence architectures, which combine aspects of autoencoding and autoregression.
