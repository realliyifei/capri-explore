# How Can Recommender Systems Benefit from Large Language Models: A Survey

CorpusID: 259129651 - [https://www.semanticscholar.org/paper/bac54736112098616f0e1c90435888ef3e119d32](https://www.semanticscholar.org/paper/bac54736112098616f0e1c90435888ef3e119d32)

Fields: Computer Science, Linguistics

## (s0) Background
(p0.0) With the rapid development of online services, recommender systems (RS) have become increasingly important to match users' information needs [Dai et al., 2021; and alleviate the problem of information overloading [Guo 1 https://github.com/CHIANGEL/Awesome-LLM-for-RecSys * Jianghao Lin and Xinyi Dai are the co-first authors.

(p0.1) † Ruiming Tang and Weinan Zhang are the co-corresponding authors. Despite the different forms of application tasks (e.g., top-N recommendation, or sequential recommendation), the common learning objective for a deep learning based recommender system is to estimate a given user's preference towards each candidate item, and finally arrange a ranked list of items presented to the user Xi et al., 2023a].
## (s9) Discussion
(p9.0) We could observe that the development trajectory about where to adapt LLM to RS is fundamentally aligned with the progress of large language models. Back to year 2021 and early days in 2022, the parameter sizes of pretrained language models are still relatively small (e.g., 340M for BERT, 1.5B for GPT2-XL). Therefore, earlier works usually tend to either incorporate these small-scale language models as simple textual feature encoders, or as scoring/ranking functions finetuned to fit the data distribution from recommender systems.

(p9.1) As the model size gradually increases, researchers discover that large language models have gained emergent abilities (e.g., instruction following, reasoning), as well as a vast amount of open-world knowledge with powerful text generation capacities. Equipped with these amazing features brought by large-scale parameters, LLM starts to not only deepen the usage in the feature encoder and scoring/ranking function stage, but also move beyond and extend their roles into other stages of the recommendation pipeline. For instance, in the feature engineering stage, we could instruct LLM to generate reliable auxiliary features and synthetic training data [Liu et al., 2023c]. In this way, open-world knowledge from LLM is injected into the recommendation model, which is usually a closed-domain system. Not to mention, participating in the pipeline control further requires sufficient logical reasoning and tool utilization capabilities, which are possessed by LLM.

(p9.2) In summary, we believe that, as the abilities of large language models are further explored, they will form gradually deeper couplings and bindings with multiple stages of the recommendation pipeline. Even further, we might need to customize large language models specifically tailored to satisfy the unique requirements of recommender systems [Lin and .
## (s10) How to Adapt LLM
(p10.0) To answer the "HOW" question about adapting LLM to RS, we carry out two orthogonal taxonomy criteria to distinguish the adaptation of LLMs to RS, resulting in a four-quadrant classification shown in Figure 3:

(p10.1) • Tune/Not Tune LLM denotes whether we will tune LLM during the training phase. The definition of tuning LLM includes both full finetuning and other parameter-efficient finetuning methods (e.g., LoRA [Hu et al., 2021]).

(p10.2) • Infer with/without CRM denotes whether we will involve conventional recommendation models (CRM) during the inference phase. Note that there are works that only use CRM to serve as independent pre-ranking functions to generate candidate item set for LLM. We categorize them as "infer without CRM", since the CRM is independent of LLM, and could be decoupled from the final recommendation task.
## (s12) Not Tune LLM; Infer w/o CRM (Quadrant 3)
(p12.0) With the emergence of large foundation models, especially ChatGPT, researchers intend to analyze the zero-shot or few-shot performance of LLM in recommendation domains, where LLM is frozen and CRM is not involved. Sileo et al.

(p12.1) [2022] apply zero-shot learning on GPT-2 by inferring the next item according to the user's behavior history, which merely defeats the random baseline. Other works [Wang and Lim, 2023;Li et al., 2023g] investigate zero-shot and few-shot recommendation setting based on the ChatGPT API, with delicate prompt engineering to instruct the LLM to perform tasks like rating prediction, pairwise comparison, and listwise ranking. Chat-REC  instructs ChatGPT to not only serve as the score/ranking function, but also take control over the recommendation pipeline, e.g., deciding when to call an independent pre-ranking model API. As illustrated in Figure 3, although a larger model size might bring performance improvement, the zero-shot or few-shot learning of LLM is still much inferior compared with the light-weight CRM tuned on the training data, indicating the importance of in-domain collaborative knowledge from recommender systems.
## (s18) LLM is Good at Reranking Hard Samples
(p18.0) Although works in quadrant 3 suffer from inferior performance for zero/few-shot learning since little in-domain collaborative knowledge is involved, researchers Hou et al., 2023b] have found that large language models such as ChatGPT are more likely to be a good reranker for hard samples. They introduce the filter-then-rerank paradigm which leverages a pre-ranking function from traditional recommender systems (e.g., matching or pre-ranking stage in industrial applications) to pre-filter those easy negative items, and thus generate a set of candidates with harder samples for LLM to rerank. In this way, the listwise reranking performance of LLM (especially ChatGPT-like APIs) could be promoted. This finding is instructive for industrial applications, where we could require LLM to only handle hard samples and leave other samples for light-weight models for saving computational costs.
## (s20) Training Efficiency
(p20.0) There are two key aspects to keep good performance of modern deep learning based recommender systems: (1) enlarge the volumes of training data (e.g., billion-level training samples), and (2) increase the model update frequency (from day-level to hour-level, or even minute-level). Both of them highly require the training efficiency. Although, as suggested in Section 3.5, tuning LLM (possibly with CRM) is a promising approach to align LLM to RS for better performance, it actually brings prohibitive adaptation costs in terms of both memory usage and time consumption. Therefore, how to ensure the efficiency when we involve LLM in the training phase is a key challenge for industrial applications.

(p20.1) Existing works mainly propose to leverage parameterefficient finetuning strategies (e.g., option tuning [Cui et al., 2022] and layerwise adaptor tuning ), which mainly solve the memory usage problem, but the time consumption is still high.

(p20.2) From an industrial perspective, we suggest adopting the long-short update strategy, when we leverage LLM for feature engineering and feature encoder. To be specific, we can cut down the training data volume and relax the update frequency for LLM (e.g.week-level) while maintaining full training data and high update frequency for CRM. The basis to support this approach is that researchers  point out that LLM has strong inductive learning capacities to produce generalized and reliable outputs via a handful of supervisions. In this way, LLM can provide aligned in-domain knowledge to CRM, while CRM act as a frequently updated adapter for LLM.
