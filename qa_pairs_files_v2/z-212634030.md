# A Survey on The Expressive Power of Graph Neural Networks

CorpusID: 212634030 - [https://www.semanticscholar.org/paper/6fb8b967dad742eb390a3090f094a12f2d909538](https://www.semanticscholar.org/paper/6fb8b967dad742eb390a3090f094a12f2d909538)

Fields: Computer Science, Mathematics

## (s11) Connection between GNNs And The WL Algorithm
(p11.0) In this section, we introduce the connection between GNNs and the WL algorithm, found by Xu et al. (2019) and Morris et al. (2019). First, the following theorem is easy to see.  2019)). For any message passing GNN and for any graphs G and H, if the 1-WL algorithm outputs that G and H are "possibly isomorphic", the embeddings h G and h H computed by the GNN are the same.

(p11.1) In other words, message passing GNNs are less powerful than the 1-WL algorithm. This is because the aggregation and update functions can be seen as the hash function of the 1-WL algorithm, and the aggregation and update functions are not necessarily injective. In the light of the correspondence between the GNNs and the 1-WL algorithm, Xu et al. (2019) proposed a GNN model that is as powerful as the 1-WL algorithm, by making the aggregation and update functions injective.
## (s13) Relational Pooling
(p13.0) In this section, we introduce another approach to build powerful GNNs proposed by Murphy et al. (2019b). The idea is very simple. The relational pooling layer takes an average of all permutations, as Jannosy pooling (Murphy et al., 2019a). Namely, let f be a message passing GNN, A be the adjacency matrix of G = (V, E, X), I n ∈ R n×n be the identity matrix, and S n be the symmetric group over V . Then, relational pooling GNNs (RP-GNNs) are defined as follows.

(p13.1) In other words, RP-GNNs concatenate a one-hot encoding of the node index to the node feature, and take an average of all permutations. The strong points of RP-GNNs is that they are obviously permutation invariant by construction and are more powerful than GINs and the 1-WL algorithm.

(p13.2) Theorem 11 (Murphy et al. (2019b), Theorem 2.2). The RP-GNNs are more powerful than the original message passing GNNs f . In particular, if f is modeled by GINs (Xu et al., 2019), and the graph has discrete attributes, the RP-GNNs are more powerful than the 1-WL algorithm.

(p13.3) However, RP-GNNs have two major drawbacks. First, RP-GNNs cannot handle graphs with different sizes since the dimension of feature vectors depend on the number n of nodes. This drawback is alleviated by adding dummy nodes when the upper bound of the number of nodes is known beforehand. The second drawback is its computation cost since it takes average of n! terms, which is not tractable in practice (e.g., 15! = 1307674368000 ≈ 10 12 ). To overcome the second issue, Murphy et al. (2019b) proposed three approximation methods. The first method uses canonical orientations, which first computes one or several canonical labeling by breath-first search, depth-first search, or using centrality scores as Niepert et al. (2016). The second method samples some permutations, instead of using all permutations. The third method uses only a part of graphs, instead of all nodes.
