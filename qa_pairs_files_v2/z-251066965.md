# Efficient High-Resolution Deep Learning: A Survey

CorpusID: 251066965 - [https://www.semanticscholar.org/paper/5169a986f8f30418f239372e6f9b59de832aaac2](https://www.semanticscholar.org/paper/5169a986f8f30418f239372e6f9b59de832aaac2)

Fields: Computer Science, Environmental Science

## (s8) B. Selective Zooming and Skipping
(p8.0) Selective zooming and skipping (SZS) methods take a more efficient approach to cutting into patches by only zooming into regions of the input image that are important. The zoom level may differ across different patches, and some patches may be entirely skipped. Reinforced Auto-Zoom Net (RAZN) [78] uses reinforcement learning to determine where to zoom in WSIs for the task of breast cancer segmentation. RAZN assumes the zoom-in action can be performed at most m times and the zooming rate is a constant r. At each zoom level i, there is a different segmentation network f θi and a different policy network g θi . Initially, policy network g θ0 takes a cropped image x 0 ∈ R H×W ×3 as input and determines whether to zoom-in or to break. If there is no need to zoom in, x 0 is given as input to segmentation network f θ0 which produces the output, otherwise, a higher-resolution imagex 0 ∈ R rH×rW ×3 is sampled from the same area and will be cut into r 2 patches of size H × W × 3. Each patch is then given to policy network g θ1 and this process is recursively repeated until all policy networks break or the maximum zoom level is reached. RAZN achieves an improved performance over other stateof-the-art methods while reducing the inference time by a factor of ∼2. Similarly, the methods in [79] and [80] use reinforcement learning for efficient object detection and aerial image classification, respectively.
## (s10) D. Task-Oriented Input Compression
(p10.0) Task-oriented input compression (TOIC) methods compress the high-resolution inputs into lightweight representations. These representations are then given to the task DNN as input instead of the high-resolution images or videos. The exact nature of the lightweight representations and the compression procedure varies from method to method and is often highly dependent on the underlying task.

(p10.1) There is an important distinction between this approach and neural image compression methods such as SlimCAE [95]. The goal of neural image compression is to learn optimal compression algorithms for the task at hand, in order to reduce the size of stored or transmitted data. Therefore, the network that compresses and decompresses this data may be very large and inefficient. Moreover, neural image compression aims to reconstruct the input from the compressed representations, whereas TOIC does not reconstruct the input data and strives to extract compact representations that are suitable for the second part of the network which is responsible for performing the task.
## (s11) E. High-Resolution Vision Transformers
(p11.0) As previously mentioned, the self-attention operation in Transformers has a high complexity that increases in a quadratic fashion with respect to the number of input tokens. This operation is formulated by

(p11.1) where query Q = XW Q ∈ R n×dq , key K = XW K ∈ R n×d k and value V = XW V ∈ R n×dv are obtained from sequence of input tokens X = (x 1 , . . . , x n ) ∈ R n×d , and W Q , W K and W V are learnable weight matrices. Due to this quadratic complexity, naive approaches, such as ViT [6], that create a long sequence of input tokens from a high-resolution image will lead to massive complexity. On the other hand, if X contains few tokens, each input token represents a large area of the original image, leading to loss of detailed information that might be crucial to some applications.
