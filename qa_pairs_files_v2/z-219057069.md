# A Survey on Explainability: Why should we believe the accuracy of a model?

CorpusID: 219057069 - [https://www.semanticscholar.org/paper/04483a3f197b3183309aaae20241e1d4bdf92aed](https://www.semanticscholar.org/paper/04483a3f197b3183309aaae20241e1d4bdf92aed)

Fields: Computer Science, Philosophy

## (s2) Need for Explainability in various domains
(p2.0) Though the AI is able to help the society in real life decision making problems, there are situations witnessed where the AI is seen to fail and is disowned in many situations. This calls for an urgent need for a valid and trustworthy explainable system that could be trusted blindly by the users and organizations. Some of the scenarios where one needs an explainable model while taking life changing decisions and have faced failure due to lack of explainability are shown in Table 1. Neural Network inferred that patients with asthma has low risk of dying and can be treated as outpatients. Husky Vs Wolf [40] Distinguishing between images of wolves and Eskimo Dogs
## (s6) Explanation Techniques in XAI in Deep Learning
(p6.0) Many scholars have proposed a number of explanation methods and techniques for deep learning. One such taxonomy is provided recently by Arrieta et al. [4] which include -Explanation by Simplification, Feature Relevance Explanation, Visual Explanation, Local Explanation, Explanation of Example, Text Explanation, Visual Explanation, Architectural Modification, etc. Amongst them, we have chosen to survey on the Visual Explanation methods and Feature Relevance Explanation methods. Table 1 provides some of the approaches and visualization techniques employed in a few research works for providing explainability.
## (s7) Framework for Explainability in Deep Learning
(p7.0) Trying to understand why a model chosen by us for a particular computation, made such a prediction is an important topic of interest which have been evolving in recent times. Understanding the underlying reason behind such prediction may be able to incur something which is called trust. This is indeed an important aspect in the learning technologies using which one can take the decision whether or not to employ the model under consideration. Explaining a prediction refers to providing qualitative understanding of the relationship between the components of an instance and the prediction made by a model in the form of textual or visual artifacts. Developing a technique that can clearly explain the predictions predicted by a classifier is a challenging task. It addresses two of the most common problems i.e. trusting a model and trusting a prediction. Helps in picking up the most relevant classifier from among a set of classifiers. Accuracy measure may not be always suffice to explain a model prediction, one also needs explainability for such prediction. The main aim is to provide insight as to why the model gave us such prediction (trust related task). Also measures faithfulness of explanations. The various characteristics that are required to be fulfilled by a model to provide a proper explanation of its prediction require a well-designed framework that encompasses all the required properties explained in [4].

(p7.1) The authors developed an explainable model called LIME abbreviated as Local Interpretable Model-Agnostic Explanations, that can explain or detect features that contributed for such predictions. It thus provides a way of how humans can understand the working of the model and have faith in its prediction. It is inbuilt with characteristics such as interpretable, local fidelity, providing global perspective. In order to attain a deeper understanding into the model, the authors mainly focused on the aspect of trust. They developed this framework with a view to answering questions such as whether the predictions should be trusted by any user; whether the explanations provided by the framework could be used in general for selection of a model from a pair; etc. Towards answering the query about trusting a prediction and trusting the model, the authors presented the LIME and SP-LIME frameworks respectively. LIME was able to provide more than 90% recall on the books and DVDs datasets for the two interpretable classifiers, Decision Trees and Logistic regression classifier. The results indicate that the explanations provided by LIME are faithful to the model. At the same time it also proved to be a powerful tool for associating the trust in the predictions and are good in generalizing.
## (s8) Name
(p8.0) Year Author Description LIME 2016 Riberio et al. [40] Provide faithful explanation against the prediction made by a classifier. Addresses the "trusting a prediction" problem. SP-LIME 2016 Riberio et al. [40] Addresses the "trusting a model" problem.
