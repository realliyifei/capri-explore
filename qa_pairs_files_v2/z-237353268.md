# Neuron-level Interpretation of Deep NLP Models: A Survey

CorpusID: 237353268 - [https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd](https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd)

Fields: Computer Science

## (s8) Causation-based methods
(p8.0) The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction.

(p8.1) Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.
## (s13) Information Theoretic Metric
(p13.0) Information theoretic metrics such as mutual information have also been used to interpret representations of deep NLP models (Voita and Titov, 2020;Pimentel et al., 2020). Here, the goal is to measure the amount of information a representation provides about a linguistic properties. Hennigen et al. (2020) used mutual information to evaluate the effectiveness of their Gaussian-based method by calculating the mutual information between subset of neurons and linguistic concepts.
## (s32) Open Issues and Future Directions
(p32.0) In the following section, we discuss several open issues and limitations related to methods, evaluation and datasets. Moreover, we provide potential future directions vital to the progress of neuron and model interpretation.

(p32.1) • DNNs are distributed in nature which encourages groups of neurons to work together to learn a concept. The current analysis methods, at large, ignore interaction between neurons while discovering neurons with respect to a concept. Trying all possible combination of neurons is a computationally intractable problem. A linear classifier using ElasticNet regularization (Dalvi et al., 2019) considers grouping of features during training -however, it's effectiveness in handling grouped neurons has not been empirically validated. Evolutionary algorithms || do not make any assumption of the underline distribution of the features and they have been effectively used for feature selection of multivariate fea-|| https://en.wikipedia.org/wiki/ Evolutionary_algorithm tures. Exploring them for neuron selection is a promising research direction to probe towards latent concepts in these models.

(p32.2) • A large number of interpretation studies rely on human-defined linguistic concepts to probe a model. It is possible that the models do not strictly adhere to the humandefined concepts and learn novel concepts about the language. This results in an incorrect or incomplete analysis. Several researchers (Michael et al., 2020; made strides in this direction by analyzing hidden structures in the input representations in an unsupervised manner. They discovered existence of novel structures not captured in the human defined categories.  also proposed BERT ConceptNet, a manual annotation of the latent concepts in BERT. Introducing similar datasets across other models enables model-centric interpretation, and is a promising research direction.
