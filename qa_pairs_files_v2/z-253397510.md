# Pretraining in Deep Reinforcement Learning: A Survey

CorpusID: 253397510 - [https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096](https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096)

Fields: Computer Science

## (s14) CHALLENGES & FUTURE DIRECTIONS
(p14.0) Although count-based approaches are shown effective for exploration, it has been shown in previous work (Ecoffet et al., 2021) that they usually suffer from detachment, in which the agent loses track of interesting areas to explore, and derailment, in which the exploratory mechanism prevents it from returning to previously visited states. Count-based approaches also tend to be short-sighted, driving the agent to get stuck in local minima (Burda et al., 2019b).
## (s16) Skill Extraction
(p16.0) SPiRL (Pertsch et al., 2021a) Variational Auto-encoder OPAL (Ajay et al., 2021) Variational Auto-encoder Parrot (Singh et al., 2021) Normalizing Flow SkiLD (Pertsch et al., 2021b) Variational Auto-encoder TRIAL  Energy-based Model FIST (Hakhamaneshi et al., 2022) Variational Auto-encoder
## (s18) Offline Pretraining
(p18.0) Despite its attractive effectiveness of learning without human supervision, online pretraining is still limited for large-scale applications. Eventually, it is difficult to reconcile online interaction with the need to train on large and diverse datasets (Levine, 2021). To address this issue, it is desired to decouple data collection and pretraining and directly leverage historical data collected from other agents or humans.
## (s20) CHALLENGES & FUTURE DIRECTIONS
(p20.0) Despite its potential to extract useful primitive skills, it is still challenging to pretrain on highly sub-optimal offline data containing random actions (Ajay et al., 2021). Besides, RL with learned skills does not usually generalize to downstream tasks efficiently, requiring millions of online interactions to converge (Hakhamaneshi et al., 2022). A possible solution is to combine with successor features (Barreto et al., 2017;Hansen et al., 2020) for fast task inference. However, strategies that directly use the pretrained policies for exploitation may result in sub-optimal solutions in such a scenario (Campos et al., 2021).
## (s22) CHALLENGES & FUTURE DIRECTIONS
(p22.0) While unsupervised representations have been shown to bring significant improvements to downstream tasks, the absence of reward signals typically leads the pretrained encoder to focus on taskirrelevant features instead of task-relevant ones in visually complex environments . To alleviate this issue, one might incorporate additional inductive bias (Janny et al., 2022) or labeled data that are cheaper to obtain. We will discuss the latter solution in Section 5.

(p22.1) Another challenge for unsupervised representation learning is how to measure its effectiveness without access to downstream tasks. Such evaluation is beneficial because it can provide a proxy metric to predict performance and promote a deeper understanding of the semantic meanings of pretrained representations. To achieve this, it is desired to analyze these representations with probing techniques and determine which properties they encode. Although previous work has made efforts in this direction , it remains unclear what properties are most indispensable for pretrained representations.
## (s29) Challenges & Future Directions
(p29.0) In spite of some promising results, how generalist models benefit from multi-modal and multitask data remains unclear. More specifically, these models might suffer from detrimental gradient interference  between modalities and tasks due to the incurred optimization challenges. To mitigate this issue, it is desired to incorporate more analysis tools for optimization landscapes (Goodfellow & Vinyals, 2015) and gradients  to tease out the precise principles.
## (s30) Task Adaptation
(p30.0) While pretraining on unsupervised experiences can result in rich transferable knowledge, it remains challenging to adapt the knowledge to downstream tasks in which reward signals are exposed. In this section, we discuss briefly various considerations for downstream task adaptation. We limit the scope to online adaptation, while adaptation with offline RL or imitation learning is also feasible (Yang & Nachum, 2021).

(p30.1) In online task adaptation, a pretrained model is given, which can be composed of various components such as policies and representations, together with a target MDP that can interact with. Given that pretraining could result in different forms of knowledge, it brings difficulties to designing principled adaptation techniques. Nevertheless, considerable efforts have been made to study this aspect.
## (s31) Representation Transfer
(p31.0) In the field of supervised learning, recent advances (Devlin et al., 2019;He et al., 2020;Chen et al., 2020) have demonstrated that good representations can be pretrained on large-scale unlabeled dataset, as evidenced by their impressive downstream performances. The most common practice is to freeze the weights of the pretrained feature encoder and train a randomly initialized task-specific network on top of that during adaptation. The success of this paradigm is essentially based on the promise that related tasks can usually be solved using similar representations.

(p31.1) For RL, it has been shown that directly reusing pretrained task-agnostic representations can significantly improve sample efficiency on downstream tasks. For instance, Schwarzer et al. (2021b) conduct experiments on the Atari 100K benchmark and find that frozen representations pretrained on exploratory offline data already form a basis of data-efficient RL. This success also extends to the cases where domain discrepancy exists between upstream and downstream tasks (Shah & Kumar, 2021;Parisi et al., 2022). However, the issue of negative transfer in the face of domain discrepancy might be exacerbated for RL due to its complexity (Shah & Kumar, 2021).

(p31.2) When adapting to tasks that have the same environment dynamics as that of the upstream task(s), successor features (Barreto et al., 2017) can be a powerful tool to aid task adaptation. The framework of successor features is based on the following decomposition of reward functions: r s, a, s = φ s, a, s w,

(p31.3) where φ (s, a, s ) ∈ R d represents features of transition (s, a, s ) and w ∈ R d encodes rewardspecifying weights. This leads to a representation of the value function that decouples the dynamics of the environment from the rewards:

(p31.4) where we call ψ π (s, a) the successor features of (s, a) under π. Intuitively, ψ π summarizes the dynamics induced by π and has been studied within the framework of online pretraining (Hansen et al., 2020;Liu & Abbeel, 2021a) by combining with skill discovery approaches to implicitly learn controllable successor features ψ π (s, a). Given a learned ψ π (s, a), the problem of task adaptation reduces to a linear regression derived from Equation 3.
