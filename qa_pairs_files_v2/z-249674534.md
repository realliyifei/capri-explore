# A Survey on Gradient Inversion: Attacks, Defenses and Future Directions

CorpusID: 249674534 - [https://www.semanticscholar.org/paper/ef10e6a5ef05f4a76012669ca73d278e6df4d709](https://www.semanticscholar.org/paper/ef10e6a5ef05f4a76012669ca73d278e6df4d709)

Fields: Computer Science, Mathematics

## (s4) Model Training for Gradient Generation
(p4.0) In order to obtain the generated gradients, the attacker needs to feed the initialized dummy data and labels into the model. Based on the error between the model outputs and the labels, the gradients of weights can be calculated through backpropagation. In the setting of distributed learning, the global model can be viewed as a white box, which means the model structure and weights are known. However, the depth of training network structures and the batch size of training data can implicitly affect the results of data recovery. Network Model. Convolutional neural networks (CNN) or multilayer perceptron (MLP) are generally adopted as training networks for computer vision. Intuitively, the deeper the network is, the more parameters it contains. This raises two serious issues. First, the computational complexity is greatly increased, which makes it difficult or impossible for the optimization process to converge. Second, even if the procedure converges, there may exist multiple locally optimal solutions, resulting in a significant difference in the ground-truth value. So far, [Geiping et al., 2020] 
## (s14) Quantification of Gradient Sharing
(p14.0) The defenses we have discussed are all active interventions to the training process. However, we can also indirectly analyze whether the gradients have a serious risk of privacy leakage. Since each user owns his/her training data and gradients, the extent of leakage can be quantified if any correlation between them is found.  utilize mutual information to formulate this problem, and demonstrate the risk of gradient leakage is related to the model status and data distribution. [Mo et al., 2021] propose V-information and sensitivity metrics to quantify the layer-wise latent privacy leakages. This indicates that research in other domains can be transferred to study GradInv. We hope this survey can shed a light on future research and inspire more progress in different domains.
