# MIXED-PRECISION NEURAL NETWORKS: A SURVEY A PREPRINT

CorpusID: 251554723 - [https://www.semanticscholar.org/paper/30fa61a9c1db9527987fc25a91f26b9f23aa5152](https://www.semanticscholar.org/paper/30fa61a9c1db9527987fc25a91f26b9f23aa5152)

Fields: Computer Science

## (s19) HAWQ-V3: Dyadic Neural Network Quantization 4
(p19.0) In [51], the authors propose HAWQ-V3, a hardware-aware fixed low-precision and MXPDNN framework (for weights and activations) with integer-only inference by solving a constrained Integer Linear Programming (ILP) formulation. It relies on uniform quantization (with rounding). Also, channel-wise symmetric quantization is used for weights, layer-wise asymmetric quantization is utilized for activations, and static quantization for all the scaling factors. Results are presented with/without distillation, so this falls in the retraining/post-training category respectively.
## (s20) Towards Mixed-Precision Quantization of Neural Networks via Constrained Optimization (MPQNNCO)
(p20.0) The authors in [92] formulate the mixed-precision problem (for weights and activations) as a discrete constrained optimization problem which they solved by relying on second-order Taylor expansion, Hessian matrix computations, and a greedy search algorithm. In particular, the optimization problem is reformulated as a Multiple Knapsack problem which is solved by the proposed greedy search algorithm efficiently. This framework relies on deterministic rounding quantization for weights and activations (where the step size is found via solving an optimization problem). Since the framework starts with a pre-trained network and relies on fine-tuning after quantization, the technique is categorized as retraining.

(p20.1) This framework finds a middle ground between search-based techniques that rely on a small number of evaluations to reduce computational complexity and other methods that rely on some criteria that are easy to compute in order to reduce the time cost of performance evaluation. In particular, the proposed framework first formulates the MXPDNN allocation problem across the layers as a discrete optimization problem constrained by model compression. Solving the original optimization problem is computationally expensive as the network needs to be evaluated on the whole training dataset for each precision assignment. As such the authors approximate the objective function by relying on second-order Taylor expansion. The approximation consists of a constant zero-term, a first-order gradient, and a second-order Hessian matrix. The zero-term is omitted (the constant does not affect the optimization solution), and the first-order gradient is also omitted with the assumption that the pre-trained model will converge to a local minimum with a nearly zero gradient vector. As such, only the Hessian matrix is used as the objective function which approximates the loss perturbation from quantization. Solving this approximation at this stage, however; still suffers from large computational overhead as the Hessian matrix has a complexity quadratic to the number of parameters. To calculate the loss perturbation incurred due to quantization of a specific precision assignment efficiently, the Hessian matrix is further approximated by further omitting the term that represents the computational cost bottleneck. In addition to the computation becoming more efficient, there is also no need to traverse the complete dataset for calculating the loss perturbation since the loss converges rapidly as the number of images increases.
## (s25) APQ: Joint Search for Network Architecture, Pruning and Quantization Policy (APQ) 5
(p25.0) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.
## (s28) Rethinking Differentiable Search for Mixed-Precision Neural Networks (EdMIPS) 6
(p28.0) EdMIPS is proposed [37] as an efficient differentiable "mixed-precision network search" (MPS) that aims to find optimal per-layer precision of weights and activations without a proxy task, by solving a constrained optimization problem. The quantization technique is similar to that of HWGQ [82]. As such, the quantization technique is deterministic rounding. Since it trains from scratch, it is categorized as training-aware.
## (s39) Comparison Against Binary Neural Networks
(p39.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p39.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p39.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
## (s68) HAWQ-V3: Dyadic Neural Network Quantization 4
(p68.0) In [51], the authors propose HAWQ-V3, a hardware-aware fixed low-precision and MXPDNN framework (for weights and activations) with integer-only inference by solving a constrained Integer Linear Programming (ILP) formulation. It relies on uniform quantization (with rounding). Also, channel-wise symmetric quantization is used for weights, layer-wise asymmetric quantization is utilized for activations, and static quantization for all the scaling factors. Results are presented with/without distillation, so this falls in the retraining/post-training category respectively.
## (s69) Towards Mixed-Precision Quantization of Neural Networks via Constrained Optimization (MPQNNCO)
(p69.0) The authors in [92] formulate the mixed-precision problem (for weights and activations) as a discrete constrained optimization problem which they solved by relying on second-order Taylor expansion, Hessian matrix computations, and a greedy search algorithm. In particular, the optimization problem is reformulated as a Multiple Knapsack problem which is solved by the proposed greedy search algorithm efficiently. This framework relies on deterministic rounding quantization for weights and activations (where the step size is found via solving an optimization problem). Since the framework starts with a pre-trained network and relies on fine-tuning after quantization, the technique is categorized as retraining.

(p69.1) This framework finds a middle ground between search-based techniques that rely on a small number of evaluations to reduce computational complexity and other methods that rely on some criteria that are easy to compute in order to reduce the time cost of performance evaluation. In particular, the proposed framework first formulates the MXPDNN allocation problem across the layers as a discrete optimization problem constrained by model compression. Solving the original optimization problem is computationally expensive as the network needs to be evaluated on the whole training dataset for each precision assignment. As such the authors approximate the objective function by relying on second-order Taylor expansion. The approximation consists of a constant zero-term, a first-order gradient, and a second-order Hessian matrix. The zero-term is omitted (the constant does not affect the optimization solution), and the first-order gradient is also omitted with the assumption that the pre-trained model will converge to a local minimum with a nearly zero gradient vector. As such, only the Hessian matrix is used as the objective function which approximates the loss perturbation from quantization. Solving this approximation at this stage, however; still suffers from large computational overhead as the Hessian matrix has a complexity quadratic to the number of parameters. To calculate the loss perturbation incurred due to quantization of a specific precision assignment efficiently, the Hessian matrix is further approximated by further omitting the term that represents the computational cost bottleneck. In addition to the computation becoming more efficient, there is also no need to traverse the complete dataset for calculating the loss perturbation since the loss converges rapidly as the number of images increases.
## (s74) APQ: Joint Search for Network Architecture, Pruning and Quantization Policy (APQ) 5
(p74.0) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.
## (s77) Rethinking Differentiable Search for Mixed-Precision Neural Networks (EdMIPS) 6
(p77.0) EdMIPS is proposed [37] as an efficient differentiable "mixed-precision network search" (MPS) that aims to find optimal per-layer precision of weights and activations without a proxy task, by solving a constrained optimization problem. The quantization technique is similar to that of HWGQ [82]. As such, the quantization technique is deterministic rounding. Since it trains from scratch, it is categorized as training-aware.
## (s88) Comparison Against Binary Neural Networks
(p88.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p88.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p88.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
## (s117) HAWQ-V3: Dyadic Neural Network Quantization 4
(p117.0) In [51], the authors propose HAWQ-V3, a hardware-aware fixed low-precision and MXPDNN framework (for weights and activations) with integer-only inference by solving a constrained Integer Linear Programming (ILP) formulation. It relies on uniform quantization (with rounding). Also, channel-wise symmetric quantization is used for weights, layer-wise asymmetric quantization is utilized for activations, and static quantization for all the scaling factors. Results are presented with/without distillation, so this falls in the retraining/post-training category respectively.
## (s118) Towards Mixed-Precision Quantization of Neural Networks via Constrained Optimization (MPQNNCO)
(p118.0) The authors in [92] formulate the mixed-precision problem (for weights and activations) as a discrete constrained optimization problem which they solved by relying on second-order Taylor expansion, Hessian matrix computations, and a greedy search algorithm. In particular, the optimization problem is reformulated as a Multiple Knapsack problem which is solved by the proposed greedy search algorithm efficiently. This framework relies on deterministic rounding quantization for weights and activations (where the step size is found via solving an optimization problem). Since the framework starts with a pre-trained network and relies on fine-tuning after quantization, the technique is categorized as retraining.

(p118.1) This framework finds a middle ground between search-based techniques that rely on a small number of evaluations to reduce computational complexity and other methods that rely on some criteria that are easy to compute in order to reduce the time cost of performance evaluation. In particular, the proposed framework first formulates the MXPDNN allocation problem across the layers as a discrete optimization problem constrained by model compression. Solving the original optimization problem is computationally expensive as the network needs to be evaluated on the whole training dataset for each precision assignment. As such the authors approximate the objective function by relying on second-order Taylor expansion. The approximation consists of a constant zero-term, a first-order gradient, and a second-order Hessian matrix. The zero-term is omitted (the constant does not affect the optimization solution), and the first-order gradient is also omitted with the assumption that the pre-trained model will converge to a local minimum with a nearly zero gradient vector. As such, only the Hessian matrix is used as the objective function which approximates the loss perturbation from quantization. Solving this approximation at this stage, however; still suffers from large computational overhead as the Hessian matrix has a complexity quadratic to the number of parameters. To calculate the loss perturbation incurred due to quantization of a specific precision assignment efficiently, the Hessian matrix is further approximated by further omitting the term that represents the computational cost bottleneck. In addition to the computation becoming more efficient, there is also no need to traverse the complete dataset for calculating the loss perturbation since the loss converges rapidly as the number of images increases.
## (s123) APQ: Joint Search for Network Architecture, Pruning and Quantization Policy (APQ) 5
(p123.0) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.
## (s126) Rethinking Differentiable Search for Mixed-Precision Neural Networks (EdMIPS) 6
(p126.0) EdMIPS is proposed [37] as an efficient differentiable "mixed-precision network search" (MPS) that aims to find optimal per-layer precision of weights and activations without a proxy task, by solving a constrained optimization problem. The quantization technique is similar to that of HWGQ [82]. As such, the quantization technique is deterministic rounding. Since it trains from scratch, it is categorized as training-aware.
## (s137) Comparison Against Binary Neural Networks
(p137.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p137.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p137.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
