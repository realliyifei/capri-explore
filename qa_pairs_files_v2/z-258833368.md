# Deep Learning Approaches to Lexical Simplification: A Survey

CorpusID: 258833368 - [https://www.semanticscholar.org/paper/a9429423352c9b15531d32a43b5cd560519a3214](https://www.semanticscholar.org/paper/a9429423352c9b15531d32a43b5cd560519a3214)

Fields: Computer Science, Linguistics

## (s2) Evaluation Metrics
(p2.0) All sub-tasks of the LS pipeline are evaluated using precision, accuracy, recall, and F1-score. Several additional metrics have also been used: potential, mean average precision (MAP), and accuracy at top-k. Potential is the ratio of predicted candidate substitutions for which at least one of the top-k candidate substitutions generated was among the gold labels (Saggion et al., 2022). MAP evaluates whether the returned top-k candidate substitutions match the gold labels as well as whether they have the same positional rank. Accuracy at top-k = [1, 2, or 3] is the ratio of instances where at least one of the candidate substitutions at k is among the gold labels.
## (s5) Substitute Selection and Ranking
(p5.0) Traditional approaches to SS are still implemented post SG. Methods such as POS-tag and antonym filtering, semantic or sentence thresholds have been used to remove inappropriate candidate substitutions after having been generating from the above deep learning approaches (Saggion et al., 2022). Nevertheless, the majority of modern deep learning approaches have minimal SS, with SS often being simultaneously conducted during SG or SR. For instance, the metric used to generate the top-k can-didate substitutions, by it either similarity between word embeddings, or a pre-train LLM's prediction score, tends not to suggest candidate substitutions that are deemed as being inappropriate by other SS methods. Likewise, SR techniques that rank candidate substitutions in order of their appropriateness will in turn move inappropriate simplifications further down the list of top-k candidate substitutions to the point that they are no longer considered.
## (s9) English
(p9.0) Personalized-LS Lee and Yeung (2018b) constructed a dataset of 12,000 English words for personalized LS. These words were ranked on a fivepoint Likert scale. 15 native Japanese speakers were tasked with rating the complexity of each word. These complexity rating were then applied to BenchLS, in turn personalizing the dataset for Japanese speakers.

(p9.1) WCL Maddela and Xu (2018) introduced the Word Complexity Lexicon (WCL). The WCL is a dataset made up of 15,000 English words annotated with complexity ratings. Annotators were 11 nonnative English speakers using a six-point Likert scale.

(p9.2) LCP-2021* The dataset provided at the LCP-2021 shared-task (CompLex) , was developed using crowd sourcing. 10,800 complex words in context were selected from three corpora covering the Bible, biomedical articles, and European Parliamentary proceedings. Their lexical complexities were annotated using a 5-point Likert scale.
