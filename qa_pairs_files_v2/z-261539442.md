# Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice

CorpusID: 261539442 - [https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f](https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f)

Fields: Computer Science

## (s20) Neuro-Cognitive Science
(p20.0) The sensitivity to fake news attacks depends on whether Internet users think fake news articles/clips are real after reading them.Arisoy et al. [52] tried to understand the sensitivity of users to text-centric fake news attacks through neurocognitive methods.They studied the neural basis related to fake news and real news through electroencephalograms (EEG), designed and ran EEG experiments on human users and analyzed the neural activities related to fake news and real news detection tasks of different types of news articles.Finally, they found that human detection of fake news may be ineffective and have potentially adverse effects.
## (s31) Explainable Fake News Detection
(p31.0) With the rapid development and application of machine learning and artificial intelligence technology in various fields, it is very important to explain the results of the algorithm's output to the user.The interpretability of artificial intelligence means that people can understand the choices made by artificial intelligence models in their decisionmaking process, including the reasons, methods and content of decision making [138].Simply put, interpretability is the ability to turn artificial intelligence from a black box into a white box.At present, explainable artificial intelligence methods are applied to different fields in different industries, including biomedical, financial applications, video payment, and media industries.The core of explainable artificial intelligence is to obtain human trust.From this, we can see that there are two important concepts that can explain artificial intelligence: trust and interpretation.For explainable artificial intelligence, the connotation of interpretation is that agents must communicate, exchange and run into different people repeatedly before they can gain human trust.Therefore, for the agent, when explaining, it is necessary to consider the different educational backgrounds, knowledge levels and other factors of the audience and then design the content and form of the explained information.

(p31.1) Figure 6 shows an interactive, explainable AI framework for human-machine communication.The main participants in the system are interpreters and interpretive audiences.Interpreters refer to artificial intelligence agents with many explainable AI methods that can make decisions based on specified tasks; the audience listens to the explanations given by the interpreters, who are generally the affected people involved in a task as well as decision makers and developers.The interpreter provides different forms of interpretation results to the interpretation audience according to different task scenarios; the interpreter, in turn, asks the interpreter questions so that the interpreter can make adjustments and optimizations.In this way, the interpreter will be more intelligent and put forward more convincing interpretation results.
## (s32) Explainable Fake News Detection
(p32.0) With the rapid development and application of machine learning and artificial intelligence technology in various fields, it is very important to explain the results of the algorithm's output to the user.The interpretability of artificial intelligence means that people can understand the choices made by artificial intelligence models in their decision-making process, including the reasons, methods and content of decision making [138].Simply put, interpretability is the ability to turn artificial intelligence from a black box into a white box.At present, explainable artificial intelligence methods are applied to different fields in different industries, including biomedical, financial applications, video payment, and media industries.The core of explainable artificial intelligence is to obtain human trust.From this, we can see that there are two important concepts that can explain artificial intelligence: trust and interpretation.For explainable artificial intelligence, the connotation of interpretation is that agents must communicate, exchange and run into different people repeatedly before they can gain human trust.Therefore, for the agent, when explaining, it is necessary to consider the different educational backgrounds, knowledge levels and other factors of the audience and then design the content and form of the explained information.

(p32.1) Figure 6 shows an interactive, explainable AI framework for human-machine communication.The main participants in the system are interpreters and interpretive audiences.Interpreters refer to artificial intelligence agents with many explainable AI methods that can make decisions based on specified tasks; the audience listens to the explanations given by the interpreters, who are generally the affected people involved in a task as well as decision makers and developers.The interpreter provides different forms of interpretation results to the interpretation audience according to different task scenarios; the interpreter, in turn, asks the interpreter questions so that the interpreter can make adjustments and optimizations.In this way, the interpreter will be more intelligent and put forward more convincing interpretation results.
