# A Review on Machine Learning Styles in Computer Vision-Techniques and Future Directions

CorpusID: 252558821 - [https://www.semanticscholar.org/paper/59694d8b594ac2bb0f91dd4a0d681133f976939a](https://www.semanticscholar.org/paper/59694d8b594ac2bb0f91dd4a0d681133f976939a)

Fields: Computer Science

## (s8) 1) SUPERVISED LEARNING
(p8.0) A machine learning task called supervised learning converts every input item to the required class label value. An object is mapped by the computer with the intended output after training. It includes a broad selection of algorithms for various supervised learning issues. Over time, applications in computer vision and machine learning have increased dramatically, with society as the only gainer. Supervised learning VOLUME 10, 2022 is broadly divided into two categories, i.e., Classification and Regression. Objects will be categorized based on recognized class categories in classification to solve various real-world challenges. In Regression, however, the correlation between dependent and independent variables is calculated and displayed using scatter plots [6]. Figure 6 depicts the Supervised Learning process flow, where the input is labeled data from which features are extracted, and the model is trained. The trained model will be applied to the test dataset to forecast the result.
## (s9) Advantages of Active Learning-
(p9.0) • To minimize the need for labeling issues such as image annotation, recognition, object detection, segmentation, and posture estimation.   algorithms that learn from other machine learning algorithms. Usually refers to the use of machine learning algorithms capable of combining predictions from different machine learning algorithms in the most effective way possible. Multitask learning algorithms capable of learning across several related prediction tasks are also referred to as meta-learning. There is meta-learning within the framework of supervised learning [41]. Figure 10 shows the work flow of meta learning style.
## (s14) 2) UNSUPERVISED LEARNING
(p14.0) Unsupervised learning majorly works on unlabelled data objects. This type of learning is frequently employed for feature extraction, spotting important patterns and structures, matching together related objects, and practical purposes [51]. Anomaly detection, clustering, density estimation, feature learning, dimensionality reduction, and association rule discovery are some of the most popular unsupervised learning tasks. Figure 12 shows the workflow of an unsupervised learning process for computer vision applications.
## (s15) a: SELF-SUPERVISED LEARNING
(p15.0) In some ways, self-supervised learning is a sort of unsupervised learning because it adheres to the condition that no labels are assigned. Self-supervised learning, on the other hand, instead of looking for high-level patterns for clustering, tries to tackle tasks typically addressed by supervised learning (e.g., image classification) without any labeling provided. Figure 13 displays the working of self-supervised learning from input data till the final output generation. Instead of recommending new self-supervised learning techniques, this learning aims to examine how current selfsupervised learning strategies might be applied to address domain adaption problems [53]. The primary task can learn a domain invariant feature representation thanks to the pretext  job connecting the source and destination domains. In the source domain, the primary job has labels; however, in the destination domain, there is no labeling requirement. In other words, we develop unsupervised domain adaptation through self-supervised learning. The forwarded data flow is represented by solid lines in the diagram, while the optional data flow is indicated by dotted lines [53]. Through multitask learning, the pretext and main task (such as object identification, classification, or semantic segmentation) are simultaneously learned. Advantages of Self-Supervised learning-• The frequency of labeling needed may be reduced with the use of self-supervised learning.
## (s20) 3) REINFORCEMENT LEARNING
(p20.0) Using input from its actions and experiences, an agent is trained in an interactive environment to achieve this machine learning technique's reward and punishment mechanisms. The agent receives rewards for successful attempts and punishment for unsuccessful ones. The agent attempts to minimize inappropriate actions and maximize appropriate ones by learning from their experiences and activities [64]. When a series of decisions are required, reinforcement learning is used. The mathematical foundation of Markov decision processes is used in most reinforcement learning contexts. Reinforcement learning is utilized in computer vision applications for object detection, video analysis, gaming, and animation. Figure 16 shows the work flow of reinforcement learning process to achieve the reward.
## (s21) 4) HYBRID LEARNING STYLES a: SEMI-SUPERVISED LEARNING
(p21.0) These algorithms are trained on data that are both labeled and unlabeled. There is a lot of labeled data and a lot of unlabeled data. Figure 17 shows how semi-supervised learning works with labeled and unlabeled data.

(p21.1) The basic approach entails clustering similar data first. Using an unsupervised learning method and then applying it VOLUME 10, 2022  to existing data. The rest of the unlabeled data is labeled using the labeled information [58].
## (s22) c: ROBOT LEARNING
(p22.0) Robot learning is a field of study that combines machine learning and robotics. It investigates learning algorithms that allow a robot to learn new skills or adapt to its surroundings. Numerous analytical systems, such as robots, are integrated with visual sensors from which they know the status of their surroundings by solving matching computer vision challenges in multiple applications. These tasks' solutions are utilized to make decisions regarding possible future actions [78].
## (s23) B. ADVANCED LEARNING STYLES 1) TRANSFER LEARNING
(p23.0) The system's capacity to recognize and apply information and abilities acquired during previous tasks to new ones. There is a need for Transfer learning to minimize the model training time and usage of the resources to solve similar kinds of functions.

(p23.1) In this, if you train a simple classifier to predict whether an image contains a particular set of objects, you could use the same knowledge the model gained during its training to recognize different but related groups of new things [79].

(p23.2) As shown in Figure 19, transfer learning takes a pre-trained model and dataset as input. It works on data and trains the model on that data to perform the machine learning tasks. Then that trained model knowledge will be used to solve similar problems. There are two types of transfer learning: one is positive transfer learning, and another is negative transfer learning. In positive transfer learning, pre-trained models can improve the performance of new tasks and the accuracy of results generated. At the same time, the negative transfer is when the implementation of new tasks degrades due to the previously trained knowledge transfer of the model.

(p23.3) Transfer learning is used in various domains like Medical applications, Biometrics, transportation, recommendation systems, and urban computing applications like traffic monitoring, health care, social security, etc. Pre-training a neural network on the source domain is a way to transfer learning that is frequently employed. for instance, ImageNet, a library of over 14 million annotated pictures divided into more than 20000 categories, then fine-tune it using examples from the target domain.

(p23.4) Machine learning models that deal with natural language processing incorporate transfer learning. Examples include teaching a model to recognize various linguistic components or embedding pre-trained layers that comprehend certain terminology or dialects. To translate models into different languages, transfer learning is used. Models' features are developed and trained using the English language. Table 15. Summarizes the different strategies used in transfer learning. Despite having the same source and target domains, the source and target tasks are different. The algorithms take advantage of the inductive biases of the source domain to enhance the target job. In the case of transductive transfer learning, the related domains are different even though the source and target tasks are comparable. For unsupervised transfer learning, the main focus is on unsupervised tasks in the target domain where the source and target domains are similar, but the tasks are different. The reusable aspects of a computer vision algorithm will be applied to a new model through transfer learning in computer vision for image and video data processing. Deep learning, a kind of machine learning that aims to emulate and duplicate the processes of the human brain, is reliant on artificial neural networks. Due to the intricacy of the models, neural network training consumes a large number of resources. To increase process efficiency and decrease resource demand, transfer learning is applied.
## (s31) A. IMBALANCED DATA
(p31.0) If a different number of images for each of the classes is existing in the input dataset. This problem is called as class imbalance. Similarly, if a set of images is not evenly distributed in the input dataset is called imbalanced data. Transfer learning, Multi-task learning and Federated learning help to overcome this unbalanced distribution of data problem. As in case of transfer learning once the model has been trained on sample dataset can be applied to solve the similar problems with the same model. In case of the Multi-task learning model can be trained with a small number of dataset. The same knowledge generated can be applied to solve all related tasks. From supervised learning Logistic regression algorithm is very useful to tackle this issue as it resample's the original training dataset to decrease the overall level of class imbalance. The authors proposed a monitoring scheme that can infer the composition of training data for each Federated Learning(FL) round, and design a new loss function -Ratio Loss to mitigate the impact of the imbalance [110].
