# A Theoretical Overview of Neural Contraction Metrics for Learning-based Control with Guaranteed Stability

CorpusID: 238259445
 
tags: #Engineering, #Mathematics, #Computer_Science

URL: [https://www.semanticscholar.org/paper/64e20af37a6bcb23932222493dc2f19c1aacaddc](https://www.semanticscholar.org/paper/64e20af37a6bcb23932222493dc2f19c1aacaddc)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

A Theoretical Overview of Neural Contraction Metrics for Learning-based Control with Guaranteed Stability
2021

Hiroyasu Tsukamoto 
Soon-Jo Chung 
Jean-Jacques Slotine 
Chuchu Fan 
A Theoretical Overview of Neural Contraction Metrics for Learning-based Control with Guaranteed Stability

IEEE CONFERENCE ON DECISION AND CONTROL (CDC), PREPRINT VERSION. ACCEPTED JULY
12021
This paper presents a theoretical overview of a Neural Contraction Metric (NCM): a neural network model of an optimal contraction metric and corresponding differential Lyapunov function, the existence of which is a necessary and sufficient condition for incremental exponential stability of nonautonomous nonlinear system trajectories. Its innovation lies in providing formal robustness guarantees for learning-based control frameworks, utilizing contraction theory as an analytical tool to study the nonlinear stability of learned systems via convex optimization. In particular, we rigorously show in this paper that, by regarding modeling errors of the learning schemes as external disturbances, the NCM control is capable of obtaining an explicit bound on the distance between a time-varying target trajectory and perturbed solution trajectories, which exponentially decreases with time even under the presence of deterministic and stochastic perturbation. These useful features permit simultaneous synthesis of a contraction metric and associated control law by a neural network, thereby enabling real-time computable and probably robust learning-based control for general control-affine nonlinear systems.

## I. INTRODUCTION

Since the development of reinforcement learning and neural networks [1], [2], leveraging Artificial Intelligence (AI) and Machine Learning (ML) methods in the field of systems and control theory has been an active field of research. Although successful, conventional black-box AI and ML approaches often lack formal robustness and stability guarantees, which are essential in implementing safety-critical control schemes for, e.g., robotic and aerospace systems.

Related Work: Contraction theory is one of the most powerful mathematical tools for providing such guarantees for learning-based control frameworks [3], [4]. Generalizing Lyapunov theory, it studies differential dynamics of a nonlinear system using a Riemannian contraction metric and its uniformly positive definite matrix, thereby designing a differential Lyapunov function to characterize a necessary and sufficient condition for exponential convergence of any couple of the system trajectories. Due to the differential nature of contraction theory which allows utilizing LTV system-type approaches [4]- [8], its nonlinear stability analysis reduces to the problem of finding a contraction metric that satisfies a Linear Matrix Inequality (LMI) [9].

Obtaining contraction metrics analytically is challenging for general nonlinear systems, and thus several techniques have been proposed to find them numerically using the LMI property. In particular, it is proposed in [10]  systems can be expressed as convex constraints, enabling systematic synthesis of a differential feedback control. Such an idea is further extended to develop the method of ConVex optimization-based Steady-state Tracking Error Minimization (CV-STEM) [4]- [7], which computes a contraction metric and its associated feedback control and state estimation laws via convex optimization for optimal deterministic/stochastic disturbance attenuation. A critical limitation of these optimization-based schemes is their computational complexity. In essence, a Neural Contraction Metric (NCM) and its extensions [4], [6]- [8], [11], [12] are developed to model the CV-STEM contraction metric using a Deep Neural Network (DNN), achieving real-time implementable control, estimation, and motion planning of nonlinear systems with guaranteed robustness and stability. We again remark that such formal guarantees are difficult to quantify for black-box AI and ML techniques without considering a contracting stability property.

Contribution: This paper presents rigorous proofs on the aforementioned theoretical guarantees of the NCM methods [4], [6]- [8], [11], [12], which provide an explicit exponential bound on the distance between a target trajectory and solution trajectories with the NCM learning-based control, even under the presence of learning errors and deterministic/stochastic perturbation. We also derive one explicit way to optimally sample NCM training data by formulating the CV-STEM, to design contracting differential feedback control that minimizes the steady-state of the computed bound for optimal disturbance attenuation. Furthermore, we provide an algorithmic overview on DNN training for simultaneous synthesis of the contraction metric and control, to see how the NCM robustness and stability guarantees corroborate the success of learning-based control.

Notation: For x ∈ R n and A ∈ R n×m , we let x , δ x, A , and A F denote the Euclidean norm, infinitesimal displacement at a fixed time, induced 2-norm, and Frobenius norm, respectively. For A ∈ R n×n , we use A 0, A 0, A ≺ 0, and A 0 for the positive definite, positive semi-definite, negative definite, negative semi-definite matrices, respectively, and sym(A) = (A + A )/2. Also, E denotes the expected value operator.

II. NEURAL CONTRACTION METRICS FOR LEARNING-BASED CONTROL [4], [6]- [8], [12] In this paper, we consider the following smooth nonautonomous (i.e. time-varying) nonlinear system, perturbed by deterministic disturbances d(x,t) with sup x,t d(x,t) =d and Gaussian white noise W (t) with G(x,t) F ≤ḡ:
dx =h(x, u,t)dt + d(x,t)dt + G(x,t)dW (t)(1)
where t ∈ R + , x : R + → R n is the system state, u ∈ R m is the system control input, h : R n × R m × R + → R n is a known smooth function, d : R n × R + → R n and G : R n × R + → R n×w are unknown bounded functions for external disturbances, W : R + → R w is a w-dimensional Wiener process, and we consider the case whered,ḡ ∈ [0, ∞) are given. Let us first introduce the following definition of contraction [3].

Definition 1. The system (1) is said to be contracting if there exist a uniformly positive definite matrix M(x,t) = Θ(x,t) T Θ(x,t) 0, ∀x,t and a feedback control law u = u * (x, x d , u d ,t) s.t. the following condition holds ∀x,t:
M + 2 sym M ∂ h(x, u * (x, x d , u d ,t),t) ∂ x + Ξ(x,t) 0 (2)
where ∃Ξ(x,t) 0, ∀x,t, M = M(x,t), and x d is the target trajectory of (1) given as follows:
x d = h(x d , u d ,t) s.t. u d = u * (x d , x d , u d ,t).(3)
We call M and u * satisfying (2) a contraction metric and contracting control law, respectively.

It is shown that (2) is a necessary and sufficient condition for incremental exponential stability of the system trajectories with respect to each other [3], [4].


## A. Problem Formulation

Finding an optimal contraction metric M for general nonlinear systems is challenging and could involve solving optimization problems at each time instant [5], [13] (see Sec. III), which is not suitable for systems with limited computational capacity. A Neural Contraction Metric (NCM) and its extensions [4], [6]- [8], [12] have been proposed to approximate a contracting control law u * , i.e., a computationally-expensive control law parameterized by the contraction metric given by (2), utilizing a Deep Neural Network (DNN).


## Definition 2. A Neural Contraction Metric (NCM) is a DNN model of a contraction metric given by Definition 1.

We consider the problem of modeling the contracting control law u * by a learning-based control law u L (x, x d , u d ,t) parameterized by the NCM of Definition 2 (see [5], [10] and Sec. III for how to obtain u * ). Let x be the trajectory of (1) controlled by u = u L , and x d be the target trajectory of (3). Since we have h(x, u L ,t) = h(x, u * ,t)+(h(x, u L ,t)−h(x, u * ,t)), a virtual system of q, which has q = x and q = x d as its particular solutions, is given as follows:
dq = h(q, u * (q, x d , u d ,t),t)dt + d q (q,t)dt + G q (q,t)dW(4)
where d q and G q are parameterized linearly to verify
d q (q = x d ,t) = 0, d q (q = x,t) = h(x, u L (x, x d , u d ,t),t) − h(x, u * (x, x d , u d ,t),t) + d(x,t), G q (q = x d ,t) = 0, and G q (q = x,t) = G(x,t).
Assumption 1. In this section, we assume the following.

• For all (x, x d , u d ,t) ∈ S, u L is learned to satisfy
u L (x, x d , u d ,t) − u * (x, x d , u d ,t) ≤ ε 0 + ε 1 x x d δ q (5)
where S = S x × S x × S u × S t , S x ⊂ R n , S u ⊂ R m , and S t ⊂ R + are some given compact sets, and x x d δ q is a path integral from x d to x parameterized by q of (4).
• h of (1) is Lipschitz with respect to u, i.e., ∃L u ∈ [0, ∞) s.t. h(x, u,t) − h(x, u ,t) ≤ L u u − u , ∀u, u ∈ R m (6)
for all x ∈ S x and t ∈ S t .
• Let S xt = S x × S t . When G = 0 in (1), ∂ M/∂ x i of M in (2) is Lipschitz with respect to x, i.e., ∃L m ∈ [0, ∞) s.t. ∂ M ∂ x i (x,t) − ∂ M ∂ x i (x ,t) ≤ L m x − x , ∀(x,t), (x ,t) ∈ S xt .(7)
Remark 1. (5) can be empirically guaranteed, e.g., by DNNs, which have been shown to generalize well to the set of unseen events that are from almost the same distribution as their training set [14], [15], and consequently, obtaining a tighter and more general bound for the learning error has been an active field of research [16]. The condition (5) has thus become a common assumption in analyzing the performance of learning-based control [4], [8], [12], [15].


## B. Nonlinear Robustness and Stability Analysis

The following theorem rigorously studies robustness and stability of the NCM-parameterized control law u L . Theorem 1. Suppose that Assumption 1 holds. Suppose also that the contraction metric M(x,t) of (2) is bounded as
mI M(x,t) mI, ∀x,t(8)
for ∃m, m ∈ (0, ∞), and u * of (2) is given with Ξ defined as
Ξ(x,t) = 2αM(x,t) + α s I (9)
where ∃α ∈ (0, ∞) is the contraction rate and α s = L mḡ 2 (α G + 1/2) with an arbitrary constant α G ∈ (0, ∞) (see (14)). If the learning error ε 1 of (5) and another arbitrary constant α d ∈ (0, ∞) (see (15)) are selected to satisfy
∃α ∈ (0, ∞) s.t. α = α − (α d /2 + L u ε 1 m/m) > 0(10)
then we have the following bound for all (x,t) ∈ S x × S t :
E x(t) − x d (t) 2 ≤ C 2α m m + E[Vs (0)]e −2α t m (11) where V s (t) = x x d δ q(t) M(q(t),t)δ q(t), C =ḡ 2 (2α G −1 + 1) + (L u ε 0 +d) 2 α −1 d ,d,ḡ
, and q are given in (1) and (4), x is the trajectory of (1) controlled by u = u L , and x d is the target trajectory given by (3).
Proof. Since ∂ M/∂ x i is Lipschitz as in (7), we have ∂ 2 M/(∂ x i ∂ x j ) ≤ L m and ∂ M/∂ x i ≤ √
2L m m using (8) as derived in [7]. Also, d q of (4) is bounded as
d q (x,t) ≤ L u (ε 0 + ε 1 x x d δ q ) +d(12)
by the Lipschitz condition (6) and learning error bound (5).
Let L be the infinitesimal differential generator in [17, p. 15]. Since we have x x d δ q ≤ V / √ m and V 2 ≤ V s [4] for V = x
x d Θδ q and M = Θ Θ, computing L V s using these bounds as in [5] yields
L V s ≤ x x d δ q (Ṁ + 2 sym(Mh x ))δ q + 2L u ε 1 m/mV s + 2 √ md ε V +ḡ 2 (m + x x d L m δ q 2 /2 + 2 2L m m δ q ) (13)
where h x = ∂ h/∂ x and d ε = L u ε 0 +d. Using the relation 2ab ≤ c −1 a 2 + cb 2 which holds for any a, b ∈ R and c ∈ (0, ∞), we have that
2 √ md ε Θδ q ≤α d Θδ q 2 + md 2 ε α −1 d (14) 2 2L m m δ q ≤L m α G δ q 2 + 2mα −1 G(15)
for any α d , α G ∈ (0, ∞). Substituting (14) and (15) into (13) along with the contraction condition (2) for u * yields (9), we have L V s ≤ −2α V s + mC due to the condition (10). This results in the desired relation (11) due to Theorem 1 of [5].
L V s ≤ x x d δ q (−Ξ + (α d + 2L u ε 1 √ χ)M + α s I)δ q + mC where χ = m/m, α s = L mḡ 2 (α G + 1/2), and C =ḡ 2 (2α G −1 + 1) +d 2 ε α −1 d . Thus, by definition of Ξ in
Theorem 1 states that, as long as we have a contracting controller u * which satisfies (2) of Definition 1, the NCM controller u L of (5) ensures that the mean-squared distance from any target trajectory x d controlled by
u d = u * (x d , x d , u d ,t)
to the one controlled by u L with deterministic and stochastic perturbation is exponentially bounded in time, even under the presence of the learning error ε 0 and ε 1 . Such an explicit exponential bound is difficult to obtain for systems without a contraction property.

In particular, when there is no stochastic disturbance in the system (1), i.e., G = 0, we have the following result. (5) and (6) of Assumption 1 hold for u * that satisfies (2) with Ξ = 2αM(x,t), where ∃α ∈ (0, ∞) is the contraction rate. Now let x d be the target trajectory of (3), and x be the trajectory controlled by u L with G = 0. If M is bounded as in (8), and if the learning error ε 1 of (5) is sufficiently small to satisfy


## Theorem 2. Suppose
∃α ∈ (0, ∞) s.t. α = α − L u ε 1 m/m > 0(16)
then we have the following bound:
x(t) − x d (t) ≤ V (0) √ m e −α t + L u ε 0 +d α m m (1 − e −α t )(17)
where V (t) = x x d Θ(q(t),t)δ q(t) and q is given by (4) with the same d q and G q = 0 (note that this virtual system indeed has q = x and q = x d as its particular solutions).

Proof. Let V = δ q Mδ q. Using (13) computed in the proof of Theorem 1, along with the bound on d q (12) and the contraction condition (2) for Ξ = 2αM, we have thaṫ
V ≤ −2αV + 2 √ m(L u (ε 0 + ε 1 V / √ m) +d) Θδ q(18)
where the relation x Although Theorems 1 and 2 are already useful for systems with a known contraction metric and corresponding differential Lyapunov function, finding them for general nonlinear systems is not always straightforward. This section delineates one approach, called ConVex optimization-based Steady-state Tracking Error Minimization (CV-STEM), to optimally design u * and M of Theorem 1 for control-affine nonlinear systems, thereby demonstrating how to construct the NCM of Definition 2 in practice for probably stable learning-based robust control.
x d δ q ≤ V / √ m is used. Since we havė V = Θδ q (d Θδ q /dt) and V = Θδ q 2 , (18) implies thatV ≤ −(α − L u ε 1 m/m)V + √ m(L u ε 0 +d),
Let us consider the following control-affine nonlinear system:
dx =( f (x,t) + B(x,t)u)dt + d(x,t)dt + G(x,t)dW (t)(19)
where t, x, u, d, G, and W are as defined in (1), and f : R n × R + → R n and B : R n × R + → R n×m are known smooth functions. (1) and (19), we can still apply the technique to be discussed in this section even for control non-affine systems (1), by treating this modeling error as external disturbance d in (19). For situations where such an assumption is not valid, we could apply (learning-based) adaptive control [8], [15], [19], [20] to estimate the non-affine part of the dynamics.
Remark 2. If h(x, u,t) − ( f (x,t) + B(x,t)u) is bounded in

## A. Contraction Theory-based Robust Control (CV-STEM)

We design a contracting control law u = u * for (19) by the following differential feedback control law [10] (see Remark 3 for more options):
u * (x, x d , u d ,t) = u d − x x d R(q,t) −1 B(q,t) M(q,t)δ q(20)
where R(x,t) 0 is a given weight matrix, M(x,t) 0 is a contraction metric to be defined in Theorem 3, q is a smooth path that connects x to x d as in (4), and x d is a given target trajectory of (3) 
with h(x, u,t) = f (x,t) + B(x,t)u. Note that (20) yields δ u * = −R(x,t) −1 B(x,t) M(x,t)δ x.
Remark 3. We could utilize other types of control laws in (20), e.g., δ u = k(x, δ x, u,t) (see Sec. IV) [10] [8]. See [4] for their trade-offs.
or u = u d − K(x, x d , u d ,t)(x − x d ) [5]-
The CV-STEM is for designing u * and M of Theorem 1 explicitly by (20) to minimize the steady-state bound of (11).

Theorem 3. Suppose that the contraction metric M(x,t) = W (x,t) −1 0 of (20) is designed by the following convex optimization (CV-STEM) [4]- [7], which minimizes the steadystate upper bound of (11):
J * CV = min ν>0,χ∈R,W 0 C 2α χ s.t. (22), (23), and (24)(21)
with the convex constraints (22), (23), and (24) given as
H(ν,W ) + 2αWW W να −1 s I 0, ∀x,t (22) − ∂ b iW + 2 sym ∂ b i ∂ xW = 0, ∀i = 1, · · · , m, ∀x,t(23)I W (x,t) χI, ∀x,t(24)
where ∂ p F = ∑ n k=1 (∂ F/∂ x k )p k for p(x,t) ∈ R n and F(x,t) ∈ R n×n , ν = m, χ = m/m,W = νW , f and B are given in (19), b i is the ith column of B, and
H(ν,W ) = − ∂W ∂t − ∂ fW + 2 sym ∂ f ∂ xW − νBR −1 B .
with the other variables defined in Theorem 1. If u * is designed as (20) for M of (21), the contraction condition (2) of Definition 1 and (8) of Theorem 1 hold with Ξ given by (9), h(x, u,t) = f (x,t) + B(x,t)u, and u = u * .

Proof. Applying Schur's complement lemma [9, pp. 7] to the constraint (22), we have that
H(ν,W ) + 2αW − α sW 2 /ν 0.(25)
It can be easily verified that, by multiplying the constraints (25), (23), and (24) by M = W −1 from both sides and by ν −1 , they can be equivalently rewritten as [5] ∂ M ∂t
+ ∂ f M + 2 sym (M(∂ f /∂ x)) − MBR −1 B M −Ξ (26) ∂ b i M + 2 sym (M(∂ b i /∂ x)) = 0 (27) mI M mI(28)
respectively, for Ξ of (9), where (28) indicates that (24) is indeed equivalent to (8). Combining (26) and (27), we geṫ M + 2 sym (M(∂ ( f (x,t) + B(x,t)u * )/∂ x)) −Ξ which implies the contraction condition (2) holds for h(x, u,t) = f (x,t) + B(x,t)u and u = u * . Furthermore, it can be seen that the problem (21) minimizes the steady-state upper bound of (11) due to the relation lim t→∞ E x(t) − x d (t) 2 = (C/(2α ))(m/m) = (C/(2α ))χ. Note that (21) 


## B. NCM Construction using CV-STEM

Now that we can construct the contracting control law u * of Theorem 1 by using Theorem 3, we are ready to explicitly design the NCM of Definition 2. Since the NCM only requires one DNN evaluation at each time instant without solving any optimization problems unlike the CV-STEM of Theorem 3, it enables real-time computation of optimal feedback control using (20) in most engineering and science applications. We formally derive its robustness and stability guarantees in the following theorems.

Theorem 4. Let u L be a learning-based controller, prameterized by the NCM of Definition 2, which approximates the contracting control law u * of (20). If Assumption 1 holds for such (see [4], [6] for state estimation).

u L , u * , and M of Theorem 3 with h(x, u,t) = f (x,t)+B(x,t)u of (19), then Theorem 1 holds, i.e., the system trajectory of (19) controlled by u L exponentially converges to a ball around the target trajectory x d of (3) with h(x, u,t) = f (x,t) + B(x,t)u.

Proof. Since u * of (20) with M of (21) guarantees the assumptions of Theorem 1 due to Theorem 3, the exponential bound (11) holds as long as Assumption 1 is satisfied.

Although Theorem 4 considers the modeling error on the learned control law, u L − u * , it is also useful to study how the learning error on the contraction metric, M L − M , affects the NCM stability performance.

Theorem 5. Let M L be the NCM of Definition 2 that models M of Theorem 3. Suppose that (7) holds for such M, and that the system (19) is controlled by (20) with M replaced by M L . Suppose also that ∃b,ρ ∈ [0, ∞) s.t. B(x,t) ≤b and R −1 (x,t) ≤ρ, ∀(x,t) ∈ S x × S t , for B in (19) and R in (20), where S x and S t are given compact sets of (5). If the NCM satisfies M L (x,t) − M(x,t) ≤ ε , ∀(x,t) ∈ S x × S t for ∃ε ∈ [0, ∞), Theorem 1 holds for ε 0 = 0 and ε 1 =ρb 2 ε .

Proof. This follows from Theorem 4, due to the facts that the learning error of (5) is given as u L − u * ≤ρbε x x d δ q due to (20), and the Lipschitz constant of (6) is as L u =b.

Theorems 4 and 5 indeed guarantee robustness and incremental stability of the NCM learning-based control law u L , which imitates the contraction theory-based robust control u * of Theorem 3 for its real-time implementability.

IV. LEARNING CERTIFIED CONTROL USING CONTRACTION METRICS [11] This section delineates one framework to train a DNN for jointly synthesizing a contraction metric and contraction control law with the robustness and stability guarantees of Theorems 4 and 5, treating the contraction constraints (2) and (8) as the loss functions. Intuitively, this can be viewed as a way to solve (21) of Theorem 3 directly by a DNN, while also avoiding the integral evaluation required in (20).


## Remark 4.

Although here we consider the system (19) with G = 0, stochastic perturbation could be incorporated in a similar way using the constraints introduced in (22) and (23). For deterministic systems (19) with G = 0, it is shown in [10] that the following conditions weaker than (22) and (23) are sufficient for the existence of a contracting differential feedback controller, which satisfies the contraction condition (2) of Definition 1 with Ξ = 2αM = 2αW −1 :
B ⊥ − ∂W ∂t − ∂ f W + 2 sym ∂ f ∂ x W + 2αW B ⊥ ≺ 0 (29) B ⊥ ∂ b i W − 2 sym ∂ b i ∂ x W B ⊥ = 0, ∀ j, x,t(30)
where B ⊥ (x,t) is an annihilator matrix of B(x,t) satisfying B ⊥ B= 0, and the other notations and variables are as defined in Theorem 3. Thus, in this section, we consider the constraints (29) and (30), instead of (22) and (23), without specifying the form of the contracting control u * as in (20).


## A. Cost Functions in Neural Network Training

The purpose of this section is to find a DNN-based controller u L (x, x d , u d ,t; θ u ) that models a contracting control law u * and dual contraction metric W L designed as
W L (x,t; θ w ) = Θ L (x,t; θ ϑ ) Θ L (x,t; θ ϑ ) + m −1 L I(31)
that models W = M −1 of Theorem 3, where θ u and θ w = {θ ϑ , m L } with m L > 0 are hyper-parameters. Note that by definition of W L in (31), it is already a positive definite matrix satisfying W L (x,t; θ w ) m −1 L I as in (8) or (24). Let C u (x, x d , u d ,t; θ w , θ u ) be the left-hand side of (2) with W (= M −1 ) = W L , u * = u L , h(x, u,t) = f (x,t) + B(x,t)u, and Ξ = 2αM, and ρ(S) be the uniform distribution over the compact set of (5). We define the contraction loss as follows:
L u (θ w , θ u ) = E (x,x d ,u d ,t)∼ρ(S) L PD (−C u (x, x d , u d ,t; θ w , θ u )) (32)
where L PD (·) ≥ 0 is a function for penalizing non-positive definiteness, i.e., L PD (A) = 0 if and only if A 0. Also, since we already have W L m −1 L I by (31), the condition (8) on the boundedness of W L can be considered by the following loss function with another hyper-parameter m L in θ w :
L c (θ w ) = E (x,x d ,u d ,t)∼ρ(S) L PD (m −1 L I −W L (x,t; θ w )).(33)
Furthermore, we utilize the weaker contraction conditions (29) and (30) also as the following loss functions to provide more guidance for DNN optimization, as simultaneously learning u L and W L by minimizing L u and L c solely is challenging as demonstrated in [11]:
L w1 (θ w ) = E (x,x d ,u d ,t)∼ρ(S) L PD (−C 1 (x,t; θ w )) L w2 (θ w ) = m ∑ j=1 E (x,x d ,u d ,t)∼ρ(S) C i 2 (x,t; θ w ) F(34)
where C 1 (x,t; θ w ) and {C j 2 (x,t; θ w )} m j=1 are the left-hand side of (29) and (30) with W = W L , respectively.

Approximating the expectations of (32)-(34) using sampled data, the empirical loss function for the NCM training is defined as follows:
L (θ u , θ w ) = 1 N N ∑ i=1 L m,i (θ u , θ w ) + L w,i (θ w ) (35) L m,i = L PD (−C u (ξ i ; θ w , θ u )) + L PD (m −1 L I −W (x i ,t i ; θ w )) L w,i = L PD (−C 1 (x i ,t i ; θ w )) + m ∑ j=1 C j 2 (x i ,t i ; θ w )) F where the training samples {ξ i = (x i , x d,i , u d,i ,t i )} N i=1
are drawn independently from ρ(S), and L PD is computed as L PD (A) = 1 K ∑ K i=1 min{0, −p i Ap i } for a given matrix A ∈ R n×n and randomly sampled K points in the set {p i ∈ R n | p i = 1} K i=1 . The robustness and stability guarantee of u L is given in the following theorem. Theorem 6. Suppose Assumption 1 holds for the DNN-based NCM controller u L learned by minimizing the loss function (35). If (19) with G = 0 is controlled by u L , (17) of Theorem 2 holds, i.e., the distance between the target trajectory x d and perturbed trajectories x is exponentially bounded.

Proof. This follows from the proof of Theorem 2 as long as u L is learned to satisfy (5) with the loss function (35). 


## B. Numerical Simulation

We briefly summarize the simulation results presented in [11] to see how the theoretical guarantees in Theorems 1-6 lead to exponential stabilization of nonlinear systems in practice (more simulation results can be found also in [4], [6]- [8], [12]). The loss function (35) is used for DNN training, and we consider (a) PVTOL: Planar Vertical-Takeoff-Vertical-Landing (PVTOL) system for drones [21], [22]; (b) Quadrotor: Physical quadrotor system [21]; (c) Neural lander: Drone flying close to the ground, where the ground effect is nonnegligible [15] (the ground effect is learned from empirical data using a 4-layer neural network); and (d) SEGWAY: Realworld Segway robot [23].

The performance is compared with the Sum of Squares (SoS)-based method [21], Model Predictive Control (MPC) [24], [25], and proximal policy optimization reinforcement learning (RL) algorithm [26].

1) DNN Training: For Θ L of the dual metric in (31), we use a 2-layer neural network with 128 neurons. For u L of Theorem 6, we use u L (x, x d , u d ,t; θ u ) = w 2 · tanh(w 1 · (x − x d )) + u d where w 1 = w 1 (x, x d ; θ u1 ) and w 2 = w 2 (x, x d ; θ u2 ) are two 2layer neural networks with 128 neurons with the hyperparameters θ u = {θ u1 , θ u2 }. We train the networks for 20 epochs with a training set with 130K samples. See [6], [7], [11], [12] for more details on the NCM dataset generation. 2) Target Trajectory Generation: The target control inputs u d (t) are generated as the linear combination of the elements in a set of sinusoidal signals with some fixed frequencies and randomly sampled a weight for each frequency component. The initial states of the target trajectories x d (0) are uniformly randomly sampled from a compact set, and then x d are obtained by integrating (3). The initial errors e(0) are uniformly randomly sampled from a compact set, and the initial states x(0) are computed as x(0) = x d (0) + e(0).

3) Simulation Results and Discussions [11]: Figure 2 shows the normalized tracking error defined as x e (t) = ||x(t) − x d (t)||/||x(0) − x d (0)|| for each model and method. For PV-TOL and the quadrotor models, both SoS and our learningbased method find a contracting tracking controller that renders x e decrease rapidly, successfully achieving exponential stabilization. While the SoS of [21] cannot solve for the Neural lander and SEGWAY examples due to their complex and nonpolynomial dynamics, and MPC produces large numerical errors for the SEGWAY example due to the sensitivity of the SEGWAY model, the proposed control of Theorem 6 yields consistently higher tracking performance for both of these models. The performance of the RL method varies with tasks: it achieves comparable results for PVTOL and the neural lander models but fails to find a contracting tracking controller within a reasonable time for the quadrotor and Segway models, as their state spaces are larger than the previous two cases. Figure 2 thus implies that our proposed method outperforms both the learning and non-learning-based methods, producing a contracting controller with smaller tracking errors. These errors are guaranteed to be bounded exponentially with time even with the learning error and external disturbances, as rigorously proven in Theorems 1-6. Again, more simulation results can be found in [4], [6]- [8], [12].


## V. CONCLUSION

In this paper, we presented a theoretical overview of the NCM for provably stable, robust, and optimal learning-based control of non-autonomous nonlinear systems. It is formally shown that, even under the presence of learning errors and deterministic/stochastic disturbances, the NCM control ensures the distance between a target trajectory and learned system trajectories to be bounded exponentially with time, enabling safe and robust use of AL and ML-based automatic control frameworks in real-world scenarios. The numerical simulation results of [11] are presented to validate the effectiveness of the NCM framework.


is convex as the objective is affine in χ, and (22), (23), and (24) are linear matrix inequalities in terms of ν, χ, andW [9, pp. 7].

## Fig. 1 .
1Illustration of NCM (x: system state; M: positive definite matrix that defines optimal contraction metric; x i and M i : sampled x and M;x: estimated system state; y: measurement; and u: system control input). Note that the target trajectory (x d , u d ) is omitted in the figure fore simplicity.

## Remark 5 .
5The objective function (21) of the CV-STEM method in Theorem 3 can also be considered in (35) as a loss function L CV (θ w ) = (C/2α )(m L /m L ), where m L and m L are given in (31) and (33). Using it in (35) would augment u L of Theorem 6 with some CV-STEM-type optimality property.

## Fig. 2 .
2Normalized tracking error on the benchmarks using different approaches. The y axes are in log-scale. The tubes are tracking errors between mean plus and minus one standard deviation over 100 trajectories (C3M: Proposed approach in Theorem 6).


that sufficient conditions for incremental exponential stabilizability of nonlinear * Graduate Aerospace Laboratories, Caltech, Pasadena, CA, {htsukamoto, sjchung}@caltech.edu. † Nonlinear Systems Laboratory, MIT, Cambridge, MA, jjs@mit.edu. ‡ Reliable Autonomous System Laboratory, MIT, Cambridge, MA, chuchu@mit.edu.


which results in (17) by applying the comparison lemma [18, pp.102-103, pp.350-353] as long as the condition (16) holds.III. A CONVEX OPTIMIZATION APPROACH FOR NCM 
ROBUST CONTROL [4]-[8] 



Reinforcement Learning: An Introduction. R S Sutton, A G Barto, MIT PressR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. MIT Press, 1998.

Neuro-Dynamic Programming. D P Bertsekas, J N Tsitsiklis, Athena Scientific. D. P. Bertsekas and J. N. Tsitsiklis, Neuro-Dynamic Programming. Athena Scientific, 1996.

On contraction analysis for nonlinear systems. W Lohmiller, J.-J E Slotine, Automatica. 6W. Lohmiller and J.-J. E. Slotine, "On contraction analysis for nonlinear systems," Automatica, no. 6, pp. 683 -696, 1998.

Contraction theory for nonlinear stability analysis and learning-based control: A tutorial overview. H Tsukamoto, S.-J Chung, J.-J E Slotine, Annu. Rev. Control, minor revision. H. Tsukamoto, S.-J. Chung, and J.-J. E. Slotine, "Contraction theory for nonlinear stability analysis and learning-based control: A tutorial overview," Annu. Rev. Control, minor revision, 2021.

Robust controller design for stochastic nonlinear systems via convex optimization. H Tsukamoto, S.-J Chung, IEEE Trans. Autom. Control. 6610H. Tsukamoto and S.-J. Chung, "Robust controller design for stochastic nonlinear systems via convex optimization," IEEE Trans. Autom. Con- trol, vol. 66, no. 10, pp. 4731-4746, 2021.

Neural contraction metrics for robust estimation and control: A convex optimization approach. H Tsukamoto, S.-J Chung, IEEE Control Syst. Lett. 51H. Tsukamoto and S.-J. Chung, "Neural contraction metrics for robust estimation and control: A convex optimization approach," IEEE Control Syst. Lett., vol. 5, no. 1, pp. 211-216, 2021.

Neural stochastic contraction metrics for learning-based control and estimation. H Tsukamoto, S.-J Chung, J.-J E Slotine, IEEE Control Syst. Lett. 55H. Tsukamoto, S.-J. Chung, and J.-J. E. Slotine, "Neural stochastic contraction metrics for learning-based control and estimation," IEEE Control Syst. Lett., vol. 5, no. 5, pp. 1825-1830, 2021.

Learning-based adaptive control via contraction theory. H Tsukamoto, S.-J Chung, J.-J Slotine, IEEE CDC. H. Tsukamoto, S.-J. Chung, and J.-J. Slotine, "Learning-based adaptive control via contraction theory," in IEEE CDC, Dec. 2021.

Linear Matrix Inequalities in System and Control Theory, ser. S Boyd, L El Ghaoui, E Feron, V Balakrishnan, Studies in Applied Mathematics. 15S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan, Linear Matrix Inequalities in System and Control Theory, ser. Studies in Applied Mathematics. Philadelphia, PA: SIAM, Jun. 1994, vol. 15.

Control contraction metrics: Convex and intrinsic criteria for nonlinear feedback design. I R Manchester, J.-J E Slotine, IEEE Trans. Autom. Control. 626I. R. Manchester and J.-J. E. Slotine, "Control contraction metrics: Convex and intrinsic criteria for nonlinear feedback design," IEEE Trans. Autom. Control, vol. 62, no. 6, pp. 3046-3053, Jun. 2017.

Learning certified control using contraction metric. D Sun, S Jha, C Fan, CoRL. D. Sun, S. Jha, and C. Fan, "Learning certified control using contraction metric," in CoRL, Nov. 2020.

Learning-based robust motion planning with guaranteed stability: A contraction theory approach. H Tsukamoto, S.-J Chung, IEEE Robot. Automat. Lett. 64H. Tsukamoto and S.-J. Chung, "Learning-based robust motion planning with guaranteed stability: A contraction theory approach," IEEE Robot. Automat. Lett., vol. 6, no. 4, pp. 6164-6171, 2021.

Observer design for stochastic nonlinear systems via contraction-based incremental stability. A P Dani, S.-J Chung, S Hutchinson, IEEE Trans. Autom. Control. 603A. P. Dani, S.-J. Chung, and S. Hutchinson, "Observer design for stochastic nonlinear systems via contraction-based incremental stability," IEEE Trans. Autom. Control, vol. 60, no. 3, pp. 700-714, 2015.

Understanding deep learning requires rethinking generalization. C Zhang, S Bengio, M Hardt, B Recht, O Vinyals, Int. Conf. Learn. Represent. C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, "Understand- ing deep learning requires rethinking generalization," in Int. Conf. Learn. Represent., Toulon, France, Apr. 2017.

Neural lander: Stable drone landing control using learned dynamics. G Shi, IEEE ICRA. G. Shi et al., "Neural lander: Stable drone landing control using learned dynamics," in IEEE ICRA, May 2019.

Spectrally-normalized margin bounds for neural networks. P L Bartlett, D J Foster, M J Telgarsky, Adv. Neural Inf. Process. Syst. 30P. L. Bartlett, D. J. Foster, and M. J. Telgarsky, "Spectrally-normalized margin bounds for neural networks," in Adv. Neural Inf. Process. Syst., vol. 30, 2017.

Stochastic Stability and Control. H J Kushner, Academic PressNew YorkH. J. Kushner, Stochastic Stability and Control. Academic Press New York, 1967.

H K Khalil, Nonlinear Systems. Prentice-Hall3rd edH. K. Khalil, Nonlinear Systems, 3rd ed. Prentice-Hall, 2002.

Adaptive nonlinear control with contraction metrics. B T Lopez, J E Slotine, IEEE Control Syst. Lett. 51B. T. Lopez and J. E. Slotine, "Adaptive nonlinear control with con- traction metrics," IEEE Control Syst. Lett., vol. 5, no. 1, pp. 205-210, 2021.

Universal adaptive control of nonlinear systems. B T Lopez, J.-J E Slotine, arXiv:2012.15815B. T. Lopez and J.-J. E. Slotine, "Universal adaptive control of nonlinear systems," arXiv:2012.15815, 2021.

Robust online motion planning via contraction theory and convex optimization. S Singh, A Majumdar, J.-J E Slotine, M Pavone, IEEE ICRA. S. Singh, A. Majumdar, J.-J. E. Slotine, and M. Pavone, "Robust online motion planning via contraction theory and convex optimization," in IEEE ICRA, May 2017, pp. 5883-5890.

Learning stabilizable nonlinear dynamics with contraction-based regularization. S Singh, S M Richards, V Sindhwani, J.-J E Slotine, M Pavone, https:/journals.sagepub.com/doi/abs/10.1177/0278364920949931Int. J. Robot. Res. S. Singh, S. M. Richards, V. Sindhwani, J.-J. E. Slotine, and M. Pavone, "Learning stabilizable nonlinear dynamics with contraction-based regu- larization," Int. J. Robot. Res., Aug. 2020.

Learning for safetycritical control with control barrier functions. A Taylor, A Singletary, Y Yue, A Ames, L4DC. A. Taylor, A. Singletary, Y. Yue, and A. Ames, "Learning for safety- critical control with control barrier functions," in L4DC, 2020, pp. 708- 717.

Differentiable MPC for end-to-end planning and control. B Amos, Adv. Neural Inf. Process. Syst. B. Amos et al., "Differentiable MPC for end-to-end planning and control," in Adv. Neural Inf. Process. Syst., 2018.

Control-limited differential dynamic programming. Y Tassa, N Mansard, E Todorov, IEEE ICRA. Y. Tassa, N. Mansard, and E. Todorov, "Control-limited differential dynamic programming," in IEEE ICRA, 2014, pp. 1168-1175.

J Schulman, arXiv:1707.06347Proximal policy optimization algorithms. J. Schulman et al., "Proximal policy optimization algorithms," arXiv:1707.06347, 2017.