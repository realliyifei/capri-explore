# Privacy-Enhancing Technologies in Federated Learning for the Internet of Healthcare Things: A Survey

CorpusID: 257767458 - [https://www.semanticscholar.org/paper/d135ec27b06266dcc013431a7778fc37e5e79f70](https://www.semanticscholar.org/paper/d135ec27b06266dcc013431a7778fc37e5e79f70)

Fields: Medicine, Computer Science

## (s12) C. Perturbation Techniques
Number of References: 7

(p12.0) A perturbation method is to protect private data and model privacy by adding random noise to the original data or training data during the training process. The differential privacy technique is a widely used perturbation method implemented in the FL frameworks in medical applications. It is one of the PETs methods and guarantees privacy [87] using probability statistical models to mask sensitive private data in a dataset [88] and protect healthcare data against inference attack on FL frameworks. By adding noise to the model parameters or data, data can be deferentially private [89] [90], and the parties cannot realize whether an individual record participates in the learning process or not. Differential privacy techniques include two categories: global differential and local differential privacy techniques. In the global differential privacy (GDP) setting, there is a trusted curator that applies carefully random noise to the real values returned for a particular query [91]. Different from GDP, a local differential privacy (LDP) technique does not need a trusted third-party. In fact, LDP allows users to locally perturb the input data, and it often produces too noisy data, as noise is applied to achieve individual record privacy [92]. As an advantage, the differential privacy technique by adding random noise makes data sets more secure because an attacker cannot distinguish which information is true. Therefore, more noises that are added to the sensitive data have a direct relationship to how the data is hard for an attacker to recognize true information about individuals in the dataset [93].
## (s13) D. Blockchain Techniques
Number of References: 6

(p13.0) Blockchain is beneficial in many non-financial industries such as healthcare due to its cryptographic security, immutability, and accountability [98]. Researchers have recently started implementing blockchain technology to decentralize traditional data management systems. For instance, blockchainbased data management prevents security breaches and assure GDPR compliance [99]. Therefore, blockchain-based PETs solutions can be used in Medical IoT to safeguard individuals' rights over their personal data [100]. Accordingly, Blockchain is a promising technique to improve the security and scalability of the FL system. This technique has provided a high level of security in the domain of healthcare by integrating blockchain into a federated learning to maintain the trained parameters [101]. The blockchain-based system is effective for decentralized federated learning training without the need for any central server which can mitigate risks of single-point failures [102]. To provide IoHT data provenance, blockchain has shown great promise, and also provides permission control of the participants to enhance the security and privacy of parameters in federated learning. Blockchain has gained popularity for managing the trust and provenance of trustworthy federated nodes, their datasets, the accuracy of the models, and the immutability of the global model [103]. A blockchain method consists of public (permissionless), private and consortium (permissioned). A public blockchain system allows any client to participate in the decentralized process without the need for authorized permission. In a private and consortium system, only the client with authorized permission can be involved in the block validation and confirmation process.
## (s17) C. Perturbation Methods
Number of References: 4

(p17.0) Similar to [109], in [116], the authors proposed a bandwidth-efficient FL framework in IoHT environment. The framework ensures privacy for FL based on Differential Privacy (DP). They discovered that exchanging the model update from a huge amount of IoHT devices needs a significant bandwidth. Therefore, they proposed the FL-SIGN-DP scheme to reduce communication costs and enhance privacy. Participants in FL-SIGN-DP only transmit the updated model's sign to the aggregation server. They used the electronic health records of roughly a million patients to assess the performance of the proposed scheme with regard to the in-hospital mortality rate. The proposed scheme is compared with centralized learning, FL-SIGN without using standard FL, differential privacy, and differential privacy with standard FL. The results showed that the FL-SIGN-DP consumes less bandwidth and can guarantee privacy protection.

(p17.1) Islam et al. [117] proposed a FL model to analyze patients' genomic data and identify the risk of heart failure. To enhance the privacy-preserving of the patient private data sharing among collaborating healthcare organizations in FL framework, they applied differential privacy mechanisms through feature selection based on statistical methods to increase scalability and accuracy in a federated setting where data are vertically partitioned. They evaluated the performance of the proposed FL framework using the IQVIA dataset and BC-TCGA dataset 11 for predicting the causes of certain heart failure and the BC-TCGA dataset for cancer prediction to compare their proposed FL method. The result demonstrated that their proposed model obtains better accuracy with the highest privacy for the IQVIA and BC-TCGA datasets in a federated training setting.
## (s19) VIII. KEY CHALLENGES FOR FUTURE RESEARCH
Number of References: 2

(p19.0) While PETs in FL have many advantages and have been growing rapidly in recent years, some challenges cannot be ignored. Existing frameworks are still at an early stage and need improving methods to enhance data privacy. 15 https://paperswithcode.com/dataset/cc-19 1) Computation cost: One of the main challenges of FL is represented by privacy-enhancing to prevent data leakage. FL needs multiple iterations to achieve the final global model. Therefore, the number of training iterations has a direct impact on increasing the cost of the training model. As shown in [111], multi-party computation is a way to protect data privacy in FL. Performing experiments with a different number of workers does not impact the computation cost, however, increasing the number of training rounds significantly boosts the computation cost. Therefore, the trade-off between privacy risk and computation time has been a promising topic for researchers.
