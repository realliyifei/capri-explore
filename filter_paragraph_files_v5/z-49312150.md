# A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress

CorpusID: 49312150 - [https://www.semanticscholar.org/paper/9d4d8509f6da094a7c31e063f307e0e8592db27f](https://www.semanticscholar.org/paper/9d4d8509f6da094a7c31e063f307e0e8592db27f)

Fields: Mathematics, Computer Science

## (s29) Theoretically Guaranteed Accuracy
Number of References: 3

(p29.0) From a theoretical viewpoint, some methods have better analytically-proved performance guarantees than others. The maximum entropy probability distribution over space of policies (or trajectories) minimizes the worst-case expected loss [52]. Consequently, maxentirl learns a behavior which is neither much better nor much worse than expert's [53]. However, the worst-case analysis may not represent the performance on real applications because the performance of optimization-based learning methods can be improved by exploiting favorable properties of the application domain. Classification based approaches such as csi and scirl admit a theoretical guarantee for the quality ofR E , in terms of optimality of learned behaviorπ E , given that both classification and regression errors are small. Nevertheless, these methods may not reduce the loss as much as mwal as the latter is the only method, in our knowledge, which has no probabilistic lower bound on the value-loss incurred [25].
## (s30) Analysis and Improvement of Complexity
Number of References: 2

(p30.0) Active birl offers a benefit over traditional birl by exhibiting reduced sample complexity (see Section 3.4). This is because it seeks to ascertain the most informative states where a demonstration is needed, and queries for it. Consequently, less demonstrations are often needed and the method becomes more targeted. Of course, this efficiency exists at the computational expense of interacting with the expert. In a different direction, Francisco et al. [34] exploiting an equivalence among the states to reduce the sample complexity, as shown in Fig. 5.2. Likewise, reirl uses fewer samples (input trajectories) as compared to alternative methods including a model-free variant of mmp [28].
## (s38) POMDP-IRL.
Number of References: 3

(p38.0) An expert with noisy sensors that senses its state imperfectly can be modeled as a partially observable MDP (POMDP) [67]. The expert's uncertainty about its current physical state is modeled as a belief (distribution) over its state space. The expert's policy is then a mapping from its beliefs to optimal actions. Choi et al. [21] propose making either this policy available to the learner or the prior belief along with the sequence of expert's observations and actions (that can be used to reconstruct the expert's sequence of beliefs). The POMDP policy is represented as a finite-state machine whose nodes give the actions to perform on receiving observations that form the edge labels. The learner conducts a search through space of policies by gradually improving on the previous policy until it explains the observed behavior. Figure 12 illustrates this approach.  [21], consider a POMDP with two actions and two observations. π E (solid lines) is a fsm with nodes {n 1 , n 2 } associated to actions and edges as observations {z 1 , z 2 }. The one-step deviating policies (dashed lines)
