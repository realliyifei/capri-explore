# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Self-Supervised Learning for Recommender Systems: A Survey

CorpusID: 247794106 - [https://www.semanticscholar.org/paper/2e6654520d8831f1721d4ec2dd1089b5d27f460f](https://www.semanticscholar.org/paper/2e6654520d8831f1721d4ec2dd1089b5d27f460f)

Fields: Computer Science

## (s31) Pros and Cons
Number of References: 2

(p31.0) Due to the flexibility to augment data and set pretext tasks, contrastive methods expand rapidly in recent years and reach out most recommendation topics. While contrastive SSR has shown remarkable effectiveness in improving recommendation with lightweight architectures, it is often compromised by the unknown criterion for high-quality data augmentations [57]. Existing contrastive methods are mostly based on arbitrary data augmentations and are selected by trial-and-error. There have been neither rigorous understanding of how and why they work nor rules or guidelines clearly telling what good augmentations are for recommendation. In addition, some common augmentations, which were considered useful, recently even have been proved having a negative impact on recommendation performance [82]. As a result, without knowing what augmentations are informative, the contrastive task may fail.

LLM judge: NO

The content partially meets the criteria by being cohesive and not interspersed with non-textual elements like mathematical equations. However, it fails on several fronts:

1. It lacks a self-contained context since it references specific studies ([57], [82]) without explaining their findings or relevance, leaving the reader without full understanding.
2. The text ends on an open question, indicating a lack of conclusion or summarization of the discussed content, making it challenging to form a comprehensive question based on the information provided.
3. There are references to "contrastive methods", "SSR", and "data augmentations" without enough background information within the text extract to understand these terms fully.

Additionally, the content hints at a broader discussion in the original paper that is not captured within the provided excerpt, indicating that it might not stand alone effectively.

## (s37) .1 Sample Prediction
Number of References: 3

(p37.0) Self-training [122], a flavor of semi-supervised learning, is linked to SSL in the Sample Prediction branch. The SSR model is pre-trained on the original data, and potential informative samples for the recommendation task are predicted using the pre-trained parameters as augmented data. These samples are then used to enhance the recommendation task or recursively generate better samples. The difference between SSL-based sample prediction and pure selftraining is that in semi-supervised learning, a finite number of unlabeled samples are available, while in SSL, samples are dynamically generated. Sequential recommendation models often perform poorly on short sequences due to limited user behaviors. To improve the model performance, ASReP [123] proposes to augment the short sequences with pseudo-prior items. Given ordered sequences, ASRep first pre-trains a Transformer-based encoder SASRec [108] in a reverse manner (i.e., from right-to-left) so that the encoder is capable of predicting the pseudo-prior items. An augmented sequence is obtained by appending the fabricated subsequence to the beginning of the original sequence. The encoder is then fine-tuned on the augmented sequences in a left-to-right manner to predict the next item in the original sequence.

LLM judge: NO

The content fails to meet criteria 5 (self-contained). It refers to specific models (SSR, ASReP, SASRec) and concepts (SSL, self-training) without providing adequate background or definitions for a reader unfamiliar with the paper or field. It assumes prior knowledge, making it difficult to understand without access to the full paper or external sources. Additionally, the mention of specific reference numbers ([122], [123], [108]) suggests that the text relies on external information, breaching the requirement for the content to be self-contained.

## (s43) Collaborative Self-Supervision
Number of References: 4

(p43.0) To get comprehensive self-supervised signals, multiple selfsupervised tasks collaborate to enhance the contrastive task by generating more informative samples under this branch.  [132] proposes a pre-training strategy for a Transformer-based model by linking a generative pretext task with a contrastive pretext task for sequential recommendation. The generative task involves masked-item prediction, with the predicted probabilities used to augment the sequence for contrasting to the original sequence. The method uses a curriculum learning strategy to arrange the contrastive task from easy to difficult based on the augmented sequences' ability to restore users' attribute information. SEPT [64] is the first SSR model to integrate SSL and tri-training [134] through a sample-based predictive task for social recommendation. It builds three graph encoders over three views with different social semantics. The encoders can predict semantically similar samples for the other two encoders, which are then used as positive self-supervision signals in the contrastive task. The improved encoders recursively predict more informative samples. COTREC [107] follows the same framework as SEPT but reduces the number of encoders to two for session-based recommendation. The two encoders are built over two session-induced temporal graphs and iteratively predict samples to enhance each other through the contrastive task. To prevent the mode collapse problem, COTREC uses an adversarial approach to keep the two encoders distinct.

LLM judge: NO

The content fails to provide a clear, self-contained overview suitable for generating a comprehensive question without prior knowledge of several concepts mentioned (e.g., contrastive task, SSL, tri-training). It does not meet the criteria of providing clarity and understanding without access to the full paper (criteria 5), as it relies on understanding specific models (SEPT, COTREC) and concepts (session-based recommendation, self-supervised signals) which are not explained. Additionally, while the text is cohesive, the reliance on specific references (e.g., [132], [64], [107], [134]) without further explanation undermines its self-containment.

## (s45) Pros and Cons
Number of References: 4

(p45.0) Hybrid methods assemble multiple self-supervised tasks to achieve enhanced and comprehensive self-supervision. Particularly, collaborative methods, where the generative/predictive task serves the contrastive task by dynamically generating samples, offer significant advantages in training effectiveness over static counterparts [64], [107], [135]. However, hybrid methods are confronted with the problem of coordinating multiple self-supervised tasks. They often struggle to balance different self-supervised tasks, requiring a manual search for hyperparameters or costly domain knowledge. Besides, different self-supervised tasks may also interfere with each other, which may require more complex architectures with a large number of parameters such as multi-gate mixture-of-experts networks [136] to separate task-shared and task-specific information. As a result, a hybrid SSR model comes at a higher training cost compared to models with only a single self-supervised task.

LLM judge: YES

What are the benefits and challenges of hybrid self-supervised learning methods compared to single-task models?

## (s51) Theory for Augmentation Selection
Number of References: 2

(p51.0) While data augmentation is essential for improving SSR performance, most current methods rely on heuristic approaches borrowed from other fields like CV, NLP, and graph learning. However, these approaches cannot be seamlessly transplanted to recommendation to deal with the user behavior data which is tightly coupled with the scenario and blended with noises and randomness. Besides, most methods augment data based on heuristics, and search the appropriate augmentations by the cumbersome trial-anderror. Although there have been some theories that try to demystify the visual view choices in contrastive learning [138], [57], the principle for augmentation selection in recommendation is seldomly studied. A solid recommendationspecific theoretical foundation which can streamline the selection process and free people from the tedious trial-anderror work is therefore urgently needed.

LLM judge: YES

Why is a solid recommendation-specific theoretical foundation urgently needed for the data augmentation in SSR performance improvement?

