# A Survey on Deep Reinforcement Learning-based Approaches for Adaptation and Generalization

CorpusID: 246904725 - [https://www.semanticscholar.org/paper/a11d62fbf84829d01aebbe6bfd5a3f1eed11e3ef](https://www.semanticscholar.org/paper/a11d62fbf84829d01aebbe6bfd5a3f1eed11e3ef)

Fields: Computer Science

## (s5) Meta Reinforcement Learning (Meta-RL)
Number of References: 6

(p5.0) Meta-RL is based on the notion of learning to learn [Thrun and Pratt, 1998]. Currently, it is widely used in the development of learning algorithms that are capable of generalization to unseen tasks/domains.  made one of the first attempts to develop a model-agnostic algorithm as opposed to DG-specific models [Khosla et al., 2012;Ghifary et al., 2015], which was demonstrated on RL settings to perform improve DG ability. [Yoon et al., 2018] inspired by the application of Bayesian inference principle in RL [Houthooft et al., 2016], proposed the first Bayesian fast adaptation method to solve the overfitting problem prevailing in vanilla meta-learning algorithms [Finn et al., 2017] to adapt to unseen tasks by approximating the uncertainties in them using a meta-update loss calling it the Chaser loss. The primary motivation behind using Meta-learning methods is to achieve faster adaptation to unseen tasks without learning from scratch by utilizing past experience.
## (s6) Skill Discovery
Number of References: 3

(p6.0) A skill (or option) in the context of RL is defined as a latent-conditioned policy that can be trained to perform useful tasks in a sparse/unknown reward environment [Sutton et al., 1999]. [Achiam et al., 2018] proposed a variational inference-based option discovery method for training an agent to discover and learn skills through environment interaction without the need of maximizing the cumulative reward for a given task.  proposed a reward-free skill learning method using an information-theoretic objective with a maximum entropy policy to keep the set of learned skills as diverse and discriminable as possible to maintain their usefulness. [Co-Reyes et al., 2018] introduced a novel hierarchical RL algorithm, which is capable of learning the skills in a continuous latent space (low-level). Also, they proposed a model that is capable of predicting the output of the learned skills. Such a capability can play a crucial role in solving more complex tasks at a higher level.
