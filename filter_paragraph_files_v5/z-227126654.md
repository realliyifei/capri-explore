# Time Series Data Imputation: A Survey on Deep Learning Approaches

CorpusID: 227126654 - [https://www.semanticscholar.org/paper/c88ee84b150ce0d3be164fd4cbaeeda7d151e3b3](https://www.semanticscholar.org/paper/c88ee84b150ce0d3be164fd4cbaeeda7d151e3b3)

Fields: Computer Science

## (s0) Introduction
Number of References: 5

(p0.0) Time series are vital in real-world applications. However, due to unexpected accidents, for example broken sensors or missing of the signals, missing values are everywhere in time series. In some datasets, the missing rate can reach 90%, which makes the data hard to be utilized [12]. The missing values significantly do harm to the downstream applications such as traditional classification or regression, sequential data integration [21] and forecasting tasks [18], leading to high demand for data imputation.

(p0.1) Our preliminary study [11] shows that imputing the missing values indeed helps significantly the prediction of fuel consumption. In the scenarios of fuel consumption prediction, missing values happen due to the errors of sensors. We propose an imputation approach named FuelNet to deal with such errors. The FuelNet generates proper values to impute missing data. With imputed data, the fuel consumption can be reduced by around 45.5%.
## (s3) Methods
Number of References: 5

(p3.0) In this section, we will first give an overall review of the relationships among the given approaches and comparisons of them and then discuss them individually with details. The main deep learning methods we researched for time series imputation are GRU-D [7], GRUI-GAN [27], E 2 GAN [28], BRITS [6] and NAOMI [25]. All of them are deep learning approaches published recently for time series imputation tasks. Among these methods, recurrent neural network (RNN) and generative adversarial network (GAN) are main architectures that are adopted. The reason is that RNN and its variations (e.g., LSTM, GRU) have been proven powerful in modeling sequence data, while GAN has been successfully applied to generation and imputation tasks.
## (s5) GRU-D
Number of References: 4

(p5.0) GRU-D is proposed by [7] as one of the early attempts to impute time series with deep learning models. It is the first one among the 5 researched paper to systematically model missing patterns into RNN for time series classification problems. It is also the first research to exploit that, RNN can model multivariable time series with the informativeness from the time series. Former works like [23,8] attempted to impute missing values with RNN by concatenating timestamps and raw data, i.e., regard timestamps as one attribute of raw data. But in [7], the concept time lag is first proposed. In this paper, Gated Recurrent Unit (GRU) is first adopted to generate missing values. In each layer of GRU, since the input can contain missing values, they replace the input x j ti with a combination of 4   The main contribution of this paper is the GRU based model GRU-D and the proposition of decay rate. To address the imputation of the missing values, they discover that • The missing variables tend to be close to some default value if its last observation happens a long time ago.
## (s7) BRITS
Number of References: 4

(p7.0) Unlike former methods, BRITS [6] is totally based on RNN structure and proposes imputation with unidirectional dynamics. Time lag (corresponding to "time gaps" in [6]) is also employed since the time series may be irregular. Similar to the idea of decay rate γ from GRU-D introduced in Section 4.2, they propose temporal decay  factor γ t = exp (−max (0, W γ δ t + b γ )). Compared to GRU-D where the time lags are considered in input and serve as the decay rate, in BRITS the hidden states update with the decay rate γ. It means when updating the hidden state, the old hidden state decays according to the time duration recorded in the time lags. Hence, the model is updated by:

(p7.1) The former model named RITS is the unidirectional version of the proposed methods in [6]. As the bidirectional version, BRITS employs bidirectional RNN by utilizing the bidirectional recurrent dynamics, i.e., they train 2 models in forward direction and backward direction respectively [17]. Thus consistency loss is introduced to take the losses of both directions into consideration.
## (s14) Consistent Query Answering
Number of References: 2

(p14.0) Following [22], query answering without determining the specific imputation of each missing value is crucial in probabilistic databases [10], when data from many sources can be inconsistent and uncertain. Therefore, consistent query answering (CQA) is needed. Missing values data in CQA problem increase the difficulty of answering the query consistently. Both the inconsistent data from different sources and missing values should be considered. Therefore, a combination of data imputation methods and CQA methods can be a potential approach.
