# A review on distance based time series classification

CorpusID: 48359428 - [https://www.semanticscholar.org/paper/1a22c4fcff260a8623a1fd677f2e7a7916343dae](https://www.semanticscholar.org/paper/1a22c4fcff260a8623a1fd677f2e7a7916343dae)

Fields: Mathematics, Computer Science

## (s2) k-Nearest Neighbour
Number of References: 7

(p2.0) This approach employs the existing time series distances within k-NN classifiers. In particular, the 1-NN classifier has mostly been used in time series classification due to its simplicity and competitive performance [Ding et al. , 2008;Lines et al. , 2012]. Given a distance measure and a time series, the 1-NN classifier predicts the class of this series as the class of the object closest to it from the training set. Despite the simplicity of this rule, a strength of the 1-NN is that as the size of the training set increases, the 1-NN classifier guarantees an error lower than two times the Bayes error [Cover & Hart, 1967]. Nevertheless, it is worth mentioning that it is very sensitive to noise in the training set, which is a common characteristic of time series datasets. This approach has been widely applied in time series classification, as it achieves, in conjunction with the DTW distance, the best accuracies achieved on many benchmark datasets. As such, quite a few studies and reviews include the 1-NN in the time series literature [Bagnall et al. , 2017;Wang et al. , 2013;Lines & Bagnall, 2015;Kaya & Gündüz-Öüdücü, 2015], and hence, it is not going to be further detailed in this review.
## (s16) Indefinite distance kernels
Number of References: 2

(p16.0) There is another method that employs a distance based indefinite kernel but takes a completely different approach to construct the kernel: the idea of this kernel is to, rather than use an existing distance measure, incorporate the concept of alignment between series into the kernel function itself. Many elastic measures for time series deal with the notion of alignment of series. The DTW distance, for instance, finds an optimal alignment between two time series such that the Euclidean distance between the aligned series is minimized. Following the same idea, in DTAK, Shimodaira et al. [2002] align two series so that their similarity is maximized. In other words, their method finds an alignment between the series that maximizes a given similarity (defined by the user), and this maximal similarity is used directly as a kernel. They give some good properties of the proposed kernel but they remark that it is not PSD, since negative eigenvalues can be found in the kernel matrices of DTAK [Cuturi, 2011].
## (s17) Dealing with the indefiniteness
Number of References: 3

(p17.0) There is a group of methods that attribute the poor performance of their kernel methods to the indefiniteness, and propose some alternatives or solutions to overcome these limitations. Jalalian & Chalup [2013], for instance, proposed the use of a special SVM called Potential Support Vector Machine (P-SVM) [Hochreiter & Obermayer, 2006] to overcome the shortcomings of learning with indefinite kernels. They employed the GDS DT W kernel within this SVM classifier which is able to handle kernel matrices that are neither positive definite nor square. They carried out an extensive experimentation including a comparison of their method with the 1-NN classifier and with the methods presented by Gudmundsson et al. [2008]. They conclude that their DTW based P-SVM method significantly outperforms both distance features and indefinite distance kernels, as well as the benchmark methods in 20 UCR datasets.
## (s19) Analyzing the indefiniteness
Number of References: 4

(p19.0) The last group of methods do not focus on solving the problems of learning with indefinite kernels but, instead, focus on a better understanding of these distance kernels and their indefiniteness. Lei & Sun [2007] theoretically analyze the GDS DT W kernel, proving that it is not a PSD kernel. This is because DTW is not a metric (it violates the triangle inequality [Casacuberta et al. , 1987]) and non-metricity prevents definiteness [Haasdonk & Bahlmann, 2004]. That is, if d is not metric, GDS d is not PSD. However, the contrary is not true and, hence, the metric property of a distance measure is not a sufficient condition to guarantee a PSD kernel. In any case, Zhang et al. [2010], hypothesized kernels based on metrics give rise to better performances than kernels based on distance measures which are not metrics. As such, they define what they called the Gaussian Elastic Metric Kernel (GEMK), a family of GDS kernels in which the distance d is replaced by an elastic measure which is also a metric. They employed GDS ERP and GDS T W ED and stated that, even if the definiteness of these kernels is not guaranteed, they did not observe any violations of their definiteness in their experimentation on 20 UCR datasets. In fact, these kernels are shown to perform better than the GDS DT W and the Gaussian kernel in those experiments. The authors attribute this to the fact that the proposed measures are both elastic and obey metricity. In order to provide some information about the most common distance measures applied in this context, table 4 shows a summary of properties of the main distance measures employed in this review. In particular, we specify if a given distance measure d is a metric or not, if it is an elastic measure or not, and if the corresponding GDS d is proven to be PSD or not. 
