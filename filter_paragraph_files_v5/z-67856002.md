# A Survey on Graph Processing Accelerators: Challenges and Opportunities

CorpusID: 67856002 - [https://www.semanticscholar.org/paper/32604282b8e53584a906d31ab1238f7b41d0dd39](https://www.semanticscholar.org/paper/32604282b8e53584a906d31ab1238f7b41d0dd39)

Fields: Computer Science, Engineering

## (s13) Discussions.
Number of References: 5

(p13.0) Edge-centric paradigm is usually used by existing graph accelerators for improving the utilization of their limited memory bandwidth [27,28,75] . However, the point is that edge-centric paradigm is lack of flexible scheduling potential in contrast to the vertex-centric one. Almost all of edges have to be processed multiple times to complete the whole process. In addition, this paradigm may also lead to a large number of random accesses to vertices. Thus, additional optimizations are often cooperatively needed, e.g., fined grained partitioning and tailored vertex update strategies [28,70] .
## (s19) 3) Heterogeneous Acceleration.
Number of References: 5

(p19.0) As the rapid development of memory integration technologies (e.g., 3D stacking), the host memory becomes large or even huge with trillions of capacity [3,50] .  [80] . For supporting efficient cooperation, the dedicated memory subsystem is needed to alleviate the transmission overhead between the host and the graph accelerator. As a result, the data organization of graphs is the key to reduce the communication overhead. In order to avoid conflicts of computing devices, runtime scheduling schemes are also important for efficient task scheduling.

(p19.1) The authors in [80] propose to accelerate graph processing under a heterogeneous architecture with CPU and FPGA. Hybrid vertex-and edge-centric models are adopted in [80] as discussed in 
## (s24) Multiple I/O Ports.
Number of References: 10

(p24.0) Another method is to design multiple I/O ports for a memory block [27,88,92] . By increasing the I/O ports, multiple memory requests can run concurrently. Usually the number of ports can be manually organized on the scratchpad memory. High MLP can be attained when the number of ports are equal to the number of processing units [16] . BRAMs on FPGAs can also be manually controlled to achieve this goal [27] . These BRAMs are usually combined together to form a memory block with multiple I/O ports.

(p24.1) 2) Improving Bandwidth Utilization. This method is widely adopted in graph accelerators [27,71,88,92,93] . For example, if the memory requests are adjacent in a vertex or edge list, these requests can be coalesced as one request for a block.
## (s31) Execution Model
Number of References: 3

(p31.0) The execution model typically has two major concerns: 1) scheduling timing, and 2) scheduling order. processors [113] . In graph accelerators, the graph is usually partitioned into subgraphs that are processed by different processing units. When a processing unit finishes its work, it has to wait for other processing units finished. Then the values of different subgraphs are synchronized [25] . During each iteration, only the local values of graph data can be accessed and updated [26] .
