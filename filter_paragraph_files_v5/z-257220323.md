# Placental Vessel Segmentation and Registration in Fetoscopy: Literature Review and MICCAI FetReg2021 Challenge Findings

CorpusID: 257220323 - [https://www.semanticscholar.org/paper/9774087e7ffba606d4d70f080fb29763e56bbd02](https://www.semanticscholar.org/paper/9774087e7ffba606d4d70f080fb29763e56bbd02)

Fields: Computer Science, Medicine, Engineering

## (s9) Deep learning-based methods
Number of References: 7

(p9.0) Existing deep learning-based methods for fetoscopic mosaicking mainly focused on training a CNN network (Bano et al., 2019(Bano et al., , 2020b for directly estimating homography between adjacent frames, extracting stable regions (Gaisser et al., 2016) in a view, or relying on flow fields (Alabi et al., 2022) for robust pair-wise images registration.

(p9.1) A deep learning-based feature extractor was proposed by Gaisser et al. (2016) that used similarity learning using contrastive loss when training a Siamese convolutional neural network (CNN) architecture between pairs of similar and dissimilar small patches extracted from ex-vivo placental images. The learned feature extractor was used for extracting features from pairs of overlapping images, followed by using LMedS for the transformation estimation. Due to motion blur and texture paucity that affected the feature extractor performance, the method was validated only on a short sequence (26 frames) that captured an ex-vivo phantom placenta. Gaisser et al. (2018) extended their similarity learning approach (Gaisser et al., 2016) for detecting stable regions on the vessels of the placenta. These stable regions' representation is used as features for placental image registration in an inwater phantom setting. The obtained homography estimation did not result in highly accurate registration, as the learned regions were not robust to visual variability in underwater placental scenes.
## (s17) Frame Registration and Mosaicking Evaluation
Number of References: 2

(p17.0) For evaluating homographies and mosaics (Task 2), we use the evaluation metric presented by Bano et al. (2020a) in the absence of groundtruth. The metric that we referred as -frame structural similarity index measure (SSIM) aims to evaluate the consistency in the adjacent frames. A visual illustration of the -frame SSIM metric is presented in Fig. 6. Given consecutive frames and a set of − 1 homographies { 1 , 2 , ..., −1 }, we evaluate the consistency between them. The ultimate clinical goal of Figure 6: Illustration of the N-frame SSIM evaluation metric from Bano et al. (2020a) fetoscopic registration is to generate consistent, comprehensible and complete mosaics that map the placental surface and guide the surgeon. Considering adjacent frames will have a large overlap along them, we evaluate the registration consistency between pairs of non-consecutive frames frames apart that have a large overlap in the FoV and present a clear view of the placental surface. Consider a source image , a target image + , and a homography transformation → + between them, we define the consistency between these two images as:
## (s21) BioPolimi
Number of References: 2

(p21.0) The team BioPolmini from Politecnico di Milano (Italy) are Chiara Lena, Ilaria Anita Cintorrino, Gaia Romana De Paolis and Jessica Biagioli. The model proposed by BioPolimi has a ResNet50 (He et al., 2016) backbone followed by the U-Net (Ronneberger et al., 2015) decoder for segmentation. The model is trained for 700 epochs with 6-fold cross-validation, using learning rate and batch size of 10 −3 and 32, respectively. To be consistent with the FetReg Challenge baseline, training images are resized to 448 × 448 pixels. Data augmentation, consisting of random crop with size 256 × 256 pixels, random rotation (in range (−45 • , +45 • )), horizontal and vertical flip and random variation in brightness (in range (−20%, +20%)), is applied to the training data. During inference, testing images are cropped in patches of dimension 256×256 pixels. The final prediction is obtained by overlapping the prediction obtained for each patch with a stride equal to 8.
## (s41) Baseline
Number of References: 3

(p41.0) As the baseline model, we trained a U-Net (Ronneberger et al., 2015) with ResNet50 (He et al., 2016) backbone as described in Bano et al. (2020a). Softmax activation is used at the final layer. Cross-entropy loss is computed and back propagated during training. Before training, the images are first resized to 448×448 pixels. To perform data augmentation, at each iteration step, a patch of 256 × 256 pixels is extracted at a random position in the image. Each of the extracted patches is augmented by applying a random rotation in range (−45 • , +45 • ), horizontal and vertical flip, scaling with a factor in the range of (−20%, +20%) and random variation in brightness (−20%, +20%) and contrast (−10%, +10%). Segmentation results are obtained by inference using 448 × 446 pixels resized input image. The baseline model is trained for 300 epochs on the training dataset. We create 6 folds, where each fold contains 3 procedures, to preserve as much variability as possible while keeping the number of samples in each fold approximately balanced. The final model is trained on the entire dataset, splitting videos in 80% for training and 20% for validation. The data is distributed to represent the same amount of variability in both subsets.
