# MIXED-PRECISION NEURAL NETWORKS: A SURVEY A PREPRINT

CorpusID: 251554723 - [https://www.semanticscholar.org/paper/30fa61a9c1db9527987fc25a91f26b9f23aa5152](https://www.semanticscholar.org/paper/30fa61a9c1db9527987fc25a91f26b9f23aa5152)

Fields: Computer Science

## (s39) Comparison Against Binary Neural Networks
Number of References: 9

(p39.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p39.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p39.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
## (s40) Pruning in MXPDNNs
Number of References: 9

(p40.0) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.

(p40.1) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.

(p40.2) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.
## (s88) Comparison Against Binary Neural Networks
Number of References: 9

(p88.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p88.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p88.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
## (s89) Pruning in MXPDNNs
Number of References: 9

(p89.0) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.

(p89.1) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.

(p89.2) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.
## (s137) Comparison Against Binary Neural Networks
Number of References: 9

(p137.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p137.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p137.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
## (s138) Pruning in MXPDNNs
Number of References: 9

(p138.0) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.

(p138.1) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.

(p138.2) Some of the aforementioned MXPDNN frameworks rely on pruning as an additional way toward more efficient neural networks. Pruning happens by removing unnecessary parameters from a large network, in order to improve generalization [184]. APQ searches for channel-wise pruning policy along with the quantization policy in a joint fashion. In particular, APQ solves a joint optimization problem that comprises two stages: 1) architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and 2) mixed-precision search. OPQ performs one-shot quantization and pruning. For pruning, OPQ computes the pruning masks for all layers, where these masks are used to remove the unimportant weights from a layer. Specifically, it first reformulates the problem of finding which weights could be removed to find the pruning ratios of all layers. The reformulated problem is solved by the Lagrangian multiplier by relying on the Newton-Raphson method. After finding the pruning ratios, the pruning masks are derived via magnitude-based thresholding. In addition, Bayesian Bits incorporates pruning via the learnable gates. The learnable gate is a variable ∈ {0, 1} that is placed on each of the quantized residual error tensors to control and optimize the effective bitwidth while jointly learning the quantization scales and the DNN parameters. The gate of the lowest bitwidth possible is what allows pruning, since deactivating this gate (i.e. setting it to 0) will yield a quantized value having 0-bits, i.e. pruned.
