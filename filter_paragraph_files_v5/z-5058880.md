# False Information on Web and Social Media: A Survey

CorpusID: 5058880 - [https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452](https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452)

Fields: Computer Science

## (s6) Bad actors: bots and sockpuppets
Number of References: 7

(p6.0) Humans are susceptible to false information and spread false information [105]. However, the creation and spread of false information is complex, and fueled by the use of nefarious actors which act independently or on a large-scale using a network of social media bots. Both deceive readers by creating an illusion of consensus towards the false piece of information, for instance by echoing it multiple times or expressing direct support for it. These accounts aim to artificially engineer the virality of their content (e.g., by 'upvoting'/promoting content in its early phase [109]) in order to spread posts with false information even faster and deeper than true information [11,19,30].

(p6.1) Lone-wolves operate by creating a handful of fake "sockpuppet" or "sybil" accounts and using them in coordination to reflect the same point of view, by writing similar reviews on e-commerce platforms or making similar comments on public forums. Lone-wolf operations using multiple accounts can be especially convincing as readers are typically not aware that a whole discussion is fabricated and actually originates from a single source. For instance, in online conversations, Kumar et al. [47] characterize this behavior by studying 61 million comments made by 2.1 million users across several discussion platforms. They found that while sockpuppets can be used with benign intention, sockpuppets with deceptive intentions are twice as common. Deceptive sockpuppets reply to each other with agreement and support, and are negative towards accounts that disagree. Moreover, these accounts hold central locations in the communication network, and are therefore in key spots to spread false content. Similarly, sybil accounts in communication and social networks are created to integrate themselves well into the network and prevent detection in order to increase their influence over others [113].
## (s14) Textual characteristics.
Number of References: 2

(p14.0) Since most reviews include textual content, researchers have extensively studied textual and linguistic features for discerning review fraud. Several works have posited that review fraudsters minimize effort by repeating the same reviews. Jindal et al. [43] provided the first well-known characterizations of review fraud, in which the authors characterized duplicate reviews (according to Jaccard similarity) across Amazon data as cases of fraud. The authors showed that many of these fraudulent duplicate reviews were from the same user on different products, rather than different users on the same product or different products. Figure 6 shows the distribution of maximum similarity between two reviewers' reviews. At the higher similarity end, 6% of the reviewers with more than one review have a maximum similarity score of 1, which is a sudden jump indicating that many reviewers copy reviews. Furthermore, Sandulescu et al. [82] showed that many review fraudsters adjust their reviews slightly so as not to post near or exactly similar reviews and be easily caught-instead, these sophisticated fraudsters tend to post semantically similar text (i.e. instead of duplicating "the hotel room had an excellent view, " the fraudster might post "the hotel room had a superb view" instead).
## (s15) Ratings characteristics
Number of References: 6

(p15.0) . Many e-commerce sites disallow users from giving feedback without giving an associated numerical rating. The rating is typically a 5-star system (1 representing the worst possible rating, and 5 representing the best), and is employed by numerous major online marketplaces including Amazon, eBay, Flipkart, and more. Prior work in review fraud has shown that those who engage in spreading fake e-commerce reviews also typically have skewed rating distributions [15,36,79,84] which are not typical of real users who share non-uniform opinions over many products. Figure 7 shows an example from Shah et al. [84], comparing aggregate (dataset-wide) rating habits from the Flipkart platform with two common, naive fraudster rating habits depicting very positive and negative raters. Some fraudulent reviewers give only positive ratings as they are created in order to inflate ratings for customer products, whereas other such reviewers give only negative ratings as they intend to slander competitors' products. Further, Kumar et al. [48] recently showed that fraudulent review writers are typically unfair, in that they give "unreliable" rating scores that differ largely from the product's average score. Furthermore, these fraudulent writers often give high ratings to products that otherwise receive highly negative ratings from fair users.
## (s17) 5.1.4
Number of References: 5

(p17.0) Graph-based characteristics. Several works show that dense subgraphs produced by coordinated or "lock-step" behavior in the underlying connections of the social (in this case, review) graph are associated with fraudulent behavior [16,71,83]. Figure 8 demonstrates this pattern in page-likes on Facebook [16]. Alternatively, other works look at the local network structure of the users instead of global structure. For example, Lin et al. [55] showed that for review platforms where multiple ratings/reviews can be given to the same product, review fraudsters often repeatedly post to the same product instead of diversifying like a real reviewer.
## (s23) 5.2.5
Number of References: 6

(p23.0) Debunking characteristics. Once false information spreads, attempts are made to debunk it and limit its spread. Recent research has shown that there is a significant time delay between the spread and its debunking. Zubiaga et al. [121] found that true information tends to be resolved faster than false information, which tends to take about 14 hours to be debunked. Shao et al. [86] came to a similar conclusion-they found a delay of 10-20 hours between the start of a rumor and sharing of its fact-checking contents.

(p23.1) But once debunking information reaches the rumor spreaders, do they stop spreading it or does it 'back-fire', as observed in in-lab settings [68] where corrections led to an increase in misperception? Several empirical studies on web-based false information suggest that debunking rumors is in fact effective, and people start deleting and questioning the rumor when presented with corrective information. Frigerri et al. [30] studied the spread of thousands of rumor reshare cascades on Facebook, and found that false information is more likely to be linked to debunking articles than true information. Moreover, once it is linked, it leads to a 4.4 times increase in deletion probability of false information than when it is not, and the probability is even higher if the link is made shortly after the post is created. Moreover, Zubiaga et al. [121] found that there are more tweets denying a rumor than supporting it after it is debunked, while prior to debunking, more tweets support the rumor. Furthermore, Vosoughi et al. [105] showed that there is a striking difference between replies on tweet containing false information than those containing true information-while people express fear, disgust, and surprise in replies, true information generates anticipation, sadness, joy, and trust. These differences can potentially be used to create early detection and debunking tools.
