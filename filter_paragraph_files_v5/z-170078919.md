# A survey of Object Classification and Detection based on 2D/3D data

CorpusID: 170078919 - [https://www.semanticscholar.org/paper/b2c768b7ddc9a038111a1361ef25828adfdcbdf8](https://www.semanticscholar.org/paper/b2c768b7ddc9a038111a1361ef25828adfdcbdf8)

Fields: Computer Science, Engineering

## (s2) Main Networks used for Image Classification
Number of References: 5

(p2.0) Some traditional algorithms used for the image classification are nearest neighbor and SVM. The features are the flattened pixel values. In the year 1989, the first important application [5] of using BP(Back Propagation) appeared. From this paper, a basic structure is shown in Figure 2. Similar structures are used in the modern neural networks such as AlexNet [6], VGG 16 [7] and ResNet [8]. The basic idea of the CNN was also introduced in [5].
## (s3) Object detection
Number of References: 5

(p3.0) In the object detection research area, just like in the image classification where the ImageNet dataset is available to train the algorithm, in the object detection, COCO(Common Objects in Context) [4] and VOC dataset can be used to train the algorithms. For the COCO dataset, the bounding box and the mask of the objects are provided. The first important contribution of using deep neural networks to solve the object detection problem is the R-CNN [20]. The region of interest(ROI) of an image is proposed by the selective search(SS) [21] algorithm, and the ROI is cropped to feed a CNN to do the detection. However, as every proposed interesting area will be calculated to predict whether that specified region is an object, the speed of this algorithm is very slow. In order to address this problem, a fast R-CNN algorithm is proposed in [22] by improving the feature map generation efficiency. In another paper which is short for faster RCNN [23], instead of using the SS algorithm to generate ROI, the ROI is proposed by using deep neural network structure. By the combination, the performance of the algorithm can also be improved itself. Figure 11: The 2D center offset box encoding method.
## (s4) Bounding box encoding method
Number of References: 5

(p4.0) In the object detection task, one import part is estimating the bounding box of the object. The 2D bounding box estimation only focus on axis-aligned boxes. The box encoding method is simply based on a simple center coordinates of the bounding box (x, y) and the offset of the bounding box: width: W and height: H. We are calling this method as 2D center offset box encoding method and it is shown in Figure 11. The RCNN, Fast RCNN, Faster RCNN, YOLO, YOLOv2 and Mask R-CNN are using this kind of bounding box encoding with a slightly difference on the loss calculation. [20], Fast RCNN [22] and Faster RCNN RCNN is an important framework in 2D image detection task which mainly uses the CNN to extract the features for each cropped interesting region. After that the extracted features are fed to a SVM to do the classification and a bounding box regression is followed to improve the bounding box prediction based on the method from [24]. It mainly has two stages: region proposal and detection. As the detection process is computation expensive, the region proposals can make the detection step mainly focus on limited interesting regions (about 2000 regions for a typical image) which greatly reduces the complexity of the whole system and achieves a good performance on the 2D image detection task. This two-stage detection framework is becoming a classical model in both 2D image based object detection and 3D image based object systems. The frame work of RCNN is shown in Figure 12. Figure 13: Fast R-CNN [22] architecture. An input image and multiple regions of interest (RoIs) are input into a fully convolutional network. Each RoI is pooled into a fixed-size feature map and then mapped to a feature vector by fully connected layers (FCs). The network has two output vectors per RoI: softmax probabilities and per-class bounding-box regression offsets. The architecture is trained end-to-end with a multi-task loss.  Fast R-CNN improves the RCNN mainly with respect to three aspects: First, instead of doing convolution operations separately for each proposed region, the Fast RCNN does the convolution operations for the whole image firstly and then uses region proposals from the feature map directly to do the further detection. The feature map level region proposals are projected from the region proposals based on the original image. Second, using the softmax layer to replace the SVM classifier to make the detection under one deep learning framework. Finally, Fast R-CNN is using the Multi-task loss to do the object classification and the bounding box regression. The Fast RCNN framework is shown in Figure 13. In order to have a same size of feature vectors from different size proposed regions, the ROI pooling is used and the ROI pooling is demonstrated in Figure 14. Faster RCNN Figure 15: The three main steps for the Faster RCNN [23], system: head(backbone network), RPN and detection network.
## (s16) Novel view point models
Number of References: 2

(p16.0) RotationNet is an extension of MVCNN [60]. In this paper, multiple views from different angles are explored. Three models of camera views are proposed as shown in Figure 37. The performance of case(i) (the same view points model as MVCNN [60]) and case(ii) are compared. Case (ii) achieves a better performance based on the ModelNet40 task. For the ModelNet40, the case(iii) model is not used.
## (s28) Comparison by performance
Number of References: 4

(p28.0) The performance comparison of the different systems is provided in Table 16 , 17 and 18 for the outdoor scenario based on the KITTI [69] dataset and in Table 12 for the indoor scenario based on the SUN-RGBD dataset [67].   3D object detection systems can be categorized by the supported application scenarios: indoor only, outdoor only or both. There are two main differences between indoor and outdoor scenarios: first, the range of indoor is small and of outdoor is large. A comparison of the indoor range and outdoor range based on two typical datasets is shown in Table 13. Second, as the distribution of the outdoor objects is more sparse and the categories of interesting objects of the outdoor scenarios is less compared with indoor scenarios, the outdoor scenarios can use BEV to generate the proposals and then do the detection. However, the indoor scenarios, generation only based on BEV will get a bad performance since there might be multiple objects in the vertical direction.

(p28.1) BEV only proposal generation algorithms such as MV3D [72] do not performance well indoors. At the same time, as the space is too large for the outdoor scenario, some 3D CNN based algorithms, such as Deep Sliding Shape [71], can work well for indoors but may have a high possibility to fail for the outdoor scenario without adjustment.  In the rest of this section we will introduce several of the latest papers organized by different application scenarios as shown in Table 14.
## (s35) FV features for MV3D
Number of References: 2

(p35.0) MV3D projects the FV into a cylinder plane to generate a dense front view map as in VeloFCN [84]. The front view map is encoded with threechannel features, which are height, distance and intensity as shown in Figure  64. Since KITTI uses a 64-beam Velodyne laser scanner, the size of map for the front view is 64 × 512.  [72] The performance of MV3D is evaluated based on the outdoor KITTI dataset. The performance of 3D object detection based on the test set can be found from the leaderboard. The performance of 3D object detection based on validation dataset is shown in Figures 65 and 66. It only provides the car detection results. Detection results for the pedestrians and cyclists are not provided.   The framework of AVOD is shown in Figure 68. AVOD is using the same encoding method as MV3D for the BEV. In AVOD, the value of M is set as 5 and the range of the LiDAR is [0, 70] × [−40, 40] × [0, 2.5] meters. So the size of the input feature for the BEV is 700 × 800 × 7. AVOD is using both the BEV and image to do the region proposals which is the main difference to the MV3D work.  VoxelNet architecture is shown in Figure 71. The feature learning network takes a raw point cloud as input, partitions the space into voxels, and transforms points within each voxel to a vector representation characterizing the shape information. The space is represented as a sparse 4D tensor. The convolutional middle layers processes the 4D tensor to aggregate spatial context. Finally, a RPN generates the 3D detection. A fixed number, T , of points from voxels containing more than T points are randomly sampled. For each point, a 7-feature is used which is (x, y, z, r, x− v x , y − v y , z − v z ) where x, y, z are the XY Z coordinates for each point. r is the received reflectance and (v x , v y , v z ) is the centroid of points in the voxel. Voxel Feature Encoding is proposed in VoxelNet. The 7-feature for each point is fed into the Voxel feature encoding layer as shown in Figure 72. Fully connected networks are used in the VFE network with element-wise MaxPooling for each point and concatenation between each point and the element-wise MaxPooling output. The input of the VFE is T × 7 and the output will be T × C where C depends on the FC layers of the VFE itself and depends on the whole VFE layers network used. Finally, an element-wise MaxPooling is used again and change the dimension of the output to 1 × C. Then for each voxel we have a one vector with C elements as shown in Figure 71. For the whole framework, we will have an input data with shape of C ×D ×H ×W .
