# A Survey on Temporal Sentence Grounding in Videos

CorpusID: 237532483 - [https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453](https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453)

Fields: Computer Science

## (s17) Spatio-temporal localization.
Number of References: 3

(p17.0) Spatial-temporal sentence grounding in videos is another extension from TSGV which mainly localizes the referring object/instance as a continuing spatialtemporal tube (i.e., a sequence of bounding boxes) extracted from an untrimmed video via a natural language description. Since fine-grained labeling process of localizing a tube (i.e., annotate a spatial region for each frame in videos) for STSGV is labor-intensive and complicated, Chen et al. [13] propose to solve this task in a weakly-supervised manner which only needs video-level descriptions, with a newly-constructed VID-sentence dataset. Besides, VOGNet [50] commits to address the task of video object grounding, which grounds objects in videos referred to the natural language descriptions, and constructs a new dataset called ActivityNet-SRL. Tang et al. [56] employ visual transformer to solve a similar task which aims to localize a spatio-temporal tube of the target person from an untrimmed video based on a given textural description with a newly-constructed HC-STVG dataset.
## (s18) 4.2.3
Number of References: 2

(p18.0) Audio-enhanced localization. The current inputs for TSGV only contain the given sentence along with the untrimmed video. However, the audio signals are not effectively exploited, which may provide extra guidance for video localization, e.g., the loud noise while using electronics in the kitchen or cheers from the audience when the football player kicks a goal. Such various forms of sounds do offer auxiliary but essential clues for more precise localization of the target moments, which has not been explored yet. Moreover, what people speak in videos can be converted into text with the Automated Speech Recognition (ASR) technique. The converted text also provides relevant information for the cross-modal alignment between video and the text query. Nowadays, there has been many works [24,68] in visual-and-language area with audio-enhanced auxiliary proving its effectiveness for performance improvements. Thus, it is a promising future direction to embed the audio information for the TSGV task.
