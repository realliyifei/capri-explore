# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Self-Supervised Learning for Recommender Systems: A Survey

CorpusID: 247794106 - [https://www.semanticscholar.org/paper/2e6654520d8831f1721d4ec2dd1089b5d27f460f](https://www.semanticscholar.org/paper/2e6654520d8831f1721d4ec2dd1089b5d27f460f)

Fields: Computer Science

## (s31) Pros and Cons
Number of References: 2

(p31.0) Due to the flexibility to augment data and set pretext tasks, contrastive methods expand rapidly in recent years and reach out most recommendation topics. While contrastive SSR has shown remarkable effectiveness in improving recommendation with lightweight architectures, it is often compromised by the unknown criterion for high-quality data augmentations [57]. Existing contrastive methods are mostly based on arbitrary data augmentations and are selected by trial-and-error. There have been neither rigorous understanding of how and why they work nor rules or guidelines clearly telling what good augmentations are for recommendation. In addition, some common augmentations, which were considered useful, recently even have been proved having a negative impact on recommendation performance [82]. As a result, without knowing what augmentations are informative, the contrastive task may fail.
## (s37) .1 Sample Prediction
Number of References: 3

(p37.0) Self-training [122], a flavor of semi-supervised learning, is linked to SSL in the Sample Prediction branch. The SSR model is pre-trained on the original data, and potential informative samples for the recommendation task are predicted using the pre-trained parameters as augmented data. These samples are then used to enhance the recommendation task or recursively generate better samples. The difference between SSL-based sample prediction and pure selftraining is that in semi-supervised learning, a finite number of unlabeled samples are available, while in SSL, samples are dynamically generated. Sequential recommendation models often perform poorly on short sequences due to limited user behaviors. To improve the model performance, ASReP [123] proposes to augment the short sequences with pseudo-prior items. Given ordered sequences, ASRep first pre-trains a Transformer-based encoder SASRec [108] in a reverse manner (i.e., from right-to-left) so that the encoder is capable of predicting the pseudo-prior items. An augmented sequence is obtained by appending the fabricated subsequence to the beginning of the original sequence. The encoder is then fine-tuned on the augmented sequences in a left-to-right manner to predict the next item in the original sequence.
## (s43) Collaborative Self-Supervision
Number of References: 4

(p43.0) To get comprehensive self-supervised signals, multiple selfsupervised tasks collaborate to enhance the contrastive task by generating more informative samples under this branch.  [132] proposes a pre-training strategy for a Transformer-based model by linking a generative pretext task with a contrastive pretext task for sequential recommendation. The generative task involves masked-item prediction, with the predicted probabilities used to augment the sequence for contrasting to the original sequence. The method uses a curriculum learning strategy to arrange the contrastive task from easy to difficult based on the augmented sequences' ability to restore users' attribute information. SEPT [64] is the first SSR model to integrate SSL and tri-training [134] through a sample-based predictive task for social recommendation. It builds three graph encoders over three views with different social semantics. The encoders can predict semantically similar samples for the other two encoders, which are then used as positive self-supervision signals in the contrastive task. The improved encoders recursively predict more informative samples. COTREC [107] follows the same framework as SEPT but reduces the number of encoders to two for session-based recommendation. The two encoders are built over two session-induced temporal graphs and iteratively predict samples to enhance each other through the contrastive task. To prevent the mode collapse problem, COTREC uses an adversarial approach to keep the two encoders distinct.
## (s45) Pros and Cons
Number of References: 4

(p45.0) Hybrid methods assemble multiple self-supervised tasks to achieve enhanced and comprehensive self-supervision. Particularly, collaborative methods, where the generative/predictive task serves the contrastive task by dynamically generating samples, offer significant advantages in training effectiveness over static counterparts [64], [107], [135]. However, hybrid methods are confronted with the problem of coordinating multiple self-supervised tasks. They often struggle to balance different self-supervised tasks, requiring a manual search for hyperparameters or costly domain knowledge. Besides, different self-supervised tasks may also interfere with each other, which may require more complex architectures with a large number of parameters such as multi-gate mixture-of-experts networks [136] to separate task-shared and task-specific information. As a result, a hybrid SSR model comes at a higher training cost compared to models with only a single self-supervised task.
## (s51) Theory for Augmentation Selection
Number of References: 2

(p51.0) While data augmentation is essential for improving SSR performance, most current methods rely on heuristic approaches borrowed from other fields like CV, NLP, and graph learning. However, these approaches cannot be seamlessly transplanted to recommendation to deal with the user behavior data which is tightly coupled with the scenario and blended with noises and randomness. Besides, most methods augment data based on heuristics, and search the appropriate augmentations by the cumbersome trial-anderror. Although there have been some theories that try to demystify the visual view choices in contrastive learning [138], [57], the principle for augmentation selection in recommendation is seldomly studied. A solid recommendationspecific theoretical foundation which can streamline the selection process and free people from the tedious trial-anderror work is therefore urgently needed.
