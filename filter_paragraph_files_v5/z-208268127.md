# Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms

CorpusID: 208268127 - [https://www.semanticscholar.org/paper/54d4a221db5a91a2487b1610374843fafff5a23d](https://www.semanticscholar.org/paper/54d4a221db5a91a2487b1610374843fafff5a23d)

Fields: Mathematics, Computer Science

## (s5) Markov/Stochastic Games
Number of References: 4

(p5.0) One direct generalization of MDP that captures the intertwinement of multiple agents is Markov games (MGs), also known as stochastic games (Shapley, 1953). Originated from (a) MDP (b) Markov game (c) Extensive-form game Figure 1: Schematic diagrams for the system evolution of a Markov decision process, a Markov game, and an extensive-form game, which correspond to the frameworks for single-and multi-agent RL, respectively. Specifically, in an MDP as in (a), the agent observes the state s and receives reward r from the system, after outputting the action a; in an MG as in (b), all agents choose actions a i simultaneously, after observing the system state s and receiving each individual reward r i ; in a two-player extensive-form game as in (c), the agents make decisions on choosing actions a i alternately, and receive each individual reward r i (z) at the end of the game, with z being the terminal history. In the imperfect information case, player 2 is uncertain about where he/she is in the game, which makes the information set non-singleton.

(p5.1) the seminal work Littman (1994), the framework of MGs has long been used in the literature to develop MARL algorithms, see §4 for more details. We introduce the formal definition as below.
## (s15) Various Information Structures
Number of References: 2

(p15.0) Compared to the single-agent case, the information structure of MARL, namely, who knows what at the training and execution, is more involved. For example, in the framework of Markov games, it suffices to observe the instantaneous state s t , in order for each agent to make decisions, since the local policy π i maps from S to ∆(A i ). On the other hand, for extensive-form games, each agent may need to recall the history of past decisions, under the common perfect recall assumption. Furthermore, as self-interested agents, each agent can scarcely access either the policy or the rewards of the opponents, but at most the action samples taken by them. This partial information aggravates the issues caused by non-stationarity, as the samples can hardly recover the exact behavior of the opponents' underlying policies, which increases the non-stationarity viewed by individual agents. The extreme case is the aforementioned independent learning scheme, which assumes the observability of only the local action and reward, and suffers from non-convergence in general (Tan, 1993).
## (s19) Multi-Agent MDP & Markov Teams
Number of References: 10

(p19.0) Consider a Markov game as in Definition 2.2 with R 1 = R 2 = · · · = R N = R, where the reward R : S × A × S → R is influenced by the joint action in A = A 1 × · · · × A N . As a result, the Q-function is identical for all agents. Hence, a straightforward algorithm proceeds by performing the standard Q-learning update (2.1) at each agent, but taking the max over the joint action space a ∈ A. Convergence to the optimal/equilibrium Q-function has been established in Szepesvári and Littman (1999); , when both state and action spaces are finite.

(p19.1) However, convergence of the Q-function does not necessarily imply that of the equilibrium policy for the Markov team, as any combination of equilibrium policies extracted at each agent may not constitute an equilibrium policy, if the equilibrium policies are non-unique, and the agents fail to agree on which one to select. Hence, convergence to the NE policy is only guaranteed if either the equilibrium is assumed to be unique , or the agents are coordinated for equilibrium selection. The latter idea has first been validated in the cooperative repeated games setting (Claus and Boutilier, 1998), a special case of Markov teams with a singleton state, where the agents are joint-action learners (JAL), maintaining a Q-value for joint actions, and learning empirical models of all others. Convergence to equilibrium point is claimed in Claus and Boutilier (1998), without a formal proof. For the actual Markov teams, this coordination has been exploited in Wang and Sandholm (2003), which proposes optimal adaptive learning (OAL), the first MARL algorithm with provable convergence to the equilibrium policy. Specifically, OAL first learns the game structure, and constructs virtual games at each state that are weakly acyclic with respect to (w.r.t.) a biased set. OAL can be shown to converge to the NE, by introducing the biased adaptive play learning algorithm for the constructed weakly acyclic games, motivated from Young (1993).
## (s22) Decentralized Paradigm with Networked Agents
Number of References: 40

(p22.0) Cooperative agents in numerous practical multi-agent systems are not always homogeneous. Agents may have different preferences, i.e., reward functions, while still form a team to maximize the return of the team-average rewardR, whereR(s, a, s ) = N −1 · i∈N R i (s, a, s ). More subtly, the reward function is sometimes not sharable with others, as the preference is kept private to each agent. This setting finds broad applications in engineering systems as sensor networks (Rabbat and Nowak, 2004), smart grid (Dall'Anese et al., 2013;, intelligent transportation systems (Adler and Blue, 2002;, and robotics (Corke et al., 2005).

(p22.1) Covering the homogeneous setting in §4.1.1 as a special case, the specified one definitely requires more coordination, as, for example, the global value function cannot be estimated locally without knowing other agents' reward functions. With a central controller, most MARL algorithms reviewed in §4.1.1 directly apply, since the controller can collect and average the rewards, and distributes the information to all agents. Nonetheless, such a controller may not exist in most aforementioned applications, due to either cost, scalability, or robustness concerns (Rabbat and Nowak, 2004;Dall'Anese et al., 2013;. Instead, the agents may be able to share/exchange information with their neighbors over a possibly time-varying and sparse communication network, as illustrated in Figure 2 (b). Though MARL under this decentralized/distributed 5 paradigm is imperative, it is relatively less-investigated, in comparison to the extensive results on distributed/consensus algorithms that solve static/one-stage optimization problems (Nedic and Ozdaglar, 2009;Agarwal and Duchi, 2011;Jakovetic et al., 2011;Tu and Sayed, 2012), which, unlike RL, involves no system dynamics, and does not maximize the long-term objective in a sequential fashion.

(p22.2) Cooperative agents in numerous practical multi-agent systems are not always homogeneous. Agents may have different preferences, i.e., reward functions, while still form a team to maximize the return of the team-average rewardR, whereR(s, a, s ) = N −1 · i∈N R i (s, a, s ). More subtly, the reward function is sometimes not sharable with others, as the preference is kept private to each agent. This setting finds broad applications in engineering systems as sensor networks (Rabbat and Nowak, 2004), smart grid (Dall'Anese et al., 2013;, intelligent transportation systems (Adler and Blue, 2002;, and robotics (Corke et al., 2005).

(p22.3) Covering the homogeneous setting in §4.1.1 as a special case, the specified one definitely requires more coordination, as, for example, the global value function cannot be estimated locally without knowing other agents' reward functions. With a central controller, most MARL algorithms reviewed in §4.1.1 directly apply, since the controller can collect and average the rewards, and distributes the information to all agents. Nonetheless, such a controller may not exist in most aforementioned applications, due to either cost, scalability, or robustness concerns (Rabbat and Nowak, 2004;Dall'Anese et al., 2013;. Instead, the agents may be able to share/exchange information with their neighbors over a possibly time-varying and sparse communication network, as illustrated in Figure 2 (b). Though MARL under this decentralized/distributed 5 paradigm is imperative, it is relatively less-investigated, in comparison to the extensive results on distributed/consensus algorithms that solve static/one-stage optimization problems (Nedic and Ozdaglar, 2009;Agarwal and Duchi, 2011;Jakovetic et al., 2011;Tu and Sayed, 2012), which, unlike RL, involves no system dynamics, and does not maximize the long-term objective in a sequential fashion.
## (s25) Other Learning Goals
Number of References: 2

(p25.0) Several other learning goals have also been explored for decentralized MARL with networked agents. Zhang et al. (2016) has considered the optimal consensus problem, where each agent over the network tracks the states of its neighbors' as well as a leader's, so that the consensus error is minimized by the joint policy. A policy iteration algorithm is then introduced, followed by a practical actor-critic algorithm using neural networks for function approximation. A similar consensus error objective is also adopted in , under the name of cooperative multi-agent graphical games. A centralized-criticdecentralized-actor scheme is utilized for developing off-policy RL algorithms.
## (s27) Competitive Setting
Number of References: 8

(p27.0) Competitive settings are usually modeled as zero-sum games. Computationally, there exists a great barrier between solving two-player and multi-player zero-sum games. In particular, even the simplest three-player matrix games, are known to be PPAD-complete (Papadimitriou, 1992;Daskalakis et al., 2009). Thus, most existing results on competitive MARL focus on two-player zero-sum games, with N = {1, 2} and R 1 + R 2 = 0 in Definitions 2.2 and 2.4. In the rest of this section, we review methods that provably find a Nash (equivalently, saddle-point) equilibrium in two-player Markov or extensive-form games. The existing algorithms can mainly be categorized into two classes: value-based and policy-based approaches, which are introduced separately in the sequel.

(p27.1) Competitive settings are usually modeled as zero-sum games. Computationally, there exists a great barrier between solving two-player and multi-player zero-sum games. In particular, even the simplest three-player matrix games, are known to be PPAD-complete (Papadimitriou, 1992;Daskalakis et al., 2009). Thus, most existing results on competitive MARL focus on two-player zero-sum games, with N = {1, 2} and R 1 + R 2 = 0 in Definitions 2.2 and 2.4. In the rest of this section, we review methods that provably find a Nash (equivalently, saddle-point) equilibrium in two-player Markov or extensive-form games. The existing algorithms can mainly be categorized into two classes: value-based and policy-based approaches, which are introduced separately in the sequel.
## (s31) Value-Based Methods
Number of References: 40

(p31.0) Under relatively stringent assumptions, several value-based methods that extend Qlearning (Watkins and Dayan, 1992) to the mixed setting are guaranteed to find an equilibrium. In particular, Hu and Wellman (2003) has proposed the Nash-Q learning algorithm for general-sum Markov games, where one maintains N action-value functions Q N = (Q 1 , . . . , Q N ) : S × A → R N for all N agents, which are updated using sample-based estimator of a Bellman operator. Specifically, letting R N = (R 1 , . . . , R N ) denote the reward functions of the agents, Nash-Q uses the following Bellman operator:

(p31.1) where Nash[Q N (s , ·)] is the objective value of the Nash equilibrium of the stage game with rewards {Q N (s , a)} a∈A . For zero-sum games, we have Q 1 = −Q 2 and thus the Bellman operator defined in (4.26) is equivalent to the one in (4.14) used by minimax-Q learning (Littman, 1994). Moreover, Hu and Wellman (2003) establishes convergence to Nash equilibrium under the restrictive assumption that Nash[Q N (s , ·)] in each iteration of the algorithm has unique Nash equilibrium. In addition,  has proposed the Friend-or-Foe Q-learning algorithm where each agent views the other agent as either a "friend" or a "foe". In this case, Nash[Q N (s , ·)] can be efficiently computed via linear programming. This algorithm can be viewed as a generalization of minimax-Q learning, and Nash equilibrium is guaranteed for two-player zero-sum games and coordination games with unique equilibria. Furthermore, Greenwald et al. (2003) has proposed correlated Qlearning, which replaces Nash[Q N (s , ·)] in (4.26) by computing a correlated equilibrium (Aumann, 1974), a more general equilibrium concept than Nash equilibrium. In a recent work, Perolat et al. (2017) has proposed a batch RL method to find an approximate Nash equilibrium via Bellman residue minimization (Maillard et al., 2010). They have proved that the global minimizer of the empirical Bellman residue is an approximate Nash equilibrium, followed by the error propagation analysis for the algorithm. Also in the batch RL regime,  has considered a simplified mixed setting for decentralized MARL: two teams of cooperative networked agents compete in a zero-sum Markov game. A decentralized variant of FQI, where the agents within one team cooperate to solve (4.3) while the two teams essentially solve (4.15), is proposed. Finite-sample error bounds have then been established for the proposed algorithm.

(p31.2) To address the scalability issue, independent learning is preferred, which, however, fails to converge in general (Tan, 1993). Arslan and Yüksel (2017) has proposed decentralized Q-learning, a two timescale modification of Q-learning, that is guaranteed to converge to the equilibrium for weakly acyclic Markov games almost surely. Each agent therein only observes local action and reward, and neither observes nor keeps track of others' actions. All agents are instructed to use the same stationary baseline policy for many consecutive stages, named exploration phase. At the end of the exploration phase, all agents are synchronized to update their baseline policies, which makes the environment stationary for long enough, and enables the convergence of Q-learning based methods. Note that these algorithms can also be applied to the cooperative setting, as these games include Markov teams as a special case.

(p31.3) Under relatively stringent assumptions, several value-based methods that extend Qlearning (Watkins and Dayan, 1992) to the mixed setting are guaranteed to find an equilibrium. In particular, Hu and Wellman (2003) has proposed the Nash-Q learning algorithm for general-sum Markov games, where one maintains N action-value functions Q N = (Q 1 , . . . , Q N ) : S × A → R N for all N agents, which are updated using sample-based estimator of a Bellman operator. Specifically, letting R N = (R 1 , . . . , R N ) denote the reward functions of the agents, Nash-Q uses the following Bellman operator:

(p31.4) where Nash[Q N (s , ·)] is the objective value of the Nash equilibrium of the stage game with rewards {Q N (s , a)} a∈A . For zero-sum games, we have Q 1 = −Q 2 and thus the Bellman operator defined in (4.26) is equivalent to the one in (4.14) used by minimax-Q learning (Littman, 1994). Moreover, Hu and Wellman (2003) establishes convergence to Nash equilibrium under the restrictive assumption that Nash[Q N (s , ·)] in each iteration of the algorithm has unique Nash equilibrium. In addition,  has proposed the Friend-or-Foe Q-learning algorithm where each agent views the other agent as either a "friend" or a "foe". In this case, Nash[Q N (s , ·)] can be efficiently computed via linear programming. This algorithm can be viewed as a generalization of minimax-Q learning, and Nash equilibrium is guaranteed for two-player zero-sum games and coordination games with unique equilibria. Furthermore, Greenwald et al. (2003) has proposed correlated Qlearning, which replaces Nash[Q N (s , ·)] in (4.26) by computing a correlated equilibrium (Aumann, 1974), a more general equilibrium concept than Nash equilibrium. In a recent work, Perolat et al. (2017) has proposed a batch RL method to find an approximate Nash equilibrium via Bellman residue minimization (Maillard et al., 2010). They have proved that the global minimizer of the empirical Bellman residue is an approximate Nash equilibrium, followed by the error propagation analysis for the algorithm. Also in the batch RL regime,  has considered a simplified mixed setting for decentralized MARL: two teams of cooperative networked agents compete in a zero-sum Markov game. A decentralized variant of FQI, where the agents within one team cooperate to solve (4.3) while the two teams essentially solve (4.15), is proposed. Finite-sample error bounds have then been established for the proposed algorithm.

(p31.5) To address the scalability issue, independent learning is preferred, which, however, fails to converge in general (Tan, 1993). Arslan and Yüksel (2017) has proposed decentralized Q-learning, a two timescale modification of Q-learning, that is guaranteed to converge to the equilibrium for weakly acyclic Markov games almost surely. Each agent therein only observes local action and reward, and neither observes nor keeps track of others' actions. All agents are instructed to use the same stationary baseline policy for many consecutive stages, named exploration phase. At the end of the exploration phase, all agents are synchronized to update their baseline policies, which makes the environment stationary for long enough, and enables the convergence of Q-learning based methods. Note that these algorithms can also be applied to the cooperative setting, as these games include Markov teams as a special case.
## (s50) Markov/Stochastic Games
Number of References: 4

(p50.0) One direct generalization of MDP that captures the intertwinement of multiple agents is Markov games (MGs), also known as stochastic games (Shapley, 1953). Originated from (a) MDP (b) Markov game (c) Extensive-form game Figure 1: Schematic diagrams for the system evolution of a Markov decision process, a Markov game, and an extensive-form game, which correspond to the frameworks for single-and multi-agent RL, respectively. Specifically, in an MDP as in (a), the agent observes the state s and receives reward r from the system, after outputting the action a; in an MG as in (b), all agents choose actions a i simultaneously, after observing the system state s and receiving each individual reward r i ; in a two-player extensive-form game as in (c), the agents make decisions on choosing actions a i alternately, and receive each individual reward r i (z) at the end of the game, with z being the terminal history. In the imperfect information case, player 2 is uncertain about where he/she is in the game, which makes the information set non-singleton.

(p50.1) the seminal work Littman (1994), the framework of MGs has long been used in the literature to develop MARL algorithms, see §4 for more details. We introduce the formal definition as below.
## (s60) Various Information Structures
Number of References: 2

(p60.0) Compared to the single-agent case, the information structure of MARL, namely, who knows what at the training and execution, is more involved. For example, in the framework of Markov games, it suffices to observe the instantaneous state s t , in order for each agent to make decisions, since the local policy π i maps from S to ∆(A i ). On the other hand, for extensive-form games, each agent may need to recall the history of past decisions, under the common perfect recall assumption. Furthermore, as self-interested agents, each agent can scarcely access either the policy or the rewards of the opponents, but at most the action samples taken by them. This partial information aggravates the issues caused by non-stationarity, as the samples can hardly recover the exact behavior of the opponents' underlying policies, which increases the non-stationarity viewed by individual agents. The extreme case is the aforementioned independent learning scheme, which assumes the observability of only the local action and reward, and suffers from non-convergence in general (Tan, 1993).
## (s64) Multi-Agent MDP & Markov Teams
Number of References: 10

(p64.0) Consider a Markov game as in Definition 2.2 with R 1 = R 2 = · · · = R N = R, where the reward R : S × A × S → R is influenced by the joint action in A = A 1 × · · · × A N . As a result, the Q-function is identical for all agents. Hence, a straightforward algorithm proceeds by performing the standard Q-learning update (2.1) at each agent, but taking the max over the joint action space a ∈ A. Convergence to the optimal/equilibrium Q-function has been established in Szepesvári and Littman (1999); , when both state and action spaces are finite.

(p64.1) However, convergence of the Q-function does not necessarily imply that of the equilibrium policy for the Markov team, as any combination of equilibrium policies extracted at each agent may not constitute an equilibrium policy, if the equilibrium policies are non-unique, and the agents fail to agree on which one to select. Hence, convergence to the NE policy is only guaranteed if either the equilibrium is assumed to be unique , or the agents are coordinated for equilibrium selection. The latter idea has first been validated in the cooperative repeated games setting (Claus and Boutilier, 1998), a special case of Markov teams with a singleton state, where the agents are joint-action learners (JAL), maintaining a Q-value for joint actions, and learning empirical models of all others. Convergence to equilibrium point is claimed in Claus and Boutilier (1998), without a formal proof. For the actual Markov teams, this coordination has been exploited in Wang and Sandholm (2003), which proposes optimal adaptive learning (OAL), the first MARL algorithm with provable convergence to the equilibrium policy. Specifically, OAL first learns the game structure, and constructs virtual games at each state that are weakly acyclic with respect to (w.r.t.) a biased set. OAL can be shown to converge to the NE, by introducing the biased adaptive play learning algorithm for the constructed weakly acyclic games, motivated from Young (1993).
## (s67) Decentralized Paradigm with Networked Agents
Number of References: 40

(p67.0) Cooperative agents in numerous practical multi-agent systems are not always homogeneous. Agents may have different preferences, i.e., reward functions, while still form a team to maximize the return of the team-average rewardR, whereR(s, a, s ) = N −1 · i∈N R i (s, a, s ). More subtly, the reward function is sometimes not sharable with others, as the preference is kept private to each agent. This setting finds broad applications in engineering systems as sensor networks (Rabbat and Nowak, 2004), smart grid (Dall'Anese et al., 2013;, intelligent transportation systems (Adler and Blue, 2002;, and robotics (Corke et al., 2005).

(p67.1) Covering the homogeneous setting in §4.1.1 as a special case, the specified one definitely requires more coordination, as, for example, the global value function cannot be estimated locally without knowing other agents' reward functions. With a central controller, most MARL algorithms reviewed in §4.1.1 directly apply, since the controller can collect and average the rewards, and distributes the information to all agents. Nonetheless, such a controller may not exist in most aforementioned applications, due to either cost, scalability, or robustness concerns (Rabbat and Nowak, 2004;Dall'Anese et al., 2013;. Instead, the agents may be able to share/exchange information with their neighbors over a possibly time-varying and sparse communication network, as illustrated in Figure 2 (b). Though MARL under this decentralized/distributed 5 paradigm is imperative, it is relatively less-investigated, in comparison to the extensive results on distributed/consensus algorithms that solve static/one-stage optimization problems (Nedic and Ozdaglar, 2009;Agarwal and Duchi, 2011;Jakovetic et al., 2011;Tu and Sayed, 2012), which, unlike RL, involves no system dynamics, and does not maximize the long-term objective in a sequential fashion.

(p67.2) Cooperative agents in numerous practical multi-agent systems are not always homogeneous. Agents may have different preferences, i.e., reward functions, while still form a team to maximize the return of the team-average rewardR, whereR(s, a, s ) = N −1 · i∈N R i (s, a, s ). More subtly, the reward function is sometimes not sharable with others, as the preference is kept private to each agent. This setting finds broad applications in engineering systems as sensor networks (Rabbat and Nowak, 2004), smart grid (Dall'Anese et al., 2013;, intelligent transportation systems (Adler and Blue, 2002;, and robotics (Corke et al., 2005).

(p67.3) Covering the homogeneous setting in §4.1.1 as a special case, the specified one definitely requires more coordination, as, for example, the global value function cannot be estimated locally without knowing other agents' reward functions. With a central controller, most MARL algorithms reviewed in §4.1.1 directly apply, since the controller can collect and average the rewards, and distributes the information to all agents. Nonetheless, such a controller may not exist in most aforementioned applications, due to either cost, scalability, or robustness concerns (Rabbat and Nowak, 2004;Dall'Anese et al., 2013;. Instead, the agents may be able to share/exchange information with their neighbors over a possibly time-varying and sparse communication network, as illustrated in Figure 2 (b). Though MARL under this decentralized/distributed 5 paradigm is imperative, it is relatively less-investigated, in comparison to the extensive results on distributed/consensus algorithms that solve static/one-stage optimization problems (Nedic and Ozdaglar, 2009;Agarwal and Duchi, 2011;Jakovetic et al., 2011;Tu and Sayed, 2012), which, unlike RL, involves no system dynamics, and does not maximize the long-term objective in a sequential fashion.
## (s70) Other Learning Goals
Number of References: 2

(p70.0) Several other learning goals have also been explored for decentralized MARL with networked agents. Zhang et al. (2016) has considered the optimal consensus problem, where each agent over the network tracks the states of its neighbors' as well as a leader's, so that the consensus error is minimized by the joint policy. A policy iteration algorithm is then introduced, followed by a practical actor-critic algorithm using neural networks for function approximation. A similar consensus error objective is also adopted in , under the name of cooperative multi-agent graphical games. A centralized-criticdecentralized-actor scheme is utilized for developing off-policy RL algorithms.
## (s72) Competitive Setting
Number of References: 8

(p72.0) Competitive settings are usually modeled as zero-sum games. Computationally, there exists a great barrier between solving two-player and multi-player zero-sum games. In particular, even the simplest three-player matrix games, are known to be PPAD-complete (Papadimitriou, 1992;Daskalakis et al., 2009). Thus, most existing results on competitive MARL focus on two-player zero-sum games, with N = {1, 2} and R 1 + R 2 = 0 in Definitions 2.2 and 2.4. In the rest of this section, we review methods that provably find a Nash (equivalently, saddle-point) equilibrium in two-player Markov or extensive-form games. The existing algorithms can mainly be categorized into two classes: value-based and policy-based approaches, which are introduced separately in the sequel.

(p72.1) Competitive settings are usually modeled as zero-sum games. Computationally, there exists a great barrier between solving two-player and multi-player zero-sum games. In particular, even the simplest three-player matrix games, are known to be PPAD-complete (Papadimitriou, 1992;Daskalakis et al., 2009). Thus, most existing results on competitive MARL focus on two-player zero-sum games, with N = {1, 2} and R 1 + R 2 = 0 in Definitions 2.2 and 2.4. In the rest of this section, we review methods that provably find a Nash (equivalently, saddle-point) equilibrium in two-player Markov or extensive-form games. The existing algorithms can mainly be categorized into two classes: value-based and policy-based approaches, which are introduced separately in the sequel.
## (s76) Value-Based Methods
Number of References: 40

(p76.0) Under relatively stringent assumptions, several value-based methods that extend Qlearning (Watkins and Dayan, 1992) to the mixed setting are guaranteed to find an equilibrium. In particular, Hu and Wellman (2003) has proposed the Nash-Q learning algorithm for general-sum Markov games, where one maintains N action-value functions Q N = (Q 1 , . . . , Q N ) : S × A → R N for all N agents, which are updated using sample-based estimator of a Bellman operator. Specifically, letting R N = (R 1 , . . . , R N ) denote the reward functions of the agents, Nash-Q uses the following Bellman operator:

(p76.1) where Nash[Q N (s , ·)] is the objective value of the Nash equilibrium of the stage game with rewards {Q N (s , a)} a∈A . For zero-sum games, we have Q 1 = −Q 2 and thus the Bellman operator defined in (4.26) is equivalent to the one in (4.14) used by minimax-Q learning (Littman, 1994). Moreover, Hu and Wellman (2003) establishes convergence to Nash equilibrium under the restrictive assumption that Nash[Q N (s , ·)] in each iteration of the algorithm has unique Nash equilibrium. In addition,  has proposed the Friend-or-Foe Q-learning algorithm where each agent views the other agent as either a "friend" or a "foe". In this case, Nash[Q N (s , ·)] can be efficiently computed via linear programming. This algorithm can be viewed as a generalization of minimax-Q learning, and Nash equilibrium is guaranteed for two-player zero-sum games and coordination games with unique equilibria. Furthermore, Greenwald et al. (2003) has proposed correlated Qlearning, which replaces Nash[Q N (s , ·)] in (4.26) by computing a correlated equilibrium (Aumann, 1974), a more general equilibrium concept than Nash equilibrium. In a recent work, Perolat et al. (2017) has proposed a batch RL method to find an approximate Nash equilibrium via Bellman residue minimization (Maillard et al., 2010). They have proved that the global minimizer of the empirical Bellman residue is an approximate Nash equilibrium, followed by the error propagation analysis for the algorithm. Also in the batch RL regime,  has considered a simplified mixed setting for decentralized MARL: two teams of cooperative networked agents compete in a zero-sum Markov game. A decentralized variant of FQI, where the agents within one team cooperate to solve (4.3) while the two teams essentially solve (4.15), is proposed. Finite-sample error bounds have then been established for the proposed algorithm.

(p76.2) To address the scalability issue, independent learning is preferred, which, however, fails to converge in general (Tan, 1993). Arslan and Yüksel (2017) has proposed decentralized Q-learning, a two timescale modification of Q-learning, that is guaranteed to converge to the equilibrium for weakly acyclic Markov games almost surely. Each agent therein only observes local action and reward, and neither observes nor keeps track of others' actions. All agents are instructed to use the same stationary baseline policy for many consecutive stages, named exploration phase. At the end of the exploration phase, all agents are synchronized to update their baseline policies, which makes the environment stationary for long enough, and enables the convergence of Q-learning based methods. Note that these algorithms can also be applied to the cooperative setting, as these games include Markov teams as a special case.

(p76.3) Under relatively stringent assumptions, several value-based methods that extend Qlearning (Watkins and Dayan, 1992) to the mixed setting are guaranteed to find an equilibrium. In particular, Hu and Wellman (2003) has proposed the Nash-Q learning algorithm for general-sum Markov games, where one maintains N action-value functions Q N = (Q 1 , . . . , Q N ) : S × A → R N for all N agents, which are updated using sample-based estimator of a Bellman operator. Specifically, letting R N = (R 1 , . . . , R N ) denote the reward functions of the agents, Nash-Q uses the following Bellman operator:

(p76.4) where Nash[Q N (s , ·)] is the objective value of the Nash equilibrium of the stage game with rewards {Q N (s , a)} a∈A . For zero-sum games, we have Q 1 = −Q 2 and thus the Bellman operator defined in (4.26) is equivalent to the one in (4.14) used by minimax-Q learning (Littman, 1994). Moreover, Hu and Wellman (2003) establishes convergence to Nash equilibrium under the restrictive assumption that Nash[Q N (s , ·)] in each iteration of the algorithm has unique Nash equilibrium. In addition,  has proposed the Friend-or-Foe Q-learning algorithm where each agent views the other agent as either a "friend" or a "foe". In this case, Nash[Q N (s , ·)] can be efficiently computed via linear programming. This algorithm can be viewed as a generalization of minimax-Q learning, and Nash equilibrium is guaranteed for two-player zero-sum games and coordination games with unique equilibria. Furthermore, Greenwald et al. (2003) has proposed correlated Qlearning, which replaces Nash[Q N (s , ·)] in (4.26) by computing a correlated equilibrium (Aumann, 1974), a more general equilibrium concept than Nash equilibrium. In a recent work, Perolat et al. (2017) has proposed a batch RL method to find an approximate Nash equilibrium via Bellman residue minimization (Maillard et al., 2010). They have proved that the global minimizer of the empirical Bellman residue is an approximate Nash equilibrium, followed by the error propagation analysis for the algorithm. Also in the batch RL regime,  has considered a simplified mixed setting for decentralized MARL: two teams of cooperative networked agents compete in a zero-sum Markov game. A decentralized variant of FQI, where the agents within one team cooperate to solve (4.3) while the two teams essentially solve (4.15), is proposed. Finite-sample error bounds have then been established for the proposed algorithm.

(p76.5) To address the scalability issue, independent learning is preferred, which, however, fails to converge in general (Tan, 1993). Arslan and Yüksel (2017) has proposed decentralized Q-learning, a two timescale modification of Q-learning, that is guaranteed to converge to the equilibrium for weakly acyclic Markov games almost surely. Each agent therein only observes local action and reward, and neither observes nor keeps track of others' actions. All agents are instructed to use the same stationary baseline policy for many consecutive stages, named exploration phase. At the end of the exploration phase, all agents are synchronized to update their baseline policies, which makes the environment stationary for long enough, and enables the convergence of Q-learning based methods. Note that these algorithms can also be applied to the cooperative setting, as these games include Markov teams as a special case.
