# Survey on Computer Vision for UAVs: Current Developments and Trends

CorpusID: 254655778 - [https://www.semanticscholar.org/paper/9b05b7957139d1996db9537a0b631ba5d7a2b44e](https://www.semanticscholar.org/paper/9b05b7957139d1996db9537a0b631ba5d7a2b44e)

Fields: Computer Science, Engineering

## (s3) Motivation of this Review
Number of References: 2

(p3.0) The aim of this article is to provide an overview of the most important efforts in the field of computer vision for UAVs, while presenting a rich bibliography in the field that could support future reading in this emerging area. An additional goal is to gather a collection of pioneering studies that could act as a road-map for this broaden research area, towards autonomous aerial agents. Since the field of computer vision for UAVs is very generic, the depicted work will focus only in surveying the areas of: a) flight control or visual servoing, b) visual localization and mapping, and c) target tracking and obstacle detection. It should be highlighted that this article classified the aforementioned categories following the Navigation -Guidance -Control scheme. The big picture is to provide a significant insight for the entire autonomous system collecting all the pieces together. The concept of navigation monitors the motion of the UAV from one place to another processing sensor data. Through this procedure the UAV can extract essential information for it's state (kinematics and dynamics -state estimation), build a model of its surroundings (mapping and obstacle detection) and even track sequential objects of interest (target tracking) to enhance the perception capabilities. Thus, by combining localization and perception capabilities, the robotic platforms are enabled for Guidance tasks. In the Guidance system, the platform processes information from perception and localization parts to decide its next move according to specified task. In this category trajectory generation and path planning are included for motion planning, mission-wise decision making or unknown area exploration. Finally, the realization of actions derived from Navigation and Guidance tasks is performed within the Control section. The controller manipulates the inputs to provide the desired output enabling actuators for force and torque production to control the vehicle's motion. Generally, different controllers have been proposed to fulfill mission enabled requirements (position, attitude, velocity and acceleration control). In the following sections the major works that employ visual sensors for each defined category will be presented, while the Navigation, Guidance and Control [18] overview scheme is provided in Fig. 3.
## (s8) Visual Localization and Mapping
Number of References: 24

(p8.0) The scope of localization and mapping for an agent is the method to localize itself locally, estimate its state and build a 3D model of its surroundings by employing among others vision sensors [48]. In Fig. 6, some visual mapping examples are depicted such as: a) [49], b) [50], c) [51]. In a) dense 3D reconstruction from downward looking camera from MAV is demonstrated, while in b) a complete aerial setup towards autonomous exploration is presented. The map shown in Fig. 6 is an occupancy map. The system relies on a stereo camera and a downward looking camera for visual inertial odometry and mapping. Similarly, in c) another approach for autonomous exploration is described, where the system uses a stereo camera and an inertial sensor for the pose estimation and mapping. The Figure depicts the image raw streams, the occupancy map and the dense pointcloud. The rest of this section briefly provides an overview of the contributions in this field. Towards this direction in [52], a visual pose estimation system from multiple cameras on-board a UAV, known as Multi-Camera Parallel Tracking and Mapping (PTAM) has been presented. This solution was based on the monocular PTAM and was able to integrate concepts from the field of multi-camera ego-motion estimation. Additionally, in this work a novel extrinsic parameter calibration method for nonoverlapping field of view cameras has been proposed.

(p8.1) The combination of a visual graph-SLAM, with a multiplicative EKF for GPS-denied navigation, has been presented in [53]. A RGB-D camera, an IMU and an altimeter sensor have been mounted on-board the UAV, while the system consisted of two subsystems, one with major priority for the UAV navigation and another for the mapping, with the first one being responsible for tasks like visual odometry, sensor fusion and vehicle control.

(p8.2) In [54] a semi-direct monocular visual odometry algorithm for UAV state estimation has been described. The proposed approach is divided in two subsystems regarding motion estimation and mapping. The first thread implements a novel pose estimation approach consisting of three parts, image alignment though minimization of photometric error between pixels, 2D feature alignment to refine 2D point coordinates and finally minimization of the reprojection error to refine pose and structure for the camera. In the second thread a probabilistic depth filter is employed for each extracted 2D feature to estimate it's 3D position. As a continuation, the authors in [55] proposed a system for real time 3D reconstruction and landing spot detection. In this work a monocular approach uses only an onboard smartphone processor for semi direct visual odometry [54], multi sensor fusion [56] and a modified version of Regularized Modular Depth Estimation (REMODE) [57]. The depth maps are merged to build the elevation map in a robot centric approach. Afterwards, the map can be used for path planning tasks. Specifically, experimental trials were performed to demonstrate autonomous landing detecting a safe flat area in the elevation map. Additionally, in [49] a system that integrated SVO odometry in an aerial platform used for trajectory following and dense 3D mapping have been presented. The pose estimations from visual odometry was fused with IMU measurements to enhance the state estimation used by the controllers to stabilize the vehicle and navigate through the path. It should be highlighted that the biases of the IMU where estimated online. The estimated position and orientation were close to ground truth values with small deviations.
## (s9) Obstacle Detection
Number of References: 12

(p9.0) Obstacle detection and avoidance capabilities of UAVs are essential towards autonomous navigation. This capability is of paramount importance in classical mobile robots, however, this is transformed into a huge necessity in the special case of autonomous aerial vehicles in order to implement algorithms that generate collision free paths, while significantly increasing the UAV's autonomy, especially in missions where there is no line of sight. Figure 7 presents visualized obstacle free paths a) [50],b) [91] c) [92], d) [93]. In this figure different obstacle detection and avoidance approaches are presented, where a), b) and c) depict identified obstacles in 3D and d) in 2D. Additionally, b) and d) demonstrate the trajectory followed to avoid objects.

(p9.1) In [93] a novel stereo vision-based obstacle avoidance technique for MAV tasks was introduced. Two stereo camera systems and an IMU were mounted on the quadrotor. Initially the stereo rigs were tightly hardware synchronized and were designed to build a 3D global obstacle map of the environment, using 3D virtual scans derived from processed range data. The second part of this approach consisted of a dynamic  path planning algorithm called Anytime Dynamic A*, which recomputed in every step a suboptimal path to the UAVs goal point. This path planner utilized the data form the obstacle map and was able to re-plan the current path.

(p9.2) In [94] a monocular based feature estimation algorithm for terrain mapping was presented, which performed obstacle avoidance for UAVs. The proposed method utilized an EKF to estimate the location of image features in the environment, with the major advantage to be the fast depth convergence of estimated feature points, which was succeeded by the utilization of inverse depth parameterization. In the presented approach, the converged points have been stored in an altitude map, which has been also used for performing the obstacle avoidance operation.
## (s31) Motivation of this Review
Number of References: 2

(p31.0) The aim of this article is to provide an overview of the most important efforts in the field of computer vision for UAVs, while presenting a rich bibliography in the field that could support future reading in this emerging area. An additional goal is to gather a collection of pioneering studies that could act as a road-map for this broaden research area, towards autonomous aerial agents. Since the field of computer vision for UAVs is very generic, the depicted work will focus only in surveying the areas of: a) flight control or visual servoing, b) visual localization and mapping, and c) target tracking and obstacle detection. It should be highlighted that this article classified the aforementioned categories following the Navigation -Guidance -Control scheme. The big picture is to provide a significant insight for the entire autonomous system collecting all the pieces together. The concept of navigation monitors the motion of the UAV from one place to another processing sensor data. Through this procedure the UAV can extract essential information for it's state (kinematics and dynamics -state estimation), build a model of its surroundings (mapping and obstacle detection) and even track sequential objects of interest (target tracking) to enhance the perception capabilities. Thus, by combining localization and perception capabilities, the robotic platforms are enabled for Guidance tasks. In the Guidance system, the platform processes information from perception and localization parts to decide its next move according to specified task. In this category trajectory generation and path planning are included for motion planning, mission-wise decision making or unknown area exploration. Finally, the realization of actions derived from Navigation and Guidance tasks is performed within the Control section. The controller manipulates the inputs to provide the desired output enabling actuators for force and torque production to control the vehicle's motion. Generally, different controllers have been proposed to fulfill mission enabled requirements (position, attitude, velocity and acceleration control). In the following sections the major works that employ visual sensors for each defined category will be presented, while the Navigation, Guidance and Control [18] overview scheme is provided in Fig. 3.
## (s36) Visual Localization and Mapping
Number of References: 24

(p36.0) The scope of localization and mapping for an agent is the method to localize itself locally, estimate its state and build a 3D model of its surroundings by employing among others vision sensors [48]. In Fig. 6, some visual mapping examples are depicted such as: a) [49], b) [50], c) [51]. In a) dense 3D reconstruction from downward looking camera from MAV is demonstrated, while in b) a complete aerial setup towards autonomous exploration is presented. The map shown in Fig. 6 is an occupancy map. The system relies on a stereo camera and a downward looking camera for visual inertial odometry and mapping. Similarly, in c) another approach for autonomous exploration is described, where the system uses a stereo camera and an inertial sensor for the pose estimation and mapping. The Figure depicts the image raw streams, the occupancy map and the dense pointcloud. The rest of this section briefly provides an overview of the contributions in this field. Towards this direction in [52], a visual pose estimation system from multiple cameras on-board a UAV, known as Multi-Camera Parallel Tracking and Mapping (PTAM) has been presented. This solution was based on the monocular PTAM and was able to integrate concepts from the field of multi-camera ego-motion estimation. Additionally, in this work a novel extrinsic parameter calibration method for nonoverlapping field of view cameras has been proposed.

(p36.1) The combination of a visual graph-SLAM, with a multiplicative EKF for GPS-denied navigation, has been presented in [53]. A RGB-D camera, an IMU and an altimeter sensor have been mounted on-board the UAV, while the system consisted of two subsystems, one with major priority for the UAV navigation and another for the mapping, with the first one being responsible for tasks like visual odometry, sensor fusion and vehicle control.

(p36.2) In [54] a semi-direct monocular visual odometry algorithm for UAV state estimation has been described. The proposed approach is divided in two subsystems regarding motion estimation and mapping. The first thread implements a novel pose estimation approach consisting of three parts, image alignment though minimization of photometric error between pixels, 2D feature alignment to refine 2D point coordinates and finally minimization of the reprojection error to refine pose and structure for the camera. In the second thread a probabilistic depth filter is employed for each extracted 2D feature to estimate it's 3D position. As a continuation, the authors in [55] proposed a system for real time 3D reconstruction and landing spot detection. In this work a monocular approach uses only an onboard smartphone processor for semi direct visual odometry [54], multi sensor fusion [56] and a modified version of Regularized Modular Depth Estimation (REMODE) [57]. The depth maps are merged to build the elevation map in a robot centric approach. Afterwards, the map can be used for path planning tasks. Specifically, experimental trials were performed to demonstrate autonomous landing detecting a safe flat area in the elevation map. Additionally, in [49] a system that integrated SVO odometry in an aerial platform used for trajectory following and dense 3D mapping have been presented. The pose estimations from visual odometry was fused with IMU measurements to enhance the state estimation used by the controllers to stabilize the vehicle and navigate through the path. It should be highlighted that the biases of the IMU where estimated online. The estimated position and orientation were close to ground truth values with small deviations.
## (s37) Obstacle Detection
Number of References: 12

(p37.0) Obstacle detection and avoidance capabilities of UAVs are essential towards autonomous navigation. This capability is of paramount importance in classical mobile robots, however, this is transformed into a huge necessity in the special case of autonomous aerial vehicles in order to implement algorithms that generate collision free paths, while significantly increasing the UAV's autonomy, especially in missions where there is no line of sight. Figure 7 presents visualized obstacle free paths a) [50],b) [91] c) [92], d) [93]. In this figure different obstacle detection and avoidance approaches are presented, where a), b) and c) depict identified obstacles in 3D and d) in 2D. Additionally, b) and d) demonstrate the trajectory followed to avoid objects.

(p37.1) In [93] a novel stereo vision-based obstacle avoidance technique for MAV tasks was introduced. Two stereo camera systems and an IMU were mounted on the quadrotor. Initially the stereo rigs were tightly hardware synchronized and were designed to build a 3D global obstacle map of the environment, using 3D virtual scans derived from processed range data. The second part of this approach consisted of a dynamic  path planning algorithm called Anytime Dynamic A*, which recomputed in every step a suboptimal path to the UAVs goal point. This path planner utilized the data form the obstacle map and was able to re-plan the current path.

(p37.2) In [94] a monocular based feature estimation algorithm for terrain mapping was presented, which performed obstacle avoidance for UAVs. The proposed method utilized an EKF to estimate the location of image features in the environment, with the major advantage to be the fast depth convergence of estimated feature points, which was succeeded by the utilization of inverse depth parameterization. In the presented approach, the converged points have been stored in an altitude map, which has been also used for performing the obstacle avoidance operation.
