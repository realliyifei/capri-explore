# AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models

CorpusID: 233481730 - [https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c](https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c)

Fields: Linguistics, Medicine, Computer Science

## (s2) FOUNDATIONS
### Question: What are the core components and their functions in transformer-based PLMs like BERT?

#Reference=7 ['b26', 'b27', 'b28', 'b29', 'b26', 'b30', 'b31']

(p2.0) In general, the core components of transformer-based PLMs like BERT and RoBERTa are embedding and transformer encoder layers (refer Figure 4). The embedding layer takes input tokens and returns a vector for each. The embedding layer has three or more sub-layers each of which provides a vector of specific embedding type for each of the input tokens. The final input vector for each token is obtained by summing all the vectors of each embedding type. The transformer encoder layer enhances each input token vector by encoding global contextual information using the self-attention mechanism. By applying a sequence of such transformer encoder layers, the model can encode complex language information in the input token vectors. Usually, the embedding layer consists of three sublayers with each sub-layer representing a particular embedding type. In some models, there are more than three also. For example, embedding layer of BERT-EHR [27] contains code, position, segment, age and gender embeddings. A detailed description of various embedding types is presented in Section 3.4. The first sublayer converts input tokens to a sequence of vectors while the other two sub-layers provide auxiliary information like position and segmentation. The first sublayer can be char, sub-word, or code embedding based. For example, BioCharBERT [28] uses CharCNN [29] on the top of character embeddings, BERT uses WordPiece [30] embeddings while BEHRT [27], MedBERT [31] and BERT-EHR [32] models use code embeddings. Unlike BioCharBERT and BERT models, the input for BEHRT, MedBERT, BERT-EHR models is patient visits where each patient visit is expressed as a sequence of codes. The final input representation X for the given input tokens {x 1 , x 2 , . . . x n }is obtained by adding the embeddings from the three sub-layers (for simplicity, we have included only three embedding types -refer Figure 5).

## (s10) Self-Supervised Learning
### Question: What is self-supervised learning and how is it applied across AI fields?

#Reference=10 ['b33', 'b12', 'b13', 'b25', 'b34', 'b35', 'b36', 'b37', 'b33', 'b33']

(p10.1) Robotics is the first AI field to use self-supervised learning methods [34]. Over the last five years, selfsupervised learning has become popular in other AI fields like natural language processing [13], [14], [26], computer vision [35], [36], and speech processing [37], [38]. SSL is a new learning paradigm that draws inspiration from both supervised and unsupervised learning methods. SSL is similar to unsupervised learning as it does not depend on human-labeled instances. It is also similar to supervised learning as it learns using supervision. However, in SSL the supervision is provided by the pseudo labels which are generated automatically from the pretraining data. SSL involves pretraining the model over a large unlabelled corpus using one or more pretraining tasks. The pseudo labels are generated depending on the definitions of pre-training tasks. SSL methods fall into three categories namely Generative, Contrastive, and Generate-Contrastive [34]. In Generative SSL, encoder maps input vector x to vector y, and decoder recovers x from y (e.g., masked language modeling). In Contrastive SSL, the encoder maps input vector x to vector y to measure similarity (e.g., mutual information maximization). In Generate-Contrastive SSL, fake samples are generated using encoder-decoder while the discriminator identifies the fake samples (e.g., replaced token detection). For more details about different SSL methods, please refer to the survey paper written by Liu et al. [34].

## (s13) Mixed-Domain Pretraining (MDPT)
### Question: What is Continual Pretraining in biomedical NLP research?

#Reference=2 ['b15', 'b17']

(p13.1) Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs. It is also referred to as further pretraining. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7). For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles [16]. In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].

## (s14) Domain-Specific Pretraining (DSPT)
### Question: What is the main drawback of continual pretraining in domain-specific language models?

#Reference=4 ['b1', 'b19', 'b42', 'b43']

(p14.0) The main drawback in continual pretraining is the general domain vocabulary. For example, the WordPiece vocabulary in BERT is learned over English Wikipedia and Books Corpus [2]. As a result, the vocabulary does not represent the biomedical domain and hence many of the biomedical words are split into several subwords which hinders the model learning during pretraining and fine-tuning. Moreover, the length of the input sequence also increases as many of the in-domain words are split into several subwords. DSPT over in-domain text allows the model to have the in-domain vocabulary (refer Figure 10). For example, PubMedBERT is trained from scratch using PubMed abstracts and PMC full-text articles [20]. PubMed achieved state-of-the-art results in the BLURB benchmark. Similarly, RoBERTa-base-PM-M3-Voc is trained from scratch over PubMed and PMC and MIMIC-III clinical notes [43]. is based on the hypothesis that pretraining over taskrelated unlabelled text allows the model to learn both domain and task-specific knowledge [44] (refer Figure  11). In TAPT, task-related unlabelled sentences are gathered, and then the model is further pretrained. TAPT is less expensive compared to other pretraining methods as it involves pretraining the model over a relatively small corpus of task-related unlabelled sentences.

## (s16) Pretraining Tasks
### Question: What do language models learn during pretraining and how are pretraining tasks classified?

#Reference=3 ['b33', 'b44', 'b46']

(p16.0) During pretraining, the language models learn language representations based on the supervision provided by one or more pretraining tasks. A pretraining task is a pseudo-supervised task whose labels are generated automatically. A pretraining task can be main or auxiliary. The main pretraining tasks allow the model to learn language representations while auxiliary pretraining tasks allow the model to gain knowledge from human-curated sources like Ontology [34], [45]- [47]. The classification of pretraining tasks is given in Figure 12 and a brief summary of various pretraining tasks is presented in Table 1.

## (s19) Auxiliary Pretraining Tasks
### Question: How do auxiliary pretraining tasks enhance in-domain models with human-curated knowledge sources like UMLS?

#Reference=4 ['b53', 'b44', 'b44', 'b46']

(p19.0) Auxiliary pretraining tasks help to inject knowledge from human-curated sources like UMLS [54] into indomain models to further enhance them. For example, the triple classification pretraining task involves identifying whether two concepts are connected by the relation or not [45]. This auxiliary task is used by Hao et al. [45] to inject UMLS relation knowledge into in-domain models. Yuan et al. [47] used two auxiliary pretraining tasks based on multi-similarity Loss and Knowledge embedding loss to further pretrain BioBERT on UMLS.

## (s21) Intermediate Fine-Tuning (IFT)
### Question: How does IFT improve model performance on small target datasets?

#Reference=6 ['b54', 'b57', 'b54', 'b58', 'b59', 'b60']

(p21.0) IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets. IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]- [58]. For example, Cengiz et al. [55] fine-tuned indomain model on general NLI datasets like SNLI [59] and MNLI [60] before fine-tuning on MedNLI [61].

## (s22) Multi-Task Fine-Tuning
### Question: What are the benefits and limitations of multi-task fine-tuning in machine learning models?

#Reference=9 ['b67', 'b69', 'b69', 'b70', 'b70', 'b71', 'b71', 'b72', 'b73']

(p22.0) Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. At the same time, due to the increase in training set size, the model is less prone to over-fitting. Multi-task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]. Moreover, having a single model for multitasks eliminates the need of deploying separate models for each task which saves computational resources, time, and deployment costs [70]. Multi-task fine-tuning may not provide the best results all the time [70]. In such cases, multi-task fine tuning can be applied iteratively to identify the best possible subset of tasks [71]. For example, Mahanjan et al. [71] applied multi-task finetuning iteratively to choose the best subset of related datasets. Finally, the authors fine-tuned the model on best subset of related datasets and achieved the best results on the Clinical STS [72] dataset. After multi-task fine-tuning, the model can be further fine-tuned on the target specific dataset separately to further enhance the performance of the model [73].

## (s24) Main Embeddings
### Question: What are the advantages and disadvantages of using character embeddings in models?

#Reference=4 ['b74', 'b27', 'b75', 'b27']

(p24.1) Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only. Each character is represented using an embedding. These embeddings are initialized randomly and learned during model pretraining. ELMo embedding model uses CharCNN to generate word representations from character embeddings [74]. Inspired by ELMo, BioCharBERT also uses CharCNN on the top of character embeddings to generate word representations [28]. AlphaBERT [75] also uses character embeddings. Unlike CharacterBERT, AlphaBERT directly combines character embeddings with position embeddings and then applies a stack of transformer encoders. The main advantage with character embeddings is the small size of vocabulary as it includes only characters. The disadvantage is longer pretraining times [28]. As the sequence length increases with character level embeddings, models are slow to pre-train.

### Question: What are the principles and popular algorithms of subword embeddings?

#Reference=5 ['b76', 'b77', 'b29', 'b78', 'b79']

(p24.2) Subword Embeddings-In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words. The main principle driving the subword embedding vocabulary construction is that frequent words should be represented as a single word and rare words should be represented in terms of meaningful subwords. Subword embedding vocabularies are always moderate in size as they use sub-words to represent rare and misspelled words. Some of the popular algorithms to generate vocabulary for sub-word embeddings are Byte-Pair Encoding (BPE) [76], Byte-Level BPE [77], Word-Piece [30], Unigram [78], and Sentencepiece [79].

### Question: How does SentencePiece address space assumption issues in BPE and WordPiece for language models?

#Reference=3 ['b79', 'b81', 'b14']

(p24.5) SentencePiece [79] -A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space. However, this assumption is not applicable in all languages. To overcome this, SentencePiece treats space as a character and includes it in the base vocabulary. The final vocabulary is generated iteratively using BPE or Unigram. XLNet [81], ALBERT [15], and T5 [4] models use SentencePiece embeddings.

### Question: What are code embeddings and how do they vary among models like BERT-EHR, MedBERT, and BEHRT?

#Reference=3 ['b31', 'b30', 'b26']

(p24.7) Code embeddings -Code embeddings map the given sequence of codes into a sequence of vectors. For example, in the case of models like BERT-EHR [32], MedBERT [31], and BEHRT [27], the input is not a sequence of words. Instead, input is patient visits. Each patient visit is represented as a sequence of codes. The number of code embeddings varies from model to model. For example, MedBERT and BEHRT include embeddings only for disease codes while BERT-EHR includes embeddings for disease, medication, procedure, and clinical notes.

## (s25) Auxiliary Embeddings
### Question: What is the purpose of auxiliary embeddings in enhancing model learning?

#Reference=3 ['b26', 'b31', 'b1']

(p25.0) Main embeddings represent the given input sequence in low dimensional space. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings. The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings. Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].

## (s29) Radiology Reports
### Question: What led to the development of radiology-specific PLMs?

#Reference=2 ['b90', 'b89']

(p29.0) Following the success of EHR-based T-BPLMs, recently researchers focused on developing PLMs specifically for radiology reports. RadCore [90] dataset consists of around 2 million radiology reports. These reports were gathered from three major healthcare organizations: Mayo Clinic, MD Anderson Cancer Center, and Medical College of Wisconsin in 2007. Meng et al. [89] further pre-trained general BERT on radiology reports with impression section headings from RadCore dataset    Table 3 contains a summary of radiology reports-based T-BPLMs.

## (s30) Social Media
### Question: How has social media influenced health-related research and services in the last decade?

#Reference=10 ['b107', 'b108', 'b109', 'b110', 'b92', 'b92', 'b94', 'b95', 'b95', 'b95']

(p30.0) In the last decade, social media has become the first choice for internet users to express their thoughts. Apart from views about general topics, various social media platforms like Twitter, Reddit, AskAPatient, WebMD are used to share health-related experiences in the form of tweets, reviews, questions, and answers [107], [108]. Recent works have shown that health-related social media data is useful in many applications to provide better health-related services [109], [110]. The performance of general T-PLMs on Wikipedia and Books Corpus is limited on health-related social media datasets [92]. This is because social media text is highly informal with a lot of nonstandard abbreviations, irregular grammar, and typos. Researchers working at the intersection of social media and health, trained social media text-based T-BPLMs to handle social media texts. CT-BERT [92] is initialized from BERT-large and further pretrained on a corpus of 22.5M covid related tweets and this model showed up to 30% improvement compared to BERT-large, on five different classification datasets. BioRedditBERT [94] is initialized from BioBERT and further pretrained on healthrelated Reddit posts. The authors showed that BioRed-ditBERT outperforms in-domains models like BioBERT, BlueBERT, PubMedBERT, ClinicalBERT by up to 1.8% in normalizing health-related entity mentions. RuDR-BERT [95] is initialized from Multilingual BERT and pretrained on the raw part of the RuDReC corpus (1.4M reviews). The authors showed that RuDR-BERT outperforms multilingual BERT and Russian BERT on Russian sentence classification and clinical entity extraction datasets by large margins. EnRuDR-BERT [95] and EnDR-BERT [95] are obtained by further pretraining multilingual BERT on Russian and English health reviews and English health reviews respectively. Table 4 contains summary of social media text-based BPLMs.

## (s31) Scientific Literature
### Question: Why is biomedical text mining becoming more popular in research?

#Reference=4 ['b15', 'b15', 'b42', 'b96']

(p31.0) In the last few decades, the amount of biomedical literature is growing at a rapid scale. As knowledge discovery from biomedical literature is useful in many applications, biomedical text mining is gaining popularity in the research community [16]. However, biomedical text significantly differs from the general text with a lot of domain-specific words. As a result, the performance of general T-PLMs is limited in many of the tasks. So, biomedical researchers focused on developing in-domain T-PLMs to handle biomedical text. PubMed and PMC are the two popular sources of biomedical text. PubMed con-  tains only biomedical literature citations and abstracts only while PMC contains full-text biomedical articles. As of March 2020, PubMed includes 30M citations and abstracts while PMC contains 7.5M full-text articles. Due to the large collection and broad coverage, these two are the first choice to pretrain T-BPLMs [16], [43], [96].

### Question: How are biomedical pre-trained language models like BioBERT developed from general BERT?

#Reference=3 ['b15', 'b97', 'b100']

(p31.1) As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text. BioBERT [16] is the first biomedical pre-trained language model which is obtained by further pretraining general BERT on biomedical literature. BioBERTpt-bio [97] is obtained by further pretraining BERT Multilingual (base) on Brazilian biomedical corpus -scientific papers from PubMed (0.8M-only literature titles) + Scielo (health -12.4M + biological-3.2M: both titles and abstracts). BioMedBERT [100] is obtained by further pretraining BERT-large on BREATHE 1.0 corpus. BioMedBERT outperformed BioBERT on biomedical question answering.

## (s32) Hybrid Corpora
### Question: What challenges exist in pretraining transformer-based PLMs with in-domain text?

#Reference=9 ['b87', 'b88', 'b40', 'b104', 'b17', 'b27', 'b42', 'b97', None]

(p32.0) It is difficult to obtain a large amount of in-domain text in some cases. For example, MIMIC [87], [88] is the largest publicly available dataset of medical records.  The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC). However, to pretrain a transformer-based PLM from scratch, we require large volumes of text. To overcome this, some of the models are pretrained on general + in-domain text [41], [104] or, in-domain + related domain text [18], [28], [43], [97]. For example, BERT (jpCR+jpW) [ Table 6 contains hybrid corpora-based T-BPLMs.

## (s34) Language-Specific
### Question: How are language-specific T-BPLMs developed following the success of English biomedical BERT models?

#Reference=10 ['b40', 'b91', 'b111', 'b95', 'b97', 'b47', 'b91', 'b112', 'b112', 'b47']

(p34.0) Following the success of BioBERT, ClinicalBERT, Pub-MedBERT in English biomedical tasks, researchers focused on developing T-BPLMs for other languages also by pretraining from scratch [41], [91], [111] or pretraining from Multilingual BERT [95], [97] or pretraining from monolingual BERT [48], [91], [112] models. For example, CHMBERT [112] is the first Chinese medical BERT model which is initialized from the general Chinese BERT model and further pretrained on a huge (185GB) corpus of Chinese medical text gathered from more than 100 hospitals. MC-BERT [48] is also initialized from general Chinese BERT and further pre-trained on hybrid corpora which includes general, biomedical, and medical texts. Table 7 contains a summary of language-specific T-BPLMs.

## (s36) Green Models
### Question: How do CPT and DSPT/SPT adapt T-PLMs to in-domain tasks and their limitations?

#Reference=2 ['b122', 'b123']

(p36.0) CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords. This kind of representation increases the overall length of the input as well as hinders the model learning. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text. However, both these approaches involve learning the model parameters from scratch which is highly expensive. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].

### Question: What are Green Models and how do they adapt T-PLMs for the biomedical domain?

#Reference=4 ['b122', 'b123', 'b122', 'b123']

(p36.1) Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123]. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction. This approach is completely inexpensive as it requires only CPU. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive. Table 9 contains summary of Green T-BPLMs.

## (s37) Debiased Models
### Question: How do T-PLMs exhibit and mitigate bias in decision-making?

#Reference=4 ['b124', 'b126', 'b127', 'b105']

(p37.0) T-PLMs are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. Zhang et al. [127] further pretrained SciBERT [105] on MIMIC-III clinical notes and showed that the performance of the model is different for different groups. The authors applied adversarial pretraining debiasing to reduce the gender bias in the model. The authors released both the models publicly to encourage further research in debiasing T-BPLMs.

## (s40) Natural Language Inference
### Question: What is Natural Language Inference and its significance in NLP?

#Reference=12 ['b136', 'b137', 'b138', 'b139', 'b66', 'b60', 'b88', 'b140', 'b15', 'b54', 'b58', 'b59']

(p40.0) Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics. It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence. Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval [136] in the general domain and medical concept normalization [137], [138], semantic relatedness [139], question answering [66] in the biomedical domain. NLI is framed as a three-way sentence pair classification problem. Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair. MedNLI [61] is the in-domain NLI dataset with around 14k instances generated from MIMIC-III [88] clinical notes. As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al. [140] further pretrained BioBERT [16] on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45%. Cengiz et al. [55] applied an ensemble of two BioBERT models and achieved an accuracy of 84.7%. The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI [59] and MultiNLI [60] and then finetuned them on MedNLI.

## (s41) Entity Extraction
### Question: What are the approaches and advancements in entity extraction techniques?

#Reference=21 ['b62', 'b149', 'b150', 'b150', 'b62', 'b151', 'b153', 'b52', 'b62', 'b151', 'b151', 'b152', 'b52', 'b104', 'b40', 'b1', 'b62', 'b63', 'b154', 'b63', 'b62']

(p41.1) Most of the existing work approaches entity extraction as sequence labeling or machine reading comprehension. In case of sequence labelling approach, BERT based models generate contextual representations for each token and then softmax layer [62], [149], [150], BiLSTM+Softmax [150], BiLSTM+CRF [62], [151]- [153] or CRF [53], [62], [151] is applied. Recent works showed adding BiLSTM on the top of the BERT model does not show much difference in performance [151], [152]. This is because transformer encoder layers in BERT based models do the same job of encoding contextual in token representations like BiLSTM. Some of the works experimented with general BERT for extracting clinical and biomedical entities. For example, Portelli et al. [53] showed that SpanBERT+CRF outperformed in-domain BERT models also in extracting clinical entities in social media text. Boudjellal et al. [104] developed ABioNER by further pretraining AraBERT [41] on general Arabic corpus + biomedical Arabic text and showed that ABioNER outperformed both multilingual BERT [2] and AraBERT on Arabic biomedical entity extraction. This shows that further pretraining general AraBERT on small in-domain text corpus improves the performance of the model. As in-domain datasets are comparatively small, some of the recent works [62], [63], [154] initially fine-tuned the models on similar datasets before fine-tuning on small target datasets. This intermediate fine-tuning allows the model to learn more task-specific knowledge which improves the performance of the model on small target datasets. For example, Gao et al. [63] proposed a novel approach entity extraction approach based on intermediate fine-tuning and semi-supervised learning. Here intermediate fine-tuning allows the model to learn more task-specific knowledge while semi-supervised learning allows to leverage task-related unlabelled data by assigning pseudo labels. Recently Sun et al. [62] formulated biomedical entity extraction as question answering and showed that BioBERT+QA outperformed BioBERT+ (Softmax / CRF / BiLSTM-CRF) on six datasets.

## (s42) Semantic Textual Similarity
### Question: What is Semantic Textual Similarity and its applications in natural language processing?

#Reference=10 ['b139', 'b137', 'b138', 'b155', 'b65', 'b156', 'b157', 'b136', 'b158', 'b72']

(p42.0) Semantic Textual Similarity quantifies the degree of semantic similarity between two phrases or sentences. Unlike NLI which classifies the given sentence pair into one of three classes, semantic textual similarity returns a value that indicates the degree of similarity. Both NLI and STS require sentence-level semantics. STS is useful in tasks like concept relatedness [139], medical concept normalization [137], [138] , duplicate text detection [155], question answering [65], [156] and text summarization [157]. Moreover, Reimers et al. [136] showed that training transformer-based PLMs on STS datasets allow the model to learn sentence-level semantics and hence better represent variable-length texts like phrases or sentences. Models like BERT learn the joint representation of given sentence pair and a task-specific sigmoid layer gives the similarity value. BIOSSES [158] and Clinical STS [72] are the commonly used datasets to train and evaluate indomain STS models.

### Question: How have recent studies improved clinical STS model performance?

#Reference=14 ['b55', 'b56', 'b65', 'b159', 'b55', 'b55', 'b56', 'b65', 'b71', 'b160', 'b161', 'b155', 'b71', 'b159']

(p42.1) Recent works exploited general models for clinical STS [56], [57], [65], [159]. For example, Yang et al. [56] achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model. As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets [56], [57], [65], [71]. Xiong et al. [160] enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE [161] based entity representations. Mutinda et al. [155] achieved a Pearson correlation score of 0.8320 by finetuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets. Mahajan et al. [71] proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset. Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning. The main drawback in the above existing works is giving '[CLS]' vector as sentence pair representation to the sigmoid layer. This is because the '[CLS]' vector contains only partial information. Unlike existing works, Wang et al. [159] applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results.

## (s43) Relation Extraction
### Question: What is relation extraction and its significance in information extraction?

#Reference=12 ['b162', 'b163', 'b164', 'b165', 'b141', 'b166', 'b167', 'b39', 'b168', 'b169', 'b170', 'b171']

(p43.0) Relation extraction is a crucial task in information extraction which identifies the semantic relations between entities in text. Entity extraction followed by relation extraction helps to convert unstructured text into structured data. Extracting relations between entities is useful in many tasks like knowledge graph construction, text summarization, and question answering. Some of relation extraction datasets are I2B2 2012 [162], AIMED [163], ChemProt [164], DDI [165], I2B2 2010 [141] and EU-ADR [166]. Wei et al. [167] achieved the best results on two datasets using MIMIC-BERT [40] +Softmax. Thillaisundaram and Togia [168] applied SciBERT +Softmax to extract relations from biomedical abstracts as part of AGAC track of BioNLP-OST 2019 shared tasks [169]. Liu et al. [170] proposed SciBERT+Softmax for relation extraction in biomedical text. They showed SciBERT+Softmax outperforms BERT+Softmax on three biomedical relation extraction datasets. The main drawback in the above existing works is that they utilize only the partial knowledge from the last layer in the form of '[CLS]' vector. Su et al. [171] added attention on the top of BioBERT to fully utilize the information in the last layer and achieved the best results on three biomedical extraction datasets. The authors generated the final representation by concatenating '[CLS]' vector and weighted sum vector of final hidden state vectors. When compared to applying LSTM on the final hidden state vectors, the attention layer gives better results.

## (s44) Text Classification
### Question: How does incorporating attention in BERT models enhance clinical text classification performance?

#Reference=4 ['b174', 'b175', 'b176', 'b177']

(p44.1) Shen et al. [173] applied various in-domain BERT for Alzheimer disease clinical notes classification and achieved the best results using PubMedBERT and BioBERT. They generated labels for the training instances using a rule-based NLP algorithm. Chen et al. [174] further pretrained the general BERT model on 1.5 drugrelated tweets and showed that further pretraining improves the performance of the model on ADR tweets classification. Recent works [175], [176] showed that adding attention on the top of the BERT model improves the performance of the model in clinical text classification. They introduced a custom attention model to aggregate the encoder output and showed that this leads to improved performance and interpretability.

## (s45) Question Answering
### Question: What are the challenges and advancements in biomedical question answering systems?

#Reference=15 ['b178', 'b179', 'b180', 'b181', 'b182', 'b183', 'b184', 'b51', 'b57', 'b184', 'b66', 'b185', 'b184', 'b57', 'b185']

(p45.0) Question Answering (QA) aims to extract answers for the given queries. QA helps to quickly find information in clinical notes or biomedical literature and thus saves a lot of time. The main challenge in developing automated QA systems for the clinical or biomedical domain is the small size of datasets. Developing large QA datasets in the clinical or biomedical domain is quite expensive and requires a lot of time also. Some of the popular in-domain QA datasets are emrQA [177], CliCR [178], PubMedQA [179] COVID-QA [180], MASH-QA [181] and Health-QA [182]. Chakraborty et al. [183] showed BioMedBERT obtained by further pretraining BERT-Large on BREATHE 1.0 corpus outperformed BioBERT on biomedical question answering. The main reason for this is the diversity of text in BREATHE 1.0 corpus. Pergola et al. [52] introduced Biomedical Entity Masking (BEM) which allows the model to learn entity-centric knowledge during further pretraining. They showed that BEM improved the performance of both general and indomain models on two in-domain QA datasets. Recent works used intermediate fine-tuning on general QA [58], [183] or NLI [66] datasets or multi-tasking [184] to improve the performance of in-domain QA models. For example, Soni et al. [183] achieved the best results on a) CliCR by intermediate fine-tuning on SQuaD using BioBERT and b) emrQA by intermediate fine-tuning on SQuaD and CliCR using Clinical BERT. Yoon et al. [58] showed that intermediate fine-tuning on general domain Squad datasets improves the performance on biomedical question answering datasets. Akdemir et al. [184] proposed a novel multi-task model based on BioBERT for biomedical question answering. They used biomedical NER as an auxiliary task and showed that transfer learning from the bioNER task improves performance on question answering tasks.

## (s46) Text Summarization
### Question: Why is automatic biomedical text summarization important for researchers?

#Reference=5 ['b186', 'b187', 'b188', 'b189', 'b190']

(p46.0) In general, sources of biomedical information like clinical notes, scientific papers, radiology reports are length in nature. Researchers and domain experts need to go through a number of biomedical documents. As biomedical documents are length in nature, there is a need for automatic biomedical text summarization which reduces the effort and time for researchers and domain experts [185], [186]. Text summarization falls into two broad categories namely extractive summarization which identifies the most relevant sentences in the document while abstractive summarization generates new text which represents the summary of the document [187]. There are no standard datasets for biomedical text summarization. Researchers usually treat scientific papers as documents and their abstracts as summaries [188], [189].

### Question: What novel approaches have been proposed for summarizing biomedical texts?

#Reference=3 ['b190', 'b191', 'b75']

(p46.2) In the case of small models, BioBERT outperformed others. Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text. They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences. Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text. BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence. The sentences with the highest score are considered as the summary. Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT.

## (s52) Semi-Supervised Learning
### Question: What are low-cost domain adaptation methods for BERT models in the biomedical field?

#Reference=6 ['b43', 'b192', 'b122', 'b123', 'b122', 'b123']

(p52.2) Fine-tuning must be done iteratively to reduce the noisy labeled instances. BERT models to the biomedical domain. Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44]. TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191]. b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123]. The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123].

## (s53) Ontology Knowledge Injection
### Question: How can ontology knowledge injection improve BioBERT and PubMedBERT models?

#Reference=6 ['b33', 'b45', 'b44', 'b46', 'b193', 'b175']

(p53.0) Models like BioBERT, PubMedBERT have achieved good results in many of the tasks. However, these models lack knowledge from human-curated knowledge sources. These models can be further enhanced by ontology knowledge injection. Ontology knowledge injection can be done in many ways namely a) continual pretraining the models on UMLS synonyms [34], [46] or relations [45] or both [47] b) continual pretraining the models on UMLS concept definitions [192] and c) feature vector constructed using ontology is added to the sequence vector learned by the models [174].

## (s54) Small Datasets
### Question: What is multi-task fine-tuning and its benefits in machine learning?

#Reference=5 ['b67', 'b69', 'b69', 'b71', 'b73']

(p54.1) Multi-Task Fine-Tuning -Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. Multi-Task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]- [71], [73].

### Question: What are popular techniques for data augmentation in low resource scenarios?

#Reference=3 ['b196', 'b159', 'b153']

(p54.2) Data Augmentation -Data augmentation helps us to create new training instances from existing instances. These newly creating training instances are close to original training data and helpful in low resource scenarios. Back translation and EDA [195] are the top popular techniques for data augmentation. For example, Wang et al. [159] used back translation to augment the training instances to train the clinical text similarity model. The domain-specific ontologies like UMLS can also be used to augment the training instances [153].

## (s55) Robustness to Noise
### Question: How do transformed-based PLMs perform on noisy instances and what are the solutions?

#Reference=6 ['b198', 'b201', 'b200', 'b201', 'b27', 'b201']

(p55.0) Transformed based PLMs have achieved the best results in many of the tasks. However, the performance of these models on noisy test instances is limited [197]- [200]. This is because the model is mostly trained on less noisy instances. As these models mostly never encounter noisy instances during fine-tuning, the performance is significantly reduced on noisy instances. Apart from this, the noisy words in the instances are split into several subtokens which affect the model learning. The robustness of models is crucial particularly insensitive domains like biomedical [199], [200]. Two possible solutions are a) CharBERT [28] -replaced the WordPiece based embedding layer with CharCNN based embedding layer. Here word representation is generated from character embeddings using CharCNN. b) Adversarial Training [200] -Here, the training set is augmented with the noisy instances. Training the model on an augmented training set exposes the model to noisy instances and hence the model performs better on noisy instances.

## (s56) Quality In-Domain Word Representations
### Question: How can T-PLMs adapt to in-domain text while addressing vocabulary limitations?

#Reference=3 ['b19', 'b122', 'b123']

(p56.0) Continual pretraining allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of in-domain text. Though the models are adapted to in-domain, they still contain general vocabulary. As the vocabulary is learned over general text, it mostly includes subwords and words which are specific to the general domain. As a result, many of the in-domain words are not represented in a meaningful way. The two possible options to represent in-domain words in a meaningful way are a) in-domain vocabulary through DSPT [20] b) extending the general vocabulary with indomain vocabulary [122], [123].

## (s58) Quality Sequence Representation
### Question: How is the final input sequence representation determined in text classification according to Devlin et al.?

#Reference=8 ['b1', 'b1', 'b136', 'b171', 'b176', 'b177', 'b56', 'b159']

(p58.0) For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation. According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information. The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space. Finally, a softmax is applied to convert it into a vector of probabilities. However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector.

## (s60) Mitigating Bias
### Question: What causes unfair decisions in deep learning-based systems and how can bias be addressed?

#Reference=10 ['b112', 'b30', 'b31', 'b124', 'b126', 'b202', 'b88', 'b127', 'b105', 'b203']

(p60.0) With the success of deep learning models in various tasks, deep learning-based systems are used to automate the decisions like department recommendation [112], disease prediction [31], [32] etc. However, these models are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. Real-world datasets have a bias in many forms. It can be based on various attributes like gender, age, ethnicity, and marital status. These attributes are considered as protected or sensitive [201]. For example, in the MIMIC-III dataset [88] a) heart disease is more common in males compared to females-an example of gender bias b) there are fewer clinical studies involving black patients compared to other groups -an example of ethnicity bias. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. There are few works that identified and addressed bias in transformer-based biomedical language models. Zhang et al. [127] using a simple word competition task showed that SciBERT [105] exhibits ethnicity bias. Moreover, the authors showed SciBERT model when further-pretrained on clinical notes exhibits performance differences for different protected attributes. They further showed that adversarial pretraining debiasing has little impact in reducing bias. Minot et al. [202] proposed an approach based on data augmentation to identify and reduce gender bias in patient notes. This is an area that needs to be explored further to improve reduce bias and improve the fairness in model decisions.

## (s61) Privacy Issues
### Question: What are the concerns and solutions regarding data leakage in biomedical language models?

#Reference=4 ['b204', 'b205', 'b206', 'b204']

(p61.0) Every patient visit is recorded in the clinical records. Apart from patient visits, clinical records contain the past and the present medical history of the patient. Such sensitive data should not be disclosed as it may harm the patients physically or mentally [203]. Usually, the clinical records are shared for research purposes only after de-identifying the sensitive information. However, it is possible to recover sensitive patient information from the de-identified medical records. Recent works showed that there is data leakage from pre-trained models in the general domain i.e., it is possible to recover personal information present in the pretraining corpora [204], [205]. Due to data leakage, the models pre-trained on proprietary corpora, cannot be released publicly. Recently, Nakamura et al. [203] proposed KART framework which can conduct various attacks to assess the leakage of sensitive information from pre-trained biomedical language models. We strongly believe there is a need for more work in this area to assess as well as address the data leakage in biomedical language models.

## (s62) Domain Adaptation
### Question: What are the challenges and solutions in developing biomedical pre-trained language models (BPLMs)?

#Reference=5 ['b15', 'b122', 'b123', 'b122', 'b123']

(p62.0) In the beginning, the standard approach to develop BPLMs is to start with general PLMs and then further pretrain them on large volumes of biomedical text [16]. The main drawback of this approach is the lack of indomain vocabulary. Without domain-specific vocabulary, many of the in-domain are split into a number of subwords which hinders model learning during pretraining or fine-tuning. Moreover, continual pretraining is quite expensive as it involves pretraining on large volumes of unlabeled text. To overcome these drawbacks, there are low-cost domain adaptation approaches that extend the general domain vocabulary with in-domain vocabulary [122], [123]. The extra in-domain vocabulary is generated using Word2vec and then aligned [122] or generated directly using WordPiece [123] over biomedical text. The main drawback in these low-cost domain adaptation approaches is an increase in the size of the model with the addition of in-domain vocabulary. Further research on this topic can result in more novel methods for lowcost domain adaptation.

## (s64) Benchmarks
### Question: What is the purpose of benchmarks in evaluating NLP model performance across tasks?

#Reference=8 ['b208', 'b209', 'b210', 'b211', 'b17', 'b19', 'b47', 'b17']

(p64.0) In general, a benchmark is a tool to evaluate the performance of models across different NLP tasks. A benchmark is required because we expect the pre-trained language models to be general and robust i.e., the models perform well across tasks rather than on one or two specific tasks. A benchmark with one or more datasets for multiple NLP tasks helps to assess the general ability and robustness of models. In general domain, we have a number of benchmarks like GLUE [207] and Super-GLUE [208] (general language understanding), XGLUE [209] (cross lingual language understanding) and LinCE [210] (code switching). In biomedical domain there are three benchmarks namely BLUE [18], BLURB [20] and ChineseBLUE [48]. BLUE introduced by Peng et al. [18] contains ten datasets for five biomedical NLP tasks, while BLURB contains thirteen datasets for six tasks and ChineseBLUE contains eight tasks with nine datasets. BLUE and ChineseBLUE include both EHR and scientific literature-based datasets, while BLURB includes only biomedical scientific literature-based datasets. The semantics of EHR and medical social media texts are different from biomedical scientific literature. So, there is a need of exclusive benchmarks for EHR and medical social media-based datasets.

## (s65) Intrinsic Probes
### Question: What do intrinsic probes reveal about PLMs' knowledge during pretraining?

#Reference=7 ['b13', 'b212', 'b213', 'b214', 'b215', 'b212', 'b214']

(p65.0) During pretraining, PLMs learn syntactic, semantic knowledge along with factual and common-sense knowledge available in the pretraining corpus [14]. Intrinsic probes through light on the knowledge learned by PLMs during pretraining. In general NLP, researchers proposed several intrinsic probes like LAMA, Negated and Misprimed LAMA [211], XLAMA [212], X-FACTR [213], MickeyProbe [214] to understand the knowledge encoded in pretrained models. For example, LAMA [211] probes the factual and common-sense knowledge of English pretrained models, while X-FACTR [213] probes the factual knowledge of multi-lingual pretrained models. However, there is no such intrinsic probes in Biomedical domain to through light on the knowledge learned by BPLMs during pretraining. This is an area which requires much attention from Biomedical NLP community.

## (s66) Efficient Models
### Question: What advancements are needed in biomedical NLP model pretraining?

#Reference=2 ['b216', 'b217']

(p66.0) Pretraining provides BPLMs with necessary background knowledge which can be transferred to downstream tasks. However, pretraining is computationally very expensive and also requires large volumes of pretraining data. So, there is need of novel model architecture which reduces the pretraining time as well as the amount of pretraining corpus. In general NLP, recently efficient models like ConvBERT [215] and DeBERTa [216] are proposed which reduces the pretraining time and amount of pretraining corpus required respectively. DeBERTa with two novel improvements (Disentangled attention mechanism and enhanced masked decoder) achieves better compared to RoBERTa. DeBERTa is pretrained on just 78GB of data while RoBERTa is pretrained on 160GB of data. ConvBERT with mixed attention block outperforms ELECTRA while using just 1/4 th of its pretraining cost. Biomedical NLP research community must focus on developing pretrained models based on these novel model architectures.

