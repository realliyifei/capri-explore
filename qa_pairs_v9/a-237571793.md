# Multi-Task Learning in Natural Language Processing: An Overview

CorpusID: 237571793 - [https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a](https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a)

Fields: Linguistics, Computer Science

## (s1) Parallel Architectures
### Question: How can the expressive power of a tree-like architecture model be enhanced for specific tasks?

#Reference=8 ['b46', 'b57', 'b136', 'b64', 'b36', 'b78', 'b88', 'b127']

(p1.2) The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task. A solution is to equip the shared trunk with task-specific encoders [47,58,137]. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages. Another way is to make different groups of tasks share different parts of the trunk [37,79,89]. Moreover, this idea can be applied to the decoder. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.

### Question: How do models control information flow in multi-task learning to reduce inter-task interference?

#Reference=5 ['b68', 'b38', 'b147', 'b132', 'b81']

(p1.5) However, different parts of the shared features are not equally important to each task. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | · | compute the absolute value in an element-wise manner. After that, S is concatenated with task-specific representations to form the input to the output layer.

### Question: How does task routing contribute to feature fusion in models like MCapsNet?

#Reference=3 ['b143', 'b135', 'b100']

(p1.8) Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task. Given tasks, the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .

## (s2) Supervision at Different
### Question: How do parallel architecture models handle tasks at different abstraction levels in NLP?

#Reference=10 ['b19', 'b79', 'b101', 'b108', 'b27', 'b56', 'b95', 'b32', 'b92', 'b10']

(p2.0) Feature Levels. Models using the parallel architecture handle multiple tasks in parallel. These tasks may concern features at different abstraction levels. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.

### Question: How can auxiliary tasks at different levels enhance MTL performance in various applications?

#Reference=7 ['b96', 'b67', 'b153', 'b153', 'b61', 'b84', 'b15']

(p2.1) In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.

## (s4) Hierarchical
### Question: What is the principle of hierarchical feature pipeline in multi-task models?

#Reference=5 ['b13', 'b43', 'b138', 'b109', 'b151']

(p4.1) In hierarchical feature pipeline, the output of one task is used as extra features for another task. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks. For example, [14] feeds the output of a question-review pair recognition model to the question answering model. [44] feeds the output of aspect term extraction to aspect-term sentiment classification. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.

### Question: How is hierarchical feature pipeline utilized in various natural language processing tasks?

#Reference=5 ['b29', 'b42', 'b32', 'b111', 'b106']

(p4.2) Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.

## (s6) Modular Architectures
### Question: What are common practices in designing modular architectures for multi-task learning?

#Reference=11 ['b57', 'b155', 'b0', 'b102', 'b48', 'b58', 'b137', 'b146', 'b118', 'b23', 'b91']

(p6.1) The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did. [1] shares word and character embedding matrices and combines them differently for different tasks. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT). Modular designs are also widely used in multi-lingual tasks. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks. Shared embeddings can be used alongside task-specific embeddings [59,138] as well. In addition to word embeddings, [147] shares label embeddings between tasks. Researchers have also developed modular architectures at a finer granularity. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer. [92] creates encoder modules on different levels, including task level, task group level, and universal level.

## (s7) Generative Adversarial Architectures
### Question: How do Generative Adversarial Networks (GANs) enhance generative tasks and multitask learning in NLP?

#Reference=5 ['b69', 'b77', 'b118', 'b126', 'b137']

(p7.0) Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.

### Question: How do generative adversarial architectures utilize unlabeled data to enhance model performance?

#Reference=2 ['b123', 'b97']

(p7.1) An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.

## (s9) Loss Construction
### Question: How does GradVac improve multi-lingual model performance through gradient manipulation?

#Reference=2 ['b128', 'b0']

(p9.10) Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g . Notice that PCGrad is a special case of GradVac when = 0. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.

## (s11) Task Scheduling
### Question: What is task scheduling in MTL model training and how is it implemented?

#Reference=5 ['b147', 'b142', 'b133', 'b2', 'b110']

(p11.0) Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.

### Question: What is the significance of sequential task learning in multi-task models?

#Reference=9 ['b49', 'b17', 'b42', 'b84', 'b48', 'b89', 'b93', 'b29', 'b42']

(p11.5) In some cases, multiple tasks are learned sequentially. Such tasks usually form a clear dependency relationship or are of different difficulty levels. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1). Unicoder [49] trains its five pre-training objectives sequentially in each step. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.

### Question: What is the pre-train then fine-tune methodology in auxiliary MTL?

#Reference=4 ['b54', 'b43', 'b124', 'b13']

(p11.6) For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data. [125] first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.

## (s14) Primary
### Question: How is auxiliary multi-task learning applied in various classification and detection tasks?

#Reference=12 ['b70', 'b55', 'b52', 'b54', 'b58', 'b27', 'b73', 'b106', 'b132', 'b13', 'b32', 'b111']

(p14.0) Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.

### Question: How does Multi-Task Learning (MTL) enhance text generation quality in NMT models?

#Reference=12 ['b26', 'b72', 'b143', 'b102', 'b127', 'b36', 'b37', 'b153', 'b154', 'b104', 'b8', 'b98']

(p14.2) For text generation tasks, MTL is brought in to improve the quality of the generated text. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155]. [105] implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing. [9] enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks. [99] views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.

## (s15) Joint MTL
### Question: How do joint MTL models differ from auxiliary MTL in optimizing task performance?

#Reference=12 ['b92', 'b109', 'b42', 'b147', 'b71', 'b101', 'b39', 'b140', 'b144', 'b7', 'b43', 'b150']

(p15.0) Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.

### Question: What is joint MTL and its applications in multi-domain and multi-formalism NLP tasks?

#Reference=21 ['b59', 'b130', 'b82', 'b20', 'b40', 'b115', 'b5', 'b4', 'b117', 'b53', 'b90', 'b31', 'b75', 'b41', 'b46', 'b3', 'b87', 'b86', 'b70', 'b28', 'b142']

(p15.2) Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118]. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [32], Predicate-Argument Structures (PAS) [76], and Prague Semantic Dependencies (PSD) [42]) jointly. In [47], a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation (AMR) [4], Semantic Dependency Parsing (SDP) [88], and Universal Dependencies (UD) [87], and it shows that joint training improves performance on the testing UCCA dataset. [71] jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT. [29] shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks. In [143], an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.

## (s16) Multi-lingual MTL
### Question: How does multi-task learning benefit multi-lingual machine learning models?

#Reference=3 ['b78', 'b77', 'b126']

(p16.0) Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.

### Question: How does multi-lingual MTL facilitate knowledge transfer and improve language processing tasks?

#Reference=2 ['b118', 'b85']

(p16.1) Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.

### Question: How can cross-lingual knowledge transfer be achieved through language representations?

#Reference=3 ['b107', 'b48', 'b64']

(p16.2) Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.

## (s17) Multimodal MTL
### Question: What is multimodal learning in NLP and its significance?

#Reference=2 ['b15', 'b15']

(p17.0) As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.

## (s18) Task Relatedness in MTL
### Question: What factors influence task selection for effective multi-task learning in NLP?

#Reference=4 ['b76', 'b52', 'b70', 'b103']

(p18.0) A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.

### Question: How does multi-task learning (MTL) performance reveal task relatedness and complementarity?

#Reference=5 ['b9', 'b53', 'b90', 'b21', 'b48']

(p18.1) As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.

## (s21) Multi-label Datasets.
### Question: How can multi-label datasets be created from existing data?

#Reference=4 ['b53', 'b90', 'b120', 'b28']

(p21.1) Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.

### Question: How can extra annotations be automatically generated for self-supervised multi-label datasets?

#Reference=14 ['b61', 'b96', 'b55', 'b85', 'b115', 'b129', 'b48', 'b124', 'b106', 'b30', 'b23', 'b32', 'b63', 'b125']

(p21.2) The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa.

