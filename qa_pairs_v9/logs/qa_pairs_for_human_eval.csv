corpusid,title,domain,section,paragraph,QA pair,subquestions
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s1),(p1.1),"Question: What do studies reveal about the characteristics of BERT's embeddings?

1. In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.
2. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
3. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.
4. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.
5. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.
6. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).",1. What do studies reveal about the characteristics of BERT's embeddings?
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s3),(p3.2),"Question: How does BERT's MLM demonstrate syntactic competence and limitations according to recent studies?

1. Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task.
2. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences.
3. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations.
4. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input.
5. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019).
6. This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","1. How does BERT's MLM demonstrate syntactic competence according to recent studies?
2. What are the limitations of BERT's MLM according to recent studies?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s4),(p4.0),"Question: What does research reveal about BERT's understanding of semantic roles?

1. To date, more studies were devoted to BERT's knowledge of syntactic rather than semantic phenomena.
2. However, we do have evidence from an MLM probing study that BERT has some knowledge for semantic roles (Ettinger, 2019).
3. BERT is even able to prefer the incorrect fillers for semantic roles that are semantically related to the correct ones, to those that are unrelated (e.g. ""to tip a chef"" should be better than ""to tip a robin"", but worse than ""to tip a waiter"").
4. Tenney et al. (2019b) showed that BERT encodes information about entity types, relations, semantic roles, and proto-roles, since this information can be detected with probing classifiers.",1. What does research reveal about BERT's understanding of semantic roles?
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s5),(p5.0),"Question: How does BERT adapt for knowledge induction and what are its limitations?

1. MLM component of BERT is easy to adapt for knowledge induction by filling in the blanks (e.g. ""Cats like to chase [ ]"").
2. There is at least one probing study of world knowledge in BERT (Ettinger, 2019), but the bulk of evidence comes from  (Petroni et al., 2019) numerous practitioners using BERT to extract such knowledge.
3. Petroni et al. (2019) showed that, for some relation types, vanilla BERT is competitive with methods relying on knowledge bases (Figure 3).
4. Davison et al. (2019) suggest that it generalizes better to unseen data.
5. However, to retrieve BERT's knowledge we need good template sentences, and there is work on their automatic extraction and augmentation (Bouraoui et al., 2019; However, BERT cannot reason based on its world knowledge.
6. Forbes et al. (2019) show that BERT can ""guess"" the affordances and properties of many objects, but does not have the information about their interactions (e.g. it ""knows"" that people can walk into houses, and that houses are big, but it cannot infer that houses are bigger than people.)  and  also show that the performance drops with the number of necessary inference steps.
7. At the same time, Poerner et al. (2019) show that some of BERT's success in factoid knowledge retrieval comes from learning stereotypical character combinations, e.g. it would predict that a person with an Italian-sounding name is Italian, even when it is factually incorrect.","1. How does BERT adapt for knowledge induction?
2. What are its limitations?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s7),(p7.9),"Question: How do attention heads in BERT models impact coreference and semantic relation tasks?

1. Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,.
2. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks.
3. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998).
4. Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","1. How do attention heads in BERT models impact coreference tasks?
2. How do attention heads in BERT models impact semantic relation tasks?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s8),(p8.2),"Question: How do BERT's middle layers impact syntactic information processing according to recent studies?

1. The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5).
2. There is conflicting evidence about syntactic chunks.
3. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling.
4. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing.
5. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.",1. How do BERT's middle layers impact syntactic information processing according to recent studies?
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s8),(p8.3),"Question: How do the final layers of BERT specifically impact its task performance and knowledge representation?

1. The final layers of BERT are the most taskspecific.
2. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable .
3. In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019).
4. At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance.
5. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers.
6. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182).
7. But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial.
8. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.","1. How do the final layers of BERT specifically impact its task performance?
2. How do the final layers of BERT impact its knowledge representation?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s10),(p10.9),"Question: How do E-BERT and ERNIE integrate external knowledge into BERT, and what are the benefits of pre-training?

1. Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE .
2. Alternatively, SemBERT  integrates semantic role information with BERT representations.
3. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides.
4. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6).
5. However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).","1. How do E-BERT and ERNIE integrate external knowledge into BERT?
2. What are the benefits of pre-training?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s11),(p11.0),"Question: What are the key findings from systematic studies on optimizing BERT architecture?

1. To date, the most systematic study of BERT architecture was performed by .
2. They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others.
3. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable.
4. Larger hidden representation size was consistently better, but the gains varied by setting.
5. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance.
6. They also publish their recommendations for other model parameters.
7. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.
8. observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.",1. What are the key findings from systematic studies on optimizing BERT architecture?
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s12),(p12.0),"Question: How does fine-tuning affect BERT's self-attention patterns?

1. Pre-training + fine-tuning workflow is a crucial part of BERT.
2. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand.
3. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns.
4. It is understandable why fine-tuning would increase the attention to  Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).",1. How does fine-tuning affect BERT's self-attention patterns?
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s12),(p12.5),"Question: How does initialization affect NLP model training and reported performance improvements?

1. Initialization can have a dramatic effect on the training process (Petrov, 2010).
2. However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018).
3. Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation.
4. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.","1. How does initialization affect NLP model training?
2. What are the reported performance improvements related to initialization?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s13),(p13.1),"Question: How do pruning Transformer heads affect model performance and efficiency?

1. Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have.
2. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance.
3. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head.
4. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance.
5. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019).
6. Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).","1. How do pruning Transformer heads affect model performance?
2. How do pruning Transformer heads affect model efficiency?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s15),(p15.0),"Question: What is Multilingual BERT and how does it perform across different tasks and languages?

1. Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary).
2. Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing.
3. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019).
4. The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019).
5. Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).","1. What is Multilingual BERT?
2. How does it perform across different tasks and languages?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s15),(p15.3),"Question: How does mBERT's MLM understand syntactic properties across languages?

1. At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019).
2. Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages.
3. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary.
4. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.",1. How does mBERT's MLM understand syntactic properties across languages?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s1),(p1.2),"Question: What is a concept in linguistic terms?

1. Concept A concept represents a coherent fragment of knowledge, such as ""a class containing certain objects as elements, where the objects have certain properties"" (Stock, 2010).
2. For example, a concept could be lexical: e.g. words ending with suffix ""ed"", morphological: e.g. gerund verbs, or semantic: e.g. names of cities.
3. We loosely define a concept C as a group of words that are coherent w.r.t to a linguistic property.
4. Table 1 shows an example sentence with different concept annotations.",1. What is a concept in linguistic terms?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s3),(p3.0),"Question: What are the limitations of visualizing neuron activations to understand their roles?

1. A simple way to discover the role of a neuron is by visualizing its activations and manually identifying the underlying concept over a set of sentences (Karpathy et al., 2015;Fyshe et al., 2015;Li et al., 2016a).
2. Given that deep NLP models are † Table 3 in Appendix gives a more comprehensive list.
3. trained using billions of neurons, it is impossible to visualize all the neurons.
4. A number of clues have been used to shortlist the neurons for visualization, for example, selecting saturated neurons, high/low variance neurons, or ignoring dead neurons (Karpathy et al., 2015) when using ReLU activation function.
5. ‡ Limitation While visualization is a simple approach to find an explanation for a neuron, it has some major limitations: i)
6. it is qualitative and subjective, ii) it cannot be scaled to the entire network due to an extensive human-in-the-loop effort, iii) it is difficult to interpret polysemous neurons that acquire multiple roles in different contexts, iv) it is ineffective in identifying group neurons, and lastly and v) not all neurons are visually interpretable.
7. Visualization nevertheless remains a useful tool when applied in combination to other interpretation methods that are discussed below.",1. What are the limitations of visualizing neuron activations to understand their roles?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s4),(p4.1),"Question: What is concept search in the context of neural network analysis?

1. Concept Search This set of methods take a neuron as an input and search for a concept that the neuron has learned.
2. They sort the input instances based on the activation values of the given neuron.
3. The top activating instances represent a concept the neuron represents.
4. Kádár et al. (2017) discovered neurons that learn various linguistic con-cepts using this approach.
5. They extracted top-20, 5-gram contexts for each neuron based on the magnitude of activations and manually identified the underlying concepts.
6. This manual effort of identifying concepts is cumbersome and requires a human-in-the-loop.
7. Na et al. (2019) addressed this by using lexical concepts of various granularities.
8. Instead of 5-gram contexts, they extracted top-k activating sentences for each neuron.
9. They parsed the sentences to create concepts (words and phrases) using the nodes of the parse trees.
10. They then created synthetic sentences that highlight a concept e.g. a particular word occurring in all synthetic sentences.
11. The neurons that activates largely on these sentences are considered to have learned the concept.
12. This methodology is useful in analyzing neurons that are responsible for multi-word concepts such as phrases and idiomatic collocations.
13. However, the synthetic sentences are often ungrammatical and lead towards a risk of identifying neurons that exhibit arbitrary behavior (like repetition) instead of concept specific behavior.",1. What is concept search in the context of neural network analysis?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s5),(p5.0),"Question: How do corpus-based methods discover neurons linked to specific concepts?

1. The second class of corpusbased methods aim to discover neurons for a given concept.
2. The underlying idea is the same i.e. to establish a link between the concept and neurons based on co-occurrences stats, but in the opposite direction.
3. The activation values play a role in weighing these links to obtained a ranked list of neurons against the concept.
4. Mu and Andreas (2020) achieved this by creating a binary mask of a neuron based on a threshold on its activation values for every sentence in the corpus.
5. Similarly, they created a binary mask for every concept based on its presence or absence in a sentence.
6. They then computed the overlap between a given neuron mask vector and a concept mask vector using intersection-over-union (IoU), and use these to generate compositional explanations.
7. Differently from them, Suau et al. (2020) used the values of neuron activations as prediction scores and computed the average precision per neuron and per concept.
8. Finally, Antverg and Belinkov (2022) considered the mean activation values of a neuron with respect to instances that posses the concept of interest.",1. How do corpus-based methods discover neurons linked to specific concepts?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s6),(p6.0),"Question: What are probing-based methods and how do they function in neural network interpretation?

1. Probing-based methods train diagnostic classifiers (Hupkes et al., 2018) over activations to identify neurons with respect to pre-defined concepts.
2. They are a global interpretation methods that discover a set of neurons with respect to each concept using supervised data annotations.
3. They are highly scalable, and can be easily applied on a large set of neurons and over a large set of concepts.
4. In the following, we cover two types of classifiers used for probing.","1. What are probing-based methods?
2. How do they function in neural network interpretation?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s7),(p7.0),"Question: How does regularization affect neuron importance in concept learning models?

1. The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed.
2. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept.
3. The regularization of the classifier directly effects the weights and therefore the ranking of neurons.
4. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero.
5. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features.
6. This translates to discovering group neurons that are jointly responsible for a concept.
7. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.",1. How does regularization affect neuron importance in concept learning models?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s7),(p7.1),"Question: What are the limitations and proposed solutions in probing classifiers for concept reflection?

1. Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018).
2. Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization.
3. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.
4. suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.","1. What are the limitations in probing classifiers for concept reflection?
2. What are the proposed solutions in probing classifiers for concept reflection?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s8),(p8.1),"Question: What is ablation and how does it identify salient neurons in neural networks?

1. Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value.
2. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance.
3. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised).
4. The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a).
5. The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019).
6. Here, the output class serves as the concept against which we want to find the salient neurons.","1. What is ablation?
2. How does it identify salient neurons in neural networks?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s8),(p8.3),"Question: What are attribution-based methods and their significance in identifying causal neurons?

1. Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018).
2. Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact.
3. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact.
4. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.","1. What are attribution-based methods?
2. What is their significance in identifying causal neurons?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s9),(p9.1),"Question: What is Corpus Generation and its significance in neuron analysis?

1. Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations.
2. It is possible that a neuron represents a diverse concept which is not featured in the corpus.
3. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations.
4. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role.
5. Corpus generation has been widely explored in Computer Vision.
6. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron.
7. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs.
8. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.","1. What is Corpus Generation?
2. What is its significance in neuron analysis?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s9),(p9.3),"Question: What is Matrix Factorization and its application in analyzing vision models?

1. Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function.
2. Given a model, the activations of an input sentence form a matrix.
3. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept.
4. MF is a local interpretation method.
5. It is commonly used in analyzing vision models (Olah et al., 2018).
6. We could not find any research using MF on the NLP models.
7. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.","1. What is Matrix Factorization?
2. What is its application in analyzing vision models?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s9),(p9.5),"Question: How do clustering methods analyze neuron groups in unsupervised learning?

1. Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion.
2. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster.
3. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.
4. aimed at identifying redundant neurons in the network.
5. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them.
6. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.",1. How do clustering methods analyze neuron groups in unsupervised learning?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s15),(p15.0),"Question: How is visualization used to evaluate neurons in linguistic studies?

1. Visualization has been used as a qualitative measure to evaluate the selected neurons.
2. For example, Dalvi et al. (2019) visualized the top neurons and showed that they focus on very specific linguistic properties.
3. They also visualized top-k activating words for the top neurons per concept to demonstrate the efficacy of their method.
4. Visualization can be a very effective tool to evaluate the interpretations when it works in tandem with other methods e.g. using Concept Search or Probingbased methods to reduce the search space towards only highly activating concepts or the most salient neurons for these concepts respectively.",1. How is visualization used to evaluate neurons in linguistic studies?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s18),(p18.1),"Question: What do studies reveal about neurons learning linguistic and conceptual patterns?

1. Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end.
2. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment.
3. For example ""I like this movie a lot"" or ""the movie is incredibly good"".
4. Similarly they discovered neurons that captured ""negation"".
5. Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained.
6. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task.
7. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"".
8. Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc.
9. They also found neurons that learn phrasal concepts.
10. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation.
11. They provided finer interpretation of the neurons by generating synthetic instances.
12. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.","1. What do studies reveal about neurons learning linguistic patterns?
2. What do studies reveal about neurons learning conceptual patterns?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s20),(p20.2),"Question: How do neurons exhibit monosemous and polysemous behavior in language processing?

1. Neurons exhibit monosemous and polysemous behavior.
2. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis.
3. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc.
4. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"".
5. concepts. Suau et al. (2020) discovered neurons that capture different senses of a word.
6. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.","1. How do neurons exhibit monosemous behavior in language processing?
2. How do neurons exhibit polysemous behavior in language processing?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s20),(p20.3),"Question: How do neurons capture syntactic and complex semantic concepts in language processing?

1. Neurons capture syntactic concepts and complex semantic concepts.
2. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates.
3. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency.
4. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases.
5. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.","1. How do neurons capture syntactic concepts in language processing?
2. How do neurons capture complex semantic concepts in language processing?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s23),(p23.0),"Question: How do human language hierarchies relate to neuron distribution in language models?

1. Human languages are hierarchical in structure where morphology and phonology sit at the bottom followed by lexemes, followed by syntactic structures.
2. Concepts such as semantics and pragmatics are placed on the top of the hierarchy.
3. analyzed linguistic hierarchy by studying the spread of neurons across layers in various pre-trained language models.
4. They extracted salient neurons with respect to different linguistic concepts (e.g. morphology and syntax) and found that neurons that capture word morphology were predominantly found in the lower and middle layers and those learning about syntax were found at the higher layers.
5. The observation was found to be true in both LSTMand the transformer-based architectures, and are inline with the findings of representation analysis (Liu et al., 2019;Tenney et al., 2019;Belinkov et al., 2020b).
6. Similarly Suau et al. (2020) analyzed sub-modules within GPT and RoBERTa transformer blocks and showed that lower layers within a transformer block accumulate more salient neurons than higher layers on the tasks of word sense disambiguation or homograph detection.
7. They also found that the neurons that learn homographs are distributed across the network as opposed to sense neurons that were more predominantly found at the lower layers.",1. How do human language hierarchies relate to neuron distribution in language models?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s24),(p24.0),"Question: How does dropout influence information distribution in neural networks?

1. While it is exciting to see that networks somewhat preserve linguistic hierarchy, many authors found that information is not discretely preserved at any individual layer, but is distributed and is redundantly present in the network.
2. This is an artifact of various training choices such as dropout that encourages the model to distribute knowledge across the network.
3. For example, Li et al. (2016b) found specialized frequency neurons in a GloVe model trained without dropout, as opposed to the variant trained with dropout where the information was more redundantly available.
4. showed that a significant amount of redundancy existed within pre-trained models.
5. They showed that 85% of the neurons across the network are redundant and at least 92% of the neurons can be removed when optimizing towards a downstream task in feature-based transfer learning.",1. How does dropout influence information distribution in neural networks?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s25),(p25.0),"Question: How do neuron distributions vary across different neural network architectures?

1. The distribution of neurons across the network has led researchers to draw interesting crossarchitectural comparisons.
2. Wu et al. (2020) performed correlation clustering of neurons across architectures and found that different architectures may have similar representations, but their individual neurons behave differently.
3. Hennigen et al. (2020) compared neurons in contextualized (BERT) embedding with neurons in the static embedding (fastText) and found that fastText required two neurons to capture any morphosyntactic phenomenon as opposed to BERT which required up to 35 neurons to obtain the same performance.
4. showed that the linguistic knowledge in BERT (auto-encoder) is highly distributed across the network as opposed to XLNet (auto-regressive) where neurons from a few layers are mainly responsible for a concept (see Figure 2).
5. Similarly Suau et al. (2020) compared RoBERTa and GPT (auto-encoder vs. generative) models and found differences in the distribution of expert neurons.
6. extended the cross-architectural comparison towards fine-tuned models.
7. They showed that after finetuning on GLUE tasks, the neurons capturing linguistic knowledge are regressed to lower layers in RoBERTa and XLNet as opposed to BERT where it is still retained at the higher layers.",1. How do neuron distributions vary across different neural network architectures?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s26),(p26.0),"Question: What do neurons in Deep NLP models learn about linguistic knowledge and hierarchy?

1. Below is a summary of the key findings that emerged from the work we covered in this survey.
2. Neurons learned within Deep NLP models capture non-trivial linguistic knowledge ranging from lexical phenomenon such as morphemes, words and multi-word expressions to highly complex global phenomenon such as semantic roles and syntactic dependencies.
3. Neuron analysis resonates with the findings of representation analysis (Belinkov et al., 2017a,b;Tenney et al., 2019;Liu et al., 2019) in demonstrating that the networks follow linguistic hierarchy.
4. Linguistic neurons are distributed across the network based on their complexity, with lower layers focused on the lexical concepts and middle and higher layers learning global phenomenon based on long-range contextual dependencies.
5. While the networks preserve linguistic hierarchy, many authors showed that information is not discretely preserved, but is rather distributed and redundantly present in the network.
6. It was also shown that a small optimal subset of neurons w.r.t any concept can be extracted from a network.
7. On another dimension, a few works showed that some concepts are localized to fewer neurons while others are distributed to a large group.
8. Finally, some interesting cross architectural analyses were drawn based on how the neurons are distributed within their layers.","1. What do neurons in Deep NLP models learn about linguistic knowledge?
2. What do neurons in Deep NLP models learn about hierarchy?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s28),(p28.0),"Question: How can identified neurons control a model's behavior regarding learned concepts?

1. Once we have identified neurons that capture a certain concept learned in a model, these can be utilized for controlling the model's behavior w.r.t to that concept.
2. Bau et al. (2019) identified Switch Neurons in NMT models that activate positively for the present-tense verbs and negatively for the past-tense verbs.
3. By manipulating the values of these neurons, they were able to successfully change output translations from present to past tense during inference.
4. The authors additionally found neurons that capture gender and number agreement concepts and manipulated them to control the system's output.
5. Another effort along this line was carried by Suau et al. (2020) Controlling model's behavior using neurons en-ables on-the-fly manipulation of output, for example it can be used to debias the output of the model against sensitive attributes like race and gender.","1. How can identified neurons control a model's behavior?
2. Regarding what learned concepts can identified neurons control a model's behavior?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s31),(p31.0),"Question: How do neurons associated with concepts explain AI model predictions?

1. Knowing the association of a neuron with a concept enables explanation of model's output.
2. Mu and Andreas (2020) identified neurons that learn certain concepts in vision and NLP models.
3. Using a composition of logical operators, they provided an explanation of model's prediction.
4. Figure 3 presents an explanation using a gender-sensitive neuron.
5. The neuron activates for contradiction when the premise contains the word man.
6. Such explanations provide a way to generate adversarial examples that change model's predictions.",1. How do neurons associated with concepts explain AI model predictions?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s3),(p3.0),"Question: What motivates the use of Masked Language Models in natural language processing?

1. As natural language data is readily available and unsupervised training paradigms have been proposed to better utilize extremely large datasets, this motivates the unsupervised learning of natural language.
2. One common approach is to predict masked words in a sentence while considering the surrounding context.
3. This training paradigm is known as the Masked Language Model.
4. This type of training allows the model to develop a deeper understanding of the relationships between words and the context in which they are used.
5. These models are trained on a large corpus of texts using techniques such as the Transformer architecture and have achieved state-of-the-art results in many NLP tasks, such as sentiment analysis and named entity recognition.
6. Notable examples of Masked Language Models include BERT [28], RoBERTa [65], and T5 [84].
7. MLMs have become an important tool in the field of natural language processing due to their success in a wide range of tasks.",1. What motivates the use of Masked Language Models in natural language processing?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s4),(p4.0),"Question: How do autoregressive language models improve few-shot and zero-shot performance?

1. Although language models are typically task-agnostic in architecture, these methods require fine-tuning on datasets of the specific downstream task.
2. Researchers found that scaling up language models significantly improves the few-shot, even zero-shot performance [16].
3. The most successful models for better few-shot and zero-show performance are Autoregressive Language Models, which are trained by generating the next word in a sequence given the preceding words.
4. These models have been widely used for downstream tasks such as text generation and question answering.","1. How do autoregressive language models improve few-shot performance?
2. How do autoregressive language models improve zero-shot performance?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s4),(p4.1),"Question: What are examples and advancements of autoregressive language models?

1. Examples of Autoregressive Language Models include GPT-3 [16], OPT [126], PaLM [22], and BLOOM [92].
2. The game changer, GPT-3, for the first time, demonstrated reasonable few-/zero-shot performance via prompting and in-context learning, thus showing the superiority of autoregressive language models.
3. There are also models such as CodeX [2] that are optimized for specific tasks such as code generation, BloombergGPT [117] for the financial domain.
4. The recent breakthrough is ChatGPT, which refines GPT-3 specifically for conversational tasks, resulting in more interactive, coherent, and context-aware conversational for various real-world applications.","1. What are examples of autoregressive language models?
2. What advancements have been made in autoregressive language models?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s7),(p7.0),"Question: How does pre-training data affect large language model performance?

1. Pre-training data plays a pivotal role in the development of large language models.
2. As the foundation of remarkable capabilities [5,47] of LLMs, the quality, quantitative, and diversity of pre-training data influence the performance of LLMs significantly [124].
3. The commonly used pretraining data consists of a myriad of text sources, including books, articles, and websites.
4. The data is carefully curated to ensure a comprehensive representation of human knowledge, linguistic nuances, and cultural perspectives.
5. The importance of pretraining data lies in its capacity to inform the language model with a rich understanding of word knowledge, grammar, syntax, and semantics, as well as the ability to recognize context and generate coherent responses.
6. The diversity of pretraining data also plays a crucial role in shaping the model's performance, and the selection of LLMs highly depends on the components of the pretraining data.
7. For example, PaLM [22] and BLOOM [92] excel in multilingual tasks and machine translation with an abundance of multilingual pretraining data.
8. Moreover, PaLM's performance in Question Answering tasks is enhanced by incorporating a considerable amount of social media conversations and Books corpus [22].
9. Likewise, code execution and code completion capabilities of GPT-3.5 (code-davinci-002) are amplified by the integration of code data in its pretraining dataset.
10. In brief, when selecting LLMs for downstream tasks, it is advisable to choose the model pre-trained on a similar field of data.",1. How does pre-training data affect large language model performance?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s8),(p8.0),"Question: What are the best model deployment strategies based on annotated data availability?

1. When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant.
2. In this section, we provide a succinct overview of the appropriate models to employ for each scenario.
3. Zero annotated data: In scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach.
4. LLMs have been shown to outperform previous zero-shot methods [120].
5. Additionally, the absence of a parameter update process ensures that catastrophic forgetting [49] is avoided since the language model parameters remain unaltered.","1. What are the best model deployment strategies?
2. How do these strategies vary based on annotated data availability?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s9),(p9.0),"Question: What challenges do LLMs face when applied to downstream tasks due to distributional differences?

1. When deploying LLMs for downstream tasks, we often face challenges stemming from distributional differences between the test/user data and that of the training data.
2. These disparities may encompass domain shifts [132], out-of-distribution variations [31], or even adversarial examples [82].
3. Such challenges significantly hinder fine-tuned modes' effectiveness in real-world applications.
4. They fit into a specific distribution and have a poor ability to generalize to OOD data.",1. What challenges do LLMs face when applied to downstream tasks due to distributional differences?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s13),(p13.2),"Question: Why is the Perspective API considered effective in detecting online toxicity?

1. CivilComments [13] even the best one is only better than random guessing [59].
2. On the other hand, most popular fine-tuned models can obtain much better performance [33].
3. and the Perspective API 3 is still one of the best for detecting toxicity.
4. This API is powered by a multilingual BERT-based model, which is tuned on publicly available toxicity data and several smaller single-language CNNs distilled from this model.
5. This might be due to the fact that toxicity is defined by subtle nuances in linguistic expressions, and large language models are unable to accurately comprehend this task solely based on the provided input.",1. Why is the Perspective API considered effective in detecting online toxicity?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s13),(p13.4),"Question: Why are LLMs not widely used in information retrieval tasks?

1. In information retrieval (IR) tasks, LLMs are not widely exploited yet.
2. One major reason is that IR tasks are fundamentally different from others.
3. There's no natural way to transform the thousands of candidate texts into a few/zero-shot form which is required by LLMs.
4. The existing evaluation results on MS MARCO(regular/TREC) [73] show that methods based on fine-tuned models have better performance [59].
5. In this evaluation, the LLMs rank passages in an unorthodox way, which requires the LLMs to produce probabilities for passages one by one.",1. Why are LLMs not widely used in information retrieval tasks?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s13),(p13.8),"Question: What are the challenges and future prospects in adapting language models for NLP tasks?

1. Transforming input from tasks like IR and sentence labeling into a few/zero-short instruction form is non-trivial.
2. There may be better ways to adapt language models to traditional NLP tasks in the future.
3. On the other hand, the upper limit of capabilities of fine-tuned models is not reached, and some methods like FLAN-tuning [67] can further boost the performance on NLU tasks.
4. Another interesting finding is that on NLU tasks, after fine-tuning, masked language models, like T5 [85], are better than most auto-regressive language models at the same scale, while some recent results imply that this gap can be bridged by scaling [22].","1. What are the challenges in adapting language models for NLP tasks?
2. What are the future prospects in adapting language models for NLP tasks?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s13),(p13.10),"Question: What are examples of tasks showcasing LLMs' generalization ability in NLP?

1. One of the representative tasks is miscellaneous text classification [59].
2. In contrast to classic domain-specific text classification tasks such as sentiment analysis, miscellaneous text classification deals with a diverse range of topics and categories that may not have a clear or strong relationship with one another.
3. It's closer to real-world cases and hard to be formatted for using fine-tuned models.
4. Another is the Adversarial NLI (ANLI)[74].
5. It is a difficult dataset composed of adversarially mined natural language inference questions in three rounds (R1, R2, and R3).
6. LLMs have shown superior performance on ANLI, especially on the R3 and R2.
7. Both examples demonstrate the exceptional ability of LLMs to generalize well on out-of-distribution and sparsely annotated data in traditional NLP tasks, surpassing that of fine-tuned models.
8. We've discussed this in the section above 3.3.",1. What are examples of tasks showcasing LLMs' generalization ability in NLP?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s15),(p15.4),"Question: How do LLMs compare to commercial tools in machine translation performance?

1. In machine translation (MT), LLMs can perform competent translation, although the average performance is slightly worse than some commercial translation tools [45] considering some automatic metrics like BLEU [78].
2. LLMs are particularly good at translating some low-resource language texts to English texts, such as in the Romanian-English translation of WMT'16 [11], zero-shot or few-shot LLMs can perform better than SOTA fine-tuned model [22].
3. This is mainly due to the fact that English resources compose the main part of the pre-training data.
4. BLOOM [92] is pre-trained on more multi-lingual data, leading to better translation quality in both rich-resource and low-resource translation.",1. How do LLMs compare to commercial tools in machine translation performance?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s15),(p15.6),"Question: What are the capabilities of LLMs in open-ended generations and code-related tasks?

1. Additionally, LLMs are highly skilled in open-ended generations.
2. One example is that the news articles generated by LLMs are almost indistinguishable from real news articles by humans [16].
3. LLMs are remarkably adept at code synthesis as well.
4. Either for text-code generation, such as HumanEval [18] and MBPP [7], or for code repairing, such as DeepFix [39], LLMs can perform pretty well.
5. GPT-4 can even pass 25% problems in Leetcode, which are not trivial for most human coders [76].
6. With training on more code data, the coding capability of LLMs can be improved further [22].","1. What are the capabilities of LLMs in open-ended generations?
2. What are the capabilities of LLMs in code-related tasks?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s18),(p18.3),"Question: What are closed-book question-answering tasks and how do LLMs perform on them?

1. Closed-book question-answering tasks require the model to answer a given question about factual knowledge without any external information.
2. It does require the memorization of real-world knowledge in the model.
3. LLMs perform better on nearly all datasets, such as on NaturalQuestions [52], WebQuestions [9], and TriviaQA[46].
4. On TriviaQA, even zero-shot LLMs is still much better [22].","1. What are closed-book question-anspiring tasks?
2. How do LLMs perform on them?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s18),(p18.4),"Question: What is the role of context in machine reading comprehension tasks?

1. The massive multitask language understanding (MMLU) [40] is also highly knowledge-intensive.
2. Some tasks only require the model to capture the self-contained knowledge in the contexts.
3. The knowledge in the contexts from the input is enough for the model to make predictions.
4. For these tasks, small fine-tuned models can work pretty well.
5. One such task is machine reading comprehension (MRC).
6. An MRC task provides several paragraphs and requires the model to predict the answer to questions based on these paragraphs.
7. We've discussed MRC in the previous section because it's also a traditional NLU task.",1. What is the role of context in machine reading comprehension tasks?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s19),(p19.0),"Question: How does scaling affect the performance and abilities of large language models (LLMs)?

1. Scaling of LLMs (e.g. parameters, training computation, etc.) can greatly empower pretrained language models.
2. With the model scaling up, a model generally becomes more capable in a range of tasks.
3. Reflected in some metrics, the performance shows a power-law relationship with the model scale.
4. For example, the cross-entropy loss which is used to measure the performance for language modeling decreases linearly with the exponential increase in the model scale, which is also called 'scaling-law' [41,47].
5. For some crucial abilities, such as reasoning, scaling the model has gradually transformed these abilities from a very low state to a usable state, and even approaching human capabilities.
6. In this section, we provide an overview of the usage of LLMs in terms of the abilities and behaviors of LLMs along with scaling.","1. How does scaling affect the performance of large language models (LLMs)?
2. How does scaling affect the abilities of large language models (LLMs)?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s22),(p22.0),"Question: What are emergent abilities in large language models (LLMs)?

1. Scaling of models also endows the model with some unprecedented, fantastic abilities that go beyond the power-law rule.
2. These abilities are called ""emergent ability"".
3. As defined in [113], emergent abilities of LLMs are abilities that are not present in smaller-scale models but are present in large-scale models.
4. This means such abilities cannot be predicted by extrapolating the performance improvements on smaller-scale models and the model suddenly gains good performance on some tasks once the scale exceeds a certain range.
5. The emergent ability is typically unpredictable and surprising, leading to tasks that emerge randomly or unexpectedly.
6. We examine concrete examples of the emergent abilities of LLMs and provide them as an important reference for deciding whether to leverage LLMs' emergent abilities.",1. What are emergent abilities in large language models (LLMs)?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s22),(p22.2),"Question: What emergent abilities do large language models like GPT-3 and PaLM exhibit?

1. For example. GPT-3 [16] shows the emergent ability for word sorting, and word unscrambling tasks.
2. PaLM [22] exhibits the emergent ability on ASCII word recognition 4 and hyperbaton 5 task.
3. The logical abilities of language models tend to emerge as the model scales up, such as logical deduction, logical sequence, and logic grid puzzles.
4. Additionally, other tasks, such as advanced coding (e.g., auto debugging, code line description), and concept understanding (e.g., novel concepts, simple Turing concepts), are also use cases with the emergent abilities of large language models.",1. What emergent abilities do large language models like GPT-3 and PaLM exhibit?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s23),(p23.3),"Question: What are the key phenomena and their explanations in scaling large language models?

1. Gaining a deeper understanding of emergent abilities, inverse scaling phenomenon and U-shape phenomenon in LLMs is essential for advancing research in this field.
2. In a certain sense, the U-shape phenomenon suggests that small-scale models and huge-scale models make predictions with different internal mechanisms.
3. From this perspective, the U-shape phenomenon can be seen as a transformation of the inverse-scaling phenomenon due to some emergent abilities from sufficiently large models [114].
4. GPT-4 [76] exhibits a reversal of the inverse scaling phenomenon in some cases, such as on a task called Hindsight Neglect.
5. The explanation for these behaviors of LLMs during scaling is still an open problem.
6. Several hypotheses have been proposed.
7. For emergent abilities, one explanation is that there may be multiple key steps for a task and the LLM cannot handle this task until it's large enough to handle every step, and another explanation is focused on the granularity of evaluation metrics [113].
8. For inverse-scaling phenomenon and u-shape phenomenon, the explanations mainly focus on the model's over-reliance on information from its prior rather than the input prompts, valid but misleading few-shot examples, and distracting easier tasks within a hard task [114].","1. What are the key phenomena in scaling large language models?
2. What are their explanations?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s26),(p26.1),"Question: Why do LLMs underperform in regression tasks compared to discrete label predictions?

1. Although LLMs have achieved remarkable success in various natural language processing tasks, their performance in regression tasks has been less impressive.
2. For example, ChatGPT's performance on the GLUE STS-B dataset, which is a regression task evaluating sentence similarity, is inferior to a fine-tuned RoBERTa performance [130].
3. The Regression tasks typically involve predicting a continuous value rather than a discrete label, posing unique challenges for LLMs.
4. One primary reason for their subpar performance is the inherent difference between the language modeling objective and the regression task objective.
5. LLMs are designed to predict the next word in a sequence or generate coherent text, with their pre-training focused on capturing linguistic patterns and relationships.
6. Consequently, their internal representations may not be well-suited for modeling continuous numerical outputs.
7. Besides, LLMs have predominantly been trained on text data, focusing on capturing the intricacies of natural language processing.
8. As a result, their performance on multimodal data, which involves handling multiple data types such as text, images, audio, video, actions, and robotics, remains largely unexplored.
9. And fine-tuned multimodal models, like BEiT [110] and PaLI [19], still dominate many tasks such as visual question answering (VQA) and image captioning.
10. Nonetheless, the recently introduced GPT-4 [76] has taken the step in multimodal fusion, but there is still a lack of detailed evaluation of its capabilities.",1. Why do LLMs underperform in regression tasks compared to discrete label predictions?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s27),(p27.3),"Question: How do LLMs perform in NLG task quality assessment compared to traditional metrics?

1. LLMs can also be used for quality assessment on some NLG tasks, such as summarization and translation.
2. On summarization tasks, GPT-4 as an evaluator achieves a higher correlation with humans than other methods with a large margin [64].
3. Some other evaluators based on LLMs [34,50,64,108] also show good human alignment in more NLG tasks, especially compared with traditional automatic metrics.
4. But the LLM evaluator may have a bias towards the LLM-generated texts [64].","1. How do LLMs perform in NLG task quality assessment?
2. How does their performance compare to traditional metrics?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s29),(p29.2),"Question: How do instruction and human alignment tuning enhance LLMs' performance and user preference?

1. Additionally, some mechanics such as instruction tuning [91,112] and human alignment tuning [77] further boost the capabilities of LLMs to better comprehend and follow user instructions.
2. These methods improve the model's ability to generate helpful, harmless, and honest responses while maintaining coherence and consistency [77,91,112].
3. While both methods can make LLMs better generalize to unseen tasks and instructions, it has been noticed that while human labelers prefer models tuned for human alignment [77] to models tuned with instructions from public NLP tasks, such as FLAN [112] and T0 [91].
4. The reason may be similar to reasons for fine-tuned models' inferiority: public NLP tasks/datasets are designed for easy and automatic evaluation, and they can only cover a small part of real-world usage.","1. How do instruction and human alignment tuning enhance LLMs' performance?
2. How do they enhance user preference?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s32),(p32.2),"Question: What are the computational and financial costs of training large AI models like T5 11B?

1. Flops, while a T5 11B model only requires 3.30 × 10 22 , which is 10 times less.
2. In addition to these costs, hardware requirements are also substantial.
3. OpenAI has collaborated with Microsoft on a supercomputer hosted in the Microsoft Azure cloud, consisting of 285k CPU cores and 10k high-end GPUs to support the training of large models.
4. For users of the OpenAI API, pricing varies based on the model and usage, with options such as GPT-3.5-turbo charging $0.002 per 1k tokens for chat service.
5. However, for users who require custom models, training costs $0.03 per 1k tokens, while usage costs $0.12 per 1k tokens [4].
6. Therefore, for users who cannot afford such a large cost, such as small startups, individual users, etc., a small, fine-tuned model is a better and more reasonable choice.","1. What are the computational costs of training large AI models like T5 11B?
2. What are the financial costs of training large AI models like T5 11B?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s32),(p32.4),"Question: What is Parameter-Efficient Tuning and its significance in model optimization?

1. Parameter-Efficient Tuning. In practice, we may tune the model on some specific datasets.
2. Parameter-Efficient Tuning (PET) is an efficient technique to tune a small portation of model parameters (or extra parameters) while freezing most parameters of the pre-trained LLMs.
3. The main goal of PEFT is to greatly decrease the computational and storage costs while keeping the performance of the original models.
4. The common techniques for PET are LoRA [42], Prefix Tuning [58], P-Tuning [62,63].
5. As an illustration, the LoRA method maintains the weights of the pre-trained model and incorporates low-rank matrices into every layer of the Transformer architecture.
6. This approach considerably minimizes the number of parameters that require training for subsequent tasks, thereby increasing overall efficiency.","1. What is Parameter-Efficient Tuning?
2. What is its significance in model optimization?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s33),(p33.2),"Question: How does fine-tuning affect model robustness and calibration in specific scenarios?

1. The models that have high accuracy on the scenario also have good robustness.
2. However, the robustness of the zero-shot becomes worse after being tuned on extra application-specific tasks data [116].
3. This may due to overfitting, which leads to poor generalizability due to the extremely high complexity of the model and the limited training samples from downstream tasks [43].
4. In a similar vein, it has been observed that fine-tuning a model can result in significant miscalibrations, owing to over-parameterization [51].
5. Therefore, fine-tuned models may not be an optimal choice when robustness and calibration are critical considerations.
6. However, human alignment has been found as a potential solution for enhancing model robustness.
7. InstructGPT davinci v2 (175B*) has been shown to outperform other models in terms of robustness.
8. On the other hand, achieving optimal calibration of the model depends on the scenario and adaptation procedure employed.","1. How does fine-tuning affect model robustness?
2. How does fine-tuning affect model calibration in specific scenarios?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s33),(p33.3),"Question: How do LLMs exhibit and potentially perpetuate societal biases and discrimination?

1. Fairness and Bias. LLMs have been shown to exhibit disparate treatment and impact, perpetuating societal biases and potentially leading to discrimination [10,17].
2. To ensure fairness and equity for all users, it is crucial to address these issues in the development and deployment of NLP models.
3. Disparities in performance between demographic groups can serve as an indicator of fairness problems.
4. LLMs are particularly susceptible to fairness issues, as significant performance disparities have been observed across demographic categories such as dialect, religion, gender, and race [59].
5. However, research has shown that aligning models with human instructions can improve LLM performance regardless of their size, with the InstructGPTmodel (davinci v2) exhibiting smaller performance disparities than other LLMs [23].","1. How do LLMs exhibit societal biases and discrimination?
2. How could LLMs potentially perpetuate societal biases and discrimination?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s33),(p33.4),"Question: What are the challenges of shortcut learning in natural language understanding tasks?

1. Spurious Biases. The shortcut learning problem has been observed in various natural language understanding tasks under the pretraining and fine-tuning paradigm, where models heavily rely on spurious correlations between input and labels in the fine-tuning data for prediction [31,35,98].
2. For example, in reading comprehension tasks, fine-tuned models tend to focus on the lexical matching of words between the question and the original passage, neglecting the intended reading comprehension task itself [53].
3. In contrast, large language models are not directly trained on fine-tuned datasets, which makes it less likely for them to learn shortcut features present in the fine-tuned dataset, thereby enhancing the model's generalization capabilities.
4. However, LLMs are not infallible and may exhibit some shortcut learning during in-context learning.
5. For example, recent preliminary studies have begun investigating the robustness of prompt-based methods in large-scale language models [111,129].
6. One such study evaluates the few-shot learning performance of GPT-3 on text classification and information extraction tasks [129].
7. and reveal that the examined LLMs are susceptible to majority label bias and position bias, where they tend to predict answers based on the frequency or position of the answers in the training data.
8. Moreover, these LLMs exhibit common token bias, favoring answers that are prevalent in their pre-training corpus.
9. Recent studies show that this positional bias can be mitigated by selecting proper prompts [68].
10. In summary, while LLMs significantly reduce the shortcut learning problem prevalent in fine-tuned models, they still exhibit some shortcut learning issues and should be approached with caution when deploying them in downstream applications.",1. What are the challenges of shortcut learning in natural language understanding tasks?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s34),(p34.3),"Question: What are the risks and necessary safeguards associated with harmful content generated by LLMs?

1. Harmful content. Due to the high coherence, quality, and plausibility of texts generated by LLMs, harmful contents from LLMs can cause significant harm, including hate speech, discrimination, incitement to violence, false narratives, and even social engineering attack.
2. The implementation of safeguards to detect and correct those contents can be mitigation [97].
3. These LLMs can also have dual-use potential by providing required illicit information, leading to risks such as the proliferation of weapons [75] and even terrorism attack planning.
4. It is crucial to ensure using these LLMs responsibly, with safeguards in place to prevent harm.
5. Also, in existing work, feedback from humans plays an important role in getting rid of harmful outputs.","1. What are the risks associated with harmful content generated by LLMs?
2. What necessary safeguards should be implemented to mitigate these risks?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s34),(p34.4),"Question: What are the privacy concerns associated with Large Language Models (LLMs)?

1. Privacy. LLMs can face serious security issues.
2. An example is the issue of user privacy.
3. It is reported that Samsung employees were using ChatGPT to process their work when they inadvertently leaked top-secret data, including the source code proper of the new program, internal meeting minutes related to the hardware, etc.
4. The Italian data protection agency declared that OpenAI, the developer of ChatGPT, illicitly gathered personal user data, leading Italy to become the first government to prohibit ChatGPT over privacy concerns [1].",1. What are the privacy concerns associated with Large Language Models (LLMs)?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s7),(p7.0),"Question: How does decomposition simplify multi-hop MRC challenges?

1. Complicated question is a basic challenge of multi-hop MRC, unlike the single-hop questions, they cannot be answered easily and require complicated reasoning.
2. Since the human reasoning about complex questions is done by decomposition, answering subquestions, summarizing, and comparing [26], then this technique has focused on simplifying the problem by decomposition of a complex question into multiple simple sub-questions.
3. It means it reduces multi-hop MRC to multiple single-hop MRC.
4. This technique mostly uses the single-hop MRC models to find the answers to sub-questions and then combine the answers to obtain the final answer.
5. In the following, the models which use this technique for multi-hop MRC will be reviewed in detail.",1. How does decomposition simplify multi-hop MRC challenges?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s11),(p11.0),"Question: What are the characteristics and applications of sequence models in MRC tasks?

1. The sequence models have been first used for single-hop MRC tasks, and most of them are based on Recurrent Neural Networks (RNNs), some studies focus on using them in multi-hop MRC.
2. It can be called state-based reasoning models and they are closer to a standard attention-based RC model with an additional ""state"" representation that is iteratively updated.
3. The changing state representation results in the model focusing on different parts of the passage during each iteration, allowing it to combine information from different parts of the passage [28].
4. Most models, presented in this section, have used advanced neural network concepts, such as attention mechanism and network memory for multi-hop reasoning.
5. In the following the models which use this technique will be reviewed in detail including the architecture alongside the superiority and the motivation of them.","1. What are the characteristics of sequence models in MRC tasks?
2. What are the applications of sequence models in MRC tasks?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s15),(p15.3),"Question: How does the PathNet model identify and score potential paths for answering questions?

1. They first find all possible path from passages.
2. It starts with selecting a passage that contains a head entity from the question, and then finds all entities and noun phrases from the same sentence.
3. Afterward, it selects the next passage that contains the potential intermediate entity identified above.
4. Finally, it is checked whether the next passage contains any of the candidate answer choices or not.
5. The resulting will be a set of entity sequences.
6. After obtaining all potential paths, it is time to score each path using the PathNet model based on two perspectives: 1) Context-based Path Scoring, which is based on the interaction with the question encoding, and 2) Passage-based Path Scoring, which is based on the interaction between the passage-based path encoding vector and the candidate encoding.
7. There is an example of the process in Figure 15 which In the Rank-1 path, the model composes the implicit located in relations between (Zoo lake, Johannesburg) and (Johannesburg, Gauteng).
8. However, this method extracts many invalid paths, then causes wasting the computing resources [35].","1. How does the PathNet model identify potential paths for answering questions?
2. How does it score these paths?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s16),(p16.1),"Question: What is the Sentence-level Multi-hop Reasoning approach in document analysis?

1. As Figure 16 shows, after embedding sentences into a matrix and constructing a graph, the policy is run to select the next sentence.
2. In this regard, the convolution-based policy network is used to learn a traversal policy from current options, previously accepted sentences, and the current question.
3. For example, in this figure, the policy has learned to select the correct next sentence (ct) from the previous sentence (ct-1).
4. After this action, the answer sentence (o1) can be selected during the next step.
5. SMR: Huo, Ge and Zhao [37] proposed a Sentence-level Multi-hop Reasoning approach named SMR while most existing approaches only use document-level or entity-level inferences.
6. In this regard, an initial sentence is first found based on the main entity in the question, and that sentence is then used to find more relevant sentences to create multiple sentence-based reasoning chains as a memory network.
7. Besides, some sentences are concatenated to prevent the mistakes of Co-Reference resolution methods and to reduce the number of hops.
8. An example of such a concatenation is shown in Figure 17.",1. What is the Sentence-level Multi-hop Reasoning approach in document analysis?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s16),(p16.2),"Question: What are the phases and methodology of ChainEx's sentence-based model for multi-hop reasoning?

1. There are two phases in this model: 1) the Selecting phase to select the most relevant sentence to the network memory state as the initial sentence of the current hop, and 2) the Establishing phase to prepare to go to the next hop by updating the network memory state.
2. Finally, the information of the reasoning chains is used to predict the final answer.
3. Figure 17: Representation enrichment via concatenation of adjacent sentences [37] ChainEx: Chen, Lin and Durrett [38] proposed a sentence-based model that does not rely on gold annotated chains or supporting facts at the training and test phases, instead, pseudo-gold reasoning chains are derived using some heuristics based on named entity recognition and coreference resolution during the training time, and it learns to extract chains from raw texts at the test time.
4. They first extract a reasoning chain over the text for a multi-hop reasoning task, and then apply a BERT-based QA system to find the answer by learning from these chains.","1. What are the phases of ChainEx's sentence-based model for multi-hop reasoning?
2. What is the methodology of ChainEx's sentence-based model for multi-hop reasoning?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s17),(p17.0),"Question: Why have graph-based techniques gained attention in multihop MRC?

1. Graph-based techniques because of their natural language relationship representation ability [39] has attracted attention in multihop MRC.
2. It is natural to model natural language context into graph structure and the process of multi-hop reasoning as moving among nodes [14].
3. The main idea of the Graph-based technique is to construct a graph based on the context and question, and then the reasoning is performed by message passing over this structure using graph neural networks.
4. The process of constructing the graph from large textual data and reasoning over it are challenging tasks.
5. There are a lot of studies that focus on these challenges which are explained in this subsection in detail.",1. Why have graph-based techniques gained attention in multihop MRC?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s19),(p19.0),"Question: How does Song et al.'s model improve global context inference in multi-hop reading comprehension?

1. Song et al. [40] focused on inferring global context as an important key in multi-hop reading comprehension, while previous studies approximate global evidence with local coreference information with DAG-styled GRU.
2. They proposed a model for better connecting global evidence, with a more complex graph compared to DAGs.
3. They construct an entity graph with three types of edges: the edge between the same entity within a passage, the edges between two mentions of different entities within a context window, and coreference-typed edges.
4. The graph might also have cycles which makes it difficult to apply a DAG network to it.
5. (A graph with three types of edges and a DAG graph are shown in Figure 21).",1. How does Song et al.'s model improve global context inference in multi-hop reading comprehension?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s19),(p19.1),"Question: How do GCN and GRN contribute to evidence aggregation in graph-based models?

1. For inferring the global context, the related information of the constructed graph has been merged.
2. In this study, two recent graph neural networks have been applied to this graph: graph convolutional network (GCN) and graph recurrent network (GRN) for evidence aggregation.
3. Afterward, an attention mechanism is applied in order to match the hidden states at each graph encoding step with the question representation.
4. Finally, a probability distribution is calculated from the matching results.
5. The architecture of this model is shown in Figure 22.
6. However, this model still only implicitly combines knowledge from all passages, and are therefore unable to provide explicit reasoning paths [28].
7. DFGN: Xiao et al. [41] proposed a model to improve the interaction between the information of documents and the entity graph.","1. How do GCN contribute to evidence aggregation in graph-based models?
2. How do GRN contribute to evidence aggregation in graph-based models?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s22),(p22.0),"Question: What is the structure and purpose of the Heterogeneous Document-Entity graph?

1. Tu et al. [46] proposed a more complex graph named Heterogeneous Document-Entity (HDE) graph with different types of nodes and edges.
2. This graph can cover different granularity levels of information in context and also enables rich information interaction among different types of nodes for accurate reasoning.
3. The nodes in the HDE graph are candidates, documents, and entities.
4. Besides, it has seven types of edges: 1) between a document node and a candidate node that appears in the same document.","1. What is the structure of the Heterogeneous Document-Entity graph?
2. What is the purpose of the Heterogeneous Document-Entity graph?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s22),(p22.2),"Question: What is the Select, Answer, and Explain (SAE) system and how does it work?

1. However, [35] have shown that if the number of inferences increases, the complexity of models will rise sharply due to the iteration of cumbersome message passing algorithm, resulting in low efficiency.
2. [46] SAE: Tu et al. [47] proposed an interpretable system named Select, Answer, and Explain (SAE) with multi-hop reasoning graphs based on GNN with contextual sentence embeddings as nodes.
3. This kind of sentence graph makes lead to an interpretable model because it can directly output supporting sentences with the answer prediction.
4. The edges capture the global information presented within each document and also the cross-document reasoning path.
5. Also, the contextual sentence embedding used in GNN is summarized over token representations based on a novel mixed attentive pooling mechanism.
6. The attention weight is calculated from both answer span logits and self-attention output on token representations.
7. This attention-based interaction enables the exploitation of complementary information between ""answer"" and ""explain"" tasks.
8. The SAE system first filters unrelated documents, and selects gold documents using a document classifier trained with a novel pairwise learning-to-rank
9. loss function.","1. What is the Select, Answer, and Explain (SAE) system?
2. How does it work?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s24),(p24.1),"Question: How does graph structure impact multi-hop question answering effectiveness?

1. GF: Shao et al. [53] attempted to answer this question: How much does graph structure contribute to answer a multi-hop question.
2. They reimplement a graph-based model-Dynamically Fused Graph Network [54]-and claimed that the graph structure can play an important role only if the pre-trained models are used in a feature-based manner, while if the pre-trained models are used in the fine-tuning approach, the graph structure may not be helpful.
3. Then the adjacency matrix based on manually defined rules and the graph structure can be regarded as prior knowledge, which could be learned by self-attention or Transformers.
4. They proved that when texts have been modeled as an entity graph, both graph-attention and self-attention can achieve comparable results, but when texts have been modeled as a sequence structure, only a 2-layer Transformer could achieve similar results as DFGN.
5. As you can see in Figure 40 when the entity graph is fully connected, a graph-attention layer will degenerate into a selfattention layer.
6. However, this study as the first attempt of a graph-free model suffers from huge performance gap compared to the state-art-of the graph based-models [14].
7. Figure 40: An example of the relation between the graph-attention network and the self-attention network [53] AMS: Yuntao et al. [52] proposed another non-graph model with a focus on the document filters step to denoise irrelevant documents, and proves that if this step has been done properly, even a single-hop model can be used for multi-hop.
8. They investigate that promising performance of the filter from Hierarchical Graph Network (HGN) (Fang et al., 2020) and showed that for 2paragraph selection, both precision and recall can achieve around 95%, and for 4-paragraph selection, recall will be nearly 99%.",1. How does graph structure impact multi-hop question answering effectiveness?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s24),(p24.2),"Question: What are the AMS and S2G models' approaches to multi-hop question answering?

1. Then they proposed Answer Multi-hop questions by Single-hop QA (AMS) models and used a single-hop QA models based on the attention mechanism with the HGN's document filter.
2. As you can see in Figure 41 after the Document denoise layer, they used an Attention-based single-hop layer.
3. AMS could achieve a comparable result with stat-of-the-art graph-bade models but still couldn't improve them.
4. Figure 41: Overall architecture of AMS [52] S2G: Wu, Zhang, and Zhao [14] investigated whether graph modeling is necessary for multi-hop.
5. In this regard, they first proved that the retrieval stage is the most important module, while the existing studies focus on the reader module by graph modeling.
6. This study presents a graph-free alternative named select-to-guide (S2G) to retrieve evidence paragraphs in a coarse-tofine manner, incorporated with two attention mechanisms, which shows conforming to the nature of multi-hop reasoning.
7. For the paragraph retrieval module, this study introduced a cascaded paragraph retrieval module that retrieves the evidence paragraphs in an explicit coarse-to-fine manner, and in multi-task module there are one shared encoder module alongside with two interdependent modules with an attention mechanism ( Figure 42).
8. However Concrete error analysis on S2G shows that there is still room for improvement on the multi-hop retriever module design.
9. Figure 42: The multi-task module of S2G [14]
10. As the last part of this section, Figure 43 has been prepared to summarize the techniques and models.",1. What are the AMS and S2G models' approaches to multi-hop question answering?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s29),(p29.0),"Question: How are model performances evaluated on HotpotQA according to the study?

1. In this section, the performance of models on HotpotQA [11] will be investigated.
2. Exact-match (EM) and F1 metrics have been used in this study to evaluate the model performance.
3. EM is used to show whether the predicated answer and the ground-truth answer are exactly the same.
4. F1 measures the average overlap between the predicted answer and the ground-truth answer.
5. In addition, there are three sets of metrics: 1) Answer EM/F1 to evaluate the answer span prediction, 2) Support EM/F1 to evaluate the supporting facts prediction, and 3) Joint EM/F1 to combine the two previous ones.",1. How are model performances evaluated on HotpotQA according to the study?
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s1),(p1.2),"Question: How can the expressive power of a tree-like architecture model be enhanced for specific tasks?

1. The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task.
2. A solution is to equip the shared trunk with task-specific encoders [47,58,137].
3. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages.
4. Another way is to make different groups of tasks share different parts of the trunk [37,79,89].
5. Moreover, this idea can be applied to the decoder.
6. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.","1. How can the expressive power of a tree-like architecture model be enhanced?
2. What specific tasks can benefit from this enhancement?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s1),(p1.5),"Question: How do models control information flow in multi-task learning to reduce inter-task interference?

1. However, different parts of the shared features are not equally important to each task.
2. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks.
3. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate.
4. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks.
5. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism.
6. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | ·
7. | compute the absolute value in an element-wise manner.
8. After that, S is concatenated with task-specific representations to form the input to the output layer.","1. How do models control information flow in multi-task learning?
2. How do they reduce inter-task interference?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s1),(p1.8),"Question: How does task routing contribute to feature fusion in models like MCapsNet?

1. Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task.
2. Given tasks , the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight.
3. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task.
4. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .",1. How does task routing contribute to feature fusion in models like MCapsNet?
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s2),(p2.0),"Question: How do parallel architecture models handle tasks at different abstraction levels in NLP?

1. Feature Levels. Models using the parallel architecture handle multiple tasks in parallel.
2. These tasks may concern features at different abstraction levels.
3. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level.
4. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c.
5. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers.
6. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features.
7. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified.
8. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.",1. How do parallel architecture models handle tasks at different abstraction levels in NLP?
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s2),(p2.1),"Question: How can auxiliary tasks at different levels enhance MTL performance in various applications?

1. In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful.
2. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154].
3. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task.
4. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks.
5. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.","1. How can auxiliary tasks at different levels enhance MTL (Multi-Task Learning) performance?
2. What are the various applications of MTL where auxiliary tasks can enhance performance?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s4),(p4.1),"Question: What is the principle of hierarchical feature pipeline in multi-task models?

1. In hierarchical feature pipeline, the output of one task is used as extra features for another task.
2. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks.
3. For example, [14] feeds the output of a question-review pair recognition model to the question answering model.
4. [44] feeds the output of aspect term extraction to aspect-term sentiment classification.
5. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations.
6. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections.
7. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.",1. What is the principle of hierarchical feature pipeline in multi-task models?
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s4),(p4.2),"Question: How is hierarchical feature pipeline utilized in various natural language processing tasks?

1. Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships.
2. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction.
3. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks.
4. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task.
5. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task.
6. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.",1. How is hierarchical feature pipeline utilized in various natural language processing tasks?
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s6),(p6.1),"Question: What are common practices in designing modular architectures for multi-task learning?

1. The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1.
2. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did.
3. [1] shares word and character embedding matrices and combines them differently for different tasks.
4. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT).
5. Modular designs are also widely used in multi-lingual tasks.
6. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks.
7. Shared embeddings can be used alongside task-specific embeddings [59,138] as well.
8. In addition to word embeddings, [147] shares label embeddings between tasks.
9. Researchers have also developed modular architectures at a finer granularity.
10. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation.
11. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer.
12. [92] creates encoder modules on different levels, including task level, task group level, and universal level.",1. What are common practices in designing modular architectures for multi-task learning?
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s7),(p7.0),"Question: How do Generative Adversarial Networks (GANs) enhance generative tasks and multitask learning in NLP?

1. Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision.
2. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones.
3. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images.
4. A similar idea can be used in MTL for NLP tasks.
5. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model.
6. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.","1. How do Generative Adversarial Networks (GANs) enhance generative tasks in NLP?
2. How do GANs enhance multitask learning in NLP?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s7),(p7.1),"Question: How do generative adversarial architectures utilize unlabeled data to enhance model performance?

1. An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized.
2. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents.
3. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach.
4. First, a discriminator that rates the quality of candidate answers is trained on labeled samples.
5. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.","1. How do generative adversarial architectures utilize unlabeled data?
2. How does this utilization enhance model performance?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s9),(p9.10),"Question: How does GradVac improve multi-lingual model performance through gradient manipulation?

1. Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients.
2. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g .
3. Notice that PCGrad is a special case of GradVac when = 0.
4. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.","1. How does GradVac improve multi-lingual model performance?
2. What role does gradient manipulation play in this improvement?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s11),(p11.0),"Question: What is task scheduling in MTL model training and how is it implemented?

1. Task scheduling determines the order of tasks in which an MTL model is trained.
2. A naive way is to train all tasks together.
3. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions.
4. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together.
5. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step.
6. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.","1. What is task scheduling in MTL model training?
2. How is it implemented?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s11),(p11.5),"Question: What is the significance of sequential task learning in multi-task models?

1. In some cases, multiple tasks are learned sequentially.
2. Such tasks usually form a clear dependency relationship or are of different difficulty levels.
3. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties.
4. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches.
5. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1).
6. Unicoder [49] trains its five pre-training objectives sequentially in each step.
7. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain.
8. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters.
9. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.",1. What is the significance of sequential task learning in multi-task models?
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s11),(p11.6),"Question: What is the pre-train then fine-tune methodology in auxiliary MTL?

1. For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks.
2. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task.
3. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data.
4. [125] first pre -trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data.
5. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.",1. What is the pre-train then fine-tune methodology in auxiliary MTL?
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s14),(p14.0),"Question: How is auxiliary multi-task learning applied in various classification and detection tasks?

1. Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification.
2. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks.
3. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction.
4. [59] enhances a rumor detection model with user credibility features.
5. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance.
6. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token.
7. In [107], text classification is boosted by learning the predominant sense of words.
8. [133] assists the fake news detection task by stance classification.
9. [14] jointly learns the answer identification task with an auxiliary question answering task.
10. To improve slot filling performance for online shopping assistants, [56]0 adds NER and segment tagging tasks as auxiliary tasks.
11. In [56]1, the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.","1. How is auxiliary multi-task learning applied in classification tasks?
2. How is auxiliary multi-task learning applied in detection tasks?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s14),(p14.2),"Question: How does Multi-Task Learning (MTL) enhance text generation quality in NMT models?

1. For text generation tasks, MTL is brought in to improve the quality of the generated text.
2. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain.
3. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks.
4. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks.
5. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data.
6. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder.
7. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks.
8. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks.
9. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155].
10. (NMT)0 implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing.
11. (NMT)1 enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks.
12. (NMT)2 views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.",1. How does Multi-Task Learning (MTL) enhance text generation quality in NMT models?
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s15),(p15.0),"Question: How do joint MTL models differ from auxiliary MTL in optimizing task performance?

1. Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously.
2. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other.
3. Table 2 gives an overview of task combinations used in joint MTL models.
4. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other.
5. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively.
6. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model.
7. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks.
8. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification.
9. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews.
10. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures.
11. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks.
12. [40,141,145] learn entity extraction alongside relation extraction.
13. For sentiment analysis tasks, [110]0 jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture.
14. [110]1 learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis.
15. [110]2 builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.",1. How do joint MTL models differ from auxiliary MTL in optimizing task performance?
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s15),(p15.2),"Question: What is joint MTL and its applications in multi-domain and multi-formalism NLP tasks?

1. Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks.
2. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions.
3. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118].
4. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces.
5. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [83]0, Predicate-Argument Structures [83]1 [83]2, and Prague Semantic Dependencies [83]3 [83]4) jointly.
6. In [83]5, a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation [83]6 [83]7, Semantic Dependency Parsing [83]8 [83]9, and Universal Dependencies [21]0 [21]1, and it shows that joint training improves performance on the testing UCCA dataset.
7. [21]2 jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT.
8. [21]3 shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks.
9. In [21]4, an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.","1. What is joint MTL?
2. What are its applications in multi-domain NLP tasks?
3. What are its applications in multi-formalism NLP tasks?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s16),(p16.0),"Question: How does multi-task learning benefit multi-lingual machine learning models?

1. Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections.
2. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models.
3. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English.
4. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively.
5. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.",1. How does multi-task learning benefit multi-lingual machine learning models?
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s16),(p16.1),"Question: How does multi-lingual MTL facilitate knowledge transfer and improve language processing tasks?

1. Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages.
2. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese.
3. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process.
4. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts.
5. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English.
6. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.","1. How does multi-lingual MTL facilitate knowledge transfer?
2. How does it improve language processing tasks?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s16),(p16.2),"Question: How can cross-lingual knowledge transfer be achieved through language representations?

1. Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations.
2. [108] learns multi-lingual representations from two tasks.
3. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples.
4. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification.
5. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training.
6. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels.
7. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way.
8. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.",1. How can cross-lingual knowledge transfer be achieved through language representations?
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s17),(p17.0),"Question: What is multimodal learning in NLP and its significance?

1. As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community.
2. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks.
3. MTL is a natural choice for implicitly injecting multimodal features into a single model.
4. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language.
5. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.","1. What is multimodal learning in NLP?
2. What is its significance?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s18),(p18.0),"Question: What factors influence task selection for effective multi-task learning in NLP?

1. A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training.
2. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks.
3. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy.
4. This finding also holds for rumor verification [53].
5. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other.
6. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels.
7. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets.
8. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.",1. What factors influence task selection for effective multi-task learning in NLP?
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s18),(p18.1),"Question: How does multi-task learning (MTL) performance reveal task relatedness and complementarity?

1. As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness.
2. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks.
3. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted.
4. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar.
5. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks.
6. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.","1. How does multi-task learning (MTL) performance reveal task relatedness?
2. How does multi-task learning (MTL) performance reveal task complementarity?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s21),(p21.1),"Question: How can multi-label datasets be created from existing data?

1. Multi-label datasets can be created by giving extra manual annotations to existing data.
2. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input.
3. [121] labels Twitter posts with 4 demographic labels.
4. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.",1. How can multi-label datasets be created from existing data?
231603122,Persuasive Natural Language Generation -A Literature Review,"Computer Science, Business, Linguistics",(s2),(p2.1),"Question: What is the focus and methodology of the literature review on persuasion and NLG?

1. The first step is the definition of the review scope of this literature review.
2. It is summarized in Figure 1 (categories applicable to this review on Persuasion and NLG research are highlighted) which is based on the taxonomy adapted by vom Brocke et al. (2009).
3. Brocke et al. 2009) This literature review focuses on outcomes of applied research in the domains of Persuasion and Natural Language Generation.
4. The goal is to integrate findings with respect to five categories concluded from the business problem of persuading individuals through textual exchange (DeMers 2016).
5. These categories were chosen as they address the psychological and technical aspects of persuasion and are a prerequisite to creating a persuasive NLG artificial intelligence.
6. We selected this field because the artificial generation of persuasion through NLG has, as this literature review reveals, not been addressed in seminal articles following our structured research approach.
7. Persuasion is already commonly studied in psychology (Quirk et al. 1985, Marwell & Schmitt 1967. Also, numerous studies in natural language processing focus on identifying and classifying persuasion in an automated way (Li et al. 2020, Rocklage et al. 2018, Iyer & Sycara 2019).
8. This paper is organized along a conceptual structure.
9. We did not take a particular perspective to provide a neutral representation of the results.
10. As an audience of this review specialized scholars having an interest in the field of persuasion and the artificial generation of it were chosen.
11. For coverage, our literature review can be categorized as representative as our research has been limited to certain journals, but does not consider the totality of the literature.
12. The second step is conceptualization of the topic .","1. What is the focus of the literature review on persuasion and NLG?
2. What is the methodology of the literature review on persuasion and NLG?"
231603122,Persuasive Natural Language Generation -A Literature Review,"Computer Science, Business, Linguistics",(s5),(p5.1),"Question: What are the determinants of linguistic appropriacy in persuasive NLG AI?

1. This category subsumes fourteen determinants that facilitate an individual's stylome and aim at matching this with linguistic appropriacy.
2. Such a stylome can be quantified and identified linguistically (Zarouali et al. 2020).
3. Aforementioned Language Expectation Theory identifies written or spoken language as a rule-based system through which persuadees develop expectations (Burgoon & Miller 1985).
4. The reason for profiling the stylome of an individual is to match these expectations (Park et al. 2015).
5. Once implemented, a persuasive NLG AI can achieve congruence between a persuasive message and the persuadee, and thus generate persuasiveness.
6. Table 2 introduces fourteen determinants of linguistic appropriacy in alphabetical order, provides a synopsis (i.e., brief summary, column two), an example for each determinant (column three), and the corresponding academic citation (in column four).  Quirk et al. 1985, Brewer 1980 Evidence Words Tendency to approve or disapprove something.
7. Words such as: according to. Quirk et al. 1985 Familiarity Degree of familiarity of a word or how easily a word can be recognized by an adult.
8. Word Frequency Indication of how often used words occur in a given language.",1. What are the determinants of linguistic appropriacy in persuasive NLG AI?
231603122,Persuasive Natural Language Generation -A Literature Review,"Computer Science, Business, Linguistics",(s10),(p10.0),"Question: What tools and datasets are used for persuasion analysis in NLP studies?

1. In the analyzed academic studies, we found that the authors use different datasets and tools to computationally process data for technical analyses of persuasion in NLP or NLG (e.g., in Guerini et al 2008a/b, Li et al. 2020, Iyer/Sycara 2019. Logically, the implementation of a persuasive NLG AI also depends on a variety of relevant tools and datasets which we identified and consolidated in Table 6. This table classifies our findings in types which are either tool or datasets (column one).
2. We identified six tools and seventeen persuasion or message datasets.
3. A software tool that is used in the context of persuasion and NLP, and datasets were chosen if they were used in the context of persuasion, textual exchange/debate and NLP.
4. We further added a synopsis (column three) explaining every tool and Authority Appealing or making reference to higher authority or experts to persuade.","1. What tools are used for persuasion analysis in NLP studies?
2. What datasets are used for persuasion analysis in NLP studies?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",(s1),(p1.0),"Question: What distinguishes this survey on multimodal learning and Transformers from others?

1. We relate this paper to existing surveys of the two specific dimensions MML and Transformers.
2. There exist a few MML surveys [1], [11], [12].
3. In particular, [1] proposed a structured, acknowledged taxonomy by five challenges, which we also adopt as part of our structure.
4. Unlike [1], [11], and [12], which review general machine learning models, we instead focus on Transformer architectures and their self-attention mechanisms.
5. Several surveys dedicated to Transformers have been recently introduced, with a range of emphases including general Transformers [48], efficient designs [49], visualization [50], computer vision tasks [51], [52], [53], [54], medical imaging [55], video tasks [56], and vision language pretraining [57].
6. While [51], [53], [54], [55] consider MML, their reviews are somewhat limited in the scope, taxonomy, and coverage.
7. To our knowledge, only a few surveys on video-language pretraining (VLP) [57], [58], [59] are relevant to MML.
8. However, VLP is only a subdomain of MML.
9. In this survey, we focus solely on the intersection of multimodal learning and Transformers.",1. What distinguishes this survey on multimodal learning and Transformers from others?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",(s3),(p3.0),"Question: Why is multimodal machine learning (MML) considered crucial in understanding human societies?

1. MML [1], [60], [61] has been an important research area in recent decades; an early multimodal application -audiovisual speech recognition was studied in 1980s [62].
2. MML is key to human societies. The world we humans live in is a multimodal environment, thus both our observations and behaviours are multimodal [63].
3. For instance, an AI navigation robot needs multimodal sensors to perceive the real-world environment [64], [65], [66], e.g., camera, LiDAR, radar, ultrasonic, GNSS, HD Map, odometer.
4. Furthermore, human behaviours, emotions, events, actions, and humour are multimodal, thus various human-centred MML tasks are widely studied, including multimodal emotion recognition [67], multimodal event representation [68], understanding multimodal humor [60]0, face-body-voice based video person-clustering [60]1, etc.",1. Why is multimodal machine learning (MML) considered crucial in understanding human societies?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",(s4),(p4.2),"Question: What are the applications and advancements of Vision Transformer in computer vision and multimodal tasks?

1. Vision Transformer (ViT) [5] is a seminal work that contributes an end-to-end solution by applying the encoder of Transformer to images.
2. Both ViT and its variants have been widely applied to various computer vision tasks, including low-level tasks [92], recognition [93], detection [94], segmentation [95], etc, and also work well for both supervised [93] and self-supervised [96], [97], [98] visual learning.
3. Moreover, some recently-released works provide further theoretical understanding for ViT, e.g., its internal representation robustness [5]0, the continuous behaviour of its latent representation propagation [5]1, [5]2.
4. Motivated by the great success of Transformer, VideoBERT [5]3 is a breakthrough work that is the first work to extend Transformer to the multimodal tasks.
5. VideoBERT demonstrates the great potential of Transformer in multimodal context.
6. Following VideoBERT, a lot of Transformer based multimodal pretraining models [5]4 have become research topics of increasing interest in the field of machine learning.","1. What are the applications of Vision Transformer in computer vision?
2. What are the advancements of Vision Transformer in multimodal tasks?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",(s10),(p10.0),"Question: How do position embeddings enhance Transformers' understanding of data structures?

1. Transformers is an open problem.
2. It can be understood as a kind of implicit coordinate basis of feature space, to provide temporal or spatial information to the Transformer.
3. For cloud point [164] and sketch drawing stroke [163], their token element is already a coordinate, meaning that position embedding is optional, not necessary.
4. Furthermore, position embedding can be regarded as a kind of general additional information.
5. In other words, from a mathematical point of view, any additional information can be added, such as detail of the manner of position embedding, e.g., the pen state of sketch drawing stroke [163], cameras and viewpoints in surveillance [165].
6. There is a comprehensive survey [166] discussing the position information in Transformers.
7. For both sentence structures (sequential) and general graph structures (sparse, arbitrary, and irregular), position embeddings help Transformers to learn or encode the underlying structures.
8. Considered from the mathematical perspective of self-attention, i.e., scaled dot-product attention, attentions are invariant to the positions of words (in text) or nodes (in graphs), if position embedding information is missing.
9. Thus, in most cases, position embedding is necessary for Transformers.",1. How do position embeddings enhance Transformers' understanding of data structures?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",(s18),(p18.0),"Question: How do users prepare multimodal inputs for Transformers?

1. Given an input from an arbitrary modality, users only need to perform two main steps, (1) tokenize the input, and (2) select an embedding space to represent the tokens, before inputting the data into Transformers.
2. In practice, both the tokenizing input and selecting embedding for the token are vital for Transformers but highly flexible, with many alternatives.
3. For instance, given an image, the solution of tokenizing and embedding is not unique.
4. Users can choose or design tokenization at multiple granularity levels -coarse-grained vs. fine-grained.
5. e.g., use ROIs (obtained by an object detector) and CNN features as tokens and token embeddings [102], use patches and linear projection as tokens and token embeddings [5], or use graph node (obtained by object detector and graph generator) and GNN features as tokens and token embeddings [181].
6. Given a tokenization plan, the subsequent embedding approaches can be diverse.
7. For example, for video input, a common tokenization is to treat the non-overlapping windows (down-sampled) over the video as tokens, and their embeddings can then be extracted by various 3D CNNs, e.g., VideoBERT [7], CBT [107], and UniVL (2)0 use S3D (2)1, ActBERT uses ResNet-3D (2)2.
8. Table 1 summarizes some common practices of multimodal inputs for Transformers, including RGB, video, audio/speech/music, text, graph, etc.",1. How do users prepare multimodal inputs for Transformers?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",(s21),(p21.12),"Question: How does VideoBERT encode global multimodal context in its architecture?

1. Thus, all the multimodal token positions can be attended as a whole sequence, such that the positions of each modality can be encoded well by conditioning the context of other modalities.
2. VideoBERT [7] is the one of the first multimodal Transformer works, where video and text are fused via early concatenation that can encode the global multimodal context well [188].
3. However, the longer sequence after concatenation will increase computational complexity.
4. Early concatenation is also termed ""all-attention"" or ""Co-Transformer"" [137].",1. How does VideoBERT encode global multimodal context in its architecture?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",(s29),(p29.0),"Question: What are common pretext tasks in Transformer based multimodal pretraining?

1. In Transformer based multimodal pretraining, the pretraining tasks/objectives are also termed pretext tasks/objectives.
2. To date, various pretext tasks have been studied, e.g., masked language modelling (MLM) [137], masked image region prediction/classification (also termed masked object classification (MOC)) [137], (also termed masked object classification (MOC)4, masked region regression (MRR) [115], visual-linguistic matching (VLM)
3. (e.g., image-text matching (ITM) (also termed masked object classification (MOC)8, image text matching (ITM), phrase-region alignment (PRA) [204], word-region alignment (WRA) [106], video-subtitle matching (VSM) (also termed masked object classification (MOC)0), masked frame modelling (MFM) (also termed masked object classification (MOC)0, frame order modelling (FOM) (also termed masked object classification (MOC)0, next sentence prediction (also termed masked object classification (MOC)1 (also termed masked object classification (MOC)2, (also termed masked object classification (MOC)3, (also termed masked object classification (MOC)4, masked sentence generation (also termed masked object classification (MOC)5 (also termed masked object classification (MOC)6, masked group modelling (also termed masked object classification (MOC)7 (also termed masked object classification (MOC)8, prefix language modelling (also termed masked object classification (MOC)9 [137]0, video conditioned masked language model [137]2, text conditioned masked frame model [137]2, visual translation language modelling [137]3 [137]4, and image-conditioned masked language modelling [137]5 [137]6.
4. These down-stream task -agnostic pretext pretraining is optional, and the down-stream task objectives can be trained directly, which will be discussed in Section 4.1.2.
5. Table 3 provides the common and representative pretext tasks for Transformer based multimodal pretraining.
6. In practice, pretext tasks can be combined, and some representative cases are summarized in Table 3 of [137]7, Table 2 of [137]8.",1. What are common pretext tasks in Transformer based multimodal pretraining?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",(s30),(p30.0),"Question: What are the limitations of multimodal pretraining Transformer methods in generative tasks?

1. In spite of the recent advances, multimodal pretraining Transformer methods still have some obvious bottlenecks.
2. For instance, as discussed by [208] in VLP field, while the BERT-style cross-modal pretraining models produce excellent results on various down-stream visionlanguage tasks, they fail to be applied to generative tasks directly.
3. As discussed in [208], both VideoBERT [7] and CBT [107] have to train a separate video-to-text decoder for video captioning.
4. This is a significant gap between the pretraining models designed for discriminative and generative tasks, as the main reason is discriminative task oriented pretraining models do not involve the decoders of Transformer.
5. Therefore, how to design more unified pipelines that can work for both discriminative and generative down-stream tasks is also an open problem to be solved.
6. Again for instance, common multimodal pretraining models often underperform for fine-grained/instance-level tasks as discussed by [137].",1. What are the limitations of multimodal pretraining Transformer methods in generative tasks?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",(s35),(p35.0),"Question: How do MML Transformers fuse information across different modalities?

1. In general, MML Transformers fuse information across multiple modalities primarily at three levels: input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion).
2. Common early fusion based MML Transformer models [7], [104], [108] are also known as one-stream architecture, allowing the adoption of the merits of BERT due to minimal architectural modification.
3. The main difference between these one-stream models is the usage of problem-specific modalities with variant masking techniques.
4. With attention operation, a noticeable fusion scheme is introduced based on a notion of bottleneck tokens [175].
5. It applies for both early and middle fusion by simply choosing to-be-fused layers.
6. We note that the simple prediction-based late fusion [247], [248] is less adopted in MML Transformers.
7. This makes sense considering the motivations of learning stronger multimodal contextual representations and great advance of computing power.
8. For enhancing and interpreting the fusion of MML, probing the interaction and measuring the fusion between modalities [249] would be an interesting direction to explore.",1. How do MML Transformers fuse information across different modalities?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",(s36),(p36.1),"Question: How do contrastive learning models facilitate zero-shot transfer in multimodal learning tasks?

1. A representative practice is to map two modalities into a common representation space with contrastive learning over paired samples.
2. The models based on this idea are often enormous in size and expensive to optimize from millions or billions of training data.
3. Consequently, successive works mostly exploit pretrained models for tackling various downstream tasks [120], [263], [264], [265], [266].
4. These alignment models have the ability of zero-shot transfer particularly for image classification via prompt engineering [267].
5. This novel perspective is mind-blowing, given that image classification is conventionally regarded as a unimodal learning problem and zero-shot classification remains an unsolved challenge despite extensive research [268].
6. This has been studied for more challenging and fine-grained tasks (e.g., object detection
7. [269], visual question answering [103], [106], [112], [263], and instance retrieval [222], [263]) by imposing region (semantic parts such as objects) level alignment.
8. Finegrained alignment will however incur more computational costs from explicit region detection and how to eliminate this whilst keeping the region-level learning capability becomes a challenge.
9. Several ideas introduced recently include random sampling [113], learning concept dictionary [203], uniform masking [270], patch projection [192], joint learning of a region detector [271], and representation aligning before mask prediction [263].",1. How do contrastive learning models facilitate zero-shot transfer in multimodal learning tasks?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",(s37),(p37.4),"Question: What are the challenges and solutions in transferring knowledge across different multimodal tasks?

1. Cross-task gap is another major obstacle to transfer [208], [274], due to the different reasoning and input-output workflows, e.g., how to use multimodal datasets to finetune the language pretrained model is difficult [274].
2. In real applications, multimodal pretrained Transformers sometimes need to handle the uni-modal data at inference stage due to the issue of missing modalities.
3. One solution is using knowledge distillation, e.g., distilling from multimodal to uni-modal attention in Transformers [275], distilling from multiple uni-modal Transformer teachers to a shared Transformer encoder [276].
4. There is a huge gap across discriminative and generative multimodal tasks.
5. As discussed in [208], the BERT-like encoder-only multimodal Transformers (e.g., VideoBERT [7], CBT [107]) need separately to train decoders for generation tasks.
6. This could create a pretrain-finetune discrepancy detrimental to the generality.
7. Recently, more and more attempts study this issue further, e.g., GilBERT [222] is a generative VLP models for a discriminative task, i.e., image-text retrieval.","1. What are the challenges in transferring knowledge across different multimodal tasks?
2. What are the solutions in transferring knowledge across different multimodal tasks?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",(s41),(p41.0),"Question: How do Transformers excel in multimodal learning according to recent studies?

1. Why and how Transformers perform so well in multimodal learning has been investigated [106], [299], [300], [301], [302], [303], [304], [305], [306].
2. These attempts mainly use probing task and ablation study.
3. Cao et al. [299] design a set of probing tasks on UNITER [106] and LXMERT [103], to evaluate what patterns are learned in pretraining.
4. Hendricks et al. [301] probe the image-language Transformers by fine-grained image-sentence pairs, and find that verb understanding is harder than subject or object understanding.
5. Chen et al. [106] examine the optimal combination of pretraining tasks via ablation study, to compare how different pretexts contribute to the Transformers.
6. Despite these attempts, the interpretability of multimodal Transformers is still under-studied to date.",1. How do Transformers excel in multimodal learning according to recent studies?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",(s42),(p42.0),"Question: What are the challenges in designing universal MML models for diverse tasks?

1. Designing the universal MML models to excel across all the unimodal and multimodal down-stream tasks with different characteristics simultaneously [115], [299] is a non-trivial challenge.
2. For instance, two-stream architectures [9], [263] are typically preferred over one-stream ones for cross-modal retrieval-like tasks in efficiency, since the representation of each modality can be pre-computed beforehand and reused repeatedly.
3. That being said, how to design task-agnostic MML architectures is still an open challenge, in addition to other design choices such as pretext and objective loss functions.
4. Furthermore, a clear gap remains between the state-of-the-art and this ultimate goal.
5. In general, existing multimodal Transformer models [9], [199], [263] are superior only for specific MML tasks, as they are designed specifically for only a subset of specific tasks [137], [142], [212], [299]9, [299]1, [299]2, [299]3, [299]4.
6. Encouragingly, several recent studies towards universal modality learning in terms of modality-agnostic network design [299]5 and more task-generic architecture design [299]6, [299]7, [299]8 have been introduced, and it is hoped this will spark further investigation.
7. To that end, instead of exhaustively exploring the vast model design space, seeking in-depth understanding and interpretation of a MML model's behaviour might be insightful for superior algorithm design, even though the interactions and synergy across different modalities are intrinsically complex and even potentially inconsistent over tasks [299]9.","1. What are the challenges in designing universal MML models?
2. How do these challenges vary for diverse tasks?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",(s42),(p42.1),"Question: What is critical for achieving fine-grained MML and what challenges does it face?

1. For more fine-grained MML, it is widely acknowledged that discovering the latent semantic alignments across modalities is critical.
2. An intuitive strategy is to leverage semantic parts (e.g., objects) pre-extracted by an off-the-shelf detector for MML [103], [104], [105], [106], [112], [204], [310].
3. This, however, is not only complex and error-prone, but computationally costly [207].
4. Several remedies introduced recently include random sampling [113], learning concept dictionary [103]0, jointly learning a region detector [103]1, and representation aligning before mask prediction [103]2.
5. Given the scale of MML training data, exploring this direction needs exhaustive computational costs, and it is supposed that industrial research teams with rich resources are more likely to afford.
6. Ideally, a favourable MML method would leave fine-grained semantic alignment across modalities to emerge on its own, which is worthy of careful investigation in the future.","1. What is critical for achieving fine-grained MML?
2. What challenges does it face?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s2),(p2.0),"Question: What are the core components and their functions in transformer-based PLMs like BERT?

1. In general, the core components of transformer-based PLMs like BERT and RoBERTa are embedding and transformer encoder layers (refer Figure 4).
2. The embedding layer takes input tokens and returns a vector for each.
3. The embedding layer has three or more sub-layers each of which provides a vector of specific embedding type for each of the input tokens.
4. The final input vector for each token is obtained by summing all the vectors of each embedding type.
5. The transformer encoder layer enhances each input token vector by encoding global contextual information using the self-attention mechanism.
6. By applying a sequence of such transformer encoder layers, the model can encode complex language information in the input token vectors.
7. Usually, the embedding layer consists of three sublayers with each sub-layer representing a particular embedding type.
8. In some models, there are more than three also.
9. For example, embedding layer of BERT-EHR [27] contains code, position, segment, age and gender embeddings.
10. A detailed description of various embedding types is presented in Section 3.4.
11. The first sublayer converts input tokens to a sequence of vectors while the other two sub-layers provide auxiliary information like position and segmentation.
12. The first sublayer can be char, sub-word, or code embedding based.
13. For example, BioCharBERT [28] uses CharCNN [29] on the top of character embeddings, BERT uses WordPiece [30] embeddings while BEHRT [27], MedBERT [31] and BERT-EHR [32] models use code embeddings.
14. Unlike BioCharBERT and BERT models, the input for BEHRT, MedBERT, BERT-EHR models is patient visits where each patient visit is expressed as a sequence of codes.
15. The final input representation X for the given input tokens {x 1 , x 2 , . . . x n }is obtained by adding the embeddings from the three sub-layers (for simplicity, we have included only three embedding types -refer Figure 5).","1. What are the core components in transformer-based PLMs like BERT?
2. What are their functions in transformer-based PLMs like BERT?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s10),(p10.1),"Question: What is self-supervised learning and how is it applied across AI fields?

1. Robotics is the first AI field to use self-supervised learning methods [34].
2. Over the last five years, selfsupervised learning has become popular in other AI fields like natural language processing [13], [14], [26], computer vision [35], [36], and speech processing [37], [38].
3. SSL is a new learning paradigm that draws inspiration from both supervised and unsupervised learning methods.
4. SSL is similar to unsupervised learning as it does not depend on human-labeled instances.
5. It is also similar to supervised learning as it learns using supervision.
6. However, in SSL the supervision is provided by the pseudo labels which are generated automatically from the pretraining data.
7. SSL involves pretraining the model over a large unlabelled corpus using one or more pretraining tasks.
8. The pseudo labels are generated depending on the definitions of pre-training tasks.
9. SSL methods fall into three categories namely Generative, Contrastive, and Generate-Contrastive [34].
10. In Generative SSL, encoder maps input vector x to vector y, and decoder recovers x from y (e.g., masked language modeling).
11. In Contrastive SSL, the encoder maps input vector x to vector y to measure similarity [13]0.
12. In Generate-Contrastive SSL, fake samples are generated using encoder-decoder while the discriminator identifies the fake samples [13]1.
13. For more details about different SSL methods, please refer to the survey paper written by Liu et al. [34].","1. What is self-supervised learning?
2. How is it applied across AI fields?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s13),(p13.1),"Question: What is Continual Pretraining in biomedical NLP research?

1. Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs.
2. It is also referred to as further pretraining.
3. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7).
4. For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles [16].
5. In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].",1. What is Continual Pretraining in biomedical NLP research?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s14),(p14.0),"Question: What is the main drawback of continual pretraining in domain-specific language models?

1. The main drawback in continual pretraining is the general domain vocabulary.
2. For example, the WordPiece vocabulary in BERT is learned over English Wikipedia and Books Corpus [2].
3. As a result, the vocabulary does not represent the biomedical domain and hence many of the biomedical words are split into several subwords which hinders the model learning during pretraining and fine-tuning.
4. Moreover, the length of the input sequence also increases as many of the in-domain words are split into several subwords.
5. DSPT over in-domain text allows the model to have the in-domain vocabulary (refer Figure 10).
6. For example, PubMedBERT is trained from scratch using PubMed abstracts and PMC full-text articles [20].
7. PubMed achieved state-of-the-art results in the BLURB benchmark.
8. Similarly, RoBERTa-base-PM-M3-Voc is trained from scratch over PubMed and PMC and MIMIC-III clinical notes [43]. is based on the hypothesis that pretraining over taskrelated unlabelled text allows the model to learn both domain and task-specific knowledge [44] (refer Figure  11).
9. In TAPT, task-related unlabelled sentences are gathered, and then the model is further pretrained.
10. TAPT is less expensive compared to other pretraining methods as it involves pretraining the model over a relatively small corpus of task-related unlabelled sentences.",1. What is the main drawback of continual pretraining in domain-specific language models?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s16),(p16.0),"Question: What do language models learn during pretraining and how are pretraining tasks classified?

1. During pretraining, the language models learn language representations based on the supervision provided by one or more pretraining tasks.
2. A pretraining task is a pseudo-supervised task whose labels are generated automatically.
3. A pretraining task can be main or auxiliary.
4. The main pretraining tasks allow the model to learn language representations while auxiliary pretraining tasks allow the model to gain knowledge from human-curated sources like Ontology [34], [45]- [47].
5. The classification of pretraining tasks is given in Figure 12 and a brief summary of various pretraining tasks is presented in Table 1.","1. What do language models learn during pretraining?
2. How are pretraining tasks classified?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s19),(p19.0),"Question: How do auxiliary pretraining tasks enhance in-domain models with human-curated knowledge sources like UMLS?

1. Auxiliary pretraining tasks help to inject knowledge from human-curated sources like UMLS [54] into indomain models to further enhance them.
2. For example, the triple classification pretraining task involves identifying whether two concepts are connected by the relation or not [45].
3. This auxiliary task is used by Hao et al. [45] to inject UMLS relation knowledge into in-domain models.
4. Yuan et al. [47] used two auxiliary pretraining tasks based on multi-similarity Loss and Knowledge embedding loss to further pretrain BioBERT on UMLS.","1. How do auxiliary pretraining tasks enhance in-domain models?
2. How do they utilize human-curated knowledge sources like UMLS?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s21),(p21.0),"Question: How does IFT improve model performance on small target datasets?

1. IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets.
2. IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains.
3. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]- [58].
4. For example, Cengiz et al. [55] fine-tuned indomain model on general NLI datasets like SNLI [59] and MNLI [60] before fine-tuning on MedNLI [61].",1. How does IFT improve model performance on small target datasets?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s22),(p22.0),"Question: What are the benefits and limitations of multi-task fine-tuning in machine learning models?

1. Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]-[69].
2. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer.
3. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks.
4. At the same time, due to the increase in training set size, the model is less prone to over-fitting.
5. Multi-task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69].
6. Moreover, having a single model for multitasks eliminates the need of deploying separate models for each task which saves computational resources, time, and deployment costs [70].
7. Multi-task fine-tuning may not provide the best results all the time [70].
8. In such cases, multi-task fine tuning can be applied iteratively to identify the best possible subset of tasks [71].
9. For example, Mahanjan et al. [71] applied multi-task finetuning iteratively to choose the best subset of related datasets.
10. Finally, the authors fine-tuned the model on best subset of related datasets and achieved the best results on the Clinical STS [72] dataset.
11. After multi-task fine-tuning, the model can be further fine-tuned on the target specific dataset separately to further enhance the performance of the model [73].","1. What are the benefits of multi-task fine-tuning in machine learning models?
2. What are the limitations of multi-task fine-tuning in machine learning models?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s24),(p24.1),"Question: What are the advantages and disadvantages of using character embeddings in models?

1. Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only.
2. Each character is represented using an embedding.
3. These embeddings are initialized randomly and learned during model pretraining.
4. ELMo embedding model uses CharCNN to generate word representations from character embeddings [74].
5. Inspired by ELMo, BioCharBERT also uses CharCNN on the top of character embeddings to generate word representations [28]. AlphaBERT [75] also uses character embeddings.
6. Unlike CharacterBERT, AlphaBERT directly combines character embeddings with position embeddings and then applies a stack of transformer encoders.
7. The main advantage with character embeddings is the small size of vocabulary as it includes only characters.
8. The disadvantage is longer pretraining times [28].
9. As the sequence length increases with character level embeddings, models are slow to pre-train.","1. What are the advantages of using character embeddings in models?
2. What are the disadvantages of using character embeddings in models?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s24),(p24.2),"Question: What are the principles and popular algorithms of subword embeddings?

1. Subword Embeddings-In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words.
2. The main principle driving the subword embedding vocabulary construction is that frequent words should be represented as a single word and rare words should be represented in terms of meaningful subwords.
3. Subword embedding vocabularies are always moderate in size as they use sub-words to represent rare and misspelled words.
4. Some of the popular algorithms to generate vocabulary for sub-word embeddings are Byte-Pair Encoding (BPE)
5. [76], Byte-Level BPE [77], Word-Piece [30], Unigram [78], and Sentencepiece [79].","1. What are the principles of subword embeddings?
2. What are the popular algorithms of subword embeddings?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s24),(p24.5),"Question: How does SentencePiece address space assumption issues in BPE and WordPiece for language models?

1. SentencePiece [79] -A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space.
2. However, this assumption is not applicable in all languages.
3. To overcome this, SentencePiece treats space as a character and includes it in the base vocabulary.
4. The final vocabulary is generated iteratively using BPE or Unigram.
5. XLNet [81], ALBERT [15], and T5 [4] models use SentencePiece embeddings.",1. How does SentencePiece address space assumption issues in BPE and WordPiece for language models?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s24),(p24.7),"Question: What are code embeddings and how do they vary among models like BERT-EHR, MedBERT, and BEHRT?

1. Code embeddings -Code embeddings map the given sequence of codes into a sequence of vectors.
2. For example, in the case of models like BERT-EHR [32], MedBERT [31], and BEHRT [27], the input is not a sequence of words.
3. Instead, input is patient visits.
4. Each patient visit is represented as a sequence of codes.
5. The number of code embeddings varies from model to model.
6. For example, MedBERT and BEHRT include embeddings only for disease codes while BERT-EHR includes embeddings for disease, medication, procedure, and clinical notes.","1. What are code embeddings?
2. How do they vary among models like BERT-EHR, MedBERT, and BEHRT?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s25),(p25.0),"Question: What is the purpose of auxiliary embeddings in enhancing model learning?

1. Main embeddings represent the given input sequence in low dimensional space.
2. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better.
3. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings.
4. The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence.
5. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings.
6. Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].",1. What is the purpose of auxiliary embeddings in enhancing model learning?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s29),(p29.0),"Question: What led to the development of radiology-specific PLMs?

1. Following the success of EHR-based T-BPLMs, recently researchers focused on developing PLMs specifically for radiology reports.
2. RadCore [90] dataset consists of around 2 million radiology reports.
3. These reports were gathered from three major healthcare organizations: Mayo Clinic, MD Anderson Cancer Center, and Medical College of Wisconsin in 2007.
4. Meng et al. [89] further pre-trained general BERT on radiology reports with impression section headings from RadCore dataset    Table 3 contains a summary of radiology reports-based T-BPLMs.",1. What led to the development of radiology-specific PLMs?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s30),(p30.0),"Question: How has social media influenced health-related research and services in the last decade?

1. In the last decade, social media has become the first choice for internet users to express their thoughts.
2. Apart from views about general topics, various social media platforms like Twitter, Reddit, AskAPatient, WebMD are used to share health-related experiences in the form of tweets, reviews, questions, and answers [107], [108].
3. Recent works have shown that health-related social media data is useful in many applications to provide better health-related services [109], [110].
4. The performance of general T-PLMs on Wikipedia and Books Corpus is limited on health-related social media datasets [92].
5. This is because social media text is highly informal with a lot of nonstandard abbreviations, irregular grammar, and typos.
6. Researchers working at the intersection of social media and health, trained social media text-based T-BPLMs to handle social media texts.
7. CT-BERT [92] is initialized from BERT-large and further pretrained on a corpus of 22.5M covid related tweets and this model showed up to 30% improvement compared to BERT-large, on five different classification datasets.
8. BioRedditBERT [94] is initialized from BioBERT and further pretrained on healthrelated Reddit posts.
9. The authors showed that BioRed-ditBERT outperforms in-domains models like BioBERT, BlueBERT, PubMedBERT, ClinicalBERT by up to 1.8% in normalizing health-related entity mentions.
10. RuDR-BERT [108]0 is initialized from Multilingual BERT and pretrained on the raw part of the RuDReC corpus (1.4M reviews).
11. The authors showed that RuDR-BERT outperforms multilingual BERT and Russian BERT on Russian sentence classification and clinical entity extraction datasets by large margins.
12. EnRuDR-BERT [108]0 and EnDR-BERT [108]0 are obtained by further pretraining multilingual BERT on Russian and English health reviews and English health reviews respectively.
13. Table 4 contains summary of social media text-based BPLMs.","1. How has social media influenced health-related research in the last decade?
2. How has social media influenced health-related services in the last decade?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s31),(p31.0),"Question: Why is biomedical text mining becoming more popular in research?

1. In the last few decades, the amount of biomedical literature is growing at a rapid scale.
2. As knowledge discovery from biomedical literature is useful in many applications, biomedical text mining is gaining popularity in the research community [16].
3. However, biomedical text significantly differs from the general text with a lot of domain-specific words.
4. As a result, the performance of general T-PLMs is limited in many of the tasks.
5. So, biomedical researchers focused on developing in-domain T-PLMs to handle biomedical text.
6. PubMed and PMC are the two popular sources of biomedical text.
7. PubMed con-  tains only biomedical literature citations and abstracts only while PMC contains full-text biomedical articles.
8. As of March 2020, PubMed includes 30M citations and abstracts while PMC contains 7.5M full-text articles.
9. Due to the large collection and broad coverage, these two are the first choice to pretrain T-BPLMs [16], [43], [96].",1. Why is biomedical text mining becoming more popular in research?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s31),(p31.1),"Question: How are biomedical pre-trained language models like BioBERT developed from general BERT?

1. As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text.
2. BioBERT [16] is the first biomedical pre-trained language model which is obtained by further pretraining general BERT on biomedical literature.
3. BioBERTpt-bio [97] is obtained by further pretraining BERT Multilingual (base) on Brazilian biomedical corpus -scientific papers from PubMed (0.8M-only literature titles) + Scielo (health -12.4M + biological-3.2M: both titles and abstracts).
4. BioMedBERT [100] is obtained by further pretraining BERT-large on BREATHE 1.0 corpus.
5. BioMedBERT outperformed BioBERT on biomedical question answering.","1. How are biomedical pre-trained language models developed?
2. What specific steps are taken to adapt general BERT into models like BioBERT?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s32),(p32.0),"Question: What challenges exist in pretraining transformer-based PLMs with in-domain text?

1. It is difficult to obtain a large amount of in-domain text in some cases.
2. For example, MIMIC [87], [88] is the largest publicly available dataset of medical records.
3. The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC).
4. However, to pretrain a transformer-based PLM from scratch, we require large volumes of text.
5. To overcome this, some of the models are pretrained on general + in-domain text [41], [104] or, in-domain + related domain text [18], [28], [43], [97].
6. For example, BERT (jpCR+jpW) [ Table 6 contains hybrid corpora-based T-BPLMs.",1. What challenges exist in pretraining transformer-based PLMs with in-domain text?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s34),(p34.0),"Question: How are language-specific T-BPLMs developed following the success of English biomedical BERT models?

1. Following the success of BioBERT, ClinicalBERT, Pub-MedBERT in English biomedical tasks, researchers focused on developing T-BPLMs for other languages also by pretraining from scratch [41], [91], [111] or pretraining from Multilingual BERT [95], [97] or pretraining from monolingual BERT [48], [91], [112] models.
2. For example, CHMBERT [112] is the first Chinese medical BERT model which is initialized from the general Chinese BERT model and further pretrained on a huge (185GB) corpus of Chinese medical text gathered from more than 100 hospitals.
3. MC-BERT [48] is also initialized from general Chinese BERT and further pre-trained on hybrid corpora which includes general, biomedical, and medical texts.
4. Table 7 contains a summary of language-specific T-BPLMs.","1. How are language-specific T-BPLMs developed?
2. What is the process followed in the development of these models after the success of English biomedical BERT models?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s36),(p36.0),"Question: How do CPT and DSPT/SPT adapt T-PLMs to in-domain tasks and their limitations?

1. CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus.
2. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords.
3. This kind of representation increases the overall length of the input as well as hinders the model learning.
4. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text.
5. However, both these approaches involve learning the model parameters from scratch which is highly expensive.
6. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].","1. How do CPT and DSPT/SPT adapt T-PLMs to in-domain tasks?
2. What are their limitations?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s36),(p36.1),"Question: What are Green Models and how do they adapt T-PLMs for the biomedical domain?

1. Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123].
2. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach.
3. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors.
4. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors.
5. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge.
6. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction.
7. This approach is completely inexpensive as it requires only CPU.
8. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module.
9. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text.
10. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive.
11. Table 9 contains summary of Green T-BPLMs.","1. What are Green Models?
2. How do they adapt T-PLMs for the biomedical domain?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s37),(p37.0),"Question: How do T-PLMs exhibit and mitigate bias in decision-making?

1. T-PLMs are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others.
2. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126].
3. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group.
4. Zhang et al. [127] further pretrained SciBERT [105] on MIMIC-III clinical notes and showed that the performance of the model is different for different groups.
5. The authors applied adversarial pretraining debiasing to reduce the gender bias in the model.
6. The authors released both the models publicly to encourage further research in debiasing T-BPLMs.","1. How do T-PLMs exhibit bias in decision-making?
2. How do T-PLMs mitigate bias in decision-making?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s40),(p40.0),"Question: What is Natural Language Inference and its significance in NLP?

1. Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics.
2. It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence.
3. Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval [136] in the general domain and medical concept normalization [137], [138], semantic relatedness [139], question answering [66] in the biomedical domain.
4. NLI is framed as a three-way sentence pair classification problem.
5. Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair.
6. MedNLI [61] is the in-domain NLI dataset with around 14k instances generated from MIMIC-III [88] clinical notes.
7. As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al. [140] further pretrained BioBERT [16] on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45%.
8. Cengiz et al. [136]0 applied an ensemble of two BioBERT models and achieved an accuracy of 84.7%.
9. The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI [136]1 and MultiNLI [136]2 and then finetuned them on MedNLI.","1. What is Natural Language Inference?
2. What is its significance in NLP?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s41),(p41.1),"Question: What are the approaches and advancements in entity extraction techniques?

1. Most of the existing work approaches entity extraction as sequence labeling or machine reading comprehension.
2. In case of sequence labelling approach, BERT based models generate contextual representations for each token and then softmax layer [62], [149], [150], BiLSTM+Softmax [150], BiLSTM+CRF [62], [149]0- [153] or CRF [149]2, [62], [149]0 is applied.
3. Recent works showed adding BiLSTM on the top of the BERT model does not show much difference in performance [149]0, [149]1.
4. This is because transformer encoder layers in BERT based models do the same job of encoding contextual in token representations like BiLSTM.
5. Some of the works experimented with general BERT for extracting clinical and biomedical entities.
6. For example, Portelli et al. [149]2 showed that SpanBERT+CRF outperformed in-domain BERT models also in extracting clinical entities in social media text.
7. Boudjellal et al. [149]3 developed ABioNER by further pretraining AraBERT [149]4 on general Arabic corpus + biomedical Arabic text and showed that ABioNER outperformed both multilingual BERT [149]5 and AraBERT on Arabic biomedical entity extraction.
8. This shows that further pretraining general AraBERT on small in-domain text corpus improves the performance of the model.
9. As in-domain datasets are comparatively small, some of the recent works [62], [149]9, [149]8 initially fine-tuned the models on similar datasets before fine-tuning on small target datasets.
10. This intermediate fine-tuning allows the model to learn more task-specific knowledge which improves the performance of the model on small target datasets.
11. For example, Gao et al. [149]9 proposed a novel approach entity extraction approach based on intermediate fine-tuning and semi-supervised learning.
12. Here intermediate fine-tuning allows the model to learn more task-specific knowledge while semi-supervised learning allows to leverage task-related unlabelled data by assigning pseudo labels.
13. Recently Sun et al. [62] formulated biomedical entity extraction as question answering and showed that BioBERT+QA outperformed BioBERT+ (Softmax / CRF / BiLSTM-CRF) on six datasets.","1. What are the approaches in entity extraction techniques?
2. What advancements have been made in entity extraction techniques?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s42),(p42.0),"Question: What is Semantic Textual Similarity and its applications in natural language processing?

1. Semantic Textual Similarity quantifies the degree of semantic similarity between two phrases or sentences.
2. Unlike NLI which classifies the given sentence pair into one of three classes, semantic textual similarity returns a value that indicates the degree of similarity.
3. Both NLI and STS require sentence-level semantics.
4. STS is useful in tasks like concept relatedness [139], medical concept normalization [137], [138] , duplicate text detection [155], question answering [65], [156] and text summarization [157].
5. Moreover, Reimers et al. [136] showed that training transformer-based PLMs on STS datasets allow the model to learn sentence-level semantics and hence better represent variable-length texts like phrases or sentences.
6. Models like BERT learn the joint representation of given sentence pair and a task-specific sigmoid layer gives the similarity value.
7. BIOSSES [158] and Clinical STS [72] are the commonly used datasets to train and evaluate indomain STS models.","1. What is Semantic Textual Similarity?
2. What are its applications in natural language processing?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s42),(p42.1),"Question: How have recent studies improved clinical STS model performance?

1. Recent works exploited general models for clinical STS [56], [57], [65], [159].
2. For example, Yang et al. [56] achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model.
3. As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets [56], [57], [65], [71].
4. Xiong et al. [160] enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE [161] based entity representations.
5. Mutinda et al. [155] achieved a Pearson correlation score of 0.8320 by finetuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets.
6. Mahajan et al. [71] proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset.
7. Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning.
8. The main drawback in the above existing works is giving '[CLS]' vector as sentence pair representation to the sigmoid layer.
9. This is because the '[CLS]' vector contains only partial information.
10. Unlike existing works, Wang et al. [159] applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results.",1. How have recent studies improved clinical STS model performance?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s43),(p43.0),"Question: What is relation extraction and its significance in information extraction?

1. Relation extraction is a crucial task in information extraction which identifies the semantic relations between entities in text.
2. Entity extraction followed by relation extraction helps to convert unstructured text into structured data.
3. Extracting relations between entities is useful in many tasks like knowledge graph construction, text summarization, and question answering.
4. Some of relation extraction datasets are I2B2 2012 [162], AIMED [163], ChemProt [164], DDI [165], I2B2 2010 [141] and EU-ADR [166].
5. Wei et al. [167] achieved the best results on two datasets using MIMIC-BERT [40] +Softmax.
6. Thillaisundaram and Togia [168] applied SciBERT +Softmax to extract relations from biomedical abstracts as part of AGAC track of BioNLP-OST 2019 shared tasks [169].
7. Liu et al. [163]0 proposed SciBERT+Softmax for relation extraction in biomedical text.
8. They showed SciBERT+Softmax outperforms BERT+Softmax on three biomedical relation extraction datasets.
9. The main drawback in the above existing works is that they utilize only the partial knowledge from the last layer in the form of '[163]3' vector.
10. Su et al. [163]2 added attention on the top of BioBERT to fully utilize the information in the last layer and achieved the best results on three biomedical extraction datasets.
11. The authors generated the final representation by concatenating '[163]3' vector and weighted sum vector of final hidden state vectors.
12. When compared to applying LSTM on the final hidden state vectors, the attention layer gives better results.","1. What is relation extraction?
2. What is its significance in information extraction?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s44),(p44.1),"Question: How does incorporating attention in BERT models enhance clinical text classification performance?

1. Shen et al. [173] applied various in-domain BERT for Alzheimer disease clinical notes classification and achieved the best results using PubMedBERT and BioBERT.
2. They generated labels for the training instances using a rule-based NLP algorithm.
3. Chen et al. [174] further pretrained the general BERT model on 1.5 drugrelated tweets and showed that further pretraining improves the performance of the model on ADR tweets classification.
4. Recent works [175], [176] showed that adding attention on the top of the BERT model improves the performance of the model in clinical text classification.
5. They introduced a custom attention model to aggregate the encoder output and showed that this leads to improved performance and interpretability.",1. How does incorporating attention in BERT models enhance clinical text classification performance?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s45),(p45.0),"Question: What are the challenges and advancements in biomedical question answering systems?

1. Question Answering (QA) aims to extract answers for the given queries.
2. QA helps to quickly find information in clinical notes or biomedical literature and thus saves a lot of time.
3. The main challenge in developing automated QA systems for the clinical or biomedical domain is the small size of datasets.
4. Developing large QA datasets in the clinical or biomedical domain is quite expensive and requires a lot of time also.
5. Some of the popular in-domain QA datasets are emrQA [177], CliCR [178], PubMedQA [179] COVID-QA [180], MASH-QA [181] and Health-QA [182].
6. Chakraborty et al. [177]4 showed BioMedBERT obtained by further pretraining BERT-Large on BREATHE 1.0 corpus outperformed BioBERT on biomedical question answering.
7. The main reason for this is the diversity of text in BREATHE 1.0 corpus.
8. Pergola et al. [52] introduced Biomedical Entity Masking (BEM) which allows the model to learn entity-centric knowledge during further pretraining.
9. They showed that BEM improved the performance of both general and indomain models on two in-domain QA datasets.
10. Recent works used intermediate fine-tuning on general QA [177]5, [177]4 or NLI [177]2 datasets or multi-tasking [177]6 to improve the performance of in-domain QA models.
11. For example, Soni et al. [177]4 achieved the best results on a) CliCR by intermediate fine-tuning on SQuaD using BioBERT and b) emrQA by intermediate fine-tuning on SQuaD and CliCR using Clinical BERT.
12. Yoon et al. [177]5 showed that intermediate fine-tuning on general domain Squad datasets improves the performance on biomedical question answering datasets.
13. Akdemir et al. [177]6 proposed a novel multi-task model based on BioBERT for biomedical question answering.
14. They used biomedical NER as an auxiliary task and showed that transfer learning from the bioNER task improves performance on question answering tasks.","1. What are the challenges in biomedical question answering systems?
2. What advancements have been made in biomedical question answering systems?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s46),(p46.0),"Question: Why is automatic biomedical text summarization important for researchers?

1. In general, sources of biomedical information like clinical notes, scientific papers, radiology reports are length in nature.
2. Researchers and domain experts need to go through a number of biomedical documents.
3. As biomedical documents are length in nature, there is a need for automatic biomedical text summarization which reduces the effort and time for researchers and domain experts [185], [186].
4. Text summarization falls into two broad categories namely extractive summarization which identifies the most relevant sentences in the document while abstractive summarization generates new text which represents the summary of the document [187].
5. There are no standard datasets for biomedical text summarization.
6. Researchers usually treat scientific papers as documents and their abstracts as summaries [188], [189].",1. Why is automatic biomedical text summarization important for researchers?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s46),(p46.2),"Question: What novel approaches have been proposed for summarizing biomedical texts?

1. In the case of small models, BioBERT outperformed others.
2. Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text.
3. They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences.
4. Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text.
5. BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence.
6. The sentences with the highest score are considered as the summary.
7. Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT.",1. What novel approaches have been proposed for summarizing biomedical texts?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s52),(p52.2),"Question: What are low-cost domain adaptation methods for BERT models in the biomedical field?

1. Fine-tuning must be done iteratively to reduce the noisy labeled instances.
2. BERT models to the biomedical domain.
3. Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44].
4. TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances.
5. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191].
6. b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123].
7. The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123].",1. What are low-cost domain adaptation methods for BERT models in the biomedical field?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s53),(p53.0),"Question: How can ontology knowledge injection improve BioBERT and PubMedBERT models?

1. Models like BioBERT, PubMedBERT have achieved good results in many of the tasks.
2. However, these models lack knowledge from human-curated knowledge sources.
3. These models can be further enhanced by ontology knowledge injection.
4. Ontology knowledge injection can be done in many ways namely a) continual pretraining the models on UMLS synonyms [34], [46] or relations [45] or both [47] b) continual pretraining the models on UMLS concept definitions [192] and c) feature vector constructed using ontology is added to the sequence vector learned by the models [174].","1. How can ontology knowledge injection improve BioBERT models?
2. How can ontology knowledge injection improve PubMedBERT models?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s54),(p54.1),"Question: What is multi-task fine-tuning and its benefits in machine learning?

1. Multi-Task Fine-Tuning -Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]-[69].
2. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer.
3. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks.
4. Multi-Task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]- [71], [73].","1. What is multi-task fine-tuning?
2. What are its benefits in machine learning?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s54),(p54.2),"Question: What are popular techniques for data augmentation in low resource scenarios?

1. Data Augmentation -Data augmentation helps us to create new training instances from existing instances.
2. These newly creating training instances are close to original training data and helpful in low resource scenarios.
3. Back translation and EDA [195] are the top popular techniques for data augmentation.
4. For example, Wang et al. [159] used back translation to augment the training instances to train the clinical text similarity model.
5. The domain-specific ontologies like UMLS can also be used to augment the training instances [153].",1. What are popular techniques for data augmentation in low resource scenarios?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s55),(p55.0),"Question: How do transformed-based PLMs perform on noisy instances and what are the solutions?

1. Transformed based PLMs have achieved the best results in many of the tasks.
2. However, the performance of these models on noisy test instances is limited [197]- [200].
3. This is because the model is mostly trained on less noisy instances.
4. As these models mostly never encounter noisy instances during fine-tuning, the performance is significantly reduced on noisy instances.
5. Apart from this, the noisy words in the instances are split into several subtokens which affect the model learning.
6. The robustness of models is crucial particularly insensitive domains like biomedical [199], [200].
7. Two possible solutions are a) CharBERT [28] -replaced the WordPiece based embedding layer with CharCNN based embedding layer.
8. Here word representation is generated from character embeddings using CharCNN.
9. b) Adversarial Training [200] -Here, the training set is augmented with the noisy instances.
10. Training the model on an augmented training set exposes the model to noisy instances and hence the model performs better on noisy instances.","1. How do transformed-based PLMs perform on noisy instances?
2. What are the solutions?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s56),(p56.0),"Question: How can T-PLMs adapt to in-domain text while addressing vocabulary limitations?

1. Continual pretraining allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of in-domain text.
2. Though the models are adapted to in-domain, they still contain general vocabulary.
3. As the vocabulary is learned over general text, it mostly includes subwords and words which are specific to the general domain.
4. As a result, many of the in-domain words are not represented in a meaningful way.
5. The two possible options to represent in-domain words in a meaningful way are a) in-domain vocabulary through DSPT [20] b) extending the general vocabulary with indomain vocabulary [122], [123].","1. How can T-PLMs adapt to in-domain text?
2. How do they address vocabulary limitations?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s58),(p58.0),"Question: How is the final input sequence representation determined in text classification according to Devlin et al.?

1. For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation.
2. According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information.
3. The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space.
4. Finally, a softmax is applied to convert it into a vector of probabilities.
5. However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector.",1. How is the final input sequence representation determined in text classification according to Devlin et al.?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s60),(p60.0),"Question: What causes unfair decisions in deep learning-based systems and how can bias be addressed?

1. With the success of deep learning models in various tasks, deep learning-based systems are used to automate the decisions like department recommendation [112], disease prediction [31], [32] etc. However, these models are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others.
2. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126].
3. Real-world datasets have a bias in many forms.
4. It can be based on various attributes like gender, age, ethnicity, and marital status.
5. These attributes are considered as protected or sensitive [201].
6. For example, in the MIMIC-III dataset [88] a) heart disease is more common in males compared to females-an example of gender bias b) there are fewer clinical studies involving black patients compared to other groups -an example of ethnicity bias.
7. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group.
8. There are few works that identified and addressed bias in transformer-based biomedical language models.
9. Zhang et al. [127] using a simple word competition task showed that SciBERT [105] exhibits ethnicity bias.
10. Moreover, the authors showed SciBERT model when further-pretrained on clinical notes exhibits performance differences for different protected attributes.
11. They further showed that adversarial pretraining debiasing has little impact in reducing bias.
12. Minot et al. [202] proposed an approach based on data augmentation to identify and reduce gender bias in patient notes.
13. This is an area that needs to be explored further to improve reduce bias and improve the fairness in model decisions.","1. What causes unfair decisions in deep learning-based systems?
2. How can bias be addressed?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s61),(p61.0),"Question: What are the concerns and solutions regarding data leakage in biomedical language models?

1. Every patient visit is recorded in the clinical records.
2. Apart from patient visits, clinical records contain the past and the present medical history of the patient.
3. Such sensitive data should not be disclosed as it may harm the patients physically or mentally [203].
4. Usually, the clinical records are shared for research purposes only after de-identifying the sensitive information.
5. However, it is possible to recover sensitive patient information from the de-identified medical records.
6. Recent works showed that there is data leakage from pre-trained models in the general domain i.e., it is possible to recover personal information present in the pretraining corpora [204], [205].
7. Due to data leakage, the models pre-trained on proprietary corpora, cannot be released publicly.
8. Recently, Nakamura et al. [203] proposed KART framework which can conduct various attacks to assess the leakage of sensitive information from pre-trained biomedical language models.
9. We strongly believe there is a need for more work in this area to assess as well as address the data leakage in biomedical language models.","1. What are the concerns regarding data leakage in biomedical language models?
2. What are the solutions regarding data leakage in biomedical language models?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s62),(p62.0),"Question: What are the challenges and solutions in developing biomedical pre-trained language models (BPLMs)?

1. In the beginning, the standard approach to develop BPLMs is to start with general PLMs and then further pretrain them on large volumes of biomedical text [16].
2. The main drawback of this approach is the lack of indomain vocabulary.
3. Without domain-specific vocabulary, many of the in-domain are split into a number of subwords which hinders model learning during pretraining or fine-tuning.
4. Moreover, continual pretraining is quite expensive as it involves pretraining on large volumes of unlabeled text.
5. To overcome these drawbacks, there are low-cost domain adaptation approaches that extend the general domain vocabulary with in-domain vocabulary [122], [123].
6. The extra in-domain vocabulary is generated using Word2vec and then aligned [122] or generated directly using WordPiece [123] over biomedical text.
7. The main drawback in these low-cost domain adaptation approaches is an increase in the size of the model with the addition of in-domain vocabulary.
8. Further research on this topic can result in more novel methods for lowcost domain adaptation.","1. What are the challenges in developing biomedical pre-trained language models (BPLMs)?
2. What are the solutions in developing biomedical pre-trained language models (BPLMs)?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s64),(p64.0),"Question: What is the purpose of benchmarks in evaluating NLP model performance across tasks?

1. In general, a benchmark is a tool to evaluate the performance of models across different NLP tasks.
2. A benchmark is required because we expect the pre-trained language models to be general and robust i.e., the models perform well across tasks rather than on one or two specific tasks.
3. A benchmark with one or more datasets for multiple NLP tasks helps to assess the general ability and robustness of models.
4. In general domain, we have a number of benchmarks like GLUE [207] and Super-GLUE [208] (general language understanding), XGLUE [209] (cross lingual language understanding) and LinCE [210] (code switching).
5. In biomedical domain there are three benchmarks namely BLUE [208]0, BLURB [20] and ChineseBLUE [48].
6. BLUE introduced by Peng et al. [208]0 contains ten datasets for five biomedical NLP tasks, while BLURB contains thirteen datasets for six tasks and ChineseBLUE contains eight tasks with nine datasets.
7. BLUE and ChineseBLUE include both EHR and scientific literature-based datasets, while BLURB includes only biomedical scientific literature-based datasets.
8. The semantics of EHR and medical social media texts are different from biomedical scientific literature.
9. So, there is a need of exclusive benchmarks for EHR and medical social media-based datasets.",1. What is the purpose of benchmarks in evaluating NLP model performance across tasks?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s65),(p65.0),"Question: What do intrinsic probes reveal about PLMs' knowledge during pretraining?

1. During pretraining, PLMs learn syntactic, semantic knowledge along with factual and common-sense knowledge available in the pretraining corpus [14].
2. Intrinsic probes through light on the knowledge learned by PLMs during pretraining.
3. In general NLP, researchers proposed several intrinsic probes like LAMA, Negated and Misprimed LAMA [211], XLAMA [212], X-FACTR [213], MickeyProbe [214] to understand the knowledge encoded in pretrained models.
4. For example, LAMA [211] probes the factual and common-sense knowledge of English pretrained models, while X-FACTR [213] probes the factual knowledge of multi-lingual pretrained models.
5. However, there is no such intrinsic probes in Biomedical domain to through light on the knowledge learned by BPLMs during pretraining.
6. This is an area which requires much attention from Biomedical NLP community.",1. What do intrinsic probes reveal about PLMs' knowledge during pretraining?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Computer Science, Linguistics",(s66),(p66.0),"Question: What advancements are needed in biomedical NLP model pretraining?

1. Pretraining provides BPLMs with necessary background knowledge which can be transferred to downstream tasks.
2. However, pretraining is computationally very expensive and also requires large volumes of pretraining data.
3. So, there is need of novel model architecture which reduces the pretraining time as well as the amount of pretraining corpus.
4. In general NLP, recently efficient models like ConvBERT [215] and DeBERTa [216] are proposed which reduces the pretraining time and amount of pretraining corpus required respectively.
5. DeBERTa with two novel improvements (Disentangled attention mechanism and enhanced masked decoder) achieves better compared to RoBERTa. DeBERTa is pretrained on just 78GB of data while RoBERTa is pretrained on 160GB of data.
6. ConvBERT with mixed attention block outperforms ELECTRA while using just 1/4 th of its pretraining cost.
7. Biomedical NLP research community must focus on developing pretrained models based on these novel model architectures.",1. What advancements are needed in biomedical NLP model pretraining?
