# Neuron-level Interpretation of Deep NLP Models: A Survey

CorpusID: 237353268 - [https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd](https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd)

Fields: Computer Science

## (s1) Definitions
### Question: What is a concept in linguistic terms?

#Reference=1 ['b44']

(p1.2) Concept A concept represents a coherent fragment of knowledge, such as "a class containing certain objects as elements, where the objects have certain properties" (Stock, 2010). For example, a concept could be lexical: e.g. words ending with suffix "ed", morphological: e.g. gerund verbs, or semantic: e.g. names of cities. We loosely define a concept C as a group of words that are coherent w.r.t to a linguistic property. Table 1 shows an example sentence with different concept annotations.

## (s3) Visualization
### Question: What are the limitations of visualizing neuron activations to understand their roles?

#Reference=4 ['b20', 'b11', 'b22', 'b20']

(p3.0) A simple way to discover the role of a neuron is by visualizing its activations and manually identifying the underlying concept over a set of sentences (Karpathy et al., 2015;Fyshe et al., 2015;Li et al., 2016a). Given that deep NLP models are † Table 3 in Appendix gives a more comprehensive list. trained using billions of neurons, it is impossible to visualize all the neurons. A number of clues have been used to shortlist the neurons for visualization, for example, selecting saturated neurons, high/low variance neurons, or ignoring dead neurons (Karpathy et al., 2015) when using ReLU activation function. ‡ Limitation While visualization is a simple approach to find an explanation for a neuron, it has some major limitations: i) it is qualitative and subjective, ii) it cannot be scaled to the entire network due to an extensive human-in-the-loop effort, iii) it is difficult to interpret polysemous neurons that acquire multiple roles in different contexts, iv) it is ineffective in identifying group neurons, and lastly and v) not all neurons are visually interpretable. Visualization nevertheless remains a useful tool when applied in combination to other interpretation methods that are discussed below.

## (s4) Corpus-based Methods
### Question: What is concept search in the context of neural network analysis?

#Reference=2 ['b19', 'b34']

(p4.1) Concept Search This set of methods take a neuron as an input and search for a concept that the neuron has learned. They sort the input instances based on the activation values of the given neuron. The top activating instances represent a concept the neuron represents. Kádár et al. (2017) discovered neurons that learn various linguistic con-cepts using this approach. They extracted top-20, 5-gram contexts for each neuron based on the magnitude of activations and manually identified the underlying concepts. This manual effort of identifying concepts is cumbersome and requires a human-in-the-loop. Na et al. (2019) addressed this by using lexical concepts of various granularities. Instead of 5-gram contexts, they extracted top-k activating sentences for each neuron. They parsed the sentences to create concepts (words and phrases) using the nodes of the parse trees. They then created synthetic sentences that highlight a concept e.g. a particular word occurring in all synthetic sentences. The neurons that activates largely on these sentences are considered to have learned the concept. This methodology is useful in analyzing neurons that are responsible for multi-word concepts such as phrases and idiomatic collocations. However, the synthetic sentences are often ungrammatical and lead towards a risk of identifying neurons that exhibit arbitrary behavior (like repetition) instead of concept specific behavior.

## (s5) Neuron Search
### Question: How do corpus-based methods discover neurons linked to specific concepts?

#Reference=1 [None]

(p5.0) The second class of corpusbased methods aim to discover neurons for a given concept. The underlying idea is the same i.e. to establish a link between the concept and neurons based on co-occurrences stats, but in the opposite direction. The activation values play a role in weighing these links to obtained a ranked list of neurons against the concept. Mu and Andreas (2020) achieved this by creating a binary mask of a neuron based on a threshold on its activation values for every sentence in the corpus. Similarly, they created a binary mask for every concept based on its presence or absence in a sentence. They then computed the overlap between a given neuron mask vector and a concept mask vector using intersection-over-union (IoU), and use these to generate compositional explanations. Differently from them, Suau et al. (2020) used the values of neuron activations as prediction scores and computed the average precision per neuron and per concept. Finally, Antverg and Belinkov (2022) considered the mean activation values of a neuron with respect to instances that posses the concept of interest.

## (s6) Probing-based Methods
### Question: What are probing-based methods and how do they function in neural network interpretation?

#Reference=1 ['b17']

(p6.0) Probing-based methods train diagnostic classifiers (Hupkes et al., 2018) over activations to identify neurons with respect to pre-defined concepts. They are a global interpretation methods that discover a set of neurons with respect to each concept using supervised data annotations. They are highly scalable, and can be easily applied on a large set of neurons and over a large set of concepts. In the following, we cover two types of classifiers used for probing.

## (s7) Linear Classifiers
### Question: How does regularization affect neuron importance in concept learning models?

#Reference=2 ['b38', 'b21']

(p7.0) The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.

### Question: What are the limitations and proposed solutions in probing classifiers for concept reflection?

#Reference=2 ['b15', 'b55']

(p7.1) Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.

## (s8) Causation-based methods
### Question: What is ablation and how does it identify salient neurons in neural networks?

#Reference=2 ['b22', 'b21']

(p8.1) Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.

### Question: What are attribution-based methods and their significance in identifying causal neurons?

#Reference=6 ['b1', 'b26', 'b49', None, 'b46', None]

(p8.3) Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.

## (s9) Miscellaneous Methods
### Question: What is Corpus Generation and its significance in neuron analysis?

#Reference=3 ['b6', 'b37', 'b19']

(p9.1) Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.

### Question: What is Matrix Factorization and its application in analyzing vision models?

#Reference=1 ['b35']

(p9.3) Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.

### Question: How do clustering methods analyze neuron groups in unsupervised learning?

#Reference=2 ['b29', 'b28']

(p9.5) Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.

## (s15) Qualitative Evaluation
### Question: How is visualization used to evaluate neurons in linguistic studies?

#Reference=1 [None]

(p15.0) Visualization has been used as a qualitative measure to evaluate the selected neurons. For example, Dalvi et al. (2019) visualized the top neurons and showed that they focus on very specific linguistic properties. They also visualized top-k activating words for the top neurons per concept to demonstrate the efficacy of their method. Visualization can be a very effective tool to evaluate the interpretations when it works in tandem with other methods e.g. using Concept Search or Probingbased methods to reduce the search space towards only highly activating concepts or the most salient neurons for these concepts respectively.

## (s18) Lexical Concepts
### Question: What do studies reveal about neurons learning linguistic and conceptual patterns?

#Reference=5 ['b20', 'b22', 'b19', 'b34', 'b37']

(p18.1) Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example "I like this movie a lot" or "the movie is incredibly good". Similarly they discovered neurons that captured "negation". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items "camera, laptop, cables" and salad items "broccoli, noodles, carrots etc". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. "law, legal" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a "horse racing" neuron identified via concept search method was in fact a general "racing" neuron by generating novel contexts against this neuron.

## (s20) Linguistic Concepts
### Question: How do neurons exhibit monosemous and polysemous behavior in language processing?

#Reference=2 ['b54', None]

(p20.2) Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example "chillax" a verb formed blending "chill" and "relax". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.

### Question: How do neurons capture syntactic and complex semantic concepts in language processing?

#Reference=4 ['b21', 'b20', 'b34', 'b42']

(p20.3) Neurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.

## (s23) Information Distribution
### Question: How do human language hierarchies relate to neuron distribution in language models?

#Reference=4 ['b25', 'b48', None, None]

(p23.0) Human languages are hierarchical in structure where morphology and phonology sit at the bottom followed by lexemes, followed by syntactic structures. Concepts such as semantics and pragmatics are placed on the top of the hierarchy.  analyzed linguistic hierarchy by studying the spread of neurons across layers in various pre-trained language models. They extracted salient neurons with respect to different linguistic concepts (e.g. morphology and syntax) and found that neurons that capture word morphology were predominantly found in the lower and middle layers and those learning about syntax were found at the higher layers. The observation was found to be true in both LSTMand the transformer-based architectures, and are inline with the findings of representation analysis (Liu et al., 2019;Tenney et al., 2019;Belinkov et al., 2020b). Similarly Suau et al. (2020) analyzed sub-modules within GPT and RoBERTa transformer blocks and showed that lower layers within a transformer block accumulate more salient neurons than higher layers on the tasks of word sense disambiguation or homograph detection. They also found that the neurons that learn homographs are distributed across the network as opposed to sense neurons that were more predominantly found at the lower layers.

## (s24) Distributivity and Redundancy
### Question: How does dropout influence information distribution in neural networks?

#Reference=1 ['b23']

(p24.0) While it is exciting to see that networks somewhat preserve linguistic hierarchy, many authors found that information is not discretely preserved at any individual layer, but is distributed and is redundantly present in the network. This is an artifact of various training choices such as dropout that encourages the model to distribute knowledge across the network. For example, Li et al. (2016b) found specialized frequency neurons in a GloVe model trained without dropout, as opposed to the variant trained with dropout where the information was more redundantly available.  showed that a significant amount of redundancy existed within pre-trained models. They showed that 85% of the neurons across the network are redundant and at least 92% of the neurons can be removed when optimizing towards a downstream task in feature-based transfer learning.

## (s25) Comparing Architectures
### Question: How do neuron distributions vary across different neural network architectures?

#Reference=2 ['b53', None]

(p25.0) The distribution of neurons across the network has led researchers to draw interesting crossarchitectural comparisons. Wu et al. (2020) performed correlation clustering of neurons across architectures and found that different architectures may have similar representations, but their individual neurons behave differently. Hennigen et al. (2020) compared neurons in contextualized (BERT) embedding with neurons in the static embedding (fastText) and found that fastText required two neurons to capture any morphosyntactic phenomenon as opposed to BERT which required up to 35 neurons to obtain the same performance.  showed that the linguistic knowledge in BERT (auto-encoder) is highly distributed across the network as opposed to XLNet (auto-regressive) where neurons from a few layers are mainly responsible for a concept (see Figure 2). Similarly Suau et al. (2020) compared RoBERTa and GPT (auto-encoder vs. generative) models and found differences in the distribution of expert neurons.  extended the cross-architectural comparison towards fine-tuned models. They showed that after finetuning on GLUE tasks, the neurons capturing linguistic knowledge are regressed to lower layers in RoBERTa and XLNet as opposed to BERT where it is still retained at the higher layers.

## (s26) Summary of Findings
### Question: What do neurons in Deep NLP models learn about linguistic knowledge and hierarchy?

#Reference=3 [None, 'b48', 'b25']

(p26.0) Below is a summary of the key findings that emerged from the work we covered in this survey. Neurons learned within Deep NLP models capture non-trivial linguistic knowledge ranging from lexical phenomenon such as morphemes, words and multi-word expressions to highly complex global phenomenon such as semantic roles and syntactic dependencies. Neuron analysis resonates with the findings of representation analysis (Belinkov et al., 2017a,b;Tenney et al., 2019;Liu et al., 2019) in demonstrating that the networks follow linguistic hierarchy. Linguistic neurons are distributed across the network based on their complexity, with lower layers focused on the lexical concepts and middle and higher layers learning global phenomenon based on long-range contextual dependencies. While the networks preserve linguistic hierarchy, many authors showed that information is not discretely preserved, but is rather distributed and redundantly present in the network. It was also shown that a small optimal subset of neurons w.r.t any concept can be extracted from a network. On another dimension, a few works showed that some concepts are localized to fewer neurons while others are distributed to a large group. Finally, some interesting cross architectural analyses were drawn based on how the neurons are distributed within their layers.

## (s28) Controlling Model's Behavior
### Question: How can identified neurons control a model's behavior regarding learned concepts?

#Reference=1 [None]

(p28.0) Once we have identified neurons that capture a certain concept learned in a model, these can be utilized for controlling the model's behavior w.r.t to that concept. Bau et al. (2019) identified Switch Neurons in NMT models that activate positively for the present-tense verbs and negatively for the past-tense verbs. By manipulating the values of these neurons, they were able to successfully change output translations from present to past tense during inference. The authors additionally found neurons that capture gender and number agreement concepts and manipulated them to control the system's output. Another effort along this line was carried by Suau et al. (2020) Controlling model's behavior using neurons en-ables on-the-fly manipulation of output, for example it can be used to debias the output of the model against sensitive attributes like race and gender.

## (s31) Compositional Explanations
### Question: How do neurons associated with concepts explain AI model predictions?

#Reference=1 ['b32']

(p31.0) Knowing the association of a neuron with a concept enables explanation of model's output. Mu and Andreas (2020) identified neurons that learn certain concepts in vision and NLP models. Using a composition of logical operators, they provided an explanation of model's prediction. Figure 3 presents an explanation using a gender-sensitive neuron. The neuron activates for contradiction when the premise contains the word man. Such explanations provide a way to generate adversarial examples that change model's predictions.

