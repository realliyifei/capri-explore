# LITERATURE REVIEW OF ATTRIBUTE LEVEL AND STRUCTURE LEVEL DATA LINKAGE TECHNIQUES

CorpusID: 9815708 - [https://www.semanticscholar.org/paper/2ee3a601957528b0d50b68bc145c7086a14c15a6](https://www.semanticscholar.org/paper/2ee3a601957528b0d50b68bc145c7086a14c15a6)

Fields: Computer Science

## (s7) Rule/Regular expression
(p7.0) The Rule / Regular expression [40] approach uses rules or set of predefined regular expressions and perform matching on tuples. Regular Expression Pattern as proposed in [40] is more flexible than regular expression alone, which is built from alphabetical elements. This is also because the Regular Expression Pattern is built from patterns over a data element, allowing the use of constructs such as "wildcards" or pattern variables. Regular Expression Pattern is quite useful when manipulating strings, and can be used in conjunction with basic pattern matching. However, the problem with this approach lies in the fact that it is relatively domain specific and tends to only work well on strings.
## (s9) String distance
(p9.0) String distance methods, also known as character-based similarity metrics [34] are used to perform data linkage based on the cost associated within the comparing strings. The cost is estimated on the number of characters which needs to be inserted, replaced or deleted for a possible string match. For example, Fig. 3 shows the cost associated in editing string "Aussie" to "Australian" (the "+" sign shows addition, the "-" sign shows deletion, and the "x" sign shows replacement). Experimental results in [34] have shown that the different distance based methodologies discovered so far are efficient under different circumstances. Some of the commonly recommended distance based metrics include Levenstein distance, Needleman-Wunsch distance, Smith-Waterman distance, Affine-gap distance, Jaro metric, Jaro and Jaro-Winkler metric, Qgram distance, and positional Q-grams distance. Through the various methods, costs are assigned to compensate for pitfalls in the system. Yet, overall, string distance pattern is most effective for typographical errors, but is hardly useful outside of this area [34].
## (s10) Term frequency
(p10.0) Term frequency [43] approach determines the frequency of strings in relation and to favour matches of less common strings, and penalizes more common strings. The Term frequency methods allow for more commonly used strings to be left out of the similarity equation. TF-IDF [43] (Term Frequency-Inverse Document Frequency) is a method using the commonality of the term (TF) along with the overall importance of the term (IDF). TF-IDF is commonly used in conjunction with cosine similarity in the vector space model. Soft TF-IDG [44] adds similar token pairs to the cosine similarity computation. According to the researchers in [44], TF-IDF can be useful for similarity computations due to its ability to give proportionate token weights. However, this approach fails to make distinctions between the similarity level of two records with the same token or weight, and is essentially unable to determine which record is more relevant.
