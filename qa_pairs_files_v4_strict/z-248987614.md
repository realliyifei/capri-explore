# Deep Learning for Visual Speech Analysis: A Survey

CorpusID: 248987614 - [https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785](https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785)

Fields: Computer Science

## (s16) Visual Quality.
(p16.0) To evaluate the quality of the synthesized video frames, reconstruction error measurement (e.g., Mean Squared Error) is a natural evaluation way. However, reconstruction error only focuses on pixel-wise alignments and ignores global visual quality. Therefore, existing works usually adopt Peak Signal-to-Noise Ratio (PSNR) and Structure Similarity Index Measure (SSIM) to evaluate the global visual quality of generated frames. More recently, Prajwal et al. [38] introduced Fr√©chet Inception Distance (FID) to measure the distance between synthetic and real data distributions, as FID is more consistent with human perception evaluation. Besides, Cumulative Probability Blur Detection (CPBD) [115], a nonreference measure, is also widely used to evaluate the loss of sharpness during video generation.

(p16.1) Audio-visual Semantic Consistency. Semantic consistency of the generated video and the driving source mainly contains audio-visual synchronization and speech consistency. For, audio-visual synchronization, Landmark Distance (LMD) [116] computes the Euclidean distance of the lip region landmarks between the synthesized video frames and ground truth frames. The other synchronization evaluation metric is to use a pre-trained audio-to-video synchronisation network [48] to predict the offset of generated frames and the ground truth. For the speech consistency, Chen et al. [42] proposed a lipsynchronization evaluation metric, i.e., Lip-Reading Similarity Distance (LRSD), which measures the Euclidean distance of semantic-level speech embeddings obtained by lip reading networks. For better evaluation of speech consistency, lip reading results (accuracy, CER, or WER) comparisons of the generated frames and ground truth are also used as consistency evaluation metrics.
## (s37) Other Methods
(p37.0) In addition, some other one-stage VSG schemes have also been proposed. Inspired by the success of the neural radiance field (NeRF) [200], Guo et al. [73] proposed the audio-driven neural radiance fields (AD-NeRF) model for VSG. As shown in Fig. 8(k), AD-NeRF takes DeepSpeech audio features as conditional input, learning an implicit neural scene representation function to map audio features to dynamic neural radiance fields for talking face rendering. Furthermore, AD-NeRF models not only the head region but also the upper body via learning two individual neural radiance fields. However, AD-NeRF does not generalize well on mismatched driving audios and speakers. As shown in Fig. 8(l), unlike the previous concatenation-based feature fusion strategy, Ye et al. [74] presented a full convolutional neural network with dynamic convolution kernels (DCKs) for crossmodal feature fusion, which extracts features from audio and reshapes features as DCKs of the fully convolutional network. Due to the simple yet effective network architecture, the realtime performance of VSG is significantly improved.
