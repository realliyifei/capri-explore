# Pretraining in Deep Reinforcement Learning: A Survey

CorpusID: 253397510 - [https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096](https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096)

Fields: Computer Science

## (s11) Data Coverage Maximization
(p11.0) Previously we have discussed how to obtain knowledge or skills, measured by the agent's own capability, from unsupervised interaction. Albeit indirectly related to the agent's ability, data diversity induced by online pretraining plays an essential role in deciding how well the agent obtains prior knowledge. In the field of supervised learning, recent advances have shown that diverse data can enhance out-of-distribution generalization (Hendrycks et al., 2020b) and robustness (Hendrycks et al., 2020a). Another supporting evidence is that most of the famed datasets are large and diverse (Deng et al., 2009;Wang et al., 2019). Motivated by the above considerations, it is desired to use data coverage maximization, usually measured by state visitation, as an objective to stimulate unsupervised learning.
## (s31) Representation Transfer
(p31.0) In the field of supervised learning, recent advances (Devlin et al., 2019;He et al., 2020;Chen et al., 2020) have demonstrated that good representations can be pretrained on large-scale unlabeled dataset, as evidenced by their impressive downstream performances. The most common practice is to freeze the weights of the pretrained feature encoder and train a randomly initialized task-specific network on top of that during adaptation. The success of this paradigm is essentially based on the promise that related tasks can usually be solved using similar representations.

(p31.1) For RL, it has been shown that directly reusing pretrained task-agnostic representations can significantly improve sample efficiency on downstream tasks. For instance, Schwarzer et al. (2021b) conduct experiments on the Atari 100K benchmark and find that frozen representations pretrained on exploratory offline data already form a basis of data-efficient RL. This success also extends to the cases where domain discrepancy exists between upstream and downstream tasks (Shah & Kumar, 2021;Parisi et al., 2022). However, the issue of negative transfer in the face of domain discrepancy might be exacerbated for RL due to its complexity (Shah & Kumar, 2021).
