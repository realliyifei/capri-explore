# A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches

CorpusID: 254408864 - [https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8](https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8)

Fields: Computer Science, Linguistics

## (s0) INTRODUCTION
(p0.0) Machine reading comprehension (MRC) is one of the most important and long-standing topics in the Natural Language Machine reading comprehension (MRC) is one of the most important and long-standing topics in Natural Language Processing (NLP). MRC provides a way to evaluate an NLP system's capability for natural language understanding. An MRC task, in brief, refers to the ability of a computer to read and understand natural language context and then find the answer to questions about that context. The emergence of large-scale single-document MRC datasets, such as SQuAD [1], CNN/Daily mail [2], has led to increased attention to this topic and different models have been proposed to address the MRC problem, such as (Chen, Bolton, and Manning, 2016) Adriana Trigiani is an Italian American best-selling author of sixteen books, television writer, film director, and entrepreneur based in Greenwich Village, New York City. (2) Based on Trigiani's 2000 best-selling novel of the same name, the story is set in the actual Virginia town of Big Stone Gap circa 1970s. The first attempt to improve the simple single-hop MRC task happened with emerging of some datasets like TriviaQA [15] and NarrativeQA [16]. These datasets addressed more challenges by introducing multiple passages per each question and also presenting a more complex kind of questions that couldn't be answered with one single sentence. Although this kind of question was more complex than single-hop questions, they still could be answered by a few nearby sentences within one passage, which means they mostly do not need multi-hop reasoning. They are generally known as the multi-passage or multi-document dataset that
## (s13) Commonsense Algorithm consists of Commonsense Selection Representation to select useful relational knowledge paths and
(p13.0) Commonsense Model Incorporation to fill the gaps of reasoning between hops of inference using Necessary and Optional Information Cell (NOIC).  [29] QFE: Nishida et al. [30] proposed a model for explainable multi-hop QA named Query Focused Extractor (QFE) which is based on a summarization idea. They use the multi-task learning of the QA model for answer selection and QFE for evidence extraction.

(p13.1) QFE as the main part of this model adaptively determines the number of evidence sentences by considering the dependency among the evidence sentences and the coverage of the question. Unlike other approaches that extract each evidence sentence separately, QFE uses an RNN and attention mechanism to consider important information in the question and the relationships between sentences. This query-aware recurrent structure enables QFE to consider the dependency among the evidence sentences and cover the important information in the question sentence. In brief, the main goal of QFE is to summarize the context according to the question. Query-focused summarization is the task of summarizing the source document with regard to the given query. The multitask learning with QFE is general in the sense that it can be combined with any QA model. The overview of QFE is shown in Figure 11. is the current summarization vector, is the query vector considering the current summarization, is the extracted sentence, updates the RNN state. Figure 11: Overview of Query Focused Extractor at step t [30] TAP: Bhargav et al. [31] proposed a deep neural architecture, called TAP (Translucent Answer Prediction) cover of two main ideas: (1) Local Interaction: Each sentence should be understood in the context of its neighboring sentences and the question, (2) Global Interaction: A global (inter-passage) interaction among sentences must be identified and used for supporting facts. TAP is a hierarchical architecture that tries to capture the local and global interactions between the sentences and consists of two main parts: (Figure 12)
## (s28) Models Performance:
(p28.0) In this section we will show the performance of the models. This investigation is helpful in several ways; it will determine the stat-of-the-result, and also shows which models and techniques has achieved the best result. On the other hand, it can show the overall performance and effectiveness of each technique in multi-hop MRC To evaluate the results of the models we need to use the evaluation metrics of the datasets. HotpotQA [11] and Wikihop [55], are two populare datasets among the reviewed studies as it has been clear in Figure 46 in which shows the percentage of use of two datasets among the reviewed models from 2018 to 2022. Then they provide a proper situation for evaluating the model's performance. 
