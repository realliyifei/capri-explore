title,section_title,answer,qua_analysis,question_by_discourse_separated,supporting_sentence,supporting_sentence_text,answer_rephrased,QA_pair_rephrased
A Primer in BERTology: What we know about how BERT works,BERT embeddings,"Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","1. What are BERT's representations and how do they differ from conventional static embeddings? 
    1.1. What does the term 'embedding' refer to in the context of BERT's representation space? 
        - sent1
        - sent2
    1.2. How do BERT's contextualized embeddings behave in terms of word senses? 
        - sent3
    1.3. How do BERT's representations vary with sentence position and why? 
        - sent4
    1.4. How do the similarity and specificity of BERT embeddings change across layers? 
        - sent5
        1.4.1. How does the vector space occupied by BERT embeddings change from lower to higher layers? 
            - sent6
            - sent7",How do BERT's contextualized embeddings differ from conventional static embeddings?,"sent1, sent2, sent3, sent4, sent5, sent6, sent7","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.

They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.

That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","Unlike conventional static embeddings (Mikolov et al., 2013a; Pennington et al., 2014), BERT's representations are contextualized, meaning every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

However, Mickus et al. (2019) note that representations of the same word vary depending on the position of the sentence in which it occurs, likely due to the NSP objective.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more context-specific representations.

Ethayarajh (2019) also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.

That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","Questions:

How do BERT's contextualized embeddings differ from conventional static embeddings?

Answer:

Unlike conventional static embeddings (Mikolov et al., 2013a; Pennington et al., 2014), BERT's representations are contextualized, meaning every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.

Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.

However, Mickus et al. (2019) note that representations of the same word vary depending on the position of the sentence in which it occurs, likely due to the NSP objective.

Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more context-specific representations.

Ethayarajh (2019) also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.

That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic)."
A Primer in BERTology: What we know about how BERT works,Syntactic knowledge,"As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","1. How is syntactic information represented in BERT?
    1.1. Is syntactic structure directly encoded in self-attention weights?
        1.1.1. What evidence suggests that syntactic structure is not directly encoded in self-attention weights? 
            - sent1
        1.1.2. What studies have been conducted to extract syntactic structures from BERT?
            - sent2, sent3
        1.1.3. What methods have been successful in recovering syntactic structures from BERT?
            - sent4, sent5
    1.2. How does BERT's MLM demonstrate syntactic competence?
        1.2.1. What evidence shows BERT's MLM takes syntactic rules into account?
            - sent6, sent7
        1.2.2. How does BERT handle negative polarity items (NPIs)?
            - sent8
    1.3. What are the limitations of BERT's syntactic knowledge?
        1.3.1. What evidence suggests BERT does not understand negation or malformed input?
            - sent9, sent10
        1.3.2. How do recent findings on adversarial attacks relate to BERT's syntactic knowledge?
            - sent11",What evidence suggests that syntactic structure is not directly encoded in BERT's self-attention weights?,"sent1, sent2, sent3","As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it.

Htut et al.(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root.

Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation.","Syntactic structure is not directly encoded in BERT's self-attention weights, but they can be transformed to reflect it.

Htut et al. (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root.

Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights but provide no quantitative evaluation.","Questions:

What evidence suggests that syntactic structure is not directly encoded in BERT's self-attention weights?

Answer:

Syntactic structure is not directly encoded in BERT's self-attention weights, but they can be transformed to reflect it.

Htut et al. (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root.

Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights but provide no quantitative evaluation."
A Primer in BERTology: What we know about how BERT works,Self-attention heads,"Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","# QUA Analysis:

1. (What is the significance of attention in understanding Transformer models?): sent1
    1.1. (What are the types of attention heads proposed by studies?): sent1
    1.2. (What is the meaning of attention weight according to Clark et al. (2019)?): sent2
    1.3. (Do self-attention heads encode nontrivial linguistic information?): sent3
        1.3.1. (What pattern do most of the model's attention heads encode?): sent4
        1.3.2. (What issue might this redundancy be related to?): sent5
    1.4. (How is attention to [CLS], [SEP], and punctuation interpreted?): sent6
        1.4.1. (Why does BERT attend a lot to [SEP] and punctuation?): sent7
        1.4.2. (What might be the function of [SEP] according to Clark et al. (2019)?): sent8
        1.4.3. (How does the importance of [SEP] change across layers?): sent9
        1.4.4. (What should be reconsidered if the hypothesis about [SEP] is correct?): sent10
2. (What is the ""heterogeneous"" self-attention pattern?): sent11
    2.1. (What do studies find about BERT heads with linguistically interpretable functions?): sent12
        2.1.1. (What do Htut et al. (2019) and Clark et al. (2019) report about BERT heads?): sent13
        2.1.2. (What do these studies find about heads attending to words in obj role?): sent14
        2.1.3. (How does the evidence for other syntactic roles vary between studies?): sent15
        2.1.4. (What supports the overall conclusion about BERT heads?): sent16
        2.1.5. (What is Hoover et al. (2019)'s hypothesis about complex dependencies?): sent17
            2.1.5.1. (What do Clark et al. (2019) and Htut et al. (2019) conclude about single heads and syntactic tree information?): sent17
3. (What is the evidence regarding attention weights and linguistic tasks?): sent18
    3.1. (How do BERT's self-attention weights compare to a uniform attention baseline?): sent19
4. (Can a BERT head be used for coreference resolution?): sent20
5. (Do attention heads specializing in semantic relations contribute to BERT's performance?): sent21
    5.1. (What did Kovaleva et al. (2019) identify about self-attention maps and core frame semantic relations?): sent22
    5.2. (Are these heads essential for BERT's success on GLUE tasks?): sent23",Do self-attention heads encode nontrivial linguistic information?,"sent3, sent4, sent5","However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 .

Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019).

This apparent redundancy must be related to the overparametrization issue (see section 7).","Kovaleva et al. (2019) showed that most self-attention heads do not directly encode any nontrivial linguistic information.

Much of the model encoded the vertical pattern, such as attention to [CLS], [SEP], and punctuation tokens, consistent with the observations by Clark et al. (2019).

This apparent redundancy is likely related to the overparametrization issue.","Questions:

Do self-attention heads encode nontrivial linguistic information?

Answer:

Kovaleva et al. (2019) showed that most self-attention heads do not directly encode any nontrivial linguistic information.

Much of the model encoded the vertical pattern, such as attention to [CLS], [SEP], and punctuation tokens, consistent with the observations by Clark et al. (2019).

This apparent redundancy is likely related to the overparametrization issue."
A Primer in BERTology: What we know about how BERT works,Self-attention heads,"Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","# QUA Analysis:

1. (What is the significance of attention in understanding Transformer models?): sent1
    1.1. (What are the types of attention heads proposed by studies?): sent1
    1.2. (What is the meaning of attention weight according to Clark et al. (2019)?): sent2
    1.3. (Do self-attention heads encode nontrivial linguistic information?): sent3
        1.3.1. (What pattern do most of the model's attention heads encode?): sent4
        1.3.2. (What issue might this redundancy be related to?): sent5
    1.4. (How is attention to [CLS], [SEP], and punctuation interpreted?): sent6
        1.4.1. (Why does BERT attend a lot to [SEP] and punctuation?): sent7
        1.4.2. (What might be the function of [SEP] according to Clark et al. (2019)?): sent8
        1.4.3. (How does the importance of [SEP] change across layers?): sent9
        1.4.4. (What should be reconsidered if the hypothesis about [SEP] is correct?): sent10
2. (What is the ""heterogeneous"" self-attention pattern?): sent11
    2.1. (What do studies find about BERT heads with linguistically interpretable functions?): sent12
        2.1.1. (What do Htut et al. (2019) and Clark et al. (2019) report about BERT heads?): sent13
        2.1.2. (What do these studies find about heads attending to words in obj role?): sent14
        2.1.3. (How does the evidence for other syntactic roles vary between studies?): sent15
        2.1.4. (What supports the overall conclusion about BERT heads?): sent16
        2.1.5. (What is Hoover et al. (2019)'s hypothesis about complex dependencies?): sent17
            2.1.5.1. (What do Clark et al. (2019) and Htut et al. (2019) conclude about single heads and syntactic tree information?): sent17
3. (What is the evidence regarding attention weights and linguistic tasks?): sent18
    3.1. (How do BERT's self-attention weights compare to a uniform attention baseline?): sent19
4. (Can a BERT head be used for coreference resolution?): sent20
5. (Do attention heads specializing in semantic relations contribute to BERT's performance?): sent21
    5.1. (What did Kovaleva et al. (2019) identify about self-attention maps and core frame semantic relations?): sent22
    5.2. (Are these heads essential for BERT's success on GLUE tasks?): sent23","How is attention to [CLS], [SEP], and punctuation interpreted in BERT?","sent6, sent7, sent8, sent9","Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation.

Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them.

They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] gets increased attention starting in layer 5, but its importance for prediction drops.","Attention to [CLS] is interpreted as attention to an aggregated sentence-level representation, but BERT also attends significantly to [SEP] and punctuation.

Clark et al. (2019) hypothesize that periods and commas are almost as frequent as [CLS] and [SEP], leading the model to rely on them.

Clark et al. (2019) also suggest that the function of [SEP] might be one of ""no-op,"" serving as a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] receives increased attention starting in layer 5, but its importance for prediction decreases.","Questions:

How is attention to [CLS], [SEP], and punctuation interpreted in BERT?

Answer:

Attention to [CLS] is interpreted as attention to an aggregated sentence-level representation, but BERT also attends significantly to [SEP] and punctuation.

Clark et al. (2019) hypothesize that periods and commas are almost as frequent as [CLS] and [SEP], leading the model to rely on them.

Clark et al. (2019) also suggest that the function of [SEP] might be one of ""no-op,"" serving as a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] receives increased attention starting in layer 5, but its importance for prediction decreases."
A Primer in BERTology: What we know about how BERT works,Self-attention heads,"Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","# QUA Analysis:

1. (What is the significance of attention in understanding Transformer models?): sent1
    1.1. (What are the types of attention heads proposed by studies?): sent1
    1.2. (What is the meaning of attention weight according to Clark et al. (2019)?): sent2
    1.3. (Do self-attention heads encode nontrivial linguistic information?): sent3
        1.3.1. (What pattern do most of the model's attention heads encode?): sent4
        1.3.2. (What issue might this redundancy be related to?): sent5
    1.4. (How is attention to [CLS], [SEP], and punctuation interpreted?): sent6
        1.4.1. (Why does BERT attend a lot to [SEP] and punctuation?): sent7
        1.4.2. (What might be the function of [SEP] according to Clark et al. (2019)?): sent8
        1.4.3. (How does the importance of [SEP] change across layers?): sent9
        1.4.4. (What should be reconsidered if the hypothesis about [SEP] is correct?): sent10
2. (What is the ""heterogeneous"" self-attention pattern?): sent11
    2.1. (What do studies find about BERT heads with linguistically interpretable functions?): sent12
        2.1.1. (What do Htut et al. (2019) and Clark et al. (2019) report about BERT heads?): sent13
        2.1.2. (What do these studies find about heads attending to words in obj role?): sent14
        2.1.3. (How does the evidence for other syntactic roles vary between studies?): sent15
        2.1.4. (What supports the overall conclusion about BERT heads?): sent16
        2.1.5. (What is Hoover et al. (2019)'s hypothesis about complex dependencies?): sent17
            2.1.5.1. (What do Clark et al. (2019) and Htut et al. (2019) conclude about single heads and syntactic tree information?): sent17
3. (What is the evidence regarding attention weights and linguistic tasks?): sent18
    3.1. (How do BERT's self-attention weights compare to a uniform attention baseline?): sent19
4. (Can a BERT head be used for coreference resolution?): sent20
5. (Do attention heads specializing in semantic relations contribute to BERT's performance?): sent21
    5.1. (What did Kovaleva et al. (2019) identify about self-attention maps and core frame semantic relations?): sent22
    5.2. (Are these heads essential for BERT's success on GLUE tasks?): sent23",What should be reconsidered if the hypothesis about [SEP] is correct?,"sent8, sent9, sent10","They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] gets increased attention starting in layer 5, but its importance for prediction drops.

If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.","If the hypothesis about [SEP] is correct, the function of [SEP] might be one of ""no-op,"" a signal to ignore the head if its pattern is not applicable to the current case.

The importance of [SEP] for prediction drops despite it getting increased attention starting in layer 5.

Attention probing studies that excluded the [SEP] and [CLS] tokens (e.g., Htut et al. (2019)) should perhaps be revisited.","Questions:

What should be reconsidered if the hypothesis about [SEP] is correct?

Answer:

If the hypothesis about [SEP] is correct, the function of [SEP] might be one of ""no-op,"" a signal to ignore the head if its pattern is not applicable to the current case.

The importance of [SEP] for prediction drops despite it getting increased attention starting in layer 5.

Attention probing studies that excluded the [SEP] and [CLS] tokens (e.g., Htut et al. (2019)) should perhaps be revisited."
A Primer in BERTology: What we know about how BERT works,Self-attention heads,"Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","# QUA Analysis:

1. (What is the significance of attention in understanding Transformer models?): sent1
    1.1. (What are the types of attention heads proposed by studies?): sent1
    1.2. (What is the meaning of attention weight according to Clark et al. (2019)?): sent2
    1.3. (Do self-attention heads encode nontrivial linguistic information?): sent3
        1.3.1. (What pattern do most of the model's attention heads encode?): sent4
        1.3.2. (What issue might this redundancy be related to?): sent5
    1.4. (How is attention to [CLS], [SEP], and punctuation interpreted?): sent6
        1.4.1. (Why does BERT attend a lot to [SEP] and punctuation?): sent7
        1.4.2. (What might be the function of [SEP] according to Clark et al. (2019)?): sent8
        1.4.3. (How does the importance of [SEP] change across layers?): sent9
        1.4.4. (What should be reconsidered if the hypothesis about [SEP] is correct?): sent10
2. (What is the ""heterogeneous"" self-attention pattern?): sent11
    2.1. (What do studies find about BERT heads with linguistically interpretable functions?): sent12
        2.1.1. (What do Htut et al. (2019) and Clark et al. (2019) report about BERT heads?): sent13
        2.1.2. (What do these studies find about heads attending to words in obj role?): sent14
        2.1.3. (How does the evidence for other syntactic roles vary between studies?): sent15
        2.1.4. (What supports the overall conclusion about BERT heads?): sent16
        2.1.5. (What is Hoover et al. (2019)'s hypothesis about complex dependencies?): sent17
            2.1.5.1. (What do Clark et al. (2019) and Htut et al. (2019) conclude about single heads and syntactic tree information?): sent17
3. (What is the evidence regarding attention weights and linguistic tasks?): sent18
    3.1. (How do BERT's self-attention weights compare to a uniform attention baseline?): sent19
4. (Can a BERT head be used for coreference resolution?): sent20
5. (Do attention heads specializing in semantic relations contribute to BERT's performance?): sent21
    5.1. (What did Kovaleva et al. (2019) identify about self-attention maps and core frame semantic relations?): sent22
    5.2. (Are these heads essential for BERT's success on GLUE tasks?): sent23",What do studies find about BERT heads with linguistically interpretable functions?,"sent12, sent13, sent14, sent15, sent16","Some BERT heads seem to specialize in certain types of syntactic relations.

Htut et al.(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions.

The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline.

The evidence for nsubj, advmod, and amod has some variation between these two studies.

The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context.","Some BERT heads seem to specialize in certain types of syntactic relations.

Htut et al. (2019) and Clark et al. (2019) report that there are BERT heads that attend significantly more than a random baseline to words in certain syntactic positions.

Both studies find that there are heads that attend to words in the object role more than the positional baseline.

The evidence for nsubj, advmod, and amod has some variation between these two studies.

The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in a machine translation context.","Questions:

What do studies find about BERT heads with linguistically interpretable functions?

Answer:

Some BERT heads seem to specialize in certain types of syntactic relations.

Htut et al. (2019) and Clark et al. (2019) report that there are BERT heads that attend significantly more than a random baseline to words in certain syntactic positions.

Both studies find that there are heads that attend to words in the object role more than the positional baseline.

The evidence for nsubj, advmod, and amod has some variation between these two studies.

The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in a machine translation context."
A Primer in BERTology: What we know about how BERT works,Self-attention heads,"Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","# QUA Analysis:

1. (What is the significance of attention in understanding Transformer models?): sent1
    1.1. (What are the types of attention heads proposed by studies?): sent1
    1.2. (What is the meaning of attention weight according to Clark et al. (2019)?): sent2
    1.3. (Do self-attention heads encode nontrivial linguistic information?): sent3
        1.3.1. (What pattern do most of the model's attention heads encode?): sent4
        1.3.2. (What issue might this redundancy be related to?): sent5
    1.4. (How is attention to [CLS], [SEP], and punctuation interpreted?): sent6
        1.4.1. (Why does BERT attend a lot to [SEP] and punctuation?): sent7
        1.4.2. (What might be the function of [SEP] according to Clark et al. (2019)?): sent8
        1.4.3. (How does the importance of [SEP] change across layers?): sent9
        1.4.4. (What should be reconsidered if the hypothesis about [SEP] is correct?): sent10
2. (What is the ""heterogeneous"" self-attention pattern?): sent11
    2.1. (What do studies find about BERT heads with linguistically interpretable functions?): sent12
        2.1.1. (What do Htut et al. (2019) and Clark et al. (2019) report about BERT heads?): sent13
        2.1.2. (What do these studies find about heads attending to words in obj role?): sent14
        2.1.3. (How does the evidence for other syntactic roles vary between studies?): sent15
        2.1.4. (What supports the overall conclusion about BERT heads?): sent16
        2.1.5. (What is Hoover et al. (2019)'s hypothesis about complex dependencies?): sent17
            2.1.5.1. (What do Clark et al. (2019) and Htut et al. (2019) conclude about single heads and syntactic tree information?): sent17
3. (What is the evidence regarding attention weights and linguistic tasks?): sent18
    3.1. (How do BERT's self-attention weights compare to a uniform attention baseline?): sent19
4. (Can a BERT head be used for coreference resolution?): sent20
5. (Do attention heads specializing in semantic relations contribute to BERT's performance?): sent21
    5.1. (What did Kovaleva et al. (2019) identify about self-attention maps and core frame semantic relations?): sent22
    5.2. (Are these heads essential for BERT's success on GLUE tasks?): sent23",How does the evidence for other syntactic roles vary between studies?,"sent14, sent15, sent16","The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline.

The evidence for nsubj, advmod, and amod has some variation between these two studies.

The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context.","The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in the obj role more than the positional baseline.

The evidence for nsubj, advmod, and amod shows some variation between these two studies.

The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in a machine translation context.","Questions:

How does the evidence for other syntactic roles vary between studies?

Answer:

The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in the obj role more than the positional baseline.

The evidence for nsubj, advmod, and amod shows some variation between these two studies.

The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in a machine translation context."
A Primer in BERTology: What we know about how BERT works,Self-attention heads,"Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","# QUA Analysis:

1. (What is the significance of attention in understanding Transformer models?): sent1
    1.1. (What are the types of attention heads proposed by studies?): sent1
    1.2. (What is the meaning of attention weight according to Clark et al. (2019)?): sent2
    1.3. (Do self-attention heads encode nontrivial linguistic information?): sent3
        1.3.1. (What pattern do most of the model's attention heads encode?): sent4
        1.3.2. (What issue might this redundancy be related to?): sent5
    1.4. (How is attention to [CLS], [SEP], and punctuation interpreted?): sent6
        1.4.1. (Why does BERT attend a lot to [SEP] and punctuation?): sent7
        1.4.2. (What might be the function of [SEP] according to Clark et al. (2019)?): sent8
        1.4.3. (How does the importance of [SEP] change across layers?): sent9
        1.4.4. (What should be reconsidered if the hypothesis about [SEP] is correct?): sent10
2. (What is the ""heterogeneous"" self-attention pattern?): sent11
    2.1. (What do studies find about BERT heads with linguistically interpretable functions?): sent12
        2.1.1. (What do Htut et al. (2019) and Clark et al. (2019) report about BERT heads?): sent13
        2.1.2. (What do these studies find about heads attending to words in obj role?): sent14
        2.1.3. (How does the evidence for other syntactic roles vary between studies?): sent15
        2.1.4. (What supports the overall conclusion about BERT heads?): sent16
        2.1.5. (What is Hoover et al. (2019)'s hypothesis about complex dependencies?): sent17
            2.1.5.1. (What do Clark et al. (2019) and Htut et al. (2019) conclude about single heads and syntactic tree information?): sent17
3. (What is the evidence regarding attention weights and linguistic tasks?): sent18
    3.1. (How do BERT's self-attention weights compare to a uniform attention baseline?): sent19
4. (Can a BERT head be used for coreference resolution?): sent20
5. (Do attention heads specializing in semantic relations contribute to BERT's performance?): sent21
    5.1. (What did Kovaleva et al. (2019) identify about self-attention maps and core frame semantic relations?): sent22
    5.2. (Are these heads essential for BERT's success on GLUE tasks?): sent23",Do attention heads specializing in semantic relations contribute to BERT's performance?,"sent21, sent22, sent23","Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks.

Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998).

Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks.

Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998).

Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","Questions:

Do attention heads specializing in semantic relations contribute to BERT's performance?

Answer:

Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks.

Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998).

Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks."
Conversational Machine Comprehension: a Literature Review,What is Conversational Machine Comprehension?,"The task of CMC is defined as: Given a passage P , the conversation history in the form of questionanswer pairs {Q 1 , A 1 , Q 2 , A 2 , ..., Q i−1 , A i−1 } and a question Q i , the model needs to predict the answer A i . The answer A i can either be a text span (s i , e i ) (Choi et al., 2018) or a free-form text {a i,1 , a i,2 , ..., a i,j } with evidence R i (Reddy et al., 2019). Single-turn MRC models cannot directly cater to CMC, as the latter is much more challenging to address. The major challenges being:

• The encoding module needs to encode not only P and A i but also the conversational history.

• General observation about information-seeking dialog in humans suggests that the starting dialogturns tend to focus on the beginning chunks of the passage and shift focus to the later chunks as the conversation progresses (Choi et al., 2018). The model is thus expected to capture these focal shifts during a conversation and reason pragmatically, instead of only matching lexically or via paraphrasing.

• Multi-turn conversations are generally incremental and co-referential. These conversational dialogs are either drilling down (the current question is a request for more information about the topic), shifting topic (the current question is not immediately relevant to something previously discussed), returning topic (the current question is asking about a topic again after it had previously been shifted away from), clarification of topic, or definition of an entity (Yatskar, 2019). The model should, therefore, be able to take context from history which may or may not be immediate.","1. What is the task of Conversational Machine Comprehension (CMC)? 
    1.1. How is the task of CMC defined? 
        - sent1
    1.2. What forms can the answer A_i take in CMC? 
        - sent2

2. Why can't single-turn MRC models directly cater to CMC?
    2.1. What makes CMC more challenging than single-turn MRC?
        - sent3
    2.2. What are the major challenges in CMC?
        2.2.1. What does the encoding module need to encode in CMC?
            - sent4
        2.2.2. What is a general observation about information-seeking dialog in humans?
            - sent5
        2.2.3. What is the model expected to capture during a conversation?
            - sent6
        2.2.4. What are the characteristics of multi-turn conversations?
            - sent7
        2.2.5. What types of conversational dialogs exist in CMC?
            - sent8
        2.2.6. What should the model be able to do with context from history?
            - sent9",What are the major challenges in Conversational Machine Comprehension (CMC)?,"sent4, sent5, sent6, sent7, sent8, sent9","The major challenges being: • The encoding module needs to encode not only P and A i but also the conversational history.

• General observation about information-seeking dialog in humans suggests that the starting dialogturns tend to focus on the beginning chunks of the passage and shift focus to the later chunks as the conversation progresses (Choi et al., 2018).

The model is thus expected to capture these focal shifts during a conversation and reason pragmatically, instead of only matching lexically or via paraphrasing.

• Multi-turn conversations are generally incremental and co-referential.

These conversational dialogs are either drilling down (the current question is a request for more information about the topic), shifting topic (the current question is not immediately relevant to something previously discussed), returning topic (the current question is asking about a topic again after it had previously been shifted away from), clarification of topic, or definition of an entity (Yatskar, 2019).

The model should, therefore, be able to take context from history which may or may not be immediate.","The major challenges in Conversational Machine Comprehension (CMC) include the need for the encoding module to encode not only P and A i but also the conversational history.

General observations about information-seeking dialog in humans suggest that the starting dialog turns tend to focus on the beginning chunks of the passage and shift focus to the later chunks as the conversation progresses (Choi et al., 2018).

The model is thus expected to capture these focal shifts during a conversation and reason pragmatically, instead of only matching lexically or via paraphrasing.

Multi-turn conversations are generally incremental and co-referential.

These conversational dialogs can involve drilling down (requesting more information about the topic), shifting topic (introducing a new topic), returning to a previous topic, clarifying the topic, or defining an entity (Yatskar, 2019).

The model should, therefore, be able to take context from history which may or may not be immediate.","Questions:

What are the major challenges in Conversational Machine Comprehension (CMC)?

Answer:

The major challenges in Conversational Machine Comprehension (CMC) include the need for the encoding module to encode not only P and A i but also the conversational history.

General observations about information-seeking dialog in humans suggest that the starting dialog turns tend to focus on the beginning chunks of the passage and shift focus to the later chunks as the conversation progresses (Choi et al., 2018).

The model is thus expected to capture these focal shifts during a conversation and reason pragmatically, instead of only matching lexically or via paraphrasing.

Multi-turn conversations are generally incremental and co-referential.

These conversational dialogs can involve drilling down (requesting more information about the topic), shifting topic (introducing a new topic), returning to a previous topic, clarifying the topic, or defining an entity (Yatskar, 2019).

The model should, therefore, be able to take context from history which may or may not be immediate."
A Survey of Unsupervised Dependency Parsing,Related Areas,"Supervised Dependency Parsing Supervised dependency parsing aims to train a dependency parser from training sentences that are manually annotated with their dependency parse trees. Generally, supervised dependency parsing approaches can be divided into graph-based approaches and transition-based approaches. A graph-based dependency parser searches for the best spanning tree of the graph that is formed by connecting all pairs of words in the input sentence. In the simplest form, a graph-based parser makes the first-order assumption that the score of a dependency tree is the summation of scores of its edges (McDonald et al., 2005). A transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets.

Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015;McDonald et al., 2011;Ma and Xia, 2014;Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either.

Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because it has to induce not only edges but also nodes of a tree. Consequently, there have been far more papers in unsupervised dependency parsing than in unsupervised constituency parsing over the past decade. More recently, however, there is a surge in interest in unsupervised constituency parsing and several novel approaches were proposed in the past two years . While we focus on unsupervised dependency parsing in this paper, most of our discussions on the classification of approaches and recent trends apply to unsupervised constituency parsing as well.

Latent Tree Models with Downstream Tasks Latent tree models treat the parse tree as a latent variable that is used in downstream tasks such as sentiment classification. While no treebank is used in training, these models rely on the performance of the downstream tasks to guide the learning of the latent parse trees. To enable end-to-end learning, the REINFORCE algorithm and the Gumbel-softmax trick (Jang et al., 2017) can be utilized (Yogatama et al., 2016;Choi et al., 2018). There also exists previous work on latent dependency tree models that utilizes structured attention mechanisms (Kim et al., 2017) for applications. Latent tree models differ from unsupervised parsing in that they utilize training signals from downstream tasks and that they aim to improve performance of downstream tasks instead of syntactic parsing.

3 General Approaches 3.1 Generative Approaches","1. What is supervised dependency parsing? (sent1)
    1.1. How are supervised dependency parsing approaches categorized? (sent2)
        1.1.1. What is a graph-based dependency parser? (sent3)
            1.1.1.1. What is the simplest form of a graph-based parser? (sent4)
        1.1.2. What is a transition-based dependency parser? (sent5)
    1.2. What are the limitations of current supervised dependency parsing approaches? (sent6)
2. What is cross-domain and cross-lingual parsing? (sent7)
    2.1. How is cross-domain and cross-lingual parsing related to unsupervised parsing? (sent8)
        2.1.1. Why is unsupervised parsing more challenging? (sent9)
3. What is unsupervised constituency parsing? (sent10)
    3.1. Why is unsupervised constituency parsing considered more difficult than unsupervised dependency parsing? (sent11)
        3.1.1. What has been the trend in research papers over the past decade? (sent12)
        3.1.2. What is the recent trend in unsupervised constituency parsing? (sent13)
    3.2. How is unsupervised constituency parsing related to the focus of this paper? (sent14)
4. What are latent tree models with downstream tasks? (sent15)
    4.1. How do latent tree models learn without a treebank? (sent16)
        4.1.1. What algorithms can be used for end-to-end learning in latent tree models? (sent17)
    4.2. What previous work exists on latent dependency tree models? (sent18)
    4.3. How do latent tree models differ from unsupervised parsing? (sent19)",What are the main approaches in supervised dependency parsing?,"sent2, sent3, sent4, sent5","Generally, supervised dependency parsing approaches can be divided into graph-based approaches and transition-based approaches.

A graph-based dependency parser searches for the best spanning tree of the graph that is formed by connecting all pairs of words in the input sentence.

In the simplest form, a graph-based parser makes the first-order assumption that the score of a dependency tree is the summation of scores of its edges (McDonald et al., 2005).

A transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right.","Supervised dependency parsing approaches can generally be divided into graph-based approaches and transition-based approaches.

A graph-based dependency parser searches for the best spanning tree of the graph formed by connecting all pairs of words in the input sentence.

In the simplest form, a graph-based parser makes the first-order assumption that the score of a dependency tree is the summation of scores of its edges (McDonald et al., 2005).

A transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right.","Questions:

What are the main approaches in supervised dependency parsing?

Answer:

Supervised dependency parsing approaches can generally be divided into graph-based approaches and transition-based approaches.

A graph-based dependency parser searches for the best spanning tree of the graph formed by connecting all pairs of words in the input sentence.

In the simplest form, a graph-based parser makes the first-order assumption that the score of a dependency tree is the summation of scores of its edges (McDonald et al., 2005).

A transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right."
A Survey of Unsupervised Dependency Parsing,Related Areas,"Supervised Dependency Parsing Supervised dependency parsing aims to train a dependency parser from training sentences that are manually annotated with their dependency parse trees. Generally, supervised dependency parsing approaches can be divided into graph-based approaches and transition-based approaches. A graph-based dependency parser searches for the best spanning tree of the graph that is formed by connecting all pairs of words in the input sentence. In the simplest form, a graph-based parser makes the first-order assumption that the score of a dependency tree is the summation of scores of its edges (McDonald et al., 2005). A transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets.

Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015;McDonald et al., 2011;Ma and Xia, 2014;Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either.

Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because it has to induce not only edges but also nodes of a tree. Consequently, there have been far more papers in unsupervised dependency parsing than in unsupervised constituency parsing over the past decade. More recently, however, there is a surge in interest in unsupervised constituency parsing and several novel approaches were proposed in the past two years . While we focus on unsupervised dependency parsing in this paper, most of our discussions on the classification of approaches and recent trends apply to unsupervised constituency parsing as well.

Latent Tree Models with Downstream Tasks Latent tree models treat the parse tree as a latent variable that is used in downstream tasks such as sentiment classification. While no treebank is used in training, these models rely on the performance of the downstream tasks to guide the learning of the latent parse trees. To enable end-to-end learning, the REINFORCE algorithm and the Gumbel-softmax trick (Jang et al., 2017) can be utilized (Yogatama et al., 2016;Choi et al., 2018). There also exists previous work on latent dependency tree models that utilizes structured attention mechanisms (Kim et al., 2017) for applications. Latent tree models differ from unsupervised parsing in that they utilize training signals from downstream tasks and that they aim to improve performance of downstream tasks instead of syntactic parsing.

3 General Approaches 3.1 Generative Approaches","1. What is supervised dependency parsing? (sent1)
    1.1. How are supervised dependency parsing approaches categorized? (sent2)
        1.1.1. What is a graph-based dependency parser? (sent3)
            1.1.1.1. What is the simplest form of a graph-based parser? (sent4)
        1.1.2. What is a transition-based dependency parser? (sent5)
    1.2. What are the limitations of current supervised dependency parsing approaches? (sent6)
2. What is cross-domain and cross-lingual parsing? (sent7)
    2.1. How is cross-domain and cross-lingual parsing related to unsupervised parsing? (sent8)
        2.1.1. Why is unsupervised parsing more challenging? (sent9)
3. What is unsupervised constituency parsing? (sent10)
    3.1. Why is unsupervised constituency parsing considered more difficult than unsupervised dependency parsing? (sent11)
        3.1.1. What has been the trend in research papers over the past decade? (sent12)
        3.1.2. What is the recent trend in unsupervised constituency parsing? (sent13)
    3.2. How is unsupervised constituency parsing related to the focus of this paper? (sent14)
4. What are latent tree models with downstream tasks? (sent15)
    4.1. How do latent tree models learn without a treebank? (sent16)
        4.1.1. What algorithms can be used for end-to-end learning in latent tree models? (sent17)
    4.2. What previous work exists on latent dependency tree models? (sent18)
    4.3. How do latent tree models differ from unsupervised parsing? (sent19)",How is cross-domain and cross-lingual parsing related to unsupervised parsing?,"sent7, sent8, sent9","Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015;McDonald et al., 2011;Ma and Xia, 2014;Duong et al., 2015).

This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language.

However, unsupervised parsing is more challenging because it does not have access to any source treebank either.","One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015; McDonald et al., 2011; Ma and Xia, 2014; Duong et al., 2015).

This approach is closely related to unsupervised parsing as both methods do not rely on treebanks in the target domain or language.

However, unsupervised parsing is more challenging because it does not have access to any source treebank either.","Questions:

How is cross-domain and cross-lingual parsing related to unsupervised parsing?

Answer:

One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015; McDonald et al., 2011; Ma and Xia, 2014; Duong et al., 2015).

This approach is closely related to unsupervised parsing as both methods do not rely on treebanks in the target domain or language.

However, unsupervised parsing is more challenging because it does not have access to any source treebank either."
A Survey of Unsupervised Dependency Parsing,Models,"A generative approach models the joint probability of the sentence and the corresponding parse tree. Traditional generative models are mostly based on probabilistic grammars. To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions. The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else. Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference. However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.

Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree. Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner. The generation of a child token is conditioned on the head token and the dependency direction. In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden  propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).

Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.  use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV.","1. What is a generative approach in unsupervised dependency parsing?
    1.1. What are traditional generative models based on? sent2
        1.1.1. Why do traditional generative models make conditional independence assumptions? sent3
            1.1.1.1. What is the simplest conditional independence assumption? sent4
            1.1.1.2. What is the benefit of making such assumptions? sent5
            1.1.1.3. What is the drawback of making such assumptions? sent6
    1.2. How do different generative models specify generation processes? sent7
        1.2.1. How do Paskin (2002) and Carroll and Charniak (1992) specify the generation process? sent8
            1.2.1.1. What is the generation process according to Paskin (2002) and Carroll and Charniak (1992)? sent9
        1.2.2. How do Klein and Manning (2004) specify the generation process? sent10
            1.2.2.1. What is the generation process according to Klein and Manning (2004)? sent11
            1.2.2.2. What additional condition is considered in the generation process of DMV? sent12
            1.2.2.3. What further extensions or modifications have been proposed to DMV? sent13, sent14, sent15
2. What other grammar formalisms have been used for unsupervised dependency parsing? sent16
    2.1. Why might similar tokens have similar syntactic behaviors in a grammar? sent17
        2.1.1. What is an example of similar syntactic behavior among tokens? sent18
        2.1.2. How can prior knowledge of syntactic similarity be captured? sent19
            2.1.2.1. What methods are used to compute generation probabilities from syntactic similarity features? sent20
            2.1.2.2. What are these methods based on? sent21",What are traditional generative models in unsupervised dependency parsing based on?,"sent2, sent3, sent4, sent5, sent6","Traditional generative models are mostly based on probabilistic grammars.

To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions.

The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else.

Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference.

However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.","Traditional generative models in unsupervised dependency parsing are mostly based on probabilistic grammars.

To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions.

The simplest assumption, known as the context-free assumption, states that the generation of a token is only dependent on its head token and is independent of anything else.

Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference.

However, these assumptions also result in the unavailability of useful information, such as context and generation history, in generating each token.","Questions:

What are traditional generative models in unsupervised dependency parsing based on?

Answer:

Traditional generative models in unsupervised dependency parsing are mostly based on probabilistic grammars.

To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions.

The simplest assumption, known as the context-free assumption, states that the generation of a token is only dependent on its head token and is independent of anything else.

Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference.

However, these assumptions also result in the unavailability of useful information, such as context and generation history, in generating each token."
A Survey of Unsupervised Dependency Parsing,Models,"A generative approach models the joint probability of the sentence and the corresponding parse tree. Traditional generative models are mostly based on probabilistic grammars. To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions. The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else. Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference. However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.

Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree. Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner. The generation of a child token is conditioned on the head token and the dependency direction. In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden  propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).

Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.  use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV.","1. What is a generative approach in unsupervised dependency parsing?
    1.1. What are traditional generative models based on? sent2
        1.1.1. Why do traditional generative models make conditional independence assumptions? sent3
            1.1.1.1. What is the simplest conditional independence assumption? sent4
            1.1.1.2. What is the benefit of making such assumptions? sent5
            1.1.1.3. What is the drawback of making such assumptions? sent6
    1.2. How do different generative models specify generation processes? sent7
        1.2.1. How do Paskin (2002) and Carroll and Charniak (1992) specify the generation process? sent8
            1.2.1.1. What is the generation process according to Paskin (2002) and Carroll and Charniak (1992)? sent9
        1.2.2. How do Klein and Manning (2004) specify the generation process? sent10
            1.2.2.1. What is the generation process according to Klein and Manning (2004)? sent11
            1.2.2.2. What additional condition is considered in the generation process of DMV? sent12
            1.2.2.3. What further extensions or modifications have been proposed to DMV? sent13, sent14, sent15
2. What other grammar formalisms have been used for unsupervised dependency parsing? sent16
    2.1. Why might similar tokens have similar syntactic behaviors in a grammar? sent17
        2.1.1. What is an example of similar syntactic behavior among tokens? sent18
        2.1.2. How can prior knowledge of syntactic similarity be captured? sent19
            2.1.2.1. What methods are used to compute generation probabilities from syntactic similarity features? sent20
            2.1.2.2. What are these methods based on? sent21",How do different generative models specify the generation processes of sentences and parse trees?,"sent7, sent8, sent9, sent10, sent11, sent12, sent13, sent14, sent15","Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree.

Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner.

The generation of a child token is conditioned on the head token and the dependency direction.

In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously.

Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token.

Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token.

Headden  propose to also introduce the valence into the condition of decision sampling.

Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context.

Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information.","Different generative models specify different generation processes of the sentence and parse tree based on their respective independence assumptions.

For example, Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner.

The generation of a child token is conditioned on the head token and the dependency direction.

In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously.

Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token.

Additionally, in the Dependency Model with Valence (DMV), the generation of a child token is conditioned on the valence, which is defined as the number of child tokens already generated from a head token.

Headden propose to also introduce the valence into the condition of decision sampling.

Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context.

Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information.","Questions:

How do different generative models specify the generation processes of sentences and parse trees?

Answer:

Different generative models specify different generation processes of the sentence and parse tree based on their respective independence assumptions.

For example, Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner.

The generation of a child token is conditioned on the head token and the dependency direction.

In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously.

Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token.

Additionally, in the Dependency Model with Valence (DMV), the generation of a child token is conditioned on the valence, which is defined as the number of child tokens already generated from a head token.

Headden propose to also introduce the valence into the condition of decision sampling.

Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context.

Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information."
A Survey of Unsupervised Dependency Parsing,Models,"A generative approach models the joint probability of the sentence and the corresponding parse tree. Traditional generative models are mostly based on probabilistic grammars. To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions. The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else. Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference. However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.

Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree. Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner. The generation of a child token is conditioned on the head token and the dependency direction. In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden  propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).

Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.  use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV.","1. What is a generative approach in unsupervised dependency parsing?
    1.1. What are traditional generative models based on? sent2
        1.1.1. Why do traditional generative models make conditional independence assumptions? sent3
            1.1.1.1. What is the simplest conditional independence assumption? sent4
            1.1.1.2. What is the benefit of making such assumptions? sent5
            1.1.1.3. What is the drawback of making such assumptions? sent6
    1.2. How do different generative models specify generation processes? sent7
        1.2.1. How do Paskin (2002) and Carroll and Charniak (1992) specify the generation process? sent8
            1.2.1.1. What is the generation process according to Paskin (2002) and Carroll and Charniak (1992)? sent9
        1.2.2. How do Klein and Manning (2004) specify the generation process? sent10
            1.2.2.1. What is the generation process according to Klein and Manning (2004)? sent11
            1.2.2.2. What additional condition is considered in the generation process of DMV? sent12
            1.2.2.3. What further extensions or modifications have been proposed to DMV? sent13, sent14, sent15
2. What other grammar formalisms have been used for unsupervised dependency parsing? sent16
    2.1. Why might similar tokens have similar syntactic behaviors in a grammar? sent17
        2.1.1. What is an example of similar syntactic behavior among tokens? sent18
        2.1.2. How can prior knowledge of syntactic similarity be captured? sent19
            2.1.2.1. What methods are used to compute generation probabilities from syntactic similarity features? sent20
            2.1.2.2. What are these methods based on? sent21",What further extensions or modifications have been proposed to the Dependency Model with Valence (DMV)?,"sent13, sent14, sent15","Headden  propose to also introduce the valence into the condition of decision sampling.

Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context.

Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information.","Headden proposes to introduce valence into the condition of decision sampling.

Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context.

Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information.","Questions:

What further extensions or modifications have been proposed to the Dependency Model with Valence (DMV)?

Answer:

Headden proposes to introduce valence into the condition of decision sampling.

Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context.

Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information."
A Survey of Unsupervised Dependency Parsing,Models,"A generative approach models the joint probability of the sentence and the corresponding parse tree. Traditional generative models are mostly based on probabilistic grammars. To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions. The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else. Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference. However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.

Based on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree. Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner. The generation of a child token is conditioned on the head token and the dependency direction. In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden  propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).

Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.  use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV.","1. What is a generative approach in unsupervised dependency parsing?
    1.1. What are traditional generative models based on? sent2
        1.1.1. Why do traditional generative models make conditional independence assumptions? sent3
            1.1.1.1. What is the simplest conditional independence assumption? sent4
            1.1.1.2. What is the benefit of making such assumptions? sent5
            1.1.1.3. What is the drawback of making such assumptions? sent6
    1.2. How do different generative models specify generation processes? sent7
        1.2.1. How do Paskin (2002) and Carroll and Charniak (1992) specify the generation process? sent8
            1.2.1.1. What is the generation process according to Paskin (2002) and Carroll and Charniak (1992)? sent9
        1.2.2. How do Klein and Manning (2004) specify the generation process? sent10
            1.2.2.1. What is the generation process according to Klein and Manning (2004)? sent11
            1.2.2.2. What additional condition is considered in the generation process of DMV? sent12
            1.2.2.3. What further extensions or modifications have been proposed to DMV? sent13, sent14, sent15
2. What other grammar formalisms have been used for unsupervised dependency parsing? sent16
    2.1. Why might similar tokens have similar syntactic behaviors in a grammar? sent17
        2.1.1. What is an example of similar syntactic behavior among tokens? sent18
        2.1.2. How can prior knowledge of syntactic similarity be captured? sent19
            2.1.2.1. What methods are used to compute generation probabilities from syntactic similarity features? sent20
            2.1.2.2. What are these methods based on? sent21",What other grammar formalisms have been used for unsupervised dependency parsing?,"sent16, sent17, sent18, sent19, sent20, sent21","In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).

Similar tokens may have similar syntactic behaviors in a grammar.

For example, all the verbs are very likely to generate a noun to the left as the subject.

One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.

use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features.

Both approaches are based on DMV.","Other grammar formalisms used for unsupervised dependency parsing include tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013).

Similar tokens may exhibit similar syntactic behaviors in a grammar.

For example, all the verbs are very likely to generate a noun to the left as the subject.

One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.

One approach is to use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) (Jiang et al., 2016) or employ a neural network to automatically learn such features.

Both approaches are based on the Dependency Model with Valence (DMV).","Questions:

What other grammar formalisms have been used for unsupervised dependency parsing?

Answer:

Other grammar formalisms used for unsupervised dependency parsing include tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013).

Similar tokens may exhibit similar syntactic behaviors in a grammar.

For example, all the verbs are very likely to generate a noun to the left as the subject.

One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.

One approach is to use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) (Jiang et al., 2016) or employ a neural network to automatically learn such features.

Both approaches are based on the Dependency Model with Valence (DMV)."
A Survey of Unsupervised Dependency Parsing,Intermediate Representation Encoder Decoder,"Autoencoder CRFAE (Cai et al., 2017) Z P (z|x) P (x|z) D-NDMV (Han et al., 2019a) Deterministic Variant S P (s|x) P (z,x|s)

Variational Autoencoder (Li et al., 2019) Z P (z|x) P (z, x) D-NDMV (Han et al., 2019a) Variational Variant S P (s|x) P (z, x|s) (Corro and Titov, 2018) Z P (z|x) P (x|z) Table 1: Major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing. Z: dependency tree. S: continuous sentence representation.x is a copy of x representing the reconstructed sentence. z is the dependency tree. s is the continuous representation of sentence x.

In addition to the EM algorithm, the learning objective can also be optimized with gradient descent. Yang et al. (2020) recently observe that gradient descent can sometimes significantly outperform EM when learning neural DMV.

Better learning results can also be achieved by manipulating the training data. Spitkovsky et al. (2010a) apply curriculum learning to DMV training, which starts with only the shortest sentences and then progresses to increasingly longer sentences. Tu and Honavar (2011) provide a theoretical analysis on the utility of curriculum learning in unsupervised dependency parsing. Spitkovsky et al. (2013) propose to treat different learning algorithms and configurations as modules and connect them to form a network. Some approaches discussed above, such as Lateen EM and curriculum learning, can be seen as special cases of this approach.","1. What are the major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing?
    1.1. What are the specific models and their components?
        1.1.1. What are the components of Autoencoder CRFAE and D-NDMV?
            1.1.1.1. (Autoencoder CRFAE): sent1
            1.1.1.2. (Deterministic Variant of D-NDMV): sent2
            1.1.1.3. (Variational Variant of D-NDMV): sent3
    1.2. What do the symbols Z, S, and x represent in these models?
        1.2.1. (Explanation of symbols): sent4, sent5

2. How can the learning objective be optimized in these models?
    2.1. What are the methods for optimizing the learning objective?
        2.1.1. (EM algorithm and gradient descent): sent6
        2.1.2. (Effectiveness of gradient descent): sent7

3. How can better learning results be achieved?
    3.1. What are the methods to manipulate the training data for better learning results?
        3.1.1. (Curriculum learning): sent8
        3.1.2. (Application of curriculum learning to DMV training): sent9
        3.1.3. (Theoretical analysis of curriculum learning): sent10
    3.2. What is the modular approach proposed by Spitkovsky et al. (2013)?
        3.2.1. (Description of the modular approach): sent11
        3.2.2. (Special cases of the modular approach): sent12",What are the components of Autoencoder CRFAE and D-NDMV in unsupervised dependency parsing?,"sent1, sent2, sent3","Autoencoder CRFAE (Cai et al., 2017) Z P (z|x) P (x|z) D-NDMV (Han et al., 2019a)

Deterministic Variant S P (s|x) P (z,x|s)Variational Autoencoder (Li et al., 2019) Z P (z|x) P (z, x)

D-NDMV (Han et al., 2019a) Variational Variant S P (s|x) P (z, x|s) (Corro and Titov, 2018) Z P (z|x) P (x|z) Table 1: Major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing.","Autoencoder CRFAE (Cai et al., 2017) and D-NDMV (Han et al., 2019a) are components in unsupervised dependency parsing.

Deterministic Variant S P (s|x) P (z,x|s) and Variational Autoencoder (Li et al., 2019) Z P (z|x) P (z, x).

D-NDMV (Han et al., 2019a) includes a variational variant with components such as \( S \) representing \( P(s|x) \) and \( P(z, x|s) \) (Corro and Titov, 2018), and \( Z \) representing \( P(z|x) \) and \( P(x|z) \).","Questions:

What are the components of Autoencoder CRFAE and D-NDMV in unsupervised dependency parsing?

Answer:

Autoencoder CRFAE (Cai et al., 2017) and D-NDMV (Han et al., 2019a) are components in unsupervised dependency parsing.

Deterministic Variant S P (s|x) P (z,x|s) and Variational Autoencoder (Li et al., 2019) Z P (z|x) P (z, x).

D-NDMV (Han et al., 2019a) includes a variational variant with components such as \( S \) representing \( P(s|x) \) and \( P(z, x|s) \) (Corro and Titov, 2018), and \( Z \) representing \( P(z|x) \) and \( P(x|z) \)."
A Survey of Unsupervised Dependency Parsing,Intermediate Representation Encoder Decoder,"Autoencoder CRFAE (Cai et al., 2017) Z P (z|x) P (x|z) D-NDMV (Han et al., 2019a) Deterministic Variant S P (s|x) P (z,x|s)

Variational Autoencoder (Li et al., 2019) Z P (z|x) P (z, x) D-NDMV (Han et al., 2019a) Variational Variant S P (s|x) P (z, x|s) (Corro and Titov, 2018) Z P (z|x) P (x|z) Table 1: Major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing. Z: dependency tree. S: continuous sentence representation.x is a copy of x representing the reconstructed sentence. z is the dependency tree. s is the continuous representation of sentence x.

In addition to the EM algorithm, the learning objective can also be optimized with gradient descent. Yang et al. (2020) recently observe that gradient descent can sometimes significantly outperform EM when learning neural DMV.

Better learning results can also be achieved by manipulating the training data. Spitkovsky et al. (2010a) apply curriculum learning to DMV training, which starts with only the shortest sentences and then progresses to increasingly longer sentences. Tu and Honavar (2011) provide a theoretical analysis on the utility of curriculum learning in unsupervised dependency parsing. Spitkovsky et al. (2013) propose to treat different learning algorithms and configurations as modules and connect them to form a network. Some approaches discussed above, such as Lateen EM and curriculum learning, can be seen as special cases of this approach.","1. What are the major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing?
    1.1. What are the specific models and their components?
        1.1.1. What are the components of Autoencoder CRFAE and D-NDMV?
            1.1.1.1. (Autoencoder CRFAE): sent1
            1.1.1.2. (Deterministic Variant of D-NDMV): sent2
            1.1.1.3. (Variational Variant of D-NDMV): sent3
    1.2. What do the symbols Z, S, and x represent in these models?
        1.2.1. (Explanation of symbols): sent4, sent5

2. How can the learning objective be optimized in these models?
    2.1. What are the methods for optimizing the learning objective?
        2.1.1. (EM algorithm and gradient descent): sent6
        2.1.2. (Effectiveness of gradient descent): sent7

3. How can better learning results be achieved?
    3.1. What are the methods to manipulate the training data for better learning results?
        3.1.1. (Curriculum learning): sent8
        3.1.2. (Application of curriculum learning to DMV training): sent9
        3.1.3. (Theoretical analysis of curriculum learning): sent10
    3.2. What is the modular approach proposed by Spitkovsky et al. (2013)?
        3.2.1. (Description of the modular approach): sent11
        3.2.2. (Special cases of the modular approach): sent12",How can the training data be manipulated to achieve better learning results?,"sent8, sent9, sent10","Better learning results can also be achieved by manipulating the training data.

Spitkovsky et al. (2010a) apply curriculum learning to DMV training, which starts with only the shortest sentences and then progresses to increasingly longer sentences.

Tu and Honavar (2011) provide a theoretical analysis on the utility of curriculum learning in unsupervised dependency parsing.","Better learning results can be achieved by manipulating the training data.

For example, Spitkovsky et al. (2010a) apply curriculum learning to DMV training, which starts with only the shortest sentences and then progresses to increasingly longer sentences.

Tu and Honavar (2011) provide a theoretical analysis on the utility of curriculum learning in unsupervised dependency parsing.","Questions:

How can the training data be manipulated to achieve better learning results?

Answer:

Better learning results can be achieved by manipulating the training data.

For example, Spitkovsky et al. (2010a) apply curriculum learning to DMV training, which starts with only the shortest sentences and then progresses to increasingly longer sentences.

Tu and Honavar (2011) provide a theoretical analysis on the utility of curriculum learning in unsupervised dependency parsing."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,How Low is Low-Resource?,"On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","1. (What are the different thresholds used to define low-resource on the dimension of task-specific labels?): sent1
    1.1. (What thresholds are used for POS tagging?): sent2
    1.2. (What thresholds are used in other studies?): sent3
    1.3. (How does the threshold vary with task complexity?): sent4
        1.3.1. (What is an example of a threshold for a more complex task like text generation?): sent5
2. (How do resource requirements depend on the language?): sent6
    2.1. (How does task performance vary between language families with limited training data?): sent7
3. (What is the perspective on the lack of a hard threshold for low-resource settings?): sent8
    3.1. (What should be done to better compare approaches in low-resource settings?): sent9
        3.1.1. (What do studies show about the performance of non-neural vs. neural methods in very small datasets?): sent10",What thresholds are used to define low-resource for part-of-speech (POS) tagging?,"sent2, sent1, sent4, sent8","For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens.

On the dimension of task-specific labels, different thresholds are used to define low-resource.

The threshold is also task-dependent and more complex tasks might also increase the resource requirements.

Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability.","For part-of-speech (POS) tagging, Garrette and Baldridge (2013) define low-resource by limiting the time of the annotators to 2 hours, resulting in up to 1-2k tokens.

On the dimension of task-specific labels, different thresholds are used to define low-resource.

The threshold is also task-dependent, and more complex tasks might increase the resource requirements.

Given the lack of a hard threshold for low-resource settings, it is viewed as a spectrum of resource availability.","Questions:

What thresholds are used to define low-resource for part-of-speech (POS) tagging?

Answer:

For part-of-speech (POS) tagging, Garrette and Baldridge (2013) define low-resource by limiting the time of the annotators to 2 hours, resulting in up to 1-2k tokens.

On the dimension of task-specific labels, different thresholds are used to define low-resource.

The threshold is also task-dependent, and more complex tasks might increase the resource requirements.

Given the lack of a hard threshold for low-resource settings, it is viewed as a spectrum of resource availability."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,How Low is Low-Resource?,"On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","1. (What are the different thresholds used to define low-resource on the dimension of task-specific labels?): sent1
    1.1. (What thresholds are used for POS tagging?): sent2
    1.2. (What thresholds are used in other studies?): sent3
    1.3. (How does the threshold vary with task complexity?): sent4
        1.3.1. (What is an example of a threshold for a more complex task like text generation?): sent5
2. (How do resource requirements depend on the language?): sent6
    2.1. (How does task performance vary between language families with limited training data?): sent7
3. (What is the perspective on the lack of a hard threshold for low-resource settings?): sent8
    3.1. (What should be done to better compare approaches in low-resource settings?): sent9
        3.1.1. (What do studies show about the performance of non-neural vs. neural methods in very small datasets?): sent10",What thresholds are used to define low-resource in other studies?,"sent3, sent1, sent4, sent8","Kann et al.(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens.

On the dimension of task-specific labels, different thresholds are used to define low-resource.

The threshold is also task-dependent and more complex tasks might also increase the resource requirements.

Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability.","Kann et al. (2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020), and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens.

On the dimension of task-specific labels, different thresholds are used to define low-resource.

The threshold is also task-dependent, with more complex tasks potentially increasing the resource requirements.

Given the lack of a hard threshold for low-resource settings, it can be viewed as a spectrum of resource availability.","Questions:

What thresholds are used to define low-resource in other studies?

Answer:

Kann et al. (2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020), and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens.

On the dimension of task-specific labels, different thresholds are used to define low-resource.

The threshold is also task-dependent, with more complex tasks potentially increasing the resource requirements.

Given the lack of a hard threshold for low-resource settings, it can be viewed as a spectrum of resource availability."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,How Low is Low-Resource?,"On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","1. (What are the different thresholds used to define low-resource on the dimension of task-specific labels?): sent1
    1.1. (What thresholds are used for POS tagging?): sent2
    1.2. (What thresholds are used in other studies?): sent3
    1.3. (How does the threshold vary with task complexity?): sent4
        1.3.1. (What is an example of a threshold for a more complex task like text generation?): sent5
2. (How do resource requirements depend on the language?): sent6
    2.1. (How does task performance vary between language families with limited training data?): sent7
3. (What is the perspective on the lack of a hard threshold for low-resource settings?): sent8
    3.1. (What should be done to better compare approaches in low-resource settings?): sent9
        3.1.1. (What do studies show about the performance of non-neural vs. neural methods in very small datasets?): sent10",How does the threshold for low-resource vary with task complexity?,"sent4, sent5, sent1, sent8","The threshold is also task-dependent and more complex tasks might also increase the resource requirements.

For text generation,  frame their work as low-resource with 350k labeled training instances.

On the dimension of task-specific labels, different thresholds are used to define low-resource.

Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability.","The threshold for low-resource is task-dependent, and more complex tasks might increase the resource requirements.

For text generation, some researchers frame their work as low-resource with 350k labeled training instances.

On the dimension of task-specific labels, different thresholds are used to define low-resource.

Given the lack of a hard threshold for low-resource settings, it is viewed as a spectrum of resource availability.","Questions:

How does the threshold for low-resource vary with task complexity?

Answer:

The threshold for low-resource is task-dependent, and more complex tasks might increase the resource requirements.

For text generation, some researchers frame their work as low-resource with 350k labeled training instances.

On the dimension of task-specific labels, different thresholds are used to define low-resource.

Given the lack of a hard threshold for low-resource settings, it is viewed as a spectrum of resource availability."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,How Low is Low-Resource?,"On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","1. (What are the different thresholds used to define low-resource on the dimension of task-specific labels?): sent1
    1.1. (What thresholds are used for POS tagging?): sent2
    1.2. (What thresholds are used in other studies?): sent3
    1.3. (How does the threshold vary with task complexity?): sent4
        1.3.1. (What is an example of a threshold for a more complex task like text generation?): sent5
2. (How do resource requirements depend on the language?): sent6
    2.1. (How does task performance vary between language families with limited training data?): sent7
3. (What is the perspective on the lack of a hard threshold for low-resource settings?): sent8
    3.1. (What should be done to better compare approaches in low-resource settings?): sent9
        3.1.1. (What do studies show about the performance of non-neural vs. neural methods in very small datasets?): sent10",How do resource requirements depend on the language?,"sent6, sent7, sent1, sent8","Similar to the task, the resource requirements can also depend on the language.

Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

On the dimension of task-specific labels, different thresholds are used to define low-resource.

Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability.","Resource requirements can depend on the language.

Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

On the dimension of task-specific labels, different thresholds are used to define low-resource.

Given the lack of a hard threshold for low-resource settings, it can be seen as a spectrum of resource availability.","Questions:

How do resource requirements depend on the language?

Answer:

Resource requirements can depend on the language.

Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

On the dimension of task-specific labels, different thresholds are used to define low-resource.

Given the lack of a hard threshold for low-resource settings, it can be seen as a spectrum of resource availability."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,How Low is Low-Resource?,"On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","1. (What are the different thresholds used to define low-resource on the dimension of task-specific labels?): sent1
    1.1. (What thresholds are used for POS tagging?): sent2
    1.2. (What thresholds are used in other studies?): sent3
    1.3. (How does the threshold vary with task complexity?): sent4
        1.3.1. (What is an example of a threshold for a more complex task like text generation?): sent5
2. (How do resource requirements depend on the language?): sent6
    2.1. (How does task performance vary between language families with limited training data?): sent7
3. (What is the perspective on the lack of a hard threshold for low-resource settings?): sent8
    3.1. (What should be done to better compare approaches in low-resource settings?): sent9
        3.1.1. (What do studies show about the performance of non-neural vs. neural methods in very small datasets?): sent10",What is the perspective on the lack of a hard threshold for low-resource settings?,"sent8, sent9, sent10, sent1","Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability.

We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches.

For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.

On the dimension of task-specific labels, different thresholds are used to define low-resource.","Given the lack of a hard threshold for low-resource settings, it is seen as a spectrum of resource availability.

It is argued that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches.

For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets, non-neural methods outperform more modern approaches, while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.

On the dimension of task-specific labels, different thresholds are used to define low-resource.","Questions:

What is the perspective on the lack of a hard threshold for low-resource settings?

Answer:

Given the lack of a hard threshold for low-resource settings, it is seen as a spectrum of resource availability.

It is argued that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches.

For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets, non-neural methods outperform more modern approaches, while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.

On the dimension of task-specific labels, different thresholds are used to define low-resource."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,Data Augmentation,"New instances can be obtained based on existing ones by modifying the features with transformations that do not change the label. In the computer vision community, this is a popular approach where, e.g., rotating an image is invariant to the classification of an image's content. For text, on the token level, this can be done by replacing words with equivalents, such as synonyms (Wei and Zou, 2019), entities of the same type (Raiman and Miller, 2017;Dai and Adel, 2020) or words that share the same morphology (Gulordava et al., 2018;Vania et al., 2019). Such replacements can also be guided by a language model that takes context into consideration (Fadaee et al., 2017;Kobayashi, 2018).

To go beyond the token level and add more diversity to the augmented sentences, data augmentation can also be performed on sentence parts. Operations that (depending on the task) do not change the label include manipulation of parts of the dependency tree (Şahin and Steedman, 2018;Vania et al., 2019;Dehouck and Gómez-Rodríguez, 2020), simplification of sentences by removal of sentence parts (Şahin and Steedman, 2018) and inversion of the subject-object relation (Min et al., 2020). For whole sentences, paraphrasing through backtranslation can be used. This is a popular approach in machine translation where target sentences are back-translated into source sentences (Bojar and Tamchyna, 2011;Hoang et al., 2018). An important aspect here is that errors in the source side/features do not seem to have a large negative effect on the generated target text the model needs to predict. It is therefore also used in other text generation tasks like abstract summarization (Parida and Motlicek, 2019) and table-to-text generation (Ma et al., 2019). Back-translation has also been leveraged for text classification (Xie et al., 2020;Hegde and Patil, 2020). This setting assumes, however, the availability of a translation system. Instead, a language model can also be used for augmenting text classification datasets (Kumar et al., 2020;Anaby-Tavor et al., 2020). It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label. It then generates additional sentences that fit this label. Ding et al. (2020) extend this idea for token level tasks.

Adversarial methods are often used to find weaknesses in machine learning models (Jin et al., 2020;Garg and Ramakrishnan, 2020). They can, however, also be utilized to augment NLP datasets (Yasunaga et al., 2018;Morris et al., 2020). Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (according to a specific score). This approach is often applied on the level of vector representations. For instance, Grundkiewicz et al. (2019) reverse the augmentation setting by applying transformations that flip the (binary) label. In their case, they introduce errors in correct sentences to obtain new training data for a grammar correction task.

Open Issues: While data augmentation is ubiquitous in the computer vision community and while most of the above-presented approaches are taskindependent, it has not found such widespread use in natural language processing. A reason might be that several of the approaches require an indepth understanding of the language. There is not yet a unified framework that allows applying data augmentation across tasks and languages. Recently, Longpre et al. (2020) hypothesised that data augmentation provides the same benefits as pretraining in transformer models. However, we argue that data augmentation might be better suited to leverage the insights of linguistic or domain experts in low-resource settings when unlabeled data or hardware resources are limited.","1. (What is data augmentation in NLP?): sent1
    1.1. (How is data augmentation applied in computer vision?): sent2
    1.2. (How can data augmentation be applied at the token level in text?): sent3, sent4
    1.3. (How can data augmentation be applied beyond the token level?): sent5
        1.3.1. (What operations can be performed on sentence parts for data augmentation?): sent6
    1.4. (How can data augmentation be applied at the sentence level?): sent7
        1.4.1. (What is back-translation and how is it used in machine translation?): sent8
        1.4.2. (What is an important aspect of back-translation?): sent9
        1.4.3. (How is back-translation used in other text generation tasks?): sent10
        1.4.4. (How is back-translation used in text classification?): sent11
        1.4.5. (What is a limitation of using back-translation?): sent12
        1.4.6. (What is an alternative to back-translation for text classification?): sent13
            1.4.6.1. (How does a language model augment text classification datasets?): sent14, sent15
            1.4.6.2. (How is this idea extended for token level tasks?): sent16
    1.5. (How are adversarial methods used in data augmentation?): sent17, sent18
        1.5.1. (How do adversarial methods apply transformations?): sent19
        1.5.2. (At what level are adversarial methods often applied?): sent20
        1.5.3. (What is an example of reversing the augmentation setting?): sent21, sent22
2. (What are the open issues in data augmentation for NLP?): sent23
    2.1. (Why is data augmentation not widely used in NLP?): sent24
    2.2. (Is there a unified framework for data augmentation in NLP?): sent25
    2.3. (What is a recent hypothesis about data augmentation?): sent26
    2.4. (What is the argument for data augmentation in low-resource settings?): sent27",How does a language model augment text classification datasets?,"sent13, sent14, sent15","Instead, a language model can also be used for augmenting text classification datasets (Kumar et al., 2020;Anaby-Tavor et al., 2020).

It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label.

It then generates additional sentences that fit this label.","A language model can be used for augmenting text classification datasets (Kumar et al., 2020; Anaby-Tavor et al., 2020).

The model is trained conditioned on a label, i.e., on the subset of the task-specific data with this label.

It then generates additional sentences that fit this label.","Questions:

How does a language model augment text classification datasets?

Answer:

A language model can be used for augmenting text classification datasets (Kumar et al., 2020; Anaby-Tavor et al., 2020).

The model is trained conditioned on a label, i.e., on the subset of the task-specific data with this label.

It then generates additional sentences that fit this label."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,Data Augmentation,"New instances can be obtained based on existing ones by modifying the features with transformations that do not change the label. In the computer vision community, this is a popular approach where, e.g., rotating an image is invariant to the classification of an image's content. For text, on the token level, this can be done by replacing words with equivalents, such as synonyms (Wei and Zou, 2019), entities of the same type (Raiman and Miller, 2017;Dai and Adel, 2020) or words that share the same morphology (Gulordava et al., 2018;Vania et al., 2019). Such replacements can also be guided by a language model that takes context into consideration (Fadaee et al., 2017;Kobayashi, 2018).

To go beyond the token level and add more diversity to the augmented sentences, data augmentation can also be performed on sentence parts. Operations that (depending on the task) do not change the label include manipulation of parts of the dependency tree (Şahin and Steedman, 2018;Vania et al., 2019;Dehouck and Gómez-Rodríguez, 2020), simplification of sentences by removal of sentence parts (Şahin and Steedman, 2018) and inversion of the subject-object relation (Min et al., 2020). For whole sentences, paraphrasing through backtranslation can be used. This is a popular approach in machine translation where target sentences are back-translated into source sentences (Bojar and Tamchyna, 2011;Hoang et al., 2018). An important aspect here is that errors in the source side/features do not seem to have a large negative effect on the generated target text the model needs to predict. It is therefore also used in other text generation tasks like abstract summarization (Parida and Motlicek, 2019) and table-to-text generation (Ma et al., 2019). Back-translation has also been leveraged for text classification (Xie et al., 2020;Hegde and Patil, 2020). This setting assumes, however, the availability of a translation system. Instead, a language model can also be used for augmenting text classification datasets (Kumar et al., 2020;Anaby-Tavor et al., 2020). It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label. It then generates additional sentences that fit this label. Ding et al. (2020) extend this idea for token level tasks.

Adversarial methods are often used to find weaknesses in machine learning models (Jin et al., 2020;Garg and Ramakrishnan, 2020). They can, however, also be utilized to augment NLP datasets (Yasunaga et al., 2018;Morris et al., 2020). Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (according to a specific score). This approach is often applied on the level of vector representations. For instance, Grundkiewicz et al. (2019) reverse the augmentation setting by applying transformations that flip the (binary) label. In their case, they introduce errors in correct sentences to obtain new training data for a grammar correction task.

Open Issues: While data augmentation is ubiquitous in the computer vision community and while most of the above-presented approaches are taskindependent, it has not found such widespread use in natural language processing. A reason might be that several of the approaches require an indepth understanding of the language. There is not yet a unified framework that allows applying data augmentation across tasks and languages. Recently, Longpre et al. (2020) hypothesised that data augmentation provides the same benefits as pretraining in transformer models. However, we argue that data augmentation might be better suited to leverage the insights of linguistic or domain experts in low-resource settings when unlabeled data or hardware resources are limited.","1. (What is data augmentation in NLP?): sent1
    1.1. (How is data augmentation applied in computer vision?): sent2
    1.2. (How can data augmentation be applied at the token level in text?): sent3, sent4
    1.3. (How can data augmentation be applied beyond the token level?): sent5
        1.3.1. (What operations can be performed on sentence parts for data augmentation?): sent6
    1.4. (How can data augmentation be applied at the sentence level?): sent7
        1.4.1. (What is back-translation and how is it used in machine translation?): sent8
        1.4.2. (What is an important aspect of back-translation?): sent9
        1.4.3. (How is back-translation used in other text generation tasks?): sent10
        1.4.4. (How is back-translation used in text classification?): sent11
        1.4.5. (What is a limitation of using back-translation?): sent12
        1.4.6. (What is an alternative to back-translation for text classification?): sent13
            1.4.6.1. (How does a language model augment text classification datasets?): sent14, sent15
            1.4.6.2. (How is this idea extended for token level tasks?): sent16
    1.5. (How are adversarial methods used in data augmentation?): sent17, sent18
        1.5.1. (How do adversarial methods apply transformations?): sent19
        1.5.2. (At what level are adversarial methods often applied?): sent20
        1.5.3. (What is an example of reversing the augmentation setting?): sent21, sent22
2. (What are the open issues in data augmentation for NLP?): sent23
    2.1. (Why is data augmentation not widely used in NLP?): sent24
    2.2. (Is there a unified framework for data augmentation in NLP?): sent25
    2.3. (What is a recent hypothesis about data augmentation?): sent26
    2.4. (What is the argument for data augmentation in low-resource settings?): sent27",How are adversarial methods used in data augmentation?,"sent17, sent18, sent19, sent20","Adversarial methods are often used to find weaknesses in machine learning models (Jin et al., 2020;Garg and Ramakrishnan, 2020).

They can, however, also be utilized to augment NLP datasets (Yasunaga et al., 2018;Morris et al., 2020).

Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (according to a specific score).

This approach is often applied on the level of vector representations.","Adversarial methods are often used to find weaknesses in machine learning models (Jin et al., 2020; Garg and Ramakrishnan, 2020).

They can, however, also be utilized to augment NLP datasets (Yasunaga et al., 2018; Morris et al., 2020).

Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (according to a specific score).

This approach is often applied on the level of vector representations.","Questions:

How are adversarial methods used in data augmentation?

Answer:

Adversarial methods are often used to find weaknesses in machine learning models (Jin et al., 2020; Garg and Ramakrishnan, 2020).

They can, however, also be utilized to augment NLP datasets (Yasunaga et al., 2018; Morris et al., 2020).

Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (according to a specific score).

This approach is often applied on the level of vector representations."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,Distant & Weak Supervision,"In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","# QUA Analysis:

1. (What is distant or weak supervision and how does it differ from data augmentation?): sent1
    1.1. (How are the corresponding labels obtained in distant or weak supervision?): sent2
    1.2. (How is distant supervision applied in named entity recognition (NER)?): sent3
    1.3. (What is the historical context and extensions of distant supervision?): sent4
    1.4. (Why is distant supervision still popular for information extraction tasks like NER and RE?): sent5
    1.5. (What range of techniques are used for automatic annotation in distant supervision?): sent6
    1.6. (How is distant supervision related to labeling rules?): sent7
        1.6.1. (What other ideas are encompassed by labeling rules?): sent8
    1.7. (How prevalent is distant supervision in other areas of NLP?): sent9
        1.7.1. (What are some successful applications of distant supervision in other NLP tasks?): sent10, sent11, sent12, sent13, sent14, sent15

2. (What are the open issues and challenges with distant supervision?): sent16
    2.1. (Why might NER and RE be particularly suited for distant supervision?): sent17
    2.2. (Is it an open question whether a task needs specific properties to be suitable for distant supervision?): sent18
    2.3. (What does existing work suggest about the potential for distant supervision in other NLP tasks?): sent19
    2.4. (What is the reliance of distant supervision methods on auxiliary data?): sent20
        2.4.1. (What challenges arise in low-resource settings regarding auxiliary data?): sent21
        2.4.2. (What findings highlight the gap between high-resource and low-resource languages for POS tagging?): sent22
        2.4.3. (What does this gap emphasize about evaluating distant supervision methods?): sent23
    2.5. (What are the trade-offs of using distant supervision compared to manual annotation?): sent24
        2.5.1. (What could the time and effort spent on distant supervision also be used for?): sent25
        2.5.2. (What information is often missing in distant supervision papers?): sent26
        2.5.3. (How does involving human experts connect distant supervision with other research directions?): sent27",How are corresponding labels obtained in distant or weak supervision?,"sent2, sent3, sent5, sent6","The corresponding labels are obtained through a (semi-)automatic process from an external source of information.

For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations.

It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a).

The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019).","The corresponding labels are obtained through a (semi-)automatic process from an external source of information.

For named entity recognition (NER), a list of location names might be obtained from a dictionary, and matches of tokens in the text with entities in the list are automatically labeled as locations.

This approach remains popular for information extraction tasks like NER and RE, where external information can be sourced from knowledge bases, gazetteers, dictionaries, and other structured knowledge sources (Luo et al., 2017; Hedderich and Klakow, 2018; Deng and Sun, 2019; Alt et al., 2019; Ye et al., 2019; Lange et al., 2019a; Nooralahzadeh et al., 2019; Le and Titov, 2019; Cao et al., 2019; Lison et al., 2020; Hedderich et al., 2021a).

The automatic annotation ranges from simple string matching to complex pipelines including classifiers and manual steps (Norman et al., 2019).","Questions:

How are corresponding labels obtained in distant or weak supervision?

Answer:

The corresponding labels are obtained through a (semi-)automatic process from an external source of information.

For named entity recognition (NER), a list of location names might be obtained from a dictionary, and matches of tokens in the text with entities in the list are automatically labeled as locations.

This approach remains popular for information extraction tasks like NER and RE, where external information can be sourced from knowledge bases, gazetteers, dictionaries, and other structured knowledge sources (Luo et al., 2017; Hedderich and Klakow, 2018; Deng and Sun, 2019; Alt et al., 2019; Ye et al., 2019; Lange et al., 2019a; Nooralahzadeh et al., 2019; Le and Titov, 2019; Cao et al., 2019; Lison et al., 2020; Hedderich et al., 2021a).

The automatic annotation ranges from simple string matching to complex pipelines including classifiers and manual steps (Norman et al., 2019)."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,Distant & Weak Supervision,"In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","# QUA Analysis:

1. (What is distant or weak supervision and how does it differ from data augmentation?): sent1
    1.1. (How are the corresponding labels obtained in distant or weak supervision?): sent2
    1.2. (How is distant supervision applied in named entity recognition (NER)?): sent3
    1.3. (What is the historical context and extensions of distant supervision?): sent4
    1.4. (Why is distant supervision still popular for information extraction tasks like NER and RE?): sent5
    1.5. (What range of techniques are used for automatic annotation in distant supervision?): sent6
    1.6. (How is distant supervision related to labeling rules?): sent7
        1.6.1. (What other ideas are encompassed by labeling rules?): sent8
    1.7. (How prevalent is distant supervision in other areas of NLP?): sent9
        1.7.1. (What are some successful applications of distant supervision in other NLP tasks?): sent10, sent11, sent12, sent13, sent14, sent15

2. (What are the open issues and challenges with distant supervision?): sent16
    2.1. (Why might NER and RE be particularly suited for distant supervision?): sent17
    2.2. (Is it an open question whether a task needs specific properties to be suitable for distant supervision?): sent18
    2.3. (What does existing work suggest about the potential for distant supervision in other NLP tasks?): sent19
    2.4. (What is the reliance of distant supervision methods on auxiliary data?): sent20
        2.4.1. (What challenges arise in low-resource settings regarding auxiliary data?): sent21
        2.4.2. (What findings highlight the gap between high-resource and low-resource languages for POS tagging?): sent22
        2.4.3. (What does this gap emphasize about evaluating distant supervision methods?): sent23
    2.5. (What are the trade-offs of using distant supervision compared to manual annotation?): sent24
        2.5.1. (What could the time and effort spent on distant supervision also be used for?): sent25
        2.5.2. (What information is often missing in distant supervision papers?): sent26
        2.5.3. (How does involving human experts connect distant supervision with other research directions?): sent27",What is the historical context and extensions of distant supervision?,"sent4, sent5, sent7, sent8","Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012).

It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a).

This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.

These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).","Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012).

It remains a popular approach for information extraction tasks like NER and RE, where external information can be obtained from knowledge bases, gazetteers, dictionaries, and other forms of structured knowledge sources (Luo et al., 2017; Hedderich and Klakow, 2018; Deng and Sun, 2019; Alt et al., 2019; Ye et al., 2019; Lange et al., 2019a; Nooralahzadeh et al., 2019; Le and Titov, 2019; Cao et al., 2019; Lison et al., 2020; Hedderich et al., 2021a).

This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.

These encompass other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017; Zheng et al., 2019; Adelani et al., 2020; Hedderich et al., 2020; Lison et al., 2020; Ren et al., 2020; Karamanolakis et al., 2021).","Questions:

What is the historical context and extensions of distant supervision?

Answer:

Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012).

It remains a popular approach for information extraction tasks like NER and RE, where external information can be obtained from knowledge bases, gazetteers, dictionaries, and other forms of structured knowledge sources (Luo et al., 2017; Hedderich and Klakow, 2018; Deng and Sun, 2019; Alt et al., 2019; Ye et al., 2019; Lange et al., 2019a; Nooralahzadeh et al., 2019; Le and Titov, 2019; Cao et al., 2019; Lison et al., 2020; Hedderich et al., 2021a).

This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.

These encompass other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017; Zheng et al., 2019; Adelani et al., 2020; Hedderich et al., 2020; Lison et al., 2020; Ren et al., 2020; Karamanolakis et al., 2021)."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,Distant & Weak Supervision,"In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","# QUA Analysis:

1. (What is distant or weak supervision and how does it differ from data augmentation?): sent1
    1.1. (How are the corresponding labels obtained in distant or weak supervision?): sent2
    1.2. (How is distant supervision applied in named entity recognition (NER)?): sent3
    1.3. (What is the historical context and extensions of distant supervision?): sent4
    1.4. (Why is distant supervision still popular for information extraction tasks like NER and RE?): sent5
    1.5. (What range of techniques are used for automatic annotation in distant supervision?): sent6
    1.6. (How is distant supervision related to labeling rules?): sent7
        1.6.1. (What other ideas are encompassed by labeling rules?): sent8
    1.7. (How prevalent is distant supervision in other areas of NLP?): sent9
        1.7.1. (What are some successful applications of distant supervision in other NLP tasks?): sent10, sent11, sent12, sent13, sent14, sent15

2. (What are the open issues and challenges with distant supervision?): sent16
    2.1. (Why might NER and RE be particularly suited for distant supervision?): sent17
    2.2. (Is it an open question whether a task needs specific properties to be suitable for distant supervision?): sent18
    2.3. (What does existing work suggest about the potential for distant supervision in other NLP tasks?): sent19
    2.4. (What is the reliance of distant supervision methods on auxiliary data?): sent20
        2.4.1. (What challenges arise in low-resource settings regarding auxiliary data?): sent21
        2.4.2. (What findings highlight the gap between high-resource and low-resource languages for POS tagging?): sent22
        2.4.3. (What does this gap emphasize about evaluating distant supervision methods?): sent23
    2.5. (What are the trade-offs of using distant supervision compared to manual annotation?): sent24
        2.5.1. (What could the time and effort spent on distant supervision also be used for?): sent25
        2.5.2. (What information is often missing in distant supervision papers?): sent26
        2.5.3. (How does involving human experts connect distant supervision with other research directions?): sent27",Why is distant supervision still popular for information extraction tasks like NER and RE?,"sent5, sent6, sent7, sent8","It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a).

The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019).

This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.

These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).","Distant supervision remains popular for information extraction tasks like NER and RE because external information can be obtained from knowledge bases, gazetteers, dictionaries, and other forms of structured knowledge sources (Luo et al., 2017; Hedderich and Klakow, 2018; Deng and Sun, 2019; Alt et al., 2019; Ye et al., 2019; Lange et al., 2019a; Nooralahzadeh et al., 2019; Le and Titov, 2019; Cao et al., 2019; Lison et al., 2020; Hedderich et al., 2021a).

The automatic annotation ranges from simple string matching to complex pipelines including classifiers and manual steps (Norman et al., 2019).

This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.

These encompass other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017; Zheng et al., 2019; Adelani et al., 2020; Hedderich et al., 2020; Lison et al., 2020; Ren et al., 2020; Karamanolakis et al., 2021).","Questions:

Why is distant supervision still popular for information extraction tasks like NER and RE?

Answer:

Distant supervision remains popular for information extraction tasks like NER and RE because external information can be obtained from knowledge bases, gazetteers, dictionaries, and other forms of structured knowledge sources (Luo et al., 2017; Hedderich and Klakow, 2018; Deng and Sun, 2019; Alt et al., 2019; Ye et al., 2019; Lange et al., 2019a; Nooralahzadeh et al., 2019; Le and Titov, 2019; Cao et al., 2019; Lison et al., 2020; Hedderich et al., 2021a).

The automatic annotation ranges from simple string matching to complex pipelines including classifiers and manual steps (Norman et al., 2019).

This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.

These encompass other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017; Zheng et al., 2019; Adelani et al., 2020; Hedderich et al., 2020; Lison et al., 2020; Ren et al., 2020; Karamanolakis et al., 2021)."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,Distant & Weak Supervision,"In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","# QUA Analysis:

1. (What is distant or weak supervision and how does it differ from data augmentation?): sent1
    1.1. (How are the corresponding labels obtained in distant or weak supervision?): sent2
    1.2. (How is distant supervision applied in named entity recognition (NER)?): sent3
    1.3. (What is the historical context and extensions of distant supervision?): sent4
    1.4. (Why is distant supervision still popular for information extraction tasks like NER and RE?): sent5
    1.5. (What range of techniques are used for automatic annotation in distant supervision?): sent6
    1.6. (How is distant supervision related to labeling rules?): sent7
        1.6.1. (What other ideas are encompassed by labeling rules?): sent8
    1.7. (How prevalent is distant supervision in other areas of NLP?): sent9
        1.7.1. (What are some successful applications of distant supervision in other NLP tasks?): sent10, sent11, sent12, sent13, sent14, sent15

2. (What are the open issues and challenges with distant supervision?): sent16
    2.1. (Why might NER and RE be particularly suited for distant supervision?): sent17
    2.2. (Is it an open question whether a task needs specific properties to be suitable for distant supervision?): sent18
    2.3. (What does existing work suggest about the potential for distant supervision in other NLP tasks?): sent19
    2.4. (What is the reliance of distant supervision methods on auxiliary data?): sent20
        2.4.1. (What challenges arise in low-resource settings regarding auxiliary data?): sent21
        2.4.2. (What findings highlight the gap between high-resource and low-resource languages for POS tagging?): sent22
        2.4.3. (What does this gap emphasize about evaluating distant supervision methods?): sent23
    2.5. (What are the trade-offs of using distant supervision compared to manual annotation?): sent24
        2.5.1. (What could the time and effort spent on distant supervision also be used for?): sent25
        2.5.2. (What information is often missing in distant supervision papers?): sent26
        2.5.3. (How does involving human experts connect distant supervision with other research directions?): sent27",What range of techniques are used for automatic annotation in distant supervision?,"sent6, sent7, sent8, sent24","The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019).

This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.

These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.","The automatic annotation techniques range from simple string matching to complex pipelines including classifiers and manual steps (Norman et al., 2019).

This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.

These encompass other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017; Zheng et al., 2019; Adelani et al., 2020; Hedderich et al., 2020; Lison et al., 2020; Ren et al., 2020; Karamanolakis et al., 2021).

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.","Questions:

What range of techniques are used for automatic annotation in distant supervision?

Answer:

The automatic annotation techniques range from simple string matching to complex pipelines including classifiers and manual steps (Norman et al., 2019).

This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.

These encompass other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017; Zheng et al., 2019; Adelani et al., 2020; Hedderich et al., 2020; Lison et al., 2020; Ren et al., 2020; Karamanolakis et al., 2021).

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,Distant & Weak Supervision,"In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","# QUA Analysis:

1. (What is distant or weak supervision and how does it differ from data augmentation?): sent1
    1.1. (How are the corresponding labels obtained in distant or weak supervision?): sent2
    1.2. (How is distant supervision applied in named entity recognition (NER)?): sent3
    1.3. (What is the historical context and extensions of distant supervision?): sent4
    1.4. (Why is distant supervision still popular for information extraction tasks like NER and RE?): sent5
    1.5. (What range of techniques are used for automatic annotation in distant supervision?): sent6
    1.6. (How is distant supervision related to labeling rules?): sent7
        1.6.1. (What other ideas are encompassed by labeling rules?): sent8
    1.7. (How prevalent is distant supervision in other areas of NLP?): sent9
        1.7.1. (What are some successful applications of distant supervision in other NLP tasks?): sent10, sent11, sent12, sent13, sent14, sent15

2. (What are the open issues and challenges with distant supervision?): sent16
    2.1. (Why might NER and RE be particularly suited for distant supervision?): sent17
    2.2. (Is it an open question whether a task needs specific properties to be suitable for distant supervision?): sent18
    2.3. (What does existing work suggest about the potential for distant supervision in other NLP tasks?): sent19
    2.4. (What is the reliance of distant supervision methods on auxiliary data?): sent20
        2.4.1. (What challenges arise in low-resource settings regarding auxiliary data?): sent21
        2.4.2. (What findings highlight the gap between high-resource and low-resource languages for POS tagging?): sent22
        2.4.3. (What does this gap emphasize about evaluating distant supervision methods?): sent23
    2.5. (What are the trade-offs of using distant supervision compared to manual annotation?): sent24
        2.5.1. (What could the time and effort spent on distant supervision also be used for?): sent25
        2.5.2. (What information is often missing in distant supervision papers?): sent26
        2.5.3. (How does involving human experts connect distant supervision with other research directions?): sent27",How is distant supervision related to labeling rules?,"sent7, sent8, sent24, sent27","This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.

These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.

Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","Distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.

These encompass other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017; Zheng et al., 2019; Adelani et al., 2020; Hedderich et al., 2020; Lison et al., 2020; Ren et al., 2020; Karamanolakis et al., 2021).

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.

Taking the human expert into the focus connects this research direction with human-computer interaction and human-in-the-loop setups (Klie et al., 2018; Qian et al., 2020).","Questions:

How is distant supervision related to labeling rules?

Answer:

Distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.

These encompass other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017; Zheng et al., 2019; Adelani et al., 2020; Hedderich et al., 2020; Lison et al., 2020; Ren et al., 2020; Karamanolakis et al., 2021).

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.

Taking the human expert into the focus connects this research direction with human-computer interaction and human-in-the-loop setups (Klie et al., 2018; Qian et al., 2020)."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,Distant & Weak Supervision,"In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","# QUA Analysis:

1. (What is distant or weak supervision and how does it differ from data augmentation?): sent1
    1.1. (How are the corresponding labels obtained in distant or weak supervision?): sent2
    1.2. (How is distant supervision applied in named entity recognition (NER)?): sent3
    1.3. (What is the historical context and extensions of distant supervision?): sent4
    1.4. (Why is distant supervision still popular for information extraction tasks like NER and RE?): sent5
    1.5. (What range of techniques are used for automatic annotation in distant supervision?): sent6
    1.6. (How is distant supervision related to labeling rules?): sent7
        1.6.1. (What other ideas are encompassed by labeling rules?): sent8
    1.7. (How prevalent is distant supervision in other areas of NLP?): sent9
        1.7.1. (What are some successful applications of distant supervision in other NLP tasks?): sent10, sent11, sent12, sent13, sent14, sent15

2. (What are the open issues and challenges with distant supervision?): sent16
    2.1. (Why might NER and RE be particularly suited for distant supervision?): sent17
    2.2. (Is it an open question whether a task needs specific properties to be suitable for distant supervision?): sent18
    2.3. (What does existing work suggest about the potential for distant supervision in other NLP tasks?): sent19
    2.4. (What is the reliance of distant supervision methods on auxiliary data?): sent20
        2.4.1. (What challenges arise in low-resource settings regarding auxiliary data?): sent21
        2.4.2. (What findings highlight the gap between high-resource and low-resource languages for POS tagging?): sent22
        2.4.3. (What does this gap emphasize about evaluating distant supervision methods?): sent23
    2.5. (What are the trade-offs of using distant supervision compared to manual annotation?): sent24
        2.5.1. (What could the time and effort spent on distant supervision also be used for?): sent25
        2.5.2. (What information is often missing in distant supervision papers?): sent26
        2.5.3. (How does involving human experts connect distant supervision with other research directions?): sent27",What are some successful applications of distant supervision in other NLP tasks?,"sent10, sent11, sent12, sent13, sent14, sent15","Nevertheless, distant supervision has also been successfully em-

(2020) build a discourse-structure dataset using guidance from sentiment annotations.

For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020).

For some classification tasks, the labels can be rephrased with simple rules into sentences.

A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;).

An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.","Distant supervision has also been successfully employed in other NLP tasks.

For example, (2020) build a discourse-structure dataset using guidance from sentiment annotations.

For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020).

For some classification tasks, the labels can be rephrased with simple rules into sentences.

A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019).

An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.","Questions:

What are some successful applications of distant supervision in other NLP tasks?

Answer:

Distant supervision has also been successfully employed in other NLP tasks.

For example, (2020) build a discourse-structure dataset using guidance from sentiment annotations.

For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020).

For some classification tasks, the labels can be rephrased with simple rules into sentences.

A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019).

An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,Distant & Weak Supervision,"In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","# QUA Analysis:

1. (What is distant or weak supervision and how does it differ from data augmentation?): sent1
    1.1. (How are the corresponding labels obtained in distant or weak supervision?): sent2
    1.2. (How is distant supervision applied in named entity recognition (NER)?): sent3
    1.3. (What is the historical context and extensions of distant supervision?): sent4
    1.4. (Why is distant supervision still popular for information extraction tasks like NER and RE?): sent5
    1.5. (What range of techniques are used for automatic annotation in distant supervision?): sent6
    1.6. (How is distant supervision related to labeling rules?): sent7
        1.6.1. (What other ideas are encompassed by labeling rules?): sent8
    1.7. (How prevalent is distant supervision in other areas of NLP?): sent9
        1.7.1. (What are some successful applications of distant supervision in other NLP tasks?): sent10, sent11, sent12, sent13, sent14, sent15

2. (What are the open issues and challenges with distant supervision?): sent16
    2.1. (Why might NER and RE be particularly suited for distant supervision?): sent17
    2.2. (Is it an open question whether a task needs specific properties to be suitable for distant supervision?): sent18
    2.3. (What does existing work suggest about the potential for distant supervision in other NLP tasks?): sent19
    2.4. (What is the reliance of distant supervision methods on auxiliary data?): sent20
        2.4.1. (What challenges arise in low-resource settings regarding auxiliary data?): sent21
        2.4.2. (What findings highlight the gap between high-resource and low-resource languages for POS tagging?): sent22
        2.4.3. (What does this gap emphasize about evaluating distant supervision methods?): sent23
    2.5. (What are the trade-offs of using distant supervision compared to manual annotation?): sent24
        2.5.1. (What could the time and effort spent on distant supervision also be used for?): sent25
        2.5.2. (What information is often missing in distant supervision papers?): sent26
        2.5.3. (How does involving human experts connect distant supervision with other research directions?): sent27",Why might NER and RE be particularly suited for distant supervision?,"sent16, sent17, sent20, sent21","Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited.

There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules.

Distant supervision methods heavily rely on auxiliary data.

In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.","The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited.

Auxiliary data like entity lists is readily available, and distant supervision often achieves reasonable results with simple surface form rules.

Distant supervision methods heavily rely on auxiliary data.

In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.","Questions:

Why might NER and RE be particularly suited for distant supervision?

Answer:

The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited.

Auxiliary data like entity lists is readily available, and distant supervision often achieves reasonable results with simple surface form rules.

Distant supervision methods heavily rely on auxiliary data.

In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,Distant & Weak Supervision,"In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","# QUA Analysis:

1. (What is distant or weak supervision and how does it differ from data augmentation?): sent1
    1.1. (How are the corresponding labels obtained in distant or weak supervision?): sent2
    1.2. (How is distant supervision applied in named entity recognition (NER)?): sent3
    1.3. (What is the historical context and extensions of distant supervision?): sent4
    1.4. (Why is distant supervision still popular for information extraction tasks like NER and RE?): sent5
    1.5. (What range of techniques are used for automatic annotation in distant supervision?): sent6
    1.6. (How is distant supervision related to labeling rules?): sent7
        1.6.1. (What other ideas are encompassed by labeling rules?): sent8
    1.7. (How prevalent is distant supervision in other areas of NLP?): sent9
        1.7.1. (What are some successful applications of distant supervision in other NLP tasks?): sent10, sent11, sent12, sent13, sent14, sent15

2. (What are the open issues and challenges with distant supervision?): sent16
    2.1. (Why might NER and RE be particularly suited for distant supervision?): sent17
    2.2. (Is it an open question whether a task needs specific properties to be suitable for distant supervision?): sent18
    2.3. (What does existing work suggest about the potential for distant supervision in other NLP tasks?): sent19
    2.4. (What is the reliance of distant supervision methods on auxiliary data?): sent20
        2.4.1. (What challenges arise in low-resource settings regarding auxiliary data?): sent21
        2.4.2. (What findings highlight the gap between high-resource and low-resource languages for POS tagging?): sent22
        2.4.3. (What does this gap emphasize about evaluating distant supervision methods?): sent23
    2.5. (What are the trade-offs of using distant supervision compared to manual annotation?): sent24
        2.5.1. (What could the time and effort spent on distant supervision also be used for?): sent25
        2.5.2. (What information is often missing in distant supervision papers?): sent26
        2.5.3. (How does involving human experts connect distant supervision with other research directions?): sent27",What challenges arise in low-resource settings regarding auxiliary data?,"sent21, sent22, sent23, sent24","In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.

Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages.

This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.","In a low-resource setting, it might be difficult to obtain not only labeled data but also auxiliary data.

Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging, pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages.

This emphasizes the need for evaluating such methods in a realistic setting and avoiding the simulation of restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.","Questions:

What challenges arise in low-resource settings regarding auxiliary data?

Answer:

In a low-resource setting, it might be difficult to obtain not only labeled data but also auxiliary data.

Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging, pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages.

This emphasizes the need for evaluating such methods in a realistic setting and avoiding the simulation of restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,Distant & Weak Supervision,"In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","# QUA Analysis:

1. (What is distant or weak supervision and how does it differ from data augmentation?): sent1
    1.1. (How are the corresponding labels obtained in distant or weak supervision?): sent2
    1.2. (How is distant supervision applied in named entity recognition (NER)?): sent3
    1.3. (What is the historical context and extensions of distant supervision?): sent4
    1.4. (Why is distant supervision still popular for information extraction tasks like NER and RE?): sent5
    1.5. (What range of techniques are used for automatic annotation in distant supervision?): sent6
    1.6. (How is distant supervision related to labeling rules?): sent7
        1.6.1. (What other ideas are encompassed by labeling rules?): sent8
    1.7. (How prevalent is distant supervision in other areas of NLP?): sent9
        1.7.1. (What are some successful applications of distant supervision in other NLP tasks?): sent10, sent11, sent12, sent13, sent14, sent15

2. (What are the open issues and challenges with distant supervision?): sent16
    2.1. (Why might NER and RE be particularly suited for distant supervision?): sent17
    2.2. (Is it an open question whether a task needs specific properties to be suitable for distant supervision?): sent18
    2.3. (What does existing work suggest about the potential for distant supervision in other NLP tasks?): sent19
    2.4. (What is the reliance of distant supervision methods on auxiliary data?): sent20
        2.4.1. (What challenges arise in low-resource settings regarding auxiliary data?): sent21
        2.4.2. (What findings highlight the gap between high-resource and low-resource languages for POS tagging?): sent22
        2.4.3. (What does this gap emphasize about evaluating distant supervision methods?): sent23
    2.5. (What are the trade-offs of using distant supervision compared to manual annotation?): sent24
        2.5.1. (What could the time and effort spent on distant supervision also be used for?): sent25
        2.5.2. (What information is often missing in distant supervision papers?): sent26
        2.5.3. (How does involving human experts connect distant supervision with other research directions?): sent27",What are the trade-offs of using distant supervision compared to manual annotation?,"sent24, sent25, sent26, sent27","While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.

This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme.

Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches.

Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","Distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, but it still requires human interaction to create automatic annotation techniques or to provide labeling rules.

This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme.

Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches.

Taking the human expert into account connects this research direction with human-computer interaction and human-in-the-loop setups (Klie et al., 2018; Qian et al., 2020).","Questions:

What are the trade-offs of using distant supervision compared to manual annotation?

Answer:

Distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, but it still requires human interaction to create automatic annotation techniques or to provide labeling rules.

This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme.

Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches.

Taking the human expert into account connects this research direction with human-computer interaction and human-in-the-loop setups (Klie et al., 2018; Qian et al., 2020)."
A Survey on Stance Detection for Mis-and Disinformation Identification,Source(s) Target,"Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

2 What is Stance?

In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","1. What are the key characteristics of stance detection datasets for mis-and disinformation detection? (sent1)
    1.1. What are the characteristics of the PHEME dataset? (sent2)
    1.2. What are the characteristics of the Emergent dataset? (sent3)
    1.3. What are the characteristics of the FNC-1 and RumourEval datasets? (sent4)
    1.4. What are the characteristics of the FEVER dataset? (sent5)
2. What does #Instances denote? (sent6)
3. What does the asterisk (*) signify in the context of the article's body? (sent7)
4. What are the sources of the datasets? (sent8)
5. What types of evidence are used in the datasets? (sent9)
6. What is stance detection? (sent10)
    6.1. How do Biber and Finegan (1988) define stance? (sent11)
    6.2. How does Du Bois (2007) define stance? (sent12)
    6.3. What is the general definition of stance detection adopted by Küçük and Can (2020)? (sent13)
    6.4. What additional category label is occasionally added to the set of stance categories? (sent14)
    6.5. How do stance detection definitions and label inventories vary? (sent15)
    6.6. How can stance detection be distinguished from other closely related NLP tasks? (sent16)",What are the key characteristics of stance detection datasets for mis-and disinformation detection?,"sent1, sent2, sent3, sent4, sent5, sent6, sent7, sent8, sent9","Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011)

Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b)

Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article *

2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '

17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection.

#Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds.

* the article's body is summarised.

Sources: Twitter, ǌ News, ɀikipedia, Reddit.

Evidence: Single, Multiple, Thread.","Stance detection datasets for mis-and disinformation detection often include context evidence and a varying number of instances, as seen in datasets like Rumour Has It (Qazvinian et al., 2011).

Another example is the PHEME dataset (Zubiaga et al., 2016b), which includes 10,000 topic tweets related to rumors.

Another notable dataset is Emergent (Ferreira and Vlachos, 2016), which includes 4,500 claim tweets related to rumors.

Another example is the FNC-1 dataset (Pomerleau and Rao, 2017), which includes 2.6K rumors and 75K fake news articles.

Another example is the dataset by Derczynski et al. (2017), which includes 7.1K rumors.

The instances denote the dataset size as a whole, with the numbers in thousands (K) and rounded to the hundreds.

The article's body is summarized.

Sources for these datasets include Twitter, news articles, Wikipedia, and Reddit.

Evidence in these datasets can be categorized as single, multiple, or thread.","Questions:

What are the key characteristics of stance detection datasets for mis-and disinformation detection?

Answer:

Stance detection datasets for mis-and disinformation detection often include context evidence and a varying number of instances, as seen in datasets like Rumour Has It (Qazvinian et al., 2011).

Another example is the PHEME dataset (Zubiaga et al., 2016b), which includes 10,000 topic tweets related to rumors.

Another notable dataset is Emergent (Ferreira and Vlachos, 2016), which includes 4,500 claim tweets related to rumors.

Another example is the FNC-1 dataset (Pomerleau and Rao, 2017), which includes 2.6K rumors and 75K fake news articles.

Another example is the dataset by Derczynski et al. (2017), which includes 7.1K rumors.

The instances denote the dataset size as a whole, with the numbers in thousands (K) and rounded to the hundreds.

The article's body is summarized.

Sources for these datasets include Twitter, news articles, Wikipedia, and Reddit.

Evidence in these datasets can be categorized as single, multiple, or thread."
A Survey on Stance Detection for Mis-and Disinformation Identification,Multiple languages,"In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.

Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","1. What is the focus of this section? 
    - sent1

2. What is the Fake News Challenge and its evaluation measure?
    - sent2
    2.1. What was the criticism of the FNC score and F1-micro?
        - sent3

3. What features did most teams use in the competition?
    - sent4
    3.1. What did Hanselowski et al. (2018a) show about the importance of features?
        - sent5
    3.2. What did Ghanem et al. (2018) find about lexical cues?
        - sent6

4. What is the performance of these models on different classes?
    - sent7
    4.1. How well is the Unrelated class detected?
        - sent8

5. What are the limitations of these models according to Hanselowski et al. (2018a)?
    - sent9
    5.1. What can be attributed to these limitations?
        - sent10

6. What did Mohtarami et al. (2018) investigate?
    - sent11
    6.1. Why does their model perform poorly on the Agree/Disagree classes?
        - sent12
        6.1.1. What is the reason for the poor performance?
            - sent13

7. What recent advancements have been made in stance detection?
    - sent14
    7.1. What specific improvement did using the pre-trained Transformer RoBERTa bring?
        - sent15

8. How successful are these models in cross-lingual settings?
    - sent16
    8.1. What are the results for Arabic stance detection using mBERT?
        - sent17
    8.2. What did Hardalov et al. (2022) achieve with pattern-exploiting training (PET)?
        - sent18
    8.3. What did Alhindi et al. (2021) show about language-specific pre-training?
        - sent19

9. What are some formulations that include an extra step for evidence retrieval?
    - sent20
    9.1. What is the FEVER score and how is it used?
        - sent21

10. What are the top systems and recent approaches in the FEVER competition?
    - sent22
    10.1. What did Zhou et al. (2019) show about adding graph networks on top of BERT?
        - sent23
    10.2. Why is the retrieval model important?
        - sent24
    10.3. What improvements did Liu et al. (2020b) and Zhong et al. (2020) achieve?
        - sent25
    10.4. What did Ye et al. (2020) experiment with?
        - sent26
    10.5. How did Si et al. (2021) and Ostrowski et al. (2021) improve multi-hop reasoning?
        - sent27
    10.6. What did Atanasova et al. (2022) introduce?
        - sent28

11. What are some notable ideas for using pre-trained language models as fact-checkers?
    - sent29
    11.1. What is the advantage of these models?
        - sent30
    11.2. What are the biases and limitations of these models?
        - sent31
        11.2.1. What is an example of insufficient context confusing the models?
            - sent32
        11.2.2. How does the performance of these models compare to supervised approaches?
            - sent33

12. What are the main challenges identified in error analysis?
    - sent34
    12.1. What are the specific challenges mentioned?
        - sent35
        - sent36
        - sent37
        - sent38

13. What is the significance of threaded stance in conversational threads?
    - sent39
    13.1. How are these approaches applied?
        - sent40
    13.2. What is a common pattern in these approaches?
        - sent41
    13.3. What did Kumar and Carley (2019) replace and use in their approach?
        - sent42
    13.4. What recent models were proposed for rumour debunking?
        - sent43
    13.5. How did Kochkina et al. (2017, 2018) model conversations?
        - sent44
    13.6. How did Li et al. (2020) deviate from the common structure?
        - sent45
    13.7. What did Tian et al. (2020) show about pre-training on stance data?
        - sent46
    13.8. What did Yang et al. (2019) achieve with per-class pre-training data?
        - sent47
    13.9. What additional information did Li et al. (2019a,b) incorporate?
        - sent48
    13.10. Why must preceding posts be taken into account?
        - sent49

14. What are the challenges for rumour detection datasets?
    - sent50
    14.1. What role does label semantics play?
        - sent51
    14.2. How does thread depth affect performance?
        - sent52
    14.3. What is the positive side of using multitask learning?
        - sent53
    14.4. What is the significance of the temporal dimension of posts?
        - sent54

15. What is Multi-Dataset Learning (MDL)?
    - sent55
    15.1. What challenges arise from combining mis-and disinformation identification with stance detection?
        - sent56
    15.2. What did earlier approaches focus on?
        - sent57
    15.3. What did Schiller et al. (2021) propose?
        - sent58
    15.4. What improvements did they show with multitask learning?
        - sent59
    15.5. What did Hardalov et al. (2021) achieve in a cross-lingual setting?
        - sent60
    15.6. How does MDL help in different scenarios?
        - sent61
    15.7. How can transferring knowledge from English stance datasets boost performance?
        - sent62

16. What does Table 2 show?
    - sent63
    16.1. How do the datasets vary?
        - sent64
    16.2. How do these factors impact performance?
        - sent65",How do models perform on the Disagree class in stance detection?,"sent7, sent12, sent15","All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance.

Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t.

In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree.","All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance.

Like previous work, their model performs poorly on the Agree/Disagree classes due to the unsupervised way of training the memory networks, as there are no gold snippets justifying the document's stance.

In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree.","Questions:

How do models perform on the Disagree class in stance detection?

Answer:

All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance.

Like previous work, their model performs poorly on the Agree/Disagree classes due to the unsupervised way of training the memory networks, as there are no gold snippets justifying the document's stance.

In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree."
A Survey on Stance Detection for Mis-and Disinformation Identification,Multiple languages,"In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.

Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","1. What is the focus of this section? 
    - sent1

2. What is the Fake News Challenge and its evaluation measure?
    - sent2
    2.1. What was the criticism of the FNC score and F1-micro?
        - sent3

3. What features did most teams use in the competition?
    - sent4
    3.1. What did Hanselowski et al. (2018a) show about the importance of features?
        - sent5
    3.2. What did Ghanem et al. (2018) find about lexical cues?
        - sent6

4. What is the performance of these models on different classes?
    - sent7
    4.1. How well is the Unrelated class detected?
        - sent8

5. What are the limitations of these models according to Hanselowski et al. (2018a)?
    - sent9
    5.1. What can be attributed to these limitations?
        - sent10

6. What did Mohtarami et al. (2018) investigate?
    - sent11
    6.1. Why does their model perform poorly on the Agree/Disagree classes?
        - sent12
        6.1.1. What is the reason for the poor performance?
            - sent13

7. What recent advancements have been made in stance detection?
    - sent14
    7.1. What specific improvement did using the pre-trained Transformer RoBERTa bring?
        - sent15

8. How successful are these models in cross-lingual settings?
    - sent16
    8.1. What are the results for Arabic stance detection using mBERT?
        - sent17
    8.2. What did Hardalov et al. (2022) achieve with pattern-exploiting training (PET)?
        - sent18
    8.3. What did Alhindi et al. (2021) show about language-specific pre-training?
        - sent19

9. What are some formulations that include an extra step for evidence retrieval?
    - sent20
    9.1. What is the FEVER score and how is it used?
        - sent21

10. What are the top systems and recent approaches in the FEVER competition?
    - sent22
    10.1. What did Zhou et al. (2019) show about adding graph networks on top of BERT?
        - sent23
    10.2. Why is the retrieval model important?
        - sent24
    10.3. What improvements did Liu et al. (2020b) and Zhong et al. (2020) achieve?
        - sent25
    10.4. What did Ye et al. (2020) experiment with?
        - sent26
    10.5. How did Si et al. (2021) and Ostrowski et al. (2021) improve multi-hop reasoning?
        - sent27
    10.6. What did Atanasova et al. (2022) introduce?
        - sent28

11. What are some notable ideas for using pre-trained language models as fact-checkers?
    - sent29
    11.1. What is the advantage of these models?
        - sent30
    11.2. What are the biases and limitations of these models?
        - sent31
        11.2.1. What is an example of insufficient context confusing the models?
            - sent32
        11.2.2. How does the performance of these models compare to supervised approaches?
            - sent33

12. What are the main challenges identified in error analysis?
    - sent34
    12.1. What are the specific challenges mentioned?
        - sent35
        - sent36
        - sent37
        - sent38

13. What is the significance of threaded stance in conversational threads?
    - sent39
    13.1. How are these approaches applied?
        - sent40
    13.2. What is a common pattern in these approaches?
        - sent41
    13.3. What did Kumar and Carley (2019) replace and use in their approach?
        - sent42
    13.4. What recent models were proposed for rumour debunking?
        - sent43
    13.5. How did Kochkina et al. (2017, 2018) model conversations?
        - sent44
    13.6. How did Li et al. (2020) deviate from the common structure?
        - sent45
    13.7. What did Tian et al. (2020) show about pre-training on stance data?
        - sent46
    13.8. What did Yang et al. (2019) achieve with per-class pre-training data?
        - sent47
    13.9. What additional information did Li et al. (2019a,b) incorporate?
        - sent48
    13.10. Why must preceding posts be taken into account?
        - sent49

14. What are the challenges for rumour detection datasets?
    - sent50
    14.1. What role does label semantics play?
        - sent51
    14.2. How does thread depth affect performance?
        - sent52
    14.3. What is the positive side of using multitask learning?
        - sent53
    14.4. What is the significance of the temporal dimension of posts?
        - sent54

15. What is Multi-Dataset Learning (MDL)?
    - sent55
    15.1. What challenges arise from combining mis-and disinformation identification with stance detection?
        - sent56
    15.2. What did earlier approaches focus on?
        - sent57
    15.3. What did Schiller et al. (2021) propose?
        - sent58
    15.4. What improvements did they show with multitask learning?
        - sent59
    15.5. What did Hardalov et al. (2021) achieve in a cross-lingual setting?
        - sent60
    15.6. How does MDL help in different scenarios?
        - sent61
    15.7. How can transferring knowledge from English stance datasets boost performance?
        - sent62

16. What does Table 2 show?
    - sent63
    16.1. How do the datasets vary?
        - sent64
    16.2. How do these factors impact performance?
        - sent65",What advancements have been made in stance detection using pre-trained Transformers?,"sent14, sent15, sent16, sent17, sent18, sent19","Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help.

In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree.

The success of these models is also seen in cross-lingual settings.

For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT.

Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets.

Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).","Guderlei and Aßenmacher (2020) showed that the most important hyper-parameter in stance detection using pre-trained Transformers is the learning rate, while freezing layers did not help.

In particular, using the pre-trained Transformer RoBERTa improved F1 scores from 18 to 58 for Disagree, and from 50 to 70 for Agree.

The success of these models is also seen in cross-lingual settings.

For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT.

Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting, showing sizeable improvements on 15 datasets.

Alhindi et al. (2021) demonstrated that language-specific pre-training was crucial, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).","Questions:

What advancements have been made in stance detection using pre-trained Transformers?

Answer:

Guderlei and Aßenmacher (2020) showed that the most important hyper-parameter in stance detection using pre-trained Transformers is the learning rate, while freezing layers did not help.

In particular, using the pre-trained Transformer RoBERTa improved F1 scores from 18 to 58 for Disagree, and from 50 to 70 for Agree.

The success of these models is also seen in cross-lingual settings.

For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT.

Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting, showing sizeable improvements on 15 datasets.

Alhindi et al. (2021) demonstrated that language-specific pre-training was crucial, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1)."
A Survey on Stance Detection for Mis-and Disinformation Identification,Multiple languages,"In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.

Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","1. What is the focus of this section? 
    - sent1

2. What is the Fake News Challenge and its evaluation measure?
    - sent2
    2.1. What was the criticism of the FNC score and F1-micro?
        - sent3

3. What features did most teams use in the competition?
    - sent4
    3.1. What did Hanselowski et al. (2018a) show about the importance of features?
        - sent5
    3.2. What did Ghanem et al. (2018) find about lexical cues?
        - sent6

4. What is the performance of these models on different classes?
    - sent7
    4.1. How well is the Unrelated class detected?
        - sent8

5. What are the limitations of these models according to Hanselowski et al. (2018a)?
    - sent9
    5.1. What can be attributed to these limitations?
        - sent10

6. What did Mohtarami et al. (2018) investigate?
    - sent11
    6.1. Why does their model perform poorly on the Agree/Disagree classes?
        - sent12
        6.1.1. What is the reason for the poor performance?
            - sent13

7. What recent advancements have been made in stance detection?
    - sent14
    7.1. What specific improvement did using the pre-trained Transformer RoBERTa bring?
        - sent15

8. How successful are these models in cross-lingual settings?
    - sent16
    8.1. What are the results for Arabic stance detection using mBERT?
        - sent17
    8.2. What did Hardalov et al. (2022) achieve with pattern-exploiting training (PET)?
        - sent18
    8.3. What did Alhindi et al. (2021) show about language-specific pre-training?
        - sent19

9. What are some formulations that include an extra step for evidence retrieval?
    - sent20
    9.1. What is the FEVER score and how is it used?
        - sent21

10. What are the top systems and recent approaches in the FEVER competition?
    - sent22
    10.1. What did Zhou et al. (2019) show about adding graph networks on top of BERT?
        - sent23
    10.2. Why is the retrieval model important?
        - sent24
    10.3. What improvements did Liu et al. (2020b) and Zhong et al. (2020) achieve?
        - sent25
    10.4. What did Ye et al. (2020) experiment with?
        - sent26
    10.5. How did Si et al. (2021) and Ostrowski et al. (2021) improve multi-hop reasoning?
        - sent27
    10.6. What did Atanasova et al. (2022) introduce?
        - sent28

11. What are some notable ideas for using pre-trained language models as fact-checkers?
    - sent29
    11.1. What is the advantage of these models?
        - sent30
    11.2. What are the biases and limitations of these models?
        - sent31
        11.2.1. What is an example of insufficient context confusing the models?
            - sent32
        11.2.2. How does the performance of these models compare to supervised approaches?
            - sent33

12. What are the main challenges identified in error analysis?
    - sent34
    12.1. What are the specific challenges mentioned?
        - sent35
        - sent36
        - sent37
        - sent38

13. What is the significance of threaded stance in conversational threads?
    - sent39
    13.1. How are these approaches applied?
        - sent40
    13.2. What is a common pattern in these approaches?
        - sent41
    13.3. What did Kumar and Carley (2019) replace and use in their approach?
        - sent42
    13.4. What recent models were proposed for rumour debunking?
        - sent43
    13.5. How did Kochkina et al. (2017, 2018) model conversations?
        - sent44
    13.6. How did Li et al. (2020) deviate from the common structure?
        - sent45
    13.7. What did Tian et al. (2020) show about pre-training on stance data?
        - sent46
    13.8. What did Yang et al. (2019) achieve with per-class pre-training data?
        - sent47
    13.9. What additional information did Li et al. (2019a,b) incorporate?
        - sent48
    13.10. Why must preceding posts be taken into account?
        - sent49

14. What are the challenges for rumour detection datasets?
    - sent50
    14.1. What role does label semantics play?
        - sent51
    14.2. How does thread depth affect performance?
        - sent52
    14.3. What is the positive side of using multitask learning?
        - sent53
    14.4. What is the significance of the temporal dimension of posts?
        - sent54

15. What is Multi-Dataset Learning (MDL)?
    - sent55
    15.1. What challenges arise from combining mis-and disinformation identification with stance detection?
        - sent56
    15.2. What did earlier approaches focus on?
        - sent57
    15.3. What did Schiller et al. (2021) propose?
        - sent58
    15.4. What improvements did they show with multitask learning?
        - sent59
    15.5. What did Hardalov et al. (2021) achieve in a cross-lingual setting?
        - sent60
    15.6. How does MDL help in different scenarios?
        - sent61
    15.7. How can transferring knowledge from English stance datasets boost performance?
        - sent62

16. What does Table 2 show?
    - sent63
    16.1. How do the datasets vary?
        - sent64
    16.2. How do these factors impact performance?
        - sent65",How successful are stance detection models in cross-lingual settings?,"sent16, sent17, sent18, sent19","The success of these models is also seen in cross-lingual settings.

For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT.

Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets.

Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).","The success of stance detection models is also observed in cross-lingual settings.

For example, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT for Arabic.

Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting, showing sizeable improvements on 15 datasets.

Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).","Questions:

How successful are stance detection models in cross-lingual settings?

Answer:

The success of stance detection models is also observed in cross-lingual settings.

For example, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT for Arabic.

Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting, showing sizeable improvements on 15 datasets.

Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1)."
A Survey on Stance Detection for Mis-and Disinformation Identification,Multiple languages,"In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.

Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","1. What is the focus of this section? 
    - sent1

2. What is the Fake News Challenge and its evaluation measure?
    - sent2
    2.1. What was the criticism of the FNC score and F1-micro?
        - sent3

3. What features did most teams use in the competition?
    - sent4
    3.1. What did Hanselowski et al. (2018a) show about the importance of features?
        - sent5
    3.2. What did Ghanem et al. (2018) find about lexical cues?
        - sent6

4. What is the performance of these models on different classes?
    - sent7
    4.1. How well is the Unrelated class detected?
        - sent8

5. What are the limitations of these models according to Hanselowski et al. (2018a)?
    - sent9
    5.1. What can be attributed to these limitations?
        - sent10

6. What did Mohtarami et al. (2018) investigate?
    - sent11
    6.1. Why does their model perform poorly on the Agree/Disagree classes?
        - sent12
        6.1.1. What is the reason for the poor performance?
            - sent13

7. What recent advancements have been made in stance detection?
    - sent14
    7.1. What specific improvement did using the pre-trained Transformer RoBERTa bring?
        - sent15

8. How successful are these models in cross-lingual settings?
    - sent16
    8.1. What are the results for Arabic stance detection using mBERT?
        - sent17
    8.2. What did Hardalov et al. (2022) achieve with pattern-exploiting training (PET)?
        - sent18
    8.3. What did Alhindi et al. (2021) show about language-specific pre-training?
        - sent19

9. What are some formulations that include an extra step for evidence retrieval?
    - sent20
    9.1. What is the FEVER score and how is it used?
        - sent21

10. What are the top systems and recent approaches in the FEVER competition?
    - sent22
    10.1. What did Zhou et al. (2019) show about adding graph networks on top of BERT?
        - sent23
    10.2. Why is the retrieval model important?
        - sent24
    10.3. What improvements did Liu et al. (2020b) and Zhong et al. (2020) achieve?
        - sent25
    10.4. What did Ye et al. (2020) experiment with?
        - sent26
    10.5. How did Si et al. (2021) and Ostrowski et al. (2021) improve multi-hop reasoning?
        - sent27
    10.6. What did Atanasova et al. (2022) introduce?
        - sent28

11. What are some notable ideas for using pre-trained language models as fact-checkers?
    - sent29
    11.1. What is the advantage of these models?
        - sent30
    11.2. What are the biases and limitations of these models?
        - sent31
        11.2.1. What is an example of insufficient context confusing the models?
            - sent32
        11.2.2. How does the performance of these models compare to supervised approaches?
            - sent33

12. What are the main challenges identified in error analysis?
    - sent34
    12.1. What are the specific challenges mentioned?
        - sent35
        - sent36
        - sent37
        - sent38

13. What is the significance of threaded stance in conversational threads?
    - sent39
    13.1. How are these approaches applied?
        - sent40
    13.2. What is a common pattern in these approaches?
        - sent41
    13.3. What did Kumar and Carley (2019) replace and use in their approach?
        - sent42
    13.4. What recent models were proposed for rumour debunking?
        - sent43
    13.5. How did Kochkina et al. (2017, 2018) model conversations?
        - sent44
    13.6. How did Li et al. (2020) deviate from the common structure?
        - sent45
    13.7. What did Tian et al. (2020) show about pre-training on stance data?
        - sent46
    13.8. What did Yang et al. (2019) achieve with per-class pre-training data?
        - sent47
    13.9. What additional information did Li et al. (2019a,b) incorporate?
        - sent48
    13.10. Why must preceding posts be taken into account?
        - sent49

14. What are the challenges for rumour detection datasets?
    - sent50
    14.1. What role does label semantics play?
        - sent51
    14.2. How does thread depth affect performance?
        - sent52
    14.3. What is the positive side of using multitask learning?
        - sent53
    14.4. What is the significance of the temporal dimension of posts?
        - sent54

15. What is Multi-Dataset Learning (MDL)?
    - sent55
    15.1. What challenges arise from combining mis-and disinformation identification with stance detection?
        - sent56
    15.2. What did earlier approaches focus on?
        - sent57
    15.3. What did Schiller et al. (2021) propose?
        - sent58
    15.4. What improvements did they show with multitask learning?
        - sent59
    15.5. What did Hardalov et al. (2021) achieve in a cross-lingual setting?
        - sent60
    15.6. How does MDL help in different scenarios?
        - sent61
    15.7. How can transferring knowledge from English stance datasets boost performance?
        - sent62

16. What does Table 2 show?
    - sent63
    16.1. How do the datasets vary?
        - sent64
    16.2. How do these factors impact performance?
        - sent65",What is the FEVER score and how is it used in fact-checking?,"sent20, sent21, sent22, sent23, sent24, sent25, sent26, sent27, sent28","Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER .

To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted.

The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021).

Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score.

Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points.

Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70.

Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes.

Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information.

Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.","The FEVER score involves an extra step for evidence retrieval, such as retrieving Wikipedia snippets.

The FEVER score is defined as the proportion of claims for which both correct evidence is returned and a correct label is predicted.

The top systems that participated in the FEVER competition used various advanced techniques, including bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018), and graph neural networks (Zhou et al., 2019; Atanasov et al., 2019; Liu et al., 2020b; Zhong et al., 2020; Weinzierl et al., 2021; Si et al., 2021).

Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching a 67.1 FEVER score.

Yet, the retrieval model is also important, as using the gold evidence set adds 1.4 points.

Liu et al. (2020b) and Zhong et al. (2020) replaced the retrieval model with a BERT-based one and used an improved mechanism to propagate information between nodes in the graph, boosting the score to 70.

Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distant-supervised pre-training, namely, CorefRoBERTa, and added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes.

Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information.

Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.","Questions:

What is the FEVER score and how is it used in fact-checking?

Answer:

The FEVER score involves an extra step for evidence retrieval, such as retrieving Wikipedia snippets.

The FEVER score is defined as the proportion of claims for which both correct evidence is returned and a correct label is predicted.

The top systems that participated in the FEVER competition used various advanced techniques, including bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018), and graph neural networks (Zhou et al., 2019; Atanasov et al., 2019; Liu et al., 2020b; Zhong et al., 2020; Weinzierl et al., 2021; Si et al., 2021).

Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching a 67.1 FEVER score.

Yet, the retrieval model is also important, as using the gold evidence set adds 1.4 points.

Liu et al. (2020b) and Zhong et al. (2020) replaced the retrieval model with a BERT-based one and used an improved mechanism to propagate information between nodes in the graph, boosting the score to 70.

Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distant-supervised pre-training, namely, CorefRoBERTa, and added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes.

Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information.

Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class."
A Survey on Stance Detection for Mis-and Disinformation Identification,Multiple languages,"In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.

Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","1. What is the focus of this section? 
    - sent1

2. What is the Fake News Challenge and its evaluation measure?
    - sent2
    2.1. What was the criticism of the FNC score and F1-micro?
        - sent3

3. What features did most teams use in the competition?
    - sent4
    3.1. What did Hanselowski et al. (2018a) show about the importance of features?
        - sent5
    3.2. What did Ghanem et al. (2018) find about lexical cues?
        - sent6

4. What is the performance of these models on different classes?
    - sent7
    4.1. How well is the Unrelated class detected?
        - sent8

5. What are the limitations of these models according to Hanselowski et al. (2018a)?
    - sent9
    5.1. What can be attributed to these limitations?
        - sent10

6. What did Mohtarami et al. (2018) investigate?
    - sent11
    6.1. Why does their model perform poorly on the Agree/Disagree classes?
        - sent12
        6.1.1. What is the reason for the poor performance?
            - sent13

7. What recent advancements have been made in stance detection?
    - sent14
    7.1. What specific improvement did using the pre-trained Transformer RoBERTa bring?
        - sent15

8. How successful are these models in cross-lingual settings?
    - sent16
    8.1. What are the results for Arabic stance detection using mBERT?
        - sent17
    8.2. What did Hardalov et al. (2022) achieve with pattern-exploiting training (PET)?
        - sent18
    8.3. What did Alhindi et al. (2021) show about language-specific pre-training?
        - sent19

9. What are some formulations that include an extra step for evidence retrieval?
    - sent20
    9.1. What is the FEVER score and how is it used?
        - sent21

10. What are the top systems and recent approaches in the FEVER competition?
    - sent22
    10.1. What did Zhou et al. (2019) show about adding graph networks on top of BERT?
        - sent23
    10.2. Why is the retrieval model important?
        - sent24
    10.3. What improvements did Liu et al. (2020b) and Zhong et al. (2020) achieve?
        - sent25
    10.4. What did Ye et al. (2020) experiment with?
        - sent26
    10.5. How did Si et al. (2021) and Ostrowski et al. (2021) improve multi-hop reasoning?
        - sent27
    10.6. What did Atanasova et al. (2022) introduce?
        - sent28

11. What are some notable ideas for using pre-trained language models as fact-checkers?
    - sent29
    11.1. What is the advantage of these models?
        - sent30
    11.2. What are the biases and limitations of these models?
        - sent31
        11.2.1. What is an example of insufficient context confusing the models?
            - sent32
        11.2.2. How does the performance of these models compare to supervised approaches?
            - sent33

12. What are the main challenges identified in error analysis?
    - sent34
    12.1. What are the specific challenges mentioned?
        - sent35
        - sent36
        - sent37
        - sent38

13. What is the significance of threaded stance in conversational threads?
    - sent39
    13.1. How are these approaches applied?
        - sent40
    13.2. What is a common pattern in these approaches?
        - sent41
    13.3. What did Kumar and Carley (2019) replace and use in their approach?
        - sent42
    13.4. What recent models were proposed for rumour debunking?
        - sent43
    13.5. How did Kochkina et al. (2017, 2018) model conversations?
        - sent44
    13.6. How did Li et al. (2020) deviate from the common structure?
        - sent45
    13.7. What did Tian et al. (2020) show about pre-training on stance data?
        - sent46
    13.8. What did Yang et al. (2019) achieve with per-class pre-training data?
        - sent47
    13.9. What additional information did Li et al. (2019a,b) incorporate?
        - sent48
    13.10. Why must preceding posts be taken into account?
        - sent49

14. What are the challenges for rumour detection datasets?
    - sent50
    14.1. What role does label semantics play?
        - sent51
    14.2. How does thread depth affect performance?
        - sent52
    14.3. What is the positive side of using multitask learning?
        - sent53
    14.4. What is the significance of the temporal dimension of posts?
        - sent54

15. What is Multi-Dataset Learning (MDL)?
    - sent55
    15.1. What challenges arise from combining mis-and disinformation identification with stance detection?
        - sent56
    15.2. What did earlier approaches focus on?
        - sent57
    15.3. What did Schiller et al. (2021) propose?
        - sent58
    15.4. What improvements did they show with multitask learning?
        - sent59
    15.5. What did Hardalov et al. (2021) achieve in a cross-lingual setting?
        - sent60
    15.6. How does MDL help in different scenarios?
        - sent61
    15.7. How can transferring knowledge from English stance datasets boost performance?
        - sent62

16. What does Table 2 show?
    - sent63
    16.1. How do the datasets vary?
        - sent64
    16.2. How do these factors impact performance?
        - sent65",What are the main challenges identified in error analysis for stance detection models?,"sent34, sent35, sent36, sent37, sent38","Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer.""

vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club.

"", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers.""

(NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm.""

is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.","Error analysis suggests that the main challenges are confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer.""

For example, ""Andrea Pirlo is an Italian professional footballer who plays for an American club.""

Other challenges include sensitivity to spelling errors, lack of relation between the article and the entities in the claim, and dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers.""

Another challenge is embedding-level confusion, where numbers tend to have similar embeddings, such as ""The heart beats at a resting rate close to 22 bpm.""

Another example of embedding-level confusion is when a claim like ""The heart beats at a resting rate close to 22 bpm"" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm.""","Questions:

What are the main challenges identified in error analysis for stance detection models?

Answer:

Error analysis suggests that the main challenges are confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer.""

For example, ""Andrea Pirlo is an Italian professional footballer who plays for an American club.""

Other challenges include sensitivity to spelling errors, lack of relation between the article and the entities in the claim, and dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers.""

Another challenge is embedding-level confusion, where numbers tend to have similar embeddings, such as ""The heart beats at a resting rate close to 22 bpm.""

Another example of embedding-level confusion is when a claim like ""The heart beats at a resting rate close to 22 bpm"" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."""
A Survey on Stance Detection for Mis-and Disinformation Identification,Multiple languages,"In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.

Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","1. What is the focus of this section? 
    - sent1

2. What is the Fake News Challenge and its evaluation measure?
    - sent2
    2.1. What was the criticism of the FNC score and F1-micro?
        - sent3

3. What features did most teams use in the competition?
    - sent4
    3.1. What did Hanselowski et al. (2018a) show about the importance of features?
        - sent5
    3.2. What did Ghanem et al. (2018) find about lexical cues?
        - sent6

4. What is the performance of these models on different classes?
    - sent7
    4.1. How well is the Unrelated class detected?
        - sent8

5. What are the limitations of these models according to Hanselowski et al. (2018a)?
    - sent9
    5.1. What can be attributed to these limitations?
        - sent10

6. What did Mohtarami et al. (2018) investigate?
    - sent11
    6.1. Why does their model perform poorly on the Agree/Disagree classes?
        - sent12
        6.1.1. What is the reason for the poor performance?
            - sent13

7. What recent advancements have been made in stance detection?
    - sent14
    7.1. What specific improvement did using the pre-trained Transformer RoBERTa bring?
        - sent15

8. How successful are these models in cross-lingual settings?
    - sent16
    8.1. What are the results for Arabic stance detection using mBERT?
        - sent17
    8.2. What did Hardalov et al. (2022) achieve with pattern-exploiting training (PET)?
        - sent18
    8.3. What did Alhindi et al. (2021) show about language-specific pre-training?
        - sent19

9. What are some formulations that include an extra step for evidence retrieval?
    - sent20
    9.1. What is the FEVER score and how is it used?
        - sent21

10. What are the top systems and recent approaches in the FEVER competition?
    - sent22
    10.1. What did Zhou et al. (2019) show about adding graph networks on top of BERT?
        - sent23
    10.2. Why is the retrieval model important?
        - sent24
    10.3. What improvements did Liu et al. (2020b) and Zhong et al. (2020) achieve?
        - sent25
    10.4. What did Ye et al. (2020) experiment with?
        - sent26
    10.5. How did Si et al. (2021) and Ostrowski et al. (2021) improve multi-hop reasoning?
        - sent27
    10.6. What did Atanasova et al. (2022) introduce?
        - sent28

11. What are some notable ideas for using pre-trained language models as fact-checkers?
    - sent29
    11.1. What is the advantage of these models?
        - sent30
    11.2. What are the biases and limitations of these models?
        - sent31
        11.2.1. What is an example of insufficient context confusing the models?
            - sent32
        11.2.2. How does the performance of these models compare to supervised approaches?
            - sent33

12. What are the main challenges identified in error analysis?
    - sent34
    12.1. What are the specific challenges mentioned?
        - sent35
        - sent36
        - sent37
        - sent38

13. What is the significance of threaded stance in conversational threads?
    - sent39
    13.1. How are these approaches applied?
        - sent40
    13.2. What is a common pattern in these approaches?
        - sent41
    13.3. What did Kumar and Carley (2019) replace and use in their approach?
        - sent42
    13.4. What recent models were proposed for rumour debunking?
        - sent43
    13.5. How did Kochkina et al. (2017, 2018) model conversations?
        - sent44
    13.6. How did Li et al. (2020) deviate from the common structure?
        - sent45
    13.7. What did Tian et al. (2020) show about pre-training on stance data?
        - sent46
    13.8. What did Yang et al. (2019) achieve with per-class pre-training data?
        - sent47
    13.9. What additional information did Li et al. (2019a,b) incorporate?
        - sent48
    13.10. Why must preceding posts be taken into account?
        - sent49

14. What are the challenges for rumour detection datasets?
    - sent50
    14.1. What role does label semantics play?
        - sent51
    14.2. How does thread depth affect performance?
        - sent52
    14.3. What is the positive side of using multitask learning?
        - sent53
    14.4. What is the significance of the temporal dimension of posts?
        - sent54

15. What is Multi-Dataset Learning (MDL)?
    - sent55
    15.1. What challenges arise from combining mis-and disinformation identification with stance detection?
        - sent56
    15.2. What did earlier approaches focus on?
        - sent57
    15.3. What did Schiller et al. (2021) propose?
        - sent58
    15.4. What improvements did they show with multitask learning?
        - sent59
    15.5. What did Hardalov et al. (2021) achieve in a cross-lingual setting?
        - sent60
    15.6. How does MDL help in different scenarios?
        - sent61
    15.7. How can transferring knowledge from English stance datasets boost performance?
        - sent62

16. What does Table 2 show?
    - sent63
    16.1. How do the datasets vary?
        - sent64
    16.2. How do these factors impact performance?
        - sent65",How is threaded stance detection applied in conversational threads?,"sent39, sent40, sent41, sent42, sent43, sent44, sent45, sent46, sent47, sent48, sent49","Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours).

A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017).

Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets.

More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020).

Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1).

Li et al. (2020) deviated from this structure and modelled the conversations as a graph.

Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection.

Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task.

Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification .

Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).","In the setting of conversational threads (Zubiaga et al., 2016b; Derczynski et al., 2017; Gorrell et al., 2019), important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, such as for detecting and debunking rumors.

A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017).

Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs and used pre-trained embeddings to encode the tweets.

More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post- and thread-level representations for rumor debunking, improving previous results on RumourEval '17 (Yu et al., 2020).

Kochkina et al. (2017, 2018) split conversations into branches, modeling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1).

Li et al. (2020) deviated from this structure and modeled the conversations as a graph.

Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumor detection.

Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task.

Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumor veracity, ranking 3rd on stance detection and 1st on veracity classification.

Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must also be taken into account (Gorrell et al., 2019).","Questions:

How is threaded stance detection applied in conversational threads?

Answer:

In the setting of conversational threads (Zubiaga et al., 2016b; Derczynski et al., 2017; Gorrell et al., 2019), important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, such as for detecting and debunking rumors.

A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017).

Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs and used pre-trained embeddings to encode the tweets.

More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post- and thread-level representations for rumor debunking, improving previous results on RumourEval '17 (Yu et al., 2020).

Kochkina et al. (2017, 2018) split conversations into branches, modeling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1).

Li et al. (2020) deviated from this structure and modeled the conversations as a graph.

Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumor detection.

Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task.

Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumor veracity, ranking 3rd on stance detection and 1st on veracity classification.

Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must also be taken into account (Gorrell et al., 2019)."
A Survey on Stance Detection for Mis-and Disinformation Identification,Multiple languages,"In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.

Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","1. What is the focus of this section? 
    - sent1

2. What is the Fake News Challenge and its evaluation measure?
    - sent2
    2.1. What was the criticism of the FNC score and F1-micro?
        - sent3

3. What features did most teams use in the competition?
    - sent4
    3.1. What did Hanselowski et al. (2018a) show about the importance of features?
        - sent5
    3.2. What did Ghanem et al. (2018) find about lexical cues?
        - sent6

4. What is the performance of these models on different classes?
    - sent7
    4.1. How well is the Unrelated class detected?
        - sent8

5. What are the limitations of these models according to Hanselowski et al. (2018a)?
    - sent9
    5.1. What can be attributed to these limitations?
        - sent10

6. What did Mohtarami et al. (2018) investigate?
    - sent11
    6.1. Why does their model perform poorly on the Agree/Disagree classes?
        - sent12
        6.1.1. What is the reason for the poor performance?
            - sent13

7. What recent advancements have been made in stance detection?
    - sent14
    7.1. What specific improvement did using the pre-trained Transformer RoBERTa bring?
        - sent15

8. How successful are these models in cross-lingual settings?
    - sent16
    8.1. What are the results for Arabic stance detection using mBERT?
        - sent17
    8.2. What did Hardalov et al. (2022) achieve with pattern-exploiting training (PET)?
        - sent18
    8.3. What did Alhindi et al. (2021) show about language-specific pre-training?
        - sent19

9. What are some formulations that include an extra step for evidence retrieval?
    - sent20
    9.1. What is the FEVER score and how is it used?
        - sent21

10. What are the top systems and recent approaches in the FEVER competition?
    - sent22
    10.1. What did Zhou et al. (2019) show about adding graph networks on top of BERT?
        - sent23
    10.2. Why is the retrieval model important?
        - sent24
    10.3. What improvements did Liu et al. (2020b) and Zhong et al. (2020) achieve?
        - sent25
    10.4. What did Ye et al. (2020) experiment with?
        - sent26
    10.5. How did Si et al. (2021) and Ostrowski et al. (2021) improve multi-hop reasoning?
        - sent27
    10.6. What did Atanasova et al. (2022) introduce?
        - sent28

11. What are some notable ideas for using pre-trained language models as fact-checkers?
    - sent29
    11.1. What is the advantage of these models?
        - sent30
    11.2. What are the biases and limitations of these models?
        - sent31
        11.2.1. What is an example of insufficient context confusing the models?
            - sent32
        11.2.2. How does the performance of these models compare to supervised approaches?
            - sent33

12. What are the main challenges identified in error analysis?
    - sent34
    12.1. What are the specific challenges mentioned?
        - sent35
        - sent36
        - sent37
        - sent38

13. What is the significance of threaded stance in conversational threads?
    - sent39
    13.1. How are these approaches applied?
        - sent40
    13.2. What is a common pattern in these approaches?
        - sent41
    13.3. What did Kumar and Carley (2019) replace and use in their approach?
        - sent42
    13.4. What recent models were proposed for rumour debunking?
        - sent43
    13.5. How did Kochkina et al. (2017, 2018) model conversations?
        - sent44
    13.6. How did Li et al. (2020) deviate from the common structure?
        - sent45
    13.7. What did Tian et al. (2020) show about pre-training on stance data?
        - sent46
    13.8. What did Yang et al. (2019) achieve with per-class pre-training data?
        - sent47
    13.9. What additional information did Li et al. (2019a,b) incorporate?
        - sent48
    13.10. Why must preceding posts be taken into account?
        - sent49

14. What are the challenges for rumour detection datasets?
    - sent50
    14.1. What role does label semantics play?
        - sent51
    14.2. How does thread depth affect performance?
        - sent52
    14.3. What is the positive side of using multitask learning?
        - sent53
    14.4. What is the significance of the temporal dimension of posts?
        - sent54

15. What is Multi-Dataset Learning (MDL)?
    - sent55
    15.1. What challenges arise from combining mis-and disinformation identification with stance detection?
        - sent56
    15.2. What did earlier approaches focus on?
        - sent57
    15.3. What did Schiller et al. (2021) propose?
        - sent58
    15.4. What improvements did they show with multitask learning?
        - sent59
    15.5. What did Hardalov et al. (2021) achieve in a cross-lingual setting?
        - sent60
    15.6. How does MDL help in different scenarios?
        - sent61
    15.7. How can transferring knowledge from English stance datasets boost performance?
        - sent62

16. What does Table 2 show?
    - sent63
    16.1. How do the datasets vary?
        - sent64
    16.2. How do these factors impact performance?
        - sent65",What are the challenges for rumour detection datasets?,"sent50, sent51, sent52, sent53, sent54","A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0.

Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1.

Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class.

On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.","A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b; Derczynski et al., 2017; Gorrell et al., 2019), as the minority class denying is extremely hard for models to learn.

Label semantics also appears to play a role, as the querying label has a similar distribution but much higher F1.

Another factor is thread depth, as performance drops significantly at higher depths, especially for the supporting class.

On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a significant boost in performance (Gorrell et al., 2019; Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016; Veyseh et al., 2017; Dungs et al., 2018). In-depth data analysis (Zubiaga et al., 2016a,b; Kochkina et al., 2017; Ma and Gao, 2020; Li et al., 2020) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.","Questions:

What are the challenges for rumour detection datasets?

Answer:

A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b; Derczynski et al., 2017; Gorrell et al., 2019), as the minority class denying is extremely hard for models to learn.

Label semantics also appears to play a role, as the querying label has a similar distribution but much higher F1.

Another factor is thread depth, as performance drops significantly at higher depths, especially for the supporting class.

On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a significant boost in performance (Gorrell et al., 2019; Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016; Veyseh et al., 2017; Dungs et al., 2018). In-depth data analysis (Zubiaga et al., 2016a,b; Kochkina et al., 2017; Ma and Gao, 2020; Li et al., 2020) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases."
A Survey on Stance Detection for Mis-and Disinformation Identification,Multiple languages,"In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.

Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","1. What is the focus of this section? 
    - sent1

2. What is the Fake News Challenge and its evaluation measure?
    - sent2
    2.1. What was the criticism of the FNC score and F1-micro?
        - sent3

3. What features did most teams use in the competition?
    - sent4
    3.1. What did Hanselowski et al. (2018a) show about the importance of features?
        - sent5
    3.2. What did Ghanem et al. (2018) find about lexical cues?
        - sent6

4. What is the performance of these models on different classes?
    - sent7
    4.1. How well is the Unrelated class detected?
        - sent8

5. What are the limitations of these models according to Hanselowski et al. (2018a)?
    - sent9
    5.1. What can be attributed to these limitations?
        - sent10

6. What did Mohtarami et al. (2018) investigate?
    - sent11
    6.1. Why does their model perform poorly on the Agree/Disagree classes?
        - sent12
        6.1.1. What is the reason for the poor performance?
            - sent13

7. What recent advancements have been made in stance detection?
    - sent14
    7.1. What specific improvement did using the pre-trained Transformer RoBERTa bring?
        - sent15

8. How successful are these models in cross-lingual settings?
    - sent16
    8.1. What are the results for Arabic stance detection using mBERT?
        - sent17
    8.2. What did Hardalov et al. (2022) achieve with pattern-exploiting training (PET)?
        - sent18
    8.3. What did Alhindi et al. (2021) show about language-specific pre-training?
        - sent19

9. What are some formulations that include an extra step for evidence retrieval?
    - sent20
    9.1. What is the FEVER score and how is it used?
        - sent21

10. What are the top systems and recent approaches in the FEVER competition?
    - sent22
    10.1. What did Zhou et al. (2019) show about adding graph networks on top of BERT?
        - sent23
    10.2. Why is the retrieval model important?
        - sent24
    10.3. What improvements did Liu et al. (2020b) and Zhong et al. (2020) achieve?
        - sent25
    10.4. What did Ye et al. (2020) experiment with?
        - sent26
    10.5. How did Si et al. (2021) and Ostrowski et al. (2021) improve multi-hop reasoning?
        - sent27
    10.6. What did Atanasova et al. (2022) introduce?
        - sent28

11. What are some notable ideas for using pre-trained language models as fact-checkers?
    - sent29
    11.1. What is the advantage of these models?
        - sent30
    11.2. What are the biases and limitations of these models?
        - sent31
        11.2.1. What is an example of insufficient context confusing the models?
            - sent32
        11.2.2. How does the performance of these models compare to supervised approaches?
            - sent33

12. What are the main challenges identified in error analysis?
    - sent34
    12.1. What are the specific challenges mentioned?
        - sent35
        - sent36
        - sent37
        - sent38

13. What is the significance of threaded stance in conversational threads?
    - sent39
    13.1. How are these approaches applied?
        - sent40
    13.2. What is a common pattern in these approaches?
        - sent41
    13.3. What did Kumar and Carley (2019) replace and use in their approach?
        - sent42
    13.4. What recent models were proposed for rumour debunking?
        - sent43
    13.5. How did Kochkina et al. (2017, 2018) model conversations?
        - sent44
    13.6. How did Li et al. (2020) deviate from the common structure?
        - sent45
    13.7. What did Tian et al. (2020) show about pre-training on stance data?
        - sent46
    13.8. What did Yang et al. (2019) achieve with per-class pre-training data?
        - sent47
    13.9. What additional information did Li et al. (2019a,b) incorporate?
        - sent48
    13.10. Why must preceding posts be taken into account?
        - sent49

14. What are the challenges for rumour detection datasets?
    - sent50
    14.1. What role does label semantics play?
        - sent51
    14.2. How does thread depth affect performance?
        - sent52
    14.3. What is the positive side of using multitask learning?
        - sent53
    14.4. What is the significance of the temporal dimension of posts?
        - sent54

15. What is Multi-Dataset Learning (MDL)?
    - sent55
    15.1. What challenges arise from combining mis-and disinformation identification with stance detection?
        - sent56
    15.2. What did earlier approaches focus on?
        - sent57
    15.3. What did Schiller et al. (2021) propose?
        - sent58
    15.4. What improvements did they show with multitask learning?
        - sent59
    15.5. What did Hardalov et al. (2021) achieve in a cross-lingual setting?
        - sent60
    15.6. How does MDL help in different scenarios?
        - sent61
    15.7. How can transferring knowledge from English stance datasets boost performance?
        - sent62

16. What does Table 2 show?
    - sent63
    16.1. How do the datasets vary?
        - sent64
    16.2. How do these factors impact performance?
        - sent65",What is Multi-Dataset Learning (MDL) and its benefits for stance detection?,"sent55, sent56, sent57, sent58, sent59, sent60, sent61, sent62","Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness.

However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance.

Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models.

They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets.

Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.

They showed that MDL helps for low-resource and substantively for full-resource scenarios.

Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance.","Multi-Dataset Learning (MDL) involves mixing data from different domains and sources to improve robustness.

However, setups that combine misinformation and disinformation identification with stance detection vary in their annotation and labeling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks. For example, Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on tasks such as question answering and natural language inference, which are weakly related to stance.

Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models.

They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets.

Hardalov et al. (2021) combined datasets with different label inventories by modeling the task as a cloze question answering one in a cross-lingual setting (Schick and Schütze, 2021).

They showed that MDL helps in both low-resource and full-resource scenarios.

Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance.","Questions:

What is Multi-Dataset Learning (MDL) and its benefits for stance detection?

Answer:

Multi-Dataset Learning (MDL) involves mixing data from different domains and sources to improve robustness.

However, setups that combine misinformation and disinformation identification with stance detection vary in their annotation and labeling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks. For example, Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on tasks such as question answering and natural language inference, which are weakly related to stance.

Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models.

They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets.

Hardalov et al. (2021) combined datasets with different label inventories by modeling the task as a cloze question answering one in a cross-lingual setting (Schick and Schütze, 2021).

They showed that MDL helps in both low-resource and full-resource scenarios.

Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance."
A Survey on Stance Detection for Mis-and Disinformation Identification,Multiple languages,"In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.

Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","1. What is the focus of this section? 
    - sent1

2. What is the Fake News Challenge and its evaluation measure?
    - sent2
    2.1. What was the criticism of the FNC score and F1-micro?
        - sent3

3. What features did most teams use in the competition?
    - sent4
    3.1. What did Hanselowski et al. (2018a) show about the importance of features?
        - sent5
    3.2. What did Ghanem et al. (2018) find about lexical cues?
        - sent6

4. What is the performance of these models on different classes?
    - sent7
    4.1. How well is the Unrelated class detected?
        - sent8

5. What are the limitations of these models according to Hanselowski et al. (2018a)?
    - sent9
    5.1. What can be attributed to these limitations?
        - sent10

6. What did Mohtarami et al. (2018) investigate?
    - sent11
    6.1. Why does their model perform poorly on the Agree/Disagree classes?
        - sent12
        6.1.1. What is the reason for the poor performance?
            - sent13

7. What recent advancements have been made in stance detection?
    - sent14
    7.1. What specific improvement did using the pre-trained Transformer RoBERTa bring?
        - sent15

8. How successful are these models in cross-lingual settings?
    - sent16
    8.1. What are the results for Arabic stance detection using mBERT?
        - sent17
    8.2. What did Hardalov et al. (2022) achieve with pattern-exploiting training (PET)?
        - sent18
    8.3. What did Alhindi et al. (2021) show about language-specific pre-training?
        - sent19

9. What are some formulations that include an extra step for evidence retrieval?
    - sent20
    9.1. What is the FEVER score and how is it used?
        - sent21

10. What are the top systems and recent approaches in the FEVER competition?
    - sent22
    10.1. What did Zhou et al. (2019) show about adding graph networks on top of BERT?
        - sent23
    10.2. Why is the retrieval model important?
        - sent24
    10.3. What improvements did Liu et al. (2020b) and Zhong et al. (2020) achieve?
        - sent25
    10.4. What did Ye et al. (2020) experiment with?
        - sent26
    10.5. How did Si et al. (2021) and Ostrowski et al. (2021) improve multi-hop reasoning?
        - sent27
    10.6. What did Atanasova et al. (2022) introduce?
        - sent28

11. What are some notable ideas for using pre-trained language models as fact-checkers?
    - sent29
    11.1. What is the advantage of these models?
        - sent30
    11.2. What are the biases and limitations of these models?
        - sent31
        11.2.1. What is an example of insufficient context confusing the models?
            - sent32
        11.2.2. How does the performance of these models compare to supervised approaches?
            - sent33

12. What are the main challenges identified in error analysis?
    - sent34
    12.1. What are the specific challenges mentioned?
        - sent35
        - sent36
        - sent37
        - sent38

13. What is the significance of threaded stance in conversational threads?
    - sent39
    13.1. How are these approaches applied?
        - sent40
    13.2. What is a common pattern in these approaches?
        - sent41
    13.3. What did Kumar and Carley (2019) replace and use in their approach?
        - sent42
    13.4. What recent models were proposed for rumour debunking?
        - sent43
    13.5. How did Kochkina et al. (2017, 2018) model conversations?
        - sent44
    13.6. How did Li et al. (2020) deviate from the common structure?
        - sent45
    13.7. What did Tian et al. (2020) show about pre-training on stance data?
        - sent46
    13.8. What did Yang et al. (2019) achieve with per-class pre-training data?
        - sent47
    13.9. What additional information did Li et al. (2019a,b) incorporate?
        - sent48
    13.10. Why must preceding posts be taken into account?
        - sent49

14. What are the challenges for rumour detection datasets?
    - sent50
    14.1. What role does label semantics play?
        - sent51
    14.2. How does thread depth affect performance?
        - sent52
    14.3. What is the positive side of using multitask learning?
        - sent53
    14.4. What is the significance of the temporal dimension of posts?
        - sent54

15. What is Multi-Dataset Learning (MDL)?
    - sent55
    15.1. What challenges arise from combining mis-and disinformation identification with stance detection?
        - sent56
    15.2. What did earlier approaches focus on?
        - sent57
    15.3. What did Schiller et al. (2021) propose?
        - sent58
    15.4. What improvements did they show with multitask learning?
        - sent59
    15.5. What did Hardalov et al. (2021) achieve in a cross-lingual setting?
        - sent60
    15.6. How does MDL help in different scenarios?
        - sent61
    15.7. How can transferring knowledge from English stance datasets boost performance?
        - sent62

16. What does Table 2 show?
    - sent63
    16.1. How do the datasets vary?
        - sent64
    16.2. How do these factors impact performance?
        - sent65",How do the datasets for stance detection vary and impact performance?,"sent63, sent64, sent65","Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1.

The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc.

Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","The state-of-the-art (SOTA) results for each dataset vary and impact performance.

The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, and evaluation measures.

Each of these factors impacts the performance, leading to sizable differences in the final score, and hence rendering the reported results hard to compare directly across these datasets.","Questions:

How do the datasets for stance detection vary and impact performance?

Answer:

The state-of-the-art (SOTA) results for each dataset vary and impact performance.

The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, and evaluation measures.

Each of these factors impacts the performance, leading to sizable differences in the final score, and hence rendering the reported results hard to compare directly across these datasets."
A Survey on Stance Detection for Mis-and Disinformation Identification,Shades of Truth,"The notion of shades of truth is important in mis-and disinformation detection. For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false. We believe that such shades could be applied to stance and used in a larger pipeline. In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).

Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance. As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance. Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.","# QUA Analysis:

1. (Why is the notion of shades of truth important in mis-and disinformation detection?): sent1
    1.1. (How is the notion of shades of truth applied in fact-checking?): sent2
    1.2. (How could shades of truth be applied to stance detection?): sent3
        1.2.1. (Are fine-grained labels used in related tasks?): sent4
2. (How has the definition of stance detection and label inventories evolved?): sent5
    2.1. (How do labels vary based on use case and setting?): sent6
        2.1.1. (What are the common labels used in stance detection?): sent6
        2.1.2. (Why are the common labels not enough to accurately assess stance?): sent6
    2.2. (How can adding label granularity improve stance detection?): sent7",Why is the notion of shades of truth important in mis-and disinformation detection?,"sent1, sent2, sent3, sent4","The notion of shades of truth is important in mis-and disinformation detection.

For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false.

We believe that such shades could be applied to stance and used in a larger pipeline.

In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).","The notion of shades of truth is important in mis-and disinformation detection.

For example, fact-checking often goes beyond binary true/false labels, with Nakov et al. (2018) using a third category half-true, Rashkin et al. (2017) including mixed and no factual evidence, and Wang (2017) and Santia and Williams (2018) adopting an even finer-grained schema with six labels, including barely true and utterly false.

Such shades of truth could also be applied to stance detection and integrated into a larger pipeline.

In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005; Rosenthal et al., 2017).","Questions:

Why is the notion of shades of truth important in mis-and disinformation detection?

Answer:

The notion of shades of truth is important in mis-and disinformation detection.

For example, fact-checking often goes beyond binary true/false labels, with Nakov et al. (2018) using a third category half-true, Rashkin et al. (2017) including mixed and no factual evidence, and Wang (2017) and Santia and Williams (2018) adopting an even finer-grained schema with six labels, including barely true and utterly false.

Such shades of truth could also be applied to stance detection and integrated into a larger pipeline.

In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005; Rosenthal et al., 2017)."
A Survey on Stance Detection for Mis-and Disinformation Identification,Shades of Truth,"The notion of shades of truth is important in mis-and disinformation detection. For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false. We believe that such shades could be applied to stance and used in a larger pipeline. In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).

Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance. As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance. Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.","# QUA Analysis:

1. (Why is the notion of shades of truth important in mis-and disinformation detection?): sent1
    1.1. (How is the notion of shades of truth applied in fact-checking?): sent2
    1.2. (How could shades of truth be applied to stance detection?): sent3
        1.2.1. (Are fine-grained labels used in related tasks?): sent4
2. (How has the definition of stance detection and label inventories evolved?): sent5
    2.1. (How do labels vary based on use case and setting?): sent6
        2.1.1. (What are the common labels used in stance detection?): sent6
        2.1.2. (Why are the common labels not enough to accurately assess stance?): sent6
    2.2. (How can adding label granularity improve stance detection?): sent7",How is the notion of shades of truth applied in fact-checking?,"sent2, sent3, sent4","For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false.

We believe that such shades could be applied to stance and used in a larger pipeline.

In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).","Fact-checking often goes beyond binary true/false labels, incorporating additional categories such as half-true (Nakov et al., 2018), mixed and no factual evidence (Rashkin et al., 2017), and a finer-grained schema with six labels, including barely true and utterly false (Wang, 2017; Santia and Williams, 2018).

Such shades of truth could also be applied to stance and used in a larger pipeline.

In fact, fine-grained labels are also common for the related task of Sentiment Analysis (Pang and Lee, 2005; Rosenthal et al., 2017).","Questions:

How is the notion of shades of truth applied in fact-checking?

Answer:

Fact-checking often goes beyond binary true/false labels, incorporating additional categories such as half-true (Nakov et al., 2018), mixed and no factual evidence (Rashkin et al., 2017), and a finer-grained schema with six labels, including barely true and utterly false (Wang, 2017; Santia and Williams, 2018).

Such shades of truth could also be applied to stance and used in a larger pipeline.

In fact, fine-grained labels are also common for the related task of Sentiment Analysis (Pang and Lee, 2005; Rosenthal et al., 2017)."
A Survey on Stance Detection for Mis-and Disinformation Identification,Shades of Truth,"The notion of shades of truth is important in mis-and disinformation detection. For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false. We believe that such shades could be applied to stance and used in a larger pipeline. In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).

Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance. As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance. Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.","# QUA Analysis:

1. (Why is the notion of shades of truth important in mis-and disinformation detection?): sent1
    1.1. (How is the notion of shades of truth applied in fact-checking?): sent2
    1.2. (How could shades of truth be applied to stance detection?): sent3
        1.2.1. (Are fine-grained labels used in related tasks?): sent4
2. (How has the definition of stance detection and label inventories evolved?): sent5
    2.1. (How do labels vary based on use case and setting?): sent6
        2.1.1. (What are the common labels used in stance detection?): sent6
        2.1.2. (Why are the common labels not enough to accurately assess stance?): sent6
    2.2. (How can adding label granularity improve stance detection?): sent7",How has the definition of stance detection and label inventories evolved?,"sent5, sent6, sent7","Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance.

As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance.

Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.","As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance.

Labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance.

Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021; Hardalov et al., 2021; Hardalov et al., 2022).","Questions:

How has the definition of stance detection and label inventories evolved?

Answer:

As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance.

Labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance.

Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021; Hardalov et al., 2021; Hardalov et al., 2022)."
Explanation-Based Human Debugging of NLP Models: A Survey,Bug Sources,"Most of the papers in Table 1 experimented on training datasets with natural artifacts (AR), which cause spurious correlation bugs (i.e., the input texts having signals which are correlated to but not the reasons for specific outputs) and undermine models' generalizability. Out of the 15 papers we surveyed, 5 used the 20Newsgroups dataset (Lang, 1995) as a case study, since it has lots of natural artifacts. For example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions. However, because 20Newsgroups is a topic classification dataset, a better model should focus more on the topic of the content since the punctuation marks can also appear in other classes, especially when we apply the model to texts in the wild. Apart from classification performance drops, natural artifacts can also cause model biases, as shown in (De-Arteaga et al., 2019;Park et al., 2018) and debugged in (Lertvittayakumjorn et al., 2020;Yao et al., 2021).

In the absence of strong natural artifacts, bugs can still be simulated using several techniques. First, using only a small subset of labeled data (SS) for training could cause the model to exploit spurious correlation leading to poor performance (Kulesza et al., 2010). Second, injecting wrong labels (WL) into the training data can obviously blunt the model quality (Koh and Liang, 2017). Third, using out-of-distribution tests (OD) can reveal that the model does not work effectively in the domains that it has not been trained on (Lertvittayakumjorn et al., 2020;Yao et al., 2021). All of these techniques give rise to undesirable model behaviors, requiring debugging. Another technique, not found in Table 1 but suggested in related work (Idahl et al., 2021), is contaminating input texts in the training data with decoys (i.e., injected artifacts) which could deceive the model into predicting for the wrong reasons. This has been experimented with in the computer vision domain (Rieger et al., 2020), and its use in the EBHD setting in NLP could be an interesting direction to explore.","# QUA Analysis:

1. (What are the sources of bugs in NLP models?): sent1
    1.1. (What kind of datasets cause spurious correlation bugs?): sent1
    1.2. (Which datasets were used in the surveyed papers and why?): sent2
        1.2.1. (What is an example of natural artifacts in the 20Newsgroups dataset?): sent3
        1.2.2. (Why should a better model focus on the topic of the content in the 20Newsgroups dataset?): sent4
    1.3. (What other issues can natural artifacts cause besides performance drops?): sent5
2. (How can bugs be simulated in the absence of strong natural artifacts?): sent6
    2.1. (What is one technique to simulate bugs?): sent7
    2.2. (What is another technique to simulate bugs?): sent8
    2.3. (What is a third technique to simulate bugs?): sent9
    2.4. (What is the consequence of using these techniques?): sent10
3. (What is another technique suggested in related work to simulate bugs?): sent11
    3.1. (Where has this technique been experimented with and what is its potential use in NLP?): sent12",What are the sources of bugs in NLP models?,"sent1, sent6, sent7, sent8, sent9, sent10, sent11","Most of the papers in Table 1 experimented on training datasets with natural artifacts (AR), which cause spurious correlation bugs (i.e., the input texts having signals which are correlated to but not the reasons for specific outputs) and undermine models' generalizability.

In the absence of strong natural artifacts, bugs can still be simulated using several techniques.

First, using only a small subset of labeled data (SS) for training could cause the model to exploit spurious correlation leading to poor performance (Kulesza et al., 2010).

Second, injecting wrong labels (WL) into the training data can obviously blunt the model quality (Koh and Liang, 2017).

Third, using out-of-distribution tests (OD) can reveal that the model does not work effectively in the domains that it has not been trained on (Lertvittayakumjorn et al., 2020;Yao et al., 2021).

All of these techniques give rise to undesirable model behaviors, requiring debugging.

Another technique, not found in Table 1 but suggested in related work (Idahl et al., 2021), is contaminating input texts in the training data with decoys (i.e., injected artifacts) which could deceive the model into predicting for the wrong reasons.","Most of the papers experimented on training datasets with natural artifacts (AR), which cause spurious correlation bugs and undermine models' generalizability.

In the absence of strong natural artifacts, bugs can still be simulated using several techniques.

First, using only a small subset of labeled data (SS) for training could cause the model to exploit spurious correlations, leading to poor performance (Kulesza et al., 2010).

Second, injecting wrong labels (WL) into the training data can obviously blunt the model quality (Koh and Liang, 2017).

Third, using out-of-distribution tests (OD) can reveal that the model does not work effectively in the domains that it has not been trained on (Lertvittayakumjorn et al., 2020; Yao et al., 2021).

All of these techniques give rise to undesirable model behaviors, requiring debugging.

Another technique suggested in related work (Idahl et al., 2021) is contaminating input texts in the training data with decoys (i.e., injected artifacts), which could deceive the model into predicting for the wrong reasons.","Questions:

What are the sources of bugs in NLP models?

Answer:

Most of the papers experimented on training datasets with natural artifacts (AR), which cause spurious correlation bugs and undermine models' generalizability.

In the absence of strong natural artifacts, bugs can still be simulated using several techniques.

First, using only a small subset of labeled data (SS) for training could cause the model to exploit spurious correlations, leading to poor performance (Kulesza et al., 2010).

Second, injecting wrong labels (WL) into the training data can obviously blunt the model quality (Koh and Liang, 2017).

Third, using out-of-distribution tests (OD) can reveal that the model does not work effectively in the domains that it has not been trained on (Lertvittayakumjorn et al., 2020; Yao et al., 2021).

All of these techniques give rise to undesirable model behaviors, requiring debugging.

Another technique suggested in related work (Idahl et al., 2021) is contaminating input texts in the training data with decoys (i.e., injected artifacts), which could deceive the model into predicting for the wrong reasons."
Explanation-Based Human Debugging of NLP Models: A Survey,Bug Sources,"Most of the papers in Table 1 experimented on training datasets with natural artifacts (AR), which cause spurious correlation bugs (i.e., the input texts having signals which are correlated to but not the reasons for specific outputs) and undermine models' generalizability. Out of the 15 papers we surveyed, 5 used the 20Newsgroups dataset (Lang, 1995) as a case study, since it has lots of natural artifacts. For example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions. However, because 20Newsgroups is a topic classification dataset, a better model should focus more on the topic of the content since the punctuation marks can also appear in other classes, especially when we apply the model to texts in the wild. Apart from classification performance drops, natural artifacts can also cause model biases, as shown in (De-Arteaga et al., 2019;Park et al., 2018) and debugged in (Lertvittayakumjorn et al., 2020;Yao et al., 2021).

In the absence of strong natural artifacts, bugs can still be simulated using several techniques. First, using only a small subset of labeled data (SS) for training could cause the model to exploit spurious correlation leading to poor performance (Kulesza et al., 2010). Second, injecting wrong labels (WL) into the training data can obviously blunt the model quality (Koh and Liang, 2017). Third, using out-of-distribution tests (OD) can reveal that the model does not work effectively in the domains that it has not been trained on (Lertvittayakumjorn et al., 2020;Yao et al., 2021). All of these techniques give rise to undesirable model behaviors, requiring debugging. Another technique, not found in Table 1 but suggested in related work (Idahl et al., 2021), is contaminating input texts in the training data with decoys (i.e., injected artifacts) which could deceive the model into predicting for the wrong reasons. This has been experimented with in the computer vision domain (Rieger et al., 2020), and its use in the EBHD setting in NLP could be an interesting direction to explore.","# QUA Analysis:

1. (What are the sources of bugs in NLP models?): sent1
    1.1. (What kind of datasets cause spurious correlation bugs?): sent1
    1.2. (Which datasets were used in the surveyed papers and why?): sent2
        1.2.1. (What is an example of natural artifacts in the 20Newsgroups dataset?): sent3
        1.2.2. (Why should a better model focus on the topic of the content in the 20Newsgroups dataset?): sent4
    1.3. (What other issues can natural artifacts cause besides performance drops?): sent5
2. (How can bugs be simulated in the absence of strong natural artifacts?): sent6
    2.1. (What is one technique to simulate bugs?): sent7
    2.2. (What is another technique to simulate bugs?): sent8
    2.3. (What is a third technique to simulate bugs?): sent9
    2.4. (What is the consequence of using these techniques?): sent10
3. (What is another technique suggested in related work to simulate bugs?): sent11
    3.1. (Where has this technique been experimented with and what is its potential use in NLP?): sent12",Why do natural artifacts in training datasets cause spurious correlation bugs?,"sent1, sent3, sent4","Most of the papers in Table 1 experimented on training datasets with natural artifacts (AR), which cause spurious correlation bugs (i.e., the input texts having signals which are correlated to but not the reasons for specific outputs) and undermine models' generalizability.

For example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions.

However, because 20Newsgroups is a topic classification dataset, a better model should focus more on the topic of the content since the punctuation marks can also appear in other classes, especially when we apply the model to texts in the wild.","Training datasets with natural artifacts (AR) cause spurious correlation bugs, where input texts have signals that are correlated to but not the reasons for specific outputs, undermining models' generalizability.

For example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions.

A better model should focus more on the topic of the content since the punctuation marks can also appear in other classes, especially when applied to texts in the wild.","Questions:

Why do natural artifacts in training datasets cause spurious correlation bugs?

Answer:

Training datasets with natural artifacts (AR) cause spurious correlation bugs, where input texts have signals that are correlated to but not the reasons for specific outputs, undermining models' generalizability.

For example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions.

A better model should focus more on the topic of the content since the punctuation marks can also appear in other classes, especially when applied to texts in the wild."
Explanation-Based Human Debugging of NLP Models: A Survey,Bug Sources,"Most of the papers in Table 1 experimented on training datasets with natural artifacts (AR), which cause spurious correlation bugs (i.e., the input texts having signals which are correlated to but not the reasons for specific outputs) and undermine models' generalizability. Out of the 15 papers we surveyed, 5 used the 20Newsgroups dataset (Lang, 1995) as a case study, since it has lots of natural artifacts. For example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions. However, because 20Newsgroups is a topic classification dataset, a better model should focus more on the topic of the content since the punctuation marks can also appear in other classes, especially when we apply the model to texts in the wild. Apart from classification performance drops, natural artifacts can also cause model biases, as shown in (De-Arteaga et al., 2019;Park et al., 2018) and debugged in (Lertvittayakumjorn et al., 2020;Yao et al., 2021).

In the absence of strong natural artifacts, bugs can still be simulated using several techniques. First, using only a small subset of labeled data (SS) for training could cause the model to exploit spurious correlation leading to poor performance (Kulesza et al., 2010). Second, injecting wrong labels (WL) into the training data can obviously blunt the model quality (Koh and Liang, 2017). Third, using out-of-distribution tests (OD) can reveal that the model does not work effectively in the domains that it has not been trained on (Lertvittayakumjorn et al., 2020;Yao et al., 2021). All of these techniques give rise to undesirable model behaviors, requiring debugging. Another technique, not found in Table 1 but suggested in related work (Idahl et al., 2021), is contaminating input texts in the training data with decoys (i.e., injected artifacts) which could deceive the model into predicting for the wrong reasons. This has been experimented with in the computer vision domain (Rieger et al., 2020), and its use in the EBHD setting in NLP could be an interesting direction to explore.","# QUA Analysis:

1. (What are the sources of bugs in NLP models?): sent1
    1.1. (What kind of datasets cause spurious correlation bugs?): sent1
    1.2. (Which datasets were used in the surveyed papers and why?): sent2
        1.2.1. (What is an example of natural artifacts in the 20Newsgroups dataset?): sent3
        1.2.2. (Why should a better model focus on the topic of the content in the 20Newsgroups dataset?): sent4
    1.3. (What other issues can natural artifacts cause besides performance drops?): sent5
2. (How can bugs be simulated in the absence of strong natural artifacts?): sent6
    2.1. (What is one technique to simulate bugs?): sent7
    2.2. (What is another technique to simulate bugs?): sent8
    2.3. (What is a third technique to simulate bugs?): sent9
    2.4. (What is the consequence of using these techniques?): sent10
3. (What is another technique suggested in related work to simulate bugs?): sent11
    3.1. (Where has this technique been experimented with and what is its potential use in NLP?): sent12",Which datasets were used in the surveyed papers and why?,"sent2, sent3, sent4","Out of the 15 papers we surveyed, 5 used the 20Newsgroups dataset (Lang, 1995) as a case study, since it has lots of natural artifacts.

For example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions.

However, because 20Newsgroups is a topic classification dataset, a better model should focus more on the topic of the content since the punctuation marks can also appear in other classes, especially when we apply the model to texts in the wild.","Out of the 15 surveyed papers, 5 used the 20Newsgroups dataset (Lang, 1995) as a case study because it contains many natural artifacts.

For example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions.

However, because 20Newsgroups is a topic classification dataset, a better model should focus more on the topic of the content since the punctuation marks can also appear in other classes.","Questions:

Which datasets were used in the surveyed papers and why?

Answer:

Out of the 15 surveyed papers, 5 used the 20Newsgroups dataset (Lang, 1995) as a case study because it contains many natural artifacts.

For example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions.

However, because 20Newsgroups is a topic classification dataset, a better model should focus more on the topic of the content since the punctuation marks can also appear in other classes."
Explanation-Based Human Debugging of NLP Models: A Survey,Bug Sources,"Most of the papers in Table 1 experimented on training datasets with natural artifacts (AR), which cause spurious correlation bugs (i.e., the input texts having signals which are correlated to but not the reasons for specific outputs) and undermine models' generalizability. Out of the 15 papers we surveyed, 5 used the 20Newsgroups dataset (Lang, 1995) as a case study, since it has lots of natural artifacts. For example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions. However, because 20Newsgroups is a topic classification dataset, a better model should focus more on the topic of the content since the punctuation marks can also appear in other classes, especially when we apply the model to texts in the wild. Apart from classification performance drops, natural artifacts can also cause model biases, as shown in (De-Arteaga et al., 2019;Park et al., 2018) and debugged in (Lertvittayakumjorn et al., 2020;Yao et al., 2021).

In the absence of strong natural artifacts, bugs can still be simulated using several techniques. First, using only a small subset of labeled data (SS) for training could cause the model to exploit spurious correlation leading to poor performance (Kulesza et al., 2010). Second, injecting wrong labels (WL) into the training data can obviously blunt the model quality (Koh and Liang, 2017). Third, using out-of-distribution tests (OD) can reveal that the model does not work effectively in the domains that it has not been trained on (Lertvittayakumjorn et al., 2020;Yao et al., 2021). All of these techniques give rise to undesirable model behaviors, requiring debugging. Another technique, not found in Table 1 but suggested in related work (Idahl et al., 2021), is contaminating input texts in the training data with decoys (i.e., injected artifacts) which could deceive the model into predicting for the wrong reasons. This has been experimented with in the computer vision domain (Rieger et al., 2020), and its use in the EBHD setting in NLP could be an interesting direction to explore.","# QUA Analysis:

1. (What are the sources of bugs in NLP models?): sent1
    1.1. (What kind of datasets cause spurious correlation bugs?): sent1
    1.2. (Which datasets were used in the surveyed papers and why?): sent2
        1.2.1. (What is an example of natural artifacts in the 20Newsgroups dataset?): sent3
        1.2.2. (Why should a better model focus on the topic of the content in the 20Newsgroups dataset?): sent4
    1.3. (What other issues can natural artifacts cause besides performance drops?): sent5
2. (How can bugs be simulated in the absence of strong natural artifacts?): sent6
    2.1. (What is one technique to simulate bugs?): sent7
    2.2. (What is another technique to simulate bugs?): sent8
    2.3. (What is a third technique to simulate bugs?): sent9
    2.4. (What is the consequence of using these techniques?): sent10
3. (What is another technique suggested in related work to simulate bugs?): sent11
    3.1. (Where has this technique been experimented with and what is its potential use in NLP?): sent12",How can bugs be simulated in the absence of strong natural artifacts?,"sent6, sent7, sent8, sent9, sent10","In the absence of strong natural artifacts, bugs can still be simulated using several techniques.

First, using only a small subset of labeled data (SS) for training could cause the model to exploit spurious correlation leading to poor performance (Kulesza et al., 2010).

Second, injecting wrong labels (WL) into the training data can obviously blunt the model quality (Koh and Liang, 2017).

Third, using out-of-distribution tests (OD) can reveal that the model does not work effectively in the domains that it has not been trained on (Lertvittayakumjorn et al., 2020;Yao et al., 2021).

All of these techniques give rise to undesirable model behaviors, requiring debugging.","In the absence of strong natural artifacts, bugs can still be simulated using several techniques.

One technique involves using only a small subset of labeled data (SS) for training, which could cause the model to exploit spurious correlations, leading to poor performance (Kulesza et al., 2010).

Another technique involves injecting wrong labels (WL) into the training data, which can blunt the model quality (Koh and Liang, 2017).

Another technique involves using out-of-distribution tests (OD), which can reveal that the model does not work effectively in domains it has not been trained on (Lertvittayakumjorn et al., 2020; Yao et al., 2021).

All of these techniques give rise to undesirable model behaviors, requiring debugging.","Questions:

How can bugs be simulated in the absence of strong natural artifacts?

Answer:

In the absence of strong natural artifacts, bugs can still be simulated using several techniques.

One technique involves using only a small subset of labeled data (SS) for training, which could cause the model to exploit spurious correlations, leading to poor performance (Kulesza et al., 2010).

Another technique involves injecting wrong labels (WL) into the training data, which can blunt the model quality (Koh and Liang, 2017).

Another technique involves using out-of-distribution tests (OD), which can reveal that the model does not work effectively in domains it has not been trained on (Lertvittayakumjorn et al., 2020; Yao et al., 2021).

All of these techniques give rise to undesirable model behaviors, requiring debugging."
A Survey of Data Augmentation Approaches for NLP,Background,"What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","1. (What is data augmentation?): sent1
    1.1. (How does data augmentation work?): sent2
    1.2. (Where has data augmentation been commonly used?): sent3
    1.3. (What are the challenges of data augmentation in NLP?): sent4
2. (What are the goals and trade-offs?): sent5
    2.1. (What are the proposed techniques for DA in NLP?): sent6
    2.2. (What should an ideal DA technique achieve?): sent7
    2.3. (What are the trade-offs between different DA techniques?): sent8, sent9, sent10
    2.4. (What should be considered regarding the distribution of augmented data?): sent11, sent12
    2.5. (What should effective DA approaches aim for?): sent13
    2.6. (What is an example of a DA technique that balances trade-offs?): sent14
3. (What is the interpretation of DA?): sent15
    3.1. (Is there a lack of research on why DA works?): sent16
    3.2. (What is the nature of existing work on DA?): sent17
    3.3. (Where is this challenge discussed further?): sent18
    3.4. (What are some existing works on the theoretical principles of DA?): sent19, sent20, sent21, sent22",What are the goals and trade-offs of data augmentation in NLP?,"sent5, sent7, sent8, sent9, sent10, sent11, sent12, sent13","What are the goals and trade-offs?

As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance.

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective DA approaches should aim for a balance.","The goals and trade-offs of data augmentation in NLP involve balancing the increase in data diversity and model robustness with the potential introduction of noise and computational overhead.

An ideal data augmentation (DA) technique should be both easy to implement and improve model performance, as DA aims to provide an alternative to collecting more data.

Most offer trade-offs between these two. Rule-based techniques are easy to implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Additionally, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective DA approaches should aim for a balance.","Questions:

What are the goals and trade-offs of data augmentation in NLP?

Answer:

The goals and trade-offs of data augmentation in NLP involve balancing the increase in data diversity and model robustness with the potential introduction of noise and computational overhead.

An ideal data augmentation (DA) technique should be both easy to implement and improve model performance, as DA aims to provide an alternative to collecting more data.

Most offer trade-offs between these two. Rule-based techniques are easy to implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Additionally, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective DA approaches should aim for a balance."
A Survey of Data Augmentation Approaches for NLP,Background,"What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","1. (What is data augmentation?): sent1
    1.1. (How does data augmentation work?): sent2
    1.2. (Where has data augmentation been commonly used?): sent3
    1.3. (What are the challenges of data augmentation in NLP?): sent4
2. (What are the goals and trade-offs?): sent5
    2.1. (What are the proposed techniques for DA in NLP?): sent6
    2.2. (What should an ideal DA technique achieve?): sent7
    2.3. (What are the trade-offs between different DA techniques?): sent8, sent9, sent10
    2.4. (What should be considered regarding the distribution of augmented data?): sent11, sent12
    2.5. (What should effective DA approaches aim for?): sent13
    2.6. (What is an example of a DA technique that balances trade-offs?): sent14
3. (What is the interpretation of DA?): sent15
    3.1. (Is there a lack of research on why DA works?): sent16
    3.2. (What is the nature of existing work on DA?): sent17
    3.3. (Where is this challenge discussed further?): sent18
    3.4. (What are some existing works on the theoretical principles of DA?): sent19, sent20, sent21, sent22",What are the proposed techniques for data augmentation in NLP?,"sent6, sent8, sent9, sent10","Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.","Many data augmentation techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

Most offer trade-offs between these two. Rule-based techniques are easy to implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.","Questions:

What are the proposed techniques for data augmentation in NLP?

Answer:

Many data augmentation techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).

Most offer trade-offs between these two. Rule-based techniques are easy to implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.

Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize."
A Survey of Data Augmentation Approaches for NLP,Background,"What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","1. (What is data augmentation?): sent1
    1.1. (How does data augmentation work?): sent2
    1.2. (Where has data augmentation been commonly used?): sent3
    1.3. (What are the challenges of data augmentation in NLP?): sent4
2. (What are the goals and trade-offs?): sent5
    2.1. (What are the proposed techniques for DA in NLP?): sent6
    2.2. (What should an ideal DA technique achieve?): sent7
    2.3. (What are the trade-offs between different DA techniques?): sent8, sent9, sent10
    2.4. (What should be considered regarding the distribution of augmented data?): sent11, sent12
    2.5. (What should effective DA approaches aim for?): sent13
    2.6. (What is an example of a DA technique that balances trade-offs?): sent14
3. (What is the interpretation of DA?): sent15
    3.1. (Is there a lack of research on why DA works?): sent16
    3.2. (What is the nature of existing work on DA?): sent17
    3.3. (Where is this challenge discussed further?): sent18
    3.4. (What are some existing works on the theoretical principles of DA?): sent19, sent20, sent21, sent22",What should be considered regarding the distribution of augmented data?,"sent11, sent12, sent13","Further, the distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective DA approaches should aim for a balance.","The distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective data augmentation (DA) approaches should aim for a balance.","Questions:

What should be considered regarding the distribution of augmented data?

Answer:

The distribution of augmented data should neither be too similar nor too different from the original.

This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.

Effective data augmentation (DA) approaches should aim for a balance."
A Survey of Data Augmentation Approaches for NLP,Background,"What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","1. (What is data augmentation?): sent1
    1.1. (How does data augmentation work?): sent2
    1.2. (Where has data augmentation been commonly used?): sent3
    1.3. (What are the challenges of data augmentation in NLP?): sent4
2. (What are the goals and trade-offs?): sent5
    2.1. (What are the proposed techniques for DA in NLP?): sent6
    2.2. (What should an ideal DA technique achieve?): sent7
    2.3. (What are the trade-offs between different DA techniques?): sent8, sent9, sent10
    2.4. (What should be considered regarding the distribution of augmented data?): sent11, sent12
    2.5. (What should effective DA approaches aim for?): sent13
    2.6. (What is an example of a DA technique that balances trade-offs?): sent14
3. (What is the interpretation of DA?): sent15
    3.1. (Is there a lack of research on why DA works?): sent16
    3.2. (What is the nature of existing work on DA?): sent17
    3.3. (Where is this challenge discussed further?): sent18
    3.4. (What are some existing works on the theoretical principles of DA?): sent19, sent20, sent21, sent22",What is an example of a DA technique that balances trade-offs?,"sent14, sent7, sent8, sent9","Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance.

Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.","Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

An ideal DA technique should be both easy to implement and improve model performance.

Most offer trade-offs between these two. Rule-based techniques are easy to implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.","Questions:

What is an example of a DA technique that balances trade-offs?

Answer:

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

An ideal DA technique should be both easy to implement and improve model performance.

Most offer trade-offs between these two. Rule-based techniques are easy to implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b).

Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts."
A Survey of Data Augmentation Approaches for NLP,Background,"What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","1. (What is data augmentation?): sent1
    1.1. (How does data augmentation work?): sent2
    1.2. (Where has data augmentation been commonly used?): sent3
    1.3. (What are the challenges of data augmentation in NLP?): sent4
2. (What are the goals and trade-offs?): sent5
    2.1. (What are the proposed techniques for DA in NLP?): sent6
    2.2. (What should an ideal DA technique achieve?): sent7
    2.3. (What are the trade-offs between different DA techniques?): sent8, sent9, sent10
    2.4. (What should be considered regarding the distribution of augmented data?): sent11, sent12
    2.5. (What should effective DA approaches aim for?): sent13
    2.6. (What is an example of a DA technique that balances trade-offs?): sent14
3. (What is the interpretation of DA?): sent15
    3.1. (Is there a lack of research on why DA works?): sent16
    3.2. (What is the nature of existing work on DA?): sent17
    3.3. (Where is this challenge discussed further?): sent18
    3.4. (What are some existing works on the theoretical principles of DA?): sent19, sent20, sent21, sent22",Is there a lack of research on why data augmentation works?,"sent16, sent17, sent18","Overall, there indeed appears to be a lack of research on why exactly DA works.

Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles.

We discuss this challenge more in §6, and highlight some of the existing work below.","There indeed appears to be a lack of research on why exactly data augmentation works.

Existing work on this topic is mainly surface-level and rarely investigates the theoretical underpinnings and principles.

This challenge is discussed in more detail in existing work.","Questions:

Is there a lack of research on why data augmentation works?

Answer:

There indeed appears to be a lack of research on why exactly data augmentation works.

Existing work on this topic is mainly surface-level and rarely investigates the theoretical underpinnings and principles.

This challenge is discussed in more detail in existing work."
A Survey of Data Augmentation Approaches for NLP,Background,"What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","1. (What is data augmentation?): sent1
    1.1. (How does data augmentation work?): sent2
    1.2. (Where has data augmentation been commonly used?): sent3
    1.3. (What are the challenges of data augmentation in NLP?): sent4
2. (What are the goals and trade-offs?): sent5
    2.1. (What are the proposed techniques for DA in NLP?): sent6
    2.2. (What should an ideal DA technique achieve?): sent7
    2.3. (What are the trade-offs between different DA techniques?): sent8, sent9, sent10
    2.4. (What should be considered regarding the distribution of augmented data?): sent11, sent12
    2.5. (What should effective DA approaches aim for?): sent13
    2.6. (What is an example of a DA technique that balances trade-offs?): sent14
3. (What is the interpretation of DA?): sent15
    3.1. (Is there a lack of research on why DA works?): sent16
    3.2. (What is the nature of existing work on DA?): sent17
    3.3. (Where is this challenge discussed further?): sent18
    3.4. (What are some existing works on the theoretical principles of DA?): sent19, sent20, sent21, sent22",What are some existing works on the theoretical principles of data augmentation?,"sent19, sent20, sent21, sent22","Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2).

Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods.

Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization.

Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","Bishop (1995) shows that training with noised examples is reducible to Tikhonov regularization, which subsumes L2.

Rajput et al. (2019) show that data augmentation can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common data augmentation methods.

Dao et al. (2019) conceptualize data augmentation transformations as kernels and identify two benefits: averaging of features and variance regularization.

Chen et al. (2020d) show that data augmentation leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","Questions:

What are some existing works on the theoretical principles of data augmentation?

Answer:

Bishop (1995) shows that training with noised examples is reducible to Tikhonov regularization, which subsumes L2.

Rajput et al. (2019) show that data augmentation can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common data augmentation methods.

Dao et al. (2019) conceptualize data augmentation transformations as kernels and identify two benefits: averaging of features and variance regularization.

Chen et al. (2020d) show that data augmentation leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant."
A Survey of Data Augmentation Approaches for NLP,Example Interpolation Techniques,"Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples. This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA). Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).

Another class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements. Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Şahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism. Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020). For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see ""Multimodal challenges"" in §6).

A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs. This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c). Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others. SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α). The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a).","# QUA Analysis:

1. What is another class of DA techniques pioneered by MIXUP?
    1.1. What is this class of techniques sometimes referred to as?
        - sent2
    1.2. What has ensuing work explored in this class of techniques?
        - sent3
    1.3. What are some examples of extensions of MIXUP in the vision community?
        - sent4
        1.3.1. What does Figure 2 illustrate in relation to this class of techniques?
            - sent5
        1.3.2. What are some examples of this paradigm?
            - sent6
            1.3.2.1. How does CUTMIX work as an example of this paradigm?
                - sent7
    1.4. What is the potential for borrowing ideas from these works for NLP?
        - sent8
    1.5. What was a bottleneck to using MIXUP for NLP tasks?
        - sent9
        1.5.1. How has this bottleneck been overcome?
            - sent10
        1.5.2. What are some later variants of MIXUP for NLP tasks?
            - sent11
        1.5.3. How does SEQ2MIXUP generalize MIXUP for sequence transduction tasks?
            - sent12
            1.5.3.1. Which version of SEQ2MIXUP is found to outperform the other?
                - sent13",What are some examples of extensions of MIXUP in the vision community?,"sent4, sent6, sent7","Another class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements.

Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020).

For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes.","Another class of extensions of MIXUP in the vision community attempts to fuse raw input image pairs together into a single image.

Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017), and COPY-PASTE (Ghiasi et al., 2020).

For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes.","Questions:

What are some examples of extensions of MIXUP in the vision community?

Answer:

Another class of extensions of MIXUP in the vision community attempts to fuse raw input image pairs together into a single image.

Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017), and COPY-PASTE (Ghiasi et al., 2020).

For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes."
A Survey of Data Augmentation Approaches for NLP,Example Interpolation Techniques,"Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples. This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA). Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).

Another class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements. Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Şahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism. Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020). For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see ""Multimodal challenges"" in §6).

A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs. This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c). Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others. SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α). The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a).","# QUA Analysis:

1. What is another class of DA techniques pioneered by MIXUP?
    1.1. What is this class of techniques sometimes referred to as?
        - sent2
    1.2. What has ensuing work explored in this class of techniques?
        - sent3
    1.3. What are some examples of extensions of MIXUP in the vision community?
        - sent4
        1.3.1. What does Figure 2 illustrate in relation to this class of techniques?
            - sent5
        1.3.2. What are some examples of this paradigm?
            - sent6
            1.3.2.1. How does CUTMIX work as an example of this paradigm?
                - sent7
    1.4. What is the potential for borrowing ideas from these works for NLP?
        - sent8
    1.5. What was a bottleneck to using MIXUP for NLP tasks?
        - sent9
        1.5.1. How has this bottleneck been overcome?
            - sent10
        1.5.2. What are some later variants of MIXUP for NLP tasks?
            - sent11
        1.5.3. How does SEQ2MIXUP generalize MIXUP for sequence transduction tasks?
            - sent12
            1.5.3.1. Which version of SEQ2MIXUP is found to outperform the other?
                - sent13",What has ensuing work explored in the class of techniques pioneered by MIXUP?,"sent3, sent11, sent12","Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).

Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others.

SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α).","Ensuing work has explored interpolating inner components (Verma et al., 2019; Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).

Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others.

SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways: the ""hard"" version samples a binary mask and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient.","Questions:

What has ensuing work explored in the class of techniques pioneered by MIXUP?

Answer:

Ensuing work has explored interpolating inner components (Verma et al., 2019; Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).

Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others.

SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways: the ""hard"" version samples a binary mask and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient."
A Survey of Data Augmentation Approaches for NLP,Example Interpolation Techniques,"Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples. This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA). Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).

Another class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements. Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Şahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism. Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020). For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see ""Multimodal challenges"" in §6).

A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs. This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c). Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others. SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α). The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a).","# QUA Analysis:

1. What is another class of DA techniques pioneered by MIXUP?
    1.1. What is this class of techniques sometimes referred to as?
        - sent2
    1.2. What has ensuing work explored in this class of techniques?
        - sent3
    1.3. What are some examples of extensions of MIXUP in the vision community?
        - sent4
        1.3.1. What does Figure 2 illustrate in relation to this class of techniques?
            - sent5
        1.3.2. What are some examples of this paradigm?
            - sent6
            1.3.2.1. How does CUTMIX work as an example of this paradigm?
                - sent7
    1.4. What is the potential for borrowing ideas from these works for NLP?
        - sent8
    1.5. What was a bottleneck to using MIXUP for NLP tasks?
        - sent9
        1.5.1. How has this bottleneck been overcome?
            - sent10
        1.5.2. What are some later variants of MIXUP for NLP tasks?
            - sent11
        1.5.3. How does SEQ2MIXUP generalize MIXUP for sequence transduction tasks?
            - sent12
            1.5.3.1. Which version of SEQ2MIXUP is found to outperform the other?
                - sent13",What are some later variants of MIXUP for NLP tasks?,"sent11, sent12, sent13","Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others.

SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α).

The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a).","Later variants of MIXUP for NLP tasks propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others.

SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways: the ""hard"" version samples a binary mask and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient.

The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a).","Questions:

What are some later variants of MIXUP for NLP tasks?

Answer:

Later variants of MIXUP for NLP tasks propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others.

SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways: the ""hard"" version samples a binary mask and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient.

The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a)."
A Survey of Data Augmentation Approaches for NLP,Rule-Based Techniques,"Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model components. Feature space DA approaches generate augmented examples in the model's feature space rather than input data. Many few-shot learning approaches (Hariharan and Girshick, 2017;Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes (see §4.4). Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold. Wei and Zou (2019) propose EASY DATA AUG-MENTATION (EDA), a set of token-level random perturbation operations including random insertion, deletion, and swap. They show improved performance on many text classification tasks. UDA (Xie et al., 2020) show how supervised DA methods can be exploited for unsupervised data through consistency training on (x, DA(x)) pairs.

For paraphrase identification, Chen et al. (2020b) construct a signed graph over the data, with individual sentences as nodes and pair labels as signed edges. They use balance theory and transitivity to infer augmented sentence pairs from this graph. Motivated by image cropping and rotation, Şahin and Steedman (2018) propose dependency tree morphing. For dependency-annotated sentences, children of the same parent are swapped (à la rotation) or some deleted (à la cropping), as seen in Figure 2. This is most beneficial for language families with rich case marking systems (e.g. Baltic and Slavic).","1. (What are DA primitives covered in this section?): sent1
    1.1. (What are feature space DA approaches?): sent2
        1.1.1. (How do few-shot learning approaches leverage feature space transformations?): sent3
        1.1.2. (What specific method does Paschali et al. (2019) use for feature space DA?): sent4
    1.2. (What are token-level random perturbation operations?): sent5
        1.2.1. (What is the impact of EDA on text classification tasks?): sent6
    1.3. (How can supervised DA methods be used for unsupervised data?): sent7
    1.4. (How is a signed graph used for paraphrase identification?): sent8
        1.4.1. (How do they infer augmented sentence pairs from the graph?): sent9
    1.5. (What is dependency tree morphing?): sent10
        1.5.1. (How is dependency tree morphing applied to dependency-annotated sentences?): sent12
            1.5.1.1. (For which language families is this most beneficial?): sent13",What are the key methods for feature space data augmentation in few-shot learning?,"sent2, sent3, sent4","Feature space DA approaches generate augmented examples in the model's feature space rather than input data.

Many few-shot learning approaches (Hariharan and Girshick, 2017;Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes (see §4.4).

Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold.","Feature space data augmentation (DA) approaches generate augmented examples in the model's feature space rather than input data.

Many few-shot learning approaches (Hariharan and Girshick, 2017; Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes.

Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold.","Questions:

What are the key methods for feature space data augmentation in few-shot learning?

Answer:

Feature space data augmentation (DA) approaches generate augmented examples in the model's feature space rather than input data.

Many few-shot learning approaches (Hariharan and Girshick, 2017; Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes.

Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold."
A Survey of Data Augmentation Approaches for NLP,Rule-Based Techniques,"Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model components. Feature space DA approaches generate augmented examples in the model's feature space rather than input data. Many few-shot learning approaches (Hariharan and Girshick, 2017;Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes (see §4.4). Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold. Wei and Zou (2019) propose EASY DATA AUG-MENTATION (EDA), a set of token-level random perturbation operations including random insertion, deletion, and swap. They show improved performance on many text classification tasks. UDA (Xie et al., 2020) show how supervised DA methods can be exploited for unsupervised data through consistency training on (x, DA(x)) pairs.

For paraphrase identification, Chen et al. (2020b) construct a signed graph over the data, with individual sentences as nodes and pair labels as signed edges. They use balance theory and transitivity to infer augmented sentence pairs from this graph. Motivated by image cropping and rotation, Şahin and Steedman (2018) propose dependency tree morphing. For dependency-annotated sentences, children of the same parent are swapped (à la rotation) or some deleted (à la cropping), as seen in Figure 2. This is most beneficial for language families with rich case marking systems (e.g. Baltic and Slavic).","1. (What are DA primitives covered in this section?): sent1
    1.1. (What are feature space DA approaches?): sent2
        1.1.1. (How do few-shot learning approaches leverage feature space transformations?): sent3
        1.1.2. (What specific method does Paschali et al. (2019) use for feature space DA?): sent4
    1.2. (What are token-level random perturbation operations?): sent5
        1.2.1. (What is the impact of EDA on text classification tasks?): sent6
    1.3. (How can supervised DA methods be used for unsupervised data?): sent7
    1.4. (How is a signed graph used for paraphrase identification?): sent8
        1.4.1. (How do they infer augmented sentence pairs from the graph?): sent9
    1.5. (What is dependency tree morphing?): sent10
        1.5.1. (How is dependency tree morphing applied to dependency-annotated sentences?): sent12
            1.5.1.1. (For which language families is this most beneficial?): sent13",What is dependency tree morphing and how is it applied to dependency-annotated sentences?,"sent10, sent12, sent13","Motivated by image cropping and rotation, Şahin and

For dependency-annotated sentences, children of the same parent are swapped (à la rotation) or some deleted (à la cropping), as seen in Figure 2.

This is most beneficial for language families with rich case marking systems (e.g. Baltic and Slavic).","Dependency tree morphing, motivated by image cropping and rotation, is applied to dependency-annotated sentences.

For dependency-annotated sentences, children of the same parent are swapped or some are deleted.

This approach is particularly beneficial for language families with rich case marking systems, such as Baltic and Slavic languages.","Questions:

What is dependency tree morphing and how is it applied to dependency-annotated sentences?

Answer:

Dependency tree morphing, motivated by image cropping and rotation, is applied to dependency-annotated sentences.

For dependency-annotated sentences, children of the same parent are swapped or some are deleted.

This approach is particularly beneficial for language families with rich case marking systems, such as Baltic and Slavic languages."
Towards Argument Mining for Social Good: A Survey,NLP-Supported Moderation: desiderata and challenges,"NLP-supported moderation represents a clear solution to the bottleneck problem affecting facilitation in digital democracy. Automatic tools can take over some of the tasks that human moderators typically perform when monitoring online discussions. For example, in Social Sciences, one of the most discussed issues in crowd-scale deliberation is ""flaming"", i.e., aggressive and disrespectful communicative behavior (Lampe et al., 2014). Here, moderators could benefit from hate-speech and trolling detection methods in NLP. NLP methods to support deliberative decisionmaking have already been applied for the realtime visualisation of argument maps (El-Assady et al., 2017). Deliberation in real-time applications has the clear potential of structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).

Beyond the real-time support to users (and moderators) provided by the methods described above, further tasks specific to AM which are part of the role of a human or (semi-)automoated moderator include: detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), relevance estimation (Potthast et al., 2019). In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion. Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b) which has a counterpart in the framing detection task (Card et al., 2015;Akyürek et al., 2020), as framing is a manipulation strategy which highlights specific aspects of an issue under discussion to promote certain interpretations.

Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012;Waseem and Hovy, 2016;Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014;Kotonya and Toni, 2020), Facts recognition and source identification (Dusmanu et al., 2017).

How to represent discourse? Thus far, we have discussed the main ingredients of a rich NLP-informed approach to deliberative discourse. These components, together with the deliberationaugmented definition of AQ sketched in section 3 are the features that the NLP moderator takes as an input. One question remains open: How to represent the argumentative discourse within a contribution (e.g. a forum post) and across contributions (e.g. an entire online deliberation campaign)? We can approach also this question from an interdisciplinary perspective. Reference work in political science aims at modeling the mechanisms of political discourse in forms of discourse networks, as defined in Leifeld (2017). A discourse network is a bipartite graph, containing two classes of nodes: actors (e.g. Angela Merkel; the left-wing party; etc.) and claims (e.g. housing opportunities should be established for refugees); Edges between actors and claims indicate the support or opposition of a certain actor to a specific claim. Discourse coalitions (Hajer, 1993) and argumentative clusters are the projection of the affiliation network on the actor and claim sides of the network (Leifeld and Haunss, 2012;Haunss et al., 2013). Recent NLP research has targeted integration machine learning in the discourse network analysis workflow (Padó et al., 2019;Haunss et al., 2020). Crucially for AM, discourse networks can integrate claims and actors with a third class of nodes, the frame nodes, which encode the reason put forward by an actor to support or reject a claim. This type of representation is perfectly compatible with a graph-based approach on argument representation which has already been established as to be preferred to a tree-structure representation both empirically (Niculae et al., 2017) and theoretically (Afantenos and Asher, 2014).

Moderation can thus be modeled as optimization of specific quantitative properties of the discourse network: participant inclusion, can be enforced by ensuring that the contributions of peripheric actor nodes receive the deserved salience; argument mapping and summarization can be modeled by identifying ""hot"" sub-graphs in the network; the impact of a contribution (the grounded notion of AQ we have been advocating thus far) can be quantified as the perturbation introduced in the network, with its long term effects on convergence or polarization.

Who moderates the (NLP) moderators? The problem of biased moderation obviously relates to the issue of bias in NLP (Blodgett et al., 2020;Caliskan et al., 2017;Bolukbasi et al., 2016;Spliethöver and Wachsmuth, 2020) and it has a clear implication in the application of NLP methods to moderation. For example, we would not want our NLP models to infer a negative impact on AQ from cues which just reveal that the user belongs to certain groups. This is a real risk when quality is equated to ""success"", in turn quantified in terms of likes, replies, retweets. The public of a forum may be sensitive to such cues, but the moderator should be unbiased with respect to them. Another source of bias is the degree of literacy of a contribution: while users who express themselves poorly are likely to be less popular with the forum public, their contributions may still be a very good move in the ""cooperation challenge"" -one that moderators (NLP or humans, online or in-person) have to ensure will not be left unexploited.","# QUA Analysis:

1. (What is NLP-supported moderation?): sent1
    1.1. (How can automatic tools assist in moderation?): sent2
        1.1.1. (What is an example of an issue in crowd-scale deliberation?): sent3
        1.1.2. (How can NLP help with issues like ""flaming""?): sent4
    1.2. (What are some applications of NLP in deliberative decision-making?): sent5
        1.2.1. (What is the potential of real-time applications in deliberation?): sent6
        1.2.2. (What further tasks can NLP support in moderation?): sent7
            1.2.2.1. (How can detecting and highlighting parts of an argument help?): sent8
            1.2.2.2. (What is another important source for moderation?): sent9
            1.2.2.3. (What other NLP tasks are crucial for healthy interaction?): sent10
2. (What has been discussed so far?): sent11
    2.1. (What are the features that the NLP moderator takes as input?): sent12
3. (What question remains open?): sent13
    3.1. (How can we approach the representation of argumentative discourse?): sent14
        3.1.1. (What is the reference work in political science for modeling political discourse?): sent15
            3.1.1.1. (What is a discourse network?): sent16
            3.1.1.2. (What are discourse coalitions and argumentative clusters?): sent17
            3.1.1.3. (How has recent NLP research integrated machine learning in discourse network analysis?): sent18
            3.1.1.4. (How can discourse networks integrate claims and actors?): sent19
            3.1.1.5. (Why is a graph-based approach preferred for argument representation?): sent20
            3.1.1.6. (How can moderation be modeled in discourse networks?): sent21
4. (Who moderates the (NLP) moderators?): sent22
    4.1. (How does the problem of biased moderation relate to bias in NLP?): sent23
        4.1.1. (What is an example of a risk in biased NLP models?): sent24
        4.1.2. (What is a real risk when quality is equated to ""success""?): sent25
        4.1.3. (How should moderators handle cues that the public may be sensitive to?): sent26
        4.1.4. (What is another source of bias in moderation?): sent27",How can NLP-supported moderation address the bottleneck problem in digital democracy?,"sent1, sent2, sent4, sent5, sent6, sent7, sent8, sent9, sent10","NLP-supported moderation represents a clear solution to the bottleneck problem affecting facilitation in digital democracy.

Automatic tools can take over some of the tasks that human moderators typically perform when monitoring online discussions.

Here, moderators could benefit from hate-speech and trolling detection methods in NLP.

NLP methods to support deliberative decisionmaking have already been applied for the realtime visualisation of argument maps (El-Assady et al., 2017).

Deliberation in real-time applications has the clear potential of structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).

Beyond the real-time support to users (and moderators) provided by the methods described above, further tasks specific to AM which are part of the role of a human or (semi-)automoated moderator include: detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), relevance estimation (Potthast et al., 2019).

In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion.

Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b) which has a counterpart in the framing detection task (Card et al., 2015;Akyürek et al., 2020), as framing is a manipulation strategy which highlights specific aspects of an issue under discussion to promote certain interpretations.

Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012;Waseem and Hovy, 2016;Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014;Kotonya and Toni, 2020), Facts recognition and source identification (Dusmanu et al., 2017).How to represent discourse?","NLP-supported moderation represents a clear solution to the bottleneck problem affecting facilitation in digital democracy.

Automatic tools can take over some of the tasks that human moderators typically perform when monitoring online discussions.

Moderators could benefit from hate-speech and trolling detection methods in NLP.

NLP methods to support deliberative decision-making have already been applied for the real-time visualization of argument maps (El-Assady et al., 2017).

Deliberation in real-time applications has the potential for structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).

Beyond the real-time support to users and moderators provided by the methods described above, further tasks specific to argument mining (AM) that are part of the role of a human or semi-automated moderator include detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), and relevance estimation (Potthast et al., 2019).

In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion.

Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b), which has a counterpart in the framing detection task (Card et al., 2015; Akyürek et al., 2020), as framing is a manipulation strategy that highlights specific aspects of an issue under discussion to promote certain interpretations.

Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014; Kotonya and Toni, 2020), and Facts recognition and source identification (Dusmanu et al., 2017).","Questions:

How can NLP-supported moderation address the bottleneck problem in digital democracy?

Answer:

NLP-supported moderation represents a clear solution to the bottleneck problem affecting facilitation in digital democracy.

Automatic tools can take over some of the tasks that human moderators typically perform when monitoring online discussions.

Moderators could benefit from hate-speech and trolling detection methods in NLP.

NLP methods to support deliberative decision-making have already been applied for the real-time visualization of argument maps (El-Assady et al., 2017).

Deliberation in real-time applications has the potential for structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).

Beyond the real-time support to users and moderators provided by the methods described above, further tasks specific to argument mining (AM) that are part of the role of a human or semi-automated moderator include detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), and relevance estimation (Potthast et al., 2019).

In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion.

Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b), which has a counterpart in the framing detection task (Card et al., 2015; Akyürek et al., 2020), as framing is a manipulation strategy that highlights specific aspects of an issue under discussion to promote certain interpretations.

Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014; Kotonya and Toni, 2020), and Facts recognition and source identification (Dusmanu et al., 2017)."
Towards Argument Mining for Social Good: A Survey,NLP-Supported Moderation: desiderata and challenges,"NLP-supported moderation represents a clear solution to the bottleneck problem affecting facilitation in digital democracy. Automatic tools can take over some of the tasks that human moderators typically perform when monitoring online discussions. For example, in Social Sciences, one of the most discussed issues in crowd-scale deliberation is ""flaming"", i.e., aggressive and disrespectful communicative behavior (Lampe et al., 2014). Here, moderators could benefit from hate-speech and trolling detection methods in NLP. NLP methods to support deliberative decisionmaking have already been applied for the realtime visualisation of argument maps (El-Assady et al., 2017). Deliberation in real-time applications has the clear potential of structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).

Beyond the real-time support to users (and moderators) provided by the methods described above, further tasks specific to AM which are part of the role of a human or (semi-)automoated moderator include: detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), relevance estimation (Potthast et al., 2019). In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion. Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b) which has a counterpart in the framing detection task (Card et al., 2015;Akyürek et al., 2020), as framing is a manipulation strategy which highlights specific aspects of an issue under discussion to promote certain interpretations.

Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012;Waseem and Hovy, 2016;Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014;Kotonya and Toni, 2020), Facts recognition and source identification (Dusmanu et al., 2017).

How to represent discourse? Thus far, we have discussed the main ingredients of a rich NLP-informed approach to deliberative discourse. These components, together with the deliberationaugmented definition of AQ sketched in section 3 are the features that the NLP moderator takes as an input. One question remains open: How to represent the argumentative discourse within a contribution (e.g. a forum post) and across contributions (e.g. an entire online deliberation campaign)? We can approach also this question from an interdisciplinary perspective. Reference work in political science aims at modeling the mechanisms of political discourse in forms of discourse networks, as defined in Leifeld (2017). A discourse network is a bipartite graph, containing two classes of nodes: actors (e.g. Angela Merkel; the left-wing party; etc.) and claims (e.g. housing opportunities should be established for refugees); Edges between actors and claims indicate the support or opposition of a certain actor to a specific claim. Discourse coalitions (Hajer, 1993) and argumentative clusters are the projection of the affiliation network on the actor and claim sides of the network (Leifeld and Haunss, 2012;Haunss et al., 2013). Recent NLP research has targeted integration machine learning in the discourse network analysis workflow (Padó et al., 2019;Haunss et al., 2020). Crucially for AM, discourse networks can integrate claims and actors with a third class of nodes, the frame nodes, which encode the reason put forward by an actor to support or reject a claim. This type of representation is perfectly compatible with a graph-based approach on argument representation which has already been established as to be preferred to a tree-structure representation both empirically (Niculae et al., 2017) and theoretically (Afantenos and Asher, 2014).

Moderation can thus be modeled as optimization of specific quantitative properties of the discourse network: participant inclusion, can be enforced by ensuring that the contributions of peripheric actor nodes receive the deserved salience; argument mapping and summarization can be modeled by identifying ""hot"" sub-graphs in the network; the impact of a contribution (the grounded notion of AQ we have been advocating thus far) can be quantified as the perturbation introduced in the network, with its long term effects on convergence or polarization.

Who moderates the (NLP) moderators? The problem of biased moderation obviously relates to the issue of bias in NLP (Blodgett et al., 2020;Caliskan et al., 2017;Bolukbasi et al., 2016;Spliethöver and Wachsmuth, 2020) and it has a clear implication in the application of NLP methods to moderation. For example, we would not want our NLP models to infer a negative impact on AQ from cues which just reveal that the user belongs to certain groups. This is a real risk when quality is equated to ""success"", in turn quantified in terms of likes, replies, retweets. The public of a forum may be sensitive to such cues, but the moderator should be unbiased with respect to them. Another source of bias is the degree of literacy of a contribution: while users who express themselves poorly are likely to be less popular with the forum public, their contributions may still be a very good move in the ""cooperation challenge"" -one that moderators (NLP or humans, online or in-person) have to ensure will not be left unexploited.","# QUA Analysis:

1. (What is NLP-supported moderation?): sent1
    1.1. (How can automatic tools assist in moderation?): sent2
        1.1.1. (What is an example of an issue in crowd-scale deliberation?): sent3
        1.1.2. (How can NLP help with issues like ""flaming""?): sent4
    1.2. (What are some applications of NLP in deliberative decision-making?): sent5
        1.2.1. (What is the potential of real-time applications in deliberation?): sent6
        1.2.2. (What further tasks can NLP support in moderation?): sent7
            1.2.2.1. (How can detecting and highlighting parts of an argument help?): sent8
            1.2.2.2. (What is another important source for moderation?): sent9
            1.2.2.3. (What other NLP tasks are crucial for healthy interaction?): sent10
2. (What has been discussed so far?): sent11
    2.1. (What are the features that the NLP moderator takes as input?): sent12
3. (What question remains open?): sent13
    3.1. (How can we approach the representation of argumentative discourse?): sent14
        3.1.1. (What is the reference work in political science for modeling political discourse?): sent15
            3.1.1.1. (What is a discourse network?): sent16
            3.1.1.2. (What are discourse coalitions and argumentative clusters?): sent17
            3.1.1.3. (How has recent NLP research integrated machine learning in discourse network analysis?): sent18
            3.1.1.4. (How can discourse networks integrate claims and actors?): sent19
            3.1.1.5. (Why is a graph-based approach preferred for argument representation?): sent20
            3.1.1.6. (How can moderation be modeled in discourse networks?): sent21
4. (Who moderates the (NLP) moderators?): sent22
    4.1. (How does the problem of biased moderation relate to bias in NLP?): sent23
        4.1.1. (What is an example of a risk in biased NLP models?): sent24
        4.1.2. (What is a real risk when quality is equated to ""success""?): sent25
        4.1.3. (How should moderators handle cues that the public may be sensitive to?): sent26
        4.1.4. (What is another source of bias in moderation?): sent27",What are the key NLP methods for supporting real-time deliberative decision-making?,"sent5, sent6, sent7, sent8, sent9, sent10","NLP methods to support deliberative decisionmaking have already been applied for the realtime visualisation of argument maps (El-Assady et al., 2017).

Deliberation in real-time applications has the clear potential of structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).

Beyond the real-time support to users (and moderators) provided by the methods described above, further tasks specific to AM which are part of the role of a human or (semi-)automoated moderator include: detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), relevance estimation (Potthast et al., 2019).

In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion.

Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b) which has a counterpart in the framing detection task (Card et al., 2015;Akyürek et al., 2020), as framing is a manipulation strategy which highlights specific aspects of an issue under discussion to promote certain interpretations.

Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012;Waseem and Hovy, 2016;Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014;Kotonya and Toni, 2020), Facts recognition and source identification (Dusmanu et al., 2017).How to represent discourse?","NLP methods to support deliberative decision-making have been applied for the real-time visualization of argument maps (El-Assady et al., 2017).

Deliberation in real-time applications has the potential for structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).

Further tasks specific to argument mining (AM) that are part of the role of a human or (semi-)automated moderator include detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), and relevance estimation (Potthast et al., 2019).

In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion.

Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b), which has a counterpart in the framing detection task (Card et al., 2015; Akyürek et al., 2020), as framing is a manipulation strategy that highlights specific aspects of an issue under discussion to promote certain interpretations.

Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014; Kotonya and Toni, 2020), and Facts recognition and source identification (Dusmanu et al., 2017).","Questions:

What are the key NLP methods for supporting real-time deliberative decision-making?

Answer:

NLP methods to support deliberative decision-making have been applied for the real-time visualization of argument maps (El-Assady et al., 2017).

Deliberation in real-time applications has the potential for structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).

Further tasks specific to argument mining (AM) that are part of the role of a human or (semi-)automated moderator include detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), and relevance estimation (Potthast et al., 2019).

In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion.

Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b), which has a counterpart in the framing detection task (Card et al., 2015; Akyürek et al., 2020), as framing is a manipulation strategy that highlights specific aspects of an issue under discussion to promote certain interpretations.

Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014; Kotonya and Toni, 2020), and Facts recognition and source identification (Dusmanu et al., 2017)."
Towards Argument Mining for Social Good: A Survey,NLP-Supported Moderation: desiderata and challenges,"NLP-supported moderation represents a clear solution to the bottleneck problem affecting facilitation in digital democracy. Automatic tools can take over some of the tasks that human moderators typically perform when monitoring online discussions. For example, in Social Sciences, one of the most discussed issues in crowd-scale deliberation is ""flaming"", i.e., aggressive and disrespectful communicative behavior (Lampe et al., 2014). Here, moderators could benefit from hate-speech and trolling detection methods in NLP. NLP methods to support deliberative decisionmaking have already been applied for the realtime visualisation of argument maps (El-Assady et al., 2017). Deliberation in real-time applications has the clear potential of structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).

Beyond the real-time support to users (and moderators) provided by the methods described above, further tasks specific to AM which are part of the role of a human or (semi-)automoated moderator include: detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), relevance estimation (Potthast et al., 2019). In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion. Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b) which has a counterpart in the framing detection task (Card et al., 2015;Akyürek et al., 2020), as framing is a manipulation strategy which highlights specific aspects of an issue under discussion to promote certain interpretations.

Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012;Waseem and Hovy, 2016;Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014;Kotonya and Toni, 2020), Facts recognition and source identification (Dusmanu et al., 2017).

How to represent discourse? Thus far, we have discussed the main ingredients of a rich NLP-informed approach to deliberative discourse. These components, together with the deliberationaugmented definition of AQ sketched in section 3 are the features that the NLP moderator takes as an input. One question remains open: How to represent the argumentative discourse within a contribution (e.g. a forum post) and across contributions (e.g. an entire online deliberation campaign)? We can approach also this question from an interdisciplinary perspective. Reference work in political science aims at modeling the mechanisms of political discourse in forms of discourse networks, as defined in Leifeld (2017). A discourse network is a bipartite graph, containing two classes of nodes: actors (e.g. Angela Merkel; the left-wing party; etc.) and claims (e.g. housing opportunities should be established for refugees); Edges between actors and claims indicate the support or opposition of a certain actor to a specific claim. Discourse coalitions (Hajer, 1993) and argumentative clusters are the projection of the affiliation network on the actor and claim sides of the network (Leifeld and Haunss, 2012;Haunss et al., 2013). Recent NLP research has targeted integration machine learning in the discourse network analysis workflow (Padó et al., 2019;Haunss et al., 2020). Crucially for AM, discourse networks can integrate claims and actors with a third class of nodes, the frame nodes, which encode the reason put forward by an actor to support or reject a claim. This type of representation is perfectly compatible with a graph-based approach on argument representation which has already been established as to be preferred to a tree-structure representation both empirically (Niculae et al., 2017) and theoretically (Afantenos and Asher, 2014).

Moderation can thus be modeled as optimization of specific quantitative properties of the discourse network: participant inclusion, can be enforced by ensuring that the contributions of peripheric actor nodes receive the deserved salience; argument mapping and summarization can be modeled by identifying ""hot"" sub-graphs in the network; the impact of a contribution (the grounded notion of AQ we have been advocating thus far) can be quantified as the perturbation introduced in the network, with its long term effects on convergence or polarization.

Who moderates the (NLP) moderators? The problem of biased moderation obviously relates to the issue of bias in NLP (Blodgett et al., 2020;Caliskan et al., 2017;Bolukbasi et al., 2016;Spliethöver and Wachsmuth, 2020) and it has a clear implication in the application of NLP methods to moderation. For example, we would not want our NLP models to infer a negative impact on AQ from cues which just reveal that the user belongs to certain groups. This is a real risk when quality is equated to ""success"", in turn quantified in terms of likes, replies, retweets. The public of a forum may be sensitive to such cues, but the moderator should be unbiased with respect to them. Another source of bias is the degree of literacy of a contribution: while users who express themselves poorly are likely to be less popular with the forum public, their contributions may still be a very good move in the ""cooperation challenge"" -one that moderators (NLP or humans, online or in-person) have to ensure will not be left unexploited.","# QUA Analysis:

1. (What is NLP-supported moderation?): sent1
    1.1. (How can automatic tools assist in moderation?): sent2
        1.1.1. (What is an example of an issue in crowd-scale deliberation?): sent3
        1.1.2. (How can NLP help with issues like ""flaming""?): sent4
    1.2. (What are some applications of NLP in deliberative decision-making?): sent5
        1.2.1. (What is the potential of real-time applications in deliberation?): sent6
        1.2.2. (What further tasks can NLP support in moderation?): sent7
            1.2.2.1. (How can detecting and highlighting parts of an argument help?): sent8
            1.2.2.2. (What is another important source for moderation?): sent9
            1.2.2.3. (What other NLP tasks are crucial for healthy interaction?): sent10
2. (What has been discussed so far?): sent11
    2.1. (What are the features that the NLP moderator takes as input?): sent12
3. (What question remains open?): sent13
    3.1. (How can we approach the representation of argumentative discourse?): sent14
        3.1.1. (What is the reference work in political science for modeling political discourse?): sent15
            3.1.1.1. (What is a discourse network?): sent16
            3.1.1.2. (What are discourse coalitions and argumentative clusters?): sent17
            3.1.1.3. (How has recent NLP research integrated machine learning in discourse network analysis?): sent18
            3.1.1.4. (How can discourse networks integrate claims and actors?): sent19
            3.1.1.5. (Why is a graph-based approach preferred for argument representation?): sent20
            3.1.1.6. (How can moderation be modeled in discourse networks?): sent21
4. (Who moderates the (NLP) moderators?): sent22
    4.1. (How does the problem of biased moderation relate to bias in NLP?): sent23
        4.1.1. (What is an example of a risk in biased NLP models?): sent24
        4.1.2. (What is a real risk when quality is equated to ""success""?): sent25
        4.1.3. (How should moderators handle cues that the public may be sensitive to?): sent26
        4.1.4. (What is another source of bias in moderation?): sent27",How can discourse networks be used to model argumentative discourse in political science?,"sent15, sent16, sent17, sent18, sent19, sent20, sent21","Reference work in political science aims at modeling the mechanisms of political discourse in forms of discourse networks, as defined in Leifeld (2017).

A discourse network is a bipartite graph, containing two classes of nodes: actors (e.g. Angela Merkel; the left-wing party; etc.) and claims (e.g. housing opportunities should be established for refugees); Edges between actors and claims indicate the support or opposition of a certain actor to a specific claim.

Discourse coalitions (Hajer, 1993) and argumentative clusters are the projection of the affiliation network on the actor and claim sides of the network (Leifeld and Haunss, 2012;Haunss et al., 2013).

Recent NLP research has targeted integration machine learning in the discourse network analysis workflow (Padó et al., 2019;Haunss et al., 2020).

Crucially for AM, discourse networks can integrate claims and actors with a third class of nodes, the frame nodes, which encode the reason put forward by an actor to support or reject a claim.

This type of representation is perfectly compatible with a graph-based approach on argument representation which has already been established as to be preferred to a tree-structure representation both empirically (Niculae et al., 2017) and theoretically (Afantenos and Asher, 2014).

Moderation can thus be modeled as optimization of specific quantitative properties of the discourse network: participant inclusion, can be enforced by ensuring that the contributions of peripheric actor nodes receive the deserved salience; argument mapping and summarization can be modeled by identifying ""hot"" sub-graphs in the network; the impact of a contribution (the grounded notion of AQ we have been advocating thus far) can be quantified as the perturbation introduced in the network, with its long term effects on convergence or polarization.","Discourse networks can be used to model the mechanisms of political discourse, as defined in Leifeld (2017).

A discourse network is a bipartite graph containing two classes of nodes: actors (e.g., Angela Merkel; the left-wing party) and claims (e.g., housing opportunities should be established for refugees). Edges between actors and claims indicate the support or opposition of a certain actor to a specific claim.

Discourse coalitions (Hajer, 1993) and argumentative clusters are the projection of the affiliation network on the actor and claim sides of the network (Leifeld and Haunss, 2012; Haunss et al., 2013).

Recent NLP research has targeted integrating machine learning into the discourse network analysis workflow (Padó et al., 2019; Haunss et al., 2020).

Crucially for argumentative modeling, discourse networks can integrate claims and actors with a third class of nodes, the frame nodes, which encode the reason put forward by an actor to support or reject a claim.

This type of representation is compatible with a graph-based approach to argument representation, which has been shown to be preferred over a tree-structure representation both empirically (Niculae et al., 2017) and theoretically (Afantenos and Asher, 2014).

Moderation can thus be modeled as the optimization of specific quantitative properties of the discourse network: participant inclusion can be enforced by ensuring that the contributions of peripheral actor nodes receive the deserved salience; argument mapping and summarization can be modeled by identifying ""hot"" sub-graphs in the network; the impact of a contribution can be quantified as the perturbation introduced in the network, with its long-term effects on convergence or polarization.","Questions:

How can discourse networks be used to model argumentative discourse in political science?

Answer:

Discourse networks can be used to model the mechanisms of political discourse, as defined in Leifeld (2017).

A discourse network is a bipartite graph containing two classes of nodes: actors (e.g., Angela Merkel; the left-wing party) and claims (e.g., housing opportunities should be established for refugees). Edges between actors and claims indicate the support or opposition of a certain actor to a specific claim.

Discourse coalitions (Hajer, 1993) and argumentative clusters are the projection of the affiliation network on the actor and claim sides of the network (Leifeld and Haunss, 2012; Haunss et al., 2013).

Recent NLP research has targeted integrating machine learning into the discourse network analysis workflow (Padó et al., 2019; Haunss et al., 2020).

Crucially for argumentative modeling, discourse networks can integrate claims and actors with a third class of nodes, the frame nodes, which encode the reason put forward by an actor to support or reject a claim.

This type of representation is compatible with a graph-based approach to argument representation, which has been shown to be preferred over a tree-structure representation both empirically (Niculae et al., 2017) and theoretically (Afantenos and Asher, 2014).

Moderation can thus be modeled as the optimization of specific quantitative properties of the discourse network: participant inclusion can be enforced by ensuring that the contributions of peripheral actor nodes receive the deserved salience; argument mapping and summarization can be modeled by identifying ""hot"" sub-graphs in the network; the impact of a contribution can be quantified as the perturbation introduced in the network, with its long-term effects on convergence or polarization."
Towards Argument Mining for Social Good: A Survey,NLP-Supported Moderation: desiderata and challenges,"NLP-supported moderation represents a clear solution to the bottleneck problem affecting facilitation in digital democracy. Automatic tools can take over some of the tasks that human moderators typically perform when monitoring online discussions. For example, in Social Sciences, one of the most discussed issues in crowd-scale deliberation is ""flaming"", i.e., aggressive and disrespectful communicative behavior (Lampe et al., 2014). Here, moderators could benefit from hate-speech and trolling detection methods in NLP. NLP methods to support deliberative decisionmaking have already been applied for the realtime visualisation of argument maps (El-Assady et al., 2017). Deliberation in real-time applications has the clear potential of structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).

Beyond the real-time support to users (and moderators) provided by the methods described above, further tasks specific to AM which are part of the role of a human or (semi-)automoated moderator include: detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), relevance estimation (Potthast et al., 2019). In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion. Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b) which has a counterpart in the framing detection task (Card et al., 2015;Akyürek et al., 2020), as framing is a manipulation strategy which highlights specific aspects of an issue under discussion to promote certain interpretations.

Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012;Waseem and Hovy, 2016;Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014;Kotonya and Toni, 2020), Facts recognition and source identification (Dusmanu et al., 2017).

How to represent discourse? Thus far, we have discussed the main ingredients of a rich NLP-informed approach to deliberative discourse. These components, together with the deliberationaugmented definition of AQ sketched in section 3 are the features that the NLP moderator takes as an input. One question remains open: How to represent the argumentative discourse within a contribution (e.g. a forum post) and across contributions (e.g. an entire online deliberation campaign)? We can approach also this question from an interdisciplinary perspective. Reference work in political science aims at modeling the mechanisms of political discourse in forms of discourse networks, as defined in Leifeld (2017). A discourse network is a bipartite graph, containing two classes of nodes: actors (e.g. Angela Merkel; the left-wing party; etc.) and claims (e.g. housing opportunities should be established for refugees); Edges between actors and claims indicate the support or opposition of a certain actor to a specific claim. Discourse coalitions (Hajer, 1993) and argumentative clusters are the projection of the affiliation network on the actor and claim sides of the network (Leifeld and Haunss, 2012;Haunss et al., 2013). Recent NLP research has targeted integration machine learning in the discourse network analysis workflow (Padó et al., 2019;Haunss et al., 2020). Crucially for AM, discourse networks can integrate claims and actors with a third class of nodes, the frame nodes, which encode the reason put forward by an actor to support or reject a claim. This type of representation is perfectly compatible with a graph-based approach on argument representation which has already been established as to be preferred to a tree-structure representation both empirically (Niculae et al., 2017) and theoretically (Afantenos and Asher, 2014).

Moderation can thus be modeled as optimization of specific quantitative properties of the discourse network: participant inclusion, can be enforced by ensuring that the contributions of peripheric actor nodes receive the deserved salience; argument mapping and summarization can be modeled by identifying ""hot"" sub-graphs in the network; the impact of a contribution (the grounded notion of AQ we have been advocating thus far) can be quantified as the perturbation introduced in the network, with its long term effects on convergence or polarization.

Who moderates the (NLP) moderators? The problem of biased moderation obviously relates to the issue of bias in NLP (Blodgett et al., 2020;Caliskan et al., 2017;Bolukbasi et al., 2016;Spliethöver and Wachsmuth, 2020) and it has a clear implication in the application of NLP methods to moderation. For example, we would not want our NLP models to infer a negative impact on AQ from cues which just reveal that the user belongs to certain groups. This is a real risk when quality is equated to ""success"", in turn quantified in terms of likes, replies, retweets. The public of a forum may be sensitive to such cues, but the moderator should be unbiased with respect to them. Another source of bias is the degree of literacy of a contribution: while users who express themselves poorly are likely to be less popular with the forum public, their contributions may still be a very good move in the ""cooperation challenge"" -one that moderators (NLP or humans, online or in-person) have to ensure will not be left unexploited.","# QUA Analysis:

1. (What is NLP-supported moderation?): sent1
    1.1. (How can automatic tools assist in moderation?): sent2
        1.1.1. (What is an example of an issue in crowd-scale deliberation?): sent3
        1.1.2. (How can NLP help with issues like ""flaming""?): sent4
    1.2. (What are some applications of NLP in deliberative decision-making?): sent5
        1.2.1. (What is the potential of real-time applications in deliberation?): sent6
        1.2.2. (What further tasks can NLP support in moderation?): sent7
            1.2.2.1. (How can detecting and highlighting parts of an argument help?): sent8
            1.2.2.2. (What is another important source for moderation?): sent9
            1.2.2.3. (What other NLP tasks are crucial for healthy interaction?): sent10
2. (What has been discussed so far?): sent11
    2.1. (What are the features that the NLP moderator takes as input?): sent12
3. (What question remains open?): sent13
    3.1. (How can we approach the representation of argumentative discourse?): sent14
        3.1.1. (What is the reference work in political science for modeling political discourse?): sent15
            3.1.1.1. (What is a discourse network?): sent16
            3.1.1.2. (What are discourse coalitions and argumentative clusters?): sent17
            3.1.1.3. (How has recent NLP research integrated machine learning in discourse network analysis?): sent18
            3.1.1.4. (How can discourse networks integrate claims and actors?): sent19
            3.1.1.5. (Why is a graph-based approach preferred for argument representation?): sent20
            3.1.1.6. (How can moderation be modeled in discourse networks?): sent21
4. (Who moderates the (NLP) moderators?): sent22
    4.1. (How does the problem of biased moderation relate to bias in NLP?): sent23
        4.1.1. (What is an example of a risk in biased NLP models?): sent24
        4.1.2. (What is a real risk when quality is equated to ""success""?): sent25
        4.1.3. (How should moderators handle cues that the public may be sensitive to?): sent26
        4.1.4. (What is another source of bias in moderation?): sent27",What are the potential biases in NLP-supported moderation and how can they be mitigated?,"sent22, sent23, sent24, sent25, sent26, sent27","Who moderates the (NLP) moderators?

The problem of biased moderation obviously relates to the issue of bias in NLP (Blodgett et al., 2020;Caliskan et al., 2017;Bolukbasi et al., 2016;Spliethöver and Wachsmuth, 2020) and it has a clear implication in the application of NLP methods to moderation.

For example, we would not want our NLP models to infer a negative impact on AQ from cues which just reveal that the user belongs to certain groups.

This is a real risk when quality is equated to ""success"", in turn quantified in terms of likes, replies, retweets.

The public of a forum may be sensitive to such cues, but the moderator should be unbiased with respect to them.

Another source of bias is the degree of literacy of a contribution: while users who express themselves poorly are likely to be less popular with the forum public, their contributions may still be a very good move in the ""cooperation challenge"" -one that moderators (NLP or humans, online or in-person) have to ensure will not be left unexploited.","Potential biases in NLP-supported moderation can arise from the data used to train the models and the inherent biases in the algorithms themselves.

The problem of biased moderation is closely related to the issue of bias in NLP (Blodgett et al., 2020; Caliskan et al., 2017; Bolukbasi et al., 2016; Spliethöver and Wachsmuth, 2020) and has clear implications for the application of NLP methods to moderation.

For example, it is undesirable for NLP models to infer a negative impact on AQ from cues that merely indicate the user's membership in certain groups.

This risk becomes evident when quality is equated to ""success,"" which is often quantified in terms of likes, replies, and retweets.

The public of a forum may be sensitive to such cues, but the moderator should be unbiased with respect to them.

Another source of bias is the degree of literacy of a contribution: while users who express themselves poorly are likely to be less popular with the forum public, their contributions may still be valuable in the ""cooperation challenge"" that moderators (NLP or humans, online or in-person) need to ensure are not overlooked.","Questions:

What are the potential biases in NLP-supported moderation and how can they be mitigated?

Answer:

Potential biases in NLP-supported moderation can arise from the data used to train the models and the inherent biases in the algorithms themselves.

The problem of biased moderation is closely related to the issue of bias in NLP (Blodgett et al., 2020; Caliskan et al., 2017; Bolukbasi et al., 2016; Spliethöver and Wachsmuth, 2020) and has clear implications for the application of NLP methods to moderation.

For example, it is undesirable for NLP models to infer a negative impact on AQ from cues that merely indicate the user's membership in certain groups.

This risk becomes evident when quality is equated to ""success,"" which is often quantified in terms of likes, replies, and retweets.

The public of a forum may be sensitive to such cues, but the moderator should be unbiased with respect to them.

Another source of bias is the degree of literacy of a contribution: while users who express themselves poorly are likely to be less popular with the forum public, their contributions may still be valuable in the ""cooperation challenge"" that moderators (NLP or humans, online or in-person) need to ensure are not overlooked."
Towards Argument Mining for Social Good: A Survey,Scaling Up Argument Mining,"In social media While overtly argumentative text, like those described above, represents the natural domain of application for AM, social media constitute a powerful source of large amounts of data (billions of words) despite facing particular challenges in AM.

Social media plays an increasingly significant role in modern political and social discourse, yet resources built for conducting AM on this type of data structure remain limited for clear reasons. These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users. Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020). Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates. The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.

Recent work has aimed to tackle such challenges in social media. Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification. They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics. Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.

Despite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020). On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited. Recent work by Schaefer and Stede (2020), among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets. On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.

The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges. However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.

Multilingual argument mining Multilinguality is an important area of research in NLP that has gained more attention recently because of the crosslingual transfer potentials of Pre-trained Language Models (Devlin et al., 2019;Conneau et al., 2020) and because of the potentials for a societal impact at a global scale. The latter is particularly important when considering AM for Social Good since language should not be a barrier for participation if the goal is to allow any productive contribution.

Various recent studies have investigated multilinguality for AM. Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese. A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling. This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores. Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.

Other work has also dealt with building non-English datasets (Lindahl, 2020;Bauwelinck and Lefever, 2020;Schaefer and Stede, 2020;Zotova et al., 2020), but there still seems to be a focus on Indo-European languages (and sometimes Chinese) with a lack of datasets and analysis extending to other languages. This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Blodgett et al., 2016) and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Davidson et al., 2019). This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact.","# QUA Analysis:

1. (What is the role of social media in Argument Mining (AM) and what challenges does it present?): sent1
    1.1. (Why is social media significant for AM?): sent2
    1.2. (What type of content is found on social media?): sent3
    1.3. (What are the challenges in distinguishing content types on social media?): sent4
    1.4. (What is the nature of language used on social media?): sent5
    1.5. (What unique challenge does social media present for AM?): sent6
2. (What recent work has been done to tackle AM challenges in social media?): sent7
    2.1. (What approach did Dusmanu et al. (2017) take?): sent8
        2.1.1. (What did Dusmanu et al. (2017) study?): sent9
    2.2. (What contributions did Habernal and Gurevych (2017) and Simpson and Gurevych (2018) make?): sent10
3. (What is a major challenge in AM for social media?): sent11
    3.1. (What is the issue with the amount of annotations?): sent12
        3.1.1. (What work has been done to construct annotated Twitter corpora?): sent13
    3.2. (What is the issue with annotation guidelines?): sent14
        3.2.1. (What did Bauwelinck and Lefever (2020) aim to provide?): sent15
4. (What are the unique challenges presented by social media platforms?): sent16
5. (Why is it essential to work well with argumentative text from social media?): sent17
6. (What is the importance of multilinguality in AM for Social Good?): sent18
    6.1. (Why is multilinguality important for societal impact?): sent19
7. (What recent studies have investigated multilinguality for AM?): sent20
    7.1. (What did Eger et al. (2019) discuss?): sent21
    7.2. (What approach did Toledo-Ronen et al. (2020) take?): sent22
        7.2.1. (How did Toledo-Ronen et al. (2020) perform in their approach?): sent23
    7.3. (What did Lai et al. (2020) investigate?): sent24
8. (What is the issue with building non-English datasets in AM?): sent25
9. (What general issue in NLP research extends to AM?): sent26
10. (Why is addressing this limitation important for AM?): sent27",What are the challenges in distinguishing content types on social media for Argument Mining (AM)?,"sent3, sent4, sent5, sent6","These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users.

Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020).

Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates.

The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.","Social media platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users.

Distinguishing between personal opinion, fact, and fake news is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020).

Additionally, the language used on social media platforms is infamously chaotic and often non-standard in comparison to the language used in more structured environments, like parliamentary debates.

The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.","Questions:

What are the challenges in distinguishing content types on social media for Argument Mining (AM)?

Answer:

Social media platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users.

Distinguishing between personal opinion, fact, and fake news is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020).

Additionally, the language used on social media platforms is infamously chaotic and often non-standard in comparison to the language used in more structured environments, like parliamentary debates.

The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data."
Towards Argument Mining for Social Good: A Survey,Scaling Up Argument Mining,"In social media While overtly argumentative text, like those described above, represents the natural domain of application for AM, social media constitute a powerful source of large amounts of data (billions of words) despite facing particular challenges in AM.

Social media plays an increasingly significant role in modern political and social discourse, yet resources built for conducting AM on this type of data structure remain limited for clear reasons. These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users. Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020). Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates. The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.

Recent work has aimed to tackle such challenges in social media. Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification. They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics. Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.

Despite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020). On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited. Recent work by Schaefer and Stede (2020), among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets. On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.

The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges. However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.

Multilingual argument mining Multilinguality is an important area of research in NLP that has gained more attention recently because of the crosslingual transfer potentials of Pre-trained Language Models (Devlin et al., 2019;Conneau et al., 2020) and because of the potentials for a societal impact at a global scale. The latter is particularly important when considering AM for Social Good since language should not be a barrier for participation if the goal is to allow any productive contribution.

Various recent studies have investigated multilinguality for AM. Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese. A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling. This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores. Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.

Other work has also dealt with building non-English datasets (Lindahl, 2020;Bauwelinck and Lefever, 2020;Schaefer and Stede, 2020;Zotova et al., 2020), but there still seems to be a focus on Indo-European languages (and sometimes Chinese) with a lack of datasets and analysis extending to other languages. This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Blodgett et al., 2016) and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Davidson et al., 2019). This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact.","# QUA Analysis:

1. (What is the role of social media in Argument Mining (AM) and what challenges does it present?): sent1
    1.1. (Why is social media significant for AM?): sent2
    1.2. (What type of content is found on social media?): sent3
    1.3. (What are the challenges in distinguishing content types on social media?): sent4
    1.4. (What is the nature of language used on social media?): sent5
    1.5. (What unique challenge does social media present for AM?): sent6
2. (What recent work has been done to tackle AM challenges in social media?): sent7
    2.1. (What approach did Dusmanu et al. (2017) take?): sent8
        2.1.1. (What did Dusmanu et al. (2017) study?): sent9
    2.2. (What contributions did Habernal and Gurevych (2017) and Simpson and Gurevych (2018) make?): sent10
3. (What is a major challenge in AM for social media?): sent11
    3.1. (What is the issue with the amount of annotations?): sent12
        3.1.1. (What work has been done to construct annotated Twitter corpora?): sent13
    3.2. (What is the issue with annotation guidelines?): sent14
        3.2.1. (What did Bauwelinck and Lefever (2020) aim to provide?): sent15
4. (What are the unique challenges presented by social media platforms?): sent16
5. (Why is it essential to work well with argumentative text from social media?): sent17
6. (What is the importance of multilinguality in AM for Social Good?): sent18
    6.1. (Why is multilinguality important for societal impact?): sent19
7. (What recent studies have investigated multilinguality for AM?): sent20
    7.1. (What did Eger et al. (2019) discuss?): sent21
    7.2. (What approach did Toledo-Ronen et al. (2020) take?): sent22
        7.2.1. (How did Toledo-Ronen et al. (2020) perform in their approach?): sent23
    7.3. (What did Lai et al. (2020) investigate?): sent24
8. (What is the issue with building non-English datasets in AM?): sent25
9. (What general issue in NLP research extends to AM?): sent26
10. (Why is addressing this limitation important for AM?): sent27",What recent work has been done to tackle Argument Mining (AM) challenges in social media?,"sent7, sent8, sent9, sent10","Recent work has aimed to tackle such challenges in social media.

Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification.

They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics.

Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.","Recent work has aimed to tackle Argument Mining (AM) challenges in social media.

Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification.

The feasibility of the approaches proposed to address these tasks is studied on a set of tweets related to the Grexit and Brexit news topics.

Habernal and Gurevych (2017) provide an extensive analysis of the steps and modeling strategies necessary to analyze social media data in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.","Questions:

What recent work has been done to tackle Argument Mining (AM) challenges in social media?

Answer:

Recent work has aimed to tackle Argument Mining (AM) challenges in social media.

Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification.

The feasibility of the approaches proposed to address these tasks is studied on a set of tweets related to the Grexit and Brexit news topics.

Habernal and Gurevych (2017) provide an extensive analysis of the steps and modeling strategies necessary to analyze social media data in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms."
Towards Argument Mining for Social Good: A Survey,Scaling Up Argument Mining,"In social media While overtly argumentative text, like those described above, represents the natural domain of application for AM, social media constitute a powerful source of large amounts of data (billions of words) despite facing particular challenges in AM.

Social media plays an increasingly significant role in modern political and social discourse, yet resources built for conducting AM on this type of data structure remain limited for clear reasons. These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users. Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020). Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates. The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.

Recent work has aimed to tackle such challenges in social media. Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification. They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics. Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.

Despite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020). On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited. Recent work by Schaefer and Stede (2020), among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets. On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.

The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges. However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.

Multilingual argument mining Multilinguality is an important area of research in NLP that has gained more attention recently because of the crosslingual transfer potentials of Pre-trained Language Models (Devlin et al., 2019;Conneau et al., 2020) and because of the potentials for a societal impact at a global scale. The latter is particularly important when considering AM for Social Good since language should not be a barrier for participation if the goal is to allow any productive contribution.

Various recent studies have investigated multilinguality for AM. Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese. A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling. This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores. Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.

Other work has also dealt with building non-English datasets (Lindahl, 2020;Bauwelinck and Lefever, 2020;Schaefer and Stede, 2020;Zotova et al., 2020), but there still seems to be a focus on Indo-European languages (and sometimes Chinese) with a lack of datasets and analysis extending to other languages. This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Blodgett et al., 2016) and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Davidson et al., 2019). This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact.","# QUA Analysis:

1. (What is the role of social media in Argument Mining (AM) and what challenges does it present?): sent1
    1.1. (Why is social media significant for AM?): sent2
    1.2. (What type of content is found on social media?): sent3
    1.3. (What are the challenges in distinguishing content types on social media?): sent4
    1.4. (What is the nature of language used on social media?): sent5
    1.5. (What unique challenge does social media present for AM?): sent6
2. (What recent work has been done to tackle AM challenges in social media?): sent7
    2.1. (What approach did Dusmanu et al. (2017) take?): sent8
        2.1.1. (What did Dusmanu et al. (2017) study?): sent9
    2.2. (What contributions did Habernal and Gurevych (2017) and Simpson and Gurevych (2018) make?): sent10
3. (What is a major challenge in AM for social media?): sent11
    3.1. (What is the issue with the amount of annotations?): sent12
        3.1.1. (What work has been done to construct annotated Twitter corpora?): sent13
    3.2. (What is the issue with annotation guidelines?): sent14
        3.2.1. (What did Bauwelinck and Lefever (2020) aim to provide?): sent15
4. (What are the unique challenges presented by social media platforms?): sent16
5. (Why is it essential to work well with argumentative text from social media?): sent17
6. (What is the importance of multilinguality in AM for Social Good?): sent18
    6.1. (Why is multilinguality important for societal impact?): sent19
7. (What recent studies have investigated multilinguality for AM?): sent20
    7.1. (What did Eger et al. (2019) discuss?): sent21
    7.2. (What approach did Toledo-Ronen et al. (2020) take?): sent22
        7.2.1. (How did Toledo-Ronen et al. (2020) perform in their approach?): sent23
    7.3. (What did Lai et al. (2020) investigate?): sent24
8. (What is the issue with building non-English datasets in AM?): sent25
9. (What general issue in NLP research extends to AM?): sent26
10. (Why is addressing this limitation important for AM?): sent27",What is a major challenge in Argument Mining (AM) for social media?,"sent11, sent12, sent14, sent16","Despite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020).

On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited.

On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis.

The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges.","One of the major challenges in Argument Mining (AM) for social media is the lack of consensus on how exactly to analyze argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020).

On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited.

On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labeled corpora rarely include motivation for the use of a particular theoretical basis.

The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges.","Questions:

What is a major challenge in Argument Mining (AM) for social media?

Answer:

One of the major challenges in Argument Mining (AM) for social media is the lack of consensus on how exactly to analyze argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020).

On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited.

On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labeled corpora rarely include motivation for the use of a particular theoretical basis.

The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges."
Towards Argument Mining for Social Good: A Survey,Scaling Up Argument Mining,"In social media While overtly argumentative text, like those described above, represents the natural domain of application for AM, social media constitute a powerful source of large amounts of data (billions of words) despite facing particular challenges in AM.

Social media plays an increasingly significant role in modern political and social discourse, yet resources built for conducting AM on this type of data structure remain limited for clear reasons. These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users. Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020). Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates. The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.

Recent work has aimed to tackle such challenges in social media. Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification. They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics. Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.

Despite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020). On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited. Recent work by Schaefer and Stede (2020), among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets. On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.

The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges. However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.

Multilingual argument mining Multilinguality is an important area of research in NLP that has gained more attention recently because of the crosslingual transfer potentials of Pre-trained Language Models (Devlin et al., 2019;Conneau et al., 2020) and because of the potentials for a societal impact at a global scale. The latter is particularly important when considering AM for Social Good since language should not be a barrier for participation if the goal is to allow any productive contribution.

Various recent studies have investigated multilinguality for AM. Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese. A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling. This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores. Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.

Other work has also dealt with building non-English datasets (Lindahl, 2020;Bauwelinck and Lefever, 2020;Schaefer and Stede, 2020;Zotova et al., 2020), but there still seems to be a focus on Indo-European languages (and sometimes Chinese) with a lack of datasets and analysis extending to other languages. This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Blodgett et al., 2016) and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Davidson et al., 2019). This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact.","# QUA Analysis:

1. (What is the role of social media in Argument Mining (AM) and what challenges does it present?): sent1
    1.1. (Why is social media significant for AM?): sent2
    1.2. (What type of content is found on social media?): sent3
    1.3. (What are the challenges in distinguishing content types on social media?): sent4
    1.4. (What is the nature of language used on social media?): sent5
    1.5. (What unique challenge does social media present for AM?): sent6
2. (What recent work has been done to tackle AM challenges in social media?): sent7
    2.1. (What approach did Dusmanu et al. (2017) take?): sent8
        2.1.1. (What did Dusmanu et al. (2017) study?): sent9
    2.2. (What contributions did Habernal and Gurevych (2017) and Simpson and Gurevych (2018) make?): sent10
3. (What is a major challenge in AM for social media?): sent11
    3.1. (What is the issue with the amount of annotations?): sent12
        3.1.1. (What work has been done to construct annotated Twitter corpora?): sent13
    3.2. (What is the issue with annotation guidelines?): sent14
        3.2.1. (What did Bauwelinck and Lefever (2020) aim to provide?): sent15
4. (What are the unique challenges presented by social media platforms?): sent16
5. (Why is it essential to work well with argumentative text from social media?): sent17
6. (What is the importance of multilinguality in AM for Social Good?): sent18
    6.1. (Why is multilinguality important for societal impact?): sent19
7. (What recent studies have investigated multilinguality for AM?): sent20
    7.1. (What did Eger et al. (2019) discuss?): sent21
    7.2. (What approach did Toledo-Ronen et al. (2020) take?): sent22
        7.2.1. (How did Toledo-Ronen et al. (2020) perform in their approach?): sent23
    7.3. (What did Lai et al. (2020) investigate?): sent24
8. (What is the issue with building non-English datasets in AM?): sent25
9. (What general issue in NLP research extends to AM?): sent26
10. (Why is addressing this limitation important for AM?): sent27",What work has been done to construct annotated Twitter corpora for Argument Mining (AM)?,"sent13, sent15, sent17","Recent work by Schaefer and Stede (2020), among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets.

Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.

However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.","Recent work by Schaefer and Stede (2020), among others, has aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets.

Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.

However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.","Questions:

What work has been done to construct annotated Twitter corpora for Argument Mining (AM)?

Answer:

Recent work by Schaefer and Stede (2020), among others, has aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets.

Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.

However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times."
Towards Argument Mining for Social Good: A Survey,Scaling Up Argument Mining,"In social media While overtly argumentative text, like those described above, represents the natural domain of application for AM, social media constitute a powerful source of large amounts of data (billions of words) despite facing particular challenges in AM.

Social media plays an increasingly significant role in modern political and social discourse, yet resources built for conducting AM on this type of data structure remain limited for clear reasons. These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users. Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020). Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates. The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.

Recent work has aimed to tackle such challenges in social media. Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification. They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics. Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.

Despite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020). On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited. Recent work by Schaefer and Stede (2020), among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets. On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.

The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges. However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.

Multilingual argument mining Multilinguality is an important area of research in NLP that has gained more attention recently because of the crosslingual transfer potentials of Pre-trained Language Models (Devlin et al., 2019;Conneau et al., 2020) and because of the potentials for a societal impact at a global scale. The latter is particularly important when considering AM for Social Good since language should not be a barrier for participation if the goal is to allow any productive contribution.

Various recent studies have investigated multilinguality for AM. Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese. A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling. This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores. Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.

Other work has also dealt with building non-English datasets (Lindahl, 2020;Bauwelinck and Lefever, 2020;Schaefer and Stede, 2020;Zotova et al., 2020), but there still seems to be a focus on Indo-European languages (and sometimes Chinese) with a lack of datasets and analysis extending to other languages. This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Blodgett et al., 2016) and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Davidson et al., 2019). This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact.","# QUA Analysis:

1. (What is the role of social media in Argument Mining (AM) and what challenges does it present?): sent1
    1.1. (Why is social media significant for AM?): sent2
    1.2. (What type of content is found on social media?): sent3
    1.3. (What are the challenges in distinguishing content types on social media?): sent4
    1.4. (What is the nature of language used on social media?): sent5
    1.5. (What unique challenge does social media present for AM?): sent6
2. (What recent work has been done to tackle AM challenges in social media?): sent7
    2.1. (What approach did Dusmanu et al. (2017) take?): sent8
        2.1.1. (What did Dusmanu et al. (2017) study?): sent9
    2.2. (What contributions did Habernal and Gurevych (2017) and Simpson and Gurevych (2018) make?): sent10
3. (What is a major challenge in AM for social media?): sent11
    3.1. (What is the issue with the amount of annotations?): sent12
        3.1.1. (What work has been done to construct annotated Twitter corpora?): sent13
    3.2. (What is the issue with annotation guidelines?): sent14
        3.2.1. (What did Bauwelinck and Lefever (2020) aim to provide?): sent15
4. (What are the unique challenges presented by social media platforms?): sent16
5. (Why is it essential to work well with argumentative text from social media?): sent17
6. (What is the importance of multilinguality in AM for Social Good?): sent18
    6.1. (Why is multilinguality important for societal impact?): sent19
7. (What recent studies have investigated multilinguality for AM?): sent20
    7.1. (What did Eger et al. (2019) discuss?): sent21
    7.2. (What approach did Toledo-Ronen et al. (2020) take?): sent22
        7.2.1. (How did Toledo-Ronen et al. (2020) perform in their approach?): sent23
    7.3. (What did Lai et al. (2020) investigate?): sent24
8. (What is the issue with building non-English datasets in AM?): sent25
9. (What general issue in NLP research extends to AM?): sent26
10. (Why is addressing this limitation important for AM?): sent27",What recent studies have investigated multilinguality for Argument Mining (AM)?,"sent20, sent21, sent22, sent23, sent24","Various recent studies have investigated multilinguality for AM.

Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese.

A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling.

This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores.

Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.","Recent studies have investigated multilinguality for Argument Mining (AM).

Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically focusing on argument components extraction and classification in German, English, and Chinese.

A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling.

This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores.

Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective, and contextual features from text and analyzing the scenarios in which each of these features is effective.","Questions:

What recent studies have investigated multilinguality for Argument Mining (AM)?

Answer:

Recent studies have investigated multilinguality for Argument Mining (AM).

Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically focusing on argument components extraction and classification in German, English, and Chinese.

A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling.

This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores.

Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective, and contextual features from text and analyzing the scenarios in which each of these features is effective."
Towards Argument Mining for Social Good: A Survey,Scaling Up Argument Mining,"In social media While overtly argumentative text, like those described above, represents the natural domain of application for AM, social media constitute a powerful source of large amounts of data (billions of words) despite facing particular challenges in AM.

Social media plays an increasingly significant role in modern political and social discourse, yet resources built for conducting AM on this type of data structure remain limited for clear reasons. These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users. Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020). Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates. The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.

Recent work has aimed to tackle such challenges in social media. Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification. They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics. Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.

Despite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020). On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited. Recent work by Schaefer and Stede (2020), among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets. On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.

The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges. However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.

Multilingual argument mining Multilinguality is an important area of research in NLP that has gained more attention recently because of the crosslingual transfer potentials of Pre-trained Language Models (Devlin et al., 2019;Conneau et al., 2020) and because of the potentials for a societal impact at a global scale. The latter is particularly important when considering AM for Social Good since language should not be a barrier for participation if the goal is to allow any productive contribution.

Various recent studies have investigated multilinguality for AM. Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese. A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling. This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores. Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.

Other work has also dealt with building non-English datasets (Lindahl, 2020;Bauwelinck and Lefever, 2020;Schaefer and Stede, 2020;Zotova et al., 2020), but there still seems to be a focus on Indo-European languages (and sometimes Chinese) with a lack of datasets and analysis extending to other languages. This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Blodgett et al., 2016) and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Davidson et al., 2019). This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact.","# QUA Analysis:

1. (What is the role of social media in Argument Mining (AM) and what challenges does it present?): sent1
    1.1. (Why is social media significant for AM?): sent2
    1.2. (What type of content is found on social media?): sent3
    1.3. (What are the challenges in distinguishing content types on social media?): sent4
    1.4. (What is the nature of language used on social media?): sent5
    1.5. (What unique challenge does social media present for AM?): sent6
2. (What recent work has been done to tackle AM challenges in social media?): sent7
    2.1. (What approach did Dusmanu et al. (2017) take?): sent8
        2.1.1. (What did Dusmanu et al. (2017) study?): sent9
    2.2. (What contributions did Habernal and Gurevych (2017) and Simpson and Gurevych (2018) make?): sent10
3. (What is a major challenge in AM for social media?): sent11
    3.1. (What is the issue with the amount of annotations?): sent12
        3.1.1. (What work has been done to construct annotated Twitter corpora?): sent13
    3.2. (What is the issue with annotation guidelines?): sent14
        3.2.1. (What did Bauwelinck and Lefever (2020) aim to provide?): sent15
4. (What are the unique challenges presented by social media platforms?): sent16
5. (Why is it essential to work well with argumentative text from social media?): sent17
6. (What is the importance of multilinguality in AM for Social Good?): sent18
    6.1. (Why is multilinguality important for societal impact?): sent19
7. (What recent studies have investigated multilinguality for AM?): sent20
    7.1. (What did Eger et al. (2019) discuss?): sent21
    7.2. (What approach did Toledo-Ronen et al. (2020) take?): sent22
        7.2.1. (How did Toledo-Ronen et al. (2020) perform in their approach?): sent23
    7.3. (What did Lai et al. (2020) investigate?): sent24
8. (What is the issue with building non-English datasets in AM?): sent25
9. (What general issue in NLP research extends to AM?): sent26
10. (Why is addressing this limitation important for AM?): sent27",What is the issue with building non-English datasets in Argument Mining (AM)?,"sent25, sent26, sent27","Other work has also dealt with building non-English datasets (Lindahl, 2020;Bauwelinck and Lefever, 2020;Schaefer and Stede, 2020;Zotova et al., 2020), but there still seems to be a focus on Indo-European languages (and sometimes Chinese) with a lack of datasets and analysis extending to other languages.

This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Blodgett et al., 2016) and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Davidson et al., 2019).

This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact.","Building non-English datasets in Argument Mining (AM) often focuses on Indo-European languages (and sometimes Chinese), with a lack of datasets and analysis extending to other languages (Lindahl, 2020; Bauwelinck and Lefever, 2020; Schaefer and Stede, 2020; Zotova et al., 2020).

This issue is not unique to Argument Mining but is a general problem in NLP research, where there is a performance bias in favor of standard dialects, such as in English (Blodgett et al., 2016), and biases that could target certain user groups instead of protecting them, as shown in Hate Speech Detection (Davidson et al., 2019).

This is an important limitation to address in AM for more inclusivity and towards a more positive societal impact.","Questions:

What is the issue with building non-English datasets in Argument Mining (AM)?

Answer:

Building non-English datasets in Argument Mining (AM) often focuses on Indo-European languages (and sometimes Chinese), with a lack of datasets and analysis extending to other languages (Lindahl, 2020; Bauwelinck and Lefever, 2020; Schaefer and Stede, 2020; Zotova et al., 2020).

This issue is not unique to Argument Mining but is a general problem in NLP research, where there is a performance bias in favor of standard dialects, such as in English (Blodgett et al., 2016), and biases that could target certain user groups instead of protecting them, as shown in Hate Speech Detection (Davidson et al., 2019).

This is an important limitation to address in AM for more inclusivity and towards a more positive societal impact."
A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,C-S across Languages: European Context,"The contexts in which people acquire and use multiple languages in Europe are diverse. Some acquire their languages simultaneously from birth, while others acquire them sequentially, either naturally or via explicit instruction. Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors. Speakers who use local dialects or minoritized varieties may also be engaged in C-S when switching between their variety and a dominant one (Mills and Washington, 2015;Blom and Gumperz, 1972). C-S in bilingual language acquisition of children has been studied across language contact contexts in Europe. In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children. From a comparative perspective, Poeste et al. (2019) analyzed C-S among bilingual, trilingual, and multilingual children growing up in Spain and Germany. In the Netherlands, Bosma and Blom (2019) focused on C-S among bilingual Frisian-Dutch children. In addition to analyzing C-S in children's speech, Juan-Garau and Perez-Vidal (2001) and Lanza (1998) investigated C-S in the interaction patterns between bilingual children and their parents (i.e. Spanish-Catalan and English-Norwegian respectively).

Within an educational setting, Kleeman (2012) observed C-S among bilingual (North Sami-Norwegian) kindergarten children in the North of Norway. Similarly, Jørgensen (1998) and Cromdal (2004) report the use of C-S for resolving disputes among bilingual (Turkish-Danish) children in Denmark and multilingual (Swedish-English and/or a Non-Scandinavian Language) children in Sweden respectively.

C-S does not only take place between standard languages but between minority languages and dialects as well. For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (2006) focused on the C-S between Welsh and English in the UK. Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy. In the Balkans, Kyuchukov (2006) analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria. C-S between dialects and/or standard vs. minority languages in computer mediated interaction was analyzed by Siebenhaar (2006) among Swiss-German dialects and by Robert-Tissot and Morel (2017) through SMS corpora collected across Germanic (i.e. English and German) and Romance languages (French, Spanish, Italian) in Switzerland.

C-S is commonly observable across immigrant contexts in Europe. In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (2006) focused on the C-S between English and Cypriot Turkish communities in London. Wei and Milroy (1995) analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (Chinese-English) families in Northeastern England. In addition, Ożańska-Ponikwia (2016) investigated the Polish-English C-S in the UK as well. C-S among immigrant community members have also been widely studied in Germany (e.g. Turkish-German C-S by Keim (2008) and Ç etinoglu (2017), Russian-German C-S by Khakimov (2016)). In the Netherlands, C-S studies include Turkish-Dutch C-S by Backus (2010) and Dutch-Morroccan C-S by Nortier (1990). In Belgium, Meeuws and Blommaert (1998) studied the French-Lingala-Swahili C-S among immigrants of Zaire and Treffers-Daller (1994) studied French-Dutch C-S in Brussels. In Spain, Jieanu (2013) describes the Romanian-Spanish C-S among the Romanian immigrants. In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well. These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands. Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.

In addition to daily communication, some linguists are also interested in the C-S observed in historical documents. While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire. Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations. In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.

Within the media context, Martin (1998) investigated English C-S in written French advertising, and Onysko (2007) investigated the English C-S in German written media through corpus analyses. Zhiganova (2016) indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.

Similar to humans, institutions and/or organizations could also have multilingual communication with their members and/or audience. For example, Wodak et al. (2012) analyzed the C-S and language choice at the institutional level for European Union institutions.","# QUA Analysis:

1. (What are the contexts in which people acquire and use multiple languages in Europe?): sent1
    1.1. (How do people acquire multiple languages in Europe?): sent2
    1.2. (What is the norm regarding multilingualism in certain zones?): sent3
    1.3. (How does C-S manifest in bilingual language acquisition of children in Europe?): sent4
        1.3.1. (What are some specific studies on C-S in bilingual children in Germany?): sent5
        1.3.2. (What comparative studies exist on C-S among children in Spain and Germany?): sent6
        1.3.3. (What studies focus on C-S among bilingual children in the Netherlands?): sent7
        1.3.4. (What studies investigate C-S in interactions between bilingual children and their parents?): sent8
        1.3.5. (What studies observe C-S in educational settings?): sent9
        1.3.6. (What studies report the use of C-S for resolving disputes among children?): sent10
2. (Does C-S occur only between standard languages?): sent11
    2.1. (What are some examples of C-S between minority languages and dialects?): sent12, sent13, sent14
    2.2. (How is C-S analyzed in computer-mediated interactions?): sent15
3. (How is C-S observable across immigrant contexts in Europe?): sent16
    3.1. (What are some studies on C-S in the UK?): sent16, sent17, sent18
    3.2. (What are some studies on C-S among immigrant communities in Germany?): sent19
    3.3. (What are some studies on C-S among immigrant communities in the Netherlands?): sent20
    3.4. (What are some studies on C-S among immigrant communities in Belgium?): sent21
    3.5. (What are some studies on C-S among immigrant communities in Spain?): sent22
    3.6. (What studies analyze C-S within computer-mediated communication among immigrant communities?): sent23
        3.6.1. (What are some specific studies on C-S in computer-mediated communication in Germany and the Netherlands?): sent24
        3.6.2. (What study compares French-Arabic C-S in computer-mediated interaction across Moroccan communities in France and the UK?): sent25
4. (Are there studies on C-S in historical documents?): sent26
    4.1. (What are some specific studies on C-S in historical documents?): sent27, sent28, sent29
5. (How is C-S observed within the media context?): sent30
    5.1. (What are some specific studies on English C-S in written media?): sent30, sent31
6. (Can institutions and/or organizations have multilingual communication?): sent32
    6.1. (What study analyzed C-S and language choice at the institutional level for European Union institutions?): sent33",What are some examples of code-switching between minority languages and dialects?,"sent12, sent13, sent14, sent11","For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (2006) focused on the C-S between Welsh and English in the UK.

Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy.

In the Balkans, Kyuchukov (2006) analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria.

C-S does not only take place between standard languages but between minority languages and dialects as well.","Themistocleous (2013) studied code-switching between Greek and Cypriot Greek, and Deuchar (2006) focused on code-switching between Welsh and English in the UK.

Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy.

In the Balkans, Kyuchukov (2006) analyzed code-switching between Turkish-Bulgarian and Romani in Bulgaria.

Code-switching does not only take place between standard languages but also between minority languages and dialects.","Questions:

What are some examples of code-switching between minority languages and dialects?

Answer:

Themistocleous (2013) studied code-switching between Greek and Cypriot Greek, and Deuchar (2006) focused on code-switching between Welsh and English in the UK.

Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy.

In the Balkans, Kyuchukov (2006) analyzed code-switching between Turkish-Bulgarian and Romani in Bulgaria.

Code-switching does not only take place between standard languages but also between minority languages and dialects."
A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,C-S across Languages: European Context,"The contexts in which people acquire and use multiple languages in Europe are diverse. Some acquire their languages simultaneously from birth, while others acquire them sequentially, either naturally or via explicit instruction. Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors. Speakers who use local dialects or minoritized varieties may also be engaged in C-S when switching between their variety and a dominant one (Mills and Washington, 2015;Blom and Gumperz, 1972). C-S in bilingual language acquisition of children has been studied across language contact contexts in Europe. In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children. From a comparative perspective, Poeste et al. (2019) analyzed C-S among bilingual, trilingual, and multilingual children growing up in Spain and Germany. In the Netherlands, Bosma and Blom (2019) focused on C-S among bilingual Frisian-Dutch children. In addition to analyzing C-S in children's speech, Juan-Garau and Perez-Vidal (2001) and Lanza (1998) investigated C-S in the interaction patterns between bilingual children and their parents (i.e. Spanish-Catalan and English-Norwegian respectively).

Within an educational setting, Kleeman (2012) observed C-S among bilingual (North Sami-Norwegian) kindergarten children in the North of Norway. Similarly, Jørgensen (1998) and Cromdal (2004) report the use of C-S for resolving disputes among bilingual (Turkish-Danish) children in Denmark and multilingual (Swedish-English and/or a Non-Scandinavian Language) children in Sweden respectively.

C-S does not only take place between standard languages but between minority languages and dialects as well. For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (2006) focused on the C-S between Welsh and English in the UK. Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy. In the Balkans, Kyuchukov (2006) analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria. C-S between dialects and/or standard vs. minority languages in computer mediated interaction was analyzed by Siebenhaar (2006) among Swiss-German dialects and by Robert-Tissot and Morel (2017) through SMS corpora collected across Germanic (i.e. English and German) and Romance languages (French, Spanish, Italian) in Switzerland.

C-S is commonly observable across immigrant contexts in Europe. In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (2006) focused on the C-S between English and Cypriot Turkish communities in London. Wei and Milroy (1995) analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (Chinese-English) families in Northeastern England. In addition, Ożańska-Ponikwia (2016) investigated the Polish-English C-S in the UK as well. C-S among immigrant community members have also been widely studied in Germany (e.g. Turkish-German C-S by Keim (2008) and Ç etinoglu (2017), Russian-German C-S by Khakimov (2016)). In the Netherlands, C-S studies include Turkish-Dutch C-S by Backus (2010) and Dutch-Morroccan C-S by Nortier (1990). In Belgium, Meeuws and Blommaert (1998) studied the French-Lingala-Swahili C-S among immigrants of Zaire and Treffers-Daller (1994) studied French-Dutch C-S in Brussels. In Spain, Jieanu (2013) describes the Romanian-Spanish C-S among the Romanian immigrants. In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well. These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands. Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.

In addition to daily communication, some linguists are also interested in the C-S observed in historical documents. While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire. Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations. In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.

Within the media context, Martin (1998) investigated English C-S in written French advertising, and Onysko (2007) investigated the English C-S in German written media through corpus analyses. Zhiganova (2016) indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.

Similar to humans, institutions and/or organizations could also have multilingual communication with their members and/or audience. For example, Wodak et al. (2012) analyzed the C-S and language choice at the institutional level for European Union institutions.","# QUA Analysis:

1. (What are the contexts in which people acquire and use multiple languages in Europe?): sent1
    1.1. (How do people acquire multiple languages in Europe?): sent2
    1.2. (What is the norm regarding multilingualism in certain zones?): sent3
    1.3. (How does C-S manifest in bilingual language acquisition of children in Europe?): sent4
        1.3.1. (What are some specific studies on C-S in bilingual children in Germany?): sent5
        1.3.2. (What comparative studies exist on C-S among children in Spain and Germany?): sent6
        1.3.3. (What studies focus on C-S among bilingual children in the Netherlands?): sent7
        1.3.4. (What studies investigate C-S in interactions between bilingual children and their parents?): sent8
        1.3.5. (What studies observe C-S in educational settings?): sent9
        1.3.6. (What studies report the use of C-S for resolving disputes among children?): sent10
2. (Does C-S occur only between standard languages?): sent11
    2.1. (What are some examples of C-S between minority languages and dialects?): sent12, sent13, sent14
    2.2. (How is C-S analyzed in computer-mediated interactions?): sent15
3. (How is C-S observable across immigrant contexts in Europe?): sent16
    3.1. (What are some studies on C-S in the UK?): sent16, sent17, sent18
    3.2. (What are some studies on C-S among immigrant communities in Germany?): sent19
    3.3. (What are some studies on C-S among immigrant communities in the Netherlands?): sent20
    3.4. (What are some studies on C-S among immigrant communities in Belgium?): sent21
    3.5. (What are some studies on C-S among immigrant communities in Spain?): sent22
    3.6. (What studies analyze C-S within computer-mediated communication among immigrant communities?): sent23
        3.6.1. (What are some specific studies on C-S in computer-mediated communication in Germany and the Netherlands?): sent24
        3.6.2. (What study compares French-Arabic C-S in computer-mediated interaction across Moroccan communities in France and the UK?): sent25
4. (Are there studies on C-S in historical documents?): sent26
    4.1. (What are some specific studies on C-S in historical documents?): sent27, sent28, sent29
5. (How is C-S observed within the media context?): sent30
    5.1. (What are some specific studies on English C-S in written media?): sent30, sent31
6. (Can institutions and/or organizations have multilingual communication?): sent32
    6.1. (What study analyzed C-S and language choice at the institutional level for European Union institutions?): sent33",What are some studies on code-switching in the UK?,"sent16, sent17, sent18, sent3","In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (2006) focused on the C-S between English and Cypriot Turkish communities in London.

Wei and Milroy (1995) analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (Chinese-English) families in Northeastern England.

In addition, Ożańska-Ponikwia (2016) investigated the Polish-English C-S in the UK as well.

Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors.","Some studies on code-switching in the UK include Georgakopoulou and Finnis (2009), who described the C-S patterns between English and Cypriot Greek, and Issa (2006), who focused on the C-S between English and Cypriot Turkish communities in London.

Wei and Milroy (1995) analyzed the code-switching between English and Chinese from a conversational analysis perspective based on the interactions of bilingual (Chinese-English) families in Northeastern England.

In addition, Ożańska-Ponikwia (2016) investigated the Polish-English code-switching in the UK as well.

Multilingualism is common in many areas where local residents may speak different languages to accommodate their interlocutors.","Questions:

What are some studies on code-switching in the UK?

Answer:

Some studies on code-switching in the UK include Georgakopoulou and Finnis (2009), who described the C-S patterns between English and Cypriot Greek, and Issa (2006), who focused on the C-S between English and Cypriot Turkish communities in London.

Wei and Milroy (1995) analyzed the code-switching between English and Chinese from a conversational analysis perspective based on the interactions of bilingual (Chinese-English) families in Northeastern England.

In addition, Ożańska-Ponikwia (2016) investigated the Polish-English code-switching in the UK as well.

Multilingualism is common in many areas where local residents may speak different languages to accommodate their interlocutors."
A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,C-S across Languages: European Context,"The contexts in which people acquire and use multiple languages in Europe are diverse. Some acquire their languages simultaneously from birth, while others acquire them sequentially, either naturally or via explicit instruction. Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors. Speakers who use local dialects or minoritized varieties may also be engaged in C-S when switching between their variety and a dominant one (Mills and Washington, 2015;Blom and Gumperz, 1972). C-S in bilingual language acquisition of children has been studied across language contact contexts in Europe. In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children. From a comparative perspective, Poeste et al. (2019) analyzed C-S among bilingual, trilingual, and multilingual children growing up in Spain and Germany. In the Netherlands, Bosma and Blom (2019) focused on C-S among bilingual Frisian-Dutch children. In addition to analyzing C-S in children's speech, Juan-Garau and Perez-Vidal (2001) and Lanza (1998) investigated C-S in the interaction patterns between bilingual children and their parents (i.e. Spanish-Catalan and English-Norwegian respectively).

Within an educational setting, Kleeman (2012) observed C-S among bilingual (North Sami-Norwegian) kindergarten children in the North of Norway. Similarly, Jørgensen (1998) and Cromdal (2004) report the use of C-S for resolving disputes among bilingual (Turkish-Danish) children in Denmark and multilingual (Swedish-English and/or a Non-Scandinavian Language) children in Sweden respectively.

C-S does not only take place between standard languages but between minority languages and dialects as well. For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (2006) focused on the C-S between Welsh and English in the UK. Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy. In the Balkans, Kyuchukov (2006) analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria. C-S between dialects and/or standard vs. minority languages in computer mediated interaction was analyzed by Siebenhaar (2006) among Swiss-German dialects and by Robert-Tissot and Morel (2017) through SMS corpora collected across Germanic (i.e. English and German) and Romance languages (French, Spanish, Italian) in Switzerland.

C-S is commonly observable across immigrant contexts in Europe. In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (2006) focused on the C-S between English and Cypriot Turkish communities in London. Wei and Milroy (1995) analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (Chinese-English) families in Northeastern England. In addition, Ożańska-Ponikwia (2016) investigated the Polish-English C-S in the UK as well. C-S among immigrant community members have also been widely studied in Germany (e.g. Turkish-German C-S by Keim (2008) and Ç etinoglu (2017), Russian-German C-S by Khakimov (2016)). In the Netherlands, C-S studies include Turkish-Dutch C-S by Backus (2010) and Dutch-Morroccan C-S by Nortier (1990). In Belgium, Meeuws and Blommaert (1998) studied the French-Lingala-Swahili C-S among immigrants of Zaire and Treffers-Daller (1994) studied French-Dutch C-S in Brussels. In Spain, Jieanu (2013) describes the Romanian-Spanish C-S among the Romanian immigrants. In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well. These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands. Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.

In addition to daily communication, some linguists are also interested in the C-S observed in historical documents. While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire. Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations. In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.

Within the media context, Martin (1998) investigated English C-S in written French advertising, and Onysko (2007) investigated the English C-S in German written media through corpus analyses. Zhiganova (2016) indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.

Similar to humans, institutions and/or organizations could also have multilingual communication with their members and/or audience. For example, Wodak et al. (2012) analyzed the C-S and language choice at the institutional level for European Union institutions.","# QUA Analysis:

1. (What are the contexts in which people acquire and use multiple languages in Europe?): sent1
    1.1. (How do people acquire multiple languages in Europe?): sent2
    1.2. (What is the norm regarding multilingualism in certain zones?): sent3
    1.3. (How does C-S manifest in bilingual language acquisition of children in Europe?): sent4
        1.3.1. (What are some specific studies on C-S in bilingual children in Germany?): sent5
        1.3.2. (What comparative studies exist on C-S among children in Spain and Germany?): sent6
        1.3.3. (What studies focus on C-S among bilingual children in the Netherlands?): sent7
        1.3.4. (What studies investigate C-S in interactions between bilingual children and their parents?): sent8
        1.3.5. (What studies observe C-S in educational settings?): sent9
        1.3.6. (What studies report the use of C-S for resolving disputes among children?): sent10
2. (Does C-S occur only between standard languages?): sent11
    2.1. (What are some examples of C-S between minority languages and dialects?): sent12, sent13, sent14
    2.2. (How is C-S analyzed in computer-mediated interactions?): sent15
3. (How is C-S observable across immigrant contexts in Europe?): sent16
    3.1. (What are some studies on C-S in the UK?): sent16, sent17, sent18
    3.2. (What are some studies on C-S among immigrant communities in Germany?): sent19
    3.3. (What are some studies on C-S among immigrant communities in the Netherlands?): sent20
    3.4. (What are some studies on C-S among immigrant communities in Belgium?): sent21
    3.5. (What are some studies on C-S among immigrant communities in Spain?): sent22
    3.6. (What studies analyze C-S within computer-mediated communication among immigrant communities?): sent23
        3.6.1. (What are some specific studies on C-S in computer-mediated communication in Germany and the Netherlands?): sent24
        3.6.2. (What study compares French-Arabic C-S in computer-mediated interaction across Moroccan communities in France and the UK?): sent25
4. (Are there studies on C-S in historical documents?): sent26
    4.1. (What are some specific studies on C-S in historical documents?): sent27, sent28, sent29
5. (How is C-S observed within the media context?): sent30
    5.1. (What are some specific studies on English C-S in written media?): sent30, sent31
6. (Can institutions and/or organizations have multilingual communication?): sent32
    6.1. (What study analyzed C-S and language choice at the institutional level for European Union institutions?): sent33",What are some specific studies on code-switching in computer-mediated communication in Germany and the Netherlands?,"sent24, sent23, sent3","These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands.

In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well.

Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors.","Specific studies on code-switching in computer-mediated communication include Greek-German code-switching by Androutsopoulos (2015) in Germany, Turkish-Dutch code-switching by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015), and a comparison of Turkish-Dutch and Moroccan-Dutch code-switching by Dorleijn and Nortier (2009) in the Netherlands.

In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer-mediated communication.

Multilingualism is common in many areas where local residents may speak different languages to accommodate their interlocutors.","Questions:

What are some specific studies on code-switching in computer-mediated communication in Germany and the Netherlands?

Answer:

Specific studies on code-switching in computer-mediated communication include Greek-German code-switching by Androutsopoulos (2015) in Germany, Turkish-Dutch code-switching by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015), and a comparison of Turkish-Dutch and Moroccan-Dutch code-switching by Dorleijn and Nortier (2009) in the Netherlands.

In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer-mediated communication.

Multilingualism is common in many areas where local residents may speak different languages to accommodate their interlocutors."
A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,C-S across Languages: European Context,"The contexts in which people acquire and use multiple languages in Europe are diverse. Some acquire their languages simultaneously from birth, while others acquire them sequentially, either naturally or via explicit instruction. Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors. Speakers who use local dialects or minoritized varieties may also be engaged in C-S when switching between their variety and a dominant one (Mills and Washington, 2015;Blom and Gumperz, 1972). C-S in bilingual language acquisition of children has been studied across language contact contexts in Europe. In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children. From a comparative perspective, Poeste et al. (2019) analyzed C-S among bilingual, trilingual, and multilingual children growing up in Spain and Germany. In the Netherlands, Bosma and Blom (2019) focused on C-S among bilingual Frisian-Dutch children. In addition to analyzing C-S in children's speech, Juan-Garau and Perez-Vidal (2001) and Lanza (1998) investigated C-S in the interaction patterns between bilingual children and their parents (i.e. Spanish-Catalan and English-Norwegian respectively).

Within an educational setting, Kleeman (2012) observed C-S among bilingual (North Sami-Norwegian) kindergarten children in the North of Norway. Similarly, Jørgensen (1998) and Cromdal (2004) report the use of C-S for resolving disputes among bilingual (Turkish-Danish) children in Denmark and multilingual (Swedish-English and/or a Non-Scandinavian Language) children in Sweden respectively.

C-S does not only take place between standard languages but between minority languages and dialects as well. For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (2006) focused on the C-S between Welsh and English in the UK. Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy. In the Balkans, Kyuchukov (2006) analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria. C-S between dialects and/or standard vs. minority languages in computer mediated interaction was analyzed by Siebenhaar (2006) among Swiss-German dialects and by Robert-Tissot and Morel (2017) through SMS corpora collected across Germanic (i.e. English and German) and Romance languages (French, Spanish, Italian) in Switzerland.

C-S is commonly observable across immigrant contexts in Europe. In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (2006) focused on the C-S between English and Cypriot Turkish communities in London. Wei and Milroy (1995) analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (Chinese-English) families in Northeastern England. In addition, Ożańska-Ponikwia (2016) investigated the Polish-English C-S in the UK as well. C-S among immigrant community members have also been widely studied in Germany (e.g. Turkish-German C-S by Keim (2008) and Ç etinoglu (2017), Russian-German C-S by Khakimov (2016)). In the Netherlands, C-S studies include Turkish-Dutch C-S by Backus (2010) and Dutch-Morroccan C-S by Nortier (1990). In Belgium, Meeuws and Blommaert (1998) studied the French-Lingala-Swahili C-S among immigrants of Zaire and Treffers-Daller (1994) studied French-Dutch C-S in Brussels. In Spain, Jieanu (2013) describes the Romanian-Spanish C-S among the Romanian immigrants. In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well. These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands. Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.

In addition to daily communication, some linguists are also interested in the C-S observed in historical documents. While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire. Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations. In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.

Within the media context, Martin (1998) investigated English C-S in written French advertising, and Onysko (2007) investigated the English C-S in German written media through corpus analyses. Zhiganova (2016) indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.

Similar to humans, institutions and/or organizations could also have multilingual communication with their members and/or audience. For example, Wodak et al. (2012) analyzed the C-S and language choice at the institutional level for European Union institutions.","# QUA Analysis:

1. (What are the contexts in which people acquire and use multiple languages in Europe?): sent1
    1.1. (How do people acquire multiple languages in Europe?): sent2
    1.2. (What is the norm regarding multilingualism in certain zones?): sent3
    1.3. (How does C-S manifest in bilingual language acquisition of children in Europe?): sent4
        1.3.1. (What are some specific studies on C-S in bilingual children in Germany?): sent5
        1.3.2. (What comparative studies exist on C-S among children in Spain and Germany?): sent6
        1.3.3. (What studies focus on C-S among bilingual children in the Netherlands?): sent7
        1.3.4. (What studies investigate C-S in interactions between bilingual children and their parents?): sent8
        1.3.5. (What studies observe C-S in educational settings?): sent9
        1.3.6. (What studies report the use of C-S for resolving disputes among children?): sent10
2. (Does C-S occur only between standard languages?): sent11
    2.1. (What are some examples of C-S between minority languages and dialects?): sent12, sent13, sent14
    2.2. (How is C-S analyzed in computer-mediated interactions?): sent15
3. (How is C-S observable across immigrant contexts in Europe?): sent16
    3.1. (What are some studies on C-S in the UK?): sent16, sent17, sent18
    3.2. (What are some studies on C-S among immigrant communities in Germany?): sent19
    3.3. (What are some studies on C-S among immigrant communities in the Netherlands?): sent20
    3.4. (What are some studies on C-S among immigrant communities in Belgium?): sent21
    3.5. (What are some studies on C-S among immigrant communities in Spain?): sent22
    3.6. (What studies analyze C-S within computer-mediated communication among immigrant communities?): sent23
        3.6.1. (What are some specific studies on C-S in computer-mediated communication in Germany and the Netherlands?): sent24
        3.6.2. (What study compares French-Arabic C-S in computer-mediated interaction across Moroccan communities in France and the UK?): sent25
4. (Are there studies on C-S in historical documents?): sent26
    4.1. (What are some specific studies on C-S in historical documents?): sent27, sent28, sent29
5. (How is C-S observed within the media context?): sent30
    5.1. (What are some specific studies on English C-S in written media?): sent30, sent31
6. (Can institutions and/or organizations have multilingual communication?): sent32
    6.1. (What study analyzed C-S and language choice at the institutional level for European Union institutions?): sent33",What study compares French-Arabic code-switching in computer-mediated interaction across Moroccan communities in France and the UK?,"sent25, sent23, sent3","Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.

In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well.

Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors.","Marley (2011) compared French-Arabic code-switching within computer-mediated interaction across Moroccan communities in France and the UK.

In addition to the analyses within spoken interactions of immigrant communities across Europe, there are also studies about code-switching within computer-mediated communication.

Multilingualism is common in many areas where local residents may speak different languages to accommodate their interlocutors.","Questions:

What study compares French-Arabic code-switching in computer-mediated interaction across Moroccan communities in France and the UK?

Answer:

Marley (2011) compared French-Arabic code-switching within computer-mediated interaction across Moroccan communities in France and the UK.

In addition to the analyses within spoken interactions of immigrant communities across Europe, there are also studies about code-switching within computer-mediated communication.

Multilingualism is common in many areas where local residents may speak different languages to accommodate their interlocutors."
A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,C-S across Languages: European Context,"The contexts in which people acquire and use multiple languages in Europe are diverse. Some acquire their languages simultaneously from birth, while others acquire them sequentially, either naturally or via explicit instruction. Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors. Speakers who use local dialects or minoritized varieties may also be engaged in C-S when switching between their variety and a dominant one (Mills and Washington, 2015;Blom and Gumperz, 1972). C-S in bilingual language acquisition of children has been studied across language contact contexts in Europe. In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children. From a comparative perspective, Poeste et al. (2019) analyzed C-S among bilingual, trilingual, and multilingual children growing up in Spain and Germany. In the Netherlands, Bosma and Blom (2019) focused on C-S among bilingual Frisian-Dutch children. In addition to analyzing C-S in children's speech, Juan-Garau and Perez-Vidal (2001) and Lanza (1998) investigated C-S in the interaction patterns between bilingual children and their parents (i.e. Spanish-Catalan and English-Norwegian respectively).

Within an educational setting, Kleeman (2012) observed C-S among bilingual (North Sami-Norwegian) kindergarten children in the North of Norway. Similarly, Jørgensen (1998) and Cromdal (2004) report the use of C-S for resolving disputes among bilingual (Turkish-Danish) children in Denmark and multilingual (Swedish-English and/or a Non-Scandinavian Language) children in Sweden respectively.

C-S does not only take place between standard languages but between minority languages and dialects as well. For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (2006) focused on the C-S between Welsh and English in the UK. Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy. In the Balkans, Kyuchukov (2006) analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria. C-S between dialects and/or standard vs. minority languages in computer mediated interaction was analyzed by Siebenhaar (2006) among Swiss-German dialects and by Robert-Tissot and Morel (2017) through SMS corpora collected across Germanic (i.e. English and German) and Romance languages (French, Spanish, Italian) in Switzerland.

C-S is commonly observable across immigrant contexts in Europe. In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (2006) focused on the C-S between English and Cypriot Turkish communities in London. Wei and Milroy (1995) analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (Chinese-English) families in Northeastern England. In addition, Ożańska-Ponikwia (2016) investigated the Polish-English C-S in the UK as well. C-S among immigrant community members have also been widely studied in Germany (e.g. Turkish-German C-S by Keim (2008) and Ç etinoglu (2017), Russian-German C-S by Khakimov (2016)). In the Netherlands, C-S studies include Turkish-Dutch C-S by Backus (2010) and Dutch-Morroccan C-S by Nortier (1990). In Belgium, Meeuws and Blommaert (1998) studied the French-Lingala-Swahili C-S among immigrants of Zaire and Treffers-Daller (1994) studied French-Dutch C-S in Brussels. In Spain, Jieanu (2013) describes the Romanian-Spanish C-S among the Romanian immigrants. In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well. These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands. Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.

In addition to daily communication, some linguists are also interested in the C-S observed in historical documents. While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire. Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations. In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.

Within the media context, Martin (1998) investigated English C-S in written French advertising, and Onysko (2007) investigated the English C-S in German written media through corpus analyses. Zhiganova (2016) indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.

Similar to humans, institutions and/or organizations could also have multilingual communication with their members and/or audience. For example, Wodak et al. (2012) analyzed the C-S and language choice at the institutional level for European Union institutions.","# QUA Analysis:

1. (What are the contexts in which people acquire and use multiple languages in Europe?): sent1
    1.1. (How do people acquire multiple languages in Europe?): sent2
    1.2. (What is the norm regarding multilingualism in certain zones?): sent3
    1.3. (How does C-S manifest in bilingual language acquisition of children in Europe?): sent4
        1.3.1. (What are some specific studies on C-S in bilingual children in Germany?): sent5
        1.3.2. (What comparative studies exist on C-S among children in Spain and Germany?): sent6
        1.3.3. (What studies focus on C-S among bilingual children in the Netherlands?): sent7
        1.3.4. (What studies investigate C-S in interactions between bilingual children and their parents?): sent8
        1.3.5. (What studies observe C-S in educational settings?): sent9
        1.3.6. (What studies report the use of C-S for resolving disputes among children?): sent10
2. (Does C-S occur only between standard languages?): sent11
    2.1. (What are some examples of C-S between minority languages and dialects?): sent12, sent13, sent14
    2.2. (How is C-S analyzed in computer-mediated interactions?): sent15
3. (How is C-S observable across immigrant contexts in Europe?): sent16
    3.1. (What are some studies on C-S in the UK?): sent16, sent17, sent18
    3.2. (What are some studies on C-S among immigrant communities in Germany?): sent19
    3.3. (What are some studies on C-S among immigrant communities in the Netherlands?): sent20
    3.4. (What are some studies on C-S among immigrant communities in Belgium?): sent21
    3.5. (What are some studies on C-S among immigrant communities in Spain?): sent22
    3.6. (What studies analyze C-S within computer-mediated communication among immigrant communities?): sent23
        3.6.1. (What are some specific studies on C-S in computer-mediated communication in Germany and the Netherlands?): sent24
        3.6.2. (What study compares French-Arabic C-S in computer-mediated interaction across Moroccan communities in France and the UK?): sent25
4. (Are there studies on C-S in historical documents?): sent26
    4.1. (What are some specific studies on C-S in historical documents?): sent27, sent28, sent29
5. (How is C-S observed within the media context?): sent30
    5.1. (What are some specific studies on English C-S in written media?): sent30, sent31
6. (Can institutions and/or organizations have multilingual communication?): sent32
    6.1. (What study analyzed C-S and language choice at the institutional level for European Union institutions?): sent33",What are some specific studies on code-switching in historical documents?,"sent27, sent28, sent29, sent26","While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire.

Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations.

In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.

In addition to daily communication, some linguists are also interested in the C-S observed in historical documents.","Swain (2002) explored Latin-Greek code-switching by Cicero, while Dunkel (2000) analyzed code-switching in Cicero's communication with Atticus in the Roman Empire.

Argenter (2001) reports cases of language mixing within the Catalan Jewish community in the 14th and 15th centuries, and Rothman (2011) highlights code-switching between Italian, Slavic, and Turkish in historical documents about Ottoman-Venetian relations.

In Switzerland, Volk and Clematide (2014) worked on detecting and annotating code-switching patterns in the diachronic and multilingual Alpine Heritage corpus.

In addition to daily communication, some linguists are also interested in the code-switching observed in historical documents.","Questions:

What are some specific studies on code-switching in historical documents?

Answer:

Swain (2002) explored Latin-Greek code-switching by Cicero, while Dunkel (2000) analyzed code-switching in Cicero's communication with Atticus in the Roman Empire.

Argenter (2001) reports cases of language mixing within the Catalan Jewish community in the 14th and 15th centuries, and Rothman (2011) highlights code-switching between Italian, Slavic, and Turkish in historical documents about Ottoman-Venetian relations.

In Switzerland, Volk and Clematide (2014) worked on detecting and annotating code-switching patterns in the diachronic and multilingual Alpine Heritage corpus.

In addition to daily communication, some linguists are also interested in the code-switching observed in historical documents."
A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,C-S across Languages: European Context,"The contexts in which people acquire and use multiple languages in Europe are diverse. Some acquire their languages simultaneously from birth, while others acquire them sequentially, either naturally or via explicit instruction. Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors. Speakers who use local dialects or minoritized varieties may also be engaged in C-S when switching between their variety and a dominant one (Mills and Washington, 2015;Blom and Gumperz, 1972). C-S in bilingual language acquisition of children has been studied across language contact contexts in Europe. In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children. From a comparative perspective, Poeste et al. (2019) analyzed C-S among bilingual, trilingual, and multilingual children growing up in Spain and Germany. In the Netherlands, Bosma and Blom (2019) focused on C-S among bilingual Frisian-Dutch children. In addition to analyzing C-S in children's speech, Juan-Garau and Perez-Vidal (2001) and Lanza (1998) investigated C-S in the interaction patterns between bilingual children and their parents (i.e. Spanish-Catalan and English-Norwegian respectively).

Within an educational setting, Kleeman (2012) observed C-S among bilingual (North Sami-Norwegian) kindergarten children in the North of Norway. Similarly, Jørgensen (1998) and Cromdal (2004) report the use of C-S for resolving disputes among bilingual (Turkish-Danish) children in Denmark and multilingual (Swedish-English and/or a Non-Scandinavian Language) children in Sweden respectively.

C-S does not only take place between standard languages but between minority languages and dialects as well. For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (2006) focused on the C-S between Welsh and English in the UK. Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy. In the Balkans, Kyuchukov (2006) analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria. C-S between dialects and/or standard vs. minority languages in computer mediated interaction was analyzed by Siebenhaar (2006) among Swiss-German dialects and by Robert-Tissot and Morel (2017) through SMS corpora collected across Germanic (i.e. English and German) and Romance languages (French, Spanish, Italian) in Switzerland.

C-S is commonly observable across immigrant contexts in Europe. In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (2006) focused on the C-S between English and Cypriot Turkish communities in London. Wei and Milroy (1995) analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (Chinese-English) families in Northeastern England. In addition, Ożańska-Ponikwia (2016) investigated the Polish-English C-S in the UK as well. C-S among immigrant community members have also been widely studied in Germany (e.g. Turkish-German C-S by Keim (2008) and Ç etinoglu (2017), Russian-German C-S by Khakimov (2016)). In the Netherlands, C-S studies include Turkish-Dutch C-S by Backus (2010) and Dutch-Morroccan C-S by Nortier (1990). In Belgium, Meeuws and Blommaert (1998) studied the French-Lingala-Swahili C-S among immigrants of Zaire and Treffers-Daller (1994) studied French-Dutch C-S in Brussels. In Spain, Jieanu (2013) describes the Romanian-Spanish C-S among the Romanian immigrants. In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well. These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands. Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.

In addition to daily communication, some linguists are also interested in the C-S observed in historical documents. While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire. Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations. In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.

Within the media context, Martin (1998) investigated English C-S in written French advertising, and Onysko (2007) investigated the English C-S in German written media through corpus analyses. Zhiganova (2016) indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.

Similar to humans, institutions and/or organizations could also have multilingual communication with their members and/or audience. For example, Wodak et al. (2012) analyzed the C-S and language choice at the institutional level for European Union institutions.","# QUA Analysis:

1. (What are the contexts in which people acquire and use multiple languages in Europe?): sent1
    1.1. (How do people acquire multiple languages in Europe?): sent2
    1.2. (What is the norm regarding multilingualism in certain zones?): sent3
    1.3. (How does C-S manifest in bilingual language acquisition of children in Europe?): sent4
        1.3.1. (What are some specific studies on C-S in bilingual children in Germany?): sent5
        1.3.2. (What comparative studies exist on C-S among children in Spain and Germany?): sent6
        1.3.3. (What studies focus on C-S among bilingual children in the Netherlands?): sent7
        1.3.4. (What studies investigate C-S in interactions between bilingual children and their parents?): sent8
        1.3.5. (What studies observe C-S in educational settings?): sent9
        1.3.6. (What studies report the use of C-S for resolving disputes among children?): sent10
2. (Does C-S occur only between standard languages?): sent11
    2.1. (What are some examples of C-S between minority languages and dialects?): sent12, sent13, sent14
    2.2. (How is C-S analyzed in computer-mediated interactions?): sent15
3. (How is C-S observable across immigrant contexts in Europe?): sent16
    3.1. (What are some studies on C-S in the UK?): sent16, sent17, sent18
    3.2. (What are some studies on C-S among immigrant communities in Germany?): sent19
    3.3. (What are some studies on C-S among immigrant communities in the Netherlands?): sent20
    3.4. (What are some studies on C-S among immigrant communities in Belgium?): sent21
    3.5. (What are some studies on C-S among immigrant communities in Spain?): sent22
    3.6. (What studies analyze C-S within computer-mediated communication among immigrant communities?): sent23
        3.6.1. (What are some specific studies on C-S in computer-mediated communication in Germany and the Netherlands?): sent24
        3.6.2. (What study compares French-Arabic C-S in computer-mediated interaction across Moroccan communities in France and the UK?): sent25
4. (Are there studies on C-S in historical documents?): sent26
    4.1. (What are some specific studies on C-S in historical documents?): sent27, sent28, sent29
5. (How is C-S observed within the media context?): sent30
    5.1. (What are some specific studies on English C-S in written media?): sent30, sent31
6. (Can institutions and/or organizations have multilingual communication?): sent32
    6.1. (What study analyzed C-S and language choice at the institutional level for European Union institutions?): sent33",What are some specific studies on English code-switching in written media?,"sent30, sent31, sent5","Within the media context, Martin (1998) investigated English C-S in written French advertising, and Onysko (2007) investigated the English C-S in German written media through corpus analyses.

Zhiganova (2016) indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.

In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children.","Martin (1998) investigated English code-switching in written French advertising, and Onysko (2007) investigated English code-switching in German written media through corpus analyses.

Zhiganova (2016) indicates that German speakers perceive code-switching into English for advertising purposes with both positive and negative consequences.

In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German code-switching, and Meisel (1994) on German-French code-switching of bilingual children.","Questions:

What are some specific studies on English code-switching in written media?

Answer:

Martin (1998) investigated English code-switching in written French advertising, and Onysko (2007) investigated English code-switching in German written media through corpus analyses.

Zhiganova (2016) indicates that German speakers perceive code-switching into English for advertising purposes with both positive and negative consequences.

In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German code-switching, and Meisel (1994) on German-French code-switching of bilingual children."
A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,C-S across Languages: Indian Context,"According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual. There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages). Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families. The census excludes languages with a population lower than 10,000 speakers. Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism. Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.

In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages. Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences. Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (i.e. social and stylistic) aspects of Hindi-English C-S from a theoretical point of view. More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele. Another observation about C-S in daily life comes from Boro (2020) who provides examples of English, Assamese and Bodo (another language spoken in the Assam region) C-S and borrowings. In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India. For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages (i.e. Kannada and Telugu) spoken in India.

Instead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (probably) encountered and collected by the authors themselves in daily life interactions over a period of time with limited means. Nowadays, these small sets of data would be regarded as insignificant in computational areas of research. However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost. For example, Nadkarni (1975) proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India. Both languages have been in contact with each other for over four hundred years. Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada. Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context. Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S. However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975).

C-S in India has been investigated through written media, advertising and film industry as well. Si (2011) analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004. Her results indicate a change of direction C-S over the years. More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions. A similar trend has been observed for Bengali movie scripts as well. Through analyzing movie scripts (between 1970s and 2010s), Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g. renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society. Within the immigrant context, Gardner-Chloros and Charles (2007) focused on the types and functions of C-S between Hindi and English across the TV programs (e.g. highly scripted vs. loosely scripted programs) of a British/Asian cable channel in the UK. Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (i.e. highly scripted). In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use (instead of C-S). Similarly, Zipp (2017) analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters (group identity). Pratapa and Choudhury (2017) perform a quantitative study of 18 recent Bollywood (Hindi) movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.

From an advertising point of view, Kathpalia and Wee Ong (2015) analyzed C-S in Hinglish (i.e. Hindi, English, Urdu, Sanskrit according to their definition) billboards about the Amul brand in India. After compiling the advertisements on billboards (1191), they classified the structures and functions of C-S. Their results indicate more intrasentential C-S than intersentential ones on the billboards. In terms of function, the advertisers used C-S to indicate figures of speech (e.g. puns, associations, contradictory associations, word-creation and repetitions) to attract the attention of the target group. Mohanty (2006) provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum. In general, high-cost English Medium (EM) education is valued by upper-class and affluent families. Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages. Sridhar (2002) explains that C-S is commonly practiced among students in schools across India. In addition, she finds it unrealistic to ask the students to separate the two languages harshly. In immigrant contexts, Martin et al. (2006) investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK. Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006). In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali. There are also studies about Bengali-English C-S in the UK school settings (Pagett, 2006) and Bangladesh (Obaidullah, 2016) as well. However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.

In their study about aphasic patients, Shyamala Chengappa and Bhat (2004) report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group. However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (2019) find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English. Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well.","1. (What is the linguistic diversity in India?): sent1
    1.1. (What are the major languages in India?): sent2
    1.2. (What language families do most people in India speak?): sent3
    1.3. (What does the census exclude?): sent4
    1.4. (What is the implication of linguistic diversity and multilingualism in India?): sent5
2. (What are the early studies on C-S in India?): sent6
    2.1. (What does Kachru (1978) study?): sent6
        2.1.1. (What influences on Indian languages does Kachru mention?): sent7
    2.2. (What does Sridhar (1978) study?): sent8
    2.3. (What does Kumar (1986) study?): sent9
    2.4. (What does Doley (2013) study?): sent10
    2.5. (What does Boro (2020) study?): sent11
    2.6. (What other external influence on Indian languages is mentioned?): sent12
        2.6.1. (What does Kapp (1997) study?): sent13
3. (How were the C-S examples in early studies collected?): sent14
    3.1. (What is the current view on small sets of data in computational research?): sent15
    3.2. (What are the consequences of ignoring early studies and data?): sent16
        3.2.1. (What example does Nadkarni (1975) provide to prove this point?): sent17
            3.2.1.1. (What is the historical context of the languages in Nadkarni's study?): sent18
            3.2.1.2. (What is the language proficiency of Saraswat Brahmins?): sent19
            3.2.1.3. (What does Nadkarni attribute the language preference to?): sent20
            3.2.1.4. (What is the functional use of Kannada among Konkani speakers?): sent21
            3.2.1.5. (What is the language learning trend among Kannada speakers?): sent22
4. (How has C-S been investigated in media and entertainment?): sent23
    4.1. (What does Si (2011) analyze?): sent23
        4.1.1. (What are the results of Si's analysis?): sent24
            4.1.1.1. (What specific trend does Si observe?): sent25
    4.2. (What similar trend is observed in Bengali movie scripts?): sent26
        4.2.1. (What does Chatterjee (2016) find in Bengali movie scripts?): sent27
    4.3. (What does Gardner-Chloros and Charles (2007) study?): sent28
        4.3.1. (What are the findings of Gardner-Chloros and Charles regarding C-S in TV programs?): sent29
        4.3.2. (What general observation do they make about C-S on TV broadcasts?): sent30
    4.4. (What does Zipp (2017) study?): sent31
    4.5. (What do Pratapa and Choudhury (2017) study?): sent32
5. (How has C-S been investigated in advertising?): sent33
    5.1. (What do Kathpalia and Wee Ong (2015) analyze?): sent33
        5.1.1. (What are the results of their analysis?): sent34
            5.1.1.1. (What specific trend do they observe in C-S structures?): sent35
            5.1.1.2. (What functions of C-S do they identify?): sent36
6. (How has C-S been investigated in education?): sent37
    6.1. (What does Mohanty (2006) study?): sent37
        6.1.1. (What is the general view on high-cost English Medium education?): sent38
        6.1.2. (What does Mohanty question about low-cost EM education?): sent39
    6.2. (What does Sridhar (2002) explain about C-S in schools?): sent40
        6.2.1. (What does Sridhar find unrealistic?): sent41
    6.3. (What does Martin et al. (2006) investigate?): sent42
    6.4. (What shift does Al-Azami (2006) reveal?): sent43
        6.4.1. (What are the C-S patterns among Bengali immigrants?): sent44
    6.5. (What other studies are mentioned about Bengali-English C-S?): sent45
    6.6. (What is lacking in the comparison of Bengali-English C-S?): sent46
7. (How has C-S been investigated in health settings?): sent47
    7.1. (What do Shyamala Chengappa and Bhat (2004) report?): sent47
        7.1.1. (What differences do they find between aphasic patients and the control group?): sent48
    7.2. (What do Deepa and Shyamala (2019) find about C-S in dementia patients?): sent48
        7.2.1. (What insights do these studies offer?): sent49",What is the linguistic diversity and multilingualism situation in India according to the 2011 Census?,"sent1, sent2, sent3, sent4, sent5","According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual.

There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages).

Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families.

The census excludes languages with a population lower than 10,000 speakers.

Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism.","According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual.

India has 121 major languages and 1599 other languages, out of which 22 are scheduled languages with official recognition, including Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, and Urdu, spoken by almost 97% of the population.

Most of the population (93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families.

The census excludes languages with a population lower than 10,000 speakers.

Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and widespread multilingualism.","Questions:

What is the linguistic diversity and multilingualism situation in India according to the 2011 Census?

Answer:

According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual.

India has 121 major languages and 1599 other languages, out of which 22 are scheduled languages with official recognition, including Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, and Urdu, spoken by almost 97% of the population.

Most of the population (93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families.

The census excludes languages with a population lower than 10,000 speakers.

Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and widespread multilingualism."
A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,C-S across Languages: Indian Context,"According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual. There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages). Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families. The census excludes languages with a population lower than 10,000 speakers. Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism. Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.

In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages. Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences. Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (i.e. social and stylistic) aspects of Hindi-English C-S from a theoretical point of view. More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele. Another observation about C-S in daily life comes from Boro (2020) who provides examples of English, Assamese and Bodo (another language spoken in the Assam region) C-S and borrowings. In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India. For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages (i.e. Kannada and Telugu) spoken in India.

Instead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (probably) encountered and collected by the authors themselves in daily life interactions over a period of time with limited means. Nowadays, these small sets of data would be regarded as insignificant in computational areas of research. However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost. For example, Nadkarni (1975) proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India. Both languages have been in contact with each other for over four hundred years. Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada. Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context. Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S. However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975).

C-S in India has been investigated through written media, advertising and film industry as well. Si (2011) analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004. Her results indicate a change of direction C-S over the years. More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions. A similar trend has been observed for Bengali movie scripts as well. Through analyzing movie scripts (between 1970s and 2010s), Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g. renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society. Within the immigrant context, Gardner-Chloros and Charles (2007) focused on the types and functions of C-S between Hindi and English across the TV programs (e.g. highly scripted vs. loosely scripted programs) of a British/Asian cable channel in the UK. Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (i.e. highly scripted). In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use (instead of C-S). Similarly, Zipp (2017) analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters (group identity). Pratapa and Choudhury (2017) perform a quantitative study of 18 recent Bollywood (Hindi) movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.

From an advertising point of view, Kathpalia and Wee Ong (2015) analyzed C-S in Hinglish (i.e. Hindi, English, Urdu, Sanskrit according to their definition) billboards about the Amul brand in India. After compiling the advertisements on billboards (1191), they classified the structures and functions of C-S. Their results indicate more intrasentential C-S than intersentential ones on the billboards. In terms of function, the advertisers used C-S to indicate figures of speech (e.g. puns, associations, contradictory associations, word-creation and repetitions) to attract the attention of the target group. Mohanty (2006) provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum. In general, high-cost English Medium (EM) education is valued by upper-class and affluent families. Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages. Sridhar (2002) explains that C-S is commonly practiced among students in schools across India. In addition, she finds it unrealistic to ask the students to separate the two languages harshly. In immigrant contexts, Martin et al. (2006) investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK. Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006). In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali. There are also studies about Bengali-English C-S in the UK school settings (Pagett, 2006) and Bangladesh (Obaidullah, 2016) as well. However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.

In their study about aphasic patients, Shyamala Chengappa and Bhat (2004) report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group. However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (2019) find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English. Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well.","1. (What is the linguistic diversity in India?): sent1
    1.1. (What are the major languages in India?): sent2
    1.2. (What language families do most people in India speak?): sent3
    1.3. (What does the census exclude?): sent4
    1.4. (What is the implication of linguistic diversity and multilingualism in India?): sent5
2. (What are the early studies on C-S in India?): sent6
    2.1. (What does Kachru (1978) study?): sent6
        2.1.1. (What influences on Indian languages does Kachru mention?): sent7
    2.2. (What does Sridhar (1978) study?): sent8
    2.3. (What does Kumar (1986) study?): sent9
    2.4. (What does Doley (2013) study?): sent10
    2.5. (What does Boro (2020) study?): sent11
    2.6. (What other external influence on Indian languages is mentioned?): sent12
        2.6.1. (What does Kapp (1997) study?): sent13
3. (How were the C-S examples in early studies collected?): sent14
    3.1. (What is the current view on small sets of data in computational research?): sent15
    3.2. (What are the consequences of ignoring early studies and data?): sent16
        3.2.1. (What example does Nadkarni (1975) provide to prove this point?): sent17
            3.2.1.1. (What is the historical context of the languages in Nadkarni's study?): sent18
            3.2.1.2. (What is the language proficiency of Saraswat Brahmins?): sent19
            3.2.1.3. (What does Nadkarni attribute the language preference to?): sent20
            3.2.1.4. (What is the functional use of Kannada among Konkani speakers?): sent21
            3.2.1.5. (What is the language learning trend among Kannada speakers?): sent22
4. (How has C-S been investigated in media and entertainment?): sent23
    4.1. (What does Si (2011) analyze?): sent23
        4.1.1. (What are the results of Si's analysis?): sent24
            4.1.1.1. (What specific trend does Si observe?): sent25
    4.2. (What similar trend is observed in Bengali movie scripts?): sent26
        4.2.1. (What does Chatterjee (2016) find in Bengali movie scripts?): sent27
    4.3. (What does Gardner-Chloros and Charles (2007) study?): sent28
        4.3.1. (What are the findings of Gardner-Chloros and Charles regarding C-S in TV programs?): sent29
        4.3.2. (What general observation do they make about C-S on TV broadcasts?): sent30
    4.4. (What does Zipp (2017) study?): sent31
    4.5. (What do Pratapa and Choudhury (2017) study?): sent32
5. (How has C-S been investigated in advertising?): sent33
    5.1. (What do Kathpalia and Wee Ong (2015) analyze?): sent33
        5.1.1. (What are the results of their analysis?): sent34
            5.1.1.1. (What specific trend do they observe in C-S structures?): sent35
            5.1.1.2. (What functions of C-S do they identify?): sent36
6. (How has C-S been investigated in education?): sent37
    6.1. (What does Mohanty (2006) study?): sent37
        6.1.1. (What is the general view on high-cost English Medium education?): sent38
        6.1.2. (What does Mohanty question about low-cost EM education?): sent39
    6.2. (What does Sridhar (2002) explain about C-S in schools?): sent40
        6.2.1. (What does Sridhar find unrealistic?): sent41
    6.3. (What does Martin et al. (2006) investigate?): sent42
    6.4. (What shift does Al-Azami (2006) reveal?): sent43
        6.4.1. (What are the C-S patterns among Bengali immigrants?): sent44
    6.5. (What other studies are mentioned about Bengali-English C-S?): sent45
    6.6. (What is lacking in the comparison of Bengali-English C-S?): sent46
7. (How has C-S been investigated in health settings?): sent47
    7.1. (What do Shyamala Chengappa and Bhat (2004) report?): sent47
        7.1.1. (What differences do they find between aphasic patients and the control group?): sent48
    7.2. (What do Deepa and Shyamala (2019) find about C-S in dementia patients?): sent48
        7.2.1. (What insights do these studies offer?): sent49",What are the early studies on code-switching (C-S) in India and their focus areas?,"sent6, sent7, sent8, sent9, sent10, sent11, sent12, sent13","Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.

In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages.

Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences.

Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (i.e. social and stylistic) aspects of Hindi-English C-S from a theoretical point of view.

More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele.

Another observation about C-S in daily life comes from Boro (2020) who provides examples of English, Assamese and Bodo (another language spoken in the Assam region) C-S and borrowings.

In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India.

For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages (i.e. Kannada and Telugu) spoken in India.","Kachru (1978) provides one of the early studies on the types and functions of code-switching (C-S) in India with a historical understanding of the multilingual context.

In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, Kachru (1978) mentions Persian and English as outside influences on Indian languages.

Similarly, Sridhar (1978) provides a comparative overview of the functions of C-S in Kannada in relation to the Perso-Arabic and English influences.

Kumar (1986) provides examples of the formal (e.g., within NPs, PPs, VPs) and functional (i.e., social and stylistic) aspects of Hindi-English code-switching from a theoretical perspective.

More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English, and local languages strategically to sell their products to multilingual clientele.

Another observation about code-switching in daily life comes from Boro (2020), who provides examples of English, Assamese, and Bodo (another language spoken in the Assam region) code-switching and borrowings.

In addition to English, Portuguese was also in contact with the local languages as a result of colonization in South India.

For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages such as Kannada and Telugu spoken in India.","Questions:

What are the early studies on code-switching (C-S) in India and their focus areas?

Answer:

Kachru (1978) provides one of the early studies on the types and functions of code-switching (C-S) in India with a historical understanding of the multilingual context.

In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, Kachru (1978) mentions Persian and English as outside influences on Indian languages.

Similarly, Sridhar (1978) provides a comparative overview of the functions of C-S in Kannada in relation to the Perso-Arabic and English influences.

Kumar (1986) provides examples of the formal (e.g., within NPs, PPs, VPs) and functional (i.e., social and stylistic) aspects of Hindi-English code-switching from a theoretical perspective.

More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English, and local languages strategically to sell their products to multilingual clientele.

Another observation about code-switching in daily life comes from Boro (2020), who provides examples of English, Assamese, and Bodo (another language spoken in the Assam region) code-switching and borrowings.

In addition to English, Portuguese was also in contact with the local languages as a result of colonization in South India.

For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages such as Kannada and Telugu spoken in India."
A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,C-S across Languages: Indian Context,"According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual. There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages). Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families. The census excludes languages with a population lower than 10,000 speakers. Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism. Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.

In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages. Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences. Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (i.e. social and stylistic) aspects of Hindi-English C-S from a theoretical point of view. More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele. Another observation about C-S in daily life comes from Boro (2020) who provides examples of English, Assamese and Bodo (another language spoken in the Assam region) C-S and borrowings. In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India. For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages (i.e. Kannada and Telugu) spoken in India.

Instead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (probably) encountered and collected by the authors themselves in daily life interactions over a period of time with limited means. Nowadays, these small sets of data would be regarded as insignificant in computational areas of research. However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost. For example, Nadkarni (1975) proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India. Both languages have been in contact with each other for over four hundred years. Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada. Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context. Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S. However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975).

C-S in India has been investigated through written media, advertising and film industry as well. Si (2011) analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004. Her results indicate a change of direction C-S over the years. More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions. A similar trend has been observed for Bengali movie scripts as well. Through analyzing movie scripts (between 1970s and 2010s), Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g. renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society. Within the immigrant context, Gardner-Chloros and Charles (2007) focused on the types and functions of C-S between Hindi and English across the TV programs (e.g. highly scripted vs. loosely scripted programs) of a British/Asian cable channel in the UK. Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (i.e. highly scripted). In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use (instead of C-S). Similarly, Zipp (2017) analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters (group identity). Pratapa and Choudhury (2017) perform a quantitative study of 18 recent Bollywood (Hindi) movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.

From an advertising point of view, Kathpalia and Wee Ong (2015) analyzed C-S in Hinglish (i.e. Hindi, English, Urdu, Sanskrit according to their definition) billboards about the Amul brand in India. After compiling the advertisements on billboards (1191), they classified the structures and functions of C-S. Their results indicate more intrasentential C-S than intersentential ones on the billboards. In terms of function, the advertisers used C-S to indicate figures of speech (e.g. puns, associations, contradictory associations, word-creation and repetitions) to attract the attention of the target group. Mohanty (2006) provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum. In general, high-cost English Medium (EM) education is valued by upper-class and affluent families. Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages. Sridhar (2002) explains that C-S is commonly practiced among students in schools across India. In addition, she finds it unrealistic to ask the students to separate the two languages harshly. In immigrant contexts, Martin et al. (2006) investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK. Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006). In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali. There are also studies about Bengali-English C-S in the UK school settings (Pagett, 2006) and Bangladesh (Obaidullah, 2016) as well. However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.

In their study about aphasic patients, Shyamala Chengappa and Bhat (2004) report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group. However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (2019) find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English. Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well.","1. (What is the linguistic diversity in India?): sent1
    1.1. (What are the major languages in India?): sent2
    1.2. (What language families do most people in India speak?): sent3
    1.3. (What does the census exclude?): sent4
    1.4. (What is the implication of linguistic diversity and multilingualism in India?): sent5
2. (What are the early studies on C-S in India?): sent6
    2.1. (What does Kachru (1978) study?): sent6
        2.1.1. (What influences on Indian languages does Kachru mention?): sent7
    2.2. (What does Sridhar (1978) study?): sent8
    2.3. (What does Kumar (1986) study?): sent9
    2.4. (What does Doley (2013) study?): sent10
    2.5. (What does Boro (2020) study?): sent11
    2.6. (What other external influence on Indian languages is mentioned?): sent12
        2.6.1. (What does Kapp (1997) study?): sent13
3. (How were the C-S examples in early studies collected?): sent14
    3.1. (What is the current view on small sets of data in computational research?): sent15
    3.2. (What are the consequences of ignoring early studies and data?): sent16
        3.2.1. (What example does Nadkarni (1975) provide to prove this point?): sent17
            3.2.1.1. (What is the historical context of the languages in Nadkarni's study?): sent18
            3.2.1.2. (What is the language proficiency of Saraswat Brahmins?): sent19
            3.2.1.3. (What does Nadkarni attribute the language preference to?): sent20
            3.2.1.4. (What is the functional use of Kannada among Konkani speakers?): sent21
            3.2.1.5. (What is the language learning trend among Kannada speakers?): sent22
4. (How has C-S been investigated in media and entertainment?): sent23
    4.1. (What does Si (2011) analyze?): sent23
        4.1.1. (What are the results of Si's analysis?): sent24
            4.1.1.1. (What specific trend does Si observe?): sent25
    4.2. (What similar trend is observed in Bengali movie scripts?): sent26
        4.2.1. (What does Chatterjee (2016) find in Bengali movie scripts?): sent27
    4.3. (What does Gardner-Chloros and Charles (2007) study?): sent28
        4.3.1. (What are the findings of Gardner-Chloros and Charles regarding C-S in TV programs?): sent29
        4.3.2. (What general observation do they make about C-S on TV broadcasts?): sent30
    4.4. (What does Zipp (2017) study?): sent31
    4.5. (What do Pratapa and Choudhury (2017) study?): sent32
5. (How has C-S been investigated in advertising?): sent33
    5.1. (What do Kathpalia and Wee Ong (2015) analyze?): sent33
        5.1.1. (What are the results of their analysis?): sent34
            5.1.1.1. (What specific trend do they observe in C-S structures?): sent35
            5.1.1.2. (What functions of C-S do they identify?): sent36
6. (How has C-S been investigated in education?): sent37
    6.1. (What does Mohanty (2006) study?): sent37
        6.1.1. (What is the general view on high-cost English Medium education?): sent38
        6.1.2. (What does Mohanty question about low-cost EM education?): sent39
    6.2. (What does Sridhar (2002) explain about C-S in schools?): sent40
        6.2.1. (What does Sridhar find unrealistic?): sent41
    6.3. (What does Martin et al. (2006) investigate?): sent42
    6.4. (What shift does Al-Azami (2006) reveal?): sent43
        6.4.1. (What are the C-S patterns among Bengali immigrants?): sent44
    6.5. (What other studies are mentioned about Bengali-English C-S?): sent45
    6.6. (What is lacking in the comparison of Bengali-English C-S?): sent46
7. (How has C-S been investigated in health settings?): sent47
    7.1. (What do Shyamala Chengappa and Bhat (2004) report?): sent47
        7.1.1. (What differences do they find between aphasic patients and the control group?): sent48
    7.2. (What do Deepa and Shyamala (2019) find about C-S in dementia patients?): sent48
        7.2.1. (What insights do these studies offer?): sent49","How were the C-S examples in early studies collected, and what are the consequences of ignoring these studies?","sent14, sent15, sent16, sent17, sent18, sent19, sent20, sent21, sent22","Instead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (probably) encountered and collected by the authors themselves in daily life interactions over a period of time with limited means.

Nowadays, these small sets of data would be regarded as insignificant in computational areas of research.

However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost.

For example, Nadkarni (1975) proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India.

Both languages have been in contact with each other for over four hundred years.

Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada.

Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context.

Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S.

However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975).C-S in India has been investigated through written media, advertising and film industry as well.","The C-S examples in early studies were likely encountered and collected by the authors themselves in daily life interactions over a period of time with limited means.

Nowadays, these small sets of data would be regarded as insignificant in computational areas of research.

However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost.

For example, Nadkarni (1975) demonstrates how social factors influence the C-S between the Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India.

Both languages have been in contact with each other for over four hundred years.

Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not code-switch between Konkani and Kannada.

Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context.

Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life, which does not involve code-switching.

However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975). C-S in India has also been investigated through written media, advertising, and the film industry.","Questions:

How were the C-S examples in early studies collected, and what are the consequences of ignoring these studies?

Answer:

The C-S examples in early studies were likely encountered and collected by the authors themselves in daily life interactions over a period of time with limited means.

Nowadays, these small sets of data would be regarded as insignificant in computational areas of research.

However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost.

For example, Nadkarni (1975) demonstrates how social factors influence the C-S between the Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India.

Both languages have been in contact with each other for over four hundred years.

Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not code-switch between Konkani and Kannada.

Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context.

Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life, which does not involve code-switching.

However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975). C-S in India has also been investigated through written media, advertising, and the film industry."
A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,C-S across Languages: Indian Context,"According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual. There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages). Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families. The census excludes languages with a population lower than 10,000 speakers. Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism. Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.

In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages. Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences. Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (i.e. social and stylistic) aspects of Hindi-English C-S from a theoretical point of view. More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele. Another observation about C-S in daily life comes from Boro (2020) who provides examples of English, Assamese and Bodo (another language spoken in the Assam region) C-S and borrowings. In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India. For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages (i.e. Kannada and Telugu) spoken in India.

Instead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (probably) encountered and collected by the authors themselves in daily life interactions over a period of time with limited means. Nowadays, these small sets of data would be regarded as insignificant in computational areas of research. However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost. For example, Nadkarni (1975) proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India. Both languages have been in contact with each other for over four hundred years. Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada. Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context. Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S. However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975).

C-S in India has been investigated through written media, advertising and film industry as well. Si (2011) analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004. Her results indicate a change of direction C-S over the years. More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions. A similar trend has been observed for Bengali movie scripts as well. Through analyzing movie scripts (between 1970s and 2010s), Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g. renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society. Within the immigrant context, Gardner-Chloros and Charles (2007) focused on the types and functions of C-S between Hindi and English across the TV programs (e.g. highly scripted vs. loosely scripted programs) of a British/Asian cable channel in the UK. Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (i.e. highly scripted). In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use (instead of C-S). Similarly, Zipp (2017) analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters (group identity). Pratapa and Choudhury (2017) perform a quantitative study of 18 recent Bollywood (Hindi) movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.

From an advertising point of view, Kathpalia and Wee Ong (2015) analyzed C-S in Hinglish (i.e. Hindi, English, Urdu, Sanskrit according to their definition) billboards about the Amul brand in India. After compiling the advertisements on billboards (1191), they classified the structures and functions of C-S. Their results indicate more intrasentential C-S than intersentential ones on the billboards. In terms of function, the advertisers used C-S to indicate figures of speech (e.g. puns, associations, contradictory associations, word-creation and repetitions) to attract the attention of the target group. Mohanty (2006) provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum. In general, high-cost English Medium (EM) education is valued by upper-class and affluent families. Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages. Sridhar (2002) explains that C-S is commonly practiced among students in schools across India. In addition, she finds it unrealistic to ask the students to separate the two languages harshly. In immigrant contexts, Martin et al. (2006) investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK. Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006). In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali. There are also studies about Bengali-English C-S in the UK school settings (Pagett, 2006) and Bangladesh (Obaidullah, 2016) as well. However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.

In their study about aphasic patients, Shyamala Chengappa and Bhat (2004) report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group. However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (2019) find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English. Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well.","1. (What is the linguistic diversity in India?): sent1
    1.1. (What are the major languages in India?): sent2
    1.2. (What language families do most people in India speak?): sent3
    1.3. (What does the census exclude?): sent4
    1.4. (What is the implication of linguistic diversity and multilingualism in India?): sent5
2. (What are the early studies on C-S in India?): sent6
    2.1. (What does Kachru (1978) study?): sent6
        2.1.1. (What influences on Indian languages does Kachru mention?): sent7
    2.2. (What does Sridhar (1978) study?): sent8
    2.3. (What does Kumar (1986) study?): sent9
    2.4. (What does Doley (2013) study?): sent10
    2.5. (What does Boro (2020) study?): sent11
    2.6. (What other external influence on Indian languages is mentioned?): sent12
        2.6.1. (What does Kapp (1997) study?): sent13
3. (How were the C-S examples in early studies collected?): sent14
    3.1. (What is the current view on small sets of data in computational research?): sent15
    3.2. (What are the consequences of ignoring early studies and data?): sent16
        3.2.1. (What example does Nadkarni (1975) provide to prove this point?): sent17
            3.2.1.1. (What is the historical context of the languages in Nadkarni's study?): sent18
            3.2.1.2. (What is the language proficiency of Saraswat Brahmins?): sent19
            3.2.1.3. (What does Nadkarni attribute the language preference to?): sent20
            3.2.1.4. (What is the functional use of Kannada among Konkani speakers?): sent21
            3.2.1.5. (What is the language learning trend among Kannada speakers?): sent22
4. (How has C-S been investigated in media and entertainment?): sent23
    4.1. (What does Si (2011) analyze?): sent23
        4.1.1. (What are the results of Si's analysis?): sent24
            4.1.1.1. (What specific trend does Si observe?): sent25
    4.2. (What similar trend is observed in Bengali movie scripts?): sent26
        4.2.1. (What does Chatterjee (2016) find in Bengali movie scripts?): sent27
    4.3. (What does Gardner-Chloros and Charles (2007) study?): sent28
        4.3.1. (What are the findings of Gardner-Chloros and Charles regarding C-S in TV programs?): sent29
        4.3.2. (What general observation do they make about C-S on TV broadcasts?): sent30
    4.4. (What does Zipp (2017) study?): sent31
    4.5. (What do Pratapa and Choudhury (2017) study?): sent32
5. (How has C-S been investigated in advertising?): sent33
    5.1. (What do Kathpalia and Wee Ong (2015) analyze?): sent33
        5.1.1. (What are the results of their analysis?): sent34
            5.1.1.1. (What specific trend do they observe in C-S structures?): sent35
            5.1.1.2. (What functions of C-S do they identify?): sent36
6. (How has C-S been investigated in education?): sent37
    6.1. (What does Mohanty (2006) study?): sent37
        6.1.1. (What is the general view on high-cost English Medium education?): sent38
        6.1.2. (What does Mohanty question about low-cost EM education?): sent39
    6.2. (What does Sridhar (2002) explain about C-S in schools?): sent40
        6.2.1. (What does Sridhar find unrealistic?): sent41
    6.3. (What does Martin et al. (2006) investigate?): sent42
    6.4. (What shift does Al-Azami (2006) reveal?): sent43
        6.4.1. (What are the C-S patterns among Bengali immigrants?): sent44
    6.5. (What other studies are mentioned about Bengali-English C-S?): sent45
    6.6. (What is lacking in the comparison of Bengali-English C-S?): sent46
7. (How has C-S been investigated in health settings?): sent47
    7.1. (What do Shyamala Chengappa and Bhat (2004) report?): sent47
        7.1.1. (What differences do they find between aphasic patients and the control group?): sent48
    7.2. (What do Deepa and Shyamala (2019) find about C-S in dementia patients?): sent48
        7.2.1. (What insights do these studies offer?): sent49",How has code-switching been investigated in Indian media and entertainment?,"sent23, sent24, sent25, sent26, sent27, sent28, sent29, sent30, sent31, sent32","Si (2011) analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004.

Her results indicate a change of direction C-S over the years.

More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions.

A similar trend has been observed for Bengali movie scripts as well.

Through analyzing movie scripts (between 1970s and 2010s), Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g. renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society.

Within the immigrant context, Gardner-Chloros and Charles (2007) focused on the types and functions of C-S between Hindi and English across the TV programs (e.g. highly scripted vs. loosely scripted programs) of a British/Asian cable channel in the UK.

Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (i.e. highly scripted).

In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use (instead of C-S).

Similarly, Zipp (2017) analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters (group identity).

Pratapa and Choudhury (2017) perform a quantitative study of 18 recent Bollywood (Hindi) movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.","Si (2011) analyzed Hindi-English code-switching in the scripts of seven Bollywood movies filmed between 1982 and 2004.

Her results indicate a change in the direction of code-switching over the years.

More specifically, Hindi was the dominant language with occasional switches to English in the early productions, but English became the dominant language, especially for younger generations, in the later productions.

A similar trend has been observed for Bengali movie scripts as well.

Through analyzing movie scripts from the 1970s to the 2010s, Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g., renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society.

Gardner-Chloros and Charles (2007) focused on the types and functions of code-switching between Hindi and English across the TV programs of a British/Asian cable channel in the UK.

Although code-switching was observed in a variety of TV shows, the least amount of code-switching was encountered in the news broadcasts, which are highly scripted.

In general, less code-switching was encountered on TV broadcasts compared to natural speech, which is attributed to the consciousness of TV personalities about pure language use.

Similarly, Zipp (2017) analyzed Gujarati-English code-switching within a radio show targeting British South Asians living in the US and concluded that code-switching was part of identity construction among youngsters (group identity).

Pratapa and Choudhury (2017) conducted a quantitative study of 18 recent Bollywood (Hindi) movies and found that code-switching is used for establishing identity, social dynamics between characters, and the socio-cultural context of the movie.","Questions:

How has code-switching been investigated in Indian media and entertainment?

Answer:

Si (2011) analyzed Hindi-English code-switching in the scripts of seven Bollywood movies filmed between 1982 and 2004.

Her results indicate a change in the direction of code-switching over the years.

More specifically, Hindi was the dominant language with occasional switches to English in the early productions, but English became the dominant language, especially for younger generations, in the later productions.

A similar trend has been observed for Bengali movie scripts as well.

Through analyzing movie scripts from the 1970s to the 2010s, Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g., renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society.

Gardner-Chloros and Charles (2007) focused on the types and functions of code-switching between Hindi and English across the TV programs of a British/Asian cable channel in the UK.

Although code-switching was observed in a variety of TV shows, the least amount of code-switching was encountered in the news broadcasts, which are highly scripted.

In general, less code-switching was encountered on TV broadcasts compared to natural speech, which is attributed to the consciousness of TV personalities about pure language use.

Similarly, Zipp (2017) analyzed Gujarati-English code-switching within a radio show targeting British South Asians living in the US and concluded that code-switching was part of identity construction among youngsters (group identity).

Pratapa and Choudhury (2017) conducted a quantitative study of 18 recent Bollywood (Hindi) movies and found that code-switching is used for establishing identity, social dynamics between characters, and the socio-cultural context of the movie."
A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,C-S across Languages: Indian Context,"According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual. There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages). Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families. The census excludes languages with a population lower than 10,000 speakers. Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism. Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.

In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages. Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences. Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (i.e. social and stylistic) aspects of Hindi-English C-S from a theoretical point of view. More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele. Another observation about C-S in daily life comes from Boro (2020) who provides examples of English, Assamese and Bodo (another language spoken in the Assam region) C-S and borrowings. In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India. For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages (i.e. Kannada and Telugu) spoken in India.

Instead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (probably) encountered and collected by the authors themselves in daily life interactions over a period of time with limited means. Nowadays, these small sets of data would be regarded as insignificant in computational areas of research. However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost. For example, Nadkarni (1975) proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India. Both languages have been in contact with each other for over four hundred years. Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada. Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context. Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S. However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975).

C-S in India has been investigated through written media, advertising and film industry as well. Si (2011) analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004. Her results indicate a change of direction C-S over the years. More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions. A similar trend has been observed for Bengali movie scripts as well. Through analyzing movie scripts (between 1970s and 2010s), Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g. renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society. Within the immigrant context, Gardner-Chloros and Charles (2007) focused on the types and functions of C-S between Hindi and English across the TV programs (e.g. highly scripted vs. loosely scripted programs) of a British/Asian cable channel in the UK. Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (i.e. highly scripted). In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use (instead of C-S). Similarly, Zipp (2017) analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters (group identity). Pratapa and Choudhury (2017) perform a quantitative study of 18 recent Bollywood (Hindi) movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.

From an advertising point of view, Kathpalia and Wee Ong (2015) analyzed C-S in Hinglish (i.e. Hindi, English, Urdu, Sanskrit according to their definition) billboards about the Amul brand in India. After compiling the advertisements on billboards (1191), they classified the structures and functions of C-S. Their results indicate more intrasentential C-S than intersentential ones on the billboards. In terms of function, the advertisers used C-S to indicate figures of speech (e.g. puns, associations, contradictory associations, word-creation and repetitions) to attract the attention of the target group. Mohanty (2006) provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum. In general, high-cost English Medium (EM) education is valued by upper-class and affluent families. Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages. Sridhar (2002) explains that C-S is commonly practiced among students in schools across India. In addition, she finds it unrealistic to ask the students to separate the two languages harshly. In immigrant contexts, Martin et al. (2006) investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK. Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006). In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali. There are also studies about Bengali-English C-S in the UK school settings (Pagett, 2006) and Bangladesh (Obaidullah, 2016) as well. However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.

In their study about aphasic patients, Shyamala Chengappa and Bhat (2004) report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group. However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (2019) find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English. Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well.","1. (What is the linguistic diversity in India?): sent1
    1.1. (What are the major languages in India?): sent2
    1.2. (What language families do most people in India speak?): sent3
    1.3. (What does the census exclude?): sent4
    1.4. (What is the implication of linguistic diversity and multilingualism in India?): sent5
2. (What are the early studies on C-S in India?): sent6
    2.1. (What does Kachru (1978) study?): sent6
        2.1.1. (What influences on Indian languages does Kachru mention?): sent7
    2.2. (What does Sridhar (1978) study?): sent8
    2.3. (What does Kumar (1986) study?): sent9
    2.4. (What does Doley (2013) study?): sent10
    2.5. (What does Boro (2020) study?): sent11
    2.6. (What other external influence on Indian languages is mentioned?): sent12
        2.6.1. (What does Kapp (1997) study?): sent13
3. (How were the C-S examples in early studies collected?): sent14
    3.1. (What is the current view on small sets of data in computational research?): sent15
    3.2. (What are the consequences of ignoring early studies and data?): sent16
        3.2.1. (What example does Nadkarni (1975) provide to prove this point?): sent17
            3.2.1.1. (What is the historical context of the languages in Nadkarni's study?): sent18
            3.2.1.2. (What is the language proficiency of Saraswat Brahmins?): sent19
            3.2.1.3. (What does Nadkarni attribute the language preference to?): sent20
            3.2.1.4. (What is the functional use of Kannada among Konkani speakers?): sent21
            3.2.1.5. (What is the language learning trend among Kannada speakers?): sent22
4. (How has C-S been investigated in media and entertainment?): sent23
    4.1. (What does Si (2011) analyze?): sent23
        4.1.1. (What are the results of Si's analysis?): sent24
            4.1.1.1. (What specific trend does Si observe?): sent25
    4.2. (What similar trend is observed in Bengali movie scripts?): sent26
        4.2.1. (What does Chatterjee (2016) find in Bengali movie scripts?): sent27
    4.3. (What does Gardner-Chloros and Charles (2007) study?): sent28
        4.3.1. (What are the findings of Gardner-Chloros and Charles regarding C-S in TV programs?): sent29
        4.3.2. (What general observation do they make about C-S on TV broadcasts?): sent30
    4.4. (What does Zipp (2017) study?): sent31
    4.5. (What do Pratapa and Choudhury (2017) study?): sent32
5. (How has C-S been investigated in advertising?): sent33
    5.1. (What do Kathpalia and Wee Ong (2015) analyze?): sent33
        5.1.1. (What are the results of their analysis?): sent34
            5.1.1.1. (What specific trend do they observe in C-S structures?): sent35
            5.1.1.2. (What functions of C-S do they identify?): sent36
6. (How has C-S been investigated in education?): sent37
    6.1. (What does Mohanty (2006) study?): sent37
        6.1.1. (What is the general view on high-cost English Medium education?): sent38
        6.1.2. (What does Mohanty question about low-cost EM education?): sent39
    6.2. (What does Sridhar (2002) explain about C-S in schools?): sent40
        6.2.1. (What does Sridhar find unrealistic?): sent41
    6.3. (What does Martin et al. (2006) investigate?): sent42
    6.4. (What shift does Al-Azami (2006) reveal?): sent43
        6.4.1. (What are the C-S patterns among Bengali immigrants?): sent44
    6.5. (What other studies are mentioned about Bengali-English C-S?): sent45
    6.6. (What is lacking in the comparison of Bengali-English C-S?): sent46
7. (How has C-S been investigated in health settings?): sent47
    7.1. (What do Shyamala Chengappa and Bhat (2004) report?): sent47
        7.1.1. (What differences do they find between aphasic patients and the control group?): sent48
    7.2. (What do Deepa and Shyamala (2019) find about C-S in dementia patients?): sent48
        7.2.1. (What insights do these studies offer?): sent49",How has code-switching been investigated in Indian advertising?,"sent33, sent34, sent35, sent36","From an advertising point of view, Kathpalia and Wee Ong (2015) analyzed C-S in Hinglish (i.e. Hindi, English, Urdu, Sanskrit according to their definition) billboards about the Amul brand in India.

After compiling the advertisements on billboards (1191), they classified the structures and functions of C-S.

Their results indicate more intrasentential C-S than intersentential ones on the billboards.

In terms of function, the advertisers used C-S to indicate figures of speech (e.g. puns, associations, contradictory associations, word-creation and repetitions) to attract the attention of the target group.","Kathpalia and Wee Ong (2015) analyzed code-switching in Hinglish billboards about the Amul brand in India from an advertising perspective.

They compiled 1191 advertisements on billboards and classified the structures and functions of code-switching.

The results indicate more intrasentential code-switching than intersentential ones on the billboards.

In terms of function, the advertisers used code-switching to indicate figures of speech (e.g., puns, associations, contradictory associations, word-creation, and repetitions) to attract the attention of the target group.","Questions:

How has code-switching been investigated in Indian advertising?

Answer:

Kathpalia and Wee Ong (2015) analyzed code-switching in Hinglish billboards about the Amul brand in India from an advertising perspective.

They compiled 1191 advertisements on billboards and classified the structures and functions of code-switching.

The results indicate more intrasentential code-switching than intersentential ones on the billboards.

In terms of function, the advertisers used code-switching to indicate figures of speech (e.g., puns, associations, contradictory associations, word-creation, and repetitions) to attract the attention of the target group."
A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,C-S across Languages: Indian Context,"According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual. There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages). Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families. The census excludes languages with a population lower than 10,000 speakers. Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism. Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.

In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages. Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences. Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (i.e. social and stylistic) aspects of Hindi-English C-S from a theoretical point of view. More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele. Another observation about C-S in daily life comes from Boro (2020) who provides examples of English, Assamese and Bodo (another language spoken in the Assam region) C-S and borrowings. In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India. For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages (i.e. Kannada and Telugu) spoken in India.

Instead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (probably) encountered and collected by the authors themselves in daily life interactions over a period of time with limited means. Nowadays, these small sets of data would be regarded as insignificant in computational areas of research. However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost. For example, Nadkarni (1975) proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India. Both languages have been in contact with each other for over four hundred years. Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada. Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context. Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S. However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975).

C-S in India has been investigated through written media, advertising and film industry as well. Si (2011) analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004. Her results indicate a change of direction C-S over the years. More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions. A similar trend has been observed for Bengali movie scripts as well. Through analyzing movie scripts (between 1970s and 2010s), Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g. renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society. Within the immigrant context, Gardner-Chloros and Charles (2007) focused on the types and functions of C-S between Hindi and English across the TV programs (e.g. highly scripted vs. loosely scripted programs) of a British/Asian cable channel in the UK. Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (i.e. highly scripted). In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use (instead of C-S). Similarly, Zipp (2017) analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters (group identity). Pratapa and Choudhury (2017) perform a quantitative study of 18 recent Bollywood (Hindi) movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.

From an advertising point of view, Kathpalia and Wee Ong (2015) analyzed C-S in Hinglish (i.e. Hindi, English, Urdu, Sanskrit according to their definition) billboards about the Amul brand in India. After compiling the advertisements on billboards (1191), they classified the structures and functions of C-S. Their results indicate more intrasentential C-S than intersentential ones on the billboards. In terms of function, the advertisers used C-S to indicate figures of speech (e.g. puns, associations, contradictory associations, word-creation and repetitions) to attract the attention of the target group. Mohanty (2006) provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum. In general, high-cost English Medium (EM) education is valued by upper-class and affluent families. Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages. Sridhar (2002) explains that C-S is commonly practiced among students in schools across India. In addition, she finds it unrealistic to ask the students to separate the two languages harshly. In immigrant contexts, Martin et al. (2006) investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK. Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006). In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali. There are also studies about Bengali-English C-S in the UK school settings (Pagett, 2006) and Bangladesh (Obaidullah, 2016) as well. However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.

In their study about aphasic patients, Shyamala Chengappa and Bhat (2004) report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group. However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (2019) find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English. Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well.","1. (What is the linguistic diversity in India?): sent1
    1.1. (What are the major languages in India?): sent2
    1.2. (What language families do most people in India speak?): sent3
    1.3. (What does the census exclude?): sent4
    1.4. (What is the implication of linguistic diversity and multilingualism in India?): sent5
2. (What are the early studies on C-S in India?): sent6
    2.1. (What does Kachru (1978) study?): sent6
        2.1.1. (What influences on Indian languages does Kachru mention?): sent7
    2.2. (What does Sridhar (1978) study?): sent8
    2.3. (What does Kumar (1986) study?): sent9
    2.4. (What does Doley (2013) study?): sent10
    2.5. (What does Boro (2020) study?): sent11
    2.6. (What other external influence on Indian languages is mentioned?): sent12
        2.6.1. (What does Kapp (1997) study?): sent13
3. (How were the C-S examples in early studies collected?): sent14
    3.1. (What is the current view on small sets of data in computational research?): sent15
    3.2. (What are the consequences of ignoring early studies and data?): sent16
        3.2.1. (What example does Nadkarni (1975) provide to prove this point?): sent17
            3.2.1.1. (What is the historical context of the languages in Nadkarni's study?): sent18
            3.2.1.2. (What is the language proficiency of Saraswat Brahmins?): sent19
            3.2.1.3. (What does Nadkarni attribute the language preference to?): sent20
            3.2.1.4. (What is the functional use of Kannada among Konkani speakers?): sent21
            3.2.1.5. (What is the language learning trend among Kannada speakers?): sent22
4. (How has C-S been investigated in media and entertainment?): sent23
    4.1. (What does Si (2011) analyze?): sent23
        4.1.1. (What are the results of Si's analysis?): sent24
            4.1.1.1. (What specific trend does Si observe?): sent25
    4.2. (What similar trend is observed in Bengali movie scripts?): sent26
        4.2.1. (What does Chatterjee (2016) find in Bengali movie scripts?): sent27
    4.3. (What does Gardner-Chloros and Charles (2007) study?): sent28
        4.3.1. (What are the findings of Gardner-Chloros and Charles regarding C-S in TV programs?): sent29
        4.3.2. (What general observation do they make about C-S on TV broadcasts?): sent30
    4.4. (What does Zipp (2017) study?): sent31
    4.5. (What do Pratapa and Choudhury (2017) study?): sent32
5. (How has C-S been investigated in advertising?): sent33
    5.1. (What do Kathpalia and Wee Ong (2015) analyze?): sent33
        5.1.1. (What are the results of their analysis?): sent34
            5.1.1.1. (What specific trend do they observe in C-S structures?): sent35
            5.1.1.2. (What functions of C-S do they identify?): sent36
6. (How has C-S been investigated in education?): sent37
    6.1. (What does Mohanty (2006) study?): sent37
        6.1.1. (What is the general view on high-cost English Medium education?): sent38
        6.1.2. (What does Mohanty question about low-cost EM education?): sent39
    6.2. (What does Sridhar (2002) explain about C-S in schools?): sent40
        6.2.1. (What does Sridhar find unrealistic?): sent41
    6.3. (What does Martin et al. (2006) investigate?): sent42
    6.4. (What shift does Al-Azami (2006) reveal?): sent43
        6.4.1. (What are the C-S patterns among Bengali immigrants?): sent44
    6.5. (What other studies are mentioned about Bengali-English C-S?): sent45
    6.6. (What is lacking in the comparison of Bengali-English C-S?): sent46
7. (How has C-S been investigated in health settings?): sent47
    7.1. (What do Shyamala Chengappa and Bhat (2004) report?): sent47
        7.1.1. (What differences do they find between aphasic patients and the control group?): sent48
    7.2. (What do Deepa and Shyamala (2019) find about C-S in dementia patients?): sent48
        7.2.1. (What insights do these studies offer?): sent49",How has code-switching been investigated in Indian education settings?,"sent37, sent38, sent39, sent40, sent41, sent42, sent43, sent44, sent45, sent46","Mohanty (2006) provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum.

In general, high-cost English Medium (EM) education is valued by upper-class and affluent families.

Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages.

Sridhar (2002) explains that C-S is commonly practiced among students in schools across India.

In addition, she finds it unrealistic to ask the students to separate the two languages harshly.

In immigrant contexts, Martin et al. (2006) investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK.

Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006).

In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali.

There are also studies about Bengali-English C-S in the UK school settings (Pagett, 2006) and Bangladesh (Obaidullah, 2016) as well.

However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.","Mohanty (2006) provides an extended overview of the multilingual education system in India, exploring the types and quality of schools across a wide spectrum.

In general, high-cost English Medium (EM) education is valued by upper-class and affluent families.

Although low-cost English Medium (EM) education is also available for lower-income families, Mohanty (2006) questions its impact in comparison to education in the local languages.

Sridhar (2002) explains that code-switching is commonly practiced among students in schools across India.

In addition, Sridhar (2002) finds it unrealistic to ask the students to separate the two languages harshly.

Martin et al. (2006) investigates how Gujarati-English code-switching is used among South Asian students in educational settings in the UK.

Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006).

In terms of code-switching patterns, first-generation immigrants integrate English words while speaking Bengali, whereas English dominates the conversations of younger generations with occasional switches to Bengali.

There are also studies about Bengali-English code-switching in UK school settings (Pagett, 2006) and in Bangladesh (Obaidullah, 2016).

However, a systematic comparison between Bengali-English code-switching in India, Bangladesh, and immigrant settings is lacking.","Questions:

How has code-switching been investigated in Indian education settings?

Answer:

Mohanty (2006) provides an extended overview of the multilingual education system in India, exploring the types and quality of schools across a wide spectrum.

In general, high-cost English Medium (EM) education is valued by upper-class and affluent families.

Although low-cost English Medium (EM) education is also available for lower-income families, Mohanty (2006) questions its impact in comparison to education in the local languages.

Sridhar (2002) explains that code-switching is commonly practiced among students in schools across India.

In addition, Sridhar (2002) finds it unrealistic to ask the students to separate the two languages harshly.

Martin et al. (2006) investigates how Gujarati-English code-switching is used among South Asian students in educational settings in the UK.

Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006).

In terms of code-switching patterns, first-generation immigrants integrate English words while speaking Bengali, whereas English dominates the conversations of younger generations with occasional switches to Bengali.

There are also studies about Bengali-English code-switching in UK school settings (Pagett, 2006) and in Bangladesh (Obaidullah, 2016).

However, a systematic comparison between Bengali-English code-switching in India, Bangladesh, and immigrant settings is lacking."
A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,C-S across Languages: Indian Context,"According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual. There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages). Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families. The census excludes languages with a population lower than 10,000 speakers. Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism. Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.

In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages. Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences. Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (i.e. social and stylistic) aspects of Hindi-English C-S from a theoretical point of view. More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele. Another observation about C-S in daily life comes from Boro (2020) who provides examples of English, Assamese and Bodo (another language spoken in the Assam region) C-S and borrowings. In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India. For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages (i.e. Kannada and Telugu) spoken in India.

Instead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (probably) encountered and collected by the authors themselves in daily life interactions over a period of time with limited means. Nowadays, these small sets of data would be regarded as insignificant in computational areas of research. However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost. For example, Nadkarni (1975) proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India. Both languages have been in contact with each other for over four hundred years. Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada. Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context. Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S. However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975).

C-S in India has been investigated through written media, advertising and film industry as well. Si (2011) analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004. Her results indicate a change of direction C-S over the years. More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions. A similar trend has been observed for Bengali movie scripts as well. Through analyzing movie scripts (between 1970s and 2010s), Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g. renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society. Within the immigrant context, Gardner-Chloros and Charles (2007) focused on the types and functions of C-S between Hindi and English across the TV programs (e.g. highly scripted vs. loosely scripted programs) of a British/Asian cable channel in the UK. Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (i.e. highly scripted). In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use (instead of C-S). Similarly, Zipp (2017) analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters (group identity). Pratapa and Choudhury (2017) perform a quantitative study of 18 recent Bollywood (Hindi) movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.

From an advertising point of view, Kathpalia and Wee Ong (2015) analyzed C-S in Hinglish (i.e. Hindi, English, Urdu, Sanskrit according to their definition) billboards about the Amul brand in India. After compiling the advertisements on billboards (1191), they classified the structures and functions of C-S. Their results indicate more intrasentential C-S than intersentential ones on the billboards. In terms of function, the advertisers used C-S to indicate figures of speech (e.g. puns, associations, contradictory associations, word-creation and repetitions) to attract the attention of the target group. Mohanty (2006) provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum. In general, high-cost English Medium (EM) education is valued by upper-class and affluent families. Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages. Sridhar (2002) explains that C-S is commonly practiced among students in schools across India. In addition, she finds it unrealistic to ask the students to separate the two languages harshly. In immigrant contexts, Martin et al. (2006) investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK. Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006). In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali. There are also studies about Bengali-English C-S in the UK school settings (Pagett, 2006) and Bangladesh (Obaidullah, 2016) as well. However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.

In their study about aphasic patients, Shyamala Chengappa and Bhat (2004) report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group. However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (2019) find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English. Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well.","1. (What is the linguistic diversity in India?): sent1
    1.1. (What are the major languages in India?): sent2
    1.2. (What language families do most people in India speak?): sent3
    1.3. (What does the census exclude?): sent4
    1.4. (What is the implication of linguistic diversity and multilingualism in India?): sent5
2. (What are the early studies on C-S in India?): sent6
    2.1. (What does Kachru (1978) study?): sent6
        2.1.1. (What influences on Indian languages does Kachru mention?): sent7
    2.2. (What does Sridhar (1978) study?): sent8
    2.3. (What does Kumar (1986) study?): sent9
    2.4. (What does Doley (2013) study?): sent10
    2.5. (What does Boro (2020) study?): sent11
    2.6. (What other external influence on Indian languages is mentioned?): sent12
        2.6.1. (What does Kapp (1997) study?): sent13
3. (How were the C-S examples in early studies collected?): sent14
    3.1. (What is the current view on small sets of data in computational research?): sent15
    3.2. (What are the consequences of ignoring early studies and data?): sent16
        3.2.1. (What example does Nadkarni (1975) provide to prove this point?): sent17
            3.2.1.1. (What is the historical context of the languages in Nadkarni's study?): sent18
            3.2.1.2. (What is the language proficiency of Saraswat Brahmins?): sent19
            3.2.1.3. (What does Nadkarni attribute the language preference to?): sent20
            3.2.1.4. (What is the functional use of Kannada among Konkani speakers?): sent21
            3.2.1.5. (What is the language learning trend among Kannada speakers?): sent22
4. (How has C-S been investigated in media and entertainment?): sent23
    4.1. (What does Si (2011) analyze?): sent23
        4.1.1. (What are the results of Si's analysis?): sent24
            4.1.1.1. (What specific trend does Si observe?): sent25
    4.2. (What similar trend is observed in Bengali movie scripts?): sent26
        4.2.1. (What does Chatterjee (2016) find in Bengali movie scripts?): sent27
    4.3. (What does Gardner-Chloros and Charles (2007) study?): sent28
        4.3.1. (What are the findings of Gardner-Chloros and Charles regarding C-S in TV programs?): sent29
        4.3.2. (What general observation do they make about C-S on TV broadcasts?): sent30
    4.4. (What does Zipp (2017) study?): sent31
    4.5. (What do Pratapa and Choudhury (2017) study?): sent32
5. (How has C-S been investigated in advertising?): sent33
    5.1. (What do Kathpalia and Wee Ong (2015) analyze?): sent33
        5.1.1. (What are the results of their analysis?): sent34
            5.1.1.1. (What specific trend do they observe in C-S structures?): sent35
            5.1.1.2. (What functions of C-S do they identify?): sent36
6. (How has C-S been investigated in education?): sent37
    6.1. (What does Mohanty (2006) study?): sent37
        6.1.1. (What is the general view on high-cost English Medium education?): sent38
        6.1.2. (What does Mohanty question about low-cost EM education?): sent39
    6.2. (What does Sridhar (2002) explain about C-S in schools?): sent40
        6.2.1. (What does Sridhar find unrealistic?): sent41
    6.3. (What does Martin et al. (2006) investigate?): sent42
    6.4. (What shift does Al-Azami (2006) reveal?): sent43
        6.4.1. (What are the C-S patterns among Bengali immigrants?): sent44
    6.5. (What other studies are mentioned about Bengali-English C-S?): sent45
    6.6. (What is lacking in the comparison of Bengali-English C-S?): sent46
7. (How has C-S been investigated in health settings?): sent47
    7.1. (What do Shyamala Chengappa and Bhat (2004) report?): sent47
        7.1.1. (What differences do they find between aphasic patients and the control group?): sent48
    7.2. (What do Deepa and Shyamala (2019) find about C-S in dementia patients?): sent48
        7.2.1. (What insights do these studies offer?): sent49",How has code-switching been investigated in health settings in India?,"sent47, sent48, sent49","In their study about aphasic patients, Shyamala Chengappa and Bhat (2004) report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group.

However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (2019) find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English.

Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well.","Shyamala Chengappa and Bhat (2004) report increased frequency of code-switching between Malayalam and English for aphasic patients in comparison to the control group.

Deepa and Shyamala (2019) find that the amount and types of code-switching could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English.

Although both studies are carried out with limited subjects, they offer insights about the use of code-switching in health settings as well.","Questions:

How has code-switching been investigated in health settings in India?

Answer:

Shyamala Chengappa and Bhat (2004) report increased frequency of code-switching between Malayalam and English for aphasic patients in comparison to the control group.

Deepa and Shyamala (2019) find that the amount and types of code-switching could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English.

Although both studies are carried out with limited subjects, they offer insights about the use of code-switching in health settings as well."
A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,User-facing applications,"Although speech and NLP models for C-S have been built for various applications, a major limitation of the work done so far in computational processing of C-S is the lack of end-to-end userfacing applications that interact directly with users in multilingual communities. For example, there is no widely-used spoken dialogue system that can understand as well as produce code-switched speech, although some voice assistants may recognize and produce C-S in limited scenarios in some locales. Although computational implementations of grammatical models of C-S exist (Bhat et al., 2016), they do not necessarily generate natural C-S utterances that a bilingual speaker would produce (Pratapa et al., 2018). Most crucially, current computational approaches to C-S language technologies do not usually take into account the linguistic and social factors that influence why and when speakers/users choose to code-switch.

Bawa et al. (2020) conducted a Wizard-of-Oz study using a Hindi-English chatbot and found that not only did bilingual users prefer chatbots that could code-switch, they also showed a preference towards bots that mimicked their own C-S patterns. Rudra et al. (2016) report a study on 430k tweets from Hindi-English bilingual users and find that Hindi is preferred for the expression of negative sentiment. In a follow-up study, Agarwal et al. (2017) find that Hindi is the preferred language for swearing in Hindi-English C-S tweets, and swearing may be a motivating factor for users to switch to Hindi. The study also finds a gender difference, with women preferring to swear in English more often than Hindi. Such studies indicate that multilingual chatbots and intelligent agents need to be able to adapt to users' linguistic styles, while also being capable of determining when and how to code-switch.

Due to the paucity of user-facing systems and standard benchmarks covering only a handful of simpler NLP tasks, it is likely that we overestimate how well computational models are able to handle C-S. In sum, language technologies for C-S seem to be constrained by the lack of availability of diverse C-S training data, evaluation benchmarks and the absence of user-facing applications. They need to go beyond pattern recognition and grammatical constraints of C-S in order to process and produce C-S the way humans do. Hence, it is important for the CL community to be aware of the vast litera-ture around C-S in linguistics, particularly as we proceed to solving more challenging tasks.","# QUA Analysis:

1. What is the major limitation of current computational processing of code-switching (C-S)?
    1.1. What is an example of the lack of end-to-end user-facing applications for C-S?
        1.1.1. (Example): sent2
    1.2. Do current grammatical models of C-S generate natural C-S utterances?
        1.2.1. (Answer): sent3
    1.3. What is the most crucial issue with current computational approaches to C-S language technologies?
        1.3.1. (Answer): sent4
    1.4. What do studies indicate about user preferences for chatbots that can code-switch?
        1.4.1. (Study 1): sent5
        1.4.2. (Study 2): sent6
        1.4.3. (Study 3): sent7, sent8
        1.4.4. (Conclusion from studies): sent9
    1.5. What is the impact of the paucity of user-facing systems and standard benchmarks on our understanding of computational models for C-S?
        1.5.1. (Impact): sent10
    1.6. What do language technologies for C-S need to do to process and produce C-S like humans?
        1.6.1. (Requirement): sent11
    1.7. Why is it important for the computational linguistics (CL) community to be aware of the literature around C-S in linguistics?
        1.7.1. (Importance): sent12",What do studies indicate about user preferences for chatbots that can code-switch?,"sent5, sent6, sent7, sent8, sent9","Bawa et al. (2020) conducted a Wizard-of-Oz study using a Hindi-English chatbot and found that not only did bilingual users prefer chatbots that could code-switch, they also showed a preference towards bots that mimicked their own C-S patterns.

Rudra et al. (2016) report a study on 430k tweets from Hindi-English bilingual users and find that Hindi is preferred for the expression of negative sentiment.

In a follow-up study, Agarwal et al. (2017) find that Hindi is the preferred language for swearing in Hindi-English C-S tweets, and swearing may be a motivating factor for users to switch to Hindi.

The study also finds a gender difference, with women preferring to swear in English more often than Hindi.

Such studies indicate that multilingual chatbots and intelligent agents need to be able to adapt to users' linguistic styles, while also being capable of determining when and how to code-switch.","Studies indicate that bilingual users prefer chatbots that can code-switch and show a preference towards bots that mimic their own code-switching patterns (Bawa et al., 2020).

Rudra et al. (2016) report a study on 430k tweets from Hindi-English bilingual users and find that Hindi is preferred for the expression of negative sentiment.

A follow-up study by Agarwal et al. (2017) finds that Hindi is the preferred language for swearing in Hindi-English code-switching tweets, and swearing may be a motivating factor for users to switch to Hindi.

The study also finds a gender difference, with women preferring to swear in English more often than Hindi.

Such studies indicate that multilingual chatbots and intelligent agents need to be able to adapt to users' linguistic styles, while also being capable of determining when and how to code-switch.","Questions:

What do studies indicate about user preferences for chatbots that can code-switch?

Answer:

Studies indicate that bilingual users prefer chatbots that can code-switch and show a preference towards bots that mimic their own code-switching patterns (Bawa et al., 2020).

Rudra et al. (2016) report a study on 430k tweets from Hindi-English bilingual users and find that Hindi is preferred for the expression of negative sentiment.

A follow-up study by Agarwal et al. (2017) finds that Hindi is the preferred language for swearing in Hindi-English code-switching tweets, and swearing may be a motivating factor for users to switch to Hindi.

The study also finds a gender difference, with women preferring to swear in English more often than Hindi.

Such studies indicate that multilingual chatbots and intelligent agents need to be able to adapt to users' linguistic styles, while also being capable of determining when and how to code-switch."
Measure and Improve Robustness in NLP Models: A Survey,Open Questions,"In addition to the challenges mentioned above, we list below a few open questions that call for additional research going forward.

Identifying Unknown Robustness Failures Existing identification around robustness failures rely heavily on human priors and error analyses, which usually pre-define a small or limited set of patterns that the model could be vulnerable to. This requires extensive amount of expertise and efforts, and might still suffer from human or subjective biases in the end. How to proactively discover and identify models' unrobust regions automatically and comprehensively remains challenging.

Interpreting and Mitigating Spurious Correlations Interpretability matters for large NLP models, especially key to the robustness and spurious patterns. How can we develop ways to attribute or interpret these vulnerable portions of NLP models and communicate these robustness failures with designers, practitioners, and users? In addition, recent work (Wallace et al., 2019c;Wang et al., 2021d;Zhang et al., 2021) show interpretability methods can be utilized to better understand how a model makes its decision, which in turn can be used to uncover models' bias, diagnose errors, and discover spurious correlations.

Furthermore, the mitigation of spurious correlations often suffers from the trade-off between removing shortcuts and sacrificing model performance Zhang et al., 2019a). Additionally, most existing mitigation strategies work in a pipeline fashion where defining and detecting spurious correlations are prerequisites, which might lead to error cascades in this process. How to design end-to-end frameworks for automatic mitigation deserves much attention.","1. (What are the open questions that call for additional research?): sent1
    1.1. (What are the challenges in identifying unknown robustness failures?): sent2
        1.1.1. (Why is identifying unknown robustness failures challenging?): sent3
        1.1.2. (What remains challenging in identifying models' unrobust regions?): sent4
    1.2. (What are the challenges in interpreting and mitigating spurious correlations?): sent5
        1.2.1. (How can we attribute or interpret vulnerable portions of NLP models?): sent6
        1.2.2. (How can interpretability methods help in understanding model decisions?): sent7
        1.2.3. (What are the challenges in mitigating spurious correlations?): sent8
        1.2.4. (What is a limitation of existing mitigation strategies?): sent9
        1.2.5. (What deserves much attention in designing frameworks for automatic mitigation?): sent10",What are the challenges in identifying unknown robustness failures in NLP models?,"sent2, sent3, sent4","Identifying Unknown Robustness Failures Existing identification around robustness failures rely heavily on human priors and error analyses, which usually pre-define a small or limited set of patterns that the model could be vulnerable to.

This requires extensive amount of expertise and efforts, and might still suffer from human or subjective biases in the end.

How to proactively discover and identify models' unrobust regions automatically and comprehensively remains challenging.","Identifying unknown robustness failures in NLP models is challenging because existing identification methods rely heavily on human priors and error analyses, which usually pre-define a small or limited set of patterns that the model could be vulnerable to.

This requires an extensive amount of expertise and effort and might still suffer from human or subjective biases in the end.

Proactively discovering and identifying models' unrobust regions automatically and comprehensively remains challenging.","Questions:

What are the challenges in identifying unknown robustness failures in NLP models?

Answer:

Identifying unknown robustness failures in NLP models is challenging because existing identification methods rely heavily on human priors and error analyses, which usually pre-define a small or limited set of patterns that the model could be vulnerable to.

This requires an extensive amount of expertise and effort and might still suffer from human or subjective biases in the end.

Proactively discovering and identifying models' unrobust regions automatically and comprehensively remains challenging."
Measure and Improve Robustness in NLP Models: A Survey,Open Questions,"In addition to the challenges mentioned above, we list below a few open questions that call for additional research going forward.

Identifying Unknown Robustness Failures Existing identification around robustness failures rely heavily on human priors and error analyses, which usually pre-define a small or limited set of patterns that the model could be vulnerable to. This requires extensive amount of expertise and efforts, and might still suffer from human or subjective biases in the end. How to proactively discover and identify models' unrobust regions automatically and comprehensively remains challenging.

Interpreting and Mitigating Spurious Correlations Interpretability matters for large NLP models, especially key to the robustness and spurious patterns. How can we develop ways to attribute or interpret these vulnerable portions of NLP models and communicate these robustness failures with designers, practitioners, and users? In addition, recent work (Wallace et al., 2019c;Wang et al., 2021d;Zhang et al., 2021) show interpretability methods can be utilized to better understand how a model makes its decision, which in turn can be used to uncover models' bias, diagnose errors, and discover spurious correlations.

Furthermore, the mitigation of spurious correlations often suffers from the trade-off between removing shortcuts and sacrificing model performance Zhang et al., 2019a). Additionally, most existing mitigation strategies work in a pipeline fashion where defining and detecting spurious correlations are prerequisites, which might lead to error cascades in this process. How to design end-to-end frameworks for automatic mitigation deserves much attention.","1. (What are the open questions that call for additional research?): sent1
    1.1. (What are the challenges in identifying unknown robustness failures?): sent2
        1.1.1. (Why is identifying unknown robustness failures challenging?): sent3
        1.1.2. (What remains challenging in identifying models' unrobust regions?): sent4
    1.2. (What are the challenges in interpreting and mitigating spurious correlations?): sent5
        1.2.1. (How can we attribute or interpret vulnerable portions of NLP models?): sent6
        1.2.2. (How can interpretability methods help in understanding model decisions?): sent7
        1.2.3. (What are the challenges in mitigating spurious correlations?): sent8
        1.2.4. (What is a limitation of existing mitigation strategies?): sent9
        1.2.5. (What deserves much attention in designing frameworks for automatic mitigation?): sent10",What are the challenges in mitigating spurious correlations in NLP models?,"sent8, sent9, sent10","Furthermore, the mitigation of spurious correlations often suffers from the trade-off between removing shortcuts and sacrificing model performance Zhang et al., 2019a).

Additionally, most existing mitigation strategies work in a pipeline fashion where defining and detecting spurious correlations are prerequisites, which might lead to error cascades in this process.

How to design end-to-end frameworks for automatic mitigation deserves much attention.","Mitigating spurious correlations often suffers from the trade-off between removing shortcuts and sacrificing model performance (Zhang et al., 2019a).

Additionally, most existing mitigation strategies work in a pipeline fashion where defining and detecting spurious correlations are prerequisites, which might lead to error cascades in this process.

Designing end-to-end frameworks for automatic mitigation deserves much attention.","Questions:

What are the challenges in mitigating spurious correlations in NLP models?

Answer:

Mitigating spurious correlations often suffers from the trade-off between removing shortcuts and sacrificing model performance (Zhang et al., 2019a).

Additionally, most existing mitigation strategies work in a pipeline fashion where defining and detecting spurious correlations are prerequisites, which might lead to error cascades in this process.

Designing end-to-end frameworks for automatic mitigation deserves much attention."
Measure and Improve Robustness in NLP Models: A Survey,Inductive-prior-based Approaches,"Another thread is to introduce inductive bias (i.e., to regularize the hypothesis space) to force the model to discard some spurious features. This is closely connected to the human-prior-based identification approaches in Section 4.1 as those human-priors can often be used to re-formulate the training objective with additional regularizers. To achieve this goal, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component. The construction of this side component usually relies on prior knowledge of what the misaligned features are. Then, methods can be built accordingly to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020). Similarly, Clark et al. (2019Utama et al. (2020a,b) propose to ensemble with a model explicitly capturing bias, where the main model is trained together with this ""bias-only"" model such that the main model is discouraged from using biases. More recent work (Xiong et al., 2021) shows the ensemble-based approaches can be further improved via better calibrating the bias-only model. Furthermore, additional regularizers have been introduced for robust fine-tuning over pre-trained models, e.g., mutual-information-based regularizers (Wang et al., 2021a) and smoothness-inducing adversarial regularization (Jiang et al., 2020).

In a broader scope, given that one of the main challenges of domain adaptation is to counter the model's tendency in learning domain-specific spurious features (Ganin et al., 2016), some methods contributing to domain adaption may have also progressed along the line of our interest, e.g., domain adversarial neural network (Ganin et al., 2016). This line of work also inspires a family of methods forcing the model to learn auxiliary-annotationinvariant representations with a side component (Ghifary et al., 2016;Wang et al., 2017;Rozantsev et al., 2018;Motiian et al., 2017;Li et al., 2018;Vernikos et al., 2020).

Despite the diverse concrete ideas introduced, the above is mainly training for small empirical loss across different domains or distributions in addition to forcing the model to be invariant to domain-specific spurious features. As an extension along this direction, invariant risk minimization (IRM) (Arjovsky et al., 2019) introduces the idea of invariant predictors across multiple environments, which was later followed and discussed by a variety of extensions (Choe et al., 2020;Ahmed et al., 2020;Rosenfeld et al., 2021). More recently, Dranker et al. (2021) applied IRM in natural language inference and found that a more naturalistic characterization of the problem setup is needed.","# QUA Analysis:

1. (What is the purpose of introducing inductive bias?): sent1
    1.1. (How is this connected to human-prior-based identification approaches?): sent2
    1.2. (How can one achieve the goal of introducing inductive bias?): sent3
        1.2.1. (What is needed to construct a side component?): sent4
        1.2.2. (What methods can be built to counter the features?): sent5
    1.3. (What are some specific ensemble-based approaches?): sent6
        1.3.1. (How can ensemble-based approaches be improved?): sent7
    1.4. (What additional regularizers have been introduced for robust fine-tuning?): sent8
    1.5. (How does domain adaptation relate to this line of work?): sent9
        1.5.1. (What methods are inspired by this line of work?): sent10
    1.6. (What is the main goal of the diverse concrete ideas introduced?): sent11
    1.7. (What is an extension along the direction of invariant predictors?): sent12
        1.7.1. (How has IRM been applied in recent work?): sent13",What is the process to achieve the goal of introducing inductive bias?,"sent3, sent4, sent5","To achieve this goal, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component.

The construction of this side component usually relies on prior knowledge of what the misaligned features are.

Then, methods can be built accordingly to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020).","To achieve the goal of introducing inductive bias, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component.

The construction of this side component usually relies on prior knowledge of what the misaligned features are.

Methods can then be built to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020).","Questions:

What is the process to achieve the goal of introducing inductive bias?

Answer:

To achieve the goal of introducing inductive bias, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component.

The construction of this side component usually relies on prior knowledge of what the misaligned features are.

Methods can then be built to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020)."
Measure and Improve Robustness in NLP Models: A Survey,Model and Training-based Approaches,"Pre-training Recent work has demonstrated pretraining as an effective way to improve NLP models' out-of-distribution robustness (Hendrycks et al., 2020a;Tu et al., 2020), potentially due to its self-supervised objective and the use of large amounts of diverse pre-training data that encourages generalization from a small number of examples that counter the spurious correlations. Tu et al. (2020) showed a few other factors can also contribute to robust accuracy, including larger model size, more fine-tuning data, and longer fine-tuning. A similar observation is made by Taori et al. (2020) in the vision domain, where the authors found training with larger and more diverse datasets offer better robustness consistently in multiple cases, compared to various robustness interventions proposed in the existing literature. In general, the training strategy with an emphasis on a subset of samples that are particularly hard for the model to learn is sometimes also referred to as group DRO (Sagawa et al., 2020a), as an extension of vanilla distributional robust optimization (DRO) (Ben-Tal et al., 2013;Duchi et al., 2021). Extensions of DRO are mostly discussing the strategies on how to identify the samples considered as minority: Nam et al. (2020) trained two models in parallel, where the ""debiased"" model focuses on examples not learned by the ""biased"" model; Lahoti et al. (2020) used an adversary model to identify samples that are challenging to the main model; Liu et al. (2021) proposed to train the model a second time via up-weighting examples that have high training losses during the first time.

When to Use Data-driven or Model-based Approaches? In many cases both the data and the model can contribute to a model's lack of robustness, hence data-driven and model-based approaches could be combined to further improve a model's robustness. One interesting phenomenon observed by (Liu et al., 2019) is to attribute models' robustness failures to blind spots in the training data, or the intrinsic learning ability of the model. The authors found that both patterns are possible: in some cases models can be inoculated via being exposed to a small amount of challenging data, similar to the data augmentation approaches mentioned in Section 5.1; on the other hand, some challenging patterns remain difficult which connects to the larger question around generalizability to unseen adversarial and counterfactual patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), which is relatively under-explored but deserves much attention.","# QUA Analysis:

1. (How does pre-training improve NLP models' out-of-distribution robustness?): sent1
    1.1. (What other factors contribute to robust accuracy according to Tu et al. (2020)?): sent2
    1.2. (What similar observation is made in the vision domain by Taori et al. (2020)?): sent3
2. (What is group DRO and how does it relate to training strategies?): sent4
    2.1. (What are the extensions of DRO and how do they identify minority samples?): sent5
3. (When should data-driven or model-based approaches be used?): sent6
    3.1. (Why should data-driven and model-based approaches be combined?): sent7
    3.2. (What phenomenon did Liu et al. (2019) observe regarding models' robustness failures?): sent8
        3.2.1. (What did the authors find about the patterns of robustness failures?): sent9",How does pre-training improve NLP models' out-of-distribution robustness?,"sent1, sent2, sent3","Pre-training Recent work has demonstrated pretraining as an effective way to improve NLP models' out-of-distribution robustness (Hendrycks et al., 2020a;Tu et al., 2020), potentially due to its self-supervised objective and the use of large amounts of diverse pre-training data that encourages generalization from a small number of examples that counter the spurious correlations.

Tu et al. (2020) showed a few other factors can also contribute to robust accuracy, including larger model size, more fine-tuning data, and longer fine-tuning.

A similar observation is made by Taori et al. (2020) in the vision domain, where the authors found training with larger and more diverse datasets offer better robustness consistently in multiple cases, compared to various robustness interventions proposed in the existing literature.","Recent work has demonstrated pretraining as an effective way to improve NLP models' out-of-distribution robustness (Hendrycks et al., 2020a; Tu et al., 2020), potentially due to its self-supervised objective and the use of large amounts of diverse pre-training data that encourages generalization from a small number of examples that counter the spurious correlations.

Tu et al. (2020) showed that other factors contributing to robust accuracy include larger model size, more fine-tuning data, and longer fine-tuning.

A similar observation is made by Taori et al. (2020) in the vision domain, where training with larger and more diverse datasets consistently offers better robustness compared to various robustness interventions proposed in the existing literature.","Questions:

How does pre-training improve NLP models' out-of-distribution robustness?

Answer:

Recent work has demonstrated pretraining as an effective way to improve NLP models' out-of-distribution robustness (Hendrycks et al., 2020a; Tu et al., 2020), potentially due to its self-supervised objective and the use of large amounts of diverse pre-training data that encourages generalization from a small number of examples that counter the spurious correlations.

Tu et al. (2020) showed that other factors contributing to robust accuracy include larger model size, more fine-tuning data, and longer fine-tuning.

A similar observation is made by Taori et al. (2020) in the vision domain, where training with larger and more diverse datasets consistently offers better robustness compared to various robustness interventions proposed in the existing literature."
Measure and Improve Robustness in NLP Models: A Survey,Model and Training-based Approaches,"Pre-training Recent work has demonstrated pretraining as an effective way to improve NLP models' out-of-distribution robustness (Hendrycks et al., 2020a;Tu et al., 2020), potentially due to its self-supervised objective and the use of large amounts of diverse pre-training data that encourages generalization from a small number of examples that counter the spurious correlations. Tu et al. (2020) showed a few other factors can also contribute to robust accuracy, including larger model size, more fine-tuning data, and longer fine-tuning. A similar observation is made by Taori et al. (2020) in the vision domain, where the authors found training with larger and more diverse datasets offer better robustness consistently in multiple cases, compared to various robustness interventions proposed in the existing literature. In general, the training strategy with an emphasis on a subset of samples that are particularly hard for the model to learn is sometimes also referred to as group DRO (Sagawa et al., 2020a), as an extension of vanilla distributional robust optimization (DRO) (Ben-Tal et al., 2013;Duchi et al., 2021). Extensions of DRO are mostly discussing the strategies on how to identify the samples considered as minority: Nam et al. (2020) trained two models in parallel, where the ""debiased"" model focuses on examples not learned by the ""biased"" model; Lahoti et al. (2020) used an adversary model to identify samples that are challenging to the main model; Liu et al. (2021) proposed to train the model a second time via up-weighting examples that have high training losses during the first time.

When to Use Data-driven or Model-based Approaches? In many cases both the data and the model can contribute to a model's lack of robustness, hence data-driven and model-based approaches could be combined to further improve a model's robustness. One interesting phenomenon observed by (Liu et al., 2019) is to attribute models' robustness failures to blind spots in the training data, or the intrinsic learning ability of the model. The authors found that both patterns are possible: in some cases models can be inoculated via being exposed to a small amount of challenging data, similar to the data augmentation approaches mentioned in Section 5.1; on the other hand, some challenging patterns remain difficult which connects to the larger question around generalizability to unseen adversarial and counterfactual patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), which is relatively under-explored but deserves much attention.","# QUA Analysis:

1. (How does pre-training improve NLP models' out-of-distribution robustness?): sent1
    1.1. (What other factors contribute to robust accuracy according to Tu et al. (2020)?): sent2
    1.2. (What similar observation is made in the vision domain by Taori et al. (2020)?): sent3
2. (What is group DRO and how does it relate to training strategies?): sent4
    2.1. (What are the extensions of DRO and how do they identify minority samples?): sent5
3. (When should data-driven or model-based approaches be used?): sent6
    3.1. (Why should data-driven and model-based approaches be combined?): sent7
    3.2. (What phenomenon did Liu et al. (2019) observe regarding models' robustness failures?): sent8
        3.2.1. (What did the authors find about the patterns of robustness failures?): sent9",Why should data-driven and model-based approaches be combined to improve model robustness?,"sent6, sent7, sent8, sent9","When to Use Data-driven or Model-based Approaches?

In many cases both the data and the model can contribute to a model's lack of robustness, hence data-driven and model-based approaches could be combined to further improve a model's robustness.

One interesting phenomenon observed by (Liu et al., 2019) is to attribute models' robustness failures to blind spots in the training data, or the intrinsic learning ability of the model.

The authors found that both patterns are possible: in some cases models can be inoculated via being exposed to a small amount of challenging data, similar to the data augmentation approaches mentioned in Section 5.1; on the other hand, some challenging patterns remain difficult which connects to the larger question around generalizability to unseen adversarial and counterfactual patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), which is relatively under-explored but deserves much attention.","Combining data-driven and model-based approaches can improve model robustness.

In many cases, both the data and the model can contribute to a model's lack of robustness, hence data-driven and model-based approaches could be combined to further improve a model's robustness.

One interesting phenomenon observed by Liu et al. (2019) is that models' robustness failures can be attributed to blind spots in the training data or the intrinsic learning ability of the model.

The authors found that in some cases, models can be inoculated by being exposed to a small amount of challenging data, similar to data augmentation approaches. However, some challenging patterns remain difficult, which connects to the larger question of generalizability to unseen adversarial and counterfactual patterns (Huang et al., 2020; Jha et al., 2020; Joshi and He, 2021), an area that is relatively under-explored but deserves much attention.","Questions:

Why should data-driven and model-based approaches be combined to improve model robustness?

Answer:

Combining data-driven and model-based approaches can improve model robustness.

In many cases, both the data and the model can contribute to a model's lack of robustness, hence data-driven and model-based approaches could be combined to further improve a model's robustness.

One interesting phenomenon observed by Liu et al. (2019) is that models' robustness failures can be attributed to blind spots in the training data or the intrinsic learning ability of the model.

The authors found that in some cases, models can be inoculated by being exposed to a small amount of challenging data, similar to data augmentation approaches. However, some challenging patterns remain difficult, which connects to the larger question of generalizability to unseen adversarial and counterfactual patterns (Huang et al., 2020; Jha et al., 2020; Joshi and He, 2021), an area that is relatively under-explored but deserves much attention."
A Survey on Dynamic Neural Networks for Natural Language Processing,Method Decision based on,"Operation options LSTM-Jump (Yu et al., 2017) hidden states skip multiple steps; stop reading Skip RNN (Campos et al., 2018) states of the update gate; hidden states skip a single step ReasoNet (Shen et al., 2017) hidden states stop reading Jumper  input sentence; hidden states stop reading RIM (Li et al., 2019) input sentence; hidden states skip a single step; stop reading Yu et al. (2018) hidden states skip multiple steps; stop reading; re-read LSTM-Shuttle (Fu and Ma, 2018) hidden states skip multiple steps; jump back multiples steps Struc. Jump-LSTM (Hansen et al., 2019) hidden states stop reading; jump to next (,;) or (.!?)

PoWER (Goyal et al., 2020) attention drop tokens TR-BERT (Ye et al., 2021) hidden states forward tokens LAT (Kim and Cho, 2021) attention forward tokens LTP  attention drop tokens Transkimmer (Guan et al., 2022) hidden states forward tokens VCRNN (Jernite et al., 2017) input token; hidden states partial update with zero-masked weights Skim-RNN (Seo et al., 2018) input token; hidden states partial update with a small RNN HM-RNN (Chung et al., 2017) states of the gates skip a single step; ""flush"" FHRNN (Ke et al., 2018) query; hidden states update the upper RNN layer ing jumped over important information. LSTM-Shuttle (Fu and Ma, 2018) proposes a bidirectional shuttling mechanism, which can jump multiple time steps both forward and backward, allowing the model to ignore unimportant information and recover lost information if needed. Structural information that naturally exists in sentences can also play a role in skimming. Structural Jump-LSTM (Hansen et al., 2019) can jump to the next word, next sub-sentence separator (a comma or colon), next sentence end symbols (a period, exclamation mark or question mark), or to the end of the text (i.e., stop reading).

In the era of Transformers, there have been works attempting to reduce computation by either skip tokens at higher layers or forward tokens to higher layers. The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores. The number of tokens to be dropped, referred to as the ""schedule,"" is optimized by combining the sparsity of a soft mask layer with the original loss function. This results in an improved balance between accuracy and processing time. TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, using reinforcement learning to train the model with a reward system that prioritizes classifier confidence while also penalizing the number of tokens retained. In contrast to PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them. The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, aiming to close the gap between pretraining and fine-tuning. The schedule for LAT is found through an evolutionary search algorithm. LTP ) trains a threshold for each Transformer layer, instead of following a predetermined schedule. It simply drops tokens with attention scores lower than the learned threshold. Transkimmer (Guan et al., 2022) incorporates a skim predictor module, consisting of a small MLP and Gumbel-Softmax reparameterization, before each layer. This module outputs a mask to determine whether a token should be dropped, and a skim loss is used to optimize the ratio of skipped tokens to total tokens, promoting sparsity.

Computation Reduction Different from skipping, computation reduction applies a reduced computational workload for some time steps instead of skipping it completely. VCRNN (Jernite et al., 2017) explores a scheduler to decide which proportion of computation to use for each time step. Upon making the decision, only the corresponding proportion of the weight matrix will be used to update the hidden states while the rest part of the weight matrix will be masked out with zero.

Instead of using part of weights to update the hidden states, Skim-RNN (Seo et al., 2018) has a big RNN and a separate small RNN. At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token. If the model decides to skim, the small RNN will update only a fraction of the hid-den states. Otherwise, a regular full update will be conducted by the big RNN.

Dynamic Hierarchical RNN Different from the aforementioned two categories of skimming, dynamic hierarchical RNN can increase computation by calling the upper layer RNN when needed. HM-RNN (Chung et al., 2017) automatically discovers the hierarchical multi-scale structure in the data for a hierarchical RNN architecture. In addition to the update and copy operations as in Skip RNN (Campos et al., 2018), they add a flush operation which ejects the summarized representation of the current time step to the upper layer and reinitializes the states for the next time step.

In question answering, only a small portion of tokens are relevant and can be used to answer the question while the rest can be safely skimmed. Based on this observation, Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering. It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding.","# QUA Analysis:

1. (What are the operation options for dynamic neural networks in NLP?): sent1
    1.1. (What are the specific operation options for LSTM-Jump, Skip RNN, ReasoNet, Jumper, RIM, Yu et al. (2018), and LSTM-Shuttle?): sent1
    1.2. (What are the specific operation options for Jump-LSTM, PoWER, TR-BERT, and LAT?): sent2
    1.3. (What are the specific operation options for Transkimmer, VCRNN, Skim-RNN, HM-RNN, and FHRNN?): sent3
2. (What is the mechanism proposed by LSTM-Shuttle?): sent4
3. (How does structural information play a role in skimming?): sent5
4. (What are the specific operation options for Structural Jump-LSTM?): sent6
5. (How do Transformers attempt to reduce computation?): sent7
    5.1. (How does PoWER-BERT reduce the number of tokens processed by each Transformer layer?): sent8
        5.1.1. (How is the schedule for PoWER-BERT optimized?): sent9
        5.1.2. (What is the result of PoWER-BERT's optimization?): sent10
    5.2. (How does TR-BERT determine which tokens to skip?): sent11
        5.2.1. (How does TR-BERT differ from PoWER-BERT in handling skipped tokens?): sent12
    5.3. (How does the Length-Adaptive Transformer (LAT) utilize LengthDrop?): sent13
        5.3.1. (How is the schedule for LAT found?): sent14
    5.4. (How does LTP train a threshold for each Transformer layer?): sent15
        5.4.1. (How does LTP drop tokens?): sent16
    5.5. (How does Transkimmer incorporate a skim predictor module?): sent17
        5.5.1. (What does the skim predictor module output and how is it optimized?): sent18
6. (What is computation reduction and how does it differ from skipping?): sent19
    6.1. (How does VCRNN explore a scheduler for computation reduction?): sent20
        6.1.1. (What happens after VCRNN makes a decision on computation proportion?): sent21
    6.2. (How does Skim-RNN decide whether to read or skim?): sent22
        6.2.1. (What happens if Skim-RNN decides to skim?): sent23, sent24
        6.2.2. (What happens if Skim-RNN decides not to skim?): sent25
7. (What is dynamic hierarchical RNN and how does it differ from skimming?): sent26
    7.1. (How does HM-RNN discover hierarchical multi-scale structure?): sent27
        7.1.1. (What operations does HM-RNN add to Skip RNN?): sent28
    7.2. (How does Focused Hierarchical RNN aim to pick up relevant information for question answering?): sent29, sent30
        7.2.1. (How does Focused Hierarchical RNN control the update of the upper layer of the RNN?): sent31",What are the operation options for dynamic neural networks in NLP?,"sent1, sent2, sent3","Operation options LSTM-Jump (Yu et al., 2017) hidden states skip multiple steps; stop reading Skip RNN (Campos et al., 2018) states of the update gate; hidden states skip a single step ReasoNet (Shen et al., 2017) hidden states stop reading Jumper  input sentence; hidden states stop reading RIM (Li et al., 2019) input sentence; hidden states skip a single step; stop reading Yu et al. (2018) hidden states skip multiple steps; stop reading; re-read LSTM-Shuttle (Fu and Ma, 2018) hidden states skip multiple steps; jump back multiples steps Struc.

Jump-LSTM (Hansen et al., 2019) hidden states stop reading; jump to next (,;) or (.!?)PoWER (Goyal et al., 2020) attention drop tokens TR-BERT (Ye et al., 2021) hidden states forward tokens LAT (Kim and Cho, 2021) attention forward tokens LTP  attention drop tokens

Transkimmer (Guan et al., 2022) hidden states forward tokens VCRNN (Jernite et al., 2017) input token; hidden states partial update with zero-masked weights Skim-RNN (Seo et al., 2018) input token; hidden states partial update with a small RNN HM-RNN (Chung et al., 2017) states of the gates skip a single step; ""flush"" FHRNN (Ke et al., 2018) query; hidden states update the upper RNN layer ing jumped over important information.","Dynamic neural networks in NLP have several operation options, including LSTM-Jump (Yu et al., 2017) where hidden states skip multiple steps, Skip RNN (Campos et al., 2018) where states of the update gate and hidden states skip a single step, and ReasoNet (Shen et al., 2017) where hidden states stop reading.

Other options include Jump-LSTM (Hansen et al., 2019) where hidden states stop reading and jump to the next step, PoWER (Goyal et al., 2020) where attention drops tokens, TR-BERT (Ye et al., 2021) where hidden states forward tokens, and LAT (Kim and Cho, 2021) where attention forwards tokens.

Other options include Transkimmer (Guan et al., 2022) where hidden states forward tokens, VCRNN (Jernite et al., 2017) where input tokens and hidden states partially update with zero-masked weights, Skim-RNN (Seo et al., 2018) where input tokens and hidden states partially update with a small RNN, HM-RNN (Chung et al., 2017) where states of the gates skip a single step, and FHRNN (Ke et al., 2018) where hidden states update the upper RNN layer.","Questions:

What are the operation options for dynamic neural networks in NLP?

Answer:

Dynamic neural networks in NLP have several operation options, including LSTM-Jump (Yu et al., 2017) where hidden states skip multiple steps, Skip RNN (Campos et al., 2018) where states of the update gate and hidden states skip a single step, and ReasoNet (Shen et al., 2017) where hidden states stop reading.

Other options include Jump-LSTM (Hansen et al., 2019) where hidden states stop reading and jump to the next step, PoWER (Goyal et al., 2020) where attention drops tokens, TR-BERT (Ye et al., 2021) where hidden states forward tokens, and LAT (Kim and Cho, 2021) where attention forwards tokens.

Other options include Transkimmer (Guan et al., 2022) where hidden states forward tokens, VCRNN (Jernite et al., 2017) where input tokens and hidden states partially update with zero-masked weights, Skim-RNN (Seo et al., 2018) where input tokens and hidden states partially update with a small RNN, HM-RNN (Chung et al., 2017) where states of the gates skip a single step, and FHRNN (Ke et al., 2018) where hidden states update the upper RNN layer."
A Survey on Dynamic Neural Networks for Natural Language Processing,Skimming,"Skimming techniques, as summarized in Table 1, skip some time steps or allocate different computation on them. Intuitively, skimming matches how human beings efficiently read text and extract information from it (Li et al., 2019). By em-phasizing the important information within a sequence and ignoring parts with little importance, skimming helps the model achieve faster inference speed and better capture long-term dependencies. The three categories of skimming are skipping and early stopping, computation reduction, and dynamic hierarchical RNN, corresponding with three motivations: to skip unimportant input, to allocate less computation to unimportant input, and to increase computation to important input only.

Skipping and Early Stopping Skipping and early stopping aim to improve efficiency for a long sequence by skipping some tokens or stopping reading early. LSTM-Jump (Yu et al., 2017) is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU). At each step, the current states are used to compute a ""jumping softmax"", which decides how many steps to jump forward and whether to stop reading. LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions. The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label. Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up. Skip RNN (Campos et al., 2018) introduces a binary gate to learn whether to skip a state update. If the gate decides to skip a time step, the hidden states will be directly copied without any update.

To stop reading early as needed, Rea-soNet (Shen et al., 2017) introduces a terminal state which decides whether to terminate early for machine reading comprehension on each time step at the token level. Jumper  first splits a paragraph to several sub-sentences and encodes them into sentence embeddings. They then apply early stopping at a sentence when the policy network decides to stop reading. Li et al. (2019) use eye-tracking devices and confirm that skipping and early stopping are common when humans read text. They propose Reading Inspired Model to mimic the behaviors of humans, which allows the model to decide whether to skip a single time step or stop reading early. Yu et al. (2018) add a rereading operation to LSTM-Jump (Yu et al., 2017) which allows the model to stay on the current time step, allocating more computation to important information.

The aforementioned techniques can only go forward, which makes it impossible to regret if hav-","1. What are skimming techniques in dynamic neural networks for NLP? (sent1)
    1.1. How do skimming techniques relate to human reading behavior? (sent2)
    1.2. What are the benefits of skimming techniques? (sent3)
    1.3. What are the categories and motivations of skimming techniques? (sent4)
        1.3.1. What is skipping and early stopping? (sent5)
            1.3.1.1. What is LSTM-Jump and how does it work? (sent6, sent7, sent8, sent9, sent10)
            1.3.1.2. What is Skip RNN and how does it work? (sent11, sent12)
            1.3.1.3. What is ReasoNet and how does it work? (sent13)
            1.3.1.4. What is Jumper and how does it work? (sent14, sent15)
            1.3.1.5. How do human reading behaviors support skipping and early stopping? (sent16)
            1.3.1.6. What is the Reading Inspired Model? (sent17)
        1.3.2. How does rereading operation enhance LSTM-Jump? (sent18)
        1.3.3. What is a limitation of the aforementioned techniques? (sent19)",What is LSTM-Jump and how does it work?,"sent6, sent7, sent8, sent9, sent10","LSTM-Jump (Yu et al., 2017) is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU).

At each step, the current states are used to compute a ""jumping softmax"", which decides how many steps to jump forward and whether to stop reading.

LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions.

The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label.

Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up.","LSTM-Jump (Yu et al., 2017) is a skipping mechanism designed to ignore irrelevant information for natural language understanding (NLU).

At each step, the current states are used to compute a ""jumping softmax,"" which determines how many steps to jump forward and whether to stop reading.

LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions.

The reward function is binary, rewarding correct predictions and penalizing incorrect ones.

Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up.","Questions:

What is LSTM-Jump and how does it work?

Answer:

LSTM-Jump (Yu et al., 2017) is a skipping mechanism designed to ignore irrelevant information for natural language understanding (NLU).

At each step, the current states are used to compute a ""jumping softmax,"" which determines how many steps to jump forward and whether to stop reading.

LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions.

The reward function is binary, rewarding correct predictions and penalizing incorrect ones.

Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up."
A Survey on Dynamic Neural Networks for Natural Language Processing,Internal classifier training Exit criterion,"DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy < θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ ) Voting  joint; sum of CE + diversity loss accumulated votes > θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ ) Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)

BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θ

CascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.

Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation. That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded. Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions. Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training. They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers. For inference, the model exits when k consecutive internal classifiers make the same prediction. Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.  further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold. Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other. However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair. They optimize these weights by a cross-level optimization algorithm.

They adopt PABEE's patience-based strategy for exiting. Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states. For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners. Entropy is used as the exit criterion. PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods. The inference is terminated when multiple layers are confident.

Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.

BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations. They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction. It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit. CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.

Cascading Cascading can be seen as a special form of early exit, performed at the model level. Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers. Therefore, they propose to use a suite of complete models with different numbers of layers for cascading. CascadeBERT executes models one by one, from the smallest to the largest. It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.

Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings. Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling. They use the maximum class probability as confidence on a per-token basis. Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers. These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens. The model completely exits when every token exits. A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification. Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking. ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane. They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.","1. (What are the different internal classifier training exit criteria?): sent1
    1.1. (What are the specific criteria for DeeBERT, RightTool, FastBERT, RomeBERT, SkipBERT, and PABEE?): sent1, sent2
    1.2. (What is the criterion for Voting?): sent3
    1.3. (What are the criteria for LeeBERT, Past-Future, PCEE-BERT, BERxiT, and CAT?): sent4, sent5
2. (What is the drawback of confidence-based early exit?): sent6
    2.1. (Why is there wasted computation in confidence-based early exit?): sent7
    2.2. (How does ensemble-based early exit address this drawback?): sent8
3. (How does PABEE address early stopping?): sent9
    3.1. (How are internal classifiers trained in PABEE?): sent10
    3.2. (When does the model exit during inference in PABEE?): sent11
    3.3. (What additional benefits does PABEE provide?): sent12
4. (What is the diversity loss in PABEE?): sent13
5. (What is the voting mechanism in PABEE?): sent14
6. (What is the strategy adopted by LeeBERT?): sent15
    6.1. (How does LeeBERT promote consistency across internal classifiers?): sent16
    6.2. (How are the weights optimized in LeeBERT?): sent17
    6.3. (What is the exit strategy in LeeBERT?): sent18
7. (What is the training method used by Liao et al. (2021)?): sent19
    7.1. (How is inference performed in Liao et al. (2021)?): sent20
    7.2. (What is the exit criterion in Liao et al. (2021)?): sent21
8. (How does PCEE-BERT combine ensemble-based and confidence-based methods?): sent22
    8.1. (When is inference terminated in PCEE-BERT?): sent23
9. (What is another stream of research for early exiting?): sent24
    9.1. (What is the training method used by BERxiT?): sent25
    9.2. (What is the learning-to-exit (LTE) module in BERxiT?): sent26
    9.3. (How does the LTE module function in BERxiT?): sent27
    9.4. (What is the meta consistency classifier in CAT?): sent28
10. (What is cascading in the context of early exit?): sent29
    10.1. (What did Li et al. (2021a) find about shallow features and internal classifiers?): sent30
    10.2. (What is the proposal by Li et al. (2021a) for cascading?): sent31
    10.3. (How does CascadeBERT function?): sent32, sent33
11. (What are the applications of early exit beyond classification?): sent34
    11.1. (What is Token-Level Early-Exit proposed by Li et al. (2021b)?): sent35
    11.2. (How does Token-Level Early-Exit function?): sent36, sent37, sent38, sent39
    11.3. (What similar ideas are presented by Elbayad et al. (2020) and Liu et al. (2021b)?): sent40
    11.4. (How is DeeBERT applied to document ranking by Xin et al. (2020a)?): sent41
    11.5. (What is ELUE and its purpose?): sent42
    11.6. (What does ELUE provide?): sent43",What are the different internal classifier training exit criteria in dynamic neural networks for NLP?,"sent1, sent2, sent3, sent4, sent5","DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy <

θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ )

Voting  joint; sum of CE + diversity loss accumulated votes >

θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ )

Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θCascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.","Different internal classifier training exit criteria in dynamic neural networks for NLP include DeeBERT (Xin et al., 2020b) using a two-stage approach with the sum of CE loss entropy < θ, RightTool (Schwartz et al., 2020) using a joint approach with the sum of CE loss calibrated max class probability > θ, and FastBERT using a two-stage approach with self-distillation entropy < θ.

RomeBERT (Geng et al., 2021) uses a joint approach with self-distillation and GR entropy < θ, SkipBERT (2022) employs a joint approach with a weighted sum of CE and KD max class probability > θ, and PABEE (Zhou et al., 2020a) utilizes a joint approach with a weighted sum of CE loss and patience (number of consistent predictions > θ).

Voting employs a joint approach with the sum of CE and diversity loss accumulated votes > θ.

LeeBERT (Zhu, 2021) uses a joint approach with an auto-weighted sum of CE and KD loss, and patience (number of consistent predictions > θ).

Past-Future (Liao et al., 2021) uses a joint approach with a weighted sum of CE and imitation learning entropy < θ, PCEE-BERT (2022a) employs a joint approach with a weighted sum of CE and patience (number of consistent IC confidence > θ), and BERxiT (Xin et al., 2021) uses an alternate approach with the sum of CE loss and estimated confidence > θ. CAT (Schuster et al., 2021) utilizes a joint approach with the average of CE loss and estimated conformity > θ, while CascadeBERT (Li et al., 2021a) employs a standard model fine-tuning with confidence calibration and calibrated max class probability > θ, placing lower BERT layers and using confidence-based early exit for higher layers to achieve maximum acceleration.","Questions:

What are the different internal classifier training exit criteria in dynamic neural networks for NLP?

Answer:

Different internal classifier training exit criteria in dynamic neural networks for NLP include DeeBERT (Xin et al., 2020b) using a two-stage approach with the sum of CE loss entropy < θ, RightTool (Schwartz et al., 2020) using a joint approach with the sum of CE loss calibrated max class probability > θ, and FastBERT using a two-stage approach with self-distillation entropy < θ.

RomeBERT (Geng et al., 2021) uses a joint approach with self-distillation and GR entropy < θ, SkipBERT (2022) employs a joint approach with a weighted sum of CE and KD max class probability > θ, and PABEE (Zhou et al., 2020a) utilizes a joint approach with a weighted sum of CE loss and patience (number of consistent predictions > θ).

Voting employs a joint approach with the sum of CE and diversity loss accumulated votes > θ.

LeeBERT (Zhu, 2021) uses a joint approach with an auto-weighted sum of CE and KD loss, and patience (number of consistent predictions > θ).

Past-Future (Liao et al., 2021) uses a joint approach with a weighted sum of CE and imitation learning entropy < θ, PCEE-BERT (2022a) employs a joint approach with a weighted sum of CE and patience (number of consistent IC confidence > θ), and BERxiT (Xin et al., 2021) uses an alternate approach with the sum of CE loss and estimated confidence > θ. CAT (Schuster et al., 2021) utilizes a joint approach with the average of CE loss and estimated conformity > θ, while CascadeBERT (Li et al., 2021a) employs a standard model fine-tuning with confidence calibration and calibrated max class probability > θ, placing lower BERT layers and using confidence-based early exit for higher layers to achieve maximum acceleration."
A Survey on Dynamic Neural Networks for Natural Language Processing,Internal classifier training Exit criterion,"DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy < θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ ) Voting  joint; sum of CE + diversity loss accumulated votes > θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ ) Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)

BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θ

CascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.

Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation. That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded. Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions. Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training. They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers. For inference, the model exits when k consecutive internal classifiers make the same prediction. Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.  further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold. Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other. However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair. They optimize these weights by a cross-level optimization algorithm.

They adopt PABEE's patience-based strategy for exiting. Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states. For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners. Entropy is used as the exit criterion. PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods. The inference is terminated when multiple layers are confident.

Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.

BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations. They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction. It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit. CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.

Cascading Cascading can be seen as a special form of early exit, performed at the model level. Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers. Therefore, they propose to use a suite of complete models with different numbers of layers for cascading. CascadeBERT executes models one by one, from the smallest to the largest. It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.

Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings. Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling. They use the maximum class probability as confidence on a per-token basis. Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers. These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens. The model completely exits when every token exits. A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification. Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking. ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane. They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.","1. (What are the different internal classifier training exit criteria?): sent1
    1.1. (What are the specific criteria for DeeBERT, RightTool, FastBERT, RomeBERT, SkipBERT, and PABEE?): sent1, sent2
    1.2. (What is the criterion for Voting?): sent3
    1.3. (What are the criteria for LeeBERT, Past-Future, PCEE-BERT, BERxiT, and CAT?): sent4, sent5
2. (What is the drawback of confidence-based early exit?): sent6
    2.1. (Why is there wasted computation in confidence-based early exit?): sent7
    2.2. (How does ensemble-based early exit address this drawback?): sent8
3. (How does PABEE address early stopping?): sent9
    3.1. (How are internal classifiers trained in PABEE?): sent10
    3.2. (When does the model exit during inference in PABEE?): sent11
    3.3. (What additional benefits does PABEE provide?): sent12
4. (What is the diversity loss in PABEE?): sent13
5. (What is the voting mechanism in PABEE?): sent14
6. (What is the strategy adopted by LeeBERT?): sent15
    6.1. (How does LeeBERT promote consistency across internal classifiers?): sent16
    6.2. (How are the weights optimized in LeeBERT?): sent17
    6.3. (What is the exit strategy in LeeBERT?): sent18
7. (What is the training method used by Liao et al. (2021)?): sent19
    7.1. (How is inference performed in Liao et al. (2021)?): sent20
    7.2. (What is the exit criterion in Liao et al. (2021)?): sent21
8. (How does PCEE-BERT combine ensemble-based and confidence-based methods?): sent22
    8.1. (When is inference terminated in PCEE-BERT?): sent23
9. (What is another stream of research for early exiting?): sent24
    9.1. (What is the training method used by BERxiT?): sent25
    9.2. (What is the learning-to-exit (LTE) module in BERxiT?): sent26
    9.3. (How does the LTE module function in BERxiT?): sent27
    9.4. (What is the meta consistency classifier in CAT?): sent28
10. (What is cascading in the context of early exit?): sent29
    10.1. (What did Li et al. (2021a) find about shallow features and internal classifiers?): sent30
    10.2. (What is the proposal by Li et al. (2021a) for cascading?): sent31
    10.3. (How does CascadeBERT function?): sent32, sent33
11. (What are the applications of early exit beyond classification?): sent34
    11.1. (What is Token-Level Early-Exit proposed by Li et al. (2021b)?): sent35
    11.2. (How does Token-Level Early-Exit function?): sent36, sent37, sent38, sent39
    11.3. (What similar ideas are presented by Elbayad et al. (2020) and Liu et al. (2021b)?): sent40
    11.4. (How is DeeBERT applied to document ranking by Xin et al. (2020a)?): sent41
    11.5. (What is ELUE and its purpose?): sent42
    11.6. (What does ELUE provide?): sent43",How does PABEE address early stopping in dynamic neural networks for NLP?,"sent9, sent10, sent11, sent12","Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training.

They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers.

For inference, the model exits when k consecutive internal classifiers make the same prediction.

Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.","PABEE (Zhou et al., 2020a) addresses early stopping in dynamic neural networks by borrowing the concept from model training, based on the similarity between overfitting and overthinking.

PABEE first jointly trains the internal classifiers with BERT by using a weighted sum of cross-entropy losses that assigns larger weights to upper classifiers.

For inference, the model exits when k consecutive internal classifiers make the same prediction.

In addition to improving performance and efficiency, PABEE can enhance adversarial robustness, which is attributed to the ensemble effect.","Questions:

How does PABEE address early stopping in dynamic neural networks for NLP?

Answer:

PABEE (Zhou et al., 2020a) addresses early stopping in dynamic neural networks by borrowing the concept from model training, based on the similarity between overfitting and overthinking.

PABEE first jointly trains the internal classifiers with BERT by using a weighted sum of cross-entropy losses that assigns larger weights to upper classifiers.

For inference, the model exits when k consecutive internal classifiers make the same prediction.

In addition to improving performance and efficiency, PABEE can enhance adversarial robustness, which is attributed to the ensemble effect."
A Survey on Dynamic Neural Networks for Natural Language Processing,Internal classifier training Exit criterion,"DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy < θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ ) Voting  joint; sum of CE + diversity loss accumulated votes > θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ ) Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)

BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θ

CascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.

Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation. That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded. Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions. Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training. They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers. For inference, the model exits when k consecutive internal classifiers make the same prediction. Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.  further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold. Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other. However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair. They optimize these weights by a cross-level optimization algorithm.

They adopt PABEE's patience-based strategy for exiting. Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states. For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners. Entropy is used as the exit criterion. PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods. The inference is terminated when multiple layers are confident.

Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.

BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations. They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction. It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit. CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.

Cascading Cascading can be seen as a special form of early exit, performed at the model level. Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers. Therefore, they propose to use a suite of complete models with different numbers of layers for cascading. CascadeBERT executes models one by one, from the smallest to the largest. It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.

Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings. Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling. They use the maximum class probability as confidence on a per-token basis. Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers. These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens. The model completely exits when every token exits. A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification. Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking. ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane. They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.","1. (What are the different internal classifier training exit criteria?): sent1
    1.1. (What are the specific criteria for DeeBERT, RightTool, FastBERT, RomeBERT, SkipBERT, and PABEE?): sent1, sent2
    1.2. (What is the criterion for Voting?): sent3
    1.3. (What are the criteria for LeeBERT, Past-Future, PCEE-BERT, BERxiT, and CAT?): sent4, sent5
2. (What is the drawback of confidence-based early exit?): sent6
    2.1. (Why is there wasted computation in confidence-based early exit?): sent7
    2.2. (How does ensemble-based early exit address this drawback?): sent8
3. (How does PABEE address early stopping?): sent9
    3.1. (How are internal classifiers trained in PABEE?): sent10
    3.2. (When does the model exit during inference in PABEE?): sent11
    3.3. (What additional benefits does PABEE provide?): sent12
4. (What is the diversity loss in PABEE?): sent13
5. (What is the voting mechanism in PABEE?): sent14
6. (What is the strategy adopted by LeeBERT?): sent15
    6.1. (How does LeeBERT promote consistency across internal classifiers?): sent16
    6.2. (How are the weights optimized in LeeBERT?): sent17
    6.3. (What is the exit strategy in LeeBERT?): sent18
7. (What is the training method used by Liao et al. (2021)?): sent19
    7.1. (How is inference performed in Liao et al. (2021)?): sent20
    7.2. (What is the exit criterion in Liao et al. (2021)?): sent21
8. (How does PCEE-BERT combine ensemble-based and confidence-based methods?): sent22
    8.1. (When is inference terminated in PCEE-BERT?): sent23
9. (What is another stream of research for early exiting?): sent24
    9.1. (What is the training method used by BERxiT?): sent25
    9.2. (What is the learning-to-exit (LTE) module in BERxiT?): sent26
    9.3. (How does the LTE module function in BERxiT?): sent27
    9.4. (What is the meta consistency classifier in CAT?): sent28
10. (What is cascading in the context of early exit?): sent29
    10.1. (What did Li et al. (2021a) find about shallow features and internal classifiers?): sent30
    10.2. (What is the proposal by Li et al. (2021a) for cascading?): sent31
    10.3. (How does CascadeBERT function?): sent32, sent33
11. (What are the applications of early exit beyond classification?): sent34
    11.1. (What is Token-Level Early-Exit proposed by Li et al. (2021b)?): sent35
    11.2. (How does Token-Level Early-Exit function?): sent36, sent37, sent38, sent39
    11.3. (What similar ideas are presented by Elbayad et al. (2020) and Liu et al. (2021b)?): sent40
    11.4. (How is DeeBERT applied to document ranking by Xin et al. (2020a)?): sent41
    11.5. (What is ELUE and its purpose?): sent42
    11.6. (What does ELUE provide?): sent43",What strategy does LeeBERT adopt to promote consistency across internal classifiers?,"sent15, sent16, sent17, sent18","Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other.

However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair.

They optimize these weights by a cross-level optimization algorithm.

They adopt PABEE's patience-based strategy for exiting.","LeeBERT (Zhu, 2021) promotes consistency across internal classifiers by distilling them to each other.

They introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair.

These weights are optimized by a cross-level optimization algorithm.

LeeBERT adopts PABEE's patience-based strategy for exiting.","Questions:

What strategy does LeeBERT adopt to promote consistency across internal classifiers?

Answer:

LeeBERT (Zhu, 2021) promotes consistency across internal classifiers by distilling them to each other.

They introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair.

These weights are optimized by a cross-level optimization algorithm.

LeeBERT adopts PABEE's patience-based strategy for exiting."
A Survey on Dynamic Neural Networks for Natural Language Processing,Internal classifier training Exit criterion,"DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy < θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ ) Voting  joint; sum of CE + diversity loss accumulated votes > θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ ) Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)

BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θ

CascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.

Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation. That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded. Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions. Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training. They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers. For inference, the model exits when k consecutive internal classifiers make the same prediction. Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.  further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold. Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other. However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair. They optimize these weights by a cross-level optimization algorithm.

They adopt PABEE's patience-based strategy for exiting. Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states. For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners. Entropy is used as the exit criterion. PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods. The inference is terminated when multiple layers are confident.

Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.

BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations. They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction. It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit. CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.

Cascading Cascading can be seen as a special form of early exit, performed at the model level. Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers. Therefore, they propose to use a suite of complete models with different numbers of layers for cascading. CascadeBERT executes models one by one, from the smallest to the largest. It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.

Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings. Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling. They use the maximum class probability as confidence on a per-token basis. Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers. These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens. The model completely exits when every token exits. A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification. Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking. ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane. They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.","1. (What are the different internal classifier training exit criteria?): sent1
    1.1. (What are the specific criteria for DeeBERT, RightTool, FastBERT, RomeBERT, SkipBERT, and PABEE?): sent1, sent2
    1.2. (What is the criterion for Voting?): sent3
    1.3. (What are the criteria for LeeBERT, Past-Future, PCEE-BERT, BERxiT, and CAT?): sent4, sent5
2. (What is the drawback of confidence-based early exit?): sent6
    2.1. (Why is there wasted computation in confidence-based early exit?): sent7
    2.2. (How does ensemble-based early exit address this drawback?): sent8
3. (How does PABEE address early stopping?): sent9
    3.1. (How are internal classifiers trained in PABEE?): sent10
    3.2. (When does the model exit during inference in PABEE?): sent11
    3.3. (What additional benefits does PABEE provide?): sent12
4. (What is the diversity loss in PABEE?): sent13
5. (What is the voting mechanism in PABEE?): sent14
6. (What is the strategy adopted by LeeBERT?): sent15
    6.1. (How does LeeBERT promote consistency across internal classifiers?): sent16
    6.2. (How are the weights optimized in LeeBERT?): sent17
    6.3. (What is the exit strategy in LeeBERT?): sent18
7. (What is the training method used by Liao et al. (2021)?): sent19
    7.1. (How is inference performed in Liao et al. (2021)?): sent20
    7.2. (What is the exit criterion in Liao et al. (2021)?): sent21
8. (How does PCEE-BERT combine ensemble-based and confidence-based methods?): sent22
    8.1. (When is inference terminated in PCEE-BERT?): sent23
9. (What is another stream of research for early exiting?): sent24
    9.1. (What is the training method used by BERxiT?): sent25
    9.2. (What is the learning-to-exit (LTE) module in BERxiT?): sent26
    9.3. (How does the LTE module function in BERxiT?): sent27
    9.4. (What is the meta consistency classifier in CAT?): sent28
10. (What is cascading in the context of early exit?): sent29
    10.1. (What did Li et al. (2021a) find about shallow features and internal classifiers?): sent30
    10.2. (What is the proposal by Li et al. (2021a) for cascading?): sent31
    10.3. (How does CascadeBERT function?): sent32, sent33
11. (What are the applications of early exit beyond classification?): sent34
    11.1. (What is Token-Level Early-Exit proposed by Li et al. (2021b)?): sent35
    11.2. (How does Token-Level Early-Exit function?): sent36, sent37, sent38, sent39
    11.3. (What similar ideas are presented by Elbayad et al. (2020) and Liu et al. (2021b)?): sent40
    11.4. (How is DeeBERT applied to document ranking by Xin et al. (2020a)?): sent41
    11.5. (What is ELUE and its purpose?): sent42
    11.6. (What does ELUE provide?): sent43",What is another stream of research for early exiting in dynamic neural networks?,"sent24, sent25, sent26, sent27, sent28","Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.

BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations.

They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction.

It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit.

CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.","Another stream of research for early exiting in dynamic neural networks is to learn a criterion for early exiting.

BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of the Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations.

A linear layer called a learning-to-exit (LTE) module is trained to predict whether the current internal classifier makes the correct prediction.

The LTE module takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit.

CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.","Questions:

What is another stream of research for early exiting in dynamic neural networks?

Answer:

Another stream of research for early exiting in dynamic neural networks is to learn a criterion for early exiting.

BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of the Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations.

A linear layer called a learning-to-exit (LTE) module is trained to predict whether the current internal classifier makes the correct prediction.

The LTE module takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit.

CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity."
A Survey on Dynamic Neural Networks for Natural Language Processing,Internal classifier training Exit criterion,"DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy < θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ ) Voting  joint; sum of CE + diversity loss accumulated votes > θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ ) Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)

BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θ

CascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.

Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation. That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded. Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions. Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training. They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers. For inference, the model exits when k consecutive internal classifiers make the same prediction. Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.  further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold. Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other. However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair. They optimize these weights by a cross-level optimization algorithm.

They adopt PABEE's patience-based strategy for exiting. Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states. For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners. Entropy is used as the exit criterion. PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods. The inference is terminated when multiple layers are confident.

Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.

BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations. They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction. It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit. CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.

Cascading Cascading can be seen as a special form of early exit, performed at the model level. Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers. Therefore, they propose to use a suite of complete models with different numbers of layers for cascading. CascadeBERT executes models one by one, from the smallest to the largest. It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.

Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings. Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling. They use the maximum class probability as confidence on a per-token basis. Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers. These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens. The model completely exits when every token exits. A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification. Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking. ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane. They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.","1. (What are the different internal classifier training exit criteria?): sent1
    1.1. (What are the specific criteria for DeeBERT, RightTool, FastBERT, RomeBERT, SkipBERT, and PABEE?): sent1, sent2
    1.2. (What is the criterion for Voting?): sent3
    1.3. (What are the criteria for LeeBERT, Past-Future, PCEE-BERT, BERxiT, and CAT?): sent4, sent5
2. (What is the drawback of confidence-based early exit?): sent6
    2.1. (Why is there wasted computation in confidence-based early exit?): sent7
    2.2. (How does ensemble-based early exit address this drawback?): sent8
3. (How does PABEE address early stopping?): sent9
    3.1. (How are internal classifiers trained in PABEE?): sent10
    3.2. (When does the model exit during inference in PABEE?): sent11
    3.3. (What additional benefits does PABEE provide?): sent12
4. (What is the diversity loss in PABEE?): sent13
5. (What is the voting mechanism in PABEE?): sent14
6. (What is the strategy adopted by LeeBERT?): sent15
    6.1. (How does LeeBERT promote consistency across internal classifiers?): sent16
    6.2. (How are the weights optimized in LeeBERT?): sent17
    6.3. (What is the exit strategy in LeeBERT?): sent18
7. (What is the training method used by Liao et al. (2021)?): sent19
    7.1. (How is inference performed in Liao et al. (2021)?): sent20
    7.2. (What is the exit criterion in Liao et al. (2021)?): sent21
8. (How does PCEE-BERT combine ensemble-based and confidence-based methods?): sent22
    8.1. (When is inference terminated in PCEE-BERT?): sent23
9. (What is another stream of research for early exiting?): sent24
    9.1. (What is the training method used by BERxiT?): sent25
    9.2. (What is the learning-to-exit (LTE) module in BERxiT?): sent26
    9.3. (How does the LTE module function in BERxiT?): sent27
    9.4. (What is the meta consistency classifier in CAT?): sent28
10. (What is cascading in the context of early exit?): sent29
    10.1. (What did Li et al. (2021a) find about shallow features and internal classifiers?): sent30
    10.2. (What is the proposal by Li et al. (2021a) for cascading?): sent31
    10.3. (How does CascadeBERT function?): sent32, sent33
11. (What are the applications of early exit beyond classification?): sent34
    11.1. (What is Token-Level Early-Exit proposed by Li et al. (2021b)?): sent35
    11.2. (How does Token-Level Early-Exit function?): sent36, sent37, sent38, sent39
    11.3. (What similar ideas are presented by Elbayad et al. (2020) and Liu et al. (2021b)?): sent40
    11.4. (How is DeeBERT applied to document ranking by Xin et al. (2020a)?): sent41
    11.5. (What is ELUE and its purpose?): sent42
    11.6. (What does ELUE provide?): sent43",What is cascading in the context of early exit for dynamic neural networks?,"sent29, sent30, sent31, sent32, sent33","Cascading Cascading can be seen as a special form of early exit, performed at the model level.

Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers.

Therefore, they propose to use a suite of complete models with different numbers of layers for cascading.

CascadeBERT executes models one by one, from the smallest to the largest.

It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.","Cascading can be seen as a special form of early exit, performed at the model level.

Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers.

Therefore, they propose to use a suite of complete models with different numbers of layers for cascading.

CascadeBERT executes models sequentially, from the smallest to the largest.

It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.","Questions:

What is cascading in the context of early exit for dynamic neural networks?

Answer:

Cascading can be seen as a special form of early exit, performed at the model level.

Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers.

Therefore, they propose to use a suite of complete models with different numbers of layers for cascading.

CascadeBERT executes models sequentially, from the smallest to the largest.

It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold."
A Survey on Dynamic Neural Networks for Natural Language Processing,Internal classifier training Exit criterion,"DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy < θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ ) Voting  joint; sum of CE + diversity loss accumulated votes > θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ ) Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)

BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θ

CascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.

Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation. That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded. Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions. Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training. They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers. For inference, the model exits when k consecutive internal classifiers make the same prediction. Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.  further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold. Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other. However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair. They optimize these weights by a cross-level optimization algorithm.

They adopt PABEE's patience-based strategy for exiting. Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states. For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners. Entropy is used as the exit criterion. PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods. The inference is terminated when multiple layers are confident.

Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.

BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations. They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction. It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit. CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.

Cascading Cascading can be seen as a special form of early exit, performed at the model level. Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers. Therefore, they propose to use a suite of complete models with different numbers of layers for cascading. CascadeBERT executes models one by one, from the smallest to the largest. It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.

Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings. Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling. They use the maximum class probability as confidence on a per-token basis. Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers. These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens. The model completely exits when every token exits. A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification. Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking. ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane. They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.","1. (What are the different internal classifier training exit criteria?): sent1
    1.1. (What are the specific criteria for DeeBERT, RightTool, FastBERT, RomeBERT, SkipBERT, and PABEE?): sent1, sent2
    1.2. (What is the criterion for Voting?): sent3
    1.3. (What are the criteria for LeeBERT, Past-Future, PCEE-BERT, BERxiT, and CAT?): sent4, sent5
2. (What is the drawback of confidence-based early exit?): sent6
    2.1. (Why is there wasted computation in confidence-based early exit?): sent7
    2.2. (How does ensemble-based early exit address this drawback?): sent8
3. (How does PABEE address early stopping?): sent9
    3.1. (How are internal classifiers trained in PABEE?): sent10
    3.2. (When does the model exit during inference in PABEE?): sent11
    3.3. (What additional benefits does PABEE provide?): sent12
4. (What is the diversity loss in PABEE?): sent13
5. (What is the voting mechanism in PABEE?): sent14
6. (What is the strategy adopted by LeeBERT?): sent15
    6.1. (How does LeeBERT promote consistency across internal classifiers?): sent16
    6.2. (How are the weights optimized in LeeBERT?): sent17
    6.3. (What is the exit strategy in LeeBERT?): sent18
7. (What is the training method used by Liao et al. (2021)?): sent19
    7.1. (How is inference performed in Liao et al. (2021)?): sent20
    7.2. (What is the exit criterion in Liao et al. (2021)?): sent21
8. (How does PCEE-BERT combine ensemble-based and confidence-based methods?): sent22
    8.1. (When is inference terminated in PCEE-BERT?): sent23
9. (What is another stream of research for early exiting?): sent24
    9.1. (What is the training method used by BERxiT?): sent25
    9.2. (What is the learning-to-exit (LTE) module in BERxiT?): sent26
    9.3. (How does the LTE module function in BERxiT?): sent27
    9.4. (What is the meta consistency classifier in CAT?): sent28
10. (What is cascading in the context of early exit?): sent29
    10.1. (What did Li et al. (2021a) find about shallow features and internal classifiers?): sent30
    10.2. (What is the proposal by Li et al. (2021a) for cascading?): sent31
    10.3. (How does CascadeBERT function?): sent32, sent33
11. (What are the applications of early exit beyond classification?): sent34
    11.1. (What is Token-Level Early-Exit proposed by Li et al. (2021b)?): sent35
    11.2. (How does Token-Level Early-Exit function?): sent36, sent37, sent38, sent39
    11.3. (What similar ideas are presented by Elbayad et al. (2020) and Liu et al. (2021b)?): sent40
    11.4. (How is DeeBERT applied to document ranking by Xin et al. (2020a)?): sent41
    11.5. (What is ELUE and its purpose?): sent42
    11.6. (What does ELUE provide?): sent43",What are the applications of early exit beyond classification in dynamic neural networks?,"sent34, sent35, sent36, sent37, sent38, sent39, sent40, sent41, sent42, sent43","Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings.

Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling.

They use the maximum class probability as confidence on a per-token basis.

Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers.

These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens.

The model completely exits when every token exits.

A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification.

Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking.

ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane.

They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.","Early exit, originally developed for classification, has been extended to more tasks and settings.

For example, Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling.

They use the maximum class probability as confidence on a per-token basis.

Once the confidence hits the threshold, the hidden states of the corresponding tokens are frozen and directly copied to upper layers.

These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens.

The model completely exits when every token exits.

A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification.

Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds for the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking.

ELUE (Liu et al., 2021a) is a benchmark that evaluates the Pareto Front of early exit models on the FLOPs-performance plane.

They provide a BERT-like baseline with jointly pretrained internal classifiers to mitigate the gap between pretraining and fine-tuning.","Questions:

What are the applications of early exit beyond classification in dynamic neural networks?

Answer:

Early exit, originally developed for classification, has been extended to more tasks and settings.

For example, Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling.

They use the maximum class probability as confidence on a per-token basis.

Once the confidence hits the threshold, the hidden states of the corresponding tokens are frozen and directly copied to upper layers.

These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens.

The model completely exits when every token exits.

A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification.

Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds for the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking.

ELUE (Liu et al., 2021a) is a benchmark that evaluates the Pareto Front of early exit models on the FLOPs-performance plane.

They provide a BERT-like baseline with jointly pretrained internal classifiers to mitigate the gap between pretraining and fine-tuning."
"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions",B Simulator,"The virtual features of the dataset are deeply connected with the simulator in which datasets are built. Here we summarize simulators frequently used during the VLN dataset creation process. House3D (Wu et al., 2018) is a realistic virtual 3D environment built based on the SUNCG (Song et al., 2017) dataset. An agent in the environment has access to first-person view RGB images, together with semantic/instance masks and depth information.

Matterport3D (Anderson et al., 2018b) simulator is a large-scale visual reinforcement learning simulation environment for research on embodied AI based on the Matterport3D dataset (Chang et al., 2017). Matterport3D contains various indoor scenes, including houses, apartments, hotels, offices, and churches. An agent can navigate between viewpoints along a pre-defined graph. Most indoors VLN datasets such as R2R and its variants are based on the Matterport3D simulator.

Habitat ( where agents could navigate and interact with objects. Based on the object interaction function, it helps to build a dataset that requires object interaction, such as ALFRED (Shridhar et al., 2020).

Gibson (Xia et al., 2018) is a real-world perception interactive environment with complex semantics. Each viewpoint has a set of RGB panoramas with global camera poses and reconstructed 3D meshes. Matterport3D dataset (Chang et al., 2017) is also integrated into the Gibson simulator.

House3D (Wu et al., 2018) converts SUNCG's static environment into a virtual environment, where the agent can navigate with physical constraints (e.g. it cannot pass through walls or objects).

LANI (Misra et al., 2018) is a 3D simulator built in Unity3D platform. The environment in LANI is a fenced, square, grass field containing randomly placed landmarks. An agent needs to navigate between landmarks following the natural language instruction. Drone navigation tasks (Blukis et al., 2018(Blukis et al., , 2019 are also built based on LANI.

Currently, most datasets and simulators focus on indoors navigable scenes partly because of the difficulty of building an outdoor photo-realistic 3D simulator out of the increased complexity. Google Street View 4 , an online API that is integrated with Google Maps, is composed of billions of realistic street-level panoramas. It has been frequently used to create outdoor VLN tasks since the development of TOUCHDOWN (Chen et al., 2019).","1. (What is the relationship between virtual features of the dataset and the simulator?): sent1
    1.1. (What simulators are frequently used during the VLN dataset creation process?): sent2
        1.1.1. (What is House3D?): sent3
            1.1.1.1. (What does an agent in House3D have access to?): sent4
            1.1.1.2. (How does House3D convert SUNCG's static environment?): sent13
        1.1.2. (What is Matterport3D?): sent5
            1.1.2.1. (What does Matterport3D contain?): sent6
            1.1.2.2. (How can an agent navigate in Matterport3D?): sent7
            1.1.2.3. (What datasets are based on Matterport3D simulator?): sent8
            1.1.2.4. (How is Matterport3D integrated into other simulators?): sent12
        1.1.3. (What is Habitat?): sent9
        1.1.4. (What is Gibson?): sent10
            1.1.4.1. (What does each viewpoint in Gibson have?): sent11
        1.1.5. (What is LANI?): sent14
            1.1.5.1. (What is the environment in LANI like?): sent15
            1.1.5.2. (What does an agent need to do in LANI?): sent16
            1.1.5.3. (What tasks are built based on LANI?): sent17
2. (Why do most datasets and simulators focus on indoors navigable scenes?): sent18
    2.1. (What is Google Street View 4?): sent19
        2.1.1. (How has Google Street View 4 been used in VLN tasks?): sent20",What are the features and functionalities of the House3D simulator?,"sent3, sent4, sent13","House3D (Wu et al., 2018) is a realistic virtual 3D environment built based on the SUNCG (Song et al., 2017) dataset.

An agent in the environment has access to first-person view RGB images, together with semantic/instance masks and depth information.

House3D (Wu et al., 2018) converts SUNCG's static environment into a virtual environment, where the agent can navigate with physical constraints (e.g. it cannot pass through walls or objects).","House3D (Wu et al., 2018) is a realistic virtual 3D environment built based on the SUNCG (Song et al., 2017) dataset.

An agent in the environment has access to first-person view RGB images, along with semantic/instance masks and depth information.

House3D (Wu et al., 2018) converts SUNCG's static environment into a virtual environment, where the agent can navigate with physical constraints, such as being unable to pass through walls or objects.","Questions:

What are the features and functionalities of the House3D simulator?

Answer:

House3D (Wu et al., 2018) is a realistic virtual 3D environment built based on the SUNCG (Song et al., 2017) dataset.

An agent in the environment has access to first-person view RGB images, along with semantic/instance masks and depth information.

House3D (Wu et al., 2018) converts SUNCG's static environment into a virtual environment, where the agent can navigate with physical constraints, such as being unable to pass through walls or objects."
"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions",B Simulator,"The virtual features of the dataset are deeply connected with the simulator in which datasets are built. Here we summarize simulators frequently used during the VLN dataset creation process. House3D (Wu et al., 2018) is a realistic virtual 3D environment built based on the SUNCG (Song et al., 2017) dataset. An agent in the environment has access to first-person view RGB images, together with semantic/instance masks and depth information.

Matterport3D (Anderson et al., 2018b) simulator is a large-scale visual reinforcement learning simulation environment for research on embodied AI based on the Matterport3D dataset (Chang et al., 2017). Matterport3D contains various indoor scenes, including houses, apartments, hotels, offices, and churches. An agent can navigate between viewpoints along a pre-defined graph. Most indoors VLN datasets such as R2R and its variants are based on the Matterport3D simulator.

Habitat ( where agents could navigate and interact with objects. Based on the object interaction function, it helps to build a dataset that requires object interaction, such as ALFRED (Shridhar et al., 2020).

Gibson (Xia et al., 2018) is a real-world perception interactive environment with complex semantics. Each viewpoint has a set of RGB panoramas with global camera poses and reconstructed 3D meshes. Matterport3D dataset (Chang et al., 2017) is also integrated into the Gibson simulator.

House3D (Wu et al., 2018) converts SUNCG's static environment into a virtual environment, where the agent can navigate with physical constraints (e.g. it cannot pass through walls or objects).

LANI (Misra et al., 2018) is a 3D simulator built in Unity3D platform. The environment in LANI is a fenced, square, grass field containing randomly placed landmarks. An agent needs to navigate between landmarks following the natural language instruction. Drone navigation tasks (Blukis et al., 2018(Blukis et al., , 2019 are also built based on LANI.

Currently, most datasets and simulators focus on indoors navigable scenes partly because of the difficulty of building an outdoor photo-realistic 3D simulator out of the increased complexity. Google Street View 4 , an online API that is integrated with Google Maps, is composed of billions of realistic street-level panoramas. It has been frequently used to create outdoor VLN tasks since the development of TOUCHDOWN (Chen et al., 2019).","1. (What is the relationship between virtual features of the dataset and the simulator?): sent1
    1.1. (What simulators are frequently used during the VLN dataset creation process?): sent2
        1.1.1. (What is House3D?): sent3
            1.1.1.1. (What does an agent in House3D have access to?): sent4
            1.1.1.2. (How does House3D convert SUNCG's static environment?): sent13
        1.1.2. (What is Matterport3D?): sent5
            1.1.2.1. (What does Matterport3D contain?): sent6
            1.1.2.2. (How can an agent navigate in Matterport3D?): sent7
            1.1.2.3. (What datasets are based on Matterport3D simulator?): sent8
            1.1.2.4. (How is Matterport3D integrated into other simulators?): sent12
        1.1.3. (What is Habitat?): sent9
        1.1.4. (What is Gibson?): sent10
            1.1.4.1. (What does each viewpoint in Gibson have?): sent11
        1.1.5. (What is LANI?): sent14
            1.1.5.1. (What is the environment in LANI like?): sent15
            1.1.5.2. (What does an agent need to do in LANI?): sent16
            1.1.5.3. (What tasks are built based on LANI?): sent17
2. (Why do most datasets and simulators focus on indoors navigable scenes?): sent18
    2.1. (What is Google Street View 4?): sent19
        2.1.1. (How has Google Street View 4 been used in VLN tasks?): sent20",What are the characteristics and uses of the Matterport3D simulator?,"sent5, sent6, sent7, sent8, sent12","Matterport3D (Anderson et al., 2018b) simulator is a large-scale visual reinforcement learning simulation environment for research on embodied AI based on the Matterport3D dataset (Chang et al., 2017).

Matterport3D contains various indoor scenes, including houses, apartments, hotels, offices, and churches.

An agent can navigate between viewpoints along a pre-defined graph.

Most indoors VLN datasets such as R2R and its variants are based on the Matterport3D simulator.

Matterport3D dataset (Chang et al., 2017) is also integrated into the Gibson simulator.","The Matterport3D (Anderson et al., 2018b) simulator is a large-scale visual reinforcement learning simulation environment for research on embodied AI based on the Matterport3D dataset (Chang et al., 2017).

Matterport3D contains various indoor scenes, including houses, apartments, hotels, offices, and churches.

An agent can navigate between viewpoints along a pre-defined graph.

Most indoor VLN datasets, such as R2R and its variants, are based on the Matterport3D simulator.

The Matterport3D dataset (Chang et al., 2017) is also integrated into the Gibson simulator.","Questions:

What are the characteristics and uses of the Matterport3D simulator?

Answer:

The Matterport3D (Anderson et al., 2018b) simulator is a large-scale visual reinforcement learning simulation environment for research on embodied AI based on the Matterport3D dataset (Chang et al., 2017).

Matterport3D contains various indoor scenes, including houses, apartments, hotels, offices, and churches.

An agent can navigate between viewpoints along a pre-defined graph.

Most indoor VLN datasets, such as R2R and its variants, are based on the Matterport3D simulator.

The Matterport3D dataset (Chang et al., 2017) is also integrated into the Gibson simulator."
"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions",B Simulator,"The virtual features of the dataset are deeply connected with the simulator in which datasets are built. Here we summarize simulators frequently used during the VLN dataset creation process. House3D (Wu et al., 2018) is a realistic virtual 3D environment built based on the SUNCG (Song et al., 2017) dataset. An agent in the environment has access to first-person view RGB images, together with semantic/instance masks and depth information.

Matterport3D (Anderson et al., 2018b) simulator is a large-scale visual reinforcement learning simulation environment for research on embodied AI based on the Matterport3D dataset (Chang et al., 2017). Matterport3D contains various indoor scenes, including houses, apartments, hotels, offices, and churches. An agent can navigate between viewpoints along a pre-defined graph. Most indoors VLN datasets such as R2R and its variants are based on the Matterport3D simulator.

Habitat ( where agents could navigate and interact with objects. Based on the object interaction function, it helps to build a dataset that requires object interaction, such as ALFRED (Shridhar et al., 2020).

Gibson (Xia et al., 2018) is a real-world perception interactive environment with complex semantics. Each viewpoint has a set of RGB panoramas with global camera poses and reconstructed 3D meshes. Matterport3D dataset (Chang et al., 2017) is also integrated into the Gibson simulator.

House3D (Wu et al., 2018) converts SUNCG's static environment into a virtual environment, where the agent can navigate with physical constraints (e.g. it cannot pass through walls or objects).

LANI (Misra et al., 2018) is a 3D simulator built in Unity3D platform. The environment in LANI is a fenced, square, grass field containing randomly placed landmarks. An agent needs to navigate between landmarks following the natural language instruction. Drone navigation tasks (Blukis et al., 2018(Blukis et al., , 2019 are also built based on LANI.

Currently, most datasets and simulators focus on indoors navigable scenes partly because of the difficulty of building an outdoor photo-realistic 3D simulator out of the increased complexity. Google Street View 4 , an online API that is integrated with Google Maps, is composed of billions of realistic street-level panoramas. It has been frequently used to create outdoor VLN tasks since the development of TOUCHDOWN (Chen et al., 2019).","1. (What is the relationship between virtual features of the dataset and the simulator?): sent1
    1.1. (What simulators are frequently used during the VLN dataset creation process?): sent2
        1.1.1. (What is House3D?): sent3
            1.1.1.1. (What does an agent in House3D have access to?): sent4
            1.1.1.2. (How does House3D convert SUNCG's static environment?): sent13
        1.1.2. (What is Matterport3D?): sent5
            1.1.2.1. (What does Matterport3D contain?): sent6
            1.1.2.2. (How can an agent navigate in Matterport3D?): sent7
            1.1.2.3. (What datasets are based on Matterport3D simulator?): sent8
            1.1.2.4. (How is Matterport3D integrated into other simulators?): sent12
        1.1.3. (What is Habitat?): sent9
        1.1.4. (What is Gibson?): sent10
            1.1.4.1. (What does each viewpoint in Gibson have?): sent11
        1.1.5. (What is LANI?): sent14
            1.1.5.1. (What is the environment in LANI like?): sent15
            1.1.5.2. (What does an agent need to do in LANI?): sent16
            1.1.5.3. (What tasks are built based on LANI?): sent17
2. (Why do most datasets and simulators focus on indoors navigable scenes?): sent18
    2.1. (What is Google Street View 4?): sent19
        2.1.1. (How has Google Street View 4 been used in VLN tasks?): sent20",What are the capabilities and applications of the LANI simulator?,"sent14, sent15, sent16, sent17","LANI (Misra et al., 2018) is a 3D simulator built in Unity3D platform.

The environment in LANI is a fenced, square, grass field containing randomly placed landmarks.

An agent needs to navigate between landmarks following the natural language instruction.

Drone navigation tasks (Blukis et al., 2018(Blukis et al., , 2019 are also built based on LANI.","LANI (Misra et al., 2018) is a 3D simulator built on the Unity3D platform.

The environment in LANI is a fenced, square, grass field containing randomly placed landmarks.

An agent in LANI needs to navigate between landmarks following natural language instructions.

Drone navigation tasks (Blukis et al., 2018; Blukis et al., 2019) are also built based on LANI.","Questions:

What are the capabilities and applications of the LANI simulator?

Answer:

LANI (Misra et al., 2018) is a 3D simulator built on the Unity3D platform.

The environment in LANI is a fenced, square, grass field containing randomly placed landmarks.

An agent in LANI needs to navigate between landmarks following natural language instructions.

Drone navigation tasks (Blukis et al., 2018; Blukis et al., 2019) are also built based on LANI."
"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions",Initial Instruction,"In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.

Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","1. What is the initial instruction given to the agent in many VLN benchmarks?
    1.1. What is an example of a natural language instruction for the whole navigation process? sent1
        1.1.1. What is the continuation of the example instruction? sent2
    1.2. What does the agent need to do to reach the target goal? sent3

2. What is the R2R dataset and how is it created?
    2.1. Who created the R2R dataset and what is it based on? sent4
    2.2. How does an embodied agent in R2R navigate? sent5
    2.3. How is R2R extended to create other VLN benchmarks? sent6
        2.3.1. What is Roomfor-Room and how does it extend R2R? sent7
        2.3.2. What are some other VLN benchmarks extended from R2R? sent8

3. What are some specific VLN benchmarks and their characteristics?
    3.1. What is TOUCHDOWN and what does the agent do in this benchmark? sent9
    3.2. What are some photo-realistic outdoor VLN datasets and what are they based on? sent10
    3.3. How is natural language used to guide drones in VLN? sent11
        3.3.1. What is LANI and how does it use natural language instructions? sent12
        3.3.2. What is the environment for current datasets on drone navigation? sent13",What is the initial instruction given to the agent in many VLN benchmarks?,"sent1, sent2, sent3","In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room.

Turn left and go through the door in the middle.""

Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal.","In many VLN benchmarks, the agent is given a natural language instruction for the entire navigation process, such as ""Go upstairs and pass the table in the living room.""

The agent might receive instructions like ""Turn left and go through the door in the middle.""

The agent needs to strictly follow the natural language instruction to reach the target goal.","Questions:

What is the initial instruction given to the agent in many VLN benchmarks?

Answer:

In many VLN benchmarks, the agent is given a natural language instruction for the entire navigation process, such as ""Go upstairs and pass the table in the living room.""

The agent might receive instructions like ""Turn left and go through the door in the middle.""

The agent needs to strictly follow the natural language instruction to reach the target goal."
"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions",Initial Instruction,"In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.

Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","1. What is the initial instruction given to the agent in many VLN benchmarks?
    1.1. What is an example of a natural language instruction for the whole navigation process? sent1
        1.1.1. What is the continuation of the example instruction? sent2
    1.2. What does the agent need to do to reach the target goal? sent3

2. What is the R2R dataset and how is it created?
    2.1. Who created the R2R dataset and what is it based on? sent4
    2.2. How does an embodied agent in R2R navigate? sent5
    2.3. How is R2R extended to create other VLN benchmarks? sent6
        2.3.1. What is Roomfor-Room and how does it extend R2R? sent7
        2.3.2. What are some other VLN benchmarks extended from R2R? sent8

3. What are some specific VLN benchmarks and their characteristics?
    3.1. What is TOUCHDOWN and what does the agent do in this benchmark? sent9
    3.2. What are some photo-realistic outdoor VLN datasets and what are they based on? sent10
    3.3. How is natural language used to guide drones in VLN? sent11
        3.3.1. What is LANI and how does it use natural language instructions? sent12
        3.3.2. What is the environment for current datasets on drone navigation? sent13",How is the R2R dataset created and extended to other VLN benchmarks?,"sent4, sent5, sent6, sent7, sent8","Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017).

An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views.

R2R is extended to create other VLN benchmarks.

Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019).

Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments.","The R2R dataset is created by Anderson et al. (2018b) based on the Matterport3D simulator (Chang et al., 2017).

An embodied agent in R2R moves through a house in the simulator by traversing edges on a navigation graph and jumping to adjacent nodes containing panoramic views.

R2R is extended to create other VLN benchmarks.

Room-for-Room joins paths in R2R to create longer trajectories (Jain et al., 2019).

Other extensions include Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021), LANI (Misra et al., 2018), RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), and SOON (Zhu et al., 2021a).","Questions:

How is the R2R dataset created and extended to other VLN benchmarks?

Answer:

The R2R dataset is created by Anderson et al. (2018b) based on the Matterport3D simulator (Chang et al., 2017).

An embodied agent in R2R moves through a house in the simulator by traversing edges on a navigation graph and jumping to adjacent nodes containing panoramic views.

R2R is extended to create other VLN benchmarks.

Room-for-Room joins paths in R2R to create longer trajectories (Jain et al., 2019).

Other extensions include Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021), LANI (Misra et al., 2018), RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), and SOON (Zhu et al., 2021a)."
"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions",Initial Instruction,"In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.

Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","1. What is the initial instruction given to the agent in many VLN benchmarks?
    1.1. What is an example of a natural language instruction for the whole navigation process? sent1
        1.1.1. What is the continuation of the example instruction? sent2
    1.2. What does the agent need to do to reach the target goal? sent3

2. What is the R2R dataset and how is it created?
    2.1. Who created the R2R dataset and what is it based on? sent4
    2.2. How does an embodied agent in R2R navigate? sent5
    2.3. How is R2R extended to create other VLN benchmarks? sent6
        2.3.1. What is Roomfor-Room and how does it extend R2R? sent7
        2.3.2. What are some other VLN benchmarks extended from R2R? sent8

3. What are some specific VLN benchmarks and their characteristics?
    3.1. What is TOUCHDOWN and what does the agent do in this benchmark? sent9
    3.2. What are some photo-realistic outdoor VLN datasets and what are they based on? sent10
    3.3. How is natural language used to guide drones in VLN? sent11
        3.3.1. What is LANI and how does it use natural language instructions? sent12
        3.3.2. What is the environment for current datasets on drone navigation? sent13",What are some specific VLN benchmarks and their characteristics?,"sent9, sent10, sent11, sent12, sent13","In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object.

Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.

Some work uses natural language to guide drones.

LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions.

Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview-rendered simulation of New York City to find a hidden object.

Most photo-realistic outdoor VLN datasets, including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019; Mehta et al., 2020), StreetNav (Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021), are proposed based on Google Street View.

Some work uses natural language to guide drones.

LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions.

Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018).","Questions:

What are some specific VLN benchmarks and their characteristics?

Answer:

In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview-rendered simulation of New York City to find a hidden object.

Most photo-realistic outdoor VLN datasets, including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019; Mehta et al., 2020), StreetNav (Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021), are proposed based on Google Street View.

Some work uses natural language to guide drones.

LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions.

Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018)."
"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions",Coarse-grained Navigation,"In real life, detailed information about the route may not be available since it may be unknown to the human instructor (oracle). Usually, instructions are more concise and contain merely information of the target goal.

RoomNav (Wu et al., 2018) requires agent navigate according to instruction ""go to X"", where X is a predefined room or object.

In Embodied QA (Das et al., 2018), the agent navigates through the environment to find answer for a given question. The instructions in REVERIE (Qi et al., 2020b) are annotated by humans, and thus more complicated and diverse. The agent navigates through the rooms and differentiates the object against multiple competing candidates. In SOON (Zhu et al., 2021a), an agent receives a long, complex coarse-to-fine instruction which gradually narrows down the search scope.

Navigation+Object Interaction For some tasks, the target object might be hidden (e.g., the spoon in a drawer), or need to change status (e.g., a sliced apple is requested but only a whole apple is available). In these scenarios, it is necessary to interact with the objects to accomplish the task (e.g., opening the drawer or cutting the apple). Interactive Question Answering (IQA) requires the agent to navigate and sometimes to interact with objects to answer a given question. Based on indoor scenes in AI2-THOR (Kolve et al., 2017), Shridhar et al. (2020) propose the ALFRED dataset, where agents are provided with both coarse-grained and fine-grained instructions complete household tasks in an interactive visual environment. CHAI (Misra et al., 2018) requires the agent to navigate and simply interact with the environments.","# QUA Analysis:

1. (Why might detailed route information not be available in real life?): sent1
    1.1. (What kind of information do instructions usually contain?): sent2
        1.1.1. (What is an example of a task with concise instructions?): sent3
        1.1.2. (What is another example of a task with concise instructions?): sent4
    1.2. (What are the characteristics of instructions in REVERIE?): sent5
        1.2.1. (What does the agent do in REVERIE?): sent6
    1.3. (What kind of instructions does the agent receive in SOON?): sent7
2. (What is Navigation+Object Interaction?): sent8
    2.1. (Why is it necessary to interact with objects in some tasks?): sent9
    2.2. (What is an example of a task that requires navigation and object interaction?): sent10
    2.3. (What dataset provides both coarse-grained and fine-grained instructions for interactive tasks?): sent11
    2.4. (What does CHAI require the agent to do?): sent12",Why might detailed route information not be available in real life?,"sent1, sent2, sent3, sent4","In real life, detailed information about the route may not be available since it may be unknown to the human instructor (oracle).

Usually, instructions are more concise and contain merely information of the target goal.

RoomNav (Wu et al., 2018) requires agent navigate according to instruction ""go to X"", where X is a predefined room or object.

In Embodied QA (Das et al., 2018), the agent navigates through the environment to find answer for a given question.","Detailed information about the route may not be available in real life since it may be unknown to the human instructor (oracle).

Usually, instructions are more concise and contain merely information of the target goal.

For example, RoomNav (Wu et al., 2018) requires the agent to navigate according to the instruction ""go to X,"" where X is a predefined room or object.

In Embodied QA (Das et al., 2018), the agent navigates through the environment to find the answer to a given question.","Questions:

Why might detailed route information not be available in real life?

Answer:

Detailed information about the route may not be available in real life since it may be unknown to the human instructor (oracle).

Usually, instructions are more concise and contain merely information of the target goal.

For example, RoomNav (Wu et al., 2018) requires the agent to navigate according to the instruction ""go to X,"" where X is a predefined room or object.

In Embodied QA (Das et al., 2018), the agent navigates through the environment to find the answer to a given question."
"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions",Coarse-grained Navigation,"In real life, detailed information about the route may not be available since it may be unknown to the human instructor (oracle). Usually, instructions are more concise and contain merely information of the target goal.

RoomNav (Wu et al., 2018) requires agent navigate according to instruction ""go to X"", where X is a predefined room or object.

In Embodied QA (Das et al., 2018), the agent navigates through the environment to find answer for a given question. The instructions in REVERIE (Qi et al., 2020b) are annotated by humans, and thus more complicated and diverse. The agent navigates through the rooms and differentiates the object against multiple competing candidates. In SOON (Zhu et al., 2021a), an agent receives a long, complex coarse-to-fine instruction which gradually narrows down the search scope.

Navigation+Object Interaction For some tasks, the target object might be hidden (e.g., the spoon in a drawer), or need to change status (e.g., a sliced apple is requested but only a whole apple is available). In these scenarios, it is necessary to interact with the objects to accomplish the task (e.g., opening the drawer or cutting the apple). Interactive Question Answering (IQA) requires the agent to navigate and sometimes to interact with objects to answer a given question. Based on indoor scenes in AI2-THOR (Kolve et al., 2017), Shridhar et al. (2020) propose the ALFRED dataset, where agents are provided with both coarse-grained and fine-grained instructions complete household tasks in an interactive visual environment. CHAI (Misra et al., 2018) requires the agent to navigate and simply interact with the environments.","# QUA Analysis:

1. (Why might detailed route information not be available in real life?): sent1
    1.1. (What kind of information do instructions usually contain?): sent2
        1.1.1. (What is an example of a task with concise instructions?): sent3
        1.1.2. (What is another example of a task with concise instructions?): sent4
    1.2. (What are the characteristics of instructions in REVERIE?): sent5
        1.2.1. (What does the agent do in REVERIE?): sent6
    1.3. (What kind of instructions does the agent receive in SOON?): sent7
2. (What is Navigation+Object Interaction?): sent8
    2.1. (Why is it necessary to interact with objects in some tasks?): sent9
    2.2. (What is an example of a task that requires navigation and object interaction?): sent10
    2.3. (What dataset provides both coarse-grained and fine-grained instructions for interactive tasks?): sent11
    2.4. (What does CHAI require the agent to do?): sent12",What is an example of a task that requires navigation and object interaction?,"sent10, sent11, sent12","Interactive Question Answering (IQA) requires the agent to navigate and sometimes to interact with objects to answer a given question.

Based on indoor scenes in AI2-THOR (Kolve et al., 2017), Shridhar et al. (2020) propose the ALFRED dataset, where agents are provided with both coarse-grained and fine-grained instructions complete household tasks in an interactive visual environment.

CHAI (Misra et al., 2018) requires the agent to navigate and simply interact with the environments.","Interactive Question Answering (IQA) requires the agent to navigate and sometimes interact with objects to answer a given question.

Shridhar et al. (2020) propose the ALFRED dataset, where agents are provided with both coarse-grained and fine-grained instructions to complete household tasks in an interactive visual environment.

Another example is CHAI (Misra et al., 2018), which requires the agent to navigate and interact with the environments.","Questions:

What is an example of a task that requires navigation and object interaction?

Answer:

Interactive Question Answering (IQA) requires the agent to navigate and sometimes interact with objects to answer a given question.

Shridhar et al. (2020) propose the ALFRED dataset, where agents are provided with both coarse-grained and fine-grained instructions to complete household tasks in an interactive visual environment.

Another example is CHAI (Misra et al., 2018), which requires the agent to navigate and interact with the environments."
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,Relation Extraction Datasets Survey,"RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

In the case study of this paper, given the scientific datasets available, we focus on the first setup.","1. What is the general context of RE datasets? 
    1.1. How have RE datasets been classified in this survey? 
        1.1.1. sent1
        1.1.2. sent2
    1.2. What is provided in Table 1? 
        1.2.1. sent3
    1.3. What is the focus of the empirical target in this survey? 
        1.3.1. sent4

2. What are the notable RE datasets in the news and web domain?
    2.1. What are the early RE datasets in this domain? 
        2.1.1. sent5
        2.1.2. sent6
        2.1.3. sent7
        2.1.4. sent8
        2.1.5. sent9
    2.2. What is the New York Times (NYT) Annotated Corpus? 
        2.2.1. sent10
        2.2.2. sent11
        2.2.3. sent12
        2.2.4. sent13
    2.3. What are the SemEval shared tasks related to RE? 
        2.3.1. sent14
        2.3.2. sent15
        2.3.3. sent16
    2.4. What are the recent RE datasets in the news and web domain? 
        2.4.1. sent32
        2.4.2. sent33
        2.4.3. sent34

3. What are the notable RE datasets in the scientific domain?
    3.1. What are the SemEval shared tasks related to the scientific domain? 
        3.1.1. sent17
        3.1.2. sent18
        3.1.3. sent19
        3.1.4. sent20
    3.2. What is the SCIERC dataset? 
        3.2.1. sent21
        3.2.2. sent22
    3.3. What is the focus of the empirical investigation in this paper? 
        3.3.1. sent23
        3.3.2. sent24

4. What are the notable RE datasets in the Wikipedia domain?
    4.1. What are the early RE datasets in this domain? 
        4.1.1. sent25
        4.1.2. sent26
    4.2. What are the recent RE datasets in this domain? 
        4.2.1. sent27
        4.2.2. sent28
        4.2.3. sent29
        4.2.4. sent30
        4.2.5. sent31
    4.3. What is the DocRED dataset? 
        4.3.1. sent36
        4.3.2. sent37
        4.3.3. sent38
    4.4. What is the DWIE dataset? 
        4.4.1. sent39
    4.5. What are the datasets focusing on specific RE aspects? 
        4.5.1. sent40
        4.5.2. sent41

5. What are the observed trends and future directions in RE datasets?
    5.1. What shifts in target domains have been observed? 
        5.1.1. sent42
    5.2. What emerging trends have been observed? 
        5.2.1. sent43
    5.3. How do different datasets lend themselves to study different aspects of RE? 
        5.3.1. sent44
    5.4. What are the proposed setups for cross-domain RE? 
        5.4.1. sent45
        5.4.2. sent46
        5.4.3. sent47",What are the early RE datasets in the news and web domain?,"sent5, sent6, sent7, sent8, sent9","The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works.

It contains annotations for named entities and relations in news articles.

In the same year, the widely studied ACE dataset was published by Doddington et al. (2004).

It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic.

The corpus is divided into six domains.","The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the earliest RE datasets in the news and web domain.

It contains annotations for named entities and relations in news articles.

In the same year, the widely studied ACE dataset was published by Doddington et al. (2004).

The ACE dataset contains annotated entities, relations, and events in broadcast transcripts, newswire, and newspaper data in English, Chinese, and Arabic.

The ACE dataset is divided into six domains.","Questions:

What are the early RE datasets in the news and web domain?

Answer:

The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the earliest RE datasets in the news and web domain.

It contains annotations for named entities and relations in news articles.

In the same year, the widely studied ACE dataset was published by Doddington et al. (2004).

The ACE dataset contains annotated entities, relations, and events in broadcast transcripts, newswire, and newspaper data in English, Chinese, and Arabic.

The ACE dataset is divided into six domains."
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,Relation Extraction Datasets Survey,"RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

In the case study of this paper, given the scientific datasets available, we focus on the first setup.","1. What is the general context of RE datasets? 
    1.1. How have RE datasets been classified in this survey? 
        1.1.1. sent1
        1.1.2. sent2
    1.2. What is provided in Table 1? 
        1.2.1. sent3
    1.3. What is the focus of the empirical target in this survey? 
        1.3.1. sent4

2. What are the notable RE datasets in the news and web domain?
    2.1. What are the early RE datasets in this domain? 
        2.1.1. sent5
        2.1.2. sent6
        2.1.3. sent7
        2.1.4. sent8
        2.1.5. sent9
    2.2. What is the New York Times (NYT) Annotated Corpus? 
        2.2.1. sent10
        2.2.2. sent11
        2.2.3. sent12
        2.2.4. sent13
    2.3. What are the SemEval shared tasks related to RE? 
        2.3.1. sent14
        2.3.2. sent15
        2.3.3. sent16
    2.4. What are the recent RE datasets in the news and web domain? 
        2.4.1. sent32
        2.4.2. sent33
        2.4.3. sent34

3. What are the notable RE datasets in the scientific domain?
    3.1. What are the SemEval shared tasks related to the scientific domain? 
        3.1.1. sent17
        3.1.2. sent18
        3.1.3. sent19
        3.1.4. sent20
    3.2. What is the SCIERC dataset? 
        3.2.1. sent21
        3.2.2. sent22
    3.3. What is the focus of the empirical investigation in this paper? 
        3.3.1. sent23
        3.3.2. sent24

4. What are the notable RE datasets in the Wikipedia domain?
    4.1. What are the early RE datasets in this domain? 
        4.1.1. sent25
        4.1.2. sent26
    4.2. What are the recent RE datasets in this domain? 
        4.2.1. sent27
        4.2.2. sent28
        4.2.3. sent29
        4.2.4. sent30
        4.2.5. sent31
    4.3. What is the DocRED dataset? 
        4.3.1. sent36
        4.3.2. sent37
        4.3.3. sent38
    4.4. What is the DWIE dataset? 
        4.4.1. sent39
    4.5. What are the datasets focusing on specific RE aspects? 
        4.5.1. sent40
        4.5.2. sent41

5. What are the observed trends and future directions in RE datasets?
    5.1. What shifts in target domains have been observed? 
        5.1.1. sent42
    5.2. What emerging trends have been observed? 
        5.2.1. sent43
    5.3. How do different datasets lend themselves to study different aspects of RE? 
        5.3.1. sent44
    5.4. What are the proposed setups for cross-domain RE? 
        5.4.1. sent45
        5.4.2. sent46
        5.4.3. sent47",What is the New York Times (NYT) Annotated Corpus?,"sent10, sent11, sent12","Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010).

It contains over 1.8 million articles by the NYT between 1987 and 2007.

NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.","The New York Times (NYT) Annotated Corpus is a widely used dataset first presented by Riedel et al. (2010).

It contains over 1.8 million articles by the NYT between 1987 and 2007.

The NYT Annotated Corpus was created using a distant supervision approach (Mintz et al., 2009), leveraging Freebase (Bollacker et al., 2008) as the knowledge base. Two further versions have been released recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set for more accurate evaluation.","Questions:

What is the New York Times (NYT) Annotated Corpus?

Answer:

The New York Times (NYT) Annotated Corpus is a widely used dataset first presented by Riedel et al. (2010).

It contains over 1.8 million articles by the NYT between 1987 and 2007.

The NYT Annotated Corpus was created using a distant supervision approach (Mintz et al., 2009), leveraging Freebase (Bollacker et al., 2008) as the knowledge base. Two further versions have been released recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set for more accurate evaluation."
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,Relation Extraction Datasets Survey,"RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

In the case study of this paper, given the scientific datasets available, we focus on the first setup.","1. What is the general context of RE datasets? 
    1.1. How have RE datasets been classified in this survey? 
        1.1.1. sent1
        1.1.2. sent2
    1.2. What is provided in Table 1? 
        1.2.1. sent3
    1.3. What is the focus of the empirical target in this survey? 
        1.3.1. sent4

2. What are the notable RE datasets in the news and web domain?
    2.1. What are the early RE datasets in this domain? 
        2.1.1. sent5
        2.1.2. sent6
        2.1.3. sent7
        2.1.4. sent8
        2.1.5. sent9
    2.2. What is the New York Times (NYT) Annotated Corpus? 
        2.2.1. sent10
        2.2.2. sent11
        2.2.3. sent12
        2.2.4. sent13
    2.3. What are the SemEval shared tasks related to RE? 
        2.3.1. sent14
        2.3.2. sent15
        2.3.3. sent16
    2.4. What are the recent RE datasets in the news and web domain? 
        2.4.1. sent32
        2.4.2. sent33
        2.4.3. sent34

3. What are the notable RE datasets in the scientific domain?
    3.1. What are the SemEval shared tasks related to the scientific domain? 
        3.1.1. sent17
        3.1.2. sent18
        3.1.3. sent19
        3.1.4. sent20
    3.2. What is the SCIERC dataset? 
        3.2.1. sent21
        3.2.2. sent22
    3.3. What is the focus of the empirical investigation in this paper? 
        3.3.1. sent23
        3.3.2. sent24

4. What are the notable RE datasets in the Wikipedia domain?
    4.1. What are the early RE datasets in this domain? 
        4.1.1. sent25
        4.1.2. sent26
    4.2. What are the recent RE datasets in this domain? 
        4.2.1. sent27
        4.2.2. sent28
        4.2.3. sent29
        4.2.4. sent30
        4.2.5. sent31
    4.3. What is the DocRED dataset? 
        4.3.1. sent36
        4.3.2. sent37
        4.3.3. sent38
    4.4. What is the DWIE dataset? 
        4.4.1. sent39
    4.5. What are the datasets focusing on specific RE aspects? 
        4.5.1. sent40
        4.5.2. sent41

5. What are the observed trends and future directions in RE datasets?
    5.1. What shifts in target domains have been observed? 
        5.1.1. sent42
    5.2. What emerging trends have been observed? 
        5.2.1. sent43
    5.3. How do different datasets lend themselves to study different aspects of RE? 
        5.3.1. sent44
    5.4. What are the proposed setups for cross-domain RE? 
        5.4.1. sent45
        5.4.2. sent46
        5.4.3. sent47",What are the SemEval shared tasks related to RE?,"sent13, sent14, sent15, sent16","3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far.

The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021).

For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals.

Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks.","Relation Extraction (RE) has been part of the SemEval shared tasks four times so far.

The two early SemEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021).

For SemEval-2007 Task 4, Girju et al. (2007) released a dataset for relation classification into seven generic semantic relations between nominals.

Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a much larger dataset for relation classification, consisting of 10,000 instances compared to the 1,500 instances in the 2007 shared task.","Questions:

What are the SemEval shared tasks related to RE?

Answer:

Relation Extraction (RE) has been part of the SemEval shared tasks four times so far.

The two early SemEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021).

For SemEval-2007 Task 4, Girju et al. (2007) released a dataset for relation classification into seven generic semantic relations between nominals.

Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a much larger dataset for relation classification, consisting of 10,000 instances compared to the 1,500 instances in the 2007 shared task."
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,Relation Extraction Datasets Survey,"RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

In the case study of this paper, given the scientific datasets available, we focus on the first setup.","1. What is the general context of RE datasets? 
    1.1. How have RE datasets been classified in this survey? 
        1.1.1. sent1
        1.1.2. sent2
    1.2. What is provided in Table 1? 
        1.2.1. sent3
    1.3. What is the focus of the empirical target in this survey? 
        1.3.1. sent4

2. What are the notable RE datasets in the news and web domain?
    2.1. What are the early RE datasets in this domain? 
        2.1.1. sent5
        2.1.2. sent6
        2.1.3. sent7
        2.1.4. sent8
        2.1.5. sent9
    2.2. What is the New York Times (NYT) Annotated Corpus? 
        2.2.1. sent10
        2.2.2. sent11
        2.2.3. sent12
        2.2.4. sent13
    2.3. What are the SemEval shared tasks related to RE? 
        2.3.1. sent14
        2.3.2. sent15
        2.3.3. sent16
    2.4. What are the recent RE datasets in the news and web domain? 
        2.4.1. sent32
        2.4.2. sent33
        2.4.3. sent34

3. What are the notable RE datasets in the scientific domain?
    3.1. What are the SemEval shared tasks related to the scientific domain? 
        3.1.1. sent17
        3.1.2. sent18
        3.1.3. sent19
        3.1.4. sent20
    3.2. What is the SCIERC dataset? 
        3.2.1. sent21
        3.2.2. sent22
    3.3. What is the focus of the empirical investigation in this paper? 
        3.3.1. sent23
        3.3.2. sent24

4. What are the notable RE datasets in the Wikipedia domain?
    4.1. What are the early RE datasets in this domain? 
        4.1.1. sent25
        4.1.2. sent26
    4.2. What are the recent RE datasets in this domain? 
        4.2.1. sent27
        4.2.2. sent28
        4.2.3. sent29
        4.2.4. sent30
        4.2.5. sent31
    4.3. What is the DocRED dataset? 
        4.3.1. sent36
        4.3.2. sent37
        4.3.3. sent38
    4.4. What is the DWIE dataset? 
        4.4.1. sent39
    4.5. What are the datasets focusing on specific RE aspects? 
        4.5.1. sent40
        4.5.2. sent41

5. What are the observed trends and future directions in RE datasets?
    5.1. What shifts in target domains have been observed? 
        5.1.1. sent42
    5.2. What emerging trends have been observed? 
        5.2.1. sent43
    5.3. How do different datasets lend themselves to study different aspects of RE? 
        5.3.1. sent44
    5.4. What are the proposed setups for cross-domain RE? 
        5.4.1. sent45
        5.4.2. sent46
        5.4.3. sent47",What are the recent RE datasets in the news and web domain?,"sent32, sent33, sent34","Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs.

This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models.

Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).","Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text by crowdsourcing relation annotations for sentences with named entity pairs.

This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models.

Sabo et al. (2021) used TACRED to create a few-shot learning relation classification (FSL RC) dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario with non-uniform label distribution, inclusion of pronouns, and common nouns.","Questions:

What are the recent RE datasets in the news and web domain?

Answer:

Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text by crowdsourcing relation annotations for sentences with named entity pairs.

This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models.

Sabo et al. (2021) used TACRED to create a few-shot learning relation classification (FSL RC) dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario with non-uniform label distribution, inclusion of pronouns, and common nouns."
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,Relation Extraction Datasets Survey,"RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

In the case study of this paper, given the scientific datasets available, we focus on the first setup.","1. What is the general context of RE datasets? 
    1.1. How have RE datasets been classified in this survey? 
        1.1.1. sent1
        1.1.2. sent2
    1.2. What is provided in Table 1? 
        1.2.1. sent3
    1.3. What is the focus of the empirical target in this survey? 
        1.3.1. sent4

2. What are the notable RE datasets in the news and web domain?
    2.1. What are the early RE datasets in this domain? 
        2.1.1. sent5
        2.1.2. sent6
        2.1.3. sent7
        2.1.4. sent8
        2.1.5. sent9
    2.2. What is the New York Times (NYT) Annotated Corpus? 
        2.2.1. sent10
        2.2.2. sent11
        2.2.3. sent12
        2.2.4. sent13
    2.3. What are the SemEval shared tasks related to RE? 
        2.3.1. sent14
        2.3.2. sent15
        2.3.3. sent16
    2.4. What are the recent RE datasets in the news and web domain? 
        2.4.1. sent32
        2.4.2. sent33
        2.4.3. sent34

3. What are the notable RE datasets in the scientific domain?
    3.1. What are the SemEval shared tasks related to the scientific domain? 
        3.1.1. sent17
        3.1.2. sent18
        3.1.3. sent19
        3.1.4. sent20
    3.2. What is the SCIERC dataset? 
        3.2.1. sent21
        3.2.2. sent22
    3.3. What is the focus of the empirical investigation in this paper? 
        3.3.1. sent23
        3.3.2. sent24

4. What are the notable RE datasets in the Wikipedia domain?
    4.1. What are the early RE datasets in this domain? 
        4.1.1. sent25
        4.1.2. sent26
    4.2. What are the recent RE datasets in this domain? 
        4.2.1. sent27
        4.2.2. sent28
        4.2.3. sent29
        4.2.4. sent30
        4.2.5. sent31
    4.3. What is the DocRED dataset? 
        4.3.1. sent36
        4.3.2. sent37
        4.3.3. sent38
    4.4. What is the DWIE dataset? 
        4.4.1. sent39
    4.5. What are the datasets focusing on specific RE aspects? 
        4.5.1. sent40
        4.5.2. sent41

5. What are the observed trends and future directions in RE datasets?
    5.1. What shifts in target domains have been observed? 
        5.1.1. sent42
    5.2. What emerging trends have been observed? 
        5.2.1. sent43
    5.3. How do different datasets lend themselves to study different aspects of RE? 
        5.3.1. sent44
    5.4. What are the proposed setups for cross-domain RE? 
        5.4.1. sent45
        5.4.2. sent46
        5.4.3. sent47",What are the SemEval shared tasks related to the scientific domain?,"sent17, sent18, sent19, sent20","In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF).

The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields.

The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7.

The data will be described in further detail in Section 4.1.","In SemEval-2017 Task 10, Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF).

The dataset, called ScienceIE, consists of 500 journal articles from the fields of Computer Science, Material Sciences, and Physics.

The year after, Gábor et al. (2018) proposed a corpus for relation classification (RC) and relation extraction (RE) made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7.

The data for this task consists of abstracts of scientific papers from the ACL Anthology.","Questions:

What are the SemEval shared tasks related to the scientific domain?

Answer:

In SemEval-2017 Task 10, Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF).

The dataset, called ScienceIE, consists of 500 journal articles from the fields of Computer Science, Material Sciences, and Physics.

The year after, Gábor et al. (2018) proposed a corpus for relation classification (RC) and relation extraction (RE) made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7.

The data for this task consists of abstracts of scientific papers from the ACL Anthology."
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,Relation Extraction Datasets Survey,"RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

In the case study of this paper, given the scientific datasets available, we focus on the first setup.","1. What is the general context of RE datasets? 
    1.1. How have RE datasets been classified in this survey? 
        1.1.1. sent1
        1.1.2. sent2
    1.2. What is provided in Table 1? 
        1.2.1. sent3
    1.3. What is the focus of the empirical target in this survey? 
        1.3.1. sent4

2. What are the notable RE datasets in the news and web domain?
    2.1. What are the early RE datasets in this domain? 
        2.1.1. sent5
        2.1.2. sent6
        2.1.3. sent7
        2.1.4. sent8
        2.1.5. sent9
    2.2. What is the New York Times (NYT) Annotated Corpus? 
        2.2.1. sent10
        2.2.2. sent11
        2.2.3. sent12
        2.2.4. sent13
    2.3. What are the SemEval shared tasks related to RE? 
        2.3.1. sent14
        2.3.2. sent15
        2.3.3. sent16
    2.4. What are the recent RE datasets in the news and web domain? 
        2.4.1. sent32
        2.4.2. sent33
        2.4.3. sent34

3. What are the notable RE datasets in the scientific domain?
    3.1. What are the SemEval shared tasks related to the scientific domain? 
        3.1.1. sent17
        3.1.2. sent18
        3.1.3. sent19
        3.1.4. sent20
    3.2. What is the SCIERC dataset? 
        3.2.1. sent21
        3.2.2. sent22
    3.3. What is the focus of the empirical investigation in this paper? 
        3.3.1. sent23
        3.3.2. sent24

4. What are the notable RE datasets in the Wikipedia domain?
    4.1. What are the early RE datasets in this domain? 
        4.1.1. sent25
        4.1.2. sent26
    4.2. What are the recent RE datasets in this domain? 
        4.2.1. sent27
        4.2.2. sent28
        4.2.3. sent29
        4.2.4. sent30
        4.2.5. sent31
    4.3. What is the DocRED dataset? 
        4.3.1. sent36
        4.3.2. sent37
        4.3.3. sent38
    4.4. What is the DWIE dataset? 
        4.4.1. sent39
    4.5. What are the datasets focusing on specific RE aspects? 
        4.5.1. sent40
        4.5.2. sent41

5. What are the observed trends and future directions in RE datasets?
    5.1. What shifts in target domains have been observed? 
        5.1.1. sent42
    5.2. What emerging trends have been observed? 
        5.2.1. sent43
    5.3. How do different datasets lend themselves to study different aspects of RE? 
        5.3.1. sent44
    5.4. What are the proposed setups for cross-domain RE? 
        5.4.1. sent45
        5.4.2. sent46
        5.4.3. sent47",What are the recent RE datasets in the Wikipedia domain?,"sent27, sent28, sent29, sent30, sent31","More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models.

The multi-lingual dimension is gaining more interest for RE.

Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

Previous datasets were restricted to the same label collection in the training set and in the test set.

To address this gap and make RE experimental scenarios more realistic,","More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE, to investigate knowledge in pretrained language models.

The multi-lingual dimension is gaining more interest for RE.

Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

Previous datasets were restricted to the same label collection in the training set and in the test set.

To address this gap and make RE experimental scenarios more realistic,","Questions:

What are the recent RE datasets in the Wikipedia domain?

Answer:

More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE, to investigate knowledge in pretrained language models.

The multi-lingual dimension is gaining more interest for RE.

Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

Previous datasets were restricted to the same label collection in the training set and in the test set.

To address this gap and make RE experimental scenarios more realistic,"
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,Relation Extraction Datasets Survey,"RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

In the case study of this paper, given the scientific datasets available, we focus on the first setup.","1. What is the general context of RE datasets? 
    1.1. How have RE datasets been classified in this survey? 
        1.1.1. sent1
        1.1.2. sent2
    1.2. What is provided in Table 1? 
        1.2.1. sent3
    1.3. What is the focus of the empirical target in this survey? 
        1.3.1. sent4

2. What are the notable RE datasets in the news and web domain?
    2.1. What are the early RE datasets in this domain? 
        2.1.1. sent5
        2.1.2. sent6
        2.1.3. sent7
        2.1.4. sent8
        2.1.5. sent9
    2.2. What is the New York Times (NYT) Annotated Corpus? 
        2.2.1. sent10
        2.2.2. sent11
        2.2.3. sent12
        2.2.4. sent13
    2.3. What are the SemEval shared tasks related to RE? 
        2.3.1. sent14
        2.3.2. sent15
        2.3.3. sent16
    2.4. What are the recent RE datasets in the news and web domain? 
        2.4.1. sent32
        2.4.2. sent33
        2.4.3. sent34

3. What are the notable RE datasets in the scientific domain?
    3.1. What are the SemEval shared tasks related to the scientific domain? 
        3.1.1. sent17
        3.1.2. sent18
        3.1.3. sent19
        3.1.4. sent20
    3.2. What is the SCIERC dataset? 
        3.2.1. sent21
        3.2.2. sent22
    3.3. What is the focus of the empirical investigation in this paper? 
        3.3.1. sent23
        3.3.2. sent24

4. What are the notable RE datasets in the Wikipedia domain?
    4.1. What are the early RE datasets in this domain? 
        4.1.1. sent25
        4.1.2. sent26
    4.2. What are the recent RE datasets in this domain? 
        4.2.1. sent27
        4.2.2. sent28
        4.2.3. sent29
        4.2.4. sent30
        4.2.5. sent31
    4.3. What is the DocRED dataset? 
        4.3.1. sent36
        4.3.2. sent37
        4.3.3. sent38
    4.4. What is the DWIE dataset? 
        4.4.1. sent39
    4.5. What are the datasets focusing on specific RE aspects? 
        4.5.1. sent40
        4.5.2. sent41

5. What are the observed trends and future directions in RE datasets?
    5.1. What shifts in target domains have been observed? 
        5.1.1. sent42
    5.2. What emerging trends have been observed? 
        5.2.1. sent43
    5.3. How do different datasets lend themselves to study different aspects of RE? 
        5.3.1. sent44
    5.4. What are the proposed setups for cross-domain RE? 
        5.4.1. sent45
        5.4.2. sent46
        5.4.3. sent47",What is the DocRED dataset?,"sent36, sent37, sent38","To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata.

The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level.

In addition to RE, DocRED annotates coreference chains.","DocRED is a document-level RE dataset from Wikipedia and Wikidata published by Yao et al. (2019).

The difference with a traditional sentence-level corpus is that both the intra- and inter-sentence relations are annotated, increasing the challenge level.

In addition to RE, DocRED annotates coreference chains.","Questions:

What is the DocRED dataset?

Answer:

DocRED is a document-level RE dataset from Wikipedia and Wikidata published by Yao et al. (2019).

The difference with a traditional sentence-level corpus is that both the intra- and inter-sentence relations are annotated, increasing the challenge level.

In addition to RE, DocRED annotates coreference chains."
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,Relation Extraction Datasets Survey,"RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

In the case study of this paper, given the scientific datasets available, we focus on the first setup.","1. What is the general context of RE datasets? 
    1.1. How have RE datasets been classified in this survey? 
        1.1.1. sent1
        1.1.2. sent2
    1.2. What is provided in Table 1? 
        1.2.1. sent3
    1.3. What is the focus of the empirical target in this survey? 
        1.3.1. sent4

2. What are the notable RE datasets in the news and web domain?
    2.1. What are the early RE datasets in this domain? 
        2.1.1. sent5
        2.1.2. sent6
        2.1.3. sent7
        2.1.4. sent8
        2.1.5. sent9
    2.2. What is the New York Times (NYT) Annotated Corpus? 
        2.2.1. sent10
        2.2.2. sent11
        2.2.3. sent12
        2.2.4. sent13
    2.3. What are the SemEval shared tasks related to RE? 
        2.3.1. sent14
        2.3.2. sent15
        2.3.3. sent16
    2.4. What are the recent RE datasets in the news and web domain? 
        2.4.1. sent32
        2.4.2. sent33
        2.4.3. sent34

3. What are the notable RE datasets in the scientific domain?
    3.1. What are the SemEval shared tasks related to the scientific domain? 
        3.1.1. sent17
        3.1.2. sent18
        3.1.3. sent19
        3.1.4. sent20
    3.2. What is the SCIERC dataset? 
        3.2.1. sent21
        3.2.2. sent22
    3.3. What is the focus of the empirical investigation in this paper? 
        3.3.1. sent23
        3.3.2. sent24

4. What are the notable RE datasets in the Wikipedia domain?
    4.1. What are the early RE datasets in this domain? 
        4.1.1. sent25
        4.1.2. sent26
    4.2. What are the recent RE datasets in this domain? 
        4.2.1. sent27
        4.2.2. sent28
        4.2.3. sent29
        4.2.4. sent30
        4.2.5. sent31
    4.3. What is the DocRED dataset? 
        4.3.1. sent36
        4.3.2. sent37
        4.3.3. sent38
    4.4. What is the DWIE dataset? 
        4.4.1. sent39
    4.5. What are the datasets focusing on specific RE aspects? 
        4.5.1. sent40
        4.5.2. sent41

5. What are the observed trends and future directions in RE datasets?
    5.1. What shifts in target domains have been observed? 
        5.1.1. sent42
    5.2. What emerging trends have been observed? 
        5.2.1. sent43
    5.3. How do different datasets lend themselves to study different aspects of RE? 
        5.3.1. sent44
    5.4. What are the proposed setups for cross-domain RE? 
        5.4.1. sent45
        5.4.2. sent46
        5.4.3. sent47",What are the proposed setups for cross-domain RE?,"sent45, sent46, sent47","Concerning crossdomain RE, we propose to distinguish three setups:1.

Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

In the case study of this paper, given the scientific datasets available, we focus on the first setup.","For cross-domain RE, three setups are proposed.

One setup involves using data from different domains but with the same relation types, which are general enough to be present in each domain (e.g., Plank and Moschitti, 2013).

Given the scientific datasets available, the focus is on the first setup.","Questions:

What are the proposed setups for cross-domain RE?

Answer:

For cross-domain RE, three setups are proposed.

One setup involves using data from different domains but with the same relation types, which are general enough to be present in each domain (e.g., Plank and Moschitti, 2013).

Given the scientific datasets available, the focus is on the first setup."
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,The Relation Extraction Task,"Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","# QUA Analysis:

1. (What does the RE task involve?): sent1
    1.1. (What are the steps in the RE pipeline?): sent2
        1.1.1. (What is the first step in the RE pipeline?): sent2
        1.1.2. (What does the first step involve?): sent3
    1.2. (What happens after entities are identified?): sent4
        1.2.1. (What are the different approaches to RE after entity identification?): sent4
        1.2.2. (What is one way to approach RE?): sent5
            1.2.2.1. (What does the two-step approach involve?): sent6
                1.2.2.1.1. (What is the challenge with the RI step?): sent7
            1.2.2.2. (What is the last step in the two-step approach?): sent8
        1.2.3. (What are alternative approaches to the two-step method?): sent9, sent10
            1.2.3.1. (How do some studies merge RI and RC?): sent9
            1.2.3.2. (How do other studies simplify the task?): sent10
        1.2.4. (How does RI relate to RC?): sent11
            1.2.4.1. (How do some studies address the approximation in RI?): sent12
                1.2.4.1.1. (What is the difference between NOTA and no-rel?): sent13
2. (How comprehensive are RE studies in addressing the pipeline?): sent15
    2.1. (What sub-steps are involved in entity extraction?): sent16
    2.2. (What is the focus of the analysis in the paper?): sent17
        2.2.1. (What does Table 2 show?): sent18
        2.2.2. (What is excluded from the analysis?): sent19
    2.3. (What are the findings of the analysis?): sent20
        2.3.1. (What is assumed for RE?): sent20
        2.3.2. (What is lacking in end-to-end models?): sent21
        2.3.3. (What is difficult to determine in the RI setup?): sent22
            2.3.3.1. (What is always performed?): sent23
            2.3.3.2. (What is the situation for RI?): sent23
                2.3.3.2.1. (When is RI clearly not done?): sent24
                2.3.3.2.2. (What is the common scenario for RI?): sent24
        2.3.4. (What is proposed to address the blurriness in evaluation?): sent25
            2.3.4.1. (Why are these details important?): sent26
3. (What are the challenges with the traditional RE pipeline?): sent27
    3.1. (What approaches have been proposed to alleviate error propagation?): sent28
        3.1.1. (What is the challenge with evaluating joint models?): sent29
            3.1.1.1. (What did Taillé et al. (2020) discuss?): sent29
            3.1.1.2. (What did they survey?): sent30
            3.1.1.3. (What did they observe and claim?): sent31
    3.2. (What is the current state of joint vs. pipeline models?): sent32
        3.2.1. (What did Zhong and Chen (2021) find?): sent33
            3.2.1.1. (Why might separate encoders be better?): sent34
        3.2.2. (Is one approach more suitable than the other for RE?): sent35
4. (What approach does the paper follow and why?): sent36",What are the steps involved in the Relation Extraction (RE) pipeline?,"sent1, sent2, sent3, sent4, sent5, sent6, sent7, sent8, sent9, sent10, sent11, sent12, sent13","Conceptually, RE involves a pipeline of steps (see Figure 2).

Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.

Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).

5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.

Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.

Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).

Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.

Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).

Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set.","Conceptually, RE involves a pipeline of steps.

The first step involves identifying the entities in the raw text and assigning them a type.

Entities can be either nominals or named entities, which means the process involves either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).

After entities are identified, approaches start to diverge as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC).

This involves first identifying from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations, such as excluding entity pairs involving distant entities or entity type pairs not licensed by the relations.

The last step, Relation Classification (RC), is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.

Some studies merge RI and RC (Seganti et al., 2021) into one step by adding a no-relation (no-rel) label.

Other studies instead reduce the task to Relation Classification (RC), assuming there exists a relation between two entities and the task is to determine the type (without a no-relation label).

Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.

Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).

Note that, in this context, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set.","Questions:

What are the steps involved in the Relation Extraction (RE) pipeline?

Answer:

Conceptually, RE involves a pipeline of steps.

The first step involves identifying the entities in the raw text and assigning them a type.

Entities can be either nominals or named entities, which means the process involves either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).

After entities are identified, approaches start to diverge as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC).

This involves first identifying from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations, such as excluding entity pairs involving distant entities or entity type pairs not licensed by the relations.

The last step, Relation Classification (RC), is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.

Some studies merge RI and RC (Seganti et al., 2021) into one step by adding a no-relation (no-rel) label.

Other studies instead reduce the task to Relation Classification (RC), assuming there exists a relation between two entities and the task is to determine the type (without a no-relation label).

Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.

Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).

Note that, in this context, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set."
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,The Relation Extraction Task,"Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","# QUA Analysis:

1. (What does the RE task involve?): sent1
    1.1. (What are the steps in the RE pipeline?): sent2
        1.1.1. (What is the first step in the RE pipeline?): sent2
        1.1.2. (What does the first step involve?): sent3
    1.2. (What happens after entities are identified?): sent4
        1.2.1. (What are the different approaches to RE after entity identification?): sent4
        1.2.2. (What is one way to approach RE?): sent5
            1.2.2.1. (What does the two-step approach involve?): sent6
                1.2.2.1.1. (What is the challenge with the RI step?): sent7
            1.2.2.2. (What is the last step in the two-step approach?): sent8
        1.2.3. (What are alternative approaches to the two-step method?): sent9, sent10
            1.2.3.1. (How do some studies merge RI and RC?): sent9
            1.2.3.2. (How do other studies simplify the task?): sent10
        1.2.4. (How does RI relate to RC?): sent11
            1.2.4.1. (How do some studies address the approximation in RI?): sent12
                1.2.4.1.1. (What is the difference between NOTA and no-rel?): sent13
2. (How comprehensive are RE studies in addressing the pipeline?): sent15
    2.1. (What sub-steps are involved in entity extraction?): sent16
    2.2. (What is the focus of the analysis in the paper?): sent17
        2.2.1. (What does Table 2 show?): sent18
        2.2.2. (What is excluded from the analysis?): sent19
    2.3. (What are the findings of the analysis?): sent20
        2.3.1. (What is assumed for RE?): sent20
        2.3.2. (What is lacking in end-to-end models?): sent21
        2.3.3. (What is difficult to determine in the RI setup?): sent22
            2.3.3.1. (What is always performed?): sent23
            2.3.3.2. (What is the situation for RI?): sent23
                2.3.3.2.1. (When is RI clearly not done?): sent24
                2.3.3.2.2. (What is the common scenario for RI?): sent24
        2.3.4. (What is proposed to address the blurriness in evaluation?): sent25
            2.3.4.1. (Why are these details important?): sent26
3. (What are the challenges with the traditional RE pipeline?): sent27
    3.1. (What approaches have been proposed to alleviate error propagation?): sent28
        3.1.1. (What is the challenge with evaluating joint models?): sent29
            3.1.1.1. (What did Taillé et al. (2020) discuss?): sent29
            3.1.1.2. (What did they survey?): sent30
            3.1.1.3. (What did they observe and claim?): sent31
    3.2. (What is the current state of joint vs. pipeline models?): sent32
        3.2.1. (What did Zhong and Chen (2021) find?): sent33
            3.2.1.1. (Why might separate encoders be better?): sent34
        3.2.2. (Is one approach more suitable than the other for RE?): sent35
4. (What approach does the paper follow and why?): sent36",What are the different approaches to RE after entity identification?,"sent4, sent5, sent9, sent10","5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label.

Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label).","After entities are identified, approaches to relation extraction (RE) become more varied as studies have approached RE from different angles.

One approach is to take two steps: Relation Identification (RI) and subsequent Relation Classification (RC).

Some studies merge Relation Identification (RI) and Relation Classification (RC) into one step by adding a no-relation (no-rel) label (Seganti et al., 2021).

Other studies instead reduce the task to Relation Classification (RC), assuming there exists a relation between two entities and the task is to determine the type (without a no-rel label).","Questions:

What are the different approaches to RE after entity identification?

Answer:

After entities are identified, approaches to relation extraction (RE) become more varied as studies have approached RE from different angles.

One approach is to take two steps: Relation Identification (RI) and subsequent Relation Classification (RC).

Some studies merge Relation Identification (RI) and Relation Classification (RC) into one step by adding a no-relation (no-rel) label (Seganti et al., 2021).

Other studies instead reduce the task to Relation Classification (RC), assuming there exists a relation between two entities and the task is to determine the type (without a no-rel label)."
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,The Relation Extraction Task,"Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","# QUA Analysis:

1. (What does the RE task involve?): sent1
    1.1. (What are the steps in the RE pipeline?): sent2
        1.1.1. (What is the first step in the RE pipeline?): sent2
        1.1.2. (What does the first step involve?): sent3
    1.2. (What happens after entities are identified?): sent4
        1.2.1. (What are the different approaches to RE after entity identification?): sent4
        1.2.2. (What is one way to approach RE?): sent5
            1.2.2.1. (What does the two-step approach involve?): sent6
                1.2.2.1.1. (What is the challenge with the RI step?): sent7
            1.2.2.2. (What is the last step in the two-step approach?): sent8
        1.2.3. (What are alternative approaches to the two-step method?): sent9, sent10
            1.2.3.1. (How do some studies merge RI and RC?): sent9
            1.2.3.2. (How do other studies simplify the task?): sent10
        1.2.4. (How does RI relate to RC?): sent11
            1.2.4.1. (How do some studies address the approximation in RI?): sent12
                1.2.4.1.1. (What is the difference between NOTA and no-rel?): sent13
2. (How comprehensive are RE studies in addressing the pipeline?): sent15
    2.1. (What sub-steps are involved in entity extraction?): sent16
    2.2. (What is the focus of the analysis in the paper?): sent17
        2.2.1. (What does Table 2 show?): sent18
        2.2.2. (What is excluded from the analysis?): sent19
    2.3. (What are the findings of the analysis?): sent20
        2.3.1. (What is assumed for RE?): sent20
        2.3.2. (What is lacking in end-to-end models?): sent21
        2.3.3. (What is difficult to determine in the RI setup?): sent22
            2.3.3.1. (What is always performed?): sent23
            2.3.3.2. (What is the situation for RI?): sent23
                2.3.3.2.1. (When is RI clearly not done?): sent24
                2.3.3.2.2. (What is the common scenario for RI?): sent24
        2.3.4. (What is proposed to address the blurriness in evaluation?): sent25
            2.3.4.1. (Why are these details important?): sent26
3. (What are the challenges with the traditional RE pipeline?): sent27
    3.1. (What approaches have been proposed to alleviate error propagation?): sent28
        3.1.1. (What is the challenge with evaluating joint models?): sent29
            3.1.1.1. (What did Taillé et al. (2020) discuss?): sent29
            3.1.1.2. (What did they survey?): sent30
            3.1.1.3. (What did they observe and claim?): sent31
    3.2. (What is the current state of joint vs. pipeline models?): sent32
        3.2.1. (What did Zhong and Chen (2021) find?): sent33
            3.2.1.1. (Why might separate encoders be better?): sent34
        3.2.2. (Is one approach more suitable than the other for RE?): sent35
4. (What approach does the paper follow and why?): sent36",How does the two-step approach to RE work?,"sent5, sent6, sent7, sent8","One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.

This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).

As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered).

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.","One way the two-step approach to relation extraction (RE) works is by taking two steps: Relation Identification (RI) and subsequent Relation Classification (RC).

This involves first identifying from all the possible entity pairs the ones that are in some kind of relation via a binary classification task (RI).

As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations, such as excluding entity pairs involving distant entities or entity type pairs not licensed by the relations.

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.","Questions:

How does the two-step approach to RE work?

Answer:

One way the two-step approach to relation extraction (RE) works is by taking two steps: Relation Identification (RI) and subsequent Relation Classification (RC).

This involves first identifying from all the possible entity pairs the ones that are in some kind of relation via a binary classification task (RI).

As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations, such as excluding entity pairs involving distant entities or entity type pairs not licensed by the relations.

The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step."
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,The Relation Extraction Task,"Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","# QUA Analysis:

1. (What does the RE task involve?): sent1
    1.1. (What are the steps in the RE pipeline?): sent2
        1.1.1. (What is the first step in the RE pipeline?): sent2
        1.1.2. (What does the first step involve?): sent3
    1.2. (What happens after entities are identified?): sent4
        1.2.1. (What are the different approaches to RE after entity identification?): sent4
        1.2.2. (What is one way to approach RE?): sent5
            1.2.2.1. (What does the two-step approach involve?): sent6
                1.2.2.1.1. (What is the challenge with the RI step?): sent7
            1.2.2.2. (What is the last step in the two-step approach?): sent8
        1.2.3. (What are alternative approaches to the two-step method?): sent9, sent10
            1.2.3.1. (How do some studies merge RI and RC?): sent9
            1.2.3.2. (How do other studies simplify the task?): sent10
        1.2.4. (How does RI relate to RC?): sent11
            1.2.4.1. (How do some studies address the approximation in RI?): sent12
                1.2.4.1.1. (What is the difference between NOTA and no-rel?): sent13
2. (How comprehensive are RE studies in addressing the pipeline?): sent15
    2.1. (What sub-steps are involved in entity extraction?): sent16
    2.2. (What is the focus of the analysis in the paper?): sent17
        2.2.1. (What does Table 2 show?): sent18
        2.2.2. (What is excluded from the analysis?): sent19
    2.3. (What are the findings of the analysis?): sent20
        2.3.1. (What is assumed for RE?): sent20
        2.3.2. (What is lacking in end-to-end models?): sent21
        2.3.3. (What is difficult to determine in the RI setup?): sent22
            2.3.3.1. (What is always performed?): sent23
            2.3.3.2. (What is the situation for RI?): sent23
                2.3.3.2.1. (When is RI clearly not done?): sent24
                2.3.3.2.2. (What is the common scenario for RI?): sent24
        2.3.4. (What is proposed to address the blurriness in evaluation?): sent25
            2.3.4.1. (Why are these details important?): sent26
3. (What are the challenges with the traditional RE pipeline?): sent27
    3.1. (What approaches have been proposed to alleviate error propagation?): sent28
        3.1.1. (What is the challenge with evaluating joint models?): sent29
            3.1.1.1. (What did Taillé et al. (2020) discuss?): sent29
            3.1.1.2. (What did they survey?): sent30
            3.1.1.3. (What did they observe and claim?): sent31
    3.2. (What is the current state of joint vs. pipeline models?): sent32
        3.2.1. (What did Zhong and Chen (2021) find?): sent33
            3.2.1.1. (Why might separate encoders be better?): sent34
        3.2.2. (Is one approach more suitable than the other for RE?): sent35
4. (What approach does the paper follow and why?): sent36",How does Relation Identification (RI) relate to Relation Classification (RC)?,"sent11, sent12, sent13","Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.

Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).

Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set.","Relation Identification (RI) is influenced by the Relation Classification (RC) setup, as relations not in the RC label set are considered negative samples in the RI phase.

Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).

In this context, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set.","Questions:

How does Relation Identification (RI) relate to Relation Classification (RC)?

Answer:

Relation Identification (RI) is influenced by the Relation Classification (RC) setup, as relations not in the RC label set are considered negative samples in the RI phase.

Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019).

In this context, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set."
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,The Relation Extraction Task,"Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","# QUA Analysis:

1. (What does the RE task involve?): sent1
    1.1. (What are the steps in the RE pipeline?): sent2
        1.1.1. (What is the first step in the RE pipeline?): sent2
        1.1.2. (What does the first step involve?): sent3
    1.2. (What happens after entities are identified?): sent4
        1.2.1. (What are the different approaches to RE after entity identification?): sent4
        1.2.2. (What is one way to approach RE?): sent5
            1.2.2.1. (What does the two-step approach involve?): sent6
                1.2.2.1.1. (What is the challenge with the RI step?): sent7
            1.2.2.2. (What is the last step in the two-step approach?): sent8
        1.2.3. (What are alternative approaches to the two-step method?): sent9, sent10
            1.2.3.1. (How do some studies merge RI and RC?): sent9
            1.2.3.2. (How do other studies simplify the task?): sent10
        1.2.4. (How does RI relate to RC?): sent11
            1.2.4.1. (How do some studies address the approximation in RI?): sent12
                1.2.4.1.1. (What is the difference between NOTA and no-rel?): sent13
2. (How comprehensive are RE studies in addressing the pipeline?): sent15
    2.1. (What sub-steps are involved in entity extraction?): sent16
    2.2. (What is the focus of the analysis in the paper?): sent17
        2.2.1. (What does Table 2 show?): sent18
        2.2.2. (What is excluded from the analysis?): sent19
    2.3. (What are the findings of the analysis?): sent20
        2.3.1. (What is assumed for RE?): sent20
        2.3.2. (What is lacking in end-to-end models?): sent21
        2.3.3. (What is difficult to determine in the RI setup?): sent22
            2.3.3.1. (What is always performed?): sent23
            2.3.3.2. (What is the situation for RI?): sent23
                2.3.3.2.1. (When is RI clearly not done?): sent24
                2.3.3.2.2. (What is the common scenario for RI?): sent24
        2.3.4. (What is proposed to address the blurriness in evaluation?): sent25
            2.3.4.1. (Why are these details important?): sent26
3. (What are the challenges with the traditional RE pipeline?): sent27
    3.1. (What approaches have been proposed to alleviate error propagation?): sent28
        3.1.1. (What is the challenge with evaluating joint models?): sent29
            3.1.1.1. (What did Taillé et al. (2020) discuss?): sent29
            3.1.1.2. (What did they survey?): sent30
            3.1.1.3. (What did they observe and claim?): sent31
    3.2. (What is the current state of joint vs. pipeline models?): sent32
        3.2.1. (What did Zhong and Chen (2021) find?): sent33
            3.2.1.1. (Why might separate encoders be better?): sent34
        3.2.2. (Is one approach more suitable than the other for RE?): sent35
4. (What approach does the paper follow and why?): sent36",What is the focus of the analysis in the paper regarding RE studies?,"sent17, sent18, sent19","6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC).

Table 2 shows such investigation.

We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE.","The analysis in the paper focuses on all the ACL papers published in the last five years that contain the Relation Extraction keyword in the title and determines which sub-task is performed (NER/MD, RI, RC) (Hendrickx et al., 2010).

The investigation is detailed in the analysis.

The analysis excludes papers that make use of distant supervision or involve knowledge bases, shared task papers, the bioNLP field, temporal RE, and Open RE.","Questions:

What is the focus of the analysis in the paper regarding RE studies?

Answer:

The analysis in the paper focuses on all the ACL papers published in the last five years that contain the Relation Extraction keyword in the title and determines which sub-task is performed (NER/MD, RI, RC) (Hendrickx et al., 2010).

The investigation is detailed in the analysis.

The analysis excludes papers that make use of distant supervision or involve knowledge bases, shared task papers, the bioNLP field, temporal RE, and Open RE."
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,The Relation Extraction Task,"Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","# QUA Analysis:

1. (What does the RE task involve?): sent1
    1.1. (What are the steps in the RE pipeline?): sent2
        1.1.1. (What is the first step in the RE pipeline?): sent2
        1.1.2. (What does the first step involve?): sent3
    1.2. (What happens after entities are identified?): sent4
        1.2.1. (What are the different approaches to RE after entity identification?): sent4
        1.2.2. (What is one way to approach RE?): sent5
            1.2.2.1. (What does the two-step approach involve?): sent6
                1.2.2.1.1. (What is the challenge with the RI step?): sent7
            1.2.2.2. (What is the last step in the two-step approach?): sent8
        1.2.3. (What are alternative approaches to the two-step method?): sent9, sent10
            1.2.3.1. (How do some studies merge RI and RC?): sent9
            1.2.3.2. (How do other studies simplify the task?): sent10
        1.2.4. (How does RI relate to RC?): sent11
            1.2.4.1. (How do some studies address the approximation in RI?): sent12
                1.2.4.1.1. (What is the difference between NOTA and no-rel?): sent13
2. (How comprehensive are RE studies in addressing the pipeline?): sent15
    2.1. (What sub-steps are involved in entity extraction?): sent16
    2.2. (What is the focus of the analysis in the paper?): sent17
        2.2.1. (What does Table 2 show?): sent18
        2.2.2. (What is excluded from the analysis?): sent19
    2.3. (What are the findings of the analysis?): sent20
        2.3.1. (What is assumed for RE?): sent20
        2.3.2. (What is lacking in end-to-end models?): sent21
        2.3.3. (What is difficult to determine in the RI setup?): sent22
            2.3.3.1. (What is always performed?): sent23
            2.3.3.2. (What is the situation for RI?): sent23
                2.3.3.2.1. (When is RI clearly not done?): sent24
                2.3.3.2.2. (What is the common scenario for RI?): sent24
        2.3.4. (What is proposed to address the blurriness in evaluation?): sent25
            2.3.4.1. (Why are these details important?): sent26
3. (What are the challenges with the traditional RE pipeline?): sent27
    3.1. (What approaches have been proposed to alleviate error propagation?): sent28
        3.1.1. (What is the challenge with evaluating joint models?): sent29
            3.1.1.1. (What did Taillé et al. (2020) discuss?): sent29
            3.1.1.2. (What did they survey?): sent30
            3.1.1.3. (What did they observe and claim?): sent31
    3.2. (What is the current state of joint vs. pipeline models?): sent32
        3.2.1. (What did Zhong and Chen (2021) find?): sent33
            3.2.1.1. (Why might separate encoders be better?): sent34
        3.2.2. (Is one approach more suitable than the other for RE?): sent35
4. (What approach does the paper follow and why?): sent36",What are the findings of the analysis on RE studies?,"sent20, sent21, sent22, sent23, sent24, sent25, sent26","The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.

Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020).

Our analysis further shows that it is difficult to determine the RI setup.

While RC is always performed, the situation is different for RI (or no-rel).

Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances).

As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled.

These details are utterly important as they impact both model estimation and evaluation.","The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.

Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020).

The analysis further shows that it is difficult to determine the RI setup.

While relation classification (RC) is always performed, the situation is different for relation identification (RI) or no-relation (no-rel).

Sometimes RI is clearly not performed, with papers assuming that every instance contains at least one relation. However, most of the time, it is either not clear from the paper or done in a simplified scenario, such as using datasets that already exclude most no-relation entity pair instances.

As this blurriness hampers fair evaluation, it is proposed that studies clearly state which step they include, i.e., whether the work focuses on RC, RI+RC, or the full RE pipeline, and how special cases (no-rel and NOTA) are handled.

These details are utterly important as they impact both model estimation and evaluation.","Questions:

What are the findings of the analysis on RE studies?

Answer:

The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.

Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020).

The analysis further shows that it is difficult to determine the RI setup.

While relation classification (RC) is always performed, the situation is different for relation identification (RI) or no-relation (no-rel).

Sometimes RI is clearly not performed, with papers assuming that every instance contains at least one relation. However, most of the time, it is either not clear from the paper or done in a simplified scenario, such as using datasets that already exclude most no-relation entity pair instances.

As this blurriness hampers fair evaluation, it is proposed that studies clearly state which step they include, i.e., whether the work focuses on RC, RI+RC, or the full RE pipeline, and how special cases (no-rel and NOTA) are handled.

These details are utterly important as they impact both model estimation and evaluation."
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,The Relation Extraction Task,"Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","# QUA Analysis:

1. (What does the RE task involve?): sent1
    1.1. (What are the steps in the RE pipeline?): sent2
        1.1.1. (What is the first step in the RE pipeline?): sent2
        1.1.2. (What does the first step involve?): sent3
    1.2. (What happens after entities are identified?): sent4
        1.2.1. (What are the different approaches to RE after entity identification?): sent4
        1.2.2. (What is one way to approach RE?): sent5
            1.2.2.1. (What does the two-step approach involve?): sent6
                1.2.2.1.1. (What is the challenge with the RI step?): sent7
            1.2.2.2. (What is the last step in the two-step approach?): sent8
        1.2.3. (What are alternative approaches to the two-step method?): sent9, sent10
            1.2.3.1. (How do some studies merge RI and RC?): sent9
            1.2.3.2. (How do other studies simplify the task?): sent10
        1.2.4. (How does RI relate to RC?): sent11
            1.2.4.1. (How do some studies address the approximation in RI?): sent12
                1.2.4.1.1. (What is the difference between NOTA and no-rel?): sent13
2. (How comprehensive are RE studies in addressing the pipeline?): sent15
    2.1. (What sub-steps are involved in entity extraction?): sent16
    2.2. (What is the focus of the analysis in the paper?): sent17
        2.2.1. (What does Table 2 show?): sent18
        2.2.2. (What is excluded from the analysis?): sent19
    2.3. (What are the findings of the analysis?): sent20
        2.3.1. (What is assumed for RE?): sent20
        2.3.2. (What is lacking in end-to-end models?): sent21
        2.3.3. (What is difficult to determine in the RI setup?): sent22
            2.3.3.1. (What is always performed?): sent23
            2.3.3.2. (What is the situation for RI?): sent23
                2.3.3.2.1. (When is RI clearly not done?): sent24
                2.3.3.2.2. (What is the common scenario for RI?): sent24
        2.3.4. (What is proposed to address the blurriness in evaluation?): sent25
            2.3.4.1. (Why are these details important?): sent26
3. (What are the challenges with the traditional RE pipeline?): sent27
    3.1. (What approaches have been proposed to alleviate error propagation?): sent28
        3.1.1. (What is the challenge with evaluating joint models?): sent29
            3.1.1.1. (What did Taillé et al. (2020) discuss?): sent29
            3.1.1.2. (What did they survey?): sent30
            3.1.1.3. (What did they observe and claim?): sent31
    3.2. (What is the current state of joint vs. pipeline models?): sent32
        3.2.1. (What did Zhong and Chen (2021) find?): sent33
            3.2.1.1. (Why might separate encoders be better?): sent34
        3.2.2. (Is one approach more suitable than the other for RE?): sent35
4. (What approach does the paper follow and why?): sent36",What are the challenges with the traditional RE pipeline?,"sent27, sent28, sent29, sent30, sent31","Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.

Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021).

However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models.

They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a).

They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.","The traditional RE pipeline is prone to error propagation by sub-tasks.

Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016; Zhang et al., 2017a; Bekoulis et al., 2018a,b; Wang and Lu, 2020; Wang et al., 2021).

However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models.

Taillé et al. (2020) surveyed the evaluation metrics of recently published works on end-to-end RE, referring to the Strict, Boundaries, and Relaxed evaluation settings proposed by Bekoulis et al. (2018a).

They observe unfair comparisons and overestimations of end-to-end models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.","Questions:

What are the challenges with the traditional RE pipeline?

Answer:

The traditional RE pipeline is prone to error propagation by sub-tasks.

Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016; Zhang et al., 2017a; Bekoulis et al., 2018a,b; Wang and Lu, 2020; Wang et al., 2021).

However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models.

Taillé et al. (2020) surveyed the evaluation metrics of recently published works on end-to-end RE, referring to the Strict, Boundaries, and Relaxed evaluation settings proposed by Bekoulis et al. (2018a).

They observe unfair comparisons and overestimations of end-to-end models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics."
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,The Relation Extraction Task,"Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","# QUA Analysis:

1. (What does the RE task involve?): sent1
    1.1. (What are the steps in the RE pipeline?): sent2
        1.1.1. (What is the first step in the RE pipeline?): sent2
        1.1.2. (What does the first step involve?): sent3
    1.2. (What happens after entities are identified?): sent4
        1.2.1. (What are the different approaches to RE after entity identification?): sent4
        1.2.2. (What is one way to approach RE?): sent5
            1.2.2.1. (What does the two-step approach involve?): sent6
                1.2.2.1.1. (What is the challenge with the RI step?): sent7
            1.2.2.2. (What is the last step in the two-step approach?): sent8
        1.2.3. (What are alternative approaches to the two-step method?): sent9, sent10
            1.2.3.1. (How do some studies merge RI and RC?): sent9
            1.2.3.2. (How do other studies simplify the task?): sent10
        1.2.4. (How does RI relate to RC?): sent11
            1.2.4.1. (How do some studies address the approximation in RI?): sent12
                1.2.4.1.1. (What is the difference between NOTA and no-rel?): sent13
2. (How comprehensive are RE studies in addressing the pipeline?): sent15
    2.1. (What sub-steps are involved in entity extraction?): sent16
    2.2. (What is the focus of the analysis in the paper?): sent17
        2.2.1. (What does Table 2 show?): sent18
        2.2.2. (What is excluded from the analysis?): sent19
    2.3. (What are the findings of the analysis?): sent20
        2.3.1. (What is assumed for RE?): sent20
        2.3.2. (What is lacking in end-to-end models?): sent21
        2.3.3. (What is difficult to determine in the RI setup?): sent22
            2.3.3.1. (What is always performed?): sent23
            2.3.3.2. (What is the situation for RI?): sent23
                2.3.3.2.1. (When is RI clearly not done?): sent24
                2.3.3.2.2. (What is the common scenario for RI?): sent24
        2.3.4. (What is proposed to address the blurriness in evaluation?): sent25
            2.3.4.1. (Why are these details important?): sent26
3. (What are the challenges with the traditional RE pipeline?): sent27
    3.1. (What approaches have been proposed to alleviate error propagation?): sent28
        3.1.1. (What is the challenge with evaluating joint models?): sent29
            3.1.1.1. (What did Taillé et al. (2020) discuss?): sent29
            3.1.1.2. (What did they survey?): sent30
            3.1.1.3. (What did they observe and claim?): sent31
    3.2. (What is the current state of joint vs. pipeline models?): sent32
        3.2.1. (What did Zhong and Chen (2021) find?): sent33
            3.2.1.1. (Why might separate encoders be better?): sent34
        3.2.2. (Is one approach more suitable than the other for RE?): sent35
4. (What approach does the paper follow and why?): sent36",What is the current state of joint vs. pipeline models in RE?,"sent32, sent33, sent34, sent35","While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.

Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.

Since the output label space is different, separate encoders could better capture distinct contextual information.

At the moment it is not clear if one approach is more suitable than the other for RE.","While some recent work shifts to joint models, it remains an open question which approach (joint or pipeline) is the most robust.

Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT), using separate encoders can surpass existing joint models.

Since the output label space is different, separate encoders could better capture distinct contextual information.

At the moment, it is not clear if one approach is more suitable than the other for RE.","Questions:

What is the current state of joint vs. pipeline models in RE?

Answer:

While some recent work shifts to joint models, it remains an open question which approach (joint or pipeline) is the most robust.

Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT), using separate encoders can surpass existing joint models.

Since the output label space is different, separate encoders could better capture distinct contextual information.

At the moment, it is not clear if one approach is more suitable than the other for RE."
Abstractive Meeting Summarization: A Survey,Evaluation methods,"As noted in Section 2, the subjectivity of meeting summaries, stemming in part from the preference for abstractive summarization, complicates the evaluation task. Evaluation requires both verifying that the content in a system-based summary follows from the original transcript and measuring the overlap with a gold-standard summary. However, given the expressive freedom encouraged by abstractive approaches, checking for semantic entailment and overlap of summary-worthy content requires deep semantic understanding, not just recognition that the same words are used.

Unfortunately, the ROUGE metric (Lin, 2004), which remains the standard for both meeting and general text summarization, scores systemproduced summaries based purely on surface lexicographic matches with a (usually single) gold summary, making it unideal for assessing abstractive summaries. If we take the abstractive summary from Figure 1 (""Some part of the casing will be made of a spongy material"") as a gold example, ROUGE would assign a higher score to a system that produces ""Some part of the casing will be made of broccoli"" than one that output ""A portion of the outer layer will be constructed from a sponge-like material,"" even though the latter is a perfect reformulation of the gold summary, while the former says something very different (and false).

A reasonable way to try to improve over ROUGE would be to take advantage of massive, pretrained, contextual word embeddings and a notion of lexical similarity rather than strict lexical overlap. Some recent efforts pursue this direction (Sai et al., 2022), including BERTScore (Zhang et al., 2020a) and MoverScore (Zhao et al., 2019a), which aim to measure the semantic distance between the contextualized mapping of a generated summary and the reference, or BARTScore (Yuan et al., 2021), which calculates the log-likelihood of a summary to have been generated, motivated by the fact that a good summary should have a high probability of being generated from a source text. Building upon these methods, DATScore and FrugalScore (Eddine et al., 2022;Kamal Eddine et al., 2022) incorporate data augmentation and knowledge distillation techniques to further improve performance and overcome their drawbacks.

Despite their rapid development, recent studies on meta-evaluation of these metrics show mixed results. Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range, e.g., there is no consensus among metrics regarding which summary is better than another in the high scoring range in which modern systems now operate. Bhandari et al. (2020b) argue that there is no one-size-fitsall metric that correlates better with human judgement than the others, and that can outperform others on all datasets.

Clearly, evaluation is itself a very challenging task. And we note that none of these metrics even touches on another central challenge for summary evaluation, namely that of factual consistency. When we summarize a meeting or detail decisions and actions items in our own words, it is important to get the facts straight. Otherwise, the resulting summaries are not reliable for an end user. While we do not know of current work that focuses on evaluation of factuality explicitly for the meeting domain, the study of factual consistency in summarization more generally is a budding research area that we discuss in Section 6.

In the absence of a clear winner for summary evaluation metrics, none of the alternatives has yet to be widely adopted. In our comparison of different summarization systems in Section 5, we therefore stick with ROUGE, which offers the additional advantage of conceptual simplicity and lower computational cost compared to metrics based on contextual embeddings and pretrained language models.","1. Why is the evaluation of meeting summaries complicated? 
    1.1. What does evaluation require? (sent2)
        1.1.1. Why is deep semantic understanding necessary for evaluation? (sent3)
    1.2. Why is the ROUGE metric unideal for assessing abstractive summaries? (sent4)
        1.2.1. What is an example illustrating the limitations of ROUGE? (sent5)
2. How can we improve over ROUGE for evaluating abstractive summaries? (sent6)
    2.1. What are some recent efforts to improve evaluation metrics? (sent7)
        2.1.1. How do DATScore and FrugalScore build upon these methods? (sent8)
    2.2. What are the results of recent studies on meta-evaluation of these metrics? (sent9)
        2.2.1. What do Peyrard (2019) and Bhandari et al. (2020a) show about these metrics? (sent10)
        2.2.2. What does Bhandari et al. (2020b) argue about the metrics? (sent11)
3. What are the challenges in evaluating summaries? (sent12)
    3.1. What is another central challenge for summary evaluation? (sent13)
        3.1.1. Why is factual consistency important in meeting summaries? (sent14)
            3.1.1.1. What happens if the facts are not straight in summaries? (sent15)
    3.2. Is there current work focusing on the evaluation of factuality in the meeting domain? (sent16)
4. What is the current state of summary evaluation metrics adoption? (sent17)
    4.1. What metric is used in the comparison of different summarization systems in Section 5, and why? (sent18)",How can we improve over ROUGE for evaluating abstractive summaries?,"sent6, sent7, sent8","A reasonable way to try to improve over ROUGE would be to take advantage of massive, pretrained, contextual word embeddings and a notion of lexical similarity rather than strict lexical overlap.

Some recent efforts pursue this direction (Sai et al., 2022), including BERTScore (Zhang et al., 2020a) and MoverScore (Zhao et al., 2019a), which aim to measure the semantic distance between the contextualized mapping of a generated summary and the reference, or BARTScore (Yuan et al., 2021), which calculates the log-likelihood of a summary to have been generated, motivated by the fact that a good summary should have a high probability of being generated from a source text.

Building upon these methods, DATScore and FrugalScore (Eddine et al., 2022;Kamal Eddine et al., 2022) incorporate data augmentation and knowledge distillation techniques to further improve performance and overcome their drawbacks.","A reasonable way to improve over ROUGE would be to take advantage of massive, pretrained, contextual word embeddings and a notion of lexical similarity rather than strict lexical overlap.

Some recent efforts pursue this direction (Sai et al., 2022), including BERTScore (Zhang et al., 2020a) and MoverScore (Zhao et al., 2019a), which aim to measure the semantic distance between the contextualized mapping of a generated summary and the reference, or BARTScore (Yuan et al., 2021), which calculates the log-likelihood of a summary to have been generated, motivated by the fact that a good summary should have a high probability of being generated from a source text.

Building upon these methods, DATScore and FrugalScore (Eddine et al., 2022; Kamal Eddine et al., 2022) incorporate data augmentation and knowledge distillation techniques to further improve performance and overcome their drawbacks.","Questions:

How can we improve over ROUGE for evaluating abstractive summaries?

Answer:

A reasonable way to improve over ROUGE would be to take advantage of massive, pretrained, contextual word embeddings and a notion of lexical similarity rather than strict lexical overlap.

Some recent efforts pursue this direction (Sai et al., 2022), including BERTScore (Zhang et al., 2020a) and MoverScore (Zhao et al., 2019a), which aim to measure the semantic distance between the contextualized mapping of a generated summary and the reference, or BARTScore (Yuan et al., 2021), which calculates the log-likelihood of a summary to have been generated, motivated by the fact that a good summary should have a high probability of being generated from a source text.

Building upon these methods, DATScore and FrugalScore (Eddine et al., 2022; Kamal Eddine et al., 2022) incorporate data augmentation and knowledge distillation techniques to further improve performance and overcome their drawbacks."
Abstractive Meeting Summarization: A Survey,Evaluation methods,"As noted in Section 2, the subjectivity of meeting summaries, stemming in part from the preference for abstractive summarization, complicates the evaluation task. Evaluation requires both verifying that the content in a system-based summary follows from the original transcript and measuring the overlap with a gold-standard summary. However, given the expressive freedom encouraged by abstractive approaches, checking for semantic entailment and overlap of summary-worthy content requires deep semantic understanding, not just recognition that the same words are used.

Unfortunately, the ROUGE metric (Lin, 2004), which remains the standard for both meeting and general text summarization, scores systemproduced summaries based purely on surface lexicographic matches with a (usually single) gold summary, making it unideal for assessing abstractive summaries. If we take the abstractive summary from Figure 1 (""Some part of the casing will be made of a spongy material"") as a gold example, ROUGE would assign a higher score to a system that produces ""Some part of the casing will be made of broccoli"" than one that output ""A portion of the outer layer will be constructed from a sponge-like material,"" even though the latter is a perfect reformulation of the gold summary, while the former says something very different (and false).

A reasonable way to try to improve over ROUGE would be to take advantage of massive, pretrained, contextual word embeddings and a notion of lexical similarity rather than strict lexical overlap. Some recent efforts pursue this direction (Sai et al., 2022), including BERTScore (Zhang et al., 2020a) and MoverScore (Zhao et al., 2019a), which aim to measure the semantic distance between the contextualized mapping of a generated summary and the reference, or BARTScore (Yuan et al., 2021), which calculates the log-likelihood of a summary to have been generated, motivated by the fact that a good summary should have a high probability of being generated from a source text. Building upon these methods, DATScore and FrugalScore (Eddine et al., 2022;Kamal Eddine et al., 2022) incorporate data augmentation and knowledge distillation techniques to further improve performance and overcome their drawbacks.

Despite their rapid development, recent studies on meta-evaluation of these metrics show mixed results. Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range, e.g., there is no consensus among metrics regarding which summary is better than another in the high scoring range in which modern systems now operate. Bhandari et al. (2020b) argue that there is no one-size-fitsall metric that correlates better with human judgement than the others, and that can outperform others on all datasets.

Clearly, evaluation is itself a very challenging task. And we note that none of these metrics even touches on another central challenge for summary evaluation, namely that of factual consistency. When we summarize a meeting or detail decisions and actions items in our own words, it is important to get the facts straight. Otherwise, the resulting summaries are not reliable for an end user. While we do not know of current work that focuses on evaluation of factuality explicitly for the meeting domain, the study of factual consistency in summarization more generally is a budding research area that we discuss in Section 6.

In the absence of a clear winner for summary evaluation metrics, none of the alternatives has yet to be widely adopted. In our comparison of different summarization systems in Section 5, we therefore stick with ROUGE, which offers the additional advantage of conceptual simplicity and lower computational cost compared to metrics based on contextual embeddings and pretrained language models.","1. Why is the evaluation of meeting summaries complicated? 
    1.1. What does evaluation require? (sent2)
        1.1.1. Why is deep semantic understanding necessary for evaluation? (sent3)
    1.2. Why is the ROUGE metric unideal for assessing abstractive summaries? (sent4)
        1.2.1. What is an example illustrating the limitations of ROUGE? (sent5)
2. How can we improve over ROUGE for evaluating abstractive summaries? (sent6)
    2.1. What are some recent efforts to improve evaluation metrics? (sent7)
        2.1.1. How do DATScore and FrugalScore build upon these methods? (sent8)
    2.2. What are the results of recent studies on meta-evaluation of these metrics? (sent9)
        2.2.1. What do Peyrard (2019) and Bhandari et al. (2020a) show about these metrics? (sent10)
        2.2.2. What does Bhandari et al. (2020b) argue about the metrics? (sent11)
3. What are the challenges in evaluating summaries? (sent12)
    3.1. What is another central challenge for summary evaluation? (sent13)
        3.1.1. Why is factual consistency important in meeting summaries? (sent14)
            3.1.1.1. What happens if the facts are not straight in summaries? (sent15)
    3.2. Is there current work focusing on the evaluation of factuality in the meeting domain? (sent16)
4. What is the current state of summary evaluation metrics adoption? (sent17)
    4.1. What metric is used in the comparison of different summarization systems in Section 5, and why? (sent18)",What are the results of recent studies on meta-evaluation of these metrics?,"sent9, sent10, sent11","Despite their rapid development, recent studies on meta-evaluation of these metrics show mixed results.

Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range, e.g., there is no consensus among metrics regarding which summary is better than another in the high scoring range in which modern systems now operate.

Bhandari et al. (2020b) argue that there is no one-size-fitsall metric that correlates better with human judgement than the others, and that can outperform others on all datasets.","Recent studies on meta-evaluation of these metrics show mixed results.

Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range, indicating no consensus among metrics regarding which summary is better than another in the high scoring range in which modern systems now operate.

Bhandari et al. (2020b) argue that there is no one-size-fits-all metric that correlates better with human judgment than the others, and that can outperform others on all datasets.","Questions:

What are the results of recent studies on meta-evaluation of these metrics?

Answer:

Recent studies on meta-evaluation of these metrics show mixed results.

Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range, indicating no consensus among metrics regarding which summary is better than another in the high scoring range in which modern systems now operate.

Bhandari et al. (2020b) argue that there is no one-size-fits-all metric that correlates better with human judgment than the others, and that can outperform others on all datasets."
Abstractive Meeting Summarization: A Survey,Evaluation methods,"As noted in Section 2, the subjectivity of meeting summaries, stemming in part from the preference for abstractive summarization, complicates the evaluation task. Evaluation requires both verifying that the content in a system-based summary follows from the original transcript and measuring the overlap with a gold-standard summary. However, given the expressive freedom encouraged by abstractive approaches, checking for semantic entailment and overlap of summary-worthy content requires deep semantic understanding, not just recognition that the same words are used.

Unfortunately, the ROUGE metric (Lin, 2004), which remains the standard for both meeting and general text summarization, scores systemproduced summaries based purely on surface lexicographic matches with a (usually single) gold summary, making it unideal for assessing abstractive summaries. If we take the abstractive summary from Figure 1 (""Some part of the casing will be made of a spongy material"") as a gold example, ROUGE would assign a higher score to a system that produces ""Some part of the casing will be made of broccoli"" than one that output ""A portion of the outer layer will be constructed from a sponge-like material,"" even though the latter is a perfect reformulation of the gold summary, while the former says something very different (and false).

A reasonable way to try to improve over ROUGE would be to take advantage of massive, pretrained, contextual word embeddings and a notion of lexical similarity rather than strict lexical overlap. Some recent efforts pursue this direction (Sai et al., 2022), including BERTScore (Zhang et al., 2020a) and MoverScore (Zhao et al., 2019a), which aim to measure the semantic distance between the contextualized mapping of a generated summary and the reference, or BARTScore (Yuan et al., 2021), which calculates the log-likelihood of a summary to have been generated, motivated by the fact that a good summary should have a high probability of being generated from a source text. Building upon these methods, DATScore and FrugalScore (Eddine et al., 2022;Kamal Eddine et al., 2022) incorporate data augmentation and knowledge distillation techniques to further improve performance and overcome their drawbacks.

Despite their rapid development, recent studies on meta-evaluation of these metrics show mixed results. Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range, e.g., there is no consensus among metrics regarding which summary is better than another in the high scoring range in which modern systems now operate. Bhandari et al. (2020b) argue that there is no one-size-fitsall metric that correlates better with human judgement than the others, and that can outperform others on all datasets.

Clearly, evaluation is itself a very challenging task. And we note that none of these metrics even touches on another central challenge for summary evaluation, namely that of factual consistency. When we summarize a meeting or detail decisions and actions items in our own words, it is important to get the facts straight. Otherwise, the resulting summaries are not reliable for an end user. While we do not know of current work that focuses on evaluation of factuality explicitly for the meeting domain, the study of factual consistency in summarization more generally is a budding research area that we discuss in Section 6.

In the absence of a clear winner for summary evaluation metrics, none of the alternatives has yet to be widely adopted. In our comparison of different summarization systems in Section 5, we therefore stick with ROUGE, which offers the additional advantage of conceptual simplicity and lower computational cost compared to metrics based on contextual embeddings and pretrained language models.","1. Why is the evaluation of meeting summaries complicated? 
    1.1. What does evaluation require? (sent2)
        1.1.1. Why is deep semantic understanding necessary for evaluation? (sent3)
    1.2. Why is the ROUGE metric unideal for assessing abstractive summaries? (sent4)
        1.2.1. What is an example illustrating the limitations of ROUGE? (sent5)
2. How can we improve over ROUGE for evaluating abstractive summaries? (sent6)
    2.1. What are some recent efforts to improve evaluation metrics? (sent7)
        2.1.1. How do DATScore and FrugalScore build upon these methods? (sent8)
    2.2. What are the results of recent studies on meta-evaluation of these metrics? (sent9)
        2.2.1. What do Peyrard (2019) and Bhandari et al. (2020a) show about these metrics? (sent10)
        2.2.2. What does Bhandari et al. (2020b) argue about the metrics? (sent11)
3. What are the challenges in evaluating summaries? (sent12)
    3.1. What is another central challenge for summary evaluation? (sent13)
        3.1.1. Why is factual consistency important in meeting summaries? (sent14)
            3.1.1.1. What happens if the facts are not straight in summaries? (sent15)
    3.2. Is there current work focusing on the evaluation of factuality in the meeting domain? (sent16)
4. What is the current state of summary evaluation metrics adoption? (sent17)
    4.1. What metric is used in the comparison of different summarization systems in Section 5, and why? (sent18)",What is another central challenge for summary evaluation?,"sent13, sent14, sent15","And we note that none of these metrics even touches on another central challenge for summary evaluation, namely that of factual consistency.

When we summarize a meeting or detail decisions and actions items in our own words, it is important to get the facts straight.

Otherwise, the resulting summaries are not reliable for an end user.","Another central challenge for summary evaluation is that of factual consistency.

When summarizing a meeting or detailing decisions and action items in one's own words, it is important to ensure factual accuracy.

Otherwise, the resulting summaries are not reliable for an end user.","Questions:

What is another central challenge for summary evaluation?

Answer:

Another central challenge for summary evaluation is that of factual consistency.

When summarizing a meeting or detailing decisions and action items in one's own words, it is important to ensure factual accuracy.

Otherwise, the resulting summaries are not reliable for an end user."
Abstractive Meeting Summarization: A Survey,Discursive information,"While most work on abstractive meeting summarization implicitly assumes that conversation is merely a linear sequence of utterances without semantic relations between them, example (1) underscores the importance of semantic relations for conversational understanding. Drawing on similar insights, certain recent approaches exploit independent theories of discourse structure, such as Rhetorical Structure Theory (RST; Mann and Thompson, 1987) and Segmented Discourse Representation Theory (SDRT; Asher, 1993;Lascarides and Asher, 2008), to improve summarization. Accounts like RST and SDRT maintain that in a coherent conversation, each (roughly) clause-level unit should be semantically related to some other part of the conversation via a discourse relation such as Question-Answer Pair (QAP), Acknowledgement (Ack), Explanation, Contrast, etc. to reflect its contribution to the larger discourse. Each coherent discourse can thus be represented as a weakly-connected graph whose edges are labeled with discourse relations. Figure 2 il-  , 2021a). The * indicates summary-level (with sentence split) ROUGE-L score (Lin, 2004).

lustrates a possible SDRT graph for example (1).

To the best of our knowledge, Feng et al. (2020) is the first work to exploit discourse graphs to generate abstractive meeting summaries. They employ a sequential discourse parser (Shi and Huang, 2019) trained on the STAC corpus of multi-party chats (Asher et al., 2016) to automatically obtain discourse graphs for the AMI and ICSI meeting corpora. Levi graph transformation (Gross and Yellen, 2003) is then used to turn graph edges labeled with discourse relation types into vertices. Their graph-to-sequence model consists of a graph convolutional network encoder (Schlichtkrull et al., 2018) that takes a meeting discourse graph as input and a PGN decoder (See et al., 2017) to generate the final summary. A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model.

An alternative approach to discourse interpretation that developed largely independently of RST and SDRT is dialogue act classification. Detailing the differences between the approaches is out of the scope of this paper, but in a nutshell, dialogue acts provide a shallower notion of discourse structure (Jurafsky et al., 1997) in that they do not entail a full graph structure over a conversation. On the other hand, systems for dialogue act labeling, such as DAMSL (Allen and Core, 1997; Core and Allen, 1997) or DiAML (Bunt et al., 2010(Bunt et al., , 2012, place more emphasis on interactive acts such as stalling to hold the floor, assessing other discourse moves, suggesting or informing.

Both the AMI and ICSI corpora provide gold dialogue act labels, and Goo and Chen (2018) use the labels from the AMI corpus to develop a sentence-gated mechanism that jointly models the relationships between dialogue acts and topic summaries to improve abstractive meeting summarization. In particular, they show that dialogue acts of the type inform are more closely linked to summary-worthy material than acts such as stall or assess. Using LSTM, their model consists of three components enhanced with various attention mechanisms: an utterance encoder, a dialogue act labeler, and a summary decoder.

Dialogue acts have also been used to good effect for summarizing decisions. Fernández et al. (2008) and Bui et al. (2009) first identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts including the issue under discussion, its resolution, or agreement with the proposed resolution. Then, key fragments of the decision related utterances are retained to form an extractive decision summary. Similar ideas can also be found in the literature on detecting and summarizing action-item-specific dialogue acts in meetings (Purver et al., 2006(Purver et al., , 2007.","1. What is the importance of semantic relations in conversational understanding for abstractive meeting summarization?
    1.1. How do recent approaches exploit independent theories of discourse structure to improve summarization? (sent2)
        1.1.1. What do RST and SDRT maintain about coherent conversations? (sent3)
        1.1.2. How can each coherent discourse be represented? (sent4)
        1.1.3. What does Figure 2 illustrate? (sent5)
    1.2. What is the first work to exploit discourse graphs for generating abstractive meeting summaries? (sent6)
        1.2.1. How do they obtain discourse graphs for the AMI and ICSI meeting corpora? (sent7)
        1.2.2. How is Levi graph transformation used in this context? (sent8)
        1.2.3. What does their graph-to-sequence model consist of? (sent9)
        1.2.4. What strategy is introduced to pretrain the model? (sent10)
2. What is an alternative approach to discourse interpretation? (sent11)
    2.1. How do dialogue acts differ from RST and SDRT? (sent12)
    2.2. What do systems for dialogue act labeling emphasize? (sent13)
    2.3. How are dialogue act labels used in the AMI and ICSI corpora? (sent14)
        2.3.1. What do Goo and Chen (2018) show about dialogue acts? (sent15)
        2.3.2. What does their model consist of? (sent16)
    2.4. How have dialogue acts been used for summarizing decisions? (sent17)
        2.4.1. How do Fernández et al. (2008) and Bui et al. (2009) use dialogue acts for decision-related discussions? (sent18)
        2.4.2. How are key fragments of decision-related utterances used? (sent19)
        2.4.3. What similar ideas exist in the literature? (sent20)",How do recent approaches exploit independent theories of discourse structure to improve summarization?,"sent2, sent3, sent4","Drawing on similar insights, certain recent approaches exploit independent theories of discourse structure, such as Rhetorical Structure Theory (RST; Mann and Thompson, 1987) and Segmented Discourse Representation Theory (SDRT; Asher, 1993;Lascarides and Asher, 2008), to improve summarization.

Accounts like RST and SDRT maintain that in a coherent conversation, each (roughly) clause-level unit should be semantically related to some other part of the conversation via a discourse relation such as Question-Answer Pair (QAP), Acknowledgement (Ack), Explanation, Contrast, etc. to reflect its contribution to the larger discourse.

Each coherent discourse can thus be represented as a weakly-connected graph whose edges are labeled with discourse relations.","Certain recent approaches exploit independent theories of discourse structure, such as Rhetorical Structure Theory (RST; Mann and Thompson, 1987) and Segmented Discourse Representation Theory (SDRT; Asher, 1993; Lascarides and Asher, 2008), to improve summarization.

Accounts like RST and SDRT maintain that in a coherent conversation, each clause-level unit should be semantically related to some other part of the conversation via a discourse relation such as Question-Answer Pair (QAP), Acknowledgement (Ack), Explanation, or Contrast to reflect its contribution to the larger discourse.

Each coherent discourse can thus be represented as a weakly-connected graph whose edges are labeled with discourse relations.","Questions:

How do recent approaches exploit independent theories of discourse structure to improve summarization?

Answer:

Certain recent approaches exploit independent theories of discourse structure, such as Rhetorical Structure Theory (RST; Mann and Thompson, 1987) and Segmented Discourse Representation Theory (SDRT; Asher, 1993; Lascarides and Asher, 2008), to improve summarization.

Accounts like RST and SDRT maintain that in a coherent conversation, each clause-level unit should be semantically related to some other part of the conversation via a discourse relation such as Question-Answer Pair (QAP), Acknowledgement (Ack), Explanation, or Contrast to reflect its contribution to the larger discourse.

Each coherent discourse can thus be represented as a weakly-connected graph whose edges are labeled with discourse relations."
Abstractive Meeting Summarization: A Survey,Discursive information,"While most work on abstractive meeting summarization implicitly assumes that conversation is merely a linear sequence of utterances without semantic relations between them, example (1) underscores the importance of semantic relations for conversational understanding. Drawing on similar insights, certain recent approaches exploit independent theories of discourse structure, such as Rhetorical Structure Theory (RST; Mann and Thompson, 1987) and Segmented Discourse Representation Theory (SDRT; Asher, 1993;Lascarides and Asher, 2008), to improve summarization. Accounts like RST and SDRT maintain that in a coherent conversation, each (roughly) clause-level unit should be semantically related to some other part of the conversation via a discourse relation such as Question-Answer Pair (QAP), Acknowledgement (Ack), Explanation, Contrast, etc. to reflect its contribution to the larger discourse. Each coherent discourse can thus be represented as a weakly-connected graph whose edges are labeled with discourse relations. Figure 2 il-  , 2021a). The * indicates summary-level (with sentence split) ROUGE-L score (Lin, 2004).

lustrates a possible SDRT graph for example (1).

To the best of our knowledge, Feng et al. (2020) is the first work to exploit discourse graphs to generate abstractive meeting summaries. They employ a sequential discourse parser (Shi and Huang, 2019) trained on the STAC corpus of multi-party chats (Asher et al., 2016) to automatically obtain discourse graphs for the AMI and ICSI meeting corpora. Levi graph transformation (Gross and Yellen, 2003) is then used to turn graph edges labeled with discourse relation types into vertices. Their graph-to-sequence model consists of a graph convolutional network encoder (Schlichtkrull et al., 2018) that takes a meeting discourse graph as input and a PGN decoder (See et al., 2017) to generate the final summary. A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model.

An alternative approach to discourse interpretation that developed largely independently of RST and SDRT is dialogue act classification. Detailing the differences between the approaches is out of the scope of this paper, but in a nutshell, dialogue acts provide a shallower notion of discourse structure (Jurafsky et al., 1997) in that they do not entail a full graph structure over a conversation. On the other hand, systems for dialogue act labeling, such as DAMSL (Allen and Core, 1997; Core and Allen, 1997) or DiAML (Bunt et al., 2010(Bunt et al., , 2012, place more emphasis on interactive acts such as stalling to hold the floor, assessing other discourse moves, suggesting or informing.

Both the AMI and ICSI corpora provide gold dialogue act labels, and Goo and Chen (2018) use the labels from the AMI corpus to develop a sentence-gated mechanism that jointly models the relationships between dialogue acts and topic summaries to improve abstractive meeting summarization. In particular, they show that dialogue acts of the type inform are more closely linked to summary-worthy material than acts such as stall or assess. Using LSTM, their model consists of three components enhanced with various attention mechanisms: an utterance encoder, a dialogue act labeler, and a summary decoder.

Dialogue acts have also been used to good effect for summarizing decisions. Fernández et al. (2008) and Bui et al. (2009) first identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts including the issue under discussion, its resolution, or agreement with the proposed resolution. Then, key fragments of the decision related utterances are retained to form an extractive decision summary. Similar ideas can also be found in the literature on detecting and summarizing action-item-specific dialogue acts in meetings (Purver et al., 2006(Purver et al., , 2007.","1. What is the importance of semantic relations in conversational understanding for abstractive meeting summarization?
    1.1. How do recent approaches exploit independent theories of discourse structure to improve summarization? (sent2)
        1.1.1. What do RST and SDRT maintain about coherent conversations? (sent3)
        1.1.2. How can each coherent discourse be represented? (sent4)
        1.1.3. What does Figure 2 illustrate? (sent5)
    1.2. What is the first work to exploit discourse graphs for generating abstractive meeting summaries? (sent6)
        1.2.1. How do they obtain discourse graphs for the AMI and ICSI meeting corpora? (sent7)
        1.2.2. How is Levi graph transformation used in this context? (sent8)
        1.2.3. What does their graph-to-sequence model consist of? (sent9)
        1.2.4. What strategy is introduced to pretrain the model? (sent10)
2. What is an alternative approach to discourse interpretation? (sent11)
    2.1. How do dialogue acts differ from RST and SDRT? (sent12)
    2.2. What do systems for dialogue act labeling emphasize? (sent13)
    2.3. How are dialogue act labels used in the AMI and ICSI corpora? (sent14)
        2.3.1. What do Goo and Chen (2018) show about dialogue acts? (sent15)
        2.3.2. What does their model consist of? (sent16)
    2.4. How have dialogue acts been used for summarizing decisions? (sent17)
        2.4.1. How do Fernández et al. (2008) and Bui et al. (2009) use dialogue acts for decision-related discussions? (sent18)
        2.4.2. How are key fragments of decision-related utterances used? (sent19)
        2.4.3. What similar ideas exist in the literature? (sent20)",What is the first work to exploit discourse graphs for generating abstractive meeting summaries?,"sent6, sent7, sent8, sent9, sent10","To the best of our knowledge, Feng et al. (2020) is the first work to exploit discourse graphs to generate abstractive meeting summaries.

They employ a sequential discourse parser (Shi and Huang, 2019) trained on the STAC corpus of multi-party chats (Asher et al., 2016) to automatically obtain discourse graphs for the AMI and ICSI meeting corpora.

Levi graph transformation (Gross and Yellen, 2003) is then used to turn graph edges labeled with discourse relation types into vertices.

Their graph-to-sequence model consists of a graph convolutional network encoder (Schlichtkrull et al., 2018) that takes a meeting discourse graph as input and a PGN decoder (See et al., 2017) to generate the final summary.

A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model.","To the best of our knowledge, Feng et al. (2020) is the first work to exploit discourse graphs to generate abstractive meeting summaries.

They employ a sequential discourse parser (Shi and Huang, 2019) trained on the STAC corpus of multi-party chats (Asher et al., 2016) to automatically obtain discourse graphs for the AMI and ICSI meeting corpora.

Levi graph transformation (Gross and Yellen, 2003) is then used to turn graph edges labeled with discourse relation types into vertices.

Their graph-to-sequence model consists of a graph convolutional network encoder (Schlichtkrull et al., 2018) that takes a meeting discourse graph as input and a PGN decoder (See et al., 2017) to generate the final summary.

A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model.","Questions:

What is the first work to exploit discourse graphs for generating abstractive meeting summaries?

Answer:

To the best of our knowledge, Feng et al. (2020) is the first work to exploit discourse graphs to generate abstractive meeting summaries.

They employ a sequential discourse parser (Shi and Huang, 2019) trained on the STAC corpus of multi-party chats (Asher et al., 2016) to automatically obtain discourse graphs for the AMI and ICSI meeting corpora.

Levi graph transformation (Gross and Yellen, 2003) is then used to turn graph edges labeled with discourse relation types into vertices.

Their graph-to-sequence model consists of a graph convolutional network encoder (Schlichtkrull et al., 2018) that takes a meeting discourse graph as input and a PGN decoder (See et al., 2017) to generate the final summary.

A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model."
Abstractive Meeting Summarization: A Survey,Discursive information,"While most work on abstractive meeting summarization implicitly assumes that conversation is merely a linear sequence of utterances without semantic relations between them, example (1) underscores the importance of semantic relations for conversational understanding. Drawing on similar insights, certain recent approaches exploit independent theories of discourse structure, such as Rhetorical Structure Theory (RST; Mann and Thompson, 1987) and Segmented Discourse Representation Theory (SDRT; Asher, 1993;Lascarides and Asher, 2008), to improve summarization. Accounts like RST and SDRT maintain that in a coherent conversation, each (roughly) clause-level unit should be semantically related to some other part of the conversation via a discourse relation such as Question-Answer Pair (QAP), Acknowledgement (Ack), Explanation, Contrast, etc. to reflect its contribution to the larger discourse. Each coherent discourse can thus be represented as a weakly-connected graph whose edges are labeled with discourse relations. Figure 2 il-  , 2021a). The * indicates summary-level (with sentence split) ROUGE-L score (Lin, 2004).

lustrates a possible SDRT graph for example (1).

To the best of our knowledge, Feng et al. (2020) is the first work to exploit discourse graphs to generate abstractive meeting summaries. They employ a sequential discourse parser (Shi and Huang, 2019) trained on the STAC corpus of multi-party chats (Asher et al., 2016) to automatically obtain discourse graphs for the AMI and ICSI meeting corpora. Levi graph transformation (Gross and Yellen, 2003) is then used to turn graph edges labeled with discourse relation types into vertices. Their graph-to-sequence model consists of a graph convolutional network encoder (Schlichtkrull et al., 2018) that takes a meeting discourse graph as input and a PGN decoder (See et al., 2017) to generate the final summary. A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model.

An alternative approach to discourse interpretation that developed largely independently of RST and SDRT is dialogue act classification. Detailing the differences between the approaches is out of the scope of this paper, but in a nutshell, dialogue acts provide a shallower notion of discourse structure (Jurafsky et al., 1997) in that they do not entail a full graph structure over a conversation. On the other hand, systems for dialogue act labeling, such as DAMSL (Allen and Core, 1997; Core and Allen, 1997) or DiAML (Bunt et al., 2010(Bunt et al., , 2012, place more emphasis on interactive acts such as stalling to hold the floor, assessing other discourse moves, suggesting or informing.

Both the AMI and ICSI corpora provide gold dialogue act labels, and Goo and Chen (2018) use the labels from the AMI corpus to develop a sentence-gated mechanism that jointly models the relationships between dialogue acts and topic summaries to improve abstractive meeting summarization. In particular, they show that dialogue acts of the type inform are more closely linked to summary-worthy material than acts such as stall or assess. Using LSTM, their model consists of three components enhanced with various attention mechanisms: an utterance encoder, a dialogue act labeler, and a summary decoder.

Dialogue acts have also been used to good effect for summarizing decisions. Fernández et al. (2008) and Bui et al. (2009) first identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts including the issue under discussion, its resolution, or agreement with the proposed resolution. Then, key fragments of the decision related utterances are retained to form an extractive decision summary. Similar ideas can also be found in the literature on detecting and summarizing action-item-specific dialogue acts in meetings (Purver et al., 2006(Purver et al., , 2007.","1. What is the importance of semantic relations in conversational understanding for abstractive meeting summarization?
    1.1. How do recent approaches exploit independent theories of discourse structure to improve summarization? (sent2)
        1.1.1. What do RST and SDRT maintain about coherent conversations? (sent3)
        1.1.2. How can each coherent discourse be represented? (sent4)
        1.1.3. What does Figure 2 illustrate? (sent5)
    1.2. What is the first work to exploit discourse graphs for generating abstractive meeting summaries? (sent6)
        1.2.1. How do they obtain discourse graphs for the AMI and ICSI meeting corpora? (sent7)
        1.2.2. How is Levi graph transformation used in this context? (sent8)
        1.2.3. What does their graph-to-sequence model consist of? (sent9)
        1.2.4. What strategy is introduced to pretrain the model? (sent10)
2. What is an alternative approach to discourse interpretation? (sent11)
    2.1. How do dialogue acts differ from RST and SDRT? (sent12)
    2.2. What do systems for dialogue act labeling emphasize? (sent13)
    2.3. How are dialogue act labels used in the AMI and ICSI corpora? (sent14)
        2.3.1. What do Goo and Chen (2018) show about dialogue acts? (sent15)
        2.3.2. What does their model consist of? (sent16)
    2.4. How have dialogue acts been used for summarizing decisions? (sent17)
        2.4.1. How do Fernández et al. (2008) and Bui et al. (2009) use dialogue acts for decision-related discussions? (sent18)
        2.4.2. How are key fragments of decision-related utterances used? (sent19)
        2.4.3. What similar ideas exist in the literature? (sent20)",How have dialogue acts been used for summarizing decisions?,"sent17, sent18, sent19, sent20","Dialogue acts have also been used to good effect for summarizing decisions.

Fernández et al. (2008) and Bui et al. (2009) first identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts including the issue under discussion, its resolution, or agreement with the proposed resolution.

Then, key fragments of the decision related utterances are retained to form an extractive decision summary.

Similar ideas can also be found in the literature on detecting and summarizing action-item-specific dialogue acts in meetings (Purver et al., 2006(Purver et al., , 2007.","Dialogue acts have been used effectively for summarizing decisions.

Fernández et al. (2008) and Bui et al. (2009) identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts, including the issue under discussion, its resolution, or agreement with the proposed resolution.

Key fragments of the decision-related utterances are then retained to form an extractive decision summary.

Similar ideas can also be found in the literature on detecting and summarizing action-item-specific dialogue acts in meetings (Purver et al., 2006; Purver et al., 2007).","Questions:

How have dialogue acts been used for summarizing decisions?

Answer:

Dialogue acts have been used effectively for summarizing decisions.

Fernández et al. (2008) and Bui et al. (2009) identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts, including the issue under discussion, its resolution, or agreement with the proposed resolution.

Key fragments of the decision-related utterances are then retained to form an extractive decision summary.

Similar ideas can also be found in the literature on detecting and summarizing action-item-specific dialogue acts in meetings (Purver et al., 2006; Purver et al., 2007)."
Abstractive Meeting Summarization: A Survey,Multimodality,"Meetings tend to take place in shared visual contexts, allowing important information to be conveyed through gestures, facial expressions, head poses, eye gaze, and so on. It is thus reasonable to think that harnessing this information could lead to improved performance over a summarization system that draws on speech transcripts alone. Indeed, Li et al. (2019) has shown that the Visual Focus Of Attention (VFOA), which is estimated based on a participant's head orientation and eye gaze, can help a multi-modal summarization system determine salient utterances, thereby improving the quality of abstractive meeting summaries. In this work, VFOA is integrated with attention mechanisms in a PGN text decoder and topic segmenter trained in a multi-task learning setting.

In the context of extractive meeting summarization, Erol et al. (2003) exploits multimodality to select segments of meeting video recordings in order to facilitate browsing of the videos for important information. Along with TF-IDF text analysis of the speech transcript, their system attends to certain visual motion activities (e.g., someone entering the meeting room or standing up to make a presentation) and to acoustic features (e.g., speech volume and degree of interaction) to enhance detection of summary-worthy events. Similar ideas can be found in Xie and Liu (2010);Nihei et al. (2016Nihei et al. ( , 2018; Nihei and Nakano (2019).

Of course, enhancing meeting transcripts with information to improve interpretation comes at a cost: most annotations are performed manually and require linguistic expertise to do well, and annotations produced automatically, as in Shi and Huang (2019) or Shang et al. (2020b), can add uncertainties to the data. It is perhaps thus unsurprising that the last couple of years have seen a shift away from Interpretation-focused methods presented in this section towards Generation methods, as shown in Table 1. Still, certain approaches showed promising results, especially Feng et al.

(2020) and Li et al. (2019), and arguably, Interpretation approaches offer other advantages that should encourage us to explore them further. Discourse graphs, for example, can provide input to graph-based networks, helping to bypass limits on input document length imposed by some Generation approaches (see Section 5.3). And a rich source representation can arguably make a single transcript more valuable for training, partially offsetting data scarcity described in Section 3.","1. (What is the role of shared visual contexts in meetings?): sent1
    1.1. (How can harnessing visual information improve summarization systems?): sent2
        1.1.1. (What evidence supports the benefit of using Visual Focus Of Attention (VFOA) in summarization?): sent3
            1.1.1.1. (How is VFOA integrated into summarization systems?): sent4
    1.2. (How is multimodality used in extractive meeting summarization?): sent5
        1.2.1. (What specific features does Erol et al. (2003) use to enhance extractive summarization?): sent6
        1.2.2. (Are there other studies that support the use of multimodality in summarization?): sent7
2. (What are the challenges of enhancing meeting transcripts with additional information?): sent8
    2.1. (What has been the recent trend in summarization methods?): sent9
        2.1.1. (Are there still promising results from Interpretation-focused methods?): sent10
            2.1.1.1. (What are the advantages of Interpretation approaches?): sent11, sent12",How can harnessing visual information improve summarization systems?,"sent2, sent3, sent4","It is thus reasonable to think that harnessing this information could lead to improved performance over a summarization system that draws on speech transcripts alone.

Indeed, Li et al. (2019) has shown that the Visual Focus Of Attention (VFOA), which is estimated based on a participant's head orientation and eye gaze, can help a multi-modal summarization system determine salient utterances, thereby improving the quality of abstractive meeting summaries.

In this work, VFOA is integrated with attention mechanisms in a PGN text decoder and topic segmenter trained in a multi-task learning setting.","Harnessing visual information could lead to improved performance over a summarization system that draws on speech transcripts alone.

Li et al. (2019) has shown that the Visual Focus Of Attention (VFOA), which is estimated based on a participant's head orientation and eye gaze, can help a multi-modal summarization system determine salient utterances, thereby improving the quality of abstractive meeting summaries.

In this approach, VFOA is integrated with attention mechanisms in a PGN text decoder and topic segmenter trained in a multi-task learning setting.","Questions:

How can harnessing visual information improve summarization systems?

Answer:

Harnessing visual information could lead to improved performance over a summarization system that draws on speech transcripts alone.

Li et al. (2019) has shown that the Visual Focus Of Attention (VFOA), which is estimated based on a participant's head orientation and eye gaze, can help a multi-modal summarization system determine salient utterances, thereby improving the quality of abstractive meeting summaries.

In this approach, VFOA is integrated with attention mechanisms in a PGN text decoder and topic segmenter trained in a multi-task learning setting."
Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,B.5 Complexity of Natural Language and SQL Query Pairs,"In terms of the complexity for SQL queries, Finegan-Dollak et al. (2018) find that models perform better on shorter SQL queries than longer SQL queries, which indicates that shorter SQL queries are easier in general. Yu et al. (2018c) define the SQL hardness as the number of SQL components. The SQL query is harder when it contains more SQL keywords such as GROUP BY and nested subqueries. Yu et al. (2018c)  In terms of the complexity of natural utterance, there is no qualitative measure of how hard the utterance is. Intuitively, models' performance can decrease when faced with longer questions from users. However, the information conveyed in longer sentences can be more complete, while there can be ambiguity in shorter sentences. Besides, there can be domain-specific phrases that confuse the model in both short and long utterances (Suhr et al., 2020). Thus, researchers need to consider various perspectives to determine the complexity of natural utterance.  Nguyen et al., 2020) and José and Cozman (2021) translate all the English questions in Spider into Chinese, Vietnamese and Portuguese, respectively. TableQA (Sun et al., 2020) follows the data collection method from WikiSQL, while DuSQL (Wang et al., 2020c) follows Spider. Both TableQA and DuSQL collect Chinese utterance and SQL query pairs across different domains. Chen et al. (2021a) propose a Chinese domain-specific dataset, ESQL.","1. (What is the complexity of SQL queries?): sent1
    1.1. (How is SQL query complexity defined?): sent2
    1.2. (What makes a SQL query harder?): sent3
2. (What is the complexity of natural utterances?): sent4
    2.1. (How does the length of questions affect model performance?): sent5
    2.2. (What are the pros and cons of longer sentences?): sent6
    2.3. (What other factors can affect the complexity of natural utterances?): sent7
    2.4. (What should researchers consider when determining the complexity of natural utterances?): sent8
3. (What are the recent efforts in translating and collecting datasets?): sent9
    3.1. (What methods do TableQA and DuSQL follow for data collection?): sent10
    3.2. (What do TableQA and DuSQL collect?): sent11
    3.3. (What is a recent proposal for a Chinese domain-specific dataset?): sent12",What are the recent efforts in translating and collecting datasets for SQL queries?,"sent9, sent10, sent11, sent12","Nguyen et al., 2020) and José and Cozman (2021) translate all the English questions in Spider into Chinese, Vietnamese and Portuguese, respectively.

TableQA (Sun et al., 2020) follows the data collection method from WikiSQL, while DuSQL (Wang et al., 2020c) follows Spider.

Both TableQA and DuSQL collect Chinese utterance and SQL query pairs across different domains.

Chen et al. (2021a) propose a Chinese domain-specific dataset, ESQL.","Nguyen et al. (2020) and José and Cozman (2021) translate all the English questions in Spider into Chinese, Vietnamese, and Portuguese, respectively.

TableQA (Sun et al., 2020) follows the data collection method from WikiSQL, while DuSQL (Wang et al., 2020c) follows Spider.

Both TableQA and DuSQL collect Chinese utterance and SQL query pairs across different domains.

Chen et al. (2021a) propose a Chinese domain-specific dataset, ESQL.","Questions:

What are the recent efforts in translating and collecting datasets for SQL queries?

Answer:

Nguyen et al. (2020) and José and Cozman (2021) translate all the English questions in Spider into Chinese, Vietnamese, and Portuguese, respectively.

TableQA (Sun et al., 2020) follows the data collection method from WikiSQL, while DuSQL (Wang et al., 2020c) follows Spider.

Both TableQA and DuSQL collect Chinese utterance and SQL query pairs across different domains.

Chen et al. (2021a) propose a Chinese domain-specific dataset, ESQL."
Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,Evaluation,"Metrics Table 4 shows widely used automatic evaluation metrics for the text-to-SQL task. Early works evaluate SQL queries by comparing the database querying results executed from the predicted SQL query and the ground-truth (or gold) SQL query (Zelle and Mooney, 1996;Yaghmazadeh et al., 2017) (2018) propose SQL query split, where no SQL query is allowed to appear in more than one set among the train, dev, and test sets. Furthermore, Yu et al. (2018c) propose a database split, where the model does not see the databases in the test set in its training time. Other splitting methods also exist to help different research topics (Shaw et al., 2021;.","# QUA Analysis:

1. (What does Table 4 show?): sent1
    1.1. (How do early works evaluate SQL queries?): sent2
    1.2. (What are some proposed methods for SQL query evaluation?): sent3, sent4, sent5
        1.2.1. (What is SQL query split?): sent3
        1.2.2. (What is database split?): sent4
        1.2.3. (Are there other splitting methods?): sent5",What are some proposed methods for SQL query evaluation in text-to-SQL tasks?,"sent3, sent4, sent5","SQL query (Zelle and Mooney, 1996;Yaghmazadeh et al., 2017) (2018) propose SQL query split, where no SQL query is allowed to appear in more than one set among the train, dev, and test sets.

Furthermore, Yu et al. (2018c) propose a database split, where the model does not see the databases in the test set in its training time.

Other splitting methods also exist to help different research topics (Shaw et al., 2021;.","Some proposed methods for SQL query evaluation in text-to-SQL tasks include SQL query split, where no SQL query is allowed to appear in more than one set among the train, dev, and test sets (Zelle and Mooney, 1996; Yaghmazadeh et al., 2017).

Furthermore, Yu et al. (2018c) propose a database split, where the model does not see the databases in the test set during its training time.

Other splitting methods also exist to help different research topics (Shaw et al., 2021).","Questions:

What are some proposed methods for SQL query evaluation in text-to-SQL tasks?

Answer:

Some proposed methods for SQL query evaluation in text-to-SQL tasks include SQL query split, where no SQL query is allowed to appear in more than one set among the train, dev, and test sets (Zelle and Mooney, 1996; Yaghmazadeh et al., 2017).

Furthermore, Yu et al. (2018c) propose a database split, where the model does not see the databases in the test set during its training time.

Other splitting methods also exist to help different research topics (Shaw et al., 2021)."
"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",New Tasks: Inspiration from Sarcasm,"Compared to sarcasm, irony is rarely seen as a term in NLP conferences. Recently we have witnessed great improvements in sarcasm processing and in this part we will discuss how new tasks in sarcasm could motivate irony research.

Data Collection As discussed, datasets are highly dependent on hashtags as a signal to extract ironical expressions. Shmueli et al. (2020) proposed an algorithm to detect sarcastic tweets from a thread based on exterior cue tweets. A distant supervision based method for extracting ironies from platforms is crucial, given ironies in conversational contexts are central topic in the future.

Intended and Perceived Irony Oprea and Magdy (2019) explored how author profiling affected the perceived sarcasm (manual labelling) versus the intended sarcasm (hashtags), and verified the difference between both. Further, Oprea and Magdy (2020) introduced iSarcasm dataset which divided intended sarcasms and perceived sarcasms. The state-of-the-art sarcasm detection models performed obviously worse than human evaluation on this dataset. Future work could focus on multimodal perceived and intended irony, especially across various cultures.

Target Identification Sarcasm target identification was firstly proposed in Joshi et al. (2018), in which sarcasm targets were classified as one target, several targets and outside. Patro et al. (2019) introduced sociolinguistic features and a deep learning framework, and improved target identification by a lot. For irony processing, most ironical expressions do not equip a specific target in itself as previously discussed. However, its ironical effects are likely in dialogue or visually grounded environment, which encourages us to enhance irony datasets in aforementioned ways.

Irony Explanation Irony, according to the definition, have opposite real meanings to literal meanings. However, this does not mean adding a single negation could interpret ironies well. Kumar et al. (2022) proposed a new task, sarcasm explanation in dialogue. Irony explanation might encounter more complex problems due to relatively low proportion of targets. Still, we should include irony explanation as a branch of multimodal irony processing like Desai et al. (2021).","# QUA Analysis:

1. (What is the comparison between irony and sarcasm in NLP conferences?): sent1
    1.1. (What recent improvements in sarcasm processing could motivate irony research?): sent2
        1.1.1. (How is data collected for irony research?): sent3
        1.1.2. (What algorithm has been proposed for detecting sarcastic tweets?): sent4
        1.1.3. (Why is distant supervision important for extracting ironies?): sent5
    1.2. (What is the difference between intended and perceived irony?): sent6
        1.2.1. (What dataset divides intended and perceived sarcasms?): sent7
        1.2.2. (How do state-of-the-art sarcasm detection models perform on this dataset?): sent8
        1.2.3. (What could future work focus on regarding irony?): sent9
    1.3. (What is sarcasm target identification and its relevance to irony processing?): sent10
        1.3.1. (How was target identification improved?): sent11
        1.3.2. (Why do most ironical expressions not have a specific target?): sent12
        1.3.3. (What encourages enhancement of irony datasets?): sent13
    1.4. (What is irony explanation and its challenges?): sent14
        1.4.1. (Why can't a single negation interpret ironies well?): sent15
        1.4.2. (What new task has been proposed for sarcasm explanation in dialogue?): sent16
        1.4.3. (Why might irony explanation encounter more complex problems?): sent17
        1.4.4. (Why should irony explanation be included in multimodal irony processing?): sent18",How is data collected for irony research?,"sent3, sent5, sent12, sent13","Data Collection As discussed, datasets are highly dependent on hashtags as a signal to extract ironical expressions.

A distant supervision based method for extracting ironies from platforms is crucial, given ironies in conversational contexts are central topic in the future.

For irony processing, most ironical expressions do not equip a specific target in itself as previously discussed.

However, its ironical effects are likely in dialogue or visually grounded environment, which encourages us to enhance irony datasets in aforementioned ways.","Datasets for irony research are highly dependent on hashtags as a signal to extract ironical expressions.

A distant supervision-based method for extracting ironies from platforms is crucial, given that ironies in conversational contexts are a central topic for future research.

For irony processing, most ironical expressions do not have a specific target in themselves.

However, the ironical effects are more likely to be observed in dialogue or visually grounded environments, which suggests the need to enhance irony datasets accordingly.","Questions:

How is data collected for irony research?

Answer:

Datasets for irony research are highly dependent on hashtags as a signal to extract ironical expressions.

A distant supervision-based method for extracting ironies from platforms is crucial, given that ironies in conversational contexts are a central topic for future research.

For irony processing, most ironical expressions do not have a specific target in themselves.

However, the ironical effects are more likely to be observed in dialogue or visually grounded environments, which suggests the need to enhance irony datasets accordingly."
"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",New Tasks: Inspiration from Sarcasm,"Compared to sarcasm, irony is rarely seen as a term in NLP conferences. Recently we have witnessed great improvements in sarcasm processing and in this part we will discuss how new tasks in sarcasm could motivate irony research.

Data Collection As discussed, datasets are highly dependent on hashtags as a signal to extract ironical expressions. Shmueli et al. (2020) proposed an algorithm to detect sarcastic tweets from a thread based on exterior cue tweets. A distant supervision based method for extracting ironies from platforms is crucial, given ironies in conversational contexts are central topic in the future.

Intended and Perceived Irony Oprea and Magdy (2019) explored how author profiling affected the perceived sarcasm (manual labelling) versus the intended sarcasm (hashtags), and verified the difference between both. Further, Oprea and Magdy (2020) introduced iSarcasm dataset which divided intended sarcasms and perceived sarcasms. The state-of-the-art sarcasm detection models performed obviously worse than human evaluation on this dataset. Future work could focus on multimodal perceived and intended irony, especially across various cultures.

Target Identification Sarcasm target identification was firstly proposed in Joshi et al. (2018), in which sarcasm targets were classified as one target, several targets and outside. Patro et al. (2019) introduced sociolinguistic features and a deep learning framework, and improved target identification by a lot. For irony processing, most ironical expressions do not equip a specific target in itself as previously discussed. However, its ironical effects are likely in dialogue or visually grounded environment, which encourages us to enhance irony datasets in aforementioned ways.

Irony Explanation Irony, according to the definition, have opposite real meanings to literal meanings. However, this does not mean adding a single negation could interpret ironies well. Kumar et al. (2022) proposed a new task, sarcasm explanation in dialogue. Irony explanation might encounter more complex problems due to relatively low proportion of targets. Still, we should include irony explanation as a branch of multimodal irony processing like Desai et al. (2021).","# QUA Analysis:

1. (What is the comparison between irony and sarcasm in NLP conferences?): sent1
    1.1. (What recent improvements in sarcasm processing could motivate irony research?): sent2
        1.1.1. (How is data collected for irony research?): sent3
        1.1.2. (What algorithm has been proposed for detecting sarcastic tweets?): sent4
        1.1.3. (Why is distant supervision important for extracting ironies?): sent5
    1.2. (What is the difference between intended and perceived irony?): sent6
        1.2.1. (What dataset divides intended and perceived sarcasms?): sent7
        1.2.2. (How do state-of-the-art sarcasm detection models perform on this dataset?): sent8
        1.2.3. (What could future work focus on regarding irony?): sent9
    1.3. (What is sarcasm target identification and its relevance to irony processing?): sent10
        1.3.1. (How was target identification improved?): sent11
        1.3.2. (Why do most ironical expressions not have a specific target?): sent12
        1.3.3. (What encourages enhancement of irony datasets?): sent13
    1.4. (What is irony explanation and its challenges?): sent14
        1.4.1. (Why can't a single negation interpret ironies well?): sent15
        1.4.2. (What new task has been proposed for sarcasm explanation in dialogue?): sent16
        1.4.3. (Why might irony explanation encounter more complex problems?): sent17
        1.4.4. (Why should irony explanation be included in multimodal irony processing?): sent18",What recent improvements in sarcasm processing could motivate irony research?,"sent2, sent4, sent5, sent6, sent7, sent8, sent10, sent11, sent16","Recently we have witnessed great improvements in sarcasm processing and in this part we will discuss how new tasks in sarcasm could motivate irony research.

Shmueli et al. (2020) proposed an algorithm to detect sarcastic tweets from a thread based on exterior cue tweets.

A distant supervision based method for extracting ironies from platforms is crucial, given ironies in conversational contexts are central topic in the future.

Intended and Perceived Irony Oprea and Magdy (2019) explored how author profiling affected the perceived sarcasm (manual labelling) versus the intended sarcasm (hashtags), and verified the difference between both.

Further, Oprea and Magdy (2020) introduced iSarcasm dataset which divided intended sarcasms and perceived sarcasms.

The state-of-the-art sarcasm detection models performed obviously worse than human evaluation on this dataset.

Target Identification Sarcasm target identification was firstly proposed in Joshi et al. (2018), in which sarcasm targets were classified as one target, several targets and outside.

Patro et al. (2019) introduced sociolinguistic features and a deep learning framework, and improved target identification by a lot.

Kumar et al. (2022) proposed a new task, sarcasm explanation in dialogue.","Recent improvements in sarcasm processing could motivate irony research by introducing new tasks in sarcasm.

Shmueli et al. (2020) proposed an algorithm to detect sarcastic tweets from a thread based on exterior cue tweets.

A distant supervision-based method for extracting ironies from platforms is crucial, given that ironies in conversational contexts are a central topic for future research.

Oprea and Magdy (2019) explored how author profiling affected the perceived sarcasm (manual labeling) versus the intended sarcasm (hashtags) and verified the difference between both.

Further, Oprea and Magdy (2020) introduced the iSarcasm dataset, which divided intended sarcasms and perceived sarcasms.

The state-of-the-art sarcasm detection models performed obviously worse than human evaluation on this dataset.

Joshi et al. (2018) introduced the concept of sarcasm target identification, classifying sarcasm targets as one target, several targets, or outside.

Patro et al. (2019) introduced sociolinguistic features and a deep learning framework, significantly improving sarcasm target identification.

Kumar et al. (2022) proposed a new task, sarcasm explanation in dialogue.","Questions:

What recent improvements in sarcasm processing could motivate irony research?

Answer:

Recent improvements in sarcasm processing could motivate irony research by introducing new tasks in sarcasm.

Shmueli et al. (2020) proposed an algorithm to detect sarcastic tweets from a thread based on exterior cue tweets.

A distant supervision-based method for extracting ironies from platforms is crucial, given that ironies in conversational contexts are a central topic for future research.

Oprea and Magdy (2019) explored how author profiling affected the perceived sarcasm (manual labeling) versus the intended sarcasm (hashtags) and verified the difference between both.

Further, Oprea and Magdy (2020) introduced the iSarcasm dataset, which divided intended sarcasms and perceived sarcasms.

The state-of-the-art sarcasm detection models performed obviously worse than human evaluation on this dataset.

Joshi et al. (2018) introduced the concept of sarcasm target identification, classifying sarcasm targets as one target, several targets, or outside.

Patro et al. (2019) introduced sociolinguistic features and a deep learning framework, significantly improving sarcasm target identification.

Kumar et al. (2022) proposed a new task, sarcasm explanation in dialogue."
"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",New Tasks: Inspiration from Sarcasm,"Compared to sarcasm, irony is rarely seen as a term in NLP conferences. Recently we have witnessed great improvements in sarcasm processing and in this part we will discuss how new tasks in sarcasm could motivate irony research.

Data Collection As discussed, datasets are highly dependent on hashtags as a signal to extract ironical expressions. Shmueli et al. (2020) proposed an algorithm to detect sarcastic tweets from a thread based on exterior cue tweets. A distant supervision based method for extracting ironies from platforms is crucial, given ironies in conversational contexts are central topic in the future.

Intended and Perceived Irony Oprea and Magdy (2019) explored how author profiling affected the perceived sarcasm (manual labelling) versus the intended sarcasm (hashtags), and verified the difference between both. Further, Oprea and Magdy (2020) introduced iSarcasm dataset which divided intended sarcasms and perceived sarcasms. The state-of-the-art sarcasm detection models performed obviously worse than human evaluation on this dataset. Future work could focus on multimodal perceived and intended irony, especially across various cultures.

Target Identification Sarcasm target identification was firstly proposed in Joshi et al. (2018), in which sarcasm targets were classified as one target, several targets and outside. Patro et al. (2019) introduced sociolinguistic features and a deep learning framework, and improved target identification by a lot. For irony processing, most ironical expressions do not equip a specific target in itself as previously discussed. However, its ironical effects are likely in dialogue or visually grounded environment, which encourages us to enhance irony datasets in aforementioned ways.

Irony Explanation Irony, according to the definition, have opposite real meanings to literal meanings. However, this does not mean adding a single negation could interpret ironies well. Kumar et al. (2022) proposed a new task, sarcasm explanation in dialogue. Irony explanation might encounter more complex problems due to relatively low proportion of targets. Still, we should include irony explanation as a branch of multimodal irony processing like Desai et al. (2021).","# QUA Analysis:

1. (What is the comparison between irony and sarcasm in NLP conferences?): sent1
    1.1. (What recent improvements in sarcasm processing could motivate irony research?): sent2
        1.1.1. (How is data collected for irony research?): sent3
        1.1.2. (What algorithm has been proposed for detecting sarcastic tweets?): sent4
        1.1.3. (Why is distant supervision important for extracting ironies?): sent5
    1.2. (What is the difference between intended and perceived irony?): sent6
        1.2.1. (What dataset divides intended and perceived sarcasms?): sent7
        1.2.2. (How do state-of-the-art sarcasm detection models perform on this dataset?): sent8
        1.2.3. (What could future work focus on regarding irony?): sent9
    1.3. (What is sarcasm target identification and its relevance to irony processing?): sent10
        1.3.1. (How was target identification improved?): sent11
        1.3.2. (Why do most ironical expressions not have a specific target?): sent12
        1.3.3. (What encourages enhancement of irony datasets?): sent13
    1.4. (What is irony explanation and its challenges?): sent14
        1.4.1. (Why can't a single negation interpret ironies well?): sent15
        1.4.2. (What new task has been proposed for sarcasm explanation in dialogue?): sent16
        1.4.3. (Why might irony explanation encounter more complex problems?): sent17
        1.4.4. (Why should irony explanation be included in multimodal irony processing?): sent18",What is the difference between intended and perceived irony?,"sent6, sent7, sent8, sent9","Intended and Perceived Irony Oprea and Magdy (2019) explored how author profiling affected the perceived sarcasm (manual labelling) versus the intended sarcasm (hashtags), and verified the difference between both.

Further, Oprea and Magdy (2020) introduced iSarcasm dataset which divided intended sarcasms and perceived sarcasms.

The state-of-the-art sarcasm detection models performed obviously worse than human evaluation on this dataset.

Future work could focus on multimodal perceived and intended irony, especially across various cultures.","Intended and perceived irony differ in that intended sarcasm is often indicated by hashtags, while perceived sarcasm is determined through manual labeling (Oprea and Magdy, 2019).

Further, Oprea and Magdy (2020) introduced the iSarcasm dataset, which divided intended sarcasms and perceived sarcasms.

The state-of-the-art sarcasm detection models performed significantly worse than human evaluation on this dataset.

Future work could focus on multimodal perceived and intended irony, especially across various cultures.","Questions:

What is the difference between intended and perceived irony?

Answer:

Intended and perceived irony differ in that intended sarcasm is often indicated by hashtags, while perceived sarcasm is determined through manual labeling (Oprea and Magdy, 2019).

Further, Oprea and Magdy (2020) introduced the iSarcasm dataset, which divided intended sarcasms and perceived sarcasms.

The state-of-the-art sarcasm detection models performed significantly worse than human evaluation on this dataset.

Future work could focus on multimodal perceived and intended irony, especially across various cultures."
"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",New Tasks: Inspiration from Sarcasm,"Compared to sarcasm, irony is rarely seen as a term in NLP conferences. Recently we have witnessed great improvements in sarcasm processing and in this part we will discuss how new tasks in sarcasm could motivate irony research.

Data Collection As discussed, datasets are highly dependent on hashtags as a signal to extract ironical expressions. Shmueli et al. (2020) proposed an algorithm to detect sarcastic tweets from a thread based on exterior cue tweets. A distant supervision based method for extracting ironies from platforms is crucial, given ironies in conversational contexts are central topic in the future.

Intended and Perceived Irony Oprea and Magdy (2019) explored how author profiling affected the perceived sarcasm (manual labelling) versus the intended sarcasm (hashtags), and verified the difference between both. Further, Oprea and Magdy (2020) introduced iSarcasm dataset which divided intended sarcasms and perceived sarcasms. The state-of-the-art sarcasm detection models performed obviously worse than human evaluation on this dataset. Future work could focus on multimodal perceived and intended irony, especially across various cultures.

Target Identification Sarcasm target identification was firstly proposed in Joshi et al. (2018), in which sarcasm targets were classified as one target, several targets and outside. Patro et al. (2019) introduced sociolinguistic features and a deep learning framework, and improved target identification by a lot. For irony processing, most ironical expressions do not equip a specific target in itself as previously discussed. However, its ironical effects are likely in dialogue or visually grounded environment, which encourages us to enhance irony datasets in aforementioned ways.

Irony Explanation Irony, according to the definition, have opposite real meanings to literal meanings. However, this does not mean adding a single negation could interpret ironies well. Kumar et al. (2022) proposed a new task, sarcasm explanation in dialogue. Irony explanation might encounter more complex problems due to relatively low proportion of targets. Still, we should include irony explanation as a branch of multimodal irony processing like Desai et al. (2021).","# QUA Analysis:

1. (What is the comparison between irony and sarcasm in NLP conferences?): sent1
    1.1. (What recent improvements in sarcasm processing could motivate irony research?): sent2
        1.1.1. (How is data collected for irony research?): sent3
        1.1.2. (What algorithm has been proposed for detecting sarcastic tweets?): sent4
        1.1.3. (Why is distant supervision important for extracting ironies?): sent5
    1.2. (What is the difference between intended and perceived irony?): sent6
        1.2.1. (What dataset divides intended and perceived sarcasms?): sent7
        1.2.2. (How do state-of-the-art sarcasm detection models perform on this dataset?): sent8
        1.2.3. (What could future work focus on regarding irony?): sent9
    1.3. (What is sarcasm target identification and its relevance to irony processing?): sent10
        1.3.1. (How was target identification improved?): sent11
        1.3.2. (Why do most ironical expressions not have a specific target?): sent12
        1.3.3. (What encourages enhancement of irony datasets?): sent13
    1.4. (What is irony explanation and its challenges?): sent14
        1.4.1. (Why can't a single negation interpret ironies well?): sent15
        1.4.2. (What new task has been proposed for sarcasm explanation in dialogue?): sent16
        1.4.3. (Why might irony explanation encounter more complex problems?): sent17
        1.4.4. (Why should irony explanation be included in multimodal irony processing?): sent18",What is sarcasm target identification and its relevance to irony processing?,"sent10, sent11, sent12, sent13","Target Identification Sarcasm target identification was firstly proposed in Joshi et al. (2018), in which sarcasm targets were classified as one target, several targets and outside.

Patro et al. (2019) introduced sociolinguistic features and a deep learning framework, and improved target identification by a lot.

For irony processing, most ironical expressions do not equip a specific target in itself as previously discussed.

However, its ironical effects are likely in dialogue or visually grounded environment, which encourages us to enhance irony datasets in aforementioned ways.","Sarcasm target identification was first proposed in Joshi et al. (2018), where sarcasm targets were classified as one target, several targets, and outside.

Patro et al. (2019) introduced sociolinguistic features and a deep learning framework, significantly improving target identification.

For irony processing, most ironical expressions do not have a specific target.

However, the ironical effects are more likely to be observed in dialogue or visually grounded environments, suggesting the need to enhance irony datasets accordingly.","Questions:

What is sarcasm target identification and its relevance to irony processing?

Answer:

Sarcasm target identification was first proposed in Joshi et al. (2018), where sarcasm targets were classified as one target, several targets, and outside.

Patro et al. (2019) introduced sociolinguistic features and a deep learning framework, significantly improving target identification.

For irony processing, most ironical expressions do not have a specific target.

However, the ironical effects are more likely to be observed in dialogue or visually grounded environments, suggesting the need to enhance irony datasets accordingly."
"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",New Tasks: Inspiration from Sarcasm,"Compared to sarcasm, irony is rarely seen as a term in NLP conferences. Recently we have witnessed great improvements in sarcasm processing and in this part we will discuss how new tasks in sarcasm could motivate irony research.

Data Collection As discussed, datasets are highly dependent on hashtags as a signal to extract ironical expressions. Shmueli et al. (2020) proposed an algorithm to detect sarcastic tweets from a thread based on exterior cue tweets. A distant supervision based method for extracting ironies from platforms is crucial, given ironies in conversational contexts are central topic in the future.

Intended and Perceived Irony Oprea and Magdy (2019) explored how author profiling affected the perceived sarcasm (manual labelling) versus the intended sarcasm (hashtags), and verified the difference between both. Further, Oprea and Magdy (2020) introduced iSarcasm dataset which divided intended sarcasms and perceived sarcasms. The state-of-the-art sarcasm detection models performed obviously worse than human evaluation on this dataset. Future work could focus on multimodal perceived and intended irony, especially across various cultures.

Target Identification Sarcasm target identification was firstly proposed in Joshi et al. (2018), in which sarcasm targets were classified as one target, several targets and outside. Patro et al. (2019) introduced sociolinguistic features and a deep learning framework, and improved target identification by a lot. For irony processing, most ironical expressions do not equip a specific target in itself as previously discussed. However, its ironical effects are likely in dialogue or visually grounded environment, which encourages us to enhance irony datasets in aforementioned ways.

Irony Explanation Irony, according to the definition, have opposite real meanings to literal meanings. However, this does not mean adding a single negation could interpret ironies well. Kumar et al. (2022) proposed a new task, sarcasm explanation in dialogue. Irony explanation might encounter more complex problems due to relatively low proportion of targets. Still, we should include irony explanation as a branch of multimodal irony processing like Desai et al. (2021).","# QUA Analysis:

1. (What is the comparison between irony and sarcasm in NLP conferences?): sent1
    1.1. (What recent improvements in sarcasm processing could motivate irony research?): sent2
        1.1.1. (How is data collected for irony research?): sent3
        1.1.2. (What algorithm has been proposed for detecting sarcastic tweets?): sent4
        1.1.3. (Why is distant supervision important for extracting ironies?): sent5
    1.2. (What is the difference between intended and perceived irony?): sent6
        1.2.1. (What dataset divides intended and perceived sarcasms?): sent7
        1.2.2. (How do state-of-the-art sarcasm detection models perform on this dataset?): sent8
        1.2.3. (What could future work focus on regarding irony?): sent9
    1.3. (What is sarcasm target identification and its relevance to irony processing?): sent10
        1.3.1. (How was target identification improved?): sent11
        1.3.2. (Why do most ironical expressions not have a specific target?): sent12
        1.3.3. (What encourages enhancement of irony datasets?): sent13
    1.4. (What is irony explanation and its challenges?): sent14
        1.4.1. (Why can't a single negation interpret ironies well?): sent15
        1.4.2. (What new task has been proposed for sarcasm explanation in dialogue?): sent16
        1.4.3. (Why might irony explanation encounter more complex problems?): sent17
        1.4.4. (Why should irony explanation be included in multimodal irony processing?): sent18",What is irony explanation and its challenges?,"sent14, sent15, sent17, sent18","Irony Explanation Irony, according to the definition, have opposite real meanings to literal meanings.

However, this does not mean adding a single negation could interpret ironies well.

Irony explanation might encounter more complex problems due to relatively low proportion of targets.

Still, we should include irony explanation as a branch of multimodal irony processing like Desai et al. (2021).","Irony, according to the definition, has opposite real meanings to literal meanings.

However, this does not mean adding a single negation could interpret ironies well.

Irony explanation might encounter more complex problems due to the relatively low proportion of targets.

Irony explanation should be included as a branch of multimodal irony processing, as suggested by Desai et al. (2021).","Questions:

What is irony explanation and its challenges?

Answer:

Irony, according to the definition, has opposite real meanings to literal meanings.

However, this does not mean adding a single negation could interpret ironies well.

Irony explanation might encounter more complex problems due to the relatively low proportion of targets.

Irony explanation should be included as a branch of multimodal irony processing, as suggested by Desai et al. (2021)."
"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",Multimodal Irony Processing,"Linguistic interactions are not solely consisted of texts. Besides, facial expressions and speech communications are crucial to convey emotions and feelings. For example, Skrelin et al. (2020) reported people could classify ironies based on phonetic characteristics only. Consequently, it is conceivable that multimodal methods could help with irony detection. Schifanella et al. (2016) made the first attempt in multimodal sarcasm detection, in which they extracted posts from three multimodal social media platforms based on hashtags. Then they used SVMs and neural networks to prove the validity of visual information in enhancing sarcasm detection.

Castro et al. (2019) made a great improvement in multimodal sarcasm detection by introducing audio features into the dataset. The experiments also verified the importance of more modalities in sarcasm processing.

Future work in multimodal irony processing should include a comprehensive multimodal irony dataset based on MUStARD dataset (Castro et al., 2019) with more fine-grained annotation schemes. Additionally, most methods (Pan et al., 2020;Liu et al., 2021) explored sarcasm by introducing intermodality and intra-modality attention in singlestream setting. How double-stream multimodal pretrained models (MPMs) will encode and interact in complex discourse settings remains an interesting problem to solve.","# QUA Analysis:

1. (What is the role of non-textual elements in linguistic interactions?): sent1
    1.1. (What are the non-textual elements that contribute to conveying emotions and feelings?): sent2
        1.1.1. (Can people classify ironies based on non-textual elements?): sent3
2. (How can multimodal methods help with irony detection?): sent4
    2.1. (What are the initial attempts in multimodal sarcasm detection?): sent5
        2.1.1. (What methods were used to validate the role of visual information in sarcasm detection?): sent6
    2.2. (What improvements have been made in multimodal sarcasm detection?): sent7
        2.2.1. (What did the experiments verify regarding the importance of modalities in sarcasm processing?): sent8
3. (What should future work in multimodal irony processing focus on?): sent9
4. (What methods have been explored for sarcasm detection?): sent10
5. (What remains an interesting problem to solve in multimodal irony processing?): sent11",What are the non-textual elements that contribute to conveying emotions and feelings in linguistic interactions?,"sent1, sent2, sent3","Linguistic interactions are not solely consisted of texts.

Besides, facial expressions and speech communications are crucial to convey emotions and feelings.

For example, Skrelin et al. (2020) reported people could classify ironies based on phonetic characteristics only.","Linguistic interactions are not solely composed of texts.

Facial expressions and speech communications are crucial to conveying emotions and feelings.

For example, Skrelin et al. (2020) reported that people could classify ironies based on phonetic characteristics alone.","Questions:

What are the non-textual elements that contribute to conveying emotions and feelings in linguistic interactions?

Answer:

Linguistic interactions are not solely composed of texts.

Facial expressions and speech communications are crucial to conveying emotions and feelings.

For example, Skrelin et al. (2020) reported that people could classify ironies based on phonetic characteristics alone."
"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",Multimodal Irony Processing,"Linguistic interactions are not solely consisted of texts. Besides, facial expressions and speech communications are crucial to convey emotions and feelings. For example, Skrelin et al. (2020) reported people could classify ironies based on phonetic characteristics only. Consequently, it is conceivable that multimodal methods could help with irony detection. Schifanella et al. (2016) made the first attempt in multimodal sarcasm detection, in which they extracted posts from three multimodal social media platforms based on hashtags. Then they used SVMs and neural networks to prove the validity of visual information in enhancing sarcasm detection.

Castro et al. (2019) made a great improvement in multimodal sarcasm detection by introducing audio features into the dataset. The experiments also verified the importance of more modalities in sarcasm processing.

Future work in multimodal irony processing should include a comprehensive multimodal irony dataset based on MUStARD dataset (Castro et al., 2019) with more fine-grained annotation schemes. Additionally, most methods (Pan et al., 2020;Liu et al., 2021) explored sarcasm by introducing intermodality and intra-modality attention in singlestream setting. How double-stream multimodal pretrained models (MPMs) will encode and interact in complex discourse settings remains an interesting problem to solve.","# QUA Analysis:

1. (What is the role of non-textual elements in linguistic interactions?): sent1
    1.1. (What are the non-textual elements that contribute to conveying emotions and feelings?): sent2
        1.1.1. (Can people classify ironies based on non-textual elements?): sent3
2. (How can multimodal methods help with irony detection?): sent4
    2.1. (What are the initial attempts in multimodal sarcasm detection?): sent5
        2.1.1. (What methods were used to validate the role of visual information in sarcasm detection?): sent6
    2.2. (What improvements have been made in multimodal sarcasm detection?): sent7
        2.2.1. (What did the experiments verify regarding the importance of modalities in sarcasm processing?): sent8
3. (What should future work in multimodal irony processing focus on?): sent9
4. (What methods have been explored for sarcasm detection?): sent10
5. (What remains an interesting problem to solve in multimodal irony processing?): sent11",How can multimodal methods help with irony detection?,"sent4, sent5, sent6, sent7, sent8","Consequently, it is conceivable that multimodal methods could help with irony detection.

Schifanella et al. (2016) made the first attempt in multimodal sarcasm detection, in which they extracted posts from three multimodal social media platforms based on hashtags.

Then they used SVMs and neural networks to prove the validity of visual information in enhancing sarcasm detection.

Castro et al. (2019) made a great improvement in multimodal sarcasm detection by introducing audio features into the dataset.

The experiments also verified the importance of more modalities in sarcasm processing.","Multimodal methods could help with irony detection.

Schifanella et al. (2016) made the first attempt in multimodal sarcasm detection by extracting posts from three multimodal social media platforms based on hashtags.

They used SVMs and neural networks to demonstrate the effectiveness of visual information in enhancing sarcasm detection.

Castro et al. (2019) made a significant improvement in multimodal sarcasm detection by incorporating audio features into the dataset.

The experiments also verified the importance of incorporating more modalities in sarcasm processing.","Questions:

How can multimodal methods help with irony detection?

Answer:

Multimodal methods could help with irony detection.

Schifanella et al. (2016) made the first attempt in multimodal sarcasm detection by extracting posts from three multimodal social media platforms based on hashtags.

They used SVMs and neural networks to demonstrate the effectiveness of visual information in enhancing sarcasm detection.

Castro et al. (2019) made a significant improvement in multimodal sarcasm detection by incorporating audio features into the dataset.

The experiments also verified the importance of incorporating more modalities in sarcasm processing."
"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",Multimodal Irony Processing,"Linguistic interactions are not solely consisted of texts. Besides, facial expressions and speech communications are crucial to convey emotions and feelings. For example, Skrelin et al. (2020) reported people could classify ironies based on phonetic characteristics only. Consequently, it is conceivable that multimodal methods could help with irony detection. Schifanella et al. (2016) made the first attempt in multimodal sarcasm detection, in which they extracted posts from three multimodal social media platforms based on hashtags. Then they used SVMs and neural networks to prove the validity of visual information in enhancing sarcasm detection.

Castro et al. (2019) made a great improvement in multimodal sarcasm detection by introducing audio features into the dataset. The experiments also verified the importance of more modalities in sarcasm processing.

Future work in multimodal irony processing should include a comprehensive multimodal irony dataset based on MUStARD dataset (Castro et al., 2019) with more fine-grained annotation schemes. Additionally, most methods (Pan et al., 2020;Liu et al., 2021) explored sarcasm by introducing intermodality and intra-modality attention in singlestream setting. How double-stream multimodal pretrained models (MPMs) will encode and interact in complex discourse settings remains an interesting problem to solve.","# QUA Analysis:

1. (What is the role of non-textual elements in linguistic interactions?): sent1
    1.1. (What are the non-textual elements that contribute to conveying emotions and feelings?): sent2
        1.1.1. (Can people classify ironies based on non-textual elements?): sent3
2. (How can multimodal methods help with irony detection?): sent4
    2.1. (What are the initial attempts in multimodal sarcasm detection?): sent5
        2.1.1. (What methods were used to validate the role of visual information in sarcasm detection?): sent6
    2.2. (What improvements have been made in multimodal sarcasm detection?): sent7
        2.2.1. (What did the experiments verify regarding the importance of modalities in sarcasm processing?): sent8
3. (What should future work in multimodal irony processing focus on?): sent9
4. (What methods have been explored for sarcasm detection?): sent10
5. (What remains an interesting problem to solve in multimodal irony processing?): sent11",What improvements have been made in multimodal sarcasm detection?,"sent7, sent8, sent9","Castro et al. (2019) made a great improvement in multimodal sarcasm detection by introducing audio features into the dataset.

The experiments also verified the importance of more modalities in sarcasm processing.

Future work in multimodal irony processing should include a comprehensive multimodal irony dataset based on MUStARD dataset (Castro et al., 2019) with more fine-grained annotation schemes.","Castro et al. (2019) improved multimodal sarcasm detection by introducing audio features into the dataset.

The experiments also verified the importance of incorporating more modalities in sarcasm processing.

Future work in multimodal irony processing should include a comprehensive multimodal irony dataset with more fine-grained annotation schemes based on the MUStARD dataset (Castro et al., 2019).","Questions:

What improvements have been made in multimodal sarcasm detection?

Answer:

Castro et al. (2019) improved multimodal sarcasm detection by introducing audio features into the dataset.

The experiments also verified the importance of incorporating more modalities in sarcasm processing.

Future work in multimodal irony processing should include a comprehensive multimodal irony dataset with more fine-grained annotation schemes based on the MUStARD dataset (Castro et al., 2019)."
"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",Multimodal Irony Processing,"Linguistic interactions are not solely consisted of texts. Besides, facial expressions and speech communications are crucial to convey emotions and feelings. For example, Skrelin et al. (2020) reported people could classify ironies based on phonetic characteristics only. Consequently, it is conceivable that multimodal methods could help with irony detection. Schifanella et al. (2016) made the first attempt in multimodal sarcasm detection, in which they extracted posts from three multimodal social media platforms based on hashtags. Then they used SVMs and neural networks to prove the validity of visual information in enhancing sarcasm detection.

Castro et al. (2019) made a great improvement in multimodal sarcasm detection by introducing audio features into the dataset. The experiments also verified the importance of more modalities in sarcasm processing.

Future work in multimodal irony processing should include a comprehensive multimodal irony dataset based on MUStARD dataset (Castro et al., 2019) with more fine-grained annotation schemes. Additionally, most methods (Pan et al., 2020;Liu et al., 2021) explored sarcasm by introducing intermodality and intra-modality attention in singlestream setting. How double-stream multimodal pretrained models (MPMs) will encode and interact in complex discourse settings remains an interesting problem to solve.","# QUA Analysis:

1. (What is the role of non-textual elements in linguistic interactions?): sent1
    1.1. (What are the non-textual elements that contribute to conveying emotions and feelings?): sent2
        1.1.1. (Can people classify ironies based on non-textual elements?): sent3
2. (How can multimodal methods help with irony detection?): sent4
    2.1. (What are the initial attempts in multimodal sarcasm detection?): sent5
        2.1.1. (What methods were used to validate the role of visual information in sarcasm detection?): sent6
    2.2. (What improvements have been made in multimodal sarcasm detection?): sent7
        2.2.1. (What did the experiments verify regarding the importance of modalities in sarcasm processing?): sent8
3. (What should future work in multimodal irony processing focus on?): sent9
4. (What methods have been explored for sarcasm detection?): sent10
5. (What remains an interesting problem to solve in multimodal irony processing?): sent11",What should future work in multimodal irony processing focus on?,"sent9, sent10, sent11","Future work in multimodal irony processing should include a comprehensive multimodal irony dataset based on MUStARD dataset (Castro et al., 2019) with more fine-grained annotation schemes.

Additionally, most methods (Pan et al., 2020;Liu et al., 2021) explored sarcasm by introducing intermodality and intra-modality attention in singlestream setting.

How double-stream multimodal pretrained models (MPMs) will encode and interact in complex discourse settings remains an interesting problem to solve.","Future work in multimodal irony processing should include a comprehensive multimodal irony dataset based on the MUStARD dataset (Castro et al., 2019) with more fine-grained annotation schemes.

Additionally, most methods (Pan et al., 2020; Liu et al., 2021) explored sarcasm by introducing intermodality and intra-modality attention in a single-stream setting.

How double-stream multimodal pretrained models (MPMs) will encode and interact in complex discourse settings remains an interesting problem to solve.","Questions:

What should future work in multimodal irony processing focus on?

Answer:

Future work in multimodal irony processing should include a comprehensive multimodal irony dataset based on the MUStARD dataset (Castro et al., 2019) with more fine-grained annotation schemes.

Additionally, most methods (Pan et al., 2020; Liu et al., 2021) explored sarcasm by introducing intermodality and intra-modality attention in a single-stream setting.

How double-stream multimodal pretrained models (MPMs) will encode and interact in complex discourse settings remains an interesting problem to solve."
"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",Irony and Sarcasm,"Most of the studies saw sarcasm as a subset of irony (Tannen et al., 2005;Barbe, 1995;Kumon-Nakamura et al., 1995;Leggitt and Gibbs, 2000;Bowes and Katz, 2011). Sarcasm is often recognized as ""a nasty, mean-spirited, or just relatively negative form of verbal irony, used on occasion to enhance the negativity expressed relative to direct, non-figurative criticism"" (Colston, 2017).

One of the peculiarities of sarcasm is whether or not the speakers intend to offend the listeners. Kumon-Nakamura et al. (1995), for example, believe that sarcastic irony always conveys a negative attitude and is intended to harm the object being discussed. The non-sarcastic irony, on the other hand, can communicate either a good or negative attitude, and it is rarely meant to be hurtful. Barbe (1995) concurred that the core difference was ""hurtful"". She claimed that irony is a face-saving strategy while sarcasm is a face-threatening action. Ridicule is another feature of sarcasm. According to Lee and Katz (1998), sarcasm is closer to ridicule than irony. Their experiment revealed that sarcasm is directed at a single person, but irony is directed toward a large group of people. Haiman et al. (1998) claimed that one of the most distinguishing characteristics of sarcasm is that the literal meaning of its words is always positive. However, he did not convey his thoughts on irony. Whereas Littman and Mey (1991) viewed this topic from another angle. While there are many various forms of ironies, they believe that there is only one type of sarcasm because ""sarcasm cannot exist independently of the communication setting.""

Cognitive scientists approached the difference in experimental studies. Previous research in child language acquisition (Glenwright and Pexman, 2010) reported that children understood sarcastic criticism later than they could understand the nonliteral meanings of irony and sarcasm, implying different pragmatic purposes of irony and sarcasm. Filik et al. (2019) utilized fMRI and found out sarcasm is associated with wider activation of semantic network in human brains compared to irony.

However, most computational linguistics researchers used irony and sarcasm interchangeably, since the boundary between these two concepts is too vague for even human beings, let alone for machines. Joshi et al. (2016) and Sulis et al. (2016) verified this claim from both human annotators and computational perspectives. Although in this paper we will mainly focus on the literal irony processing and discuss the inspirations from recent research output in sarcasm processing, we aim to encourage unify sarcasm under the framework of irony via fine-grained annotation schemes.","# QUA Analysis:

1. (What is the relationship between sarcasm and irony?): sent1
    1.1. (How is sarcasm defined in relation to irony?): sent2
    1.2. (What are the peculiarities of sarcasm?): sent3
        1.2.1. (Do speakers intend to offend listeners with sarcasm?): sent4
        1.2.2. (How does non-sarcastic irony differ from sarcastic irony?): sent5
        1.2.3. (What is the core difference between sarcasm and irony?): sent6
            1.2.3.1. (How does Barbe (1995) describe the difference?): sent7
    1.3. (What other features distinguish sarcasm from irony?): sent8
        1.3.1. (How is sarcasm related to ridicule?): sent9
            1.3.1.1. (What did Lee and Katz (1998) find about sarcasm and ridicule?): sent10
        1.3.2. (What is a distinguishing characteristic of sarcasm according to Haiman et al. (1998)?): sent11
            1.3.2.1. (Did Haiman et al. (1998) discuss irony?): sent12
    1.4. (Are there different perspectives on sarcasm and irony?): sent13
        1.4.1. (What is Littman and Mey's (1991) perspective on sarcasm and irony?): sent14
2. (How do cognitive scientists approach the difference between sarcasm and irony?): sent15
    2.1. (What did previous research in child language acquisition find?): sent16
    2.2. (What did Filik et al. (2019) discover using fMRI?): sent17
3. (How do computational linguistics researchers view the relationship between sarcasm and irony?): sent18
    3.1. (What did Joshi et al. (2016) and Sulis et al. (2016) verify?): sent19
4. (What is the focus of this paper?): sent20",How is sarcasm defined in relation to irony?,"sent2, sent4, sent5, sent6, sent7","Sarcasm is often recognized as ""a nasty, mean-spirited, or just relatively negative form of verbal irony, used on occasion to enhance the negativity expressed relative to direct, non-figurative criticism"" (Colston, 2017).

Kumon-Nakamura et al. (1995), for example, believe that sarcastic irony always conveys a negative attitude and is intended to harm the object being discussed.

The non-sarcastic irony, on the other hand, can communicate either a good or negative attitude, and it is rarely meant to be hurtful.

Barbe (1995) concurred that the core difference was ""hurtful"".

She claimed that irony is a face-saving strategy while sarcasm is a face-threatening action.","Sarcasm is often recognized as ""a nasty, mean-spirited, or just relatively negative form of verbal irony, used on occasion to enhance the negativity expressed relative to direct, non-figurative criticism"" (Colston, 2017).

Kumon-Nakamura et al. (1995), for example, believe that sarcastic irony always conveys a negative attitude and is intended to harm the object being discussed.

Non-sarcastic irony, on the other hand, can communicate either a positive or negative attitude and is rarely meant to be hurtful.

Barbe (1995) concurred that the core difference was ""hurtful"".

Barbe (1995) claimed that irony is a face-saving strategy while sarcasm is a face-threatening action.","Questions:

How is sarcasm defined in relation to irony?

Answer:

Sarcasm is often recognized as ""a nasty, mean-spirited, or just relatively negative form of verbal irony, used on occasion to enhance the negativity expressed relative to direct, non-figurative criticism"" (Colston, 2017).

Kumon-Nakamura et al. (1995), for example, believe that sarcastic irony always conveys a negative attitude and is intended to harm the object being discussed.

Non-sarcastic irony, on the other hand, can communicate either a positive or negative attitude and is rarely meant to be hurtful.

Barbe (1995) concurred that the core difference was ""hurtful"".

Barbe (1995) claimed that irony is a face-saving strategy while sarcasm is a face-threatening action."
"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",Irony and Sarcasm,"Most of the studies saw sarcasm as a subset of irony (Tannen et al., 2005;Barbe, 1995;Kumon-Nakamura et al., 1995;Leggitt and Gibbs, 2000;Bowes and Katz, 2011). Sarcasm is often recognized as ""a nasty, mean-spirited, or just relatively negative form of verbal irony, used on occasion to enhance the negativity expressed relative to direct, non-figurative criticism"" (Colston, 2017).

One of the peculiarities of sarcasm is whether or not the speakers intend to offend the listeners. Kumon-Nakamura et al. (1995), for example, believe that sarcastic irony always conveys a negative attitude and is intended to harm the object being discussed. The non-sarcastic irony, on the other hand, can communicate either a good or negative attitude, and it is rarely meant to be hurtful. Barbe (1995) concurred that the core difference was ""hurtful"". She claimed that irony is a face-saving strategy while sarcasm is a face-threatening action. Ridicule is another feature of sarcasm. According to Lee and Katz (1998), sarcasm is closer to ridicule than irony. Their experiment revealed that sarcasm is directed at a single person, but irony is directed toward a large group of people. Haiman et al. (1998) claimed that one of the most distinguishing characteristics of sarcasm is that the literal meaning of its words is always positive. However, he did not convey his thoughts on irony. Whereas Littman and Mey (1991) viewed this topic from another angle. While there are many various forms of ironies, they believe that there is only one type of sarcasm because ""sarcasm cannot exist independently of the communication setting.""

Cognitive scientists approached the difference in experimental studies. Previous research in child language acquisition (Glenwright and Pexman, 2010) reported that children understood sarcastic criticism later than they could understand the nonliteral meanings of irony and sarcasm, implying different pragmatic purposes of irony and sarcasm. Filik et al. (2019) utilized fMRI and found out sarcasm is associated with wider activation of semantic network in human brains compared to irony.

However, most computational linguistics researchers used irony and sarcasm interchangeably, since the boundary between these two concepts is too vague for even human beings, let alone for machines. Joshi et al. (2016) and Sulis et al. (2016) verified this claim from both human annotators and computational perspectives. Although in this paper we will mainly focus on the literal irony processing and discuss the inspirations from recent research output in sarcasm processing, we aim to encourage unify sarcasm under the framework of irony via fine-grained annotation schemes.","# QUA Analysis:

1. (What is the relationship between sarcasm and irony?): sent1
    1.1. (How is sarcasm defined in relation to irony?): sent2
    1.2. (What are the peculiarities of sarcasm?): sent3
        1.2.1. (Do speakers intend to offend listeners with sarcasm?): sent4
        1.2.2. (How does non-sarcastic irony differ from sarcastic irony?): sent5
        1.2.3. (What is the core difference between sarcasm and irony?): sent6
            1.2.3.1. (How does Barbe (1995) describe the difference?): sent7
    1.3. (What other features distinguish sarcasm from irony?): sent8
        1.3.1. (How is sarcasm related to ridicule?): sent9
            1.3.1.1. (What did Lee and Katz (1998) find about sarcasm and ridicule?): sent10
        1.3.2. (What is a distinguishing characteristic of sarcasm according to Haiman et al. (1998)?): sent11
            1.3.2.1. (Did Haiman et al. (1998) discuss irony?): sent12
    1.4. (Are there different perspectives on sarcasm and irony?): sent13
        1.4.1. (What is Littman and Mey's (1991) perspective on sarcasm and irony?): sent14
2. (How do cognitive scientists approach the difference between sarcasm and irony?): sent15
    2.1. (What did previous research in child language acquisition find?): sent16
    2.2. (What did Filik et al. (2019) discover using fMRI?): sent17
3. (How do computational linguistics researchers view the relationship between sarcasm and irony?): sent18
    3.1. (What did Joshi et al. (2016) and Sulis et al. (2016) verify?): sent19
4. (What is the focus of this paper?): sent20",What are the peculiarities of sarcasm?,"sent3, sent4, sent8, sent9, sent10","One of the peculiarities of sarcasm is whether or not the speakers intend to offend the listeners.

Kumon-Nakamura et al. (1995), for example, believe that sarcastic irony always conveys a negative attitude and is intended to harm the object being discussed.

Ridicule is another feature of sarcasm.

According to Lee and Katz (1998), sarcasm is closer to ridicule than irony.

Their experiment revealed that sarcasm is directed at a single person, but irony is directed toward a large group of people.","One of the peculiarities of sarcasm is whether or not the speakers intend to offend the listeners.

Kumon-Nakamura et al. (1995), for example, believe that sarcastic irony always conveys a negative attitude and is intended to harm the object being discussed.

Ridicule is another feature of sarcasm.

According to Lee and Katz (1998), sarcasm is closer to ridicule than irony.

Their experiment revealed that sarcasm is directed at a single person, whereas irony is directed toward a larger group of people.","Questions:

What are the peculiarities of sarcasm?

Answer:

One of the peculiarities of sarcasm is whether or not the speakers intend to offend the listeners.

Kumon-Nakamura et al. (1995), for example, believe that sarcastic irony always conveys a negative attitude and is intended to harm the object being discussed.

Ridicule is another feature of sarcasm.

According to Lee and Katz (1998), sarcasm is closer to ridicule than irony.

Their experiment revealed that sarcasm is directed at a single person, whereas irony is directed toward a larger group of people."
"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",Irony and Sarcasm,"Most of the studies saw sarcasm as a subset of irony (Tannen et al., 2005;Barbe, 1995;Kumon-Nakamura et al., 1995;Leggitt and Gibbs, 2000;Bowes and Katz, 2011). Sarcasm is often recognized as ""a nasty, mean-spirited, or just relatively negative form of verbal irony, used on occasion to enhance the negativity expressed relative to direct, non-figurative criticism"" (Colston, 2017).

One of the peculiarities of sarcasm is whether or not the speakers intend to offend the listeners. Kumon-Nakamura et al. (1995), for example, believe that sarcastic irony always conveys a negative attitude and is intended to harm the object being discussed. The non-sarcastic irony, on the other hand, can communicate either a good or negative attitude, and it is rarely meant to be hurtful. Barbe (1995) concurred that the core difference was ""hurtful"". She claimed that irony is a face-saving strategy while sarcasm is a face-threatening action. Ridicule is another feature of sarcasm. According to Lee and Katz (1998), sarcasm is closer to ridicule than irony. Their experiment revealed that sarcasm is directed at a single person, but irony is directed toward a large group of people. Haiman et al. (1998) claimed that one of the most distinguishing characteristics of sarcasm is that the literal meaning of its words is always positive. However, he did not convey his thoughts on irony. Whereas Littman and Mey (1991) viewed this topic from another angle. While there are many various forms of ironies, they believe that there is only one type of sarcasm because ""sarcasm cannot exist independently of the communication setting.""

Cognitive scientists approached the difference in experimental studies. Previous research in child language acquisition (Glenwright and Pexman, 2010) reported that children understood sarcastic criticism later than they could understand the nonliteral meanings of irony and sarcasm, implying different pragmatic purposes of irony and sarcasm. Filik et al. (2019) utilized fMRI and found out sarcasm is associated with wider activation of semantic network in human brains compared to irony.

However, most computational linguistics researchers used irony and sarcasm interchangeably, since the boundary between these two concepts is too vague for even human beings, let alone for machines. Joshi et al. (2016) and Sulis et al. (2016) verified this claim from both human annotators and computational perspectives. Although in this paper we will mainly focus on the literal irony processing and discuss the inspirations from recent research output in sarcasm processing, we aim to encourage unify sarcasm under the framework of irony via fine-grained annotation schemes.","# QUA Analysis:

1. (What is the relationship between sarcasm and irony?): sent1
    1.1. (How is sarcasm defined in relation to irony?): sent2
    1.2. (What are the peculiarities of sarcasm?): sent3
        1.2.1. (Do speakers intend to offend listeners with sarcasm?): sent4
        1.2.2. (How does non-sarcastic irony differ from sarcastic irony?): sent5
        1.2.3. (What is the core difference between sarcasm and irony?): sent6
            1.2.3.1. (How does Barbe (1995) describe the difference?): sent7
    1.3. (What other features distinguish sarcasm from irony?): sent8
        1.3.1. (How is sarcasm related to ridicule?): sent9
            1.3.1.1. (What did Lee and Katz (1998) find about sarcasm and ridicule?): sent10
        1.3.2. (What is a distinguishing characteristic of sarcasm according to Haiman et al. (1998)?): sent11
            1.3.2.1. (Did Haiman et al. (1998) discuss irony?): sent12
    1.4. (Are there different perspectives on sarcasm and irony?): sent13
        1.4.1. (What is Littman and Mey's (1991) perspective on sarcasm and irony?): sent14
2. (How do cognitive scientists approach the difference between sarcasm and irony?): sent15
    2.1. (What did previous research in child language acquisition find?): sent16
    2.2. (What did Filik et al. (2019) discover using fMRI?): sent17
3. (How do computational linguistics researchers view the relationship between sarcasm and irony?): sent18
    3.1. (What did Joshi et al. (2016) and Sulis et al. (2016) verify?): sent19
4. (What is the focus of this paper?): sent20",How does non-sarcastic irony differ from sarcastic irony?,"sent5, sent6, sent7","The non-sarcastic irony, on the other hand, can communicate either a good or negative attitude, and it is rarely meant to be hurtful.

Barbe (1995) concurred that the core difference was ""hurtful"".

She claimed that irony is a face-saving strategy while sarcasm is a face-threatening action.","Non-sarcastic irony can communicate either a good or negative attitude and is rarely meant to be hurtful.

Barbe (1995) concurred that the core difference was ""hurtful"".

Irony is considered a face-saving strategy, whereas sarcasm is viewed as a face-threatening action.","Questions:

How does non-sarcastic irony differ from sarcastic irony?

Answer:

Non-sarcastic irony can communicate either a good or negative attitude and is rarely meant to be hurtful.

Barbe (1995) concurred that the core difference was ""hurtful"".

Irony is considered a face-saving strategy, whereas sarcasm is viewed as a face-threatening action."
"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives",Irony and Sarcasm,"Most of the studies saw sarcasm as a subset of irony (Tannen et al., 2005;Barbe, 1995;Kumon-Nakamura et al., 1995;Leggitt and Gibbs, 2000;Bowes and Katz, 2011). Sarcasm is often recognized as ""a nasty, mean-spirited, or just relatively negative form of verbal irony, used on occasion to enhance the negativity expressed relative to direct, non-figurative criticism"" (Colston, 2017).

One of the peculiarities of sarcasm is whether or not the speakers intend to offend the listeners. Kumon-Nakamura et al. (1995), for example, believe that sarcastic irony always conveys a negative attitude and is intended to harm the object being discussed. The non-sarcastic irony, on the other hand, can communicate either a good or negative attitude, and it is rarely meant to be hurtful. Barbe (1995) concurred that the core difference was ""hurtful"". She claimed that irony is a face-saving strategy while sarcasm is a face-threatening action. Ridicule is another feature of sarcasm. According to Lee and Katz (1998), sarcasm is closer to ridicule than irony. Their experiment revealed that sarcasm is directed at a single person, but irony is directed toward a large group of people. Haiman et al. (1998) claimed that one of the most distinguishing characteristics of sarcasm is that the literal meaning of its words is always positive. However, he did not convey his thoughts on irony. Whereas Littman and Mey (1991) viewed this topic from another angle. While there are many various forms of ironies, they believe that there is only one type of sarcasm because ""sarcasm cannot exist independently of the communication setting.""

Cognitive scientists approached the difference in experimental studies. Previous research in child language acquisition (Glenwright and Pexman, 2010) reported that children understood sarcastic criticism later than they could understand the nonliteral meanings of irony and sarcasm, implying different pragmatic purposes of irony and sarcasm. Filik et al. (2019) utilized fMRI and found out sarcasm is associated with wider activation of semantic network in human brains compared to irony.

However, most computational linguistics researchers used irony and sarcasm interchangeably, since the boundary between these two concepts is too vague for even human beings, let alone for machines. Joshi et al. (2016) and Sulis et al. (2016) verified this claim from both human annotators and computational perspectives. Although in this paper we will mainly focus on the literal irony processing and discuss the inspirations from recent research output in sarcasm processing, we aim to encourage unify sarcasm under the framework of irony via fine-grained annotation schemes.","# QUA Analysis:

1. (What is the relationship between sarcasm and irony?): sent1
    1.1. (How is sarcasm defined in relation to irony?): sent2
    1.2. (What are the peculiarities of sarcasm?): sent3
        1.2.1. (Do speakers intend to offend listeners with sarcasm?): sent4
        1.2.2. (How does non-sarcastic irony differ from sarcastic irony?): sent5
        1.2.3. (What is the core difference between sarcasm and irony?): sent6
            1.2.3.1. (How does Barbe (1995) describe the difference?): sent7
    1.3. (What other features distinguish sarcasm from irony?): sent8
        1.3.1. (How is sarcasm related to ridicule?): sent9
            1.3.1.1. (What did Lee and Katz (1998) find about sarcasm and ridicule?): sent10
        1.3.2. (What is a distinguishing characteristic of sarcasm according to Haiman et al. (1998)?): sent11
            1.3.2.1. (Did Haiman et al. (1998) discuss irony?): sent12
    1.4. (Are there different perspectives on sarcasm and irony?): sent13
        1.4.1. (What is Littman and Mey's (1991) perspective on sarcasm and irony?): sent14
2. (How do cognitive scientists approach the difference between sarcasm and irony?): sent15
    2.1. (What did previous research in child language acquisition find?): sent16
    2.2. (What did Filik et al. (2019) discover using fMRI?): sent17
3. (How do computational linguistics researchers view the relationship between sarcasm and irony?): sent18
    3.1. (What did Joshi et al. (2016) and Sulis et al. (2016) verify?): sent19
4. (What is the focus of this paper?): sent20",How do cognitive scientists approach the difference between sarcasm and irony?,"sent15, sent16, sent17","""Cognitive scientists approached the difference in experimental studies.

Previous research in child language acquisition (Glenwright and Pexman, 2010) reported that children understood sarcastic criticism later than they could understand the nonliteral meanings of irony and sarcasm, implying different pragmatic purposes of irony and sarcasm.

Filik et al. (2019) utilized fMRI and found out sarcasm is associated with wider activation of semantic network in human brains compared to irony.","Cognitive scientists have approached the difference between sarcasm and irony in experimental studies.

Previous research in child language acquisition (Glenwright and Pexman, 2010) reported that children understood sarcastic criticism later than they could understand the nonliteral meanings of irony and sarcasm, implying different pragmatic purposes of irony and sarcasm.

Filik et al. (2019) utilized fMRI and found that sarcasm is associated with wider activation of the semantic network in human brains compared to irony.","Questions:

How do cognitive scientists approach the difference between sarcasm and irony?

Answer:

Cognitive scientists have approached the difference between sarcasm and irony in experimental studies.

Previous research in child language acquisition (Glenwright and Pexman, 2010) reported that children understood sarcastic criticism later than they could understand the nonliteral meanings of irony and sarcasm, implying different pragmatic purposes of irony and sarcasm.

Filik et al. (2019) utilized fMRI and found that sarcasm is associated with wider activation of the semantic network in human brains compared to irony."
A Decade of Knowledge Graphs in Natural Language Processing: A Survey,Knowledge Graph Construction,"The task of entity extraction is a starting point in constructing KGs and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020). Once the relevant entities are singled out, relationships and interactions between them are found with the task of relation extraction (Zhang et al., 2019a). A lot of papers use both entity extraction and relation extraction to construct new KGs, e.g., for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018). Entity linking is a task of linking entities recognized in some text to already existing entities in KGs (Moon et al., 2018;. Since synonymous or similar entities often exist in different KGs or in different languages, entity alignment can be performed to reduce redundancy and repetition in future tasks (Gangemi et al., 2016;Chen et al., 2018). Coming up with the rules and schemes of KGs, i.e., their structure and format of knowledge presented in it, is done with the task of ontology construction (Haussmann et al., 2019).","# QUA Analysis:

1. (What is the starting point in constructing Knowledge Graphs (KGs)?): sent1
    1.1. (What happens after relevant entities are singled out?): sent2
        1.1.1. (How are new KGs constructed using entity and relation extraction?): sent3
2. (What is entity linking?): sent4
    2.1. (What is entity alignment and why is it performed?): sent4
3. (How are the rules and schemes of KGs determined?): sent5",What are the tasks involved in constructing Knowledge Graphs (KGs)?,"sent1, sent2, sent3, sent4, sent5","The task of entity extraction is a starting point in constructing KGs and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020).

Once the relevant entities are singled out, relationships and interactions between them are found with the task of relation extraction (Zhang et al., 2019a).

A lot of papers use both entity extraction and relation extraction to construct new KGs, e.g., for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018).

Entity linking is a task of linking entities recognized in some text to already existing entities in KGs (Moon et al., 2018;. Since synonymous or similar entities often exist in different KGs or in different languages, entity alignment can be performed to reduce redundancy and repetition in future tasks (Gangemi et al., 2016;Chen et al., 2018).

Coming up with the rules and schemes of KGs, i.e., their structure and format of knowledge presented in it, is done with the task of ontology construction (Haussmann et al., 2019).","Entity extraction is a starting point in constructing Knowledge Graphs (KGs) and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020).

Once the relevant entities are singled out, relationships and interactions between them are identified through the task of relation extraction (Zhang et al., 2019a).

Many papers utilize both entity extraction and relation extraction to construct new KGs, such as for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018).

Entity linking involves connecting entities recognized in text to existing entities in KGs (Moon et al., 2018). Since synonymous or similar entities often exist in different KGs or languages, entity alignment can be performed to reduce redundancy and repetition in future tasks (Gangemi et al., 2016; Chen et al., 2018).

Ontology construction involves coming up with the rules and schemes of KGs, i.e., their structure and format of knowledge presented in it (Haussmann et al., 2019).","Questions:

What are the tasks involved in constructing Knowledge Graphs (KGs)?

Answer:

Entity extraction is a starting point in constructing Knowledge Graphs (KGs) and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020).

Once the relevant entities are singled out, relationships and interactions between them are identified through the task of relation extraction (Zhang et al., 2019a).

Many papers utilize both entity extraction and relation extraction to construct new KGs, such as for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018).

Entity linking involves connecting entities recognized in text to existing entities in KGs (Moon et al., 2018). Since synonymous or similar entities often exist in different KGs or languages, entity alignment can be performed to reduce redundancy and repetition in future tasks (Gangemi et al., 2016; Chen et al., 2018).

Ontology construction involves coming up with the rules and schemes of KGs, i.e., their structure and format of knowledge presented in it (Haussmann et al., 2019)."
A Decade of Knowledge Graphs in Natural Language Processing: A Survey,Knowledge Graph Construction,"The task of entity extraction is a starting point in constructing KGs and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020). Once the relevant entities are singled out, relationships and interactions between them are found with the task of relation extraction (Zhang et al., 2019a). A lot of papers use both entity extraction and relation extraction to construct new KGs, e.g., for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018). Entity linking is a task of linking entities recognized in some text to already existing entities in KGs (Moon et al., 2018;. Since synonymous or similar entities often exist in different KGs or in different languages, entity alignment can be performed to reduce redundancy and repetition in future tasks (Gangemi et al., 2016;Chen et al., 2018). Coming up with the rules and schemes of KGs, i.e., their structure and format of knowledge presented in it, is done with the task of ontology construction (Haussmann et al., 2019).","# QUA Analysis:

1. (What is the starting point in constructing Knowledge Graphs (KGs)?): sent1
    1.1. (What happens after relevant entities are singled out?): sent2
        1.1.1. (How are new KGs constructed using entity and relation extraction?): sent3
2. (What is entity linking?): sent4
    2.1. (What is entity alignment and why is it performed?): sent4
3. (How are the rules and schemes of KGs determined?): sent5",How are new Knowledge Graphs (KGs) constructed using entity and relation extraction?,"sent1, sent2, sent3","The task of entity extraction is a starting point in constructing KGs and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020).

Once the relevant entities are singled out, relationships and interactions between them are found with the task of relation extraction (Zhang et al., 2019a).

A lot of papers use both entity extraction and relation extraction to construct new KGs, e.g., for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018).","Entity extraction is a starting point in constructing Knowledge Graphs (KGs) and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020).

Once the relevant entities are identified, relationships and interactions between them are determined through the task of relation extraction (Zhang et al., 2019a).

Many papers utilize both entity extraction and relation extraction to construct new KGs, such as for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018).","Questions:

How are new Knowledge Graphs (KGs) constructed using entity and relation extraction?

Answer:

Entity extraction is a starting point in constructing Knowledge Graphs (KGs) and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020).

Once the relevant entities are identified, relationships and interactions between them are determined through the task of relation extraction (Zhang et al., 2019a).

Many papers utilize both entity extraction and relation extraction to construct new KGs, such as for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018)."
A Decade of Knowledge Graphs in Natural Language Processing: A Survey,Knowledge Application,"Existing KGs can be used in a multitude of popular NLP tasks. Here we outline the most popular ones.

Question answering (QA) was found to be the most common NLP task using KGs. This task is typically divided into textual QA and question answering over knowledge bases (KBQA). Textual QA derives answers from unstructured documents while KBQA does so from predefined knowledge bases (Fu et al., 2020). KBQA is naturally tied to KGs while textual QA can also be approached by using KGs as a source of common-sense knowledge when answering questions. As Zhu et al. (2021) conclude, this approach is desired not only because it is helpful for generating answers, but also because it makes answers more interpretable.

Semantic search refers to ""search with meaning"", where the goal is not just to search for literal matches, but to understand the search intent and query context as well (Bast et al., 2016). This label denoted studies that use KGs for search, recommendations, and analytics. Examples are a big semantic network of everyday concepts called ConceptNet (Speer et al., 2017) and a KG of scholarly communications and the relationships, among them the Microsoft Academic Graph .

Conversational interfaces constitute another NLP field that can benefit from world knowledge contained in KGs. Zhou et al. (2018)  Natural language generation (NLG) is a subfield of NLP and computational linguistics that is concerned with models which generate natural language output from scratch. KGs are used in this subfield for producing natural language text from KGs (Koncel-Kedziorski et al., 2019), generating question-answer pairs (Reddy et al., 2017), the multi-modal task of image captioning (Lu et al., 2018), or data augmentation in low-resource settings (Sharifirad et al., 2018).

Text analysis combines various analytical NLP techniques and methods that are applied to process and understand textual data. Exemplary tasks are sentiment detection (Kumar et al., 2018), topic modeling , or word sense disambiguation .

Augmented language models are a combination of large pretrained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) with knowledge contained in KGs. Since PLMs derive their knowledge from huge amounts of unstructured training data, a rising research trend is in combining them with structured knowledge. Knowledge from KGs can be infused into language models in their input, architecture, output, or some combination thereof (Colon-Hernandez et al., 2021). Some notable examples we outline are ERNIE (Zhang et al., 2019b), COMET (Bosselut et al., 2019), K-BERT (Liu et al., 2020b), and KEPLER (Wang et al., 2021b). Table 3 shows the distribution of papers according to the different research and contribution types as defined in Table 4 and 5 in the Appendix. It shows that most papers conduct validation research, investigating new techniques or methods that have not yet been implemented in practice. A considerable number of papers, although significantly less, focus on solution proposals of approaches by demonstrating their advantages and applicability by a small example or argumentation. However, these papers usually lack a profound empirical evaluation. Secondary research accounts for only a small number of papers and is severely underrepresented in the research field of KGs in NLP. As already mentioned in Section 1 and Section 2, there is a notable lack of studies that summarize, compile, or synthesize existing research regarding KGs in NLP. Moreover, evaluation research papers that implement and evaluate approaches in an industry context are equally scarce. Opinion papers are almost non-existent.","1. What are the popular NLP tasks that use existing KGs? (sent1)
    1.1. What are the most popular NLP tasks using KGs? (sent2)
        1.1.1. What is the most common NLP task using KGs? (sent3)
            1.1.1.1. How is the QA task typically divided? (sent4)
                1.1.1.1.1. What is the difference between textual QA and KBQA? (sent5)
                1.1.1.1.2. How are KGs used in textual QA and KBQA? (sent6)
                    1.1.1.1.2.1. Why is using KGs in textual QA desired? (sent7)
    1.2. What is semantic search? (sent8)
        1.2.1. How are KGs used in semantic search? (sent9)
            1.2.1.1. What are examples of KGs used in semantic search? (sent10)
    1.3. How do conversational interfaces benefit from KGs? (sent11)
    1.4. How is NLG related to KGs? (sent12)
        1.4.1. How are KGs used in NLG? (sent13)
    1.5. What is text analysis? (sent14)
        1.5.1. What are exemplary tasks in text analysis? (sent15)
    1.6. What are augmented language models? (sent16)
        1.6.1. Why is there a trend in combining PLMs with structured knowledge? (sent17)
        1.6.2. How can knowledge from KGs be infused into language models? (sent18)
            1.6.2.1. What are notable examples of augmented language models? (sent19)
2. What does Table 3 show? (sent20)
    2.1. What does Table 3 indicate about the distribution of papers? (sent21)
        2.1.1. What do most papers focus on? (sent21)
        2.1.2. What do a considerable number of papers focus on? (sent22)
            2.1.2.1. What is a common issue with these papers? (sent23)
        2.1.3. What is the status of secondary research in the field? (sent24)
            2.1.3.1. What is the notable lack in the research field of KGs in NLP? (sent25)
        2.1.4. What is the status of evaluation research papers? (sent26)
        2.1.5. What is the status of opinion papers? (sent27)",How is the QA task typically divided in NLP using KGs?,"sent4, sent5, sent6","This task is typically divided into textual QA and question answering over knowledge bases (KBQA).

Textual QA derives answers from unstructured documents while KBQA does so from predefined knowledge bases (Fu et al., 2020).

KBQA is naturally tied to KGs while textual QA can also be approached by using KGs as a source of common-sense knowledge when answering questions.","The QA task is typically divided into textual QA and question answering over knowledge bases (KBQA).

Textual QA derives answers from unstructured documents while KBQA does so from predefined knowledge bases (Fu et al., 2020).

KBQA is naturally tied to KGs, while textual QA can also utilize KGs as a source of common-sense knowledge when answering questions.","Questions:

How is the QA task typically divided in NLP using KGs?

Answer:

The QA task is typically divided into textual QA and question answering over knowledge bases (KBQA).

Textual QA derives answers from unstructured documents while KBQA does so from predefined knowledge bases (Fu et al., 2020).

KBQA is naturally tied to KGs, while textual QA can also utilize KGs as a source of common-sense knowledge when answering questions."
A Decade of Knowledge Graphs in Natural Language Processing: A Survey,Knowledge Application,"Existing KGs can be used in a multitude of popular NLP tasks. Here we outline the most popular ones.

Question answering (QA) was found to be the most common NLP task using KGs. This task is typically divided into textual QA and question answering over knowledge bases (KBQA). Textual QA derives answers from unstructured documents while KBQA does so from predefined knowledge bases (Fu et al., 2020). KBQA is naturally tied to KGs while textual QA can also be approached by using KGs as a source of common-sense knowledge when answering questions. As Zhu et al. (2021) conclude, this approach is desired not only because it is helpful for generating answers, but also because it makes answers more interpretable.

Semantic search refers to ""search with meaning"", where the goal is not just to search for literal matches, but to understand the search intent and query context as well (Bast et al., 2016). This label denoted studies that use KGs for search, recommendations, and analytics. Examples are a big semantic network of everyday concepts called ConceptNet (Speer et al., 2017) and a KG of scholarly communications and the relationships, among them the Microsoft Academic Graph .

Conversational interfaces constitute another NLP field that can benefit from world knowledge contained in KGs. Zhou et al. (2018)  Natural language generation (NLG) is a subfield of NLP and computational linguistics that is concerned with models which generate natural language output from scratch. KGs are used in this subfield for producing natural language text from KGs (Koncel-Kedziorski et al., 2019), generating question-answer pairs (Reddy et al., 2017), the multi-modal task of image captioning (Lu et al., 2018), or data augmentation in low-resource settings (Sharifirad et al., 2018).

Text analysis combines various analytical NLP techniques and methods that are applied to process and understand textual data. Exemplary tasks are sentiment detection (Kumar et al., 2018), topic modeling , or word sense disambiguation .

Augmented language models are a combination of large pretrained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) with knowledge contained in KGs. Since PLMs derive their knowledge from huge amounts of unstructured training data, a rising research trend is in combining them with structured knowledge. Knowledge from KGs can be infused into language models in their input, architecture, output, or some combination thereof (Colon-Hernandez et al., 2021). Some notable examples we outline are ERNIE (Zhang et al., 2019b), COMET (Bosselut et al., 2019), K-BERT (Liu et al., 2020b), and KEPLER (Wang et al., 2021b). Table 3 shows the distribution of papers according to the different research and contribution types as defined in Table 4 and 5 in the Appendix. It shows that most papers conduct validation research, investigating new techniques or methods that have not yet been implemented in practice. A considerable number of papers, although significantly less, focus on solution proposals of approaches by demonstrating their advantages and applicability by a small example or argumentation. However, these papers usually lack a profound empirical evaluation. Secondary research accounts for only a small number of papers and is severely underrepresented in the research field of KGs in NLP. As already mentioned in Section 1 and Section 2, there is a notable lack of studies that summarize, compile, or synthesize existing research regarding KGs in NLP. Moreover, evaluation research papers that implement and evaluate approaches in an industry context are equally scarce. Opinion papers are almost non-existent.","1. What are the popular NLP tasks that use existing KGs? (sent1)
    1.1. What are the most popular NLP tasks using KGs? (sent2)
        1.1.1. What is the most common NLP task using KGs? (sent3)
            1.1.1.1. How is the QA task typically divided? (sent4)
                1.1.1.1.1. What is the difference between textual QA and KBQA? (sent5)
                1.1.1.1.2. How are KGs used in textual QA and KBQA? (sent6)
                    1.1.1.1.2.1. Why is using KGs in textual QA desired? (sent7)
    1.2. What is semantic search? (sent8)
        1.2.1. How are KGs used in semantic search? (sent9)
            1.2.1.1. What are examples of KGs used in semantic search? (sent10)
    1.3. How do conversational interfaces benefit from KGs? (sent11)
    1.4. How is NLG related to KGs? (sent12)
        1.4.1. How are KGs used in NLG? (sent13)
    1.5. What is text analysis? (sent14)
        1.5.1. What are exemplary tasks in text analysis? (sent15)
    1.6. What are augmented language models? (sent16)
        1.6.1. Why is there a trend in combining PLMs with structured knowledge? (sent17)
        1.6.2. How can knowledge from KGs be infused into language models? (sent18)
            1.6.2.1. What are notable examples of augmented language models? (sent19)
2. What does Table 3 show? (sent20)
    2.1. What does Table 3 indicate about the distribution of papers? (sent21)
        2.1.1. What do most papers focus on? (sent21)
        2.1.2. What do a considerable number of papers focus on? (sent22)
            2.1.2.1. What is a common issue with these papers? (sent23)
        2.1.3. What is the status of secondary research in the field? (sent24)
            2.1.3.1. What is the notable lack in the research field of KGs in NLP? (sent25)
        2.1.4. What is the status of evaluation research papers? (sent26)
        2.1.5. What is the status of opinion papers? (sent27)",What are augmented language models in the context of KGs?,"sent16, sent17, sent18","Augmented language models are a combination of large pretrained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) with knowledge contained in KGs.

Since PLMs derive their knowledge from huge amounts of unstructured training data, a rising research trend is in combining them with structured knowledge.

Knowledge from KGs can be infused into language models in their input, architecture, output, or some combination thereof (Colon-Hernandez et al., 2021).","Augmented language models are a combination of large pretrained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) with knowledge contained in KGs.

Since PLMs derive their knowledge from huge amounts of unstructured training data, a rising research trend is in combining them with structured knowledge.

Knowledge from KGs can be infused into language models in their input, architecture, output, or some combination thereof (Colon-Hernandez et al., 2021).","Questions:

What are augmented language models in the context of KGs?

Answer:

Augmented language models are a combination of large pretrained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) with knowledge contained in KGs.

Since PLMs derive their knowledge from huge amounts of unstructured training data, a rising research trend is in combining them with structured knowledge.

Knowledge from KGs can be infused into language models in their input, architecture, output, or some combination thereof (Colon-Hernandez et al., 2021)."
"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Recommendations,"To avoid harms caused by biases in KGs and their embeddings, we identify and recommend several actions for practitioners and researchers.

Transparency and Accountability KGs should by default be published with bias-sensitive documentation to facilitate transparency and accountability regarding potential risks. Data Statements (Bender and Friedman, 2018) report curation criteria, language variety, demographics of the data authors and annotators, relevant indicators of context, quality, and provenance. Datasheets for Datasets (Gebru et al., 2021) additionally state motivation, composition, preparation, distribution, and maintenance. The associated questionnaire can accompany the dataset creation process to avoid risks early on. Especially in the case of ongoing crowdsourcing efforts for encyclopedic KGs the demographic background of contributors should be reported (Demartini, 2019). Researchers using subsets of these KGs, should investigate respective data dumps for potential biases and report limitations transparently. Similarly, KG embedding models should be published with Model Cards (Mitchell et al., 2019) documenting intended use, underlying data, ethical considerations, and limitations. Stating the contact details for reporting problems and concerns establishes accountability (Mitchell et al., 2019;Gebru et al., 2021).

Improving Representativeness To tackle selection bias, data collection should aim to employ authors and annotators from diverse social groups and with varied cultural imprints. Annotations should be determined via aggregation (see Hovy and Prabhumoye, 2021). For open editable KGs, interventions like edit-a-thons are helpful to introduce more authors from underrepresented groups (Vetter et al., 2022) (e.g., the Art+Feminism campaign aims to fill the gender gap in Wikimedia knowledge bases 8 ). In order for such interventions to take effect, research must update data bases and benchmarks frequently (see Koch et al., 2021). In addition, the timeliness of encyclopedic data is necessary to avoid perpetuating historic biases.

Tackling Algorithmic Bias Evaluation and prevention of harmful biases must become part of the development pipeline (Stanczak and Augenstein, 2021). Algorithmic biases are best evaluated with a combination of multiple quantitative (Section 5) and qualitative measures (Kraft et al., 2022;Dev et al., 2021), considering multiple demographic dimensions (beyond gender and occupation). Evaluating the content of attributions in light of social discourse and the intended use of a technology facilitates an assessment of potential harms (Selbst et al., 2019). Downstream task bias may exist independently from a measured embedding bias (Goldfarb-Tarrant et al., 2021), therefore a taskand context-oriented evaluation is preferred (Section 6). We have presented several bias-mitigating strategies for different KGEs, which might alleviate the issue in some cases (Section 7). However, more research is needed to establish more effective and robust mitigation methods, as well as metrics used to evaluate their impact (Gonen and Goldberg, 2019;Blodgett et al., 2020).","1. (What actions are recommended to avoid harms caused by biases in KGs and their embeddings?): sent1
    1.1. (What actions are recommended for transparency and accountability?): sent2
        1.1.1. (What should be included in bias-sensitive documentation?): sent3, sent4
            1.1.1.1. (What do Data Statements report?): sent3
            1.1.1.2. (What do Datasheets for Datasets state?): sent4
        1.1.2. (How can risks be avoided early on in the dataset creation process?): sent5
        1.1.3. (What should be reported in ongoing crowdsourcing efforts for encyclopedic KGs?): sent6
        1.1.4. (What should researchers do when using subsets of KGs?): sent7
        1.1.5. (What documentation should accompany KG embedding models?): sent8
        1.1.6. (How can accountability be established?): sent9
    1.2. (What actions are recommended for improving representativeness?): sent10
        1.2.1. (How should data collection be conducted to tackle selection bias?): sent10
        1.2.2. (How should annotations be determined?): sent11
        1.2.3. (What interventions are helpful for open editable KGs?): sent12
        1.2.4. (What must research do for interventions to take effect?): sent13
        1.2.5. (Why is the timeliness of encyclopedic data necessary?): sent14
    1.3. (What actions are recommended for tackling algorithmic bias?): sent15
        1.3.1. (How should algorithmic biases be evaluated?): sent16
        1.3.2. (How can potential harms be assessed?): sent17
        1.3.3. (Why is a task- and context-oriented evaluation preferred?): sent18
        1.3.4. (What bias-mitigating strategies have been presented?): sent19
        1.3.5. (What is needed to establish more effective and robust mitigation methods?): sent20",What actions are recommended to avoid harms caused by biases in knowledge graphs and their embeddings?,"sent1, sent2, sent10, sent15","To avoid harms caused by biases in KGs and their embeddings, we identify and recommend several actions for practitioners and researchers.

Transparency and Accountability KGs should by default be published with bias-sensitive documentation to facilitate transparency and accountability regarding potential risks.

Improving Representativeness To tackle selection bias, data collection should aim to employ authors and annotators from diverse social groups and with varied cultural imprints.

Tackling Algorithmic Bias Evaluation and prevention of harmful biases must become part of the development pipeline (Stanczak and Augenstein, 2021).","To avoid harms caused by biases in KGs and their embeddings, several actions are recommended for practitioners and researchers.

KGs should by default be published with bias-sensitive documentation to facilitate transparency and accountability regarding potential risks.

Improving representativeness is crucial; data collection should aim to employ authors and annotators from diverse social groups and with varied cultural imprints.

Evaluation and prevention of harmful biases must become part of the development pipeline (Stanczak and Augenstein, 2021).","Questions:

What actions are recommended to avoid harms caused by biases in knowledge graphs and their embeddings?

Answer:

To avoid harms caused by biases in KGs and their embeddings, several actions are recommended for practitioners and researchers.

KGs should by default be published with bias-sensitive documentation to facilitate transparency and accountability regarding potential risks.

Improving representativeness is crucial; data collection should aim to employ authors and annotators from diverse social groups and with varied cultural imprints.

Evaluation and prevention of harmful biases must become part of the development pipeline (Stanczak and Augenstein, 2021)."
"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Recommendations,"To avoid harms caused by biases in KGs and their embeddings, we identify and recommend several actions for practitioners and researchers.

Transparency and Accountability KGs should by default be published with bias-sensitive documentation to facilitate transparency and accountability regarding potential risks. Data Statements (Bender and Friedman, 2018) report curation criteria, language variety, demographics of the data authors and annotators, relevant indicators of context, quality, and provenance. Datasheets for Datasets (Gebru et al., 2021) additionally state motivation, composition, preparation, distribution, and maintenance. The associated questionnaire can accompany the dataset creation process to avoid risks early on. Especially in the case of ongoing crowdsourcing efforts for encyclopedic KGs the demographic background of contributors should be reported (Demartini, 2019). Researchers using subsets of these KGs, should investigate respective data dumps for potential biases and report limitations transparently. Similarly, KG embedding models should be published with Model Cards (Mitchell et al., 2019) documenting intended use, underlying data, ethical considerations, and limitations. Stating the contact details for reporting problems and concerns establishes accountability (Mitchell et al., 2019;Gebru et al., 2021).

Improving Representativeness To tackle selection bias, data collection should aim to employ authors and annotators from diverse social groups and with varied cultural imprints. Annotations should be determined via aggregation (see Hovy and Prabhumoye, 2021). For open editable KGs, interventions like edit-a-thons are helpful to introduce more authors from underrepresented groups (Vetter et al., 2022) (e.g., the Art+Feminism campaign aims to fill the gender gap in Wikimedia knowledge bases 8 ). In order for such interventions to take effect, research must update data bases and benchmarks frequently (see Koch et al., 2021). In addition, the timeliness of encyclopedic data is necessary to avoid perpetuating historic biases.

Tackling Algorithmic Bias Evaluation and prevention of harmful biases must become part of the development pipeline (Stanczak and Augenstein, 2021). Algorithmic biases are best evaluated with a combination of multiple quantitative (Section 5) and qualitative measures (Kraft et al., 2022;Dev et al., 2021), considering multiple demographic dimensions (beyond gender and occupation). Evaluating the content of attributions in light of social discourse and the intended use of a technology facilitates an assessment of potential harms (Selbst et al., 2019). Downstream task bias may exist independently from a measured embedding bias (Goldfarb-Tarrant et al., 2021), therefore a taskand context-oriented evaluation is preferred (Section 6). We have presented several bias-mitigating strategies for different KGEs, which might alleviate the issue in some cases (Section 7). However, more research is needed to establish more effective and robust mitigation methods, as well as metrics used to evaluate their impact (Gonen and Goldberg, 2019;Blodgett et al., 2020).","1. (What actions are recommended to avoid harms caused by biases in KGs and their embeddings?): sent1
    1.1. (What actions are recommended for transparency and accountability?): sent2
        1.1.1. (What should be included in bias-sensitive documentation?): sent3, sent4
            1.1.1.1. (What do Data Statements report?): sent3
            1.1.1.2. (What do Datasheets for Datasets state?): sent4
        1.1.2. (How can risks be avoided early on in the dataset creation process?): sent5
        1.1.3. (What should be reported in ongoing crowdsourcing efforts for encyclopedic KGs?): sent6
        1.1.4. (What should researchers do when using subsets of KGs?): sent7
        1.1.5. (What documentation should accompany KG embedding models?): sent8
        1.1.6. (How can accountability be established?): sent9
    1.2. (What actions are recommended for improving representativeness?): sent10
        1.2.1. (How should data collection be conducted to tackle selection bias?): sent10
        1.2.2. (How should annotations be determined?): sent11
        1.2.3. (What interventions are helpful for open editable KGs?): sent12
        1.2.4. (What must research do for interventions to take effect?): sent13
        1.2.5. (Why is the timeliness of encyclopedic data necessary?): sent14
    1.3. (What actions are recommended for tackling algorithmic bias?): sent15
        1.3.1. (How should algorithmic biases be evaluated?): sent16
        1.3.2. (How can potential harms be assessed?): sent17
        1.3.3. (Why is a task- and context-oriented evaluation preferred?): sent18
        1.3.4. (What bias-mitigating strategies have been presented?): sent19
        1.3.5. (What is needed to establish more effective and robust mitigation methods?): sent20",How can risks be avoided early on in the dataset creation process?,"sent5, sent6, sent7","The associated questionnaire can accompany the dataset creation process to avoid risks early on.

Especially in the case of ongoing crowdsourcing efforts for encyclopedic KGs the demographic background of contributors should be reported (Demartini, 2019).

Researchers using subsets of these KGs, should investigate respective data dumps for potential biases and report limitations transparently.","A questionnaire can accompany the dataset creation process to avoid risks early on.

Especially in the case of ongoing crowdsourcing efforts for encyclopedic knowledge graphs, the demographic background of contributors should be reported (Demartini, 2019).

Researchers using subsets of these knowledge graphs should investigate respective data dumps for potential biases and report limitations transparently.","Questions:

How can risks be avoided early on in the dataset creation process?

Answer:

A questionnaire can accompany the dataset creation process to avoid risks early on.

Especially in the case of ongoing crowdsourcing efforts for encyclopedic knowledge graphs, the demographic background of contributors should be reported (Demartini, 2019).

Researchers using subsets of these knowledge graphs should investigate respective data dumps for potential biases and report limitations transparently."
"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Recommendations,"To avoid harms caused by biases in KGs and their embeddings, we identify and recommend several actions for practitioners and researchers.

Transparency and Accountability KGs should by default be published with bias-sensitive documentation to facilitate transparency and accountability regarding potential risks. Data Statements (Bender and Friedman, 2018) report curation criteria, language variety, demographics of the data authors and annotators, relevant indicators of context, quality, and provenance. Datasheets for Datasets (Gebru et al., 2021) additionally state motivation, composition, preparation, distribution, and maintenance. The associated questionnaire can accompany the dataset creation process to avoid risks early on. Especially in the case of ongoing crowdsourcing efforts for encyclopedic KGs the demographic background of contributors should be reported (Demartini, 2019). Researchers using subsets of these KGs, should investigate respective data dumps for potential biases and report limitations transparently. Similarly, KG embedding models should be published with Model Cards (Mitchell et al., 2019) documenting intended use, underlying data, ethical considerations, and limitations. Stating the contact details for reporting problems and concerns establishes accountability (Mitchell et al., 2019;Gebru et al., 2021).

Improving Representativeness To tackle selection bias, data collection should aim to employ authors and annotators from diverse social groups and with varied cultural imprints. Annotations should be determined via aggregation (see Hovy and Prabhumoye, 2021). For open editable KGs, interventions like edit-a-thons are helpful to introduce more authors from underrepresented groups (Vetter et al., 2022) (e.g., the Art+Feminism campaign aims to fill the gender gap in Wikimedia knowledge bases 8 ). In order for such interventions to take effect, research must update data bases and benchmarks frequently (see Koch et al., 2021). In addition, the timeliness of encyclopedic data is necessary to avoid perpetuating historic biases.

Tackling Algorithmic Bias Evaluation and prevention of harmful biases must become part of the development pipeline (Stanczak and Augenstein, 2021). Algorithmic biases are best evaluated with a combination of multiple quantitative (Section 5) and qualitative measures (Kraft et al., 2022;Dev et al., 2021), considering multiple demographic dimensions (beyond gender and occupation). Evaluating the content of attributions in light of social discourse and the intended use of a technology facilitates an assessment of potential harms (Selbst et al., 2019). Downstream task bias may exist independently from a measured embedding bias (Goldfarb-Tarrant et al., 2021), therefore a taskand context-oriented evaluation is preferred (Section 6). We have presented several bias-mitigating strategies for different KGEs, which might alleviate the issue in some cases (Section 7). However, more research is needed to establish more effective and robust mitigation methods, as well as metrics used to evaluate their impact (Gonen and Goldberg, 2019;Blodgett et al., 2020).","1. (What actions are recommended to avoid harms caused by biases in KGs and their embeddings?): sent1
    1.1. (What actions are recommended for transparency and accountability?): sent2
        1.1.1. (What should be included in bias-sensitive documentation?): sent3, sent4
            1.1.1.1. (What do Data Statements report?): sent3
            1.1.1.2. (What do Datasheets for Datasets state?): sent4
        1.1.2. (How can risks be avoided early on in the dataset creation process?): sent5
        1.1.3. (What should be reported in ongoing crowdsourcing efforts for encyclopedic KGs?): sent6
        1.1.4. (What should researchers do when using subsets of KGs?): sent7
        1.1.5. (What documentation should accompany KG embedding models?): sent8
        1.1.6. (How can accountability be established?): sent9
    1.2. (What actions are recommended for improving representativeness?): sent10
        1.2.1. (How should data collection be conducted to tackle selection bias?): sent10
        1.2.2. (How should annotations be determined?): sent11
        1.2.3. (What interventions are helpful for open editable KGs?): sent12
        1.2.4. (What must research do for interventions to take effect?): sent13
        1.2.5. (Why is the timeliness of encyclopedic data necessary?): sent14
    1.3. (What actions are recommended for tackling algorithmic bias?): sent15
        1.3.1. (How should algorithmic biases be evaluated?): sent16
        1.3.2. (How can potential harms be assessed?): sent17
        1.3.3. (Why is a task- and context-oriented evaluation preferred?): sent18
        1.3.4. (What bias-mitigating strategies have been presented?): sent19
        1.3.5. (What is needed to establish more effective and robust mitigation methods?): sent20",How should data collection be conducted to tackle selection bias in knowledge graphs?,"sent10, sent11, sent12","Improving Representativeness To tackle selection bias, data collection should aim to employ authors and annotators from diverse social groups and with varied cultural imprints.

Annotations should be determined via aggregation (see Hovy and Prabhumoye, 2021).

For open editable KGs, interventions like edit-a-thons are helpful to introduce more authors from underrepresented groups (Vetter et al., 2022) (e.g., the Art+Feminism campaign aims to fill the gender gap in Wikimedia knowledge bases 8 ).","To tackle selection bias, data collection should aim to employ authors and annotators from diverse social groups and with varied cultural imprints.

Annotations should be determined via aggregation (Hovy and Prabhumoye, 2021).

For open editable knowledge graphs, interventions like edit-a-thons are helpful to introduce more authors from underrepresented groups (Vetter et al., 2022).","Questions:

How should data collection be conducted to tackle selection bias in knowledge graphs?

Answer:

To tackle selection bias, data collection should aim to employ authors and annotators from diverse social groups and with varied cultural imprints.

Annotations should be determined via aggregation (Hovy and Prabhumoye, 2021).

For open editable knowledge graphs, interventions like edit-a-thons are helpful to introduce more authors from underrepresented groups (Vetter et al., 2022)."
"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",Recommendations,"To avoid harms caused by biases in KGs and their embeddings, we identify and recommend several actions for practitioners and researchers.

Transparency and Accountability KGs should by default be published with bias-sensitive documentation to facilitate transparency and accountability regarding potential risks. Data Statements (Bender and Friedman, 2018) report curation criteria, language variety, demographics of the data authors and annotators, relevant indicators of context, quality, and provenance. Datasheets for Datasets (Gebru et al., 2021) additionally state motivation, composition, preparation, distribution, and maintenance. The associated questionnaire can accompany the dataset creation process to avoid risks early on. Especially in the case of ongoing crowdsourcing efforts for encyclopedic KGs the demographic background of contributors should be reported (Demartini, 2019). Researchers using subsets of these KGs, should investigate respective data dumps for potential biases and report limitations transparently. Similarly, KG embedding models should be published with Model Cards (Mitchell et al., 2019) documenting intended use, underlying data, ethical considerations, and limitations. Stating the contact details for reporting problems and concerns establishes accountability (Mitchell et al., 2019;Gebru et al., 2021).

Improving Representativeness To tackle selection bias, data collection should aim to employ authors and annotators from diverse social groups and with varied cultural imprints. Annotations should be determined via aggregation (see Hovy and Prabhumoye, 2021). For open editable KGs, interventions like edit-a-thons are helpful to introduce more authors from underrepresented groups (Vetter et al., 2022) (e.g., the Art+Feminism campaign aims to fill the gender gap in Wikimedia knowledge bases 8 ). In order for such interventions to take effect, research must update data bases and benchmarks frequently (see Koch et al., 2021). In addition, the timeliness of encyclopedic data is necessary to avoid perpetuating historic biases.

Tackling Algorithmic Bias Evaluation and prevention of harmful biases must become part of the development pipeline (Stanczak and Augenstein, 2021). Algorithmic biases are best evaluated with a combination of multiple quantitative (Section 5) and qualitative measures (Kraft et al., 2022;Dev et al., 2021), considering multiple demographic dimensions (beyond gender and occupation). Evaluating the content of attributions in light of social discourse and the intended use of a technology facilitates an assessment of potential harms (Selbst et al., 2019). Downstream task bias may exist independently from a measured embedding bias (Goldfarb-Tarrant et al., 2021), therefore a taskand context-oriented evaluation is preferred (Section 6). We have presented several bias-mitigating strategies for different KGEs, which might alleviate the issue in some cases (Section 7). However, more research is needed to establish more effective and robust mitigation methods, as well as metrics used to evaluate their impact (Gonen and Goldberg, 2019;Blodgett et al., 2020).","1. (What actions are recommended to avoid harms caused by biases in KGs and their embeddings?): sent1
    1.1. (What actions are recommended for transparency and accountability?): sent2
        1.1.1. (What should be included in bias-sensitive documentation?): sent3, sent4
            1.1.1.1. (What do Data Statements report?): sent3
            1.1.1.2. (What do Datasheets for Datasets state?): sent4
        1.1.2. (How can risks be avoided early on in the dataset creation process?): sent5
        1.1.3. (What should be reported in ongoing crowdsourcing efforts for encyclopedic KGs?): sent6
        1.1.4. (What should researchers do when using subsets of KGs?): sent7
        1.1.5. (What documentation should accompany KG embedding models?): sent8
        1.1.6. (How can accountability be established?): sent9
    1.2. (What actions are recommended for improving representativeness?): sent10
        1.2.1. (How should data collection be conducted to tackle selection bias?): sent10
        1.2.2. (How should annotations be determined?): sent11
        1.2.3. (What interventions are helpful for open editable KGs?): sent12
        1.2.4. (What must research do for interventions to take effect?): sent13
        1.2.5. (Why is the timeliness of encyclopedic data necessary?): sent14
    1.3. (What actions are recommended for tackling algorithmic bias?): sent15
        1.3.1. (How should algorithmic biases be evaluated?): sent16
        1.3.2. (How can potential harms be assessed?): sent17
        1.3.3. (Why is a task- and context-oriented evaluation preferred?): sent18
        1.3.4. (What bias-mitigating strategies have been presented?): sent19
        1.3.5. (What is needed to establish more effective and robust mitigation methods?): sent20",How should algorithmic biases be evaluated in knowledge graph embeddings?,"sent16, sent17, sent18","Algorithmic biases are best evaluated with a combination of multiple quantitative (Section 5) and qualitative measures (Kraft et al., 2022;Dev et al., 2021), considering multiple demographic dimensions (beyond gender and occupation).

Evaluating the content of attributions in light of social discourse and the intended use of a technology facilitates an assessment of potential harms (Selbst et al., 2019).

Downstream task bias may exist independently from a measured embedding bias (Goldfarb-Tarrant et al., 2021), therefore a taskand context-oriented evaluation is preferred (Section 6).","Algorithmic biases in knowledge graph embeddings are best evaluated with a combination of multiple quantitative and qualitative measures (Kraft et al., 2022; Dev et al., 2021), considering multiple demographic dimensions beyond gender and occupation.

Evaluating the content of attributions in light of social discourse and the intended use of a technology facilitates an assessment of potential harms (Selbst et al., 2019).

Downstream task bias may exist independently from a measured embedding bias (Goldfarb-Tarrant et al., 2021), therefore a task- and context-oriented evaluation is preferred.","Questions:

How should algorithmic biases be evaluated in knowledge graph embeddings?

Answer:

Algorithmic biases in knowledge graph embeddings are best evaluated with a combination of multiple quantitative and qualitative measures (Kraft et al., 2022; Dev et al., 2021), considering multiple demographic dimensions beyond gender and occupation.

Evaluating the content of attributions in light of social discourse and the intended use of a technology facilitates an assessment of potential harms (Selbst et al., 2019).

Downstream task bias may exist independently from a measured embedding bias (Goldfarb-Tarrant et al., 2021), therefore a task- and context-oriented evaluation is preferred."
A Survey of Automatic Text Summarization using Graph Neural Networks,Related Surveys,"As the application of GNNs to ATS is rather novel, to the best of our knowledge there is only one survey on the topic. The survey by  gives an introduction to the topic. However, it does not provide much detail on GNNs in the context of ATS, nor does it cover all models in the space with a taxonomy. As for GNNs themselves, there exists a large number of surveys on GNNs as a technology. In particular, we want to highlight the surveys by  and  which provide a general overview on GNNs and their applications in different fields. The survey by Abadal et al.

(2021) provides more technical and theoretical details on GNNs. The survey by Wu et al. (2021a) is of particular interest as it focuses on the usage of GNNs for NLP as a domain. Additionally, we want to note here the survey by Wu et al. (2020a) on the usage of GNNs in recommender systems and the review by Malekzadeh et al. (2021) on the usage of GNNs for text classification.

For a more theoretical approach we point the reader to the analysis by Xu et al. (2018) which establishes a number of important theoretical properties for GNNs. An analysis of the Vapnik-Chervonenkis (VC) dimension of GNNs was performed by Veličković et al. (2018).

In general, the contributions of our survey are as follows:

1. We provide a detailed explanation of GNNs in the context of ATS.

2. We introduce a simple taxonomy for GNN models used for ATS.

3. We provide a comprehensive overview of innovative GNN models for ATS and discuss further directions for future research.

The rest of survey is structured as follows. First we will give a comprehensive explanation of GNNs in the context of ATS. Next, we will explore a number of interesting and innovative models. Finally, we will finish with a conclusion and an outlook on the future usage of GNNs for ATS.","# QUA Analysis:

1. (What is the current state of surveys on the application of GNNs to ATS?): sent1
    1.1. (What does the existing survey on GNNs and ATS cover?): sent2
    1.2. (What are the limitations of the existing survey on GNNs and ATS?): sent3
2. (What is the state of surveys on GNNs as a technology in general?): sent4
    2.1. (Which surveys provide a general overview of GNNs and their applications?): sent5
    2.2. (Which survey provides more technical and theoretical details on GNNs?): sent6
    2.3. (Which survey focuses on the usage of GNNs for NLP?): sent7
    2.4. (Which surveys cover the usage of GNNs in specific domains like recommender systems and text classification?): sent8
    2.5. (Which analysis provides important theoretical properties for GNNs?): sent9
    2.6. (Which analysis examines the VC dimension of GNNs?): sent10
3. (What are the contributions of the current survey?): sent11
    3.1. (What detailed explanation does the current survey provide?): sent12
    3.2. (What taxonomy does the current survey introduce?): sent13
    3.3. (What overview and discussion does the current survey provide?): sent14
4. (How is the rest of the survey structured?): sent15
    4.1. (What will be covered first in the survey?): sent16
    4.2. (What will be explored next in the survey?): sent17
    4.3. (What will be the final part of the survey?): sent18",What is the current state of surveys on the application of GNNs to ATS?,"sent1, sent2, sent3","As the application of GNNs to ATS is rather novel, to the best of our knowledge there is only one survey on the topic.

The survey by  gives an introduction to the topic.

However, it does not provide much detail on GNNs in the context of ATS, nor does it cover all models in the space with a taxonomy.","As the application of GNNs to ATS is rather novel, there is only one survey on the topic to the best of current knowledge.

The survey provides an introduction to the topic.

However, it does not provide much detail on GNNs in the context of ATS, nor does it cover all models in the space with a taxonomy.","Questions:

What is the current state of surveys on the application of GNNs to ATS?

Answer:

As the application of GNNs to ATS is rather novel, there is only one survey on the topic to the best of current knowledge.

The survey provides an introduction to the topic.

However, it does not provide much detail on GNNs in the context of ATS, nor does it cover all models in the space with a taxonomy."
A Survey of Automatic Text Summarization using Graph Neural Networks,Related Surveys,"As the application of GNNs to ATS is rather novel, to the best of our knowledge there is only one survey on the topic. The survey by  gives an introduction to the topic. However, it does not provide much detail on GNNs in the context of ATS, nor does it cover all models in the space with a taxonomy. As for GNNs themselves, there exists a large number of surveys on GNNs as a technology. In particular, we want to highlight the surveys by  and  which provide a general overview on GNNs and their applications in different fields. The survey by Abadal et al.

(2021) provides more technical and theoretical details on GNNs. The survey by Wu et al. (2021a) is of particular interest as it focuses on the usage of GNNs for NLP as a domain. Additionally, we want to note here the survey by Wu et al. (2020a) on the usage of GNNs in recommender systems and the review by Malekzadeh et al. (2021) on the usage of GNNs for text classification.

For a more theoretical approach we point the reader to the analysis by Xu et al. (2018) which establishes a number of important theoretical properties for GNNs. An analysis of the Vapnik-Chervonenkis (VC) dimension of GNNs was performed by Veličković et al. (2018).

In general, the contributions of our survey are as follows:

1. We provide a detailed explanation of GNNs in the context of ATS.

2. We introduce a simple taxonomy for GNN models used for ATS.

3. We provide a comprehensive overview of innovative GNN models for ATS and discuss further directions for future research.

The rest of survey is structured as follows. First we will give a comprehensive explanation of GNNs in the context of ATS. Next, we will explore a number of interesting and innovative models. Finally, we will finish with a conclusion and an outlook on the future usage of GNNs for ATS.","# QUA Analysis:

1. (What is the current state of surveys on the application of GNNs to ATS?): sent1
    1.1. (What does the existing survey on GNNs and ATS cover?): sent2
    1.2. (What are the limitations of the existing survey on GNNs and ATS?): sent3
2. (What is the state of surveys on GNNs as a technology in general?): sent4
    2.1. (Which surveys provide a general overview of GNNs and their applications?): sent5
    2.2. (Which survey provides more technical and theoretical details on GNNs?): sent6
    2.3. (Which survey focuses on the usage of GNNs for NLP?): sent7
    2.4. (Which surveys cover the usage of GNNs in specific domains like recommender systems and text classification?): sent8
    2.5. (Which analysis provides important theoretical properties for GNNs?): sent9
    2.6. (Which analysis examines the VC dimension of GNNs?): sent10
3. (What are the contributions of the current survey?): sent11
    3.1. (What detailed explanation does the current survey provide?): sent12
    3.2. (What taxonomy does the current survey introduce?): sent13
    3.3. (What overview and discussion does the current survey provide?): sent14
4. (How is the rest of the survey structured?): sent15
    4.1. (What will be covered first in the survey?): sent16
    4.2. (What will be explored next in the survey?): sent17
    4.3. (What will be the final part of the survey?): sent18",What is the state of surveys on GNNs as a technology in general?,"sent4, sent5, sent6, sent7, sent8, sent9, sent10","As for GNNs themselves, there exists a large number of surveys on GNNs as a technology.

In particular, we want to highlight the surveys by  and  which provide a general overview on GNNs and their applications in different fields.

The survey by Abadal et al.(2021) provides more technical and theoretical details on GNNs.

The survey by Wu et al. (2021a) is of particular interest as it focuses on the usage of GNNs for NLP as a domain.

Additionally, we want to note here the survey by Wu et al. (2020a) on the usage of GNNs in recommender systems and the review by Malekzadeh et al. (2021) on the usage of GNNs for text classification.

For a more theoretical approach we point the reader to the analysis by Xu et al. (2018) which establishes a number of important theoretical properties for GNNs.

An analysis of the Vapnik-Chervonenkis (VC) dimension of GNNs was performed by Veličković et al. (2018).","There exists a large number of surveys on GNNs as a technology.

In particular, the surveys by various authors provide a general overview of GNNs and their applications in different fields.

The survey by Abadal et al. (2021) provides more technical and theoretical details on GNNs.

The survey by Wu et al. (2021a) is of particular interest as it focuses on the usage of GNNs for NLP as a domain.

Additionally, the survey by Wu et al. (2020a) on the usage of GNNs in recommender systems and the review by Malekzadeh et al. (2021) on the usage of GNNs for text classification are noteworthy.

For a more theoretical approach, the analysis by Xu et al. (2018) establishes a number of important theoretical properties for GNNs.

An analysis of the Vapnik-Chervonenkis (VC) dimension of GNNs was performed by Veličković et al. (2018).","Questions:

What is the state of surveys on GNNs as a technology in general?

Answer:

There exists a large number of surveys on GNNs as a technology.

In particular, the surveys by various authors provide a general overview of GNNs and their applications in different fields.

The survey by Abadal et al. (2021) provides more technical and theoretical details on GNNs.

The survey by Wu et al. (2021a) is of particular interest as it focuses on the usage of GNNs for NLP as a domain.

Additionally, the survey by Wu et al. (2020a) on the usage of GNNs in recommender systems and the review by Malekzadeh et al. (2021) on the usage of GNNs for text classification are noteworthy.

For a more theoretical approach, the analysis by Xu et al. (2018) establishes a number of important theoretical properties for GNNs.

An analysis of the Vapnik-Chervonenkis (VC) dimension of GNNs was performed by Veličković et al. (2018)."
A Survey of Automatic Text Summarization using Graph Neural Networks,Related Surveys,"As the application of GNNs to ATS is rather novel, to the best of our knowledge there is only one survey on the topic. The survey by  gives an introduction to the topic. However, it does not provide much detail on GNNs in the context of ATS, nor does it cover all models in the space with a taxonomy. As for GNNs themselves, there exists a large number of surveys on GNNs as a technology. In particular, we want to highlight the surveys by  and  which provide a general overview on GNNs and their applications in different fields. The survey by Abadal et al.

(2021) provides more technical and theoretical details on GNNs. The survey by Wu et al. (2021a) is of particular interest as it focuses on the usage of GNNs for NLP as a domain. Additionally, we want to note here the survey by Wu et al. (2020a) on the usage of GNNs in recommender systems and the review by Malekzadeh et al. (2021) on the usage of GNNs for text classification.

For a more theoretical approach we point the reader to the analysis by Xu et al. (2018) which establishes a number of important theoretical properties for GNNs. An analysis of the Vapnik-Chervonenkis (VC) dimension of GNNs was performed by Veličković et al. (2018).

In general, the contributions of our survey are as follows:

1. We provide a detailed explanation of GNNs in the context of ATS.

2. We introduce a simple taxonomy for GNN models used for ATS.

3. We provide a comprehensive overview of innovative GNN models for ATS and discuss further directions for future research.

The rest of survey is structured as follows. First we will give a comprehensive explanation of GNNs in the context of ATS. Next, we will explore a number of interesting and innovative models. Finally, we will finish with a conclusion and an outlook on the future usage of GNNs for ATS.","# QUA Analysis:

1. (What is the current state of surveys on the application of GNNs to ATS?): sent1
    1.1. (What does the existing survey on GNNs and ATS cover?): sent2
    1.2. (What are the limitations of the existing survey on GNNs and ATS?): sent3
2. (What is the state of surveys on GNNs as a technology in general?): sent4
    2.1. (Which surveys provide a general overview of GNNs and their applications?): sent5
    2.2. (Which survey provides more technical and theoretical details on GNNs?): sent6
    2.3. (Which survey focuses on the usage of GNNs for NLP?): sent7
    2.4. (Which surveys cover the usage of GNNs in specific domains like recommender systems and text classification?): sent8
    2.5. (Which analysis provides important theoretical properties for GNNs?): sent9
    2.6. (Which analysis examines the VC dimension of GNNs?): sent10
3. (What are the contributions of the current survey?): sent11
    3.1. (What detailed explanation does the current survey provide?): sent12
    3.2. (What taxonomy does the current survey introduce?): sent13
    3.3. (What overview and discussion does the current survey provide?): sent14
4. (How is the rest of the survey structured?): sent15
    4.1. (What will be covered first in the survey?): sent16
    4.2. (What will be explored next in the survey?): sent17
    4.3. (What will be the final part of the survey?): sent18",What are the contributions of the current survey on GNNs for ATS?,"sent11, sent12, sent13, sent14","In general, the contributions of our survey are as follows:1.

We provide a detailed explanation of GNNs in the context of ATS.

2. We introduce a simple taxonomy for GNN models used for ATS.3.

We provide a comprehensive overview of innovative GNN models for ATS and discuss further directions for future research.","The contributions of the current survey on GNNs for ATS are as follows:

A detailed explanation of GNNs in the context of ATS is provided.

A simple taxonomy for GNN models used for ATS is introduced.

A comprehensive overview of innovative GNN models for ATS is provided, along with a discussion on further directions for future research.","Questions:

What are the contributions of the current survey on GNNs for ATS?

Answer:

The contributions of the current survey on GNNs for ATS are as follows:

A detailed explanation of GNNs in the context of ATS is provided.

A simple taxonomy for GNN models used for ATS is introduced.

A comprehensive overview of innovative GNN models for ATS is provided, along with a discussion on further directions for future research."
A Survey of Automatic Text Summarization using Graph Neural Networks,Related Surveys,"As the application of GNNs to ATS is rather novel, to the best of our knowledge there is only one survey on the topic. The survey by  gives an introduction to the topic. However, it does not provide much detail on GNNs in the context of ATS, nor does it cover all models in the space with a taxonomy. As for GNNs themselves, there exists a large number of surveys on GNNs as a technology. In particular, we want to highlight the surveys by  and  which provide a general overview on GNNs and their applications in different fields. The survey by Abadal et al.

(2021) provides more technical and theoretical details on GNNs. The survey by Wu et al. (2021a) is of particular interest as it focuses on the usage of GNNs for NLP as a domain. Additionally, we want to note here the survey by Wu et al. (2020a) on the usage of GNNs in recommender systems and the review by Malekzadeh et al. (2021) on the usage of GNNs for text classification.

For a more theoretical approach we point the reader to the analysis by Xu et al. (2018) which establishes a number of important theoretical properties for GNNs. An analysis of the Vapnik-Chervonenkis (VC) dimension of GNNs was performed by Veličković et al. (2018).

In general, the contributions of our survey are as follows:

1. We provide a detailed explanation of GNNs in the context of ATS.

2. We introduce a simple taxonomy for GNN models used for ATS.

3. We provide a comprehensive overview of innovative GNN models for ATS and discuss further directions for future research.

The rest of survey is structured as follows. First we will give a comprehensive explanation of GNNs in the context of ATS. Next, we will explore a number of interesting and innovative models. Finally, we will finish with a conclusion and an outlook on the future usage of GNNs for ATS.","# QUA Analysis:

1. (What is the current state of surveys on the application of GNNs to ATS?): sent1
    1.1. (What does the existing survey on GNNs and ATS cover?): sent2
    1.2. (What are the limitations of the existing survey on GNNs and ATS?): sent3
2. (What is the state of surveys on GNNs as a technology in general?): sent4
    2.1. (Which surveys provide a general overview of GNNs and their applications?): sent5
    2.2. (Which survey provides more technical and theoretical details on GNNs?): sent6
    2.3. (Which survey focuses on the usage of GNNs for NLP?): sent7
    2.4. (Which surveys cover the usage of GNNs in specific domains like recommender systems and text classification?): sent8
    2.5. (Which analysis provides important theoretical properties for GNNs?): sent9
    2.6. (Which analysis examines the VC dimension of GNNs?): sent10
3. (What are the contributions of the current survey?): sent11
    3.1. (What detailed explanation does the current survey provide?): sent12
    3.2. (What taxonomy does the current survey introduce?): sent13
    3.3. (What overview and discussion does the current survey provide?): sent14
4. (How is the rest of the survey structured?): sent15
    4.1. (What will be covered first in the survey?): sent16
    4.2. (What will be explored next in the survey?): sent17
    4.3. (What will be the final part of the survey?): sent18",How is the rest of the survey on GNNs for ATS structured?,"sent15, sent16, sent17, sent18","The rest of survey is structured as follows.

First we will give a comprehensive explanation of GNNs in the context of ATS.

Next, we will explore a number of interesting and innovative models.

Finally, we will finish with a conclusion and an outlook on the future usage of GNNs for ATS.","The rest of the survey is structured as follows.

First, a comprehensive explanation of GNNs in the context of ATS is provided.

Next, a number of interesting and innovative models are explored.

Finally, the survey concludes with a summary and an outlook on the future usage of GNNs for ATS.","Questions:

How is the rest of the survey on GNNs for ATS structured?

Answer:

The rest of the survey is structured as follows.

First, a comprehensive explanation of GNNs in the context of ATS is provided.

Next, a number of interesting and innovative models are explored.

Finally, the survey concludes with a summary and an outlook on the future usage of GNNs for ATS."
A Survey of Automatic Text Summarization using Graph Neural Networks,Spatial Convolution and Message Passing,"One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs. As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image. The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node. Generalizing this idea to a non-regular grids leads to spatial convolution. However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation. The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.

Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme. However, such a description suffers from scalability issues due to it directly operating over the entire graph. As such modern GNNs use, what is commonly referred to as, message passing. In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations. Thereby the network is able to diffuse information throughout the graph. Consequently, the more iterations, the further outwards information is propagated throughout the graph. In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution. Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:

This first equation describes how messages are generated. A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.

The above equation is the core of the message passing framework and describes how each node feature is updated. The first part consists in the application of a permutation-invariant reduction function ρ. This function aggregates all incoming messages to a node. Then another differen-tiable function ψ combines the reduced messages received with the previous state. Using these two equations one can utilize message passing for learnable layers.

The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1 i,j = x t j and the aggregation is the normalized sum of messages, i.e.

where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree. Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.

The above presented convolution layer does not allow the model to filter unimportant neighbours. Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour. A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure. The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages. Just as in transformers GAT is formulated with multi-head attention. The modification to the previously presented convolution layer follows closely the common attention formulation. Formally, x t+1

where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads. The attention scores are computed with

. This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ). We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2. Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be. The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity. Taking the above description the attention score for GATv2 is modified as follows

). In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.

There are numerous modifications and extensions to the basic convolution presented here. However, for ATS models, GAT layers are dominating as the workhorse for most models. The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST. We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT. Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.

In ATS the graphs used are in nearly all cases not homogeneous. However, the equations presented here do not work for heterogeneous graphs. The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph. In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.

Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models. Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes. This is usually done with some function f where f is commonly the mean, max or sum.

We want to explicitly point out to the reader that the construction of GNNs does not require special datasets. All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail. The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.","1. What is spatial convolution in the context of GNNs?
    1.1. How is spatial convolution in GNNs related to convolution in CNNs? (sent1)
    1.2. How can 2D convolution be visualized in the context of images? (sent2, sent3)
    1.3. How is spatial convolution generalized to non-regular grids? (sent4)
    1.4. What are the challenges of applying spatial convolution to graphs? (sent5, sent6)

2. How is convolution translated into a mathematical formulation for GNNs?
    2.1. What is the direct translation of convolution into a mathematical formulation? (sent7)
    2.2. What are the scalability issues with this direct translation? (sent8)
    2.3. What alternative method is used in modern GNNs to address these issues? (sent9)
    2.4. How does message passing work in practice? (sent10, sent11, sent12, sent13)

3. How is message passing formally defined?
    3.1. What are the equations that define message passing? (sent14)
    3.2. How are messages generated in message passing? (sent15)
    3.3. How are node features updated in message passing? (sent16, sent17, sent18, sent19)
    3.4. How can message passing be utilized for learnable layers? (sent20)
    3.5. How is the convolution layer for a GNN defined? (sent21, sent22)
    3.6. Why is it important for the reduction function to be permutation-invariant? (sent23)

4. What are the limitations of the presented convolution layer and how are they addressed?
    4.1. What is the limitation of the presented convolution layer? (sent24)
    4.2. How do graph attention networks (GAT) address this limitation? (sent25, sent26, sent27)
    4.3. How is GAT formulated? (sent28, sent29, sent30, sent31, sent32)
    4.4. What is GATv2 and how does it improve upon GAT? (sent33, sent34, sent35, sent36)
    4.5. What is the performance of GATv2? (sent37)

5. What are the modifications and extensions to the basic convolution presented?
    5.1. What are the dominant layers in ATS models? (sent38, sent39)
    5.2. Why are GAT layers dominant in ATS models? (sent40)
    5.3. What is the future expectation for GATv2? (sent41, sent42)

6. How do the equations presented work for heterogeneous graphs?
    6.1. What is the issue with the equations presented for heterogeneous graphs? (sent43, sent44)
    6.2. What is the solution for this problem? (sent45, sent46)

7. What is the role of pooling in GNNs?
    7.1. How is pooling achieved in GNNs? (sent47, sent48, sent49)

8. What datasets are required for constructing GNNs for ATS?
    8.1. Are special datasets required for constructing GNNs? (sent50)
    8.2. What common benchmark datasets are used for ATS? (sent51)
    8.3. What is the requirement for any ATS or textual dataset? (sent52)",How does message passing work in practice in GNNs?,"sent10, sent11, sent12, sent13","In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations.

Thereby the network is able to diffuse information throughout the graph.

Consequently, the more iterations, the further outwards information is propagated throughout the graph.

In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution.","In practice, nodes within the graph exchange messages (perform convolutions) with their neighbors for a number of iterations.

This process allows the network to diffuse information throughout the graph.

Consequently, the more iterations, the further outwards information is propagated throughout the graph.

In the terminology of CNNs, the more message passing iterations, the larger the receptive field of the convolution.","Questions:

How does message passing work in practice in GNNs?

Answer:

In practice, nodes within the graph exchange messages (perform convolutions) with their neighbors for a number of iterations.

This process allows the network to diffuse information throughout the graph.

Consequently, the more iterations, the further outwards information is propagated throughout the graph.

In the terminology of CNNs, the more message passing iterations, the larger the receptive field of the convolution."
A Survey of Automatic Text Summarization using Graph Neural Networks,Spatial Convolution and Message Passing,"One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs. As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image. The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node. Generalizing this idea to a non-regular grids leads to spatial convolution. However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation. The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.

Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme. However, such a description suffers from scalability issues due to it directly operating over the entire graph. As such modern GNNs use, what is commonly referred to as, message passing. In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations. Thereby the network is able to diffuse information throughout the graph. Consequently, the more iterations, the further outwards information is propagated throughout the graph. In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution. Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:

This first equation describes how messages are generated. A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.

The above equation is the core of the message passing framework and describes how each node feature is updated. The first part consists in the application of a permutation-invariant reduction function ρ. This function aggregates all incoming messages to a node. Then another differen-tiable function ψ combines the reduced messages received with the previous state. Using these two equations one can utilize message passing for learnable layers.

The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1 i,j = x t j and the aggregation is the normalized sum of messages, i.e.

where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree. Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.

The above presented convolution layer does not allow the model to filter unimportant neighbours. Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour. A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure. The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages. Just as in transformers GAT is formulated with multi-head attention. The modification to the previously presented convolution layer follows closely the common attention formulation. Formally, x t+1

where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads. The attention scores are computed with

. This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ). We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2. Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be. The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity. Taking the above description the attention score for GATv2 is modified as follows

). In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.

There are numerous modifications and extensions to the basic convolution presented here. However, for ATS models, GAT layers are dominating as the workhorse for most models. The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST. We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT. Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.

In ATS the graphs used are in nearly all cases not homogeneous. However, the equations presented here do not work for heterogeneous graphs. The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph. In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.

Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models. Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes. This is usually done with some function f where f is commonly the mean, max or sum.

We want to explicitly point out to the reader that the construction of GNNs does not require special datasets. All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail. The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.","1. What is spatial convolution in the context of GNNs?
    1.1. How is spatial convolution in GNNs related to convolution in CNNs? (sent1)
    1.2. How can 2D convolution be visualized in the context of images? (sent2, sent3)
    1.3. How is spatial convolution generalized to non-regular grids? (sent4)
    1.4. What are the challenges of applying spatial convolution to graphs? (sent5, sent6)

2. How is convolution translated into a mathematical formulation for GNNs?
    2.1. What is the direct translation of convolution into a mathematical formulation? (sent7)
    2.2. What are the scalability issues with this direct translation? (sent8)
    2.3. What alternative method is used in modern GNNs to address these issues? (sent9)
    2.4. How does message passing work in practice? (sent10, sent11, sent12, sent13)

3. How is message passing formally defined?
    3.1. What are the equations that define message passing? (sent14)
    3.2. How are messages generated in message passing? (sent15)
    3.3. How are node features updated in message passing? (sent16, sent17, sent18, sent19)
    3.4. How can message passing be utilized for learnable layers? (sent20)
    3.5. How is the convolution layer for a GNN defined? (sent21, sent22)
    3.6. Why is it important for the reduction function to be permutation-invariant? (sent23)

4. What are the limitations of the presented convolution layer and how are they addressed?
    4.1. What is the limitation of the presented convolution layer? (sent24)
    4.2. How do graph attention networks (GAT) address this limitation? (sent25, sent26, sent27)
    4.3. How is GAT formulated? (sent28, sent29, sent30, sent31, sent32)
    4.4. What is GATv2 and how does it improve upon GAT? (sent33, sent34, sent35, sent36)
    4.5. What is the performance of GATv2? (sent37)

5. What are the modifications and extensions to the basic convolution presented?
    5.1. What are the dominant layers in ATS models? (sent38, sent39)
    5.2. Why are GAT layers dominant in ATS models? (sent40)
    5.3. What is the future expectation for GATv2? (sent41, sent42)

6. How do the equations presented work for heterogeneous graphs?
    6.1. What is the issue with the equations presented for heterogeneous graphs? (sent43, sent44)
    6.2. What is the solution for this problem? (sent45, sent46)

7. What is the role of pooling in GNNs?
    7.1. How is pooling achieved in GNNs? (sent47, sent48, sent49)

8. What datasets are required for constructing GNNs for ATS?
    8.1. Are special datasets required for constructing GNNs? (sent50)
    8.2. What common benchmark datasets are used for ATS? (sent51)
    8.3. What is the requirement for any ATS or textual dataset? (sent52)",How are node features updated in the message passing framework?,"sent16, sent17, sent18, sent19","The above equation is the core of the message passing framework and describes how each node feature is updated.

The first part consists in the application of a permutation-invariant reduction function ρ.

This function aggregates all incoming messages to a node.

Then another differen-tiable function ψ combines the reduced messages received with the previous state.","The core of the message passing framework describes how each node feature is updated.

The first part involves the application of a permutation-invariant reduction function ρ.

This function aggregates all incoming messages to a node.

Then, another differentiable function ψ combines the reduced messages received with the previous state.","Questions:

How are node features updated in the message passing framework?

Answer:

The core of the message passing framework describes how each node feature is updated.

The first part involves the application of a permutation-invariant reduction function ρ.

This function aggregates all incoming messages to a node.

Then, another differentiable function ψ combines the reduced messages received with the previous state."
A Survey of Automatic Text Summarization using Graph Neural Networks,Spatial Convolution and Message Passing,"One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs. As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image. The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node. Generalizing this idea to a non-regular grids leads to spatial convolution. However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation. The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.

Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme. However, such a description suffers from scalability issues due to it directly operating over the entire graph. As such modern GNNs use, what is commonly referred to as, message passing. In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations. Thereby the network is able to diffuse information throughout the graph. Consequently, the more iterations, the further outwards information is propagated throughout the graph. In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution. Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:

This first equation describes how messages are generated. A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.

The above equation is the core of the message passing framework and describes how each node feature is updated. The first part consists in the application of a permutation-invariant reduction function ρ. This function aggregates all incoming messages to a node. Then another differen-tiable function ψ combines the reduced messages received with the previous state. Using these two equations one can utilize message passing for learnable layers.

The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1 i,j = x t j and the aggregation is the normalized sum of messages, i.e.

where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree. Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.

The above presented convolution layer does not allow the model to filter unimportant neighbours. Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour. A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure. The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages. Just as in transformers GAT is formulated with multi-head attention. The modification to the previously presented convolution layer follows closely the common attention formulation. Formally, x t+1

where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads. The attention scores are computed with

. This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ). We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2. Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be. The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity. Taking the above description the attention score for GATv2 is modified as follows

). In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.

There are numerous modifications and extensions to the basic convolution presented here. However, for ATS models, GAT layers are dominating as the workhorse for most models. The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST. We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT. Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.

In ATS the graphs used are in nearly all cases not homogeneous. However, the equations presented here do not work for heterogeneous graphs. The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph. In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.

Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models. Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes. This is usually done with some function f where f is commonly the mean, max or sum.

We want to explicitly point out to the reader that the construction of GNNs does not require special datasets. All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail. The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.","1. What is spatial convolution in the context of GNNs?
    1.1. How is spatial convolution in GNNs related to convolution in CNNs? (sent1)
    1.2. How can 2D convolution be visualized in the context of images? (sent2, sent3)
    1.3. How is spatial convolution generalized to non-regular grids? (sent4)
    1.4. What are the challenges of applying spatial convolution to graphs? (sent5, sent6)

2. How is convolution translated into a mathematical formulation for GNNs?
    2.1. What is the direct translation of convolution into a mathematical formulation? (sent7)
    2.2. What are the scalability issues with this direct translation? (sent8)
    2.3. What alternative method is used in modern GNNs to address these issues? (sent9)
    2.4. How does message passing work in practice? (sent10, sent11, sent12, sent13)

3. How is message passing formally defined?
    3.1. What are the equations that define message passing? (sent14)
    3.2. How are messages generated in message passing? (sent15)
    3.3. How are node features updated in message passing? (sent16, sent17, sent18, sent19)
    3.4. How can message passing be utilized for learnable layers? (sent20)
    3.5. How is the convolution layer for a GNN defined? (sent21, sent22)
    3.6. Why is it important for the reduction function to be permutation-invariant? (sent23)

4. What are the limitations of the presented convolution layer and how are they addressed?
    4.1. What is the limitation of the presented convolution layer? (sent24)
    4.2. How do graph attention networks (GAT) address this limitation? (sent25, sent26, sent27)
    4.3. How is GAT formulated? (sent28, sent29, sent30, sent31, sent32)
    4.4. What is GATv2 and how does it improve upon GAT? (sent33, sent34, sent35, sent36)
    4.5. What is the performance of GATv2? (sent37)

5. What are the modifications and extensions to the basic convolution presented?
    5.1. What are the dominant layers in ATS models? (sent38, sent39)
    5.2. Why are GAT layers dominant in ATS models? (sent40)
    5.3. What is the future expectation for GATv2? (sent41, sent42)

6. How do the equations presented work for heterogeneous graphs?
    6.1. What is the issue with the equations presented for heterogeneous graphs? (sent43, sent44)
    6.2. What is the solution for this problem? (sent45, sent46)

7. What is the role of pooling in GNNs?
    7.1. How is pooling achieved in GNNs? (sent47, sent48, sent49)

8. What datasets are required for constructing GNNs for ATS?
    8.1. Are special datasets required for constructing GNNs? (sent50)
    8.2. What common benchmark datasets are used for ATS? (sent51)
    8.3. What is the requirement for any ATS or textual dataset? (sent52)",How do graph attention networks (GAT) address the limitation of the presented convolution layer?,"sent25, sent26, sent27","Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour.

A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure.

The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages.","Graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbor, inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017).

GAT employs spatial convolution variants to enhance the model's ability to focus on relevant neighbors.

The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages.","Questions:

How do graph attention networks (GAT) address the limitation of the presented convolution layer?

Answer:

Graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbor, inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017).

GAT employs spatial convolution variants to enhance the model's ability to focus on relevant neighbors.

The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages."
A Survey of Automatic Text Summarization using Graph Neural Networks,Spatial Convolution and Message Passing,"One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs. As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image. The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node. Generalizing this idea to a non-regular grids leads to spatial convolution. However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation. The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.

Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme. However, such a description suffers from scalability issues due to it directly operating over the entire graph. As such modern GNNs use, what is commonly referred to as, message passing. In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations. Thereby the network is able to diffuse information throughout the graph. Consequently, the more iterations, the further outwards information is propagated throughout the graph. In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution. Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:

This first equation describes how messages are generated. A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.

The above equation is the core of the message passing framework and describes how each node feature is updated. The first part consists in the application of a permutation-invariant reduction function ρ. This function aggregates all incoming messages to a node. Then another differen-tiable function ψ combines the reduced messages received with the previous state. Using these two equations one can utilize message passing for learnable layers.

The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1 i,j = x t j and the aggregation is the normalized sum of messages, i.e.

where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree. Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.

The above presented convolution layer does not allow the model to filter unimportant neighbours. Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour. A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure. The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages. Just as in transformers GAT is formulated with multi-head attention. The modification to the previously presented convolution layer follows closely the common attention formulation. Formally, x t+1

where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads. The attention scores are computed with

. This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ). We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2. Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be. The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity. Taking the above description the attention score for GATv2 is modified as follows

). In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.

There are numerous modifications and extensions to the basic convolution presented here. However, for ATS models, GAT layers are dominating as the workhorse for most models. The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST. We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT. Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.

In ATS the graphs used are in nearly all cases not homogeneous. However, the equations presented here do not work for heterogeneous graphs. The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph. In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.

Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models. Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes. This is usually done with some function f where f is commonly the mean, max or sum.

We want to explicitly point out to the reader that the construction of GNNs does not require special datasets. All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail. The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.","1. What is spatial convolution in the context of GNNs?
    1.1. How is spatial convolution in GNNs related to convolution in CNNs? (sent1)
    1.2. How can 2D convolution be visualized in the context of images? (sent2, sent3)
    1.3. How is spatial convolution generalized to non-regular grids? (sent4)
    1.4. What are the challenges of applying spatial convolution to graphs? (sent5, sent6)

2. How is convolution translated into a mathematical formulation for GNNs?
    2.1. What is the direct translation of convolution into a mathematical formulation? (sent7)
    2.2. What are the scalability issues with this direct translation? (sent8)
    2.3. What alternative method is used in modern GNNs to address these issues? (sent9)
    2.4. How does message passing work in practice? (sent10, sent11, sent12, sent13)

3. How is message passing formally defined?
    3.1. What are the equations that define message passing? (sent14)
    3.2. How are messages generated in message passing? (sent15)
    3.3. How are node features updated in message passing? (sent16, sent17, sent18, sent19)
    3.4. How can message passing be utilized for learnable layers? (sent20)
    3.5. How is the convolution layer for a GNN defined? (sent21, sent22)
    3.6. Why is it important for the reduction function to be permutation-invariant? (sent23)

4. What are the limitations of the presented convolution layer and how are they addressed?
    4.1. What is the limitation of the presented convolution layer? (sent24)
    4.2. How do graph attention networks (GAT) address this limitation? (sent25, sent26, sent27)
    4.3. How is GAT formulated? (sent28, sent29, sent30, sent31, sent32)
    4.4. What is GATv2 and how does it improve upon GAT? (sent33, sent34, sent35, sent36)
    4.5. What is the performance of GATv2? (sent37)

5. What are the modifications and extensions to the basic convolution presented?
    5.1. What are the dominant layers in ATS models? (sent38, sent39)
    5.2. Why are GAT layers dominant in ATS models? (sent40)
    5.3. What is the future expectation for GATv2? (sent41, sent42)

6. How do the equations presented work for heterogeneous graphs?
    6.1. What is the issue with the equations presented for heterogeneous graphs? (sent43, sent44)
    6.2. What is the solution for this problem? (sent45, sent46)

7. What is the role of pooling in GNNs?
    7.1. How is pooling achieved in GNNs? (sent47, sent48, sent49)

8. What datasets are required for constructing GNNs for ATS?
    8.1. Are special datasets required for constructing GNNs? (sent50)
    8.2. What common benchmark datasets are used for ATS? (sent51)
    8.3. What is the requirement for any ATS or textual dataset? (sent52)",How is GAT formulated in GNNs?,"sent28, sent29, sent30, sent31, sent32","Just as in transformers GAT is formulated with multi-head attention.

The modification to the previously presented convolution layer follows closely the common attention formulation.

Formally, x t+1where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads.

The attention scores are computed with.

This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ).","GAT is formulated with multi-head attention, similar to transformers.

The modification to the previously presented convolution layer follows closely the common attention formulation.

Formally, \( \alpha_{i,j} \) is the attention score between node \( i \) and node \( j \), and \( K \) denotes the number of concatenated heads.

The attention scores are computed with a shared linear transformation followed by a LeakyReLU activation.

This score is then normalized to obtain the attention score per edge \( \alpha_{i,j} = \text{softmax}_i (r_{i,j}) \).","Questions:

How is GAT formulated in GNNs?

Answer:

GAT is formulated with multi-head attention, similar to transformers.

The modification to the previously presented convolution layer follows closely the common attention formulation.

Formally, \( \alpha_{i,j} \) is the attention score between node \( i \) and node \( j \), and \( K \) denotes the number of concatenated heads.

The attention scores are computed with a shared linear transformation followed by a LeakyReLU activation.

This score is then normalized to obtain the attention score per edge \( \alpha_{i,j} = \text{softmax}_i (r_{i,j}) \)."
A Survey of Automatic Text Summarization using Graph Neural Networks,Spatial Convolution and Message Passing,"One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs. As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image. The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node. Generalizing this idea to a non-regular grids leads to spatial convolution. However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation. The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.

Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme. However, such a description suffers from scalability issues due to it directly operating over the entire graph. As such modern GNNs use, what is commonly referred to as, message passing. In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations. Thereby the network is able to diffuse information throughout the graph. Consequently, the more iterations, the further outwards information is propagated throughout the graph. In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution. Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:

This first equation describes how messages are generated. A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.

The above equation is the core of the message passing framework and describes how each node feature is updated. The first part consists in the application of a permutation-invariant reduction function ρ. This function aggregates all incoming messages to a node. Then another differen-tiable function ψ combines the reduced messages received with the previous state. Using these two equations one can utilize message passing for learnable layers.

The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1 i,j = x t j and the aggregation is the normalized sum of messages, i.e.

where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree. Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.

The above presented convolution layer does not allow the model to filter unimportant neighbours. Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour. A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure. The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages. Just as in transformers GAT is formulated with multi-head attention. The modification to the previously presented convolution layer follows closely the common attention formulation. Formally, x t+1

where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads. The attention scores are computed with

. This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ). We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2. Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be. The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity. Taking the above description the attention score for GATv2 is modified as follows

). In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.

There are numerous modifications and extensions to the basic convolution presented here. However, for ATS models, GAT layers are dominating as the workhorse for most models. The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST. We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT. Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.

In ATS the graphs used are in nearly all cases not homogeneous. However, the equations presented here do not work for heterogeneous graphs. The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph. In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.

Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models. Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes. This is usually done with some function f where f is commonly the mean, max or sum.

We want to explicitly point out to the reader that the construction of GNNs does not require special datasets. All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail. The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.","1. What is spatial convolution in the context of GNNs?
    1.1. How is spatial convolution in GNNs related to convolution in CNNs? (sent1)
    1.2. How can 2D convolution be visualized in the context of images? (sent2, sent3)
    1.3. How is spatial convolution generalized to non-regular grids? (sent4)
    1.4. What are the challenges of applying spatial convolution to graphs? (sent5, sent6)

2. How is convolution translated into a mathematical formulation for GNNs?
    2.1. What is the direct translation of convolution into a mathematical formulation? (sent7)
    2.2. What are the scalability issues with this direct translation? (sent8)
    2.3. What alternative method is used in modern GNNs to address these issues? (sent9)
    2.4. How does message passing work in practice? (sent10, sent11, sent12, sent13)

3. How is message passing formally defined?
    3.1. What are the equations that define message passing? (sent14)
    3.2. How are messages generated in message passing? (sent15)
    3.3. How are node features updated in message passing? (sent16, sent17, sent18, sent19)
    3.4. How can message passing be utilized for learnable layers? (sent20)
    3.5. How is the convolution layer for a GNN defined? (sent21, sent22)
    3.6. Why is it important for the reduction function to be permutation-invariant? (sent23)

4. What are the limitations of the presented convolution layer and how are they addressed?
    4.1. What is the limitation of the presented convolution layer? (sent24)
    4.2. How do graph attention networks (GAT) address this limitation? (sent25, sent26, sent27)
    4.3. How is GAT formulated? (sent28, sent29, sent30, sent31, sent32)
    4.4. What is GATv2 and how does it improve upon GAT? (sent33, sent34, sent35, sent36)
    4.5. What is the performance of GATv2? (sent37)

5. What are the modifications and extensions to the basic convolution presented?
    5.1. What are the dominant layers in ATS models? (sent38, sent39)
    5.2. Why are GAT layers dominant in ATS models? (sent40)
    5.3. What is the future expectation for GATv2? (sent41, sent42)

6. How do the equations presented work for heterogeneous graphs?
    6.1. What is the issue with the equations presented for heterogeneous graphs? (sent43, sent44)
    6.2. What is the solution for this problem? (sent45, sent46)

7. What is the role of pooling in GNNs?
    7.1. How is pooling achieved in GNNs? (sent47, sent48, sent49)

8. What datasets are required for constructing GNNs for ATS?
    8.1. Are special datasets required for constructing GNNs? (sent50)
    8.2. What common benchmark datasets are used for ATS? (sent51)
    8.3. What is the requirement for any ATS or textual dataset? (sent52)",What is GATv2 and how does it improve upon GAT?,"sent33, sent34, sent35, sent36","We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2.

Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be.

The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity.

Taking the above description the attention score for GATv2 is modified as follows).","A recent development known as GATv2 is highlighted by Brody et al. (2021).

The main improvement of GATv2 is that in the calculation, both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be.

The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity.

The attention score for GATv2 is modified to enhance its expressive power.","Questions:

What is GATv2 and how does it improve upon GAT?

Answer:

A recent development known as GATv2 is highlighted by Brody et al. (2021).

The main improvement of GATv2 is that in the calculation, both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be.

The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity.

The attention score for GATv2 is modified to enhance its expressive power."
A Survey of Automatic Text Summarization using Graph Neural Networks,Spatial Convolution and Message Passing,"One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs. As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image. The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node. Generalizing this idea to a non-regular grids leads to spatial convolution. However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation. The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.

Directly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme. However, such a description suffers from scalability issues due to it directly operating over the entire graph. As such modern GNNs use, what is commonly referred to as, message passing. In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations. Thereby the network is able to diffuse information throughout the graph. Consequently, the more iterations, the further outwards information is propagated throughout the graph. In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution. Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:

This first equation describes how messages are generated. A differentiable function ϕ generates messages m for each edge which connects nodes using the node features and edge feature present.

The above equation is the core of the message passing framework and describes how each node feature is updated. The first part consists in the application of a permutation-invariant reduction function ρ. This function aggregates all incoming messages to a node. Then another differen-tiable function ψ combines the reduced messages received with the previous state. Using these two equations one can utilize message passing for learnable layers.

The convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1 i,j = x t j and the aggregation is the normalized sum of messages, i.e.

where M (i) represents the set of messages received by node i, σ is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree. Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.

The above presented convolution layer does not allow the model to filter unimportant neighbours. Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veličković et al., 2018) assign attention scores to each neighbour. A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure. The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages. Just as in transformers GAT is formulated with multi-head attention. The modification to the previously presented convolution layer follows closely the common attention formulation. Formally, x t+1

where α i,j is the attention score between node i and node j and K denotes the number of concatenated heads. The attention scores are computed with

. This score is then normalized to obtain the attention score per edge α i,j = sof tmax i (r i,j ). We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2. Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be. The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity. Taking the above description the attention score for GATv2 is modified as follows

). In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.

There are numerous modifications and extensions to the basic convolution presented here. However, for ATS models, GAT layers are dominating as the workhorse for most models. The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST. We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT. Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.

In ATS the graphs used are in nearly all cases not homogeneous. However, the equations presented here do not work for heterogeneous graphs. The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph. In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.

Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models. Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes. This is usually done with some function f where f is commonly the mean, max or sum.

We want to explicitly point out to the reader that the construction of GNNs does not require special datasets. All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail. The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.","1. What is spatial convolution in the context of GNNs?
    1.1. How is spatial convolution in GNNs related to convolution in CNNs? (sent1)
    1.2. How can 2D convolution be visualized in the context of images? (sent2, sent3)
    1.3. How is spatial convolution generalized to non-regular grids? (sent4)
    1.4. What are the challenges of applying spatial convolution to graphs? (sent5, sent6)

2. How is convolution translated into a mathematical formulation for GNNs?
    2.1. What is the direct translation of convolution into a mathematical formulation? (sent7)
    2.2. What are the scalability issues with this direct translation? (sent8)
    2.3. What alternative method is used in modern GNNs to address these issues? (sent9)
    2.4. How does message passing work in practice? (sent10, sent11, sent12, sent13)

3. How is message passing formally defined?
    3.1. What are the equations that define message passing? (sent14)
    3.2. How are messages generated in message passing? (sent15)
    3.3. How are node features updated in message passing? (sent16, sent17, sent18, sent19)
    3.4. How can message passing be utilized for learnable layers? (sent20)
    3.5. How is the convolution layer for a GNN defined? (sent21, sent22)
    3.6. Why is it important for the reduction function to be permutation-invariant? (sent23)

4. What are the limitations of the presented convolution layer and how are they addressed?
    4.1. What is the limitation of the presented convolution layer? (sent24)
    4.2. How do graph attention networks (GAT) address this limitation? (sent25, sent26, sent27)
    4.3. How is GAT formulated? (sent28, sent29, sent30, sent31, sent32)
    4.4. What is GATv2 and how does it improve upon GAT? (sent33, sent34, sent35, sent36)
    4.5. What is the performance of GATv2? (sent37)

5. What are the modifications and extensions to the basic convolution presented?
    5.1. What are the dominant layers in ATS models? (sent38, sent39)
    5.2. Why are GAT layers dominant in ATS models? (sent40)
    5.3. What is the future expectation for GATv2? (sent41, sent42)

6. How do the equations presented work for heterogeneous graphs?
    6.1. What is the issue with the equations presented for heterogeneous graphs? (sent43, sent44)
    6.2. What is the solution for this problem? (sent45, sent46)

7. What is the role of pooling in GNNs?
    7.1. How is pooling achieved in GNNs? (sent47, sent48, sent49)

8. What datasets are required for constructing GNNs for ATS?
    8.1. Are special datasets required for constructing GNNs? (sent50)
    8.2. What common benchmark datasets are used for ATS? (sent51)
    8.3. What is the requirement for any ATS or textual dataset? (sent52)",How is pooling achieved in GNNs?,"sent47, sent48, sent49","Convolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models.

Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes.

This is usually done with some function f where f is commonly the mean, max or sum.","Pooling is an important and common operation in GNNs, especially when they are used jointly with other models.

Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes.

This is usually done with some function \( f \), where \( f \) is commonly the mean, max, or sum.","Questions:

How is pooling achieved in GNNs?

Answer:

Pooling is an important and common operation in GNNs, especially when they are used jointly with other models.

Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes.

This is usually done with some function \( f \), where \( f \) is commonly the mean, max, or sum."
A Survey of Automatic Text Summarization using Graph Neural Networks,Standalone GNNs,"We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by . We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS. An illustration of the general concepts presented here can be seen in Figure 3.

The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes. The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge. The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document. This is a flexible structure, as it can be used in a single-document but also multidocument setting.  Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism. Inspired by HSG.

The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word. The neural network consists of a modified GAT layer. The GAT is modified to consider the TF-IDF value of the connecting edge. Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution. In total three convolution layers are used, word-sentence, sentence-word and word-document. The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.

The classification itself is done by a single linear layer. The model then does not directly use the predicted nodes to produce the summary. Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.

The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset. One should especially note the flexibility and ability to use this model for two tasks.

An older model by Muratore et al. (2010) can be considered a precursor to this architecture. A simple extension to the HSG model is proposed by Ya et al. (2021). In their extension they modify the model for query constraints for the summary. This is achieved by adding a query node to the graph structure. Additionally, they introduce a mu-tual information maximization mechanism during training.

A model which further follows this structure is the one by Linmei et al. (2019). The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution. The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels. In particular, they encode the semantic and syntactical relationship between sentences within the graph.

This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019). They introduce an additional universal feature vector which is added to each sentence node embedding. This universal feature vector is learned from a large unrelated and general corpus. This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.

Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a). The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities. The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens. The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence. Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram. Additionally, sequentially occurring words and entities are connected with a directed edge. This setup shows how one can encode a substantial amount of implicit information in an explicit manner.

HAHSum uses a GAT for each of the five node type combinations found within the graph. Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.

The results for HAHSum show that GNNs can perform very well. The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset. The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS . The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness. Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by . Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents. That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution. These sub-graphs are then ranked and thereby selected for a summary. This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem. Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","1. (What is the focus of the discussion on standalone GNN models?): sent1
    1.1. (Why is HeterSumGraph (HSG) chosen for discussion?): sent2
        1.1.1. (Where can the general concepts of HSG be seen?): sent3
    1.2. (How does the HSG model encode text into a graph?): sent4
        1.2.1. (How are the connections between nodes decided in HSG?): sent5, sent6
        1.2.2. (What is the flexibility of this structure?): sent7
    1.3. (What does Figure 3 illustrate?): sent8
    1.4. (How are the feature vectors and edge weights obtained in HSG?): sent9
    1.5. (What does the neural network in HSG consist of?): sent10
        1.5.1. (How is the GAT modified in HSG?): sent11
        1.5.2. (What additional layer is applied in HSG?): sent12
        1.5.3. (How many convolution layers are used in HSG?): sent13
    1.6. (What is the training task for the HSG model?): sent14
        1.6.1. (How is the classification done in HSG?): sent15
        1.6.2. (How does the model produce the summary?): sent16
            1.6.2.1. (What technique is used during sentence selection?): sent17
    1.7. (What are the results of the HSG model?): sent18
        1.7.1. (What should be noted about the HSG model?): sent19
    1.8. (What is a precursor to the HSG architecture?): sent20
    1.9. (What extension to the HSG model is proposed by Ya et al. (2021)?): sent21
        1.9.1. (How is the model modified for query constraints?): sent22
            1.9.1.1. (How is the query node added to the graph structure?): sent23
            1.9.1.2. (What mechanism is introduced during training?): sent24
    1.10. (What model follows the HSG structure?): sent25
        1.10.1. (How is the attention mechanism extended in this model?): sent26
    1.11. (What does the GNN model by Jing et al. (2021) encode?): sent27
        1.11.1. (What relationships are encoded within the graph?): sent28
    1.12. (What idea is followed by Antognini and Faltings (2019)?): sent29
        1.12.1. (What additional feature is introduced?): sent30
            1.12.1.1. (How is the universal feature vector learned?): sent31
        1.12.2. (What is unique about this model?): sent32
    1.13. (What is the HAHSum model by Jia et al. (2020a)?): sent33
        1.13.1. (How is the input graph for HAHSum constructed?): sent34
            1.13.1.1. (What types of nodes are used in HAHSum?): sent35
            1.13.1.2. (How is the graph built in HAHSum?): sent36, sent37, sent38
        1.13.2. (What does this setup show?): sent39
        1.13.3. (How does HAHSum use GAT?): sent40
            1.13.3.1. (What layers are applied in HAHSum?): sent41
        1.13.4. (What are the results for HAHSum?): sent42
            1.13.4.1. (On which datasets was HAHSum tested?): sent43
            1.13.4.2. (How does HAHSum perform compared to other models?): sent44
            1.13.4.3. (What do the results of the Amazon Mechanical Turk experiment show?): sent45
    1.14. (What is another recent GNN model for multi-document summarization?): sent46
        1.14.1. (How does the SgSum model differ from other approaches?): sent47
            1.14.1.1. (How does SgSum transform documents and generate sub-graphs?): sent48
            1.14.1.2. (How are sub-graphs selected for a summary?): sent49
            1.14.1.3. (What is innovative about the SgSum approach?): sent50
            1.14.1.4. (What does SgSum output?): sent51",How does the HeterSumGraph (HSG) model encode text into a graph?,"sent4, sent5, sent6, sent7","The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes.

The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge.

The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document.

This is a flexible structure, as it can be used in a single-document but also multidocument setting.","The HSG model encodes each text into a graph with three node types: sentence nodes, word nodes, and document nodes.

The connection between these nodes is determined by inclusion, meaning that if a word represented by a word node appears in a sentence, their respective nodes are connected by an edge.

The same principle applies to document nodes, which are connected depending on whether a word, represented by a word node, occurs within the document.

This flexible structure allows the HSG model to be used in both single-document and multi-document settings.","Questions:

How does the HeterSumGraph (HSG) model encode text into a graph?

Answer:

The HSG model encodes each text into a graph with three node types: sentence nodes, word nodes, and document nodes.

The connection between these nodes is determined by inclusion, meaning that if a word represented by a word node appears in a sentence, their respective nodes are connected by an edge.

The same principle applies to document nodes, which are connected depending on whether a word, represented by a word node, occurs within the document.

This flexible structure allows the HSG model to be used in both single-document and multi-document settings."
A Survey of Automatic Text Summarization using Graph Neural Networks,Standalone GNNs,"We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by . We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS. An illustration of the general concepts presented here can be seen in Figure 3.

The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes. The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge. The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document. This is a flexible structure, as it can be used in a single-document but also multidocument setting.  Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism. Inspired by HSG.

The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word. The neural network consists of a modified GAT layer. The GAT is modified to consider the TF-IDF value of the connecting edge. Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution. In total three convolution layers are used, word-sentence, sentence-word and word-document. The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.

The classification itself is done by a single linear layer. The model then does not directly use the predicted nodes to produce the summary. Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.

The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset. One should especially note the flexibility and ability to use this model for two tasks.

An older model by Muratore et al. (2010) can be considered a precursor to this architecture. A simple extension to the HSG model is proposed by Ya et al. (2021). In their extension they modify the model for query constraints for the summary. This is achieved by adding a query node to the graph structure. Additionally, they introduce a mu-tual information maximization mechanism during training.

A model which further follows this structure is the one by Linmei et al. (2019). The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution. The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels. In particular, they encode the semantic and syntactical relationship between sentences within the graph.

This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019). They introduce an additional universal feature vector which is added to each sentence node embedding. This universal feature vector is learned from a large unrelated and general corpus. This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.

Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a). The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities. The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens. The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence. Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram. Additionally, sequentially occurring words and entities are connected with a directed edge. This setup shows how one can encode a substantial amount of implicit information in an explicit manner.

HAHSum uses a GAT for each of the five node type combinations found within the graph. Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.

The results for HAHSum show that GNNs can perform very well. The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset. The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS . The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness. Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by . Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents. That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution. These sub-graphs are then ranked and thereby selected for a summary. This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem. Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","1. (What is the focus of the discussion on standalone GNN models?): sent1
    1.1. (Why is HeterSumGraph (HSG) chosen for discussion?): sent2
        1.1.1. (Where can the general concepts of HSG be seen?): sent3
    1.2. (How does the HSG model encode text into a graph?): sent4
        1.2.1. (How are the connections between nodes decided in HSG?): sent5, sent6
        1.2.2. (What is the flexibility of this structure?): sent7
    1.3. (What does Figure 3 illustrate?): sent8
    1.4. (How are the feature vectors and edge weights obtained in HSG?): sent9
    1.5. (What does the neural network in HSG consist of?): sent10
        1.5.1. (How is the GAT modified in HSG?): sent11
        1.5.2. (What additional layer is applied in HSG?): sent12
        1.5.3. (How many convolution layers are used in HSG?): sent13
    1.6. (What is the training task for the HSG model?): sent14
        1.6.1. (How is the classification done in HSG?): sent15
        1.6.2. (How does the model produce the summary?): sent16
            1.6.2.1. (What technique is used during sentence selection?): sent17
    1.7. (What are the results of the HSG model?): sent18
        1.7.1. (What should be noted about the HSG model?): sent19
    1.8. (What is a precursor to the HSG architecture?): sent20
    1.9. (What extension to the HSG model is proposed by Ya et al. (2021)?): sent21
        1.9.1. (How is the model modified for query constraints?): sent22
            1.9.1.1. (How is the query node added to the graph structure?): sent23
            1.9.1.2. (What mechanism is introduced during training?): sent24
    1.10. (What model follows the HSG structure?): sent25
        1.10.1. (How is the attention mechanism extended in this model?): sent26
    1.11. (What does the GNN model by Jing et al. (2021) encode?): sent27
        1.11.1. (What relationships are encoded within the graph?): sent28
    1.12. (What idea is followed by Antognini and Faltings (2019)?): sent29
        1.12.1. (What additional feature is introduced?): sent30
            1.12.1.1. (How is the universal feature vector learned?): sent31
        1.12.2. (What is unique about this model?): sent32
    1.13. (What is the HAHSum model by Jia et al. (2020a)?): sent33
        1.13.1. (How is the input graph for HAHSum constructed?): sent34
            1.13.1.1. (What types of nodes are used in HAHSum?): sent35
            1.13.1.2. (How is the graph built in HAHSum?): sent36, sent37, sent38
        1.13.2. (What does this setup show?): sent39
        1.13.3. (How does HAHSum use GAT?): sent40
            1.13.3.1. (What layers are applied in HAHSum?): sent41
        1.13.4. (What are the results for HAHSum?): sent42
            1.13.4.1. (On which datasets was HAHSum tested?): sent43
            1.13.4.2. (How does HAHSum perform compared to other models?): sent44
            1.13.4.3. (What do the results of the Amazon Mechanical Turk experiment show?): sent45
    1.14. (What is another recent GNN model for multi-document summarization?): sent46
        1.14.1. (How does the SgSum model differ from other approaches?): sent47
            1.14.1.1. (How does SgSum transform documents and generate sub-graphs?): sent48
            1.14.1.2. (How are sub-graphs selected for a summary?): sent49
            1.14.1.3. (What is innovative about the SgSum approach?): sent50
            1.14.1.4. (What does SgSum output?): sent51",How are the feature vectors and edge weights obtained in the HeterSumGraph (HSG) model?,"sent9, sent10, sent11","Inspired by HSG. The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word.

The neural network consists of a modified GAT layer.

The GAT is modified to consider the TF-IDF value of the connecting edge.","The feature vectors for all nodes in the HeterSumGraph (HSG) model are obtained by encoders, and the edge weights are obtained by computing the TF-IDF score for each word.

The neural network in the HeterSumGraph (HSG) model consists of a modified GAT layer.

The GAT layer is modified to consider the TF-IDF value of the connecting edge.","Questions:

How are the feature vectors and edge weights obtained in the HeterSumGraph (HSG) model?

Answer:

The feature vectors for all nodes in the HeterSumGraph (HSG) model are obtained by encoders, and the edge weights are obtained by computing the TF-IDF score for each word.

The neural network in the HeterSumGraph (HSG) model consists of a modified GAT layer.

The GAT layer is modified to consider the TF-IDF value of the connecting edge."
A Survey of Automatic Text Summarization using Graph Neural Networks,Standalone GNNs,"We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by . We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS. An illustration of the general concepts presented here can be seen in Figure 3.

The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes. The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge. The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document. This is a flexible structure, as it can be used in a single-document but also multidocument setting.  Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism. Inspired by HSG.

The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word. The neural network consists of a modified GAT layer. The GAT is modified to consider the TF-IDF value of the connecting edge. Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution. In total three convolution layers are used, word-sentence, sentence-word and word-document. The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.

The classification itself is done by a single linear layer. The model then does not directly use the predicted nodes to produce the summary. Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.

The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset. One should especially note the flexibility and ability to use this model for two tasks.

An older model by Muratore et al. (2010) can be considered a precursor to this architecture. A simple extension to the HSG model is proposed by Ya et al. (2021). In their extension they modify the model for query constraints for the summary. This is achieved by adding a query node to the graph structure. Additionally, they introduce a mu-tual information maximization mechanism during training.

A model which further follows this structure is the one by Linmei et al. (2019). The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution. The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels. In particular, they encode the semantic and syntactical relationship between sentences within the graph.

This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019). They introduce an additional universal feature vector which is added to each sentence node embedding. This universal feature vector is learned from a large unrelated and general corpus. This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.

Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a). The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities. The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens. The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence. Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram. Additionally, sequentially occurring words and entities are connected with a directed edge. This setup shows how one can encode a substantial amount of implicit information in an explicit manner.

HAHSum uses a GAT for each of the five node type combinations found within the graph. Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.

The results for HAHSum show that GNNs can perform very well. The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset. The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS . The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness. Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by . Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents. That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution. These sub-graphs are then ranked and thereby selected for a summary. This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem. Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","1. (What is the focus of the discussion on standalone GNN models?): sent1
    1.1. (Why is HeterSumGraph (HSG) chosen for discussion?): sent2
        1.1.1. (Where can the general concepts of HSG be seen?): sent3
    1.2. (How does the HSG model encode text into a graph?): sent4
        1.2.1. (How are the connections between nodes decided in HSG?): sent5, sent6
        1.2.2. (What is the flexibility of this structure?): sent7
    1.3. (What does Figure 3 illustrate?): sent8
    1.4. (How are the feature vectors and edge weights obtained in HSG?): sent9
    1.5. (What does the neural network in HSG consist of?): sent10
        1.5.1. (How is the GAT modified in HSG?): sent11
        1.5.2. (What additional layer is applied in HSG?): sent12
        1.5.3. (How many convolution layers are used in HSG?): sent13
    1.6. (What is the training task for the HSG model?): sent14
        1.6.1. (How is the classification done in HSG?): sent15
        1.6.2. (How does the model produce the summary?): sent16
            1.6.2.1. (What technique is used during sentence selection?): sent17
    1.7. (What are the results of the HSG model?): sent18
        1.7.1. (What should be noted about the HSG model?): sent19
    1.8. (What is a precursor to the HSG architecture?): sent20
    1.9. (What extension to the HSG model is proposed by Ya et al. (2021)?): sent21
        1.9.1. (How is the model modified for query constraints?): sent22
            1.9.1.1. (How is the query node added to the graph structure?): sent23
            1.9.1.2. (What mechanism is introduced during training?): sent24
    1.10. (What model follows the HSG structure?): sent25
        1.10.1. (How is the attention mechanism extended in this model?): sent26
    1.11. (What does the GNN model by Jing et al. (2021) encode?): sent27
        1.11.1. (What relationships are encoded within the graph?): sent28
    1.12. (What idea is followed by Antognini and Faltings (2019)?): sent29
        1.12.1. (What additional feature is introduced?): sent30
            1.12.1.1. (How is the universal feature vector learned?): sent31
        1.12.2. (What is unique about this model?): sent32
    1.13. (What is the HAHSum model by Jia et al. (2020a)?): sent33
        1.13.1. (How is the input graph for HAHSum constructed?): sent34
            1.13.1.1. (What types of nodes are used in HAHSum?): sent35
            1.13.1.2. (How is the graph built in HAHSum?): sent36, sent37, sent38
        1.13.2. (What does this setup show?): sent39
        1.13.3. (How does HAHSum use GAT?): sent40
            1.13.3.1. (What layers are applied in HAHSum?): sent41
        1.13.4. (What are the results for HAHSum?): sent42
            1.13.4.1. (On which datasets was HAHSum tested?): sent43
            1.13.4.2. (How does HAHSum perform compared to other models?): sent44
            1.13.4.3. (What do the results of the Amazon Mechanical Turk experiment show?): sent45
    1.14. (What is another recent GNN model for multi-document summarization?): sent46
        1.14.1. (How does the SgSum model differ from other approaches?): sent47
            1.14.1.1. (How does SgSum transform documents and generate sub-graphs?): sent48
            1.14.1.2. (How are sub-graphs selected for a summary?): sent49
            1.14.1.3. (What is innovative about the SgSum approach?): sent50
            1.14.1.4. (What does SgSum output?): sent51",How is the input graph for HAHSum constructed?,"sent34, sent35, sent36, sent37, sent38","The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities.

The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens.

The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence.

Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram.

Additionally, sequentially occurring words and entities are connected with a directed edge.","The construction of the input graph for HAHSum aims to significantly reduce semantic sparsity by utilizing named entities.

The model uses three types of nodes: named entity nodes, word nodes, and sentence nodes, with the named entity nodes being anonymized tokens.

Word nodes are connected with a directed edge to a sentence node if they occur within the sentence.

Two named entities are connected with an undirected edge if they represent the same entity, and two sentence nodes are connected with an undirected edge if they share a trigram.

Additionally, sequentially occurring words and entities are connected with a directed edge.","Questions:

How is the input graph for HAHSum constructed?

Answer:

The construction of the input graph for HAHSum aims to significantly reduce semantic sparsity by utilizing named entities.

The model uses three types of nodes: named entity nodes, word nodes, and sentence nodes, with the named entity nodes being anonymized tokens.

Word nodes are connected with a directed edge to a sentence node if they occur within the sentence.

Two named entities are connected with an undirected edge if they represent the same entity, and two sentence nodes are connected with an undirected edge if they share a trigram.

Additionally, sequentially occurring words and entities are connected with a directed edge."
A Survey of Automatic Text Summarization using Graph Neural Networks,Standalone GNNs,"We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by . We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS. An illustration of the general concepts presented here can be seen in Figure 3.

The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes. The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge. The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document. This is a flexible structure, as it can be used in a single-document but also multidocument setting.  Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism. Inspired by HSG.

The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word. The neural network consists of a modified GAT layer. The GAT is modified to consider the TF-IDF value of the connecting edge. Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution. In total three convolution layers are used, word-sentence, sentence-word and word-document. The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.

The classification itself is done by a single linear layer. The model then does not directly use the predicted nodes to produce the summary. Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.

The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset. One should especially note the flexibility and ability to use this model for two tasks.

An older model by Muratore et al. (2010) can be considered a precursor to this architecture. A simple extension to the HSG model is proposed by Ya et al. (2021). In their extension they modify the model for query constraints for the summary. This is achieved by adding a query node to the graph structure. Additionally, they introduce a mu-tual information maximization mechanism during training.

A model which further follows this structure is the one by Linmei et al. (2019). The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution. The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels. In particular, they encode the semantic and syntactical relationship between sentences within the graph.

This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019). They introduce an additional universal feature vector which is added to each sentence node embedding. This universal feature vector is learned from a large unrelated and general corpus. This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.

Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a). The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities. The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens. The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence. Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram. Additionally, sequentially occurring words and entities are connected with a directed edge. This setup shows how one can encode a substantial amount of implicit information in an explicit manner.

HAHSum uses a GAT for each of the five node type combinations found within the graph. Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.

The results for HAHSum show that GNNs can perform very well. The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset. The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS . The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness. Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by . Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents. That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution. These sub-graphs are then ranked and thereby selected for a summary. This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem. Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","1. (What is the focus of the discussion on standalone GNN models?): sent1
    1.1. (Why is HeterSumGraph (HSG) chosen for discussion?): sent2
        1.1.1. (Where can the general concepts of HSG be seen?): sent3
    1.2. (How does the HSG model encode text into a graph?): sent4
        1.2.1. (How are the connections between nodes decided in HSG?): sent5, sent6
        1.2.2. (What is the flexibility of this structure?): sent7
    1.3. (What does Figure 3 illustrate?): sent8
    1.4. (How are the feature vectors and edge weights obtained in HSG?): sent9
    1.5. (What does the neural network in HSG consist of?): sent10
        1.5.1. (How is the GAT modified in HSG?): sent11
        1.5.2. (What additional layer is applied in HSG?): sent12
        1.5.3. (How many convolution layers are used in HSG?): sent13
    1.6. (What is the training task for the HSG model?): sent14
        1.6.1. (How is the classification done in HSG?): sent15
        1.6.2. (How does the model produce the summary?): sent16
            1.6.2.1. (What technique is used during sentence selection?): sent17
    1.7. (What are the results of the HSG model?): sent18
        1.7.1. (What should be noted about the HSG model?): sent19
    1.8. (What is a precursor to the HSG architecture?): sent20
    1.9. (What extension to the HSG model is proposed by Ya et al. (2021)?): sent21
        1.9.1. (How is the model modified for query constraints?): sent22
            1.9.1.1. (How is the query node added to the graph structure?): sent23
            1.9.1.2. (What mechanism is introduced during training?): sent24
    1.10. (What model follows the HSG structure?): sent25
        1.10.1. (How is the attention mechanism extended in this model?): sent26
    1.11. (What does the GNN model by Jing et al. (2021) encode?): sent27
        1.11.1. (What relationships are encoded within the graph?): sent28
    1.12. (What idea is followed by Antognini and Faltings (2019)?): sent29
        1.12.1. (What additional feature is introduced?): sent30
            1.12.1.1. (How is the universal feature vector learned?): sent31
        1.12.2. (What is unique about this model?): sent32
    1.13. (What is the HAHSum model by Jia et al. (2020a)?): sent33
        1.13.1. (How is the input graph for HAHSum constructed?): sent34
            1.13.1.1. (What types of nodes are used in HAHSum?): sent35
            1.13.1.2. (How is the graph built in HAHSum?): sent36, sent37, sent38
        1.13.2. (What does this setup show?): sent39
        1.13.3. (How does HAHSum use GAT?): sent40
            1.13.3.1. (What layers are applied in HAHSum?): sent41
        1.13.4. (What are the results for HAHSum?): sent42
            1.13.4.1. (On which datasets was HAHSum tested?): sent43
            1.13.4.2. (How does HAHSum perform compared to other models?): sent44
            1.13.4.3. (What do the results of the Amazon Mechanical Turk experiment show?): sent45
    1.14. (What is another recent GNN model for multi-document summarization?): sent46
        1.14.1. (How does the SgSum model differ from other approaches?): sent47
            1.14.1.1. (How does SgSum transform documents and generate sub-graphs?): sent48
            1.14.1.2. (How are sub-graphs selected for a summary?): sent49
            1.14.1.3. (What is innovative about the SgSum approach?): sent50
            1.14.1.4. (What does SgSum output?): sent51",What are the results for HAHSum?,"sent42, sent43, sent44, sent45","The results for HAHSum show that GNNs can perform very well.

The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset.

The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS .

The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness.","The results for HAHSum show that GNNs can perform very well.

The model was tested on the CNN/Daily Mail, Newsroom, and NYT datasets.

The model outperforms very powerful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS.

The results of an Amazon Mechanical Turk experiment corroborate these findings and show that HAHSum produces summaries with superior fluency and conciseness for human readers.","Questions:

What are the results for HAHSum?

Answer:

The results for HAHSum show that GNNs can perform very well.

The model was tested on the CNN/Daily Mail, Newsroom, and NYT datasets.

The model outperforms very powerful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS.

The results of an Amazon Mechanical Turk experiment corroborate these findings and show that HAHSum produces summaries with superior fluency and conciseness for human readers."
A Survey of Automatic Text Summarization using Graph Neural Networks,Standalone GNNs,"We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by . We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS. An illustration of the general concepts presented here can be seen in Figure 3.

The HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes. The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge. The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document. This is a flexible structure, as it can be used in a single-document but also multidocument setting.  Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism. Inspired by HSG.

The feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word. The neural network consists of a modified GAT layer. The GAT is modified to consider the TF-IDF value of the connecting edge. Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution. In total three convolution layers are used, word-sentence, sentence-word and word-document. The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.

The classification itself is done by a single linear layer. The model then does not directly use the predicted nodes to produce the summary. Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.

The results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset. One should especially note the flexibility and ability to use this model for two tasks.

An older model by Muratore et al. (2010) can be considered a precursor to this architecture. A simple extension to the HSG model is proposed by Ya et al. (2021). In their extension they modify the model for query constraints for the summary. This is achieved by adding a query node to the graph structure. Additionally, they introduce a mu-tual information maximization mechanism during training.

A model which further follows this structure is the one by Linmei et al. (2019). The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution. The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels. In particular, they encode the semantic and syntactical relationship between sentences within the graph.

This idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019). They introduce an additional universal feature vector which is added to each sentence node embedding. This universal feature vector is learned from a large unrelated and general corpus. This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.

Taking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a). The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities. The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens. The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence. Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram. Additionally, sequentially occurring words and entities are connected with a directed edge. This setup shows how one can encode a substantial amount of implicit information in an explicit manner.

HAHSum uses a GAT for each of the five node type combinations found within the graph. Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.

The results for HAHSum show that GNNs can perform very well. The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset. The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS . The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness. Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by . Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents. That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution. These sub-graphs are then ranked and thereby selected for a summary. This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem. Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","1. (What is the focus of the discussion on standalone GNN models?): sent1
    1.1. (Why is HeterSumGraph (HSG) chosen for discussion?): sent2
        1.1.1. (Where can the general concepts of HSG be seen?): sent3
    1.2. (How does the HSG model encode text into a graph?): sent4
        1.2.1. (How are the connections between nodes decided in HSG?): sent5, sent6
        1.2.2. (What is the flexibility of this structure?): sent7
    1.3. (What does Figure 3 illustrate?): sent8
    1.4. (How are the feature vectors and edge weights obtained in HSG?): sent9
    1.5. (What does the neural network in HSG consist of?): sent10
        1.5.1. (How is the GAT modified in HSG?): sent11
        1.5.2. (What additional layer is applied in HSG?): sent12
        1.5.3. (How many convolution layers are used in HSG?): sent13
    1.6. (What is the training task for the HSG model?): sent14
        1.6.1. (How is the classification done in HSG?): sent15
        1.6.2. (How does the model produce the summary?): sent16
            1.6.2.1. (What technique is used during sentence selection?): sent17
    1.7. (What are the results of the HSG model?): sent18
        1.7.1. (What should be noted about the HSG model?): sent19
    1.8. (What is a precursor to the HSG architecture?): sent20
    1.9. (What extension to the HSG model is proposed by Ya et al. (2021)?): sent21
        1.9.1. (How is the model modified for query constraints?): sent22
            1.9.1.1. (How is the query node added to the graph structure?): sent23
            1.9.1.2. (What mechanism is introduced during training?): sent24
    1.10. (What model follows the HSG structure?): sent25
        1.10.1. (How is the attention mechanism extended in this model?): sent26
    1.11. (What does the GNN model by Jing et al. (2021) encode?): sent27
        1.11.1. (What relationships are encoded within the graph?): sent28
    1.12. (What idea is followed by Antognini and Faltings (2019)?): sent29
        1.12.1. (What additional feature is introduced?): sent30
            1.12.1.1. (How is the universal feature vector learned?): sent31
        1.12.2. (What is unique about this model?): sent32
    1.13. (What is the HAHSum model by Jia et al. (2020a)?): sent33
        1.13.1. (How is the input graph for HAHSum constructed?): sent34
            1.13.1.1. (What types of nodes are used in HAHSum?): sent35
            1.13.1.2. (How is the graph built in HAHSum?): sent36, sent37, sent38
        1.13.2. (What does this setup show?): sent39
        1.13.3. (How does HAHSum use GAT?): sent40
            1.13.3.1. (What layers are applied in HAHSum?): sent41
        1.13.4. (What are the results for HAHSum?): sent42
            1.13.4.1. (On which datasets was HAHSum tested?): sent43
            1.13.4.2. (How does HAHSum perform compared to other models?): sent44
            1.13.4.3. (What do the results of the Amazon Mechanical Turk experiment show?): sent45
    1.14. (What is another recent GNN model for multi-document summarization?): sent46
        1.14.1. (How does the SgSum model differ from other approaches?): sent47
            1.14.1.1. (How does SgSum transform documents and generate sub-graphs?): sent48
            1.14.1.2. (How are sub-graphs selected for a summary?): sent49
            1.14.1.3. (What is innovative about the SgSum approach?): sent50
            1.14.1.4. (What does SgSum output?): sent51",How does the SgSum model differ from other approaches?,"sent46, sent47, sent48, sent49, sent50, sent51","Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by .

Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents.

That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution.

These sub-graphs are then ranked and thereby selected for a summary.

This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem.

Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.","The SgSum model is a recent GNN model that has achieved great performance in the task of multi-document summarization.

Unlike other approaches, the SgSum model uses graph pooling to extract sub-graphs from encoded documents.

It first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution.

These sub-graphs are then ranked and selected for inclusion in the summary.

This innovative approach casts the problem of multi-document summarization as a simple subgraph selection problem.

Additionally, the SgSum model outputs an integral summary, where the entire summary is presented as a sub-graph of sentences.","Questions:

How does the SgSum model differ from other approaches?

Answer:

The SgSum model is a recent GNN model that has achieved great performance in the task of multi-document summarization.

Unlike other approaches, the SgSum model uses graph pooling to extract sub-graphs from encoded documents.

It first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution.

These sub-graphs are then ranked and selected for inclusion in the summary.

This innovative approach casts the problem of multi-document summarization as a simple subgraph selection problem.

Additionally, the SgSum model outputs an integral summary, where the entire summary is presented as a sub-graph of sentences."
A Survey of Active Learning for Natural Language Processing,Performance Prediction,"Predicting performance can be another indicator for querying. Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set. This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model. This strategy can be computationally costly since retraining is needed for each candidate.

Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set. Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b). This learning-to-select strategy may have some constraints. First, it requires labeled data (maybe from another domain) to train the policy. To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations. Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.

A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model. Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling. Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold. For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b).","1. (What is the role of predicting performance in querying?): sent1
    1.1. (Why should selected instances reduce future errors?): sent2
        1.1.1. (What strategy is motivated by reducing future errors?): sent3
            1.1.1.1. (What is a drawback of the expected error reduction strategy?): sent4
    1.2. (What are recent methods proposed to select instances?): sent5
        1.2.1. (What techniques are used to train policy models for instance selection?): sent6
        1.2.2. (What are the constraints of the learning-to-select strategy?): sent7
            1.2.2.1. (What is the first constraint of the learning-to-select strategy?): sent8
                1.2.2.1.1. (How can the reliance on labeled data be mitigated?): sent9
            1.2.2.2. (What is another constraint related to learning signals?): sent10
    1.3. (What is a simpler idea related to performance prediction?): sent11
        1.3.1. (How is the simpler idea applied in practice?): sent12
            1.3.1.1. (What is an example of training a smaller model for instance loss prediction?): sent12
            1.3.1.2. (What is an example of using a neural model for correctness judgment?): sent12
            1.3.1.3. (What is an example of learning a policy for expert querying in sequence labeling?): sent12
        1.3.2. (What is an example inspired by data maps for selecting ambiguous instances?): sent13
    1.4. (What special techniques are used in machine translation for seeking erroneous instances?): sent14",What motivates the expected error reduction strategy in performance prediction for querying?,"sent2, sent3, sent4","Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set.

This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model.

This strategy can be computationally costly since retraining is needed for each candidate.","The selected instances should ideally be the ones that most reduce future errors if labeled and added to the training set.

This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model.

This strategy can be computationally costly since retraining is needed for each candidate.","Questions:

What motivates the expected error reduction strategy in performance prediction for querying?

Answer:

The selected instances should ideally be the ones that most reduce future errors if labeled and added to the training set.

This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model.

This strategy can be computationally costly since retraining is needed for each candidate."
A Survey of Active Learning for Natural Language Processing,Performance Prediction,"Predicting performance can be another indicator for querying. Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set. This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model. This strategy can be computationally costly since retraining is needed for each candidate.

Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set. Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b). This learning-to-select strategy may have some constraints. First, it requires labeled data (maybe from another domain) to train the policy. To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations. Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.

A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model. Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling. Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold. For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b).","1. (What is the role of predicting performance in querying?): sent1
    1.1. (Why should selected instances reduce future errors?): sent2
        1.1.1. (What strategy is motivated by reducing future errors?): sent3
            1.1.1.1. (What is a drawback of the expected error reduction strategy?): sent4
    1.2. (What are recent methods proposed to select instances?): sent5
        1.2.1. (What techniques are used to train policy models for instance selection?): sent6
        1.2.2. (What are the constraints of the learning-to-select strategy?): sent7
            1.2.2.1. (What is the first constraint of the learning-to-select strategy?): sent8
                1.2.2.1.1. (How can the reliance on labeled data be mitigated?): sent9
            1.2.2.2. (What is another constraint related to learning signals?): sent10
    1.3. (What is a simpler idea related to performance prediction?): sent11
        1.3.1. (How is the simpler idea applied in practice?): sent12
            1.3.1.1. (What is an example of training a smaller model for instance loss prediction?): sent12
            1.3.1.2. (What is an example of using a neural model for correctness judgment?): sent12
            1.3.1.3. (What is an example of learning a policy for expert querying in sequence labeling?): sent12
        1.3.2. (What is an example inspired by data maps for selecting ambiguous instances?): sent13
    1.4. (What special techniques are used in machine translation for seeking erroneous instances?): sent14",What recent methods have been proposed to select instances that lead to fewer errors?,"sent5, sent6, sent7","Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set.

Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b).

This learning-to-select strategy may have some constraints.","Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set.

Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017; Fang et al., 2017; Liu et al., 2018a,b).

This learning-to-select strategy may have some constraints.","Questions:

What recent methods have been proposed to select instances that lead to fewer errors?

Answer:

Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set.

Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017; Fang et al., 2017; Liu et al., 2018a,b).

This learning-to-select strategy may have some constraints."
A Survey of Active Learning for Natural Language Processing,Performance Prediction,"Predicting performance can be another indicator for querying. Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set. This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model. This strategy can be computationally costly since retraining is needed for each candidate.

Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set. Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b). This learning-to-select strategy may have some constraints. First, it requires labeled data (maybe from another domain) to train the policy. To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations. Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.

A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model. Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling. Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold. For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b).","1. (What is the role of predicting performance in querying?): sent1
    1.1. (Why should selected instances reduce future errors?): sent2
        1.1.1. (What strategy is motivated by reducing future errors?): sent3
            1.1.1.1. (What is a drawback of the expected error reduction strategy?): sent4
    1.2. (What are recent methods proposed to select instances?): sent5
        1.2.1. (What techniques are used to train policy models for instance selection?): sent6
        1.2.2. (What are the constraints of the learning-to-select strategy?): sent7
            1.2.2.1. (What is the first constraint of the learning-to-select strategy?): sent8
                1.2.2.1.1. (How can the reliance on labeled data be mitigated?): sent9
            1.2.2.2. (What is another constraint related to learning signals?): sent10
    1.3. (What is a simpler idea related to performance prediction?): sent11
        1.3.1. (How is the simpler idea applied in practice?): sent12
            1.3.1.1. (What is an example of training a smaller model for instance loss prediction?): sent12
            1.3.1.2. (What is an example of using a neural model for correctness judgment?): sent12
            1.3.1.3. (What is an example of learning a policy for expert querying in sequence labeling?): sent12
        1.3.2. (What is an example inspired by data maps for selecting ambiguous instances?): sent13
    1.4. (What special techniques are used in machine translation for seeking erroneous instances?): sent14",What are the constraints of the learning-to-select strategy in performance prediction?,"sent7, sent8, sent9, sent10","This learning-to-select strategy may have some constraints.

First, it requires labeled data (maybe from another domain) to train the policy.

To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations.

Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.","The learning-to-select strategy may have some constraints.

First, it requires labeled data (maybe from another domain) to train the policy.

To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for active learning simulations.

Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.","Questions:

What are the constraints of the learning-to-select strategy in performance prediction?

Answer:

The learning-to-select strategy may have some constraints.

First, it requires labeled data (maybe from another domain) to train the policy.

To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for active learning simulations.

Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks."
A Survey of Active Learning for Natural Language Processing,Performance Prediction,"Predicting performance can be another indicator for querying. Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set. This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model. This strategy can be computationally costly since retraining is needed for each candidate.

Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set. Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b). This learning-to-select strategy may have some constraints. First, it requires labeled data (maybe from another domain) to train the policy. To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations. Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.

A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model. Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling. Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold. For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b).","1. (What is the role of predicting performance in querying?): sent1
    1.1. (Why should selected instances reduce future errors?): sent2
        1.1.1. (What strategy is motivated by reducing future errors?): sent3
            1.1.1.1. (What is a drawback of the expected error reduction strategy?): sent4
    1.2. (What are recent methods proposed to select instances?): sent5
        1.2.1. (What techniques are used to train policy models for instance selection?): sent6
        1.2.2. (What are the constraints of the learning-to-select strategy?): sent7
            1.2.2.1. (What is the first constraint of the learning-to-select strategy?): sent8
                1.2.2.1.1. (How can the reliance on labeled data be mitigated?): sent9
            1.2.2.2. (What is another constraint related to learning signals?): sent10
    1.3. (What is a simpler idea related to performance prediction?): sent11
        1.3.1. (How is the simpler idea applied in practice?): sent12
            1.3.1.1. (What is an example of training a smaller model for instance loss prediction?): sent12
            1.3.1.2. (What is an example of using a neural model for correctness judgment?): sent12
            1.3.1.3. (What is an example of learning a policy for expert querying in sequence labeling?): sent12
        1.3.2. (What is an example inspired by data maps for selecting ambiguous instances?): sent13
    1.4. (What special techniques are used in machine translation for seeking erroneous instances?): sent14",What is a simpler idea related to performance prediction for selecting instances?,"sent11, sent12, sent13","A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model.

Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling.

Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold.","A simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model.

Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have also been adopted for NLP (Cai et al., 2021). In a similar spirit, a neural model is employed to judge the correctness of the model prediction for SRL, and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling.

Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correctness over the training iterations is close to a predefined threshold.","Questions:

What is a simpler idea related to performance prediction for selecting instances?

Answer:

A simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model.

Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have also been adopted for NLP (Cai et al., 2021). In a similar spirit, a neural model is employed to judge the correctness of the model prediction for SRL, and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling.

Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correctness over the training iterations is close to a predefined threshold."
A Survey of Active Learning for Natural Language Processing,Density,"With the motivation to avoid outliers, density-based strategies prefer instances that are more representative of the unlabeled set. Selecting by n-gram or word counts (Ambati et al., 2010a;Zhao et al., 2020b) can be regarded as a simple way of density measurement. Generally, the common measurement is an instance's average similarity to all other instances (McCallum and Nigam, 1998;. While it may be costly to calculate similarities of all instance pairs, considering only k-nearest neighbor instances has been proposed as an alternative option (Zhu et al., , 2009).","# QUA Analysis:

1. (What is the motivation behind density-based strategies?): sent1
    1.1. (How do density-based strategies prefer instances?): sent1
    1.2. (What is a simple way of density measurement?): sent2
    1.3. (What is the common measurement for density?): sent3
        1.3.1. (What is the common measurement based on?): sent3
        1.3.2. (What is an alternative option to costly similarity calculations?): sent3",What is the motivation behind density-based strategies in active learning for NLP?,"sent1, sent2, sent3","With the motivation to avoid outliers, density-based strategies prefer instances that are more representative of the unlabeled set.

Selecting by n-gram or word counts (Ambati et al., 2010a;Zhao et al., 2020b) can be regarded as a simple way of density measurement.

Generally, the common measurement is an instance's average similarity to all other instances (McCallum and Nigam, 1998;. While it may be costly to calculate similarities of all instance pairs, considering only k-nearest neighbor instances has been proposed as an alternative option (Zhu et al., , 2009).","Density-based strategies in active learning for NLP are motivated by the desire to avoid outliers and prefer instances that are more representative of the unlabeled set.

Selecting by n-gram or word counts (Ambati et al., 2010a; Zhao et al., 2020b) can be regarded as a simple way of density measurement.

Generally, the common measurement is an instance's average similarity to all other instances (McCallum and Nigam, 1998). While it may be costly to calculate similarities of all instance pairs, considering only k-nearest neighbor instances has been proposed as an alternative option (Zhu et al., 2009).","Questions:

What is the motivation behind density-based strategies in active learning for NLP?

Answer:

Density-based strategies in active learning for NLP are motivated by the desire to avoid outliers and prefer instances that are more representative of the unlabeled set.

Selecting by n-gram or word counts (Ambati et al., 2010a; Zhao et al., 2020b) can be regarded as a simple way of density measurement.

Generally, the common measurement is an instance's average similarity to all other instances (McCallum and Nigam, 1998). While it may be costly to calculate similarities of all instance pairs, considering only k-nearest neighbor instances has been proposed as an alternative option (Zhu et al., 2009)."
A Survey of Active Learning for Natural Language Processing,Density,"With the motivation to avoid outliers, density-based strategies prefer instances that are more representative of the unlabeled set. Selecting by n-gram or word counts (Ambati et al., 2010a;Zhao et al., 2020b) can be regarded as a simple way of density measurement. Generally, the common measurement is an instance's average similarity to all other instances (McCallum and Nigam, 1998;. While it may be costly to calculate similarities of all instance pairs, considering only k-nearest neighbor instances has been proposed as an alternative option (Zhu et al., , 2009).","# QUA Analysis:

1. (What is the motivation behind density-based strategies?): sent1
    1.1. (How do density-based strategies prefer instances?): sent1
    1.2. (What is a simple way of density measurement?): sent2
    1.3. (What is the common measurement for density?): sent3
        1.3.1. (What is the common measurement based on?): sent3
        1.3.2. (What is an alternative option to costly similarity calculations?): sent3",How do density-based strategies prefer instances in active learning for NLP?,"sent1, sent2, sent3","With the motivation to avoid outliers, density-based strategies prefer instances that are more representative of the unlabeled set.

Selecting by n-gram or word counts (Ambati et al., 2010a;Zhao et al., 2020b) can be regarded as a simple way of density measurement.

Generally, the common measurement is an instance's average similarity to all other instances (McCallum and Nigam, 1998;. While it may be costly to calculate similarities of all instance pairs, considering only k-nearest neighbor instances has been proposed as an alternative option (Zhu et al., , 2009).","Density-based strategies prefer instances that are more representative of the unlabeled set to avoid outliers.

Selecting by n-gram or word counts (Ambati et al., 2010a; Zhao et al., 2020b) can be regarded as a simple way of density measurement.

Generally, the common measurement is an instance's average similarity to all other instances (McCallum and Nigam, 1998). While it may be costly to calculate similarities of all instance pairs, considering only k-nearest neighbor instances has been proposed as an alternative option (Zhu et al., 2009).","Questions:

How do density-based strategies prefer instances in active learning for NLP?

Answer:

Density-based strategies prefer instances that are more representative of the unlabeled set to avoid outliers.

Selecting by n-gram or word counts (Ambati et al., 2010a; Zhao et al., 2020b) can be regarded as a simple way of density measurement.

Generally, the common measurement is an instance's average similarity to all other instances (McCallum and Nigam, 1998). While it may be costly to calculate similarities of all instance pairs, considering only k-nearest neighbor instances has been proposed as an alternative option (Zhu et al., 2009)."
A Survey of Active Learning for Natural Language Processing,Output Uncertainty,"Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","1. What is uncertainty sampling and why is it commonly used?
    1.1. How does uncertainty sampling work?
        1.1.1. What does uncertainty sampling prefer? (sent2)
        1.1.2. What are the typical uncertainty sampling strategies for probabilistic models? (sent3)
        1.1.3. How are uncertainty-based strategies revisited with modern models? (sent4)
        1.1.4. How can similar ideas be applied to non-probabilistic models? (sent5)
2. What is another way to measure output uncertainty? (sent6)
    2.1. How does the model's output behave near the decision boundary? (sent7)
    2.2. What are the recent methods to check instances' local divergence? (sent8)",How can similar ideas to uncertainty sampling be applied to non-probabilistic models?,"sent5, sent6, sent7","For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.

If an instance is near the decision boundary, the model's outputs may be different within its local region.","For non-probabilistic models, similar ideas can be utilized by selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000; Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.

If an instance is near the decision boundary, the model's outputs may vary within its local region.","Questions:

How can similar ideas to uncertainty sampling be applied to non-probabilistic models?

Answer:

For non-probabilistic models, similar ideas can be utilized by selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000; Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.

If an instance is near the decision boundary, the model's outputs may vary within its local region."
A Survey of Active Learning for Natural Language Processing,Output Uncertainty,"Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","1. What is uncertainty sampling and why is it commonly used?
    1.1. How does uncertainty sampling work?
        1.1.1. What does uncertainty sampling prefer? (sent2)
        1.1.2. What are the typical uncertainty sampling strategies for probabilistic models? (sent3)
        1.1.3. How are uncertainty-based strategies revisited with modern models? (sent4)
        1.1.4. How can similar ideas be applied to non-probabilistic models? (sent5)
2. What is another way to measure output uncertainty? (sent6)
    2.1. How does the model's output behave near the decision boundary? (sent7)
    2.2. What are the recent methods to check instances' local divergence? (sent8)",What are the recent methods to check instances' local divergence?,"sent6, sent7, sent8","Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.

If an instance is near the decision boundary, the model's outputs may be different within its local region.

In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.

If an instance is near the decision boundary, the model's outputs may vary within its local region.

Recent works examine different ways to check instances' local divergence, such as nearest-neighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b), and data augmentation (Jiang et al., 2020).","Questions:

What are the recent methods to check instances' local divergence?

Answer:

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.

If an instance is near the decision boundary, the model's outputs may vary within its local region.

Recent works examine different ways to check instances' local divergence, such as nearest-neighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b), and data augmentation (Jiang et al., 2020)."
Transformers for Tabular Data Representation: A Survey of Models and Applications,Downstream Tasks,"Using neural representations for tabular data show, improvements in performance in several downstream tasks. In this section, we describe the tasks and define their input and output. While they all consume tables, settings can be quite heterogeneous, with systems exploiting different information, even for the same task. A summary of the covered tasks along their input, output, and some representative systems addressing them is shown in Table 4. We detail next the mandatory input elements and the different contexts.  (Dagan et al., 2013;Korman et al., 2018), checking facts with tables consists of verifying if a textual input claim is true or false against a trusted database (TAPEX, DECO, TABFACT), also provided as input. Some fact-checking systems, such as FEVEROUS, also output the cells used for verification as evidence (Nakov et al., 2021;Karagiannis et al., 2020).

Question Answering (QA): In the free text setting, QA aims at retrieving passages that include the answer to a given question. In the tabular data setting, it consists of returning as output the cells that answer an input consisting of a question and a table. One can distinguish two levels of complexity. Simple QA involves lookup queries on tables (DTR, CLTR), while a more complex QA  Most of the systems in this survey aim at improving accuracy in QA with respect to hand-crafted embeddings.

Semantic Parsing (SP): In the tabular data setting, given a question and a table as input, SP generates a declarative query in SQL over the table's schema to retrieve the answer to the question. While in QA the interest is in directly getting the answer, SP produces the (interpretable) query to obtain it (TABERT, GRAPPA, TAPEX).  table that can be used to answer the question. TR is helpful when trying to reduce the search space for a QA task (GTR, DTR, MMR). It is a challenging task given the limited input size of transformers, that is, their constraint to sequences of 512 tokens.  We observe that most tasks can be seen as traditional NLP problems where structured data replace free text, such as the case of QA where answers are located in tabular data instead of documents (Gupta and Gupta, 2012). TFC involves retrieving cells that entail or refute a given statement, whereas on free text the corresponding objective is to select sentences as evidence (Thorne et al., 2018). SP is the task of converting natural language utterance into a logical form (Berant and Liang, 2014), which in this setting is expressed as a declarative query over a relational table. TR on tabular data corresponds to passage retrieval on free text (Kaszkiel and Zobel, 1997). TCP is analogous to predicting missing words or values in a sentence . Finally, TMP can be related to syntactic parsing in NLP (Van Gompel and Pickering, 2007), where relationships between different tokens are depicted.

We conclude this section with an analysis of the performance of the systems over the different downstream tasks. For every task, we selected datasets for which at least two systems have reported results. All datasets reported in Table 5 (Ghasemi-Gol and Szekely, 2018), which contains web tables annotated with their type (relational, entity, matrix, list, and non-data), and EntiTab (Zhang and Balog, 2017), which contains web tables annotated with possible header labels for the column population task. Table 5 also contains the size, expressed as number of parameters, of the largest model used by every system. As some systems are not comparable on any shared datasets, we report here their size: TABFACT (110M), MMR (87M), TURL (314M), RPT (139M), and CLTR (235M). Larger models do not correlate with better performance across different systems for the same task, but a larger model always brings higher scores for the same system, as expected. Execution times for training and testing depend on the size of the model and the computing architecture.

The results in the table show that some tasks, such as TFC and TMP, can already be handled successfully by the systems, while some tasks, such as TCP, TR, and SP, are harder. QA is the task supported by most systems and the quality of the results vary depending on the dataset at hand. Differences in performance can be explained with different improvements across the systems. For example, MATE has better performance with respect to TAPAS in two tasks because of its mechanism to deal with larger input tables. Similarly, TUTA improves over TABERT because it handles tabular data beyond relational tables. Finally, TABERT and TAPAS are the systems that show most coverage in terms of tasks, with multiple papers using them as baselines in their experiments. For the systems that are not reported in Table 5, we notice that TURL obtains similar F1 results for column type prediction, but on a dataset different from VizNet; TURL, TABBIE, and TABERT also report comparable MAP results for the row population task (not in Table 5) over different datasets.","# QUA Analysis:

1. (What is the general observation about neural representations for tabular data?): sent1
    1.1. (What will be described in this section?): sent2
        1.1.1. (What is the nature of the settings for these tasks?): sent3
        1.1.2. (What does Table 4 summarize?): sent4
        1.1.3. (What will be detailed next?): sent5
2. (What is fact-checking with tables?): sent6
    2.1. (What do some fact-checking systems output?): sent7
3. (What is Question Answering (QA) in the tabular data setting?): sent8, sent9
    3.1. (What are the levels of complexity in QA?): sent10
        3.1.1. (What is simple QA?): sent11
4. (What is Semantic Parsing (SP) in the tabular data setting?): sent12
    4.1. (How does SP differ from QA?): sent13
5. (What is the role of TR in QA tasks?): sent14
    5.1. (What is the challenge in TR?): sent15
6. (How can most tasks be seen?): sent16
    6.1. (What does TFC involve?): sent17
    6.2. (What is SP in general?): sent18
    6.3. (What does TR on tabular data correspond to?): sent19
    6.4. (What is TCP analogous to?): sent20
    6.5. (What is TMP related to?): sent21
7. (What will be concluded in this section?): sent22
    7.1. (What datasets were selected for the analysis?): sent23
        7.1.1. (What datasets are reported in Table 5?): sent24
        7.1.2. (What additional information does Table 5 contain?): sent25
        7.1.3. (What is reported for systems not comparable on shared datasets?): sent26
    7.2. (What is the correlation between model size and performance?): sent27
    7.3. (What do execution times depend on?): sent28
    7.4. (What do the results in the table show about task difficulty?): sent29
    7.5. (What is the task supported by most systems and how does the quality vary?): sent30
    7.6. (What explains differences in performance?): sent31
        7.6.1. (What is an example of better performance due to a specific mechanism?): sent32
        7.6.2. (What is another example of improved performance?): sent33
    7.7. (Which systems show the most coverage in terms of tasks?): sent34
    7.8. (What is noticed about systems not reported in Table 5?): sent35",What are the different downstream tasks for neural representations of tabular data?,"sent1, sent2, sent3, sent4, sent5","Using neural representations for tabular data show, improvements in performance in several downstream tasks.

In this section, we describe the tasks and define their input and output.

While they all consume tables, settings can be quite heterogeneous, with systems exploiting different information, even for the same task.

A summary of the covered tasks along their input, output, and some representative systems addressing them is shown in Table 4.

We detail next the mandatory input elements and the different contexts.","Using neural representations for tabular data shows improvements in performance in several downstream tasks.

The tasks and their input and output are described.

While they all consume tables, settings can be quite heterogeneous, with systems exploiting different information, even for the same task.

The covered tasks vary in their input, output, and the representative systems addressing them.

Next, the mandatory input elements and the different contexts are detailed.","Questions:

What are the different downstream tasks for neural representations of tabular data?

Answer:

Using neural representations for tabular data shows improvements in performance in several downstream tasks.

The tasks and their input and output are described.

While they all consume tables, settings can be quite heterogeneous, with systems exploiting different information, even for the same task.

The covered tasks vary in their input, output, and the representative systems addressing them.

Next, the mandatory input elements and the different contexts are detailed."
Transformers for Tabular Data Representation: A Survey of Models and Applications,Downstream Tasks,"Using neural representations for tabular data show, improvements in performance in several downstream tasks. In this section, we describe the tasks and define their input and output. While they all consume tables, settings can be quite heterogeneous, with systems exploiting different information, even for the same task. A summary of the covered tasks along their input, output, and some representative systems addressing them is shown in Table 4. We detail next the mandatory input elements and the different contexts.  (Dagan et al., 2013;Korman et al., 2018), checking facts with tables consists of verifying if a textual input claim is true or false against a trusted database (TAPEX, DECO, TABFACT), also provided as input. Some fact-checking systems, such as FEVEROUS, also output the cells used for verification as evidence (Nakov et al., 2021;Karagiannis et al., 2020).

Question Answering (QA): In the free text setting, QA aims at retrieving passages that include the answer to a given question. In the tabular data setting, it consists of returning as output the cells that answer an input consisting of a question and a table. One can distinguish two levels of complexity. Simple QA involves lookup queries on tables (DTR, CLTR), while a more complex QA  Most of the systems in this survey aim at improving accuracy in QA with respect to hand-crafted embeddings.

Semantic Parsing (SP): In the tabular data setting, given a question and a table as input, SP generates a declarative query in SQL over the table's schema to retrieve the answer to the question. While in QA the interest is in directly getting the answer, SP produces the (interpretable) query to obtain it (TABERT, GRAPPA, TAPEX).  table that can be used to answer the question. TR is helpful when trying to reduce the search space for a QA task (GTR, DTR, MMR). It is a challenging task given the limited input size of transformers, that is, their constraint to sequences of 512 tokens.  We observe that most tasks can be seen as traditional NLP problems where structured data replace free text, such as the case of QA where answers are located in tabular data instead of documents (Gupta and Gupta, 2012). TFC involves retrieving cells that entail or refute a given statement, whereas on free text the corresponding objective is to select sentences as evidence (Thorne et al., 2018). SP is the task of converting natural language utterance into a logical form (Berant and Liang, 2014), which in this setting is expressed as a declarative query over a relational table. TR on tabular data corresponds to passage retrieval on free text (Kaszkiel and Zobel, 1997). TCP is analogous to predicting missing words or values in a sentence . Finally, TMP can be related to syntactic parsing in NLP (Van Gompel and Pickering, 2007), where relationships between different tokens are depicted.

We conclude this section with an analysis of the performance of the systems over the different downstream tasks. For every task, we selected datasets for which at least two systems have reported results. All datasets reported in Table 5 (Ghasemi-Gol and Szekely, 2018), which contains web tables annotated with their type (relational, entity, matrix, list, and non-data), and EntiTab (Zhang and Balog, 2017), which contains web tables annotated with possible header labels for the column population task. Table 5 also contains the size, expressed as number of parameters, of the largest model used by every system. As some systems are not comparable on any shared datasets, we report here their size: TABFACT (110M), MMR (87M), TURL (314M), RPT (139M), and CLTR (235M). Larger models do not correlate with better performance across different systems for the same task, but a larger model always brings higher scores for the same system, as expected. Execution times for training and testing depend on the size of the model and the computing architecture.

The results in the table show that some tasks, such as TFC and TMP, can already be handled successfully by the systems, while some tasks, such as TCP, TR, and SP, are harder. QA is the task supported by most systems and the quality of the results vary depending on the dataset at hand. Differences in performance can be explained with different improvements across the systems. For example, MATE has better performance with respect to TAPAS in two tasks because of its mechanism to deal with larger input tables. Similarly, TUTA improves over TABERT because it handles tabular data beyond relational tables. Finally, TABERT and TAPAS are the systems that show most coverage in terms of tasks, with multiple papers using them as baselines in their experiments. For the systems that are not reported in Table 5, we notice that TURL obtains similar F1 results for column type prediction, but on a dataset different from VizNet; TURL, TABBIE, and TABERT also report comparable MAP results for the row population task (not in Table 5) over different datasets.","# QUA Analysis:

1. (What is the general observation about neural representations for tabular data?): sent1
    1.1. (What will be described in this section?): sent2
        1.1.1. (What is the nature of the settings for these tasks?): sent3
        1.1.2. (What does Table 4 summarize?): sent4
        1.1.3. (What will be detailed next?): sent5
2. (What is fact-checking with tables?): sent6
    2.1. (What do some fact-checking systems output?): sent7
3. (What is Question Answering (QA) in the tabular data setting?): sent8, sent9
    3.1. (What are the levels of complexity in QA?): sent10
        3.1.1. (What is simple QA?): sent11
4. (What is Semantic Parsing (SP) in the tabular data setting?): sent12
    4.1. (How does SP differ from QA?): sent13
5. (What is the role of TR in QA tasks?): sent14
    5.1. (What is the challenge in TR?): sent15
6. (How can most tasks be seen?): sent16
    6.1. (What does TFC involve?): sent17
    6.2. (What is SP in general?): sent18
    6.3. (What does TR on tabular data correspond to?): sent19
    6.4. (What is TCP analogous to?): sent20
    6.5. (What is TMP related to?): sent21
7. (What will be concluded in this section?): sent22
    7.1. (What datasets were selected for the analysis?): sent23
        7.1.1. (What datasets are reported in Table 5?): sent24
        7.1.2. (What additional information does Table 5 contain?): sent25
        7.1.3. (What is reported for systems not comparable on shared datasets?): sent26
    7.2. (What is the correlation between model size and performance?): sent27
    7.3. (What do execution times depend on?): sent28
    7.4. (What do the results in the table show about task difficulty?): sent29
    7.5. (What is the task supported by most systems and how does the quality vary?): sent30
    7.6. (What explains differences in performance?): sent31
        7.6.1. (What is an example of better performance due to a specific mechanism?): sent32
        7.6.2. (What is another example of improved performance?): sent33
    7.7. (Which systems show the most coverage in terms of tasks?): sent34
    7.8. (What is noticed about systems not reported in Table 5?): sent35",What is Question Answering (QA) in the context of tabular data and its levels of complexity?,"sent8, sent9, sent10, sent11","Question Answering (QA): In the free text setting, QA aims at retrieving passages that include the answer to a given question.

In the tabular data setting, it consists of returning as output the cells that answer an input consisting of a question and a table.

One can distinguish two levels of complexity.

Simple QA involves lookup queries on tables (DTR, CLTR), while a more complex QA  Most of the systems in this survey aim at improving accuracy in QA with respect to hand-crafted embeddings.","Question Answering (QA) in the context of tabular data aims at retrieving passages that include the answer to a given question.

In the tabular data setting, it consists of returning as output the cells that answer an input consisting of a question and a table.

One can distinguish two levels of complexity.

Simple QA involves lookup queries on tables (DTR, CLTR), while more complex QA aims at improving accuracy with respect to hand-crafted embeddings.","Questions:

What is Question Answering (QA) in the context of tabular data and its levels of complexity?

Answer:

Question Answering (QA) in the context of tabular data aims at retrieving passages that include the answer to a given question.

In the tabular data setting, it consists of returning as output the cells that answer an input consisting of a question and a table.

One can distinguish two levels of complexity.

Simple QA involves lookup queries on tables (DTR, CLTR), while more complex QA aims at improving accuracy with respect to hand-crafted embeddings."
Transformers for Tabular Data Representation: A Survey of Models and Applications,Downstream Tasks,"Using neural representations for tabular data show, improvements in performance in several downstream tasks. In this section, we describe the tasks and define their input and output. While they all consume tables, settings can be quite heterogeneous, with systems exploiting different information, even for the same task. A summary of the covered tasks along their input, output, and some representative systems addressing them is shown in Table 4. We detail next the mandatory input elements and the different contexts.  (Dagan et al., 2013;Korman et al., 2018), checking facts with tables consists of verifying if a textual input claim is true or false against a trusted database (TAPEX, DECO, TABFACT), also provided as input. Some fact-checking systems, such as FEVEROUS, also output the cells used for verification as evidence (Nakov et al., 2021;Karagiannis et al., 2020).

Question Answering (QA): In the free text setting, QA aims at retrieving passages that include the answer to a given question. In the tabular data setting, it consists of returning as output the cells that answer an input consisting of a question and a table. One can distinguish two levels of complexity. Simple QA involves lookup queries on tables (DTR, CLTR), while a more complex QA  Most of the systems in this survey aim at improving accuracy in QA with respect to hand-crafted embeddings.

Semantic Parsing (SP): In the tabular data setting, given a question and a table as input, SP generates a declarative query in SQL over the table's schema to retrieve the answer to the question. While in QA the interest is in directly getting the answer, SP produces the (interpretable) query to obtain it (TABERT, GRAPPA, TAPEX).  table that can be used to answer the question. TR is helpful when trying to reduce the search space for a QA task (GTR, DTR, MMR). It is a challenging task given the limited input size of transformers, that is, their constraint to sequences of 512 tokens.  We observe that most tasks can be seen as traditional NLP problems where structured data replace free text, such as the case of QA where answers are located in tabular data instead of documents (Gupta and Gupta, 2012). TFC involves retrieving cells that entail or refute a given statement, whereas on free text the corresponding objective is to select sentences as evidence (Thorne et al., 2018). SP is the task of converting natural language utterance into a logical form (Berant and Liang, 2014), which in this setting is expressed as a declarative query over a relational table. TR on tabular data corresponds to passage retrieval on free text (Kaszkiel and Zobel, 1997). TCP is analogous to predicting missing words or values in a sentence . Finally, TMP can be related to syntactic parsing in NLP (Van Gompel and Pickering, 2007), where relationships between different tokens are depicted.

We conclude this section with an analysis of the performance of the systems over the different downstream tasks. For every task, we selected datasets for which at least two systems have reported results. All datasets reported in Table 5 (Ghasemi-Gol and Szekely, 2018), which contains web tables annotated with their type (relational, entity, matrix, list, and non-data), and EntiTab (Zhang and Balog, 2017), which contains web tables annotated with possible header labels for the column population task. Table 5 also contains the size, expressed as number of parameters, of the largest model used by every system. As some systems are not comparable on any shared datasets, we report here their size: TABFACT (110M), MMR (87M), TURL (314M), RPT (139M), and CLTR (235M). Larger models do not correlate with better performance across different systems for the same task, but a larger model always brings higher scores for the same system, as expected. Execution times for training and testing depend on the size of the model and the computing architecture.

The results in the table show that some tasks, such as TFC and TMP, can already be handled successfully by the systems, while some tasks, such as TCP, TR, and SP, are harder. QA is the task supported by most systems and the quality of the results vary depending on the dataset at hand. Differences in performance can be explained with different improvements across the systems. For example, MATE has better performance with respect to TAPAS in two tasks because of its mechanism to deal with larger input tables. Similarly, TUTA improves over TABERT because it handles tabular data beyond relational tables. Finally, TABERT and TAPAS are the systems that show most coverage in terms of tasks, with multiple papers using them as baselines in their experiments. For the systems that are not reported in Table 5, we notice that TURL obtains similar F1 results for column type prediction, but on a dataset different from VizNet; TURL, TABBIE, and TABERT also report comparable MAP results for the row population task (not in Table 5) over different datasets.","# QUA Analysis:

1. (What is the general observation about neural representations for tabular data?): sent1
    1.1. (What will be described in this section?): sent2
        1.1.1. (What is the nature of the settings for these tasks?): sent3
        1.1.2. (What does Table 4 summarize?): sent4
        1.1.3. (What will be detailed next?): sent5
2. (What is fact-checking with tables?): sent6
    2.1. (What do some fact-checking systems output?): sent7
3. (What is Question Answering (QA) in the tabular data setting?): sent8, sent9
    3.1. (What are the levels of complexity in QA?): sent10
        3.1.1. (What is simple QA?): sent11
4. (What is Semantic Parsing (SP) in the tabular data setting?): sent12
    4.1. (How does SP differ from QA?): sent13
5. (What is the role of TR in QA tasks?): sent14
    5.1. (What is the challenge in TR?): sent15
6. (How can most tasks be seen?): sent16
    6.1. (What does TFC involve?): sent17
    6.2. (What is SP in general?): sent18
    6.3. (What does TR on tabular data correspond to?): sent19
    6.4. (What is TCP analogous to?): sent20
    6.5. (What is TMP related to?): sent21
7. (What will be concluded in this section?): sent22
    7.1. (What datasets were selected for the analysis?): sent23
        7.1.1. (What datasets are reported in Table 5?): sent24
        7.1.2. (What additional information does Table 5 contain?): sent25
        7.1.3. (What is reported for systems not comparable on shared datasets?): sent26
    7.2. (What is the correlation between model size and performance?): sent27
    7.3. (What do execution times depend on?): sent28
    7.4. (What do the results in the table show about task difficulty?): sent29
    7.5. (What is the task supported by most systems and how does the quality vary?): sent30
    7.6. (What explains differences in performance?): sent31
        7.6.1. (What is an example of better performance due to a specific mechanism?): sent32
        7.6.2. (What is another example of improved performance?): sent33
    7.7. (Which systems show the most coverage in terms of tasks?): sent34
    7.8. (What is noticed about systems not reported in Table 5?): sent35",How can most tasks involving tabular data be seen in relation to traditional NLP problems?,"sent16, sent17, sent18, sent19, sent20, sent21","We observe that most tasks can be seen as traditional NLP problems where structured data replace free text, such as the case of QA where answers are located in tabular data instead of documents (Gupta and Gupta, 2012).

TFC involves retrieving cells that entail or refute a given statement, whereas on free text the corresponding objective is to select sentences as evidence (Thorne et al., 2018).

SP is the task of converting natural language utterance into a logical form (Berant and Liang, 2014), which in this setting is expressed as a declarative query over a relational table.

TR on tabular data corresponds to passage retrieval on free text (Kaszkiel and Zobel, 1997).

TCP is analogous to predicting missing words or values in a sentence .

Finally, TMP can be related to syntactic parsing in NLP (Van Gompel and Pickering, 2007), where relationships between different tokens are depicted.","Most tasks involving tabular data can be seen as traditional NLP problems where structured data replace free text, such as the case of QA where answers are located in tabular data instead of documents (Gupta and Gupta, 2012).

TFC involves retrieving cells that entail or refute a given statement, whereas on free text the corresponding objective is to select sentences as evidence (Thorne et al., 2018).

SP is the task of converting natural language utterances into a logical form (Berant and Liang, 2014), which in this setting is expressed as a declarative query over a relational table.

TR on tabular data corresponds to passage retrieval on free text (Kaszkiel and Zobel, 1997).

TCP is analogous to predicting missing words or values in a sentence.

Finally, TMP can be related to syntactic parsing in NLP (Van Gompel and Pickering, 2007), where relationships between different tokens are depicted.","Questions:

How can most tasks involving tabular data be seen in relation to traditional NLP problems?

Answer:

Most tasks involving tabular data can be seen as traditional NLP problems where structured data replace free text, such as the case of QA where answers are located in tabular data instead of documents (Gupta and Gupta, 2012).

TFC involves retrieving cells that entail or refute a given statement, whereas on free text the corresponding objective is to select sentences as evidence (Thorne et al., 2018).

SP is the task of converting natural language utterances into a logical form (Berant and Liang, 2014), which in this setting is expressed as a declarative query over a relational table.

TR on tabular data corresponds to passage retrieval on free text (Kaszkiel and Zobel, 1997).

TCP is analogous to predicting missing words or values in a sentence.

Finally, TMP can be related to syntactic parsing in NLP (Van Gompel and Pickering, 2007), where relationships between different tokens are depicted."
Transformers for Tabular Data Representation: A Survey of Models and Applications,Downstream Tasks,"Using neural representations for tabular data show, improvements in performance in several downstream tasks. In this section, we describe the tasks and define their input and output. While they all consume tables, settings can be quite heterogeneous, with systems exploiting different information, even for the same task. A summary of the covered tasks along their input, output, and some representative systems addressing them is shown in Table 4. We detail next the mandatory input elements and the different contexts.  (Dagan et al., 2013;Korman et al., 2018), checking facts with tables consists of verifying if a textual input claim is true or false against a trusted database (TAPEX, DECO, TABFACT), also provided as input. Some fact-checking systems, such as FEVEROUS, also output the cells used for verification as evidence (Nakov et al., 2021;Karagiannis et al., 2020).

Question Answering (QA): In the free text setting, QA aims at retrieving passages that include the answer to a given question. In the tabular data setting, it consists of returning as output the cells that answer an input consisting of a question and a table. One can distinguish two levels of complexity. Simple QA involves lookup queries on tables (DTR, CLTR), while a more complex QA  Most of the systems in this survey aim at improving accuracy in QA with respect to hand-crafted embeddings.

Semantic Parsing (SP): In the tabular data setting, given a question and a table as input, SP generates a declarative query in SQL over the table's schema to retrieve the answer to the question. While in QA the interest is in directly getting the answer, SP produces the (interpretable) query to obtain it (TABERT, GRAPPA, TAPEX).  table that can be used to answer the question. TR is helpful when trying to reduce the search space for a QA task (GTR, DTR, MMR). It is a challenging task given the limited input size of transformers, that is, their constraint to sequences of 512 tokens.  We observe that most tasks can be seen as traditional NLP problems where structured data replace free text, such as the case of QA where answers are located in tabular data instead of documents (Gupta and Gupta, 2012). TFC involves retrieving cells that entail or refute a given statement, whereas on free text the corresponding objective is to select sentences as evidence (Thorne et al., 2018). SP is the task of converting natural language utterance into a logical form (Berant and Liang, 2014), which in this setting is expressed as a declarative query over a relational table. TR on tabular data corresponds to passage retrieval on free text (Kaszkiel and Zobel, 1997). TCP is analogous to predicting missing words or values in a sentence . Finally, TMP can be related to syntactic parsing in NLP (Van Gompel and Pickering, 2007), where relationships between different tokens are depicted.

We conclude this section with an analysis of the performance of the systems over the different downstream tasks. For every task, we selected datasets for which at least two systems have reported results. All datasets reported in Table 5 (Ghasemi-Gol and Szekely, 2018), which contains web tables annotated with their type (relational, entity, matrix, list, and non-data), and EntiTab (Zhang and Balog, 2017), which contains web tables annotated with possible header labels for the column population task. Table 5 also contains the size, expressed as number of parameters, of the largest model used by every system. As some systems are not comparable on any shared datasets, we report here their size: TABFACT (110M), MMR (87M), TURL (314M), RPT (139M), and CLTR (235M). Larger models do not correlate with better performance across different systems for the same task, but a larger model always brings higher scores for the same system, as expected. Execution times for training and testing depend on the size of the model and the computing architecture.

The results in the table show that some tasks, such as TFC and TMP, can already be handled successfully by the systems, while some tasks, such as TCP, TR, and SP, are harder. QA is the task supported by most systems and the quality of the results vary depending on the dataset at hand. Differences in performance can be explained with different improvements across the systems. For example, MATE has better performance with respect to TAPAS in two tasks because of its mechanism to deal with larger input tables. Similarly, TUTA improves over TABERT because it handles tabular data beyond relational tables. Finally, TABERT and TAPAS are the systems that show most coverage in terms of tasks, with multiple papers using them as baselines in their experiments. For the systems that are not reported in Table 5, we notice that TURL obtains similar F1 results for column type prediction, but on a dataset different from VizNet; TURL, TABBIE, and TABERT also report comparable MAP results for the row population task (not in Table 5) over different datasets.","# QUA Analysis:

1. (What is the general observation about neural representations for tabular data?): sent1
    1.1. (What will be described in this section?): sent2
        1.1.1. (What is the nature of the settings for these tasks?): sent3
        1.1.2. (What does Table 4 summarize?): sent4
        1.1.3. (What will be detailed next?): sent5
2. (What is fact-checking with tables?): sent6
    2.1. (What do some fact-checking systems output?): sent7
3. (What is Question Answering (QA) in the tabular data setting?): sent8, sent9
    3.1. (What are the levels of complexity in QA?): sent10
        3.1.1. (What is simple QA?): sent11
4. (What is Semantic Parsing (SP) in the tabular data setting?): sent12
    4.1. (How does SP differ from QA?): sent13
5. (What is the role of TR in QA tasks?): sent14
    5.1. (What is the challenge in TR?): sent15
6. (How can most tasks be seen?): sent16
    6.1. (What does TFC involve?): sent17
    6.2. (What is SP in general?): sent18
    6.3. (What does TR on tabular data correspond to?): sent19
    6.4. (What is TCP analogous to?): sent20
    6.5. (What is TMP related to?): sent21
7. (What will be concluded in this section?): sent22
    7.1. (What datasets were selected for the analysis?): sent23
        7.1.1. (What datasets are reported in Table 5?): sent24
        7.1.2. (What additional information does Table 5 contain?): sent25
        7.1.3. (What is reported for systems not comparable on shared datasets?): sent26
    7.2. (What is the correlation between model size and performance?): sent27
    7.3. (What do execution times depend on?): sent28
    7.4. (What do the results in the table show about task difficulty?): sent29
    7.5. (What is the task supported by most systems and how does the quality vary?): sent30
    7.6. (What explains differences in performance?): sent31
        7.6.1. (What is an example of better performance due to a specific mechanism?): sent32
        7.6.2. (What is another example of improved performance?): sent33
    7.7. (Which systems show the most coverage in terms of tasks?): sent34
    7.8. (What is noticed about systems not reported in Table 5?): sent35",What datasets and additional information are reported in Table 5 for the analysis of system performance?,"sent23, sent24, sent25, sent26","For every task, we selected datasets for which at least two systems have reported results.

All datasets reported in Table 5 (Ghasemi-Gol and Szekely, 2018), which contains web tables annotated with their type (relational, entity, matrix, list, and non-data), and EntiTab (Zhang and Balog, 2017), which contains web tables annotated with possible header labels for the column population task.

Table 5 also contains the size, expressed as number of parameters, of the largest model used by every system.

As some systems are not comparable on any shared datasets, we report here their size: TABFACT (110M), MMR (87M), TURL (314M), RPT (139M), and CLTR (235M).","For every task, datasets were selected for which at least two systems have reported results.

All datasets include web tables annotated with their type (relational, entity, matrix, list, and non-data) (Ghasemi-Gol and Szekely, 2018) and web tables annotated with possible header labels for the column population task (Zhang and Balog, 2017).

The size, expressed as the number of parameters, of the largest model used by every system is also included.

As some systems are not comparable on any shared datasets, their sizes are reported as follows: TABFACT (110M), MMR (87M), TURL (314M), RPT (139M), and CLTR (235M).","Questions:

What datasets and additional information are reported in Table 5 for the analysis of system performance?

Answer:

For every task, datasets were selected for which at least two systems have reported results.

All datasets include web tables annotated with their type (relational, entity, matrix, list, and non-data) (Ghasemi-Gol and Szekely, 2018) and web tables annotated with possible header labels for the column population task (Zhang and Balog, 2017).

The size, expressed as the number of parameters, of the largest model used by every system is also included.

As some systems are not comparable on any shared datasets, their sizes are reported as follows: TABFACT (110M), MMR (87M), TURL (314M), RPT (139M), and CLTR (235M)."
Transformers for Tabular Data Representation: A Survey of Models and Applications,Downstream Tasks,"Using neural representations for tabular data show, improvements in performance in several downstream tasks. In this section, we describe the tasks and define their input and output. While they all consume tables, settings can be quite heterogeneous, with systems exploiting different information, even for the same task. A summary of the covered tasks along their input, output, and some representative systems addressing them is shown in Table 4. We detail next the mandatory input elements and the different contexts.  (Dagan et al., 2013;Korman et al., 2018), checking facts with tables consists of verifying if a textual input claim is true or false against a trusted database (TAPEX, DECO, TABFACT), also provided as input. Some fact-checking systems, such as FEVEROUS, also output the cells used for verification as evidence (Nakov et al., 2021;Karagiannis et al., 2020).

Question Answering (QA): In the free text setting, QA aims at retrieving passages that include the answer to a given question. In the tabular data setting, it consists of returning as output the cells that answer an input consisting of a question and a table. One can distinguish two levels of complexity. Simple QA involves lookup queries on tables (DTR, CLTR), while a more complex QA  Most of the systems in this survey aim at improving accuracy in QA with respect to hand-crafted embeddings.

Semantic Parsing (SP): In the tabular data setting, given a question and a table as input, SP generates a declarative query in SQL over the table's schema to retrieve the answer to the question. While in QA the interest is in directly getting the answer, SP produces the (interpretable) query to obtain it (TABERT, GRAPPA, TAPEX).  table that can be used to answer the question. TR is helpful when trying to reduce the search space for a QA task (GTR, DTR, MMR). It is a challenging task given the limited input size of transformers, that is, their constraint to sequences of 512 tokens.  We observe that most tasks can be seen as traditional NLP problems where structured data replace free text, such as the case of QA where answers are located in tabular data instead of documents (Gupta and Gupta, 2012). TFC involves retrieving cells that entail or refute a given statement, whereas on free text the corresponding objective is to select sentences as evidence (Thorne et al., 2018). SP is the task of converting natural language utterance into a logical form (Berant and Liang, 2014), which in this setting is expressed as a declarative query over a relational table. TR on tabular data corresponds to passage retrieval on free text (Kaszkiel and Zobel, 1997). TCP is analogous to predicting missing words or values in a sentence . Finally, TMP can be related to syntactic parsing in NLP (Van Gompel and Pickering, 2007), where relationships between different tokens are depicted.

We conclude this section with an analysis of the performance of the systems over the different downstream tasks. For every task, we selected datasets for which at least two systems have reported results. All datasets reported in Table 5 (Ghasemi-Gol and Szekely, 2018), which contains web tables annotated with their type (relational, entity, matrix, list, and non-data), and EntiTab (Zhang and Balog, 2017), which contains web tables annotated with possible header labels for the column population task. Table 5 also contains the size, expressed as number of parameters, of the largest model used by every system. As some systems are not comparable on any shared datasets, we report here their size: TABFACT (110M), MMR (87M), TURL (314M), RPT (139M), and CLTR (235M). Larger models do not correlate with better performance across different systems for the same task, but a larger model always brings higher scores for the same system, as expected. Execution times for training and testing depend on the size of the model and the computing architecture.

The results in the table show that some tasks, such as TFC and TMP, can already be handled successfully by the systems, while some tasks, such as TCP, TR, and SP, are harder. QA is the task supported by most systems and the quality of the results vary depending on the dataset at hand. Differences in performance can be explained with different improvements across the systems. For example, MATE has better performance with respect to TAPAS in two tasks because of its mechanism to deal with larger input tables. Similarly, TUTA improves over TABERT because it handles tabular data beyond relational tables. Finally, TABERT and TAPAS are the systems that show most coverage in terms of tasks, with multiple papers using them as baselines in their experiments. For the systems that are not reported in Table 5, we notice that TURL obtains similar F1 results for column type prediction, but on a dataset different from VizNet; TURL, TABBIE, and TABERT also report comparable MAP results for the row population task (not in Table 5) over different datasets.","# QUA Analysis:

1. (What is the general observation about neural representations for tabular data?): sent1
    1.1. (What will be described in this section?): sent2
        1.1.1. (What is the nature of the settings for these tasks?): sent3
        1.1.2. (What does Table 4 summarize?): sent4
        1.1.3. (What will be detailed next?): sent5
2. (What is fact-checking with tables?): sent6
    2.1. (What do some fact-checking systems output?): sent7
3. (What is Question Answering (QA) in the tabular data setting?): sent8, sent9
    3.1. (What are the levels of complexity in QA?): sent10
        3.1.1. (What is simple QA?): sent11
4. (What is Semantic Parsing (SP) in the tabular data setting?): sent12
    4.1. (How does SP differ from QA?): sent13
5. (What is the role of TR in QA tasks?): sent14
    5.1. (What is the challenge in TR?): sent15
6. (How can most tasks be seen?): sent16
    6.1. (What does TFC involve?): sent17
    6.2. (What is SP in general?): sent18
    6.3. (What does TR on tabular data correspond to?): sent19
    6.4. (What is TCP analogous to?): sent20
    6.5. (What is TMP related to?): sent21
7. (What will be concluded in this section?): sent22
    7.1. (What datasets were selected for the analysis?): sent23
        7.1.1. (What datasets are reported in Table 5?): sent24
        7.1.2. (What additional information does Table 5 contain?): sent25
        7.1.3. (What is reported for systems not comparable on shared datasets?): sent26
    7.2. (What is the correlation between model size and performance?): sent27
    7.3. (What do execution times depend on?): sent28
    7.4. (What do the results in the table show about task difficulty?): sent29
    7.5. (What is the task supported by most systems and how does the quality vary?): sent30
    7.6. (What explains differences in performance?): sent31
        7.6.1. (What is an example of better performance due to a specific mechanism?): sent32
        7.6.2. (What is another example of improved performance?): sent33
    7.7. (Which systems show the most coverage in terms of tasks?): sent34
    7.8. (What is noticed about systems not reported in Table 5?): sent35",What explains the differences in performance across systems and what are some examples?,"sent31, sent32, sent33","Differences in performance can be explained with different improvements across the systems.

For example, MATE has better performance with respect to TAPAS in two tasks because of its mechanism to deal with larger input tables.

Similarly, TUTA improves over TABERT because it handles tabular data beyond relational tables.","Differences in performance can be explained by various improvements across the systems.

For example, MATE has better performance compared to TAPAS in two tasks due to its mechanism for handling larger input tables.

Similarly, TUTA improves over TABERT because it handles tabular data beyond relational tables.","Questions:

What explains the differences in performance across systems and what are some examples?

Answer:

Differences in performance can be explained by various improvements across the systems.

For example, MATE has better performance compared to TAPAS in two tasks due to its mechanism for handling larger input tables.

Similarly, TUTA improves over TABERT because it handles tabular data beyond relational tables."
Transformers for Tabular Data Representation: A Survey of Models and Applications,Vanilla Transformer,"The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules. The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Residual connections and layer-normalization modules are also used. Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions.

The transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017;Raffel et al., 2020), an encoder-only , or decoder-only (Radford et al., 2019;Brown et al., 2020) model. The choice of the architecture depends on the final task. Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data. In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input. The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX).","1. (What is the vanilla transformer?): sent1
    1.1. (What does the encoder block consist of?): sent2
        1.1.1. (What is the purpose of multi-head attention in the encoder block?): sent3
        1.1.2. (What other modules are used in the encoder block?): sent4
    1.2. (What does the decoder block consist of?): sent5
2. (How can the transformer architecture be used?): sent6
    2.1. (What determines the choice of the architecture?): sent7
    2.2. (What are encoder-only models used for?): sent8
        2.2.1. (How is pretraining done for encoder-only models?): sent9
    2.3. (What is the encoder-decoder architecture used for?): sent10",What are the components of the vanilla transformer model?,"sent1, sent2, sent5","The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules.

The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network.

Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions.","The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules.

The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network.

Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions.","Questions:

What are the components of the vanilla transformer model?

Answer:

The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules.

The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network.

Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions."
Transformers for Tabular Data Representation: A Survey of Models and Applications,Training Datasets,"We present both the datasets used for pre-training and for fine-tuning in the downstream tasks. Pre-training tables are not annotated, in some cases scraped from the web, while data used for fine-tuning have task-dependent annotation labels. The datasets consist of tables and their context, such as table metadata, surrounding texts, claims or questions. To construct large pre-training datasets and in an attempt to reduce bias, multiple sources can be used, independently of the target task at hand. For instance, unlike TAPAS (Herzig et al., 2020), which only uses Wikipedia Tables for QA, and TABULARNET , which only uses Spreadsheets for TMP, TABERT  uses Wikipedia Tables and WDC  for SP; GRAPPA uses Wikipedia Tables, Spider, and WikiSQL for SP; MMR (Kostić et al., 2021) uses NQ, OTT-QA, and WikiSQL for TR; MATE  uses Wikipedia Tables and HybridQA for QA; and TUTA  uses Wikipedia Tables, WDC, and spreadsheets for TMP. In general, it is recommended to utilize different data sources for pre-training to ensure covering different kinds and content, and thus, improve the scope of representations. For instance, Wikipedia tables has a large number of relational tables (Bhagavatula et al., 2015), while WDC and Spreadsheets include also entity tables and spreadsheets with complex structure. Table 2 summarizes the main characteristics for the most common datasets. We mark the tasks for which the dataset has been used by ✔ under the column ''Task''. We note that the top four datasets are mostly used for pre-training, while the others can be used for fine-tuning as well since they include annotations for the target task, for example, questions/answers for QA. The column ''Large Tables'' is a binary indicator, where ✔ and ✘ indicate whether or not the tabular corpus include large tables and hence whether or not some pre-processing is needed to reduce table content to meet the limits of the transformer architecture (512 input tokens in most cases). Some works, such as TABERT, TAPEX (Liu et al., 2021a), and CLTR (Pan et al., 2021), apply filtering in any case to reduce noisy input. Finally, the ''Context'' column describes additional text that come with the tables. This can be text describing the table, such as caption or title of the document containing the table; table metadata, such as table orientation, header row, and keys; or questions and claims that can be addressed with the table.","1. What datasets are presented in this section?
    1.1. What are the types of datasets used for pre-training and fine-tuning? (sent1)
        1.1.1. How are pre-training and fine-tuning datasets different? (sent2)
        1.1.2. What do the datasets consist of? (sent3)
        1.1.3. How are large pre-training datasets constructed? (sent4)
            1.1.3.1. What are some examples of datasets used for different tasks? (sent5)
            1.1.3.2. Why is it recommended to use different data sources for pre-training? (sent6)
                1.1.3.2.1. What are the characteristics of different data sources? (sent7)
    1.2. How are the main characteristics of the most common datasets summarized? (sent8)
        1.2.1. How are tasks marked for each dataset? (sent9)
        1.2.2. What is noted about the top four datasets? (sent10)
        1.2.3. What does the ""Large Tables"" column indicate? (sent11)
            1.2.3.1. What preprocessing is applied to reduce noisy input? (sent12)
        1.2.4. What does the ""Context"" column describe? (sent13)
            1.2.4.1. What additional text can come with the tables? (sent14)",What are the types of datasets used for pre-training and fine-tuning in tabular data representation?,"sent1, sent2, sent3","We present both the datasets used for pre-training and for fine-tuning in the downstream tasks.

Pre-training tables are not annotated, in some cases scraped from the web, while data used for fine-tuning have task-dependent annotation labels.

The datasets consist of tables and their context, such as table metadata, surrounding texts, claims or questions.","The datasets used for pre-training and fine-tuning in tabular data representation are presented.

Pre-training tables are not annotated and are sometimes scraped from the web, while data used for fine-tuning have task-dependent annotation labels.

The datasets consist of tables and their context, such as table metadata, surrounding texts, claims, or questions.","Questions:

What are the types of datasets used for pre-training and fine-tuning in tabular data representation?

Answer:

The datasets used for pre-training and fine-tuning in tabular data representation are presented.

Pre-training tables are not annotated and are sometimes scraped from the web, while data used for fine-tuning have task-dependent annotation labels.

The datasets consist of tables and their context, such as table metadata, surrounding texts, claims, or questions."
Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources,Limitations,"On Dataset Documentation. Throughout our survey process, we found inadequate dataset documentation, limiting the coverage of our survey. We suggest that individual researchers provide the input data source and the labeling methodology; if people were involved in dataset creation, their demographic information should be provided, as well. Such information can help researchers analyze potential bias embedded in the dataset (Bender and Friedman, 2018).

Surveyed Dataset Collection Process. Despite our best efforts, we do not claim to cover all relevant datasets. Our collection process overlooks datasets that are published at non-ACL venues and not in Hugging Face as well as papers that do not match our search keywords. For instance, we missed multilingual machine reading comprehension datasets (Gupta and Khade, 2020;Asai et al., 2018) and morphology datasets (McCarthy et al., 2020). We also found a very low presence of indigenous language datasets. None of 10 indigenous American languages from a recent study (Ebrahimi et al., 2022) was represented in our survey. That said, we host http: //multilingual-dataset-survey.github.io where researchers can submit their dataset information and periodically update our analysis. Furthermore, we constantly encountered poorly written documentation or unavailable datasets during our annotation processes. During annotation, whenever this paper's dataset annotators encountered unclear documentation, they made their best guess to put datasets into predefined categories. If no evidence could be found for the inference, they put ""not mentioned"" as a result. All unclear decision were adjudicated by at least three annotators.

Using Country Names as a Proxy for Languages Spoken. In Section 5, we attempted to approximate the number of NLP researchers with language proficiency in different languages. To do this, we mapped the names of ACL submission country to the most commonly spoken languages in those countries. We acknowledge that (1) the country of origin of researchers might be different from the country of submission, (2) researchers native language might not be listed in the commonly spoken languages and (3) the mapping might be incomprehensive.

MTurk Pilot Study. Due to our limited data points, although our MTurk study showed that data quality could be improved if the language qualification were applied in the collection process on MTurk, and our previous recommendations do not currently apply, we acknowledge that more research at scale should be done to statistically confirm the conclusion. Furthermore, languages other than the supported 5 languages might still be unsuitable for gathering multilingual data on MTurk As we identify that many datasets use texts beyond Wikipedia (see Table 4), we instead use the mC4 corpora (Xue et al., 2021), a much larger collection of texts in 101 languages drawn from the public Common Crawl web scrape and used for training the mT5 model. Specifically, we use the number of tokens in mC4 to estimate the amount of unlabeled data. Figure 7a shows a scatter plot where the x-axis represents the number of tokens in mC4 and y-axis represents the number of labeled datasets available in the languages. The availability of unlabeled text corpora and the number of labeled datasets show a high correlation (ρ = 0.794). We also analyze the relationship between the number of labeled datasets and the number of Wikipedia articles in Figure 7b. Again we observe a high correlation of ρ = 0.767. Classification (sentiment analysis) annotated (authors, linguists) collected from Wikipedia collected from curated source (exams, scientific papers, etc) collected from media (news) collected from social media or commercial sources collected from web crowdsourced curated linguistic resources multiple modes template-based Figure 6: The distribution over input source per task type. The size of bar represents the # of datasets of that task type and input source.

(a) The number of tokens (billions) in mC4 (Xue et al., 2021).

(b) The number of Wikipedia articles. Figure 7: Relationships between the number of datasets and number of tokens in mC4 and Wikipedia articles. English is removed.","1. (What are the limitations related to dataset documentation?): sent1
    1.1. (What suggestions are made to improve dataset documentation?): sent2
    1.2. (Why is it important to provide detailed dataset documentation?): sent3
2. (What are the limitations of the surveyed dataset collection process?): sent4
    2.1. (What specific datasets were missed in the collection process?): sent5, sent6
    2.2. (What is the issue with indigenous language datasets?): sent7, sent8
    2.3. (What solution is proposed to address the limitations of the dataset collection process?): sent9
3. (What issues were encountered during the annotation process?): sent10
    3.1. (How did annotators handle unclear documentation during annotation?): sent11
    3.2. (What was done if no evidence could be found for the inference?): sent12
    3.3. (How were unclear decisions adjudicated?): sent13
4. (What are the limitations of using country names as a proxy for languages spoken?): sent14
    4.1. (How was the number of NLP researchers with language proficiency approximated?): sent15, sent16
    4.2. (What are the acknowledged limitations of this approximation?): sent17
5. (What are the limitations of the MTurk pilot study?): sent18
    5.1. (What is the conclusion regarding the applicability of previous recommendations?): sent19
6. (What is the relationship between the number of tokens in mC4 and the number of labeled datasets?): sent20
    6.1. (What does Figure 7a show?): sent21
    6.2. (What is the observed correlation between the availability of unlabeled text corpora and the number of labeled datasets?): sent22
7. (What is the relationship between the number of labeled datasets and the number of Wikipedia articles?): sent23
    7.1. (What does Figure 7b show?): sent24
8. (What does Figure 6 illustrate?): sent25
    8.1. (What does the size of the bar in Figure 6 represent?): sent26
9. (What do Figures 7a and 7b illustrate?): sent27, sent28
    9.1. (What is the overall relationship shown in Figure 7?): sent29
10. (What is the specific note about English in the analysis?): sent30",What are the limitations related to dataset documentation?,"sent1, sent10, sent11, sent12, sent13","On Dataset Documentation. Throughout our survey process, we found inadequate dataset documentation, limiting the coverage of our survey.

Furthermore, we constantly encountered poorly written documentation or unavailable datasets during our annotation processes.

During annotation, whenever this paper's dataset annotators encountered unclear documentation, they made their best guess to put datasets into predefined categories.

If no evidence could be found for the inference, they put ""not mentioned"" as a result.

All unclear decision were adjudicated by at least three annotators.","Inadequate dataset documentation was found throughout the survey process, limiting the coverage of the survey.

Furthermore, poorly written documentation or unavailable datasets were frequently encountered during annotation processes.

During annotation, unclear documentation often required annotators to make their best guess to categorize datasets.

If no evidence could be found for the inference, annotators would mark ""not mentioned"" as a result.

All unclear decisions were adjudicated by at least three annotators.","Questions:

What are the limitations related to dataset documentation?

Answer:

Inadequate dataset documentation was found throughout the survey process, limiting the coverage of the survey.

Furthermore, poorly written documentation or unavailable datasets were frequently encountered during annotation processes.

During annotation, unclear documentation often required annotators to make their best guess to categorize datasets.

If no evidence could be found for the inference, annotators would mark ""not mentioned"" as a result.

All unclear decisions were adjudicated by at least three annotators."
Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources,Limitations,"On Dataset Documentation. Throughout our survey process, we found inadequate dataset documentation, limiting the coverage of our survey. We suggest that individual researchers provide the input data source and the labeling methodology; if people were involved in dataset creation, their demographic information should be provided, as well. Such information can help researchers analyze potential bias embedded in the dataset (Bender and Friedman, 2018).

Surveyed Dataset Collection Process. Despite our best efforts, we do not claim to cover all relevant datasets. Our collection process overlooks datasets that are published at non-ACL venues and not in Hugging Face as well as papers that do not match our search keywords. For instance, we missed multilingual machine reading comprehension datasets (Gupta and Khade, 2020;Asai et al., 2018) and morphology datasets (McCarthy et al., 2020). We also found a very low presence of indigenous language datasets. None of 10 indigenous American languages from a recent study (Ebrahimi et al., 2022) was represented in our survey. That said, we host http: //multilingual-dataset-survey.github.io where researchers can submit their dataset information and periodically update our analysis. Furthermore, we constantly encountered poorly written documentation or unavailable datasets during our annotation processes. During annotation, whenever this paper's dataset annotators encountered unclear documentation, they made their best guess to put datasets into predefined categories. If no evidence could be found for the inference, they put ""not mentioned"" as a result. All unclear decision were adjudicated by at least three annotators.

Using Country Names as a Proxy for Languages Spoken. In Section 5, we attempted to approximate the number of NLP researchers with language proficiency in different languages. To do this, we mapped the names of ACL submission country to the most commonly spoken languages in those countries. We acknowledge that (1) the country of origin of researchers might be different from the country of submission, (2) researchers native language might not be listed in the commonly spoken languages and (3) the mapping might be incomprehensive.

MTurk Pilot Study. Due to our limited data points, although our MTurk study showed that data quality could be improved if the language qualification were applied in the collection process on MTurk, and our previous recommendations do not currently apply, we acknowledge that more research at scale should be done to statistically confirm the conclusion. Furthermore, languages other than the supported 5 languages might still be unsuitable for gathering multilingual data on MTurk As we identify that many datasets use texts beyond Wikipedia (see Table 4), we instead use the mC4 corpora (Xue et al., 2021), a much larger collection of texts in 101 languages drawn from the public Common Crawl web scrape and used for training the mT5 model. Specifically, we use the number of tokens in mC4 to estimate the amount of unlabeled data. Figure 7a shows a scatter plot where the x-axis represents the number of tokens in mC4 and y-axis represents the number of labeled datasets available in the languages. The availability of unlabeled text corpora and the number of labeled datasets show a high correlation (ρ = 0.794). We also analyze the relationship between the number of labeled datasets and the number of Wikipedia articles in Figure 7b. Again we observe a high correlation of ρ = 0.767. Classification (sentiment analysis) annotated (authors, linguists) collected from Wikipedia collected from curated source (exams, scientific papers, etc) collected from media (news) collected from social media or commercial sources collected from web crowdsourced curated linguistic resources multiple modes template-based Figure 6: The distribution over input source per task type. The size of bar represents the # of datasets of that task type and input source.

(a) The number of tokens (billions) in mC4 (Xue et al., 2021).

(b) The number of Wikipedia articles. Figure 7: Relationships between the number of datasets and number of tokens in mC4 and Wikipedia articles. English is removed.","1. (What are the limitations related to dataset documentation?): sent1
    1.1. (What suggestions are made to improve dataset documentation?): sent2
    1.2. (Why is it important to provide detailed dataset documentation?): sent3
2. (What are the limitations of the surveyed dataset collection process?): sent4
    2.1. (What specific datasets were missed in the collection process?): sent5, sent6
    2.2. (What is the issue with indigenous language datasets?): sent7, sent8
    2.3. (What solution is proposed to address the limitations of the dataset collection process?): sent9
3. (What issues were encountered during the annotation process?): sent10
    3.1. (How did annotators handle unclear documentation during annotation?): sent11
    3.2. (What was done if no evidence could be found for the inference?): sent12
    3.3. (How were unclear decisions adjudicated?): sent13
4. (What are the limitations of using country names as a proxy for languages spoken?): sent14
    4.1. (How was the number of NLP researchers with language proficiency approximated?): sent15, sent16
    4.2. (What are the acknowledged limitations of this approximation?): sent17
5. (What are the limitations of the MTurk pilot study?): sent18
    5.1. (What is the conclusion regarding the applicability of previous recommendations?): sent19
6. (What is the relationship between the number of tokens in mC4 and the number of labeled datasets?): sent20
    6.1. (What does Figure 7a show?): sent21
    6.2. (What is the observed correlation between the availability of unlabeled text corpora and the number of labeled datasets?): sent22
7. (What is the relationship between the number of labeled datasets and the number of Wikipedia articles?): sent23
    7.1. (What does Figure 7b show?): sent24
8. (What does Figure 6 illustrate?): sent25
    8.1. (What does the size of the bar in Figure 6 represent?): sent26
9. (What do Figures 7a and 7b illustrate?): sent27, sent28
    9.1. (What is the overall relationship shown in Figure 7?): sent29
10. (What is the specific note about English in the analysis?): sent30",What are the limitations of the surveyed dataset collection process?,"sent4, sent5, sent6, sent7, sent8","Despite our best efforts, we do not claim to cover all relevant datasets.

Our collection process overlooks datasets that are published at non-ACL venues and not in Hugging Face as well as papers that do not match our search keywords.

For instance, we missed multilingual machine reading comprehension datasets (Gupta and Khade, 2020;Asai et al., 2018) and morphology datasets (McCarthy et al., 2020).

We also found a very low presence of indigenous language datasets.

None of 10 indigenous American languages from a recent study (Ebrahimi et al., 2022) was represented in our survey.","Despite best efforts, the surveyed dataset collection process does not cover all relevant datasets.

The collection process overlooks datasets that are published at non-ACL venues and not in Hugging Face, as well as papers that do not match the search keywords.

For instance, multilingual machine reading comprehension datasets (Gupta and Khade, 2020; Asai et al., 2018) and morphology datasets (McCarthy et al., 2020) were missed.

Additionally, there is a very low presence of indigenous language datasets.

None of 10 indigenous American languages from a recent study (Ebrahimi et al., 2022) was represented in the survey.","Questions:

What are the limitations of the surveyed dataset collection process?

Answer:

Despite best efforts, the surveyed dataset collection process does not cover all relevant datasets.

The collection process overlooks datasets that are published at non-ACL venues and not in Hugging Face, as well as papers that do not match the search keywords.

For instance, multilingual machine reading comprehension datasets (Gupta and Khade, 2020; Asai et al., 2018) and morphology datasets (McCarthy et al., 2020) were missed.

Additionally, there is a very low presence of indigenous language datasets.

None of 10 indigenous American languages from a recent study (Ebrahimi et al., 2022) was represented in the survey."
Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources,Limitations,"On Dataset Documentation. Throughout our survey process, we found inadequate dataset documentation, limiting the coverage of our survey. We suggest that individual researchers provide the input data source and the labeling methodology; if people were involved in dataset creation, their demographic information should be provided, as well. Such information can help researchers analyze potential bias embedded in the dataset (Bender and Friedman, 2018).

Surveyed Dataset Collection Process. Despite our best efforts, we do not claim to cover all relevant datasets. Our collection process overlooks datasets that are published at non-ACL venues and not in Hugging Face as well as papers that do not match our search keywords. For instance, we missed multilingual machine reading comprehension datasets (Gupta and Khade, 2020;Asai et al., 2018) and morphology datasets (McCarthy et al., 2020). We also found a very low presence of indigenous language datasets. None of 10 indigenous American languages from a recent study (Ebrahimi et al., 2022) was represented in our survey. That said, we host http: //multilingual-dataset-survey.github.io where researchers can submit their dataset information and periodically update our analysis. Furthermore, we constantly encountered poorly written documentation or unavailable datasets during our annotation processes. During annotation, whenever this paper's dataset annotators encountered unclear documentation, they made their best guess to put datasets into predefined categories. If no evidence could be found for the inference, they put ""not mentioned"" as a result. All unclear decision were adjudicated by at least three annotators.

Using Country Names as a Proxy for Languages Spoken. In Section 5, we attempted to approximate the number of NLP researchers with language proficiency in different languages. To do this, we mapped the names of ACL submission country to the most commonly spoken languages in those countries. We acknowledge that (1) the country of origin of researchers might be different from the country of submission, (2) researchers native language might not be listed in the commonly spoken languages and (3) the mapping might be incomprehensive.

MTurk Pilot Study. Due to our limited data points, although our MTurk study showed that data quality could be improved if the language qualification were applied in the collection process on MTurk, and our previous recommendations do not currently apply, we acknowledge that more research at scale should be done to statistically confirm the conclusion. Furthermore, languages other than the supported 5 languages might still be unsuitable for gathering multilingual data on MTurk As we identify that many datasets use texts beyond Wikipedia (see Table 4), we instead use the mC4 corpora (Xue et al., 2021), a much larger collection of texts in 101 languages drawn from the public Common Crawl web scrape and used for training the mT5 model. Specifically, we use the number of tokens in mC4 to estimate the amount of unlabeled data. Figure 7a shows a scatter plot where the x-axis represents the number of tokens in mC4 and y-axis represents the number of labeled datasets available in the languages. The availability of unlabeled text corpora and the number of labeled datasets show a high correlation (ρ = 0.794). We also analyze the relationship between the number of labeled datasets and the number of Wikipedia articles in Figure 7b. Again we observe a high correlation of ρ = 0.767. Classification (sentiment analysis) annotated (authors, linguists) collected from Wikipedia collected from curated source (exams, scientific papers, etc) collected from media (news) collected from social media or commercial sources collected from web crowdsourced curated linguistic resources multiple modes template-based Figure 6: The distribution over input source per task type. The size of bar represents the # of datasets of that task type and input source.

(a) The number of tokens (billions) in mC4 (Xue et al., 2021).

(b) The number of Wikipedia articles. Figure 7: Relationships between the number of datasets and number of tokens in mC4 and Wikipedia articles. English is removed.","1. (What are the limitations related to dataset documentation?): sent1
    1.1. (What suggestions are made to improve dataset documentation?): sent2
    1.2. (Why is it important to provide detailed dataset documentation?): sent3
2. (What are the limitations of the surveyed dataset collection process?): sent4
    2.1. (What specific datasets were missed in the collection process?): sent5, sent6
    2.2. (What is the issue with indigenous language datasets?): sent7, sent8
    2.3. (What solution is proposed to address the limitations of the dataset collection process?): sent9
3. (What issues were encountered during the annotation process?): sent10
    3.1. (How did annotators handle unclear documentation during annotation?): sent11
    3.2. (What was done if no evidence could be found for the inference?): sent12
    3.3. (How were unclear decisions adjudicated?): sent13
4. (What are the limitations of using country names as a proxy for languages spoken?): sent14
    4.1. (How was the number of NLP researchers with language proficiency approximated?): sent15, sent16
    4.2. (What are the acknowledged limitations of this approximation?): sent17
5. (What are the limitations of the MTurk pilot study?): sent18
    5.1. (What is the conclusion regarding the applicability of previous recommendations?): sent19
6. (What is the relationship between the number of tokens in mC4 and the number of labeled datasets?): sent20
    6.1. (What does Figure 7a show?): sent21
    6.2. (What is the observed correlation between the availability of unlabeled text corpora and the number of labeled datasets?): sent22
7. (What is the relationship between the number of labeled datasets and the number of Wikipedia articles?): sent23
    7.1. (What does Figure 7b show?): sent24
8. (What does Figure 6 illustrate?): sent25
    8.1. (What does the size of the bar in Figure 6 represent?): sent26
9. (What do Figures 7a and 7b illustrate?): sent27, sent28
    9.1. (What is the overall relationship shown in Figure 7?): sent29
10. (What is the specific note about English in the analysis?): sent30",What issues were encountered during the annotation process?,"sent10, sent11, sent12, sent13","Furthermore, we constantly encountered poorly written documentation or unavailable datasets during our annotation processes.

During annotation, whenever this paper's dataset annotators encountered unclear documentation, they made their best guess to put datasets into predefined categories.

If no evidence could be found for the inference, they put ""not mentioned"" as a result.

All unclear decision were adjudicated by at least three annotators.","Poorly written documentation or unavailable datasets were constantly encountered during the annotation process.

Whenever annotators encountered unclear documentation, they made their best guess to categorize the datasets into predefined categories.

If no evidence could be found for the inference, they put ""not mentioned"" as a result.

All unclear decisions were adjudicated by at least three annotators.","Questions:

What issues were encountered during the annotation process?

Answer:

Poorly written documentation or unavailable datasets were constantly encountered during the annotation process.

Whenever annotators encountered unclear documentation, they made their best guess to categorize the datasets into predefined categories.

If no evidence could be found for the inference, they put ""not mentioned"" as a result.

All unclear decisions were adjudicated by at least three annotators."
Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources,Limitations,"On Dataset Documentation. Throughout our survey process, we found inadequate dataset documentation, limiting the coverage of our survey. We suggest that individual researchers provide the input data source and the labeling methodology; if people were involved in dataset creation, their demographic information should be provided, as well. Such information can help researchers analyze potential bias embedded in the dataset (Bender and Friedman, 2018).

Surveyed Dataset Collection Process. Despite our best efforts, we do not claim to cover all relevant datasets. Our collection process overlooks datasets that are published at non-ACL venues and not in Hugging Face as well as papers that do not match our search keywords. For instance, we missed multilingual machine reading comprehension datasets (Gupta and Khade, 2020;Asai et al., 2018) and morphology datasets (McCarthy et al., 2020). We also found a very low presence of indigenous language datasets. None of 10 indigenous American languages from a recent study (Ebrahimi et al., 2022) was represented in our survey. That said, we host http: //multilingual-dataset-survey.github.io where researchers can submit their dataset information and periodically update our analysis. Furthermore, we constantly encountered poorly written documentation or unavailable datasets during our annotation processes. During annotation, whenever this paper's dataset annotators encountered unclear documentation, they made their best guess to put datasets into predefined categories. If no evidence could be found for the inference, they put ""not mentioned"" as a result. All unclear decision were adjudicated by at least three annotators.

Using Country Names as a Proxy for Languages Spoken. In Section 5, we attempted to approximate the number of NLP researchers with language proficiency in different languages. To do this, we mapped the names of ACL submission country to the most commonly spoken languages in those countries. We acknowledge that (1) the country of origin of researchers might be different from the country of submission, (2) researchers native language might not be listed in the commonly spoken languages and (3) the mapping might be incomprehensive.

MTurk Pilot Study. Due to our limited data points, although our MTurk study showed that data quality could be improved if the language qualification were applied in the collection process on MTurk, and our previous recommendations do not currently apply, we acknowledge that more research at scale should be done to statistically confirm the conclusion. Furthermore, languages other than the supported 5 languages might still be unsuitable for gathering multilingual data on MTurk As we identify that many datasets use texts beyond Wikipedia (see Table 4), we instead use the mC4 corpora (Xue et al., 2021), a much larger collection of texts in 101 languages drawn from the public Common Crawl web scrape and used for training the mT5 model. Specifically, we use the number of tokens in mC4 to estimate the amount of unlabeled data. Figure 7a shows a scatter plot where the x-axis represents the number of tokens in mC4 and y-axis represents the number of labeled datasets available in the languages. The availability of unlabeled text corpora and the number of labeled datasets show a high correlation (ρ = 0.794). We also analyze the relationship between the number of labeled datasets and the number of Wikipedia articles in Figure 7b. Again we observe a high correlation of ρ = 0.767. Classification (sentiment analysis) annotated (authors, linguists) collected from Wikipedia collected from curated source (exams, scientific papers, etc) collected from media (news) collected from social media or commercial sources collected from web crowdsourced curated linguistic resources multiple modes template-based Figure 6: The distribution over input source per task type. The size of bar represents the # of datasets of that task type and input source.

(a) The number of tokens (billions) in mC4 (Xue et al., 2021).

(b) The number of Wikipedia articles. Figure 7: Relationships between the number of datasets and number of tokens in mC4 and Wikipedia articles. English is removed.","1. (What are the limitations related to dataset documentation?): sent1
    1.1. (What suggestions are made to improve dataset documentation?): sent2
    1.2. (Why is it important to provide detailed dataset documentation?): sent3
2. (What are the limitations of the surveyed dataset collection process?): sent4
    2.1. (What specific datasets were missed in the collection process?): sent5, sent6
    2.2. (What is the issue with indigenous language datasets?): sent7, sent8
    2.3. (What solution is proposed to address the limitations of the dataset collection process?): sent9
3. (What issues were encountered during the annotation process?): sent10
    3.1. (How did annotators handle unclear documentation during annotation?): sent11
    3.2. (What was done if no evidence could be found for the inference?): sent12
    3.3. (How were unclear decisions adjudicated?): sent13
4. (What are the limitations of using country names as a proxy for languages spoken?): sent14
    4.1. (How was the number of NLP researchers with language proficiency approximated?): sent15, sent16
    4.2. (What are the acknowledged limitations of this approximation?): sent17
5. (What are the limitations of the MTurk pilot study?): sent18
    5.1. (What is the conclusion regarding the applicability of previous recommendations?): sent19
6. (What is the relationship between the number of tokens in mC4 and the number of labeled datasets?): sent20
    6.1. (What does Figure 7a show?): sent21
    6.2. (What is the observed correlation between the availability of unlabeled text corpora and the number of labeled datasets?): sent22
7. (What is the relationship between the number of labeled datasets and the number of Wikipedia articles?): sent23
    7.1. (What does Figure 7b show?): sent24
8. (What does Figure 6 illustrate?): sent25
    8.1. (What does the size of the bar in Figure 6 represent?): sent26
9. (What do Figures 7a and 7b illustrate?): sent27, sent28
    9.1. (What is the overall relationship shown in Figure 7?): sent29
10. (What is the specific note about English in the analysis?): sent30",How did annotators handle unclear documentation during annotation?,"sent11, sent12, sent13","During annotation, whenever this paper's dataset annotators encountered unclear documentation, they made their best guess to put datasets into predefined categories.

If no evidence could be found for the inference, they put ""not mentioned"" as a result.

All unclear decision were adjudicated by at least three annotators.","During annotation, whenever annotators encountered unclear documentation, they made their best guess to put datasets into predefined categories.

If no evidence could be found for the inference, they put ""not mentioned"" as a result.

All unclear decisions were adjudicated by at least three annotators.","Questions:

How did annotators handle unclear documentation during annotation?

Answer:

During annotation, whenever annotators encountered unclear documentation, they made their best guess to put datasets into predefined categories.

If no evidence could be found for the inference, they put ""not mentioned"" as a result.

All unclear decisions were adjudicated by at least three annotators."
Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources,Limitations,"On Dataset Documentation. Throughout our survey process, we found inadequate dataset documentation, limiting the coverage of our survey. We suggest that individual researchers provide the input data source and the labeling methodology; if people were involved in dataset creation, their demographic information should be provided, as well. Such information can help researchers analyze potential bias embedded in the dataset (Bender and Friedman, 2018).

Surveyed Dataset Collection Process. Despite our best efforts, we do not claim to cover all relevant datasets. Our collection process overlooks datasets that are published at non-ACL venues and not in Hugging Face as well as papers that do not match our search keywords. For instance, we missed multilingual machine reading comprehension datasets (Gupta and Khade, 2020;Asai et al., 2018) and morphology datasets (McCarthy et al., 2020). We also found a very low presence of indigenous language datasets. None of 10 indigenous American languages from a recent study (Ebrahimi et al., 2022) was represented in our survey. That said, we host http: //multilingual-dataset-survey.github.io where researchers can submit their dataset information and periodically update our analysis. Furthermore, we constantly encountered poorly written documentation or unavailable datasets during our annotation processes. During annotation, whenever this paper's dataset annotators encountered unclear documentation, they made their best guess to put datasets into predefined categories. If no evidence could be found for the inference, they put ""not mentioned"" as a result. All unclear decision were adjudicated by at least three annotators.

Using Country Names as a Proxy for Languages Spoken. In Section 5, we attempted to approximate the number of NLP researchers with language proficiency in different languages. To do this, we mapped the names of ACL submission country to the most commonly spoken languages in those countries. We acknowledge that (1) the country of origin of researchers might be different from the country of submission, (2) researchers native language might not be listed in the commonly spoken languages and (3) the mapping might be incomprehensive.

MTurk Pilot Study. Due to our limited data points, although our MTurk study showed that data quality could be improved if the language qualification were applied in the collection process on MTurk, and our previous recommendations do not currently apply, we acknowledge that more research at scale should be done to statistically confirm the conclusion. Furthermore, languages other than the supported 5 languages might still be unsuitable for gathering multilingual data on MTurk As we identify that many datasets use texts beyond Wikipedia (see Table 4), we instead use the mC4 corpora (Xue et al., 2021), a much larger collection of texts in 101 languages drawn from the public Common Crawl web scrape and used for training the mT5 model. Specifically, we use the number of tokens in mC4 to estimate the amount of unlabeled data. Figure 7a shows a scatter plot where the x-axis represents the number of tokens in mC4 and y-axis represents the number of labeled datasets available in the languages. The availability of unlabeled text corpora and the number of labeled datasets show a high correlation (ρ = 0.794). We also analyze the relationship between the number of labeled datasets and the number of Wikipedia articles in Figure 7b. Again we observe a high correlation of ρ = 0.767. Classification (sentiment analysis) annotated (authors, linguists) collected from Wikipedia collected from curated source (exams, scientific papers, etc) collected from media (news) collected from social media or commercial sources collected from web crowdsourced curated linguistic resources multiple modes template-based Figure 6: The distribution over input source per task type. The size of bar represents the # of datasets of that task type and input source.

(a) The number of tokens (billions) in mC4 (Xue et al., 2021).

(b) The number of Wikipedia articles. Figure 7: Relationships between the number of datasets and number of tokens in mC4 and Wikipedia articles. English is removed.","1. (What are the limitations related to dataset documentation?): sent1
    1.1. (What suggestions are made to improve dataset documentation?): sent2
    1.2. (Why is it important to provide detailed dataset documentation?): sent3
2. (What are the limitations of the surveyed dataset collection process?): sent4
    2.1. (What specific datasets were missed in the collection process?): sent5, sent6
    2.2. (What is the issue with indigenous language datasets?): sent7, sent8
    2.3. (What solution is proposed to address the limitations of the dataset collection process?): sent9
3. (What issues were encountered during the annotation process?): sent10
    3.1. (How did annotators handle unclear documentation during annotation?): sent11
    3.2. (What was done if no evidence could be found for the inference?): sent12
    3.3. (How were unclear decisions adjudicated?): sent13
4. (What are the limitations of using country names as a proxy for languages spoken?): sent14
    4.1. (How was the number of NLP researchers with language proficiency approximated?): sent15, sent16
    4.2. (What are the acknowledged limitations of this approximation?): sent17
5. (What are the limitations of the MTurk pilot study?): sent18
    5.1. (What is the conclusion regarding the applicability of previous recommendations?): sent19
6. (What is the relationship between the number of tokens in mC4 and the number of labeled datasets?): sent20
    6.1. (What does Figure 7a show?): sent21
    6.2. (What is the observed correlation between the availability of unlabeled text corpora and the number of labeled datasets?): sent22
7. (What is the relationship between the number of labeled datasets and the number of Wikipedia articles?): sent23
    7.1. (What does Figure 7b show?): sent24
8. (What does Figure 6 illustrate?): sent25
    8.1. (What does the size of the bar in Figure 6 represent?): sent26
9. (What do Figures 7a and 7b illustrate?): sent27, sent28
    9.1. (What is the overall relationship shown in Figure 7?): sent29
10. (What is the specific note about English in the analysis?): sent30",What is the relationship between the number of tokens in mC4 and the number of labeled datasets?,"sent20, sent21, sent22","Specifically, we use the number of tokens in mC4 to estimate the amount of unlabeled data.

Figure 7a shows a scatter plot where the x-axis represents the number of tokens in mC4 and y-axis represents the number of labeled datasets available in the languages.

The availability of unlabeled text corpora and the number of labeled datasets show a high correlation (ρ = 0.794).","The number of tokens in mC4 is used to estimate the amount of unlabeled data.

A scatter plot illustrates the relationship between the number of tokens in mC4 and the number of labeled datasets available in the languages.

The availability of unlabeled text corpora and the number of labeled datasets show a high correlation (ρ = 0.794).","Questions:

What is the relationship between the number of tokens in mC4 and the number of labeled datasets?

Answer:

The number of tokens in mC4 is used to estimate the amount of unlabeled data.

A scatter plot illustrates the relationship between the number of tokens in mC4 and the number of labeled datasets available in the languages.

The availability of unlabeled text corpora and the number of labeled datasets show a high correlation (ρ = 0.794)."
Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources,Limitations,"On Dataset Documentation. Throughout our survey process, we found inadequate dataset documentation, limiting the coverage of our survey. We suggest that individual researchers provide the input data source and the labeling methodology; if people were involved in dataset creation, their demographic information should be provided, as well. Such information can help researchers analyze potential bias embedded in the dataset (Bender and Friedman, 2018).

Surveyed Dataset Collection Process. Despite our best efforts, we do not claim to cover all relevant datasets. Our collection process overlooks datasets that are published at non-ACL venues and not in Hugging Face as well as papers that do not match our search keywords. For instance, we missed multilingual machine reading comprehension datasets (Gupta and Khade, 2020;Asai et al., 2018) and morphology datasets (McCarthy et al., 2020). We also found a very low presence of indigenous language datasets. None of 10 indigenous American languages from a recent study (Ebrahimi et al., 2022) was represented in our survey. That said, we host http: //multilingual-dataset-survey.github.io where researchers can submit their dataset information and periodically update our analysis. Furthermore, we constantly encountered poorly written documentation or unavailable datasets during our annotation processes. During annotation, whenever this paper's dataset annotators encountered unclear documentation, they made their best guess to put datasets into predefined categories. If no evidence could be found for the inference, they put ""not mentioned"" as a result. All unclear decision were adjudicated by at least three annotators.

Using Country Names as a Proxy for Languages Spoken. In Section 5, we attempted to approximate the number of NLP researchers with language proficiency in different languages. To do this, we mapped the names of ACL submission country to the most commonly spoken languages in those countries. We acknowledge that (1) the country of origin of researchers might be different from the country of submission, (2) researchers native language might not be listed in the commonly spoken languages and (3) the mapping might be incomprehensive.

MTurk Pilot Study. Due to our limited data points, although our MTurk study showed that data quality could be improved if the language qualification were applied in the collection process on MTurk, and our previous recommendations do not currently apply, we acknowledge that more research at scale should be done to statistically confirm the conclusion. Furthermore, languages other than the supported 5 languages might still be unsuitable for gathering multilingual data on MTurk As we identify that many datasets use texts beyond Wikipedia (see Table 4), we instead use the mC4 corpora (Xue et al., 2021), a much larger collection of texts in 101 languages drawn from the public Common Crawl web scrape and used for training the mT5 model. Specifically, we use the number of tokens in mC4 to estimate the amount of unlabeled data. Figure 7a shows a scatter plot where the x-axis represents the number of tokens in mC4 and y-axis represents the number of labeled datasets available in the languages. The availability of unlabeled text corpora and the number of labeled datasets show a high correlation (ρ = 0.794). We also analyze the relationship between the number of labeled datasets and the number of Wikipedia articles in Figure 7b. Again we observe a high correlation of ρ = 0.767. Classification (sentiment analysis) annotated (authors, linguists) collected from Wikipedia collected from curated source (exams, scientific papers, etc) collected from media (news) collected from social media or commercial sources collected from web crowdsourced curated linguistic resources multiple modes template-based Figure 6: The distribution over input source per task type. The size of bar represents the # of datasets of that task type and input source.

(a) The number of tokens (billions) in mC4 (Xue et al., 2021).

(b) The number of Wikipedia articles. Figure 7: Relationships between the number of datasets and number of tokens in mC4 and Wikipedia articles. English is removed.","1. (What are the limitations related to dataset documentation?): sent1
    1.1. (What suggestions are made to improve dataset documentation?): sent2
    1.2. (Why is it important to provide detailed dataset documentation?): sent3
2. (What are the limitations of the surveyed dataset collection process?): sent4
    2.1. (What specific datasets were missed in the collection process?): sent5, sent6
    2.2. (What is the issue with indigenous language datasets?): sent7, sent8
    2.3. (What solution is proposed to address the limitations of the dataset collection process?): sent9
3. (What issues were encountered during the annotation process?): sent10
    3.1. (How did annotators handle unclear documentation during annotation?): sent11
    3.2. (What was done if no evidence could be found for the inference?): sent12
    3.3. (How were unclear decisions adjudicated?): sent13
4. (What are the limitations of using country names as a proxy for languages spoken?): sent14
    4.1. (How was the number of NLP researchers with language proficiency approximated?): sent15, sent16
    4.2. (What are the acknowledged limitations of this approximation?): sent17
5. (What are the limitations of the MTurk pilot study?): sent18
    5.1. (What is the conclusion regarding the applicability of previous recommendations?): sent19
6. (What is the relationship between the number of tokens in mC4 and the number of labeled datasets?): sent20
    6.1. (What does Figure 7a show?): sent21
    6.2. (What is the observed correlation between the availability of unlabeled text corpora and the number of labeled datasets?): sent22
7. (What is the relationship between the number of labeled datasets and the number of Wikipedia articles?): sent23
    7.1. (What does Figure 7b show?): sent24
8. (What does Figure 6 illustrate?): sent25
    8.1. (What does the size of the bar in Figure 6 represent?): sent26
9. (What do Figures 7a and 7b illustrate?): sent27, sent28
    9.1. (What is the overall relationship shown in Figure 7?): sent29
10. (What is the specific note about English in the analysis?): sent30",What do Figures 7a and 7b illustrate?,"sent27, sent28, sent29","(a) The number of tokens (billions) in mC4 (Xue et al., 2021).

(b) The number of Wikipedia articles.

Figure 7: Relationships between the number of datasets and number of tokens in mC4 and Wikipedia articles.","The number of tokens (billions) in mC4 (Xue et al., 2021).

The number of Wikipedia articles.

The relationships between the number of datasets and the number of tokens in mC4 and Wikipedia articles.","Questions:

What do Figures 7a and 7b illustrate?

Answer:

The number of tokens (billions) in mC4 (Xue et al., 2021).

The number of Wikipedia articles.

The relationships between the number of datasets and the number of tokens in mC4 and Wikipedia articles."
Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources,Suggestions for the NLP Community,"To Foster Language-proficient Researchers and Community Efforts. Our analysis shows that the availability of NLP researchers who are fluent in languages highly correlates with the availability of datasets. Moreover, monolingual test suites cover only 16 languages, such as Chinese (Xu et al., 2020), Indic Languages (Kakwani et al., 2020), Polish (Rybak et al., 2020), Persian (Khashabi et al., 2021), Russian (Shavrina et al., 2020) or Arabic (Seelawi et al., 2021), where efforts are driven by language-proficient NLP researchers. Organizing these large-scale, inter-organization efforts can be challenging but have profound effects. Recent community efforts such as Masakhane * spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021). Developing a directory of language-proficient NLP researchers interested in global collaboration could foster more cooperation. In the long run, globalized NLP education like AFIRM * will be necessary. A directory of potential funding sources to support multlingual data collection can also be helpful.

On Inclusive Venues. The academic publication/conference reviewing system should also reward efforts to develop language-specific resources, without perceiving this as a niche, low-impact effort (Rogers et al., 2022). As a community, we should encourage efforts to create and provide region-specific (e.g., Nordic Conference on Computational Linguistics, Pacific Asia Conference on Language, Information and Computation), language-oriented (e.g., Deep Learning for Low-Resource NLP, AfricaNLP, Workshop on Indian Language Data: Resources and Evaluation), and data-oriented (e.g., NeurIPS dataset and benchmark track) venues for introducing multilingual datasets. Adapting existing systems to new and low-resource languages poses a challenging and intriguing task as well as substantial research inquiries. The community should continue supporting such efforts and expand evaluation data for diverse target languages.","# QUA Analysis:

1. (What is the main suggestion for fostering language-proficient researchers and community efforts?): sent1
    1.1. (What does the analysis show about the availability of NLP researchers and datasets?): sent2
    1.2. (What is the coverage of monolingual test suites and who drives these efforts?): sent3
    1.3. (What are the challenges and effects of organizing large-scale, inter-organization efforts?): sent4
    1.4. (What are some examples of recent community efforts and their outcomes?): sent5
    1.5. (What could foster more cooperation among language-proficient NLP researchers?): sent6
    1.6. (What is necessary for the long run in globalized NLP education?): sent7
    1.7. (What additional resource could be helpful for multilingual data collection?): sent8

2. (What is the main suggestion for inclusive venues?): sent9
    2.1. (What should the academic publication/conference reviewing system do?): sent9
    2.2. (What efforts should the community encourage regarding venues?): sent10
    2.3. (What challenges and research inquiries are posed by adapting existing systems to new and low-resource languages?): sent11
    2.4. (What should the community continue to support and expand?): sent12",How does the availability of language-proficient NLP researchers correlate with dataset availability?,"sent2, sent3, sent5","Our analysis shows that the availability of NLP researchers who are fluent in languages highly correlates with the availability of datasets.

Moreover, monolingual test suites cover only 16 languages, such as Chinese (Xu et al., 2020), Indic Languages (Kakwani et al., 2020), Polish (Rybak et al., 2020), Persian (Khashabi et al., 2021), Russian (Shavrina et al., 2020) or Arabic (Seelawi et al., 2021), where efforts are driven by language-proficient NLP researchers.

Recent community efforts such as Masakhane * spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021).","The availability of NLP researchers who are fluent in languages highly correlates with the availability of datasets.

Monolingual test suites cover only 16 languages, such as Chinese (Xu et al., 2020), Indic Languages (Kakwani et al., 2020), Polish (Rybak et al., 2020), Persian (Khashabi et al., 2021), Russian (Shavrina et al., 2020), or Arabic (Seelawi et al., 2021), where efforts are driven by language-proficient NLP researchers.

Recent community efforts, such as Masakhane, spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021).","Questions:

How does the availability of language-proficient NLP researchers correlate with dataset availability?

Answer:

The availability of NLP researchers who are fluent in languages highly correlates with the availability of datasets.

Monolingual test suites cover only 16 languages, such as Chinese (Xu et al., 2020), Indic Languages (Kakwani et al., 2020), Polish (Rybak et al., 2020), Persian (Khashabi et al., 2021), Russian (Shavrina et al., 2020), or Arabic (Seelawi et al., 2021), where efforts are driven by language-proficient NLP researchers.

Recent community efforts, such as Masakhane, spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021)."
Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources,Suggestions for the NLP Community,"To Foster Language-proficient Researchers and Community Efforts. Our analysis shows that the availability of NLP researchers who are fluent in languages highly correlates with the availability of datasets. Moreover, monolingual test suites cover only 16 languages, such as Chinese (Xu et al., 2020), Indic Languages (Kakwani et al., 2020), Polish (Rybak et al., 2020), Persian (Khashabi et al., 2021), Russian (Shavrina et al., 2020) or Arabic (Seelawi et al., 2021), where efforts are driven by language-proficient NLP researchers. Organizing these large-scale, inter-organization efforts can be challenging but have profound effects. Recent community efforts such as Masakhane * spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021). Developing a directory of language-proficient NLP researchers interested in global collaboration could foster more cooperation. In the long run, globalized NLP education like AFIRM * will be necessary. A directory of potential funding sources to support multlingual data collection can also be helpful.

On Inclusive Venues. The academic publication/conference reviewing system should also reward efforts to develop language-specific resources, without perceiving this as a niche, low-impact effort (Rogers et al., 2022). As a community, we should encourage efforts to create and provide region-specific (e.g., Nordic Conference on Computational Linguistics, Pacific Asia Conference on Language, Information and Computation), language-oriented (e.g., Deep Learning for Low-Resource NLP, AfricaNLP, Workshop on Indian Language Data: Resources and Evaluation), and data-oriented (e.g., NeurIPS dataset and benchmark track) venues for introducing multilingual datasets. Adapting existing systems to new and low-resource languages poses a challenging and intriguing task as well as substantial research inquiries. The community should continue supporting such efforts and expand evaluation data for diverse target languages.","# QUA Analysis:

1. (What is the main suggestion for fostering language-proficient researchers and community efforts?): sent1
    1.1. (What does the analysis show about the availability of NLP researchers and datasets?): sent2
    1.2. (What is the coverage of monolingual test suites and who drives these efforts?): sent3
    1.3. (What are the challenges and effects of organizing large-scale, inter-organization efforts?): sent4
    1.4. (What are some examples of recent community efforts and their outcomes?): sent5
    1.5. (What could foster more cooperation among language-proficient NLP researchers?): sent6
    1.6. (What is necessary for the long run in globalized NLP education?): sent7
    1.7. (What additional resource could be helpful for multilingual data collection?): sent8

2. (What is the main suggestion for inclusive venues?): sent9
    2.1. (What should the academic publication/conference reviewing system do?): sent9
    2.2. (What efforts should the community encourage regarding venues?): sent10
    2.3. (What challenges and research inquiries are posed by adapting existing systems to new and low-resource languages?): sent11
    2.4. (What should the community continue to support and expand?): sent12","What are the challenges and effects of organizing large-scale, inter-organization efforts in NLP?","sent4, sent5, sent6","Organizing these large-scale, inter-organization efforts can be challenging but have profound effects.

Recent community efforts such as Masakhane * spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021).

Developing a directory of language-proficient NLP researchers interested in global collaboration could foster more cooperation.","Organizing large-scale, inter-organization efforts in NLP can be challenging but have profound effects.

Recent community efforts, such as Masakhane, spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021).

Developing a directory of language-proficient NLP researchers interested in global collaboration could foster more cooperation.","Questions:

What are the challenges and effects of organizing large-scale, inter-organization efforts in NLP?

Answer:

Organizing large-scale, inter-organization efforts in NLP can be challenging but have profound effects.

Recent community efforts, such as Masakhane, spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021).

Developing a directory of language-proficient NLP researchers interested in global collaboration could foster more cooperation."
Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources,Suggestions for the NLP Community,"To Foster Language-proficient Researchers and Community Efforts. Our analysis shows that the availability of NLP researchers who are fluent in languages highly correlates with the availability of datasets. Moreover, monolingual test suites cover only 16 languages, such as Chinese (Xu et al., 2020), Indic Languages (Kakwani et al., 2020), Polish (Rybak et al., 2020), Persian (Khashabi et al., 2021), Russian (Shavrina et al., 2020) or Arabic (Seelawi et al., 2021), where efforts are driven by language-proficient NLP researchers. Organizing these large-scale, inter-organization efforts can be challenging but have profound effects. Recent community efforts such as Masakhane * spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021). Developing a directory of language-proficient NLP researchers interested in global collaboration could foster more cooperation. In the long run, globalized NLP education like AFIRM * will be necessary. A directory of potential funding sources to support multlingual data collection can also be helpful.

On Inclusive Venues. The academic publication/conference reviewing system should also reward efforts to develop language-specific resources, without perceiving this as a niche, low-impact effort (Rogers et al., 2022). As a community, we should encourage efforts to create and provide region-specific (e.g., Nordic Conference on Computational Linguistics, Pacific Asia Conference on Language, Information and Computation), language-oriented (e.g., Deep Learning for Low-Resource NLP, AfricaNLP, Workshop on Indian Language Data: Resources and Evaluation), and data-oriented (e.g., NeurIPS dataset and benchmark track) venues for introducing multilingual datasets. Adapting existing systems to new and low-resource languages poses a challenging and intriguing task as well as substantial research inquiries. The community should continue supporting such efforts and expand evaluation data for diverse target languages.","# QUA Analysis:

1. (What is the main suggestion for fostering language-proficient researchers and community efforts?): sent1
    1.1. (What does the analysis show about the availability of NLP researchers and datasets?): sent2
    1.2. (What is the coverage of monolingual test suites and who drives these efforts?): sent3
    1.3. (What are the challenges and effects of organizing large-scale, inter-organization efforts?): sent4
    1.4. (What are some examples of recent community efforts and their outcomes?): sent5
    1.5. (What could foster more cooperation among language-proficient NLP researchers?): sent6
    1.6. (What is necessary for the long run in globalized NLP education?): sent7
    1.7. (What additional resource could be helpful for multilingual data collection?): sent8

2. (What is the main suggestion for inclusive venues?): sent9
    2.1. (What should the academic publication/conference reviewing system do?): sent9
    2.2. (What efforts should the community encourage regarding venues?): sent10
    2.3. (What challenges and research inquiries are posed by adapting existing systems to new and low-resource languages?): sent11
    2.4. (What should the community continue to support and expand?): sent12",What are some examples of recent community efforts for under-resourced languages and their outcomes?,"sent5, sent6, sent8","Recent community efforts such as Masakhane * spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021).

Developing a directory of language-proficient NLP researchers interested in global collaboration could foster more cooperation.

A directory of potential funding sources to support multlingual data collection can also be helpful.","Recent community efforts such as Masakhane spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021).

Developing a directory of language-proficient NLP researchers interested in global collaboration could foster more cooperation.

A directory of potential funding sources to support multilingual data collection can also be helpful.","Questions:

What are some examples of recent community efforts for under-resourced languages and their outcomes?

Answer:

Recent community efforts such as Masakhane spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021).

Developing a directory of language-proficient NLP researchers interested in global collaboration could foster more cooperation.

A directory of potential funding sources to support multilingual data collection can also be helpful."
Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources,Suggestions for the NLP Community,"To Foster Language-proficient Researchers and Community Efforts. Our analysis shows that the availability of NLP researchers who are fluent in languages highly correlates with the availability of datasets. Moreover, monolingual test suites cover only 16 languages, such as Chinese (Xu et al., 2020), Indic Languages (Kakwani et al., 2020), Polish (Rybak et al., 2020), Persian (Khashabi et al., 2021), Russian (Shavrina et al., 2020) or Arabic (Seelawi et al., 2021), where efforts are driven by language-proficient NLP researchers. Organizing these large-scale, inter-organization efforts can be challenging but have profound effects. Recent community efforts such as Masakhane * spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021). Developing a directory of language-proficient NLP researchers interested in global collaboration could foster more cooperation. In the long run, globalized NLP education like AFIRM * will be necessary. A directory of potential funding sources to support multlingual data collection can also be helpful.

On Inclusive Venues. The academic publication/conference reviewing system should also reward efforts to develop language-specific resources, without perceiving this as a niche, low-impact effort (Rogers et al., 2022). As a community, we should encourage efforts to create and provide region-specific (e.g., Nordic Conference on Computational Linguistics, Pacific Asia Conference on Language, Information and Computation), language-oriented (e.g., Deep Learning for Low-Resource NLP, AfricaNLP, Workshop on Indian Language Data: Resources and Evaluation), and data-oriented (e.g., NeurIPS dataset and benchmark track) venues for introducing multilingual datasets. Adapting existing systems to new and low-resource languages poses a challenging and intriguing task as well as substantial research inquiries. The community should continue supporting such efforts and expand evaluation data for diverse target languages.","# QUA Analysis:

1. (What is the main suggestion for fostering language-proficient researchers and community efforts?): sent1
    1.1. (What does the analysis show about the availability of NLP researchers and datasets?): sent2
    1.2. (What is the coverage of monolingual test suites and who drives these efforts?): sent3
    1.3. (What are the challenges and effects of organizing large-scale, inter-organization efforts?): sent4
    1.4. (What are some examples of recent community efforts and their outcomes?): sent5
    1.5. (What could foster more cooperation among language-proficient NLP researchers?): sent6
    1.6. (What is necessary for the long run in globalized NLP education?): sent7
    1.7. (What additional resource could be helpful for multilingual data collection?): sent8

2. (What is the main suggestion for inclusive venues?): sent9
    2.1. (What should the academic publication/conference reviewing system do?): sent9
    2.2. (What efforts should the community encourage regarding venues?): sent10
    2.3. (What challenges and research inquiries are posed by adapting existing systems to new and low-resource languages?): sent11
    2.4. (What should the community continue to support and expand?): sent12",What is necessary for fostering globalized NLP education in the long run?,"sent7, sent8, sent12","In the long run, globalized NLP education like AFIRM * will be necessary.

A directory of potential funding sources to support multlingual data collection can also be helpful.

The community should continue supporting such efforts and expand evaluation data for diverse target languages.","In the long run, globalized NLP education will be necessary.

A directory of potential funding sources to support multilingual data collection can also be helpful.

The community should continue supporting such efforts and expand evaluation data for diverse target languages.","Questions:

What is necessary for fostering globalized NLP education in the long run?

Answer:

In the long run, globalized NLP education will be necessary.

A directory of potential funding sources to support multilingual data collection can also be helpful.

The community should continue supporting such efforts and expand evaluation data for diverse target languages."
Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources,Suggestions for the NLP Community,"To Foster Language-proficient Researchers and Community Efforts. Our analysis shows that the availability of NLP researchers who are fluent in languages highly correlates with the availability of datasets. Moreover, monolingual test suites cover only 16 languages, such as Chinese (Xu et al., 2020), Indic Languages (Kakwani et al., 2020), Polish (Rybak et al., 2020), Persian (Khashabi et al., 2021), Russian (Shavrina et al., 2020) or Arabic (Seelawi et al., 2021), where efforts are driven by language-proficient NLP researchers. Organizing these large-scale, inter-organization efforts can be challenging but have profound effects. Recent community efforts such as Masakhane * spur research for under-resourced languages, resulting in new valuable resources for underrepresented languages (e.g., MasakhaNER; Adelani et al. 2021). Developing a directory of language-proficient NLP researchers interested in global collaboration could foster more cooperation. In the long run, globalized NLP education like AFIRM * will be necessary. A directory of potential funding sources to support multlingual data collection can also be helpful.

On Inclusive Venues. The academic publication/conference reviewing system should also reward efforts to develop language-specific resources, without perceiving this as a niche, low-impact effort (Rogers et al., 2022). As a community, we should encourage efforts to create and provide region-specific (e.g., Nordic Conference on Computational Linguistics, Pacific Asia Conference on Language, Information and Computation), language-oriented (e.g., Deep Learning for Low-Resource NLP, AfricaNLP, Workshop on Indian Language Data: Resources and Evaluation), and data-oriented (e.g., NeurIPS dataset and benchmark track) venues for introducing multilingual datasets. Adapting existing systems to new and low-resource languages poses a challenging and intriguing task as well as substantial research inquiries. The community should continue supporting such efforts and expand evaluation data for diverse target languages.","# QUA Analysis:

1. (What is the main suggestion for fostering language-proficient researchers and community efforts?): sent1
    1.1. (What does the analysis show about the availability of NLP researchers and datasets?): sent2
    1.2. (What is the coverage of monolingual test suites and who drives these efforts?): sent3
    1.3. (What are the challenges and effects of organizing large-scale, inter-organization efforts?): sent4
    1.4. (What are some examples of recent community efforts and their outcomes?): sent5
    1.5. (What could foster more cooperation among language-proficient NLP researchers?): sent6
    1.6. (What is necessary for the long run in globalized NLP education?): sent7
    1.7. (What additional resource could be helpful for multilingual data collection?): sent8

2. (What is the main suggestion for inclusive venues?): sent9
    2.1. (What should the academic publication/conference reviewing system do?): sent9
    2.2. (What efforts should the community encourage regarding venues?): sent10
    2.3. (What challenges and research inquiries are posed by adapting existing systems to new and low-resource languages?): sent11
    2.4. (What should the community continue to support and expand?): sent12",What should the academic publication/conference reviewing system do to support language-specific resources?,"sent9, sent10, sent12","On Inclusive Venues. The academic publication/conference reviewing system should also reward efforts to develop language-specific resources, without perceiving this as a niche, low-impact effort (Rogers et al., 2022).

As a community, we should encourage efforts to create and provide region-specific (e.g., Nordic Conference on Computational Linguistics, Pacific Asia Conference on Language, Information and Computation), language-oriented (e.g., Deep Learning for Low-Resource NLP, AfricaNLP, Workshop on Indian Language Data: Resources and Evaluation), and data-oriented (e.g., NeurIPS dataset and benchmark track) venues for introducing multilingual datasets.

The community should continue supporting such efforts and expand evaluation data for diverse target languages.","The academic publication/conference reviewing system should reward efforts to develop language-specific resources, without perceiving this as a niche, low-impact effort (Rogers et al., 2022).

Efforts to create and provide region-specific, language-oriented, and data-oriented venues for introducing multilingual datasets should be encouraged (Rogers et al., 2022).

The community should continue supporting such efforts and expand evaluation data for diverse target languages.","Questions:

What should the academic publication/conference reviewing system do to support language-specific resources?

Answer:

The academic publication/conference reviewing system should reward efforts to develop language-specific resources, without perceiving this as a niche, low-impact effort (Rogers et al., 2022).

Efforts to create and provide region-specific, language-oriented, and data-oriented venues for introducing multilingual datasets should be encouraged (Rogers et al., 2022).

The community should continue supporting such efforts and expand evaluation data for diverse target languages."
The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges,Linguistic-Driven Approaches,"Equivalence Constraint In a well-formed codeswitched sentence, the switching takes place at those points where the grammatical constraints of both languages are satisfied (Poplack, 1980). Fung (2012, 2013) incorporate this syntactic constraint to a statistical code-switch language model (LM) and evaluate the model on Chinese-English code-switched speech recognition. On the same line of work, Pratapa et al. (2018a); Pratapa and Choudhury (2021) implement the same constraint to Hindi-English CSW data by producing parse trees of parallel sentences and matching the surface order of child nodes in the trees. Winata et al. (2019c) apply the constraint to generate synthetic CSW text and find that combining the real CSW data with synthetic CSW data can effectively improve the perplexity. They also treat parallel sentences as a linear structure and only allow switching on non-crossing alignments.

Matrix-Embedded Language Framework (MLF) Myers-Scotton (1997) proposed that in bilingual CSW, there exists an asymmetrical relationship between the dominant matrix language and the subordinate embedded language.

Matrix language provides the frame of the sentence by governing all or most of the most of the grammatical morphemes as well as word order, whereas syntactic elements that bear no or only limited grammatical function can be provided by the embedded language (Johanson, 1999;Myers-Scotton, 2005 Functional Head Constraint Belazi et al. (1994) posit that it is impossible to switch languages between a functional head and its complement because of the strong relationship between the two constituents. Li and Fung (2014) use the constraint of the LM by first expanding the search network with a translation model and then using parsing to restrict paths to those permissible under the constraint.","1. What is the Equivalence Constraint in code-switching?
    1.1. How has the Equivalence Constraint been applied in research?
        1.1.1. How did Fung (2012, 2013) incorporate the Equivalence Constraint?
            1.1.1.1. sent2
        1.1.2. How did Pratapa et al. (2018a); Pratapa and Choudhury (2021) implement the Equivalence Constraint?
            1.1.2.1. sent3
        1.1.3. How did Winata et al. (2019c) apply the Equivalence Constraint?
            1.1.3.1. sent4
            1.1.3.2. What additional method did Winata et al. (2019c) use in their application of the Equivalence Constraint?
                1.1.3.2.1. sent5

2. What is the Matrix-Embedded Language Framework (MLF)?
    2.1. What is the relationship between the matrix language and the embedded language in MLF?
        2.1.1. How does the matrix language function in a sentence?
            2.1.1.1. sent7

3. What is the Functional Head Constraint in code-switching?
    3.1. What does the Functional Head Constraint propose?
        3.1.1. What is the basis for the Functional Head Constraint?
            3.1.1.1. sent8
    3.2. How has the Functional Head Constraint been applied in research?
        3.2.1. How did Li and Fung (2014) use the Functional Head Constraint?
            3.2.1.1. sent9",How has the Equivalence Constraint been applied to Chinese-English code-switched speech recognition?,"sent2, sent4, sent5","Fung (2012, 2013) incorporate this syntactic constraint to a statistical code-switch language model (LM) and evaluate the model on Chinese-English code-switched speech recognition.

Winata et al. (2019c) apply the constraint to generate synthetic CSW text and find that combining the real CSW data with synthetic CSW data can effectively improve the perplexity.

They also treat parallel sentences as a linear structure and only allow switching on non-crossing alignments.","The Equivalence Constraint has been incorporated into a statistical code-switch language model (LM) and evaluated on Chinese-English code-switched speech recognition (Fung, 2012, 2013).

Winata et al. (2019c) apply the constraint to generate synthetic code-switched text and find that combining the real code-switched data with synthetic code-switched data can effectively improve the perplexity.

They also treat parallel sentences as a linear structure and only allow switching on non-crossing alignments.","Questions:

How has the Equivalence Constraint been applied to Chinese-English code-switched speech recognition?

Answer:

The Equivalence Constraint has been incorporated into a statistical code-switch language model (LM) and evaluated on Chinese-English code-switched speech recognition (Fung, 2012, 2013).

Winata et al. (2019c) apply the constraint to generate synthetic code-switched text and find that combining the real code-switched data with synthetic code-switched data can effectively improve the perplexity.

They also treat parallel sentences as a linear structure and only allow switching on non-crossing alignments."
The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges,Utilizing Neural Networks,"Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time. In contrast, the statistical and rule-based approaches are diminishing. Compared to ISCA, we see more adaptation of the pre-training model. This is because ACL work is more text-based focused, where pre-trained LMs are more widely available.

Neural-Based Models Figure 5 shows that the trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods diminished gradually through time, but they are still used even with a low percentage. RNN and LSTM architectures are commonly used in sequence modeling, such as language modeling (Adel et al., 2013;Vu and Schultz, 2014;Adel et al., 2014c;Winata et al., 2018a;Garg et al., 2018a;Winata et al., 2019c) and CSW identification (Samih et al., 2016a). DNN-based and hybrid HMM-DNN models are used in speech recognition models .

Pre-trained Embeddings Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer. Common pre-trained embeddings used in the literature are monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017). A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018). A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b). Characterbased embeddings have also been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b;Attia et al., 2018;Aguilar et al., 2021). Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b). In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning.

Language Models Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R to deal with CSW data (Khanuja et al., 2020b;Aguilar and Solorio, 2020;Pant and Dadu, 2020;Patwa et al., 2020;Winata et al., 2021a). These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages. Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets. Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding that is suitable for modeling the noisy CSW text. Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context. While XLM-R provides the best result, it is also computationally heavy. There needed to be more exploration of larger models. We see that pre-trained LMs provide better empirical results on current benchmark tasks and enables an end-to-end approach. Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available. However, the downside is that there is little understanding of how and when the LMs would fail, thus we encourage more interpretability work on these LMs in CSW setting.  et al., 2023), especially on different CSW variations. CSW style can vary in different regions of the world, and it would be interesting to gather more datasets on unexplored and unknown styles, which can be useful for further research and investigation on linguistics and NLP. Therefore, one future direction is to broaden the language scope of CSW research.","1. (What is the general trend in NLP regarding neural methods and pre-trained models?): sent1
    1.1. (How do statistical and rule-based approaches compare to neural methods?): sent2
    1.2. (How does the adaptation of pre-training models differ between ISCA and ACL?): sent3
        1.2.1. (Why is there more adaptation of pre-training models in ACL work?): sent4
    1.3. (What does Figure 5 show about the trend of neural-based models?): sent5
2. (What are the common architectures used in sequence modeling?): sent6
3. (What models are used in speech recognition?): sent7
4. (How are pre-trained embeddings utilized in neural-based approaches?): sent8
    4.1. (What are common pre-trained embeddings used in the literature?): sent9
    4.2. (How are monolingual embeddings typically utilized?): sent10
    4.3. (What is a more recent approach to merging embeddings?): sent11
    4.4. (How are character-based embeddings used to address out-of-vocabulary issues?): sent12
    4.5. (What is another approach to training bilingual embeddings?): sent13
    4.6. (How is wav2vec 2.0 utilized in the speech domain?): sent14
5. (How are multilingual LMs used to deal with CSW data?): sent15
    5.1. (How are these models fine-tuned?): sent16
        5.1.1. (Why do some approaches use synthetic CSW data for fine-tuning?): sent17
    5.2. (What is the character-based subword module proposed by Aguilar et al. (2021) for?): sent18
    5.3. (What does Winata et al. (2021a) compare in their study?): sent19
        5.3.1. (What is the result of using XLM-R?): sent20
        5.3.2. (What is needed for further exploration?): sent21
6. (What are the benefits of pre-trained LMs on benchmark tasks?): sent22
    6.1. (What is the theoretical implication of using pre-trained LMs for CSW tasks?): sent23
    6.2. (What is the downside of using pre-trained LMs?): sent24
        6.2.1. (What is encouraged for future work on LMs in CSW settings?): sent24
7. (What is a future direction for CSW research?): sent25, sent26",What is the general trend in NLP regarding neural methods and pre-trained models?,"sent1, sent2, sent5","Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time.

In contrast, the statistical and rule-based approaches are diminishing.

Neural-Based Models Figure 5 shows that the trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods diminished gradually through time, but they are still used even with a low percentage.","The adoption of neural methods and pre-trained models has been growing in popularity over time, following general NLP trends.

In contrast, the statistical and rule-based approaches are diminishing.

The trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods has gradually diminished over time, although they are still used to a lesser extent.","Questions:

What is the general trend in NLP regarding neural methods and pre-trained models?

Answer:

The adoption of neural methods and pre-trained models has been growing in popularity over time, following general NLP trends.

In contrast, the statistical and rule-based approaches are diminishing.

The trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods has gradually diminished over time, although they are still used to a lesser extent."
The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges,Utilizing Neural Networks,"Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time. In contrast, the statistical and rule-based approaches are diminishing. Compared to ISCA, we see more adaptation of the pre-training model. This is because ACL work is more text-based focused, where pre-trained LMs are more widely available.

Neural-Based Models Figure 5 shows that the trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods diminished gradually through time, but they are still used even with a low percentage. RNN and LSTM architectures are commonly used in sequence modeling, such as language modeling (Adel et al., 2013;Vu and Schultz, 2014;Adel et al., 2014c;Winata et al., 2018a;Garg et al., 2018a;Winata et al., 2019c) and CSW identification (Samih et al., 2016a). DNN-based and hybrid HMM-DNN models are used in speech recognition models .

Pre-trained Embeddings Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer. Common pre-trained embeddings used in the literature are monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017). A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018). A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b). Characterbased embeddings have also been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b;Attia et al., 2018;Aguilar et al., 2021). Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b). In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning.

Language Models Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R to deal with CSW data (Khanuja et al., 2020b;Aguilar and Solorio, 2020;Pant and Dadu, 2020;Patwa et al., 2020;Winata et al., 2021a). These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages. Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets. Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding that is suitable for modeling the noisy CSW text. Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context. While XLM-R provides the best result, it is also computationally heavy. There needed to be more exploration of larger models. We see that pre-trained LMs provide better empirical results on current benchmark tasks and enables an end-to-end approach. Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available. However, the downside is that there is little understanding of how and when the LMs would fail, thus we encourage more interpretability work on these LMs in CSW setting.  et al., 2023), especially on different CSW variations. CSW style can vary in different regions of the world, and it would be interesting to gather more datasets on unexplored and unknown styles, which can be useful for further research and investigation on linguistics and NLP. Therefore, one future direction is to broaden the language scope of CSW research.","1. (What is the general trend in NLP regarding neural methods and pre-trained models?): sent1
    1.1. (How do statistical and rule-based approaches compare to neural methods?): sent2
    1.2. (How does the adaptation of pre-training models differ between ISCA and ACL?): sent3
        1.2.1. (Why is there more adaptation of pre-training models in ACL work?): sent4
    1.3. (What does Figure 5 show about the trend of neural-based models?): sent5
2. (What are the common architectures used in sequence modeling?): sent6
3. (What models are used in speech recognition?): sent7
4. (How are pre-trained embeddings utilized in neural-based approaches?): sent8
    4.1. (What are common pre-trained embeddings used in the literature?): sent9
    4.2. (How are monolingual embeddings typically utilized?): sent10
    4.3. (What is a more recent approach to merging embeddings?): sent11
    4.4. (How are character-based embeddings used to address out-of-vocabulary issues?): sent12
    4.5. (What is another approach to training bilingual embeddings?): sent13
    4.6. (How is wav2vec 2.0 utilized in the speech domain?): sent14
5. (How are multilingual LMs used to deal with CSW data?): sent15
    5.1. (How are these models fine-tuned?): sent16
        5.1.1. (Why do some approaches use synthetic CSW data for fine-tuning?): sent17
    5.2. (What is the character-based subword module proposed by Aguilar et al. (2021) for?): sent18
    5.3. (What does Winata et al. (2021a) compare in their study?): sent19
        5.3.1. (What is the result of using XLM-R?): sent20
        5.3.2. (What is needed for further exploration?): sent21
6. (What are the benefits of pre-trained LMs on benchmark tasks?): sent22
    6.1. (What is the theoretical implication of using pre-trained LMs for CSW tasks?): sent23
    6.2. (What is the downside of using pre-trained LMs?): sent24
        6.2.1. (What is encouraged for future work on LMs in CSW settings?): sent24
7. (What is a future direction for CSW research?): sent25, sent26",How are pre-trained embeddings utilized in neural-based approaches?,"sent8, sent9, sent10, sent11, sent12, sent13, sent14","Pre-trained Embeddings Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer.

Common pre-trained embeddings used in the literature are monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017).

A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018).

A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b).

Characterbased embeddings have also been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b;Attia et al., 2018;Aguilar et al., 2021).

Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b).

In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning.","Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer.

Common pre-trained embeddings used in the literature include monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017).

A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018).

A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b).

Character-based embeddings have also been explored in the literature to address the out-of-vocabulary issues on word embeddings (Winata et al., 2018b; Attia et al., 2018; Aguilar et al., 2021).

Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b).

In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning.","Questions:

How are pre-trained embeddings utilized in neural-based approaches?

Answer:

Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer.

Common pre-trained embeddings used in the literature include monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017).

A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018).

A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b).

Character-based embeddings have also been explored in the literature to address the out-of-vocabulary issues on word embeddings (Winata et al., 2018b; Attia et al., 2018; Aguilar et al., 2021).

Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b).

In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning."
The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges,Utilizing Neural Networks,"Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time. In contrast, the statistical and rule-based approaches are diminishing. Compared to ISCA, we see more adaptation of the pre-training model. This is because ACL work is more text-based focused, where pre-trained LMs are more widely available.

Neural-Based Models Figure 5 shows that the trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods diminished gradually through time, but they are still used even with a low percentage. RNN and LSTM architectures are commonly used in sequence modeling, such as language modeling (Adel et al., 2013;Vu and Schultz, 2014;Adel et al., 2014c;Winata et al., 2018a;Garg et al., 2018a;Winata et al., 2019c) and CSW identification (Samih et al., 2016a). DNN-based and hybrid HMM-DNN models are used in speech recognition models .

Pre-trained Embeddings Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer. Common pre-trained embeddings used in the literature are monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017). A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018). A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b). Characterbased embeddings have also been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b;Attia et al., 2018;Aguilar et al., 2021). Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b). In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning.

Language Models Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R to deal with CSW data (Khanuja et al., 2020b;Aguilar and Solorio, 2020;Pant and Dadu, 2020;Patwa et al., 2020;Winata et al., 2021a). These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages. Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets. Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding that is suitable for modeling the noisy CSW text. Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context. While XLM-R provides the best result, it is also computationally heavy. There needed to be more exploration of larger models. We see that pre-trained LMs provide better empirical results on current benchmark tasks and enables an end-to-end approach. Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available. However, the downside is that there is little understanding of how and when the LMs would fail, thus we encourage more interpretability work on these LMs in CSW setting.  et al., 2023), especially on different CSW variations. CSW style can vary in different regions of the world, and it would be interesting to gather more datasets on unexplored and unknown styles, which can be useful for further research and investigation on linguistics and NLP. Therefore, one future direction is to broaden the language scope of CSW research.","1. (What is the general trend in NLP regarding neural methods and pre-trained models?): sent1
    1.1. (How do statistical and rule-based approaches compare to neural methods?): sent2
    1.2. (How does the adaptation of pre-training models differ between ISCA and ACL?): sent3
        1.2.1. (Why is there more adaptation of pre-training models in ACL work?): sent4
    1.3. (What does Figure 5 show about the trend of neural-based models?): sent5
2. (What are the common architectures used in sequence modeling?): sent6
3. (What models are used in speech recognition?): sent7
4. (How are pre-trained embeddings utilized in neural-based approaches?): sent8
    4.1. (What are common pre-trained embeddings used in the literature?): sent9
    4.2. (How are monolingual embeddings typically utilized?): sent10
    4.3. (What is a more recent approach to merging embeddings?): sent11
    4.4. (How are character-based embeddings used to address out-of-vocabulary issues?): sent12
    4.5. (What is another approach to training bilingual embeddings?): sent13
    4.6. (How is wav2vec 2.0 utilized in the speech domain?): sent14
5. (How are multilingual LMs used to deal with CSW data?): sent15
    5.1. (How are these models fine-tuned?): sent16
        5.1.1. (Why do some approaches use synthetic CSW data for fine-tuning?): sent17
    5.2. (What is the character-based subword module proposed by Aguilar et al. (2021) for?): sent18
    5.3. (What does Winata et al. (2021a) compare in their study?): sent19
        5.3.1. (What is the result of using XLM-R?): sent20
        5.3.2. (What is needed for further exploration?): sent21
6. (What are the benefits of pre-trained LMs on benchmark tasks?): sent22
    6.1. (What is the theoretical implication of using pre-trained LMs for CSW tasks?): sent23
    6.2. (What is the downside of using pre-trained LMs?): sent24
        6.2.1. (What is encouraged for future work on LMs in CSW settings?): sent24
7. (What is a future direction for CSW research?): sent25, sent26",How are multilingual LMs used to deal with CSW data?,"sent15, sent16, sent17, sent18, sent19, sent20, sent21","Language Models Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R to deal with CSW data (Khanuja et al., 2020b;Aguilar and Solorio, 2020;Pant and Dadu, 2020;Patwa et al., 2020;Winata et al., 2021a).

These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages.

Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets.

Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding that is suitable for modeling the noisy CSW text.

Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context.

While XLM-R provides the best result, it is also computationally heavy.

There needed to be more exploration of larger models.","Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R, to deal with CSW data (Khanuja et al., 2020b; Aguilar and Solorio, 2020; Pant and Dadu, 2020; Patwa et al., 2020; Winata et al., 2021a).

These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages.

Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets.

Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding suitable for modeling the noisy CSW text.

Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context.

While XLM-R provides the best result, it is also computationally heavy.

There is a need for more exploration of larger models.","Questions:

How are multilingual LMs used to deal with CSW data?

Answer:

Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R, to deal with CSW data (Khanuja et al., 2020b; Aguilar and Solorio, 2020; Pant and Dadu, 2020; Patwa et al., 2020; Winata et al., 2021a).

These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages.

Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets.

Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding suitable for modeling the noisy CSW text.

Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context.

While XLM-R provides the best result, it is also computationally heavy.

There is a need for more exploration of larger models."
The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges,Utilizing Neural Networks,"Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time. In contrast, the statistical and rule-based approaches are diminishing. Compared to ISCA, we see more adaptation of the pre-training model. This is because ACL work is more text-based focused, where pre-trained LMs are more widely available.

Neural-Based Models Figure 5 shows that the trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods diminished gradually through time, but they are still used even with a low percentage. RNN and LSTM architectures are commonly used in sequence modeling, such as language modeling (Adel et al., 2013;Vu and Schultz, 2014;Adel et al., 2014c;Winata et al., 2018a;Garg et al., 2018a;Winata et al., 2019c) and CSW identification (Samih et al., 2016a). DNN-based and hybrid HMM-DNN models are used in speech recognition models .

Pre-trained Embeddings Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer. Common pre-trained embeddings used in the literature are monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017). A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018). A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b). Characterbased embeddings have also been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b;Attia et al., 2018;Aguilar et al., 2021). Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b). In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning.

Language Models Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R to deal with CSW data (Khanuja et al., 2020b;Aguilar and Solorio, 2020;Pant and Dadu, 2020;Patwa et al., 2020;Winata et al., 2021a). These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages. Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets. Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding that is suitable for modeling the noisy CSW text. Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context. While XLM-R provides the best result, it is also computationally heavy. There needed to be more exploration of larger models. We see that pre-trained LMs provide better empirical results on current benchmark tasks and enables an end-to-end approach. Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available. However, the downside is that there is little understanding of how and when the LMs would fail, thus we encourage more interpretability work on these LMs in CSW setting.  et al., 2023), especially on different CSW variations. CSW style can vary in different regions of the world, and it would be interesting to gather more datasets on unexplored and unknown styles, which can be useful for further research and investigation on linguistics and NLP. Therefore, one future direction is to broaden the language scope of CSW research.","1. (What is the general trend in NLP regarding neural methods and pre-trained models?): sent1
    1.1. (How do statistical and rule-based approaches compare to neural methods?): sent2
    1.2. (How does the adaptation of pre-training models differ between ISCA and ACL?): sent3
        1.2.1. (Why is there more adaptation of pre-training models in ACL work?): sent4
    1.3. (What does Figure 5 show about the trend of neural-based models?): sent5
2. (What are the common architectures used in sequence modeling?): sent6
3. (What models are used in speech recognition?): sent7
4. (How are pre-trained embeddings utilized in neural-based approaches?): sent8
    4.1. (What are common pre-trained embeddings used in the literature?): sent9
    4.2. (How are monolingual embeddings typically utilized?): sent10
    4.3. (What is a more recent approach to merging embeddings?): sent11
    4.4. (How are character-based embeddings used to address out-of-vocabulary issues?): sent12
    4.5. (What is another approach to training bilingual embeddings?): sent13
    4.6. (How is wav2vec 2.0 utilized in the speech domain?): sent14
5. (How are multilingual LMs used to deal with CSW data?): sent15
    5.1. (How are these models fine-tuned?): sent16
        5.1.1. (Why do some approaches use synthetic CSW data for fine-tuning?): sent17
    5.2. (What is the character-based subword module proposed by Aguilar et al. (2021) for?): sent18
    5.3. (What does Winata et al. (2021a) compare in their study?): sent19
        5.3.1. (What is the result of using XLM-R?): sent20
        5.3.2. (What is needed for further exploration?): sent21
6. (What are the benefits of pre-trained LMs on benchmark tasks?): sent22
    6.1. (What is the theoretical implication of using pre-trained LMs for CSW tasks?): sent23
    6.2. (What is the downside of using pre-trained LMs?): sent24
        6.2.1. (What is encouraged for future work on LMs in CSW settings?): sent24
7. (What is a future direction for CSW research?): sent25, sent26",What are the benefits of pre-trained LMs on benchmark tasks?,"sent22, sent23, sent24","We see that pre-trained LMs provide better empirical results on current benchmark tasks and enables an end-to-end approach.

Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available.

However, the downside is that there is little understanding of how and when the LMs would fail, thus we encourage more interpretability work on these LMs in CSW setting.  et al., 2023), especially on different CSW variations.","Pre-trained LMs provide better empirical results on current benchmark tasks and enable an end-to-end approach.

Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available.

However, the downside is that there is little understanding of how and when the LMs would fail, thus more interpretability work on these LMs in CSW settings is encouraged (et al., 2023), especially on different CSW variations.","Questions:

What are the benefits of pre-trained LMs on benchmark tasks?

Answer:

Pre-trained LMs provide better empirical results on current benchmark tasks and enable an end-to-end approach.

Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available.

However, the downside is that there is little understanding of how and when the LMs would fail, thus more interpretability work on these LMs in CSW settings is encouraged (et al., 2023), especially on different CSW variations."
Let's Negotiate! A Survey of Negotiation Dialogue Systems,Negotiation in Human,"Humans negotiate everyday in their daily routines. Negotiation is used to manage conflict and is the primary give-and-take process by which people try to reach an agreement (Fisher et al., 2011;Lewicki et al., 2011). Research on negotiation has been conducted for almost 60 years in the field of psychology, political science, and communication. It has evolved over the past decades from exploring game theory (Walton and McKersie, 1991), behavior decisions driven by the cognitive revolution in psychology (Bazerman and Neale, 1993), to cultural differences in the 2000s (Bazerman et al., 2000). Negotiation research, however, is now forced to confront the implications of human/AI collaborations given recent advancements in machine learning (Sycara and Dai, 2010). Converging efforts from social scientists and data scientists who incorporate insights from both fields will be fruitful in maximizing expectations and outcomes during negotiation processes.

Negotiation is a process by which two or more parties attempt to resolve their opposing interests. Strategy of negotiation can be distributive, such as bargaining (Fershtman, 1990) and integrative, such as maximizing unilateral interests (Bazerman and Neale, 1993), both of which are used in various social situations such as informal, peer to peer, organizational, and diplomatic country to country settings. The implications for enhancing outcomes are thus large and important to understand. Research from psychology demonstrates that the negotiation process can be affected by psychological factors, such as personality (Sharma et al., 2013), relationship (Olekalns and Smith, 2003), social status (Blader and Chen, 2012), and cultural background (Leung and Cohen, 2011).","# QUA Analysis:

1. (What is the role of negotiation in daily human activities?): sent1
    1.1. (What is negotiation used for?): sent2
    1.2. (How long has research on negotiation been conducted and in which fields?): sent3
        1.2.1. (How has negotiation research evolved over the decades?): sent4
        1.2.2. (What is the current focus of negotiation research?): sent5
            1.2.2.1. (What will be the result of converging efforts from social scientists and data scientists?): sent6
    1.3. (What is negotiation?): sent7
        1.3.1. (What are the strategies of negotiation and where are they used?): sent8
    1.4. (Why is it important to understand the implications for enhancing outcomes?): sent9
    1.5. (What factors can affect the negotiation process according to psychology research?): sent10",How has negotiation research evolved over the decades?,"sent3, sent4, sent5","Research on negotiation has been conducted for almost 60 years in the field of psychology, political science, and communication.

It has evolved over the past decades from exploring game theory (Walton and McKersie, 1991), behavior decisions driven by the cognitive revolution in psychology (Bazerman and Neale, 1993), to cultural differences in the 2000s (Bazerman et al., 2000).

Negotiation research, however, is now forced to confront the implications of human/AI collaborations given recent advancements in machine learning (Sycara and Dai, 2010).","Research on negotiation has been conducted for almost 60 years in the fields of psychology, political science, and communication.

Over the past decades, it has evolved from exploring game theory (Walton and McKersie, 1991), behavior decisions driven by the cognitive revolution in psychology (Bazerman and Neale, 1993), to cultural differences in the 2000s (Bazerman et al., 2000).

Negotiation research, however, is now forced to confront the implications of human/AI collaborations given recent advancements in machine learning (Sycara and Dai, 2010).","Questions:

How has negotiation research evolved over the decades?

Answer:

Research on negotiation has been conducted for almost 60 years in the fields of psychology, political science, and communication.

Over the past decades, it has evolved from exploring game theory (Walton and McKersie, 1991), behavior decisions driven by the cognitive revolution in psychology (Bazerman and Neale, 1993), to cultural differences in the 2000s (Bazerman et al., 2000).

Negotiation research, however, is now forced to confront the implications of human/AI collaborations given recent advancements in machine learning (Sycara and Dai, 2010)."
Let's Negotiate! A Survey of Negotiation Dialogue Systems,Negotiation in Human,"Humans negotiate everyday in their daily routines. Negotiation is used to manage conflict and is the primary give-and-take process by which people try to reach an agreement (Fisher et al., 2011;Lewicki et al., 2011). Research on negotiation has been conducted for almost 60 years in the field of psychology, political science, and communication. It has evolved over the past decades from exploring game theory (Walton and McKersie, 1991), behavior decisions driven by the cognitive revolution in psychology (Bazerman and Neale, 1993), to cultural differences in the 2000s (Bazerman et al., 2000). Negotiation research, however, is now forced to confront the implications of human/AI collaborations given recent advancements in machine learning (Sycara and Dai, 2010). Converging efforts from social scientists and data scientists who incorporate insights from both fields will be fruitful in maximizing expectations and outcomes during negotiation processes.

Negotiation is a process by which two or more parties attempt to resolve their opposing interests. Strategy of negotiation can be distributive, such as bargaining (Fershtman, 1990) and integrative, such as maximizing unilateral interests (Bazerman and Neale, 1993), both of which are used in various social situations such as informal, peer to peer, organizational, and diplomatic country to country settings. The implications for enhancing outcomes are thus large and important to understand. Research from psychology demonstrates that the negotiation process can be affected by psychological factors, such as personality (Sharma et al., 2013), relationship (Olekalns and Smith, 2003), social status (Blader and Chen, 2012), and cultural background (Leung and Cohen, 2011).","# QUA Analysis:

1. (What is the role of negotiation in daily human activities?): sent1
    1.1. (What is negotiation used for?): sent2
    1.2. (How long has research on negotiation been conducted and in which fields?): sent3
        1.2.1. (How has negotiation research evolved over the decades?): sent4
        1.2.2. (What is the current focus of negotiation research?): sent5
            1.2.2.1. (What will be the result of converging efforts from social scientists and data scientists?): sent6
    1.3. (What is negotiation?): sent7
        1.3.1. (What are the strategies of negotiation and where are they used?): sent8
    1.4. (Why is it important to understand the implications for enhancing outcomes?): sent9
    1.5. (What factors can affect the negotiation process according to psychology research?): sent10",What is the role of negotiation in daily human activities?,"sent1, sent2, sent7","Humans negotiate everyday in their daily routines.

Negotiation is used to manage conflict and is the primary give-and-take process by which people try to reach an agreement (Fisher et al., 2011;Lewicki et al., 2011).

Negotiation is a process by which two or more parties attempt to resolve their opposing interests.","Humans negotiate every day in their daily routines.

Negotiation is used to manage conflict and is the primary give-and-take process by which people try to reach an agreement (Fisher et al., 2011; Lewicki et al., 2011).

Negotiation is a process by which two or more parties attempt to resolve their opposing interests.","Questions:

What is the role of negotiation in daily human activities?

Answer:

Humans negotiate every day in their daily routines.

Negotiation is used to manage conflict and is the primary give-and-take process by which people try to reach an agreement (Fisher et al., 2011; Lewicki et al., 2011).

Negotiation is a process by which two or more parties attempt to resolve their opposing interests."
Let's Negotiate! A Survey of Negotiation Dialogue Systems,Negotiation in Human,"Humans negotiate everyday in their daily routines. Negotiation is used to manage conflict and is the primary give-and-take process by which people try to reach an agreement (Fisher et al., 2011;Lewicki et al., 2011). Research on negotiation has been conducted for almost 60 years in the field of psychology, political science, and communication. It has evolved over the past decades from exploring game theory (Walton and McKersie, 1991), behavior decisions driven by the cognitive revolution in psychology (Bazerman and Neale, 1993), to cultural differences in the 2000s (Bazerman et al., 2000). Negotiation research, however, is now forced to confront the implications of human/AI collaborations given recent advancements in machine learning (Sycara and Dai, 2010). Converging efforts from social scientists and data scientists who incorporate insights from both fields will be fruitful in maximizing expectations and outcomes during negotiation processes.

Negotiation is a process by which two or more parties attempt to resolve their opposing interests. Strategy of negotiation can be distributive, such as bargaining (Fershtman, 1990) and integrative, such as maximizing unilateral interests (Bazerman and Neale, 1993), both of which are used in various social situations such as informal, peer to peer, organizational, and diplomatic country to country settings. The implications for enhancing outcomes are thus large and important to understand. Research from psychology demonstrates that the negotiation process can be affected by psychological factors, such as personality (Sharma et al., 2013), relationship (Olekalns and Smith, 2003), social status (Blader and Chen, 2012), and cultural background (Leung and Cohen, 2011).","# QUA Analysis:

1. (What is the role of negotiation in daily human activities?): sent1
    1.1. (What is negotiation used for?): sent2
    1.2. (How long has research on negotiation been conducted and in which fields?): sent3
        1.2.1. (How has negotiation research evolved over the decades?): sent4
        1.2.2. (What is the current focus of negotiation research?): sent5
            1.2.2.1. (What will be the result of converging efforts from social scientists and data scientists?): sent6
    1.3. (What is negotiation?): sent7
        1.3.1. (What are the strategies of negotiation and where are they used?): sent8
    1.4. (Why is it important to understand the implications for enhancing outcomes?): sent9
    1.5. (What factors can affect the negotiation process according to psychology research?): sent10",What factors can affect the negotiation process according to psychology research?,"sent10, sent2, sent7","Research from psychology demonstrates that the negotiation process can be affected by psychological factors, such as personality (Sharma et al., 2013), relationship (Olekalns and Smith, 2003), social status (Blader and Chen, 2012), and cultural background (Leung and Cohen, 2011).

Negotiation is used to manage conflict and is the primary give-and-take process by which people try to reach an agreement (Fisher et al., 2011;Lewicki et al., 2011).

Negotiation is a process by which two or more parties attempt to resolve their opposing interests.","Psychology research demonstrates that the negotiation process can be affected by psychological factors, such as personality (Sharma et al., 2013), relationship (Olekalns and Smith, 2003), social status (Blader and Chen, 2012), and cultural background (Leung and Cohen, 2011).

Negotiation is used to manage conflict and is the primary give-and-take process by which people try to reach an agreement (Fisher et al., 2011; Lewicki et al., 2011).

Negotiation is a process by which two or more parties attempt to resolve their opposing interests.","Questions:

What factors can affect the negotiation process according to psychology research?

Answer:

Psychology research demonstrates that the negotiation process can be affected by psychological factors, such as personality (Sharma et al., 2013), relationship (Olekalns and Smith, 2003), social status (Blader and Chen, 2012), and cultural background (Leung and Cohen, 2011).

Negotiation is used to manage conflict and is the primary give-and-take process by which people try to reach an agreement (Fisher et al., 2011; Lewicki et al., 2011).

Negotiation is a process by which two or more parties attempt to resolve their opposing interests."
Let's Negotiate! A Survey of Negotiation Dialogue Systems,Negotiation in Human,"Humans negotiate everyday in their daily routines. Negotiation is used to manage conflict and is the primary give-and-take process by which people try to reach an agreement (Fisher et al., 2011;Lewicki et al., 2011). Research on negotiation has been conducted for almost 60 years in the field of psychology, political science, and communication. It has evolved over the past decades from exploring game theory (Walton and McKersie, 1991), behavior decisions driven by the cognitive revolution in psychology (Bazerman and Neale, 1993), to cultural differences in the 2000s (Bazerman et al., 2000). Negotiation research, however, is now forced to confront the implications of human/AI collaborations given recent advancements in machine learning (Sycara and Dai, 2010). Converging efforts from social scientists and data scientists who incorporate insights from both fields will be fruitful in maximizing expectations and outcomes during negotiation processes.

Negotiation is a process by which two or more parties attempt to resolve their opposing interests. Strategy of negotiation can be distributive, such as bargaining (Fershtman, 1990) and integrative, such as maximizing unilateral interests (Bazerman and Neale, 1993), both of which are used in various social situations such as informal, peer to peer, organizational, and diplomatic country to country settings. The implications for enhancing outcomes are thus large and important to understand. Research from psychology demonstrates that the negotiation process can be affected by psychological factors, such as personality (Sharma et al., 2013), relationship (Olekalns and Smith, 2003), social status (Blader and Chen, 2012), and cultural background (Leung and Cohen, 2011).","# QUA Analysis:

1. (What is the role of negotiation in daily human activities?): sent1
    1.1. (What is negotiation used for?): sent2
    1.2. (How long has research on negotiation been conducted and in which fields?): sent3
        1.2.1. (How has negotiation research evolved over the decades?): sent4
        1.2.2. (What is the current focus of negotiation research?): sent5
            1.2.2.1. (What will be the result of converging efforts from social scientists and data scientists?): sent6
    1.3. (What is negotiation?): sent7
        1.3.1. (What are the strategies of negotiation and where are they used?): sent8
    1.4. (Why is it important to understand the implications for enhancing outcomes?): sent9
    1.5. (What factors can affect the negotiation process according to psychology research?): sent10",Why is it important to understand the implications for enhancing outcomes in negotiation?,"sent9, sent6, sent5","The implications for enhancing outcomes are thus large and important to understand.

Converging efforts from social scientists and data scientists who incorporate insights from both fields will be fruitful in maximizing expectations and outcomes during negotiation processes.

Negotiation research, however, is now forced to confront the implications of human/AI collaborations given recent advancements in machine learning (Sycara and Dai, 2010).","Understanding the implications for enhancing outcomes in negotiation is important.

Converging efforts from social scientists and data scientists who incorporate insights from both fields will be fruitful in maximizing expectations and outcomes during negotiation processes.

Negotiation research is now compelled to address the implications of human/AI collaborations given recent advancements in machine learning (Sycara and Dai, 2010).","Questions:

Why is it important to understand the implications for enhancing outcomes in negotiation?

Answer:

Understanding the implications for enhancing outcomes in negotiation is important.

Converging efforts from social scientists and data scientists who incorporate insights from both fields will be fruitful in maximizing expectations and outcomes during negotiation processes.

Negotiation research is now compelled to address the implications of human/AI collaborations given recent advancements in machine learning (Sycara and Dai, 2010)."
Let's Negotiate! A Survey of Negotiation Dialogue Systems,Supervised Learning,"Supervised learning (SL) is another popular paradigm for policy learning. (Lewis et al., 2017) adopt a Seq2Seq model to learn what action should be taken by maximizing the likelihood of the training data. However, supervised learning only aims to mimic the average human behavior, so He et al. (2018) propose to finetune the supervised model to directly optimize for a particular dialogue reward function, which is defined as i) the utility function of the final price for the buyer and seller ii) the difference between two agents' utilities iii) the number of utterances in the dialogue. Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training. The system response would be generated conditioned on the user utterance, dialogue context, and the predicted negotiation strategy. In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm. Recently, Dutt et al. (2021b) propose a generalised framework for identifying resisting strategies in persuasive negotiations using a pre-trained BERT model (Devlin et al., 2019).","1. What is supervised learning (SL) in the context of policy learning? 
    1.1. How is supervised learning applied in negotiation dialogue systems?
        1.1.1. How does Lewis et al. (2017) apply supervised learning? 
            - sent2
        1.1.2. What are the limitations of supervised learning and how are they addressed?
            - sent3
        1.1.3. How does Zhou et al. (2020) apply supervised learning?
            - sent4
            1.1.3.1. How is the system response generated in Zhou et al. (2020)?
                - sent5
        1.1.4. How does Joshi et al. (2021) enhance supervised learning?
            - sent6
        1.1.5. What recent advancements have been made in supervised learning for negotiation dialogue systems?
            - sent7",How is supervised learning applied in negotiation dialogue systems?,"sent1, sent2, sent4, sent6, sent7","Supervised learning (SL) is another popular paradigm for policy learning.

(Lewis et al., 2017) adopt a Seq2Seq model to learn what action should be taken by maximizing the likelihood of the training data.

Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training.

In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm.

Recently, Dutt et al. (2021b) propose a generalised framework for identifying resisting strategies in persuasive negotiations using a pre-trained BERT model (Devlin et al., 2019).","Supervised learning (SL) is a popular paradigm for policy learning in negotiation dialogue systems.

For example, Lewis et al. (2017) adopt a Seq2Seq model to learn what action should be taken by maximizing the likelihood of the training data.

Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training.

In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm.

Recently, Dutt et al. (2021b) propose a generalized framework for identifying resisting strategies in persuasive negotiations using a pre-trained BERT model (Devlin et al., 2019).","Questions:

How is supervised learning applied in negotiation dialogue systems?

Answer:

Supervised learning (SL) is a popular paradigm for policy learning in negotiation dialogue systems.

For example, Lewis et al. (2017) adopt a Seq2Seq model to learn what action should be taken by maximizing the likelihood of the training data.

Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training.

In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm.

Recently, Dutt et al. (2021b) propose a generalized framework for identifying resisting strategies in persuasive negotiations using a pre-trained BERT model (Devlin et al., 2019)."
Let's Negotiate! A Survey of Negotiation Dialogue Systems,Reinforcement Learning,"Reinforcement learning (RL) is one of the most common frameworks chosen for policy learning. English and Heeman (2005) are the first to use RL techniques for negotiation dialogue systems. They employed a single-agent pattern to learn the policy of two opponents individually. But singleagent RL techniques are not well suited for concurrent learning where each agent is trained against a continuously changing environment. Therefore, Georgila et al. (2014) further advances the framework with concurrent progress using multi-agent RL techniques, which simultaneously model two parties and provide a way to deal with multi-issues scenarios. Besides, Keizer et al. (2017) propose to learn the action of the target agents with a Qlearning reward function. They further propose a method based on hand-crafted rules and a method using Random Forest trained on a large human negotiation corpus from (Afantenos et al., 2012).

Most recent works try to equip RL with deep learning techniques. For instance, Zhang et al. (2020) propose OPPA, which lets the target agent behave given the system actions. The system actions are predicted and conditioned on the target agent's actions. The reward of the actions for the target agent is obtained by predicting a structured output given the whole dialogue. Besides, Shi et al. (2021) use a modular framework containing a language model to generate responses, a response detector would automatically annotate the response with a negotiation strategy, and an RL-based reward function to assign a score to the strategy. Instead of the modular framework which separates policy learning and response generation, Gao et al. (2021) propose an integrated framework with deep Q-learning, which includes multiple channel negotiation skills. It allows agents to leverage parameterized DQN to learn a comprehensive negotiation strategy that integrates linguistic communication skills and bidding strategies.","# QUA Analysis:

1. What is the role of Reinforcement Learning (RL) in policy learning for negotiation dialogue systems? 
    1.1. Who first used RL techniques for negotiation dialogue systems? 
        1.1.1. (Who first used RL techniques for negotiation dialogue systems?): sent2
        1.1.2. (How did they employ RL techniques?): sent3
        1.1.3. (What are the limitations of single-agent RL techniques?): sent4
    1.2. How has the RL framework been advanced for negotiation dialogue systems?
        1.2.1. (How has the RL framework been advanced?): sent5
        1.2.2. (What other methods have been proposed for learning the actions of target agents?): sent6, sent7
    1.3. How are recent works enhancing RL with deep learning techniques?
        1.3.1. (How are recent works enhancing RL with deep learning techniques?): sent8
        1.3.2. (What is an example of a recent work that enhances RL with deep learning techniques?): sent9, sent10, sent11
        1.3.3. (What other frameworks have been proposed recently?): sent12, sent13, sent14",How has the RL framework been advanced for negotiation dialogue systems?,"sent5, sent6, sent7","Therefore, Georgila et al. (2014) further advances the framework with concurrent progress using multi-agent RL techniques, which simultaneously model two parties and provide a way to deal with multi-issues scenarios.

Besides, Keizer et al. (2017) propose to learn the action of the target agents with a Qlearning reward function.

They further propose a method based on hand-crafted rules and a method using Random Forest trained on a large human negotiation corpus from (Afantenos et al., 2012).","Georgila et al. (2014) further advanced the framework with concurrent progress using multi-agent RL techniques, which simultaneously model two parties and provide a way to deal with multi-issue scenarios.

Additionally, Keizer et al. (2017) propose to learn the action of the target agents with a Q-learning reward function.

Additionally, they propose a method based on hand-crafted rules and a method using Random Forest trained on a large human negotiation corpus from Afantenos et al. (2012).","Questions:

How has the RL framework been advanced for negotiation dialogue systems?

Answer:

Georgila et al. (2014) further advanced the framework with concurrent progress using multi-agent RL techniques, which simultaneously model two parties and provide a way to deal with multi-issue scenarios.

Additionally, Keizer et al. (2017) propose to learn the action of the target agents with a Q-learning reward function.

Additionally, they propose a method based on hand-crafted rules and a method using Random Forest trained on a large human negotiation corpus from Afantenos et al. (2012)."
Let's Negotiate! A Survey of Negotiation Dialogue Systems,Reinforcement Learning,"Reinforcement learning (RL) is one of the most common frameworks chosen for policy learning. English and Heeman (2005) are the first to use RL techniques for negotiation dialogue systems. They employed a single-agent pattern to learn the policy of two opponents individually. But singleagent RL techniques are not well suited for concurrent learning where each agent is trained against a continuously changing environment. Therefore, Georgila et al. (2014) further advances the framework with concurrent progress using multi-agent RL techniques, which simultaneously model two parties and provide a way to deal with multi-issues scenarios. Besides, Keizer et al. (2017) propose to learn the action of the target agents with a Qlearning reward function. They further propose a method based on hand-crafted rules and a method using Random Forest trained on a large human negotiation corpus from (Afantenos et al., 2012).

Most recent works try to equip RL with deep learning techniques. For instance, Zhang et al. (2020) propose OPPA, which lets the target agent behave given the system actions. The system actions are predicted and conditioned on the target agent's actions. The reward of the actions for the target agent is obtained by predicting a structured output given the whole dialogue. Besides, Shi et al. (2021) use a modular framework containing a language model to generate responses, a response detector would automatically annotate the response with a negotiation strategy, and an RL-based reward function to assign a score to the strategy. Instead of the modular framework which separates policy learning and response generation, Gao et al. (2021) propose an integrated framework with deep Q-learning, which includes multiple channel negotiation skills. It allows agents to leverage parameterized DQN to learn a comprehensive negotiation strategy that integrates linguistic communication skills and bidding strategies.","# QUA Analysis:

1. What is the role of Reinforcement Learning (RL) in policy learning for negotiation dialogue systems? 
    1.1. Who first used RL techniques for negotiation dialogue systems? 
        1.1.1. (Who first used RL techniques for negotiation dialogue systems?): sent2
        1.1.2. (How did they employ RL techniques?): sent3
        1.1.3. (What are the limitations of single-agent RL techniques?): sent4
    1.2. How has the RL framework been advanced for negotiation dialogue systems?
        1.2.1. (How has the RL framework been advanced?): sent5
        1.2.2. (What other methods have been proposed for learning the actions of target agents?): sent6, sent7
    1.3. How are recent works enhancing RL with deep learning techniques?
        1.3.1. (How are recent works enhancing RL with deep learning techniques?): sent8
        1.3.2. (What is an example of a recent work that enhances RL with deep learning techniques?): sent9, sent10, sent11
        1.3.3. (What other frameworks have been proposed recently?): sent12, sent13, sent14",How are recent works enhancing RL with deep learning techniques?,"sent8, sent9, sent10, sent11, sent12, sent13, sent14","Most recent works try to equip RL with deep learning techniques.

For instance, Zhang et al. (2020) propose OPPA, which lets the target agent behave given the system actions.

The system actions are predicted and conditioned on the target agent's actions.

The reward of the actions for the target agent is obtained by predicting a structured output given the whole dialogue.

Besides, Shi et al. (2021) use a modular framework containing a language model to generate responses, a response detector would automatically annotate the response with a negotiation strategy, and an RL-based reward function to assign a score to the strategy.

Instead of the modular framework which separates policy learning and response generation, Gao et al. (2021) propose an integrated framework with deep Q-learning, which includes multiple channel negotiation skills.

It allows agents to leverage parameterized DQN to learn a comprehensive negotiation strategy that integrates linguistic communication skills and bidding strategies.","Most recent works aim to enhance reinforcement learning (RL) with deep learning techniques.

For instance, Zhang et al. (2020) propose OPPA, which lets the target agent behave given the system actions.

The system actions are predicted and conditioned on the target agent's actions.

The reward of the actions for the target agent is obtained by predicting a structured output given the whole dialogue.

Additionally, Shi et al. (2021) use a modular framework that includes a language model to generate responses, a response detector to automatically annotate the response with a negotiation strategy, and an RL-based reward function to assign a score to the strategy.

Instead of the modular framework which separates policy learning and response generation, Gao et al. (2021) propose an integrated framework with deep Q-learning, which includes multiple channel negotiation skills.

It allows agents to leverage parameterized DQN to learn a comprehensive negotiation strategy that integrates linguistic communication skills and bidding strategies.","Questions:

How are recent works enhancing RL with deep learning techniques?

Answer:

Most recent works aim to enhance reinforcement learning (RL) with deep learning techniques.

For instance, Zhang et al. (2020) propose OPPA, which lets the target agent behave given the system actions.

The system actions are predicted and conditioned on the target agent's actions.

The reward of the actions for the target agent is obtained by predicting a structured output given the whole dialogue.

Additionally, Shi et al. (2021) use a modular framework that includes a language model to generate responses, a response detector to automatically annotate the response with a negotiation strategy, and an RL-based reward function to assign a score to the strategy.

Instead of the modular framework which separates policy learning and response generation, Gao et al. (2021) propose an integrated framework with deep Q-learning, which includes multiple channel negotiation skills.

It allows agents to leverage parameterized DQN to learn a comprehensive negotiation strategy that integrates linguistic communication skills and bidding strategies."
A Survey of Deep Learning for Mathematical Reasoning,Analysis of Deep Learning Methods,"Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but

Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but","# QUA Analysis:

1. (Is the current representation of numeracy sufficient?): sent1
    1.1. (How are numbers currently represented in deep learning techniques?): sent2
        1.1.1. (What is the standard practice for representing numbers?): sent2
        1.1.2. (How did early neural network methods handle numbers?): sent3
        1.1.3. (How do recent language models handle numbers?): sent4
    1.2. (Are the current tokenization approaches effective?): sent5
        1.2.1. (What are the issues with current tokenization approaches?): sent6, sent7
        1.2.2. (What are the consequences of inconsistent representation?): sent8, sent9
        1.2.3. (What evidence is there of models struggling with numbers?): sent10, sent11
    1.3. (What are potential solutions to improve numeracy representation?): sent12

2. (Is the current representation of numeracy sufficient?): sent13
    2.1. (How are numbers currently represented in deep learning techniques?): sent13
        2.1.1. (What is the standard practice for representing numbers?): sent13
        2.1.2. (How did early neural network methods handle numbers?): sent14
        2.1.3. (How do recent language models handle numbers?): sent15
    2.2. (Are the current tokenization approaches effective?): sent16
        2.2.1. (What are the issues with current tokenization approaches?): sent17, sent18
        2.2.2. (What are the consequences of inconsistent representation?): sent19, sent20
        2.2.3. (What evidence is there of models struggling with numbers?): sent21, sent22
    2.3. (What are potential solutions to improve numeracy representation?): sent23",How are numbers currently represented in deep learning techniques?,"sent2, sent3, sent4","The standard practice for deep learning techniques is to treat numbers in the same way as words.

Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token.

Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens.","The standard practice for deep learning techniques is to treat numbers in the same way as words.

Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token.

Recent language models use subword tokenization techniques (Wu et al., 2016; Sennrich et al., 2016) to split numbers into atomic tokens.","Questions:

How are numbers currently represented in deep learning techniques?

Answer:

The standard practice for deep learning techniques is to treat numbers in the same way as words.

Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token.

Recent language models use subword tokenization techniques (Wu et al., 2016; Sennrich et al., 2016) to split numbers into atomic tokens."
A Survey of Deep Learning for Mathematical Reasoning,High-quality Reasoning Chains,"Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

6 Discussion and Findings

Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

6 Discussion and Findings","# QUA Analysis:

1. (What is the early chain of thought work?): sent1
    1.1. (What are the disadvantages of manually creating reasoning chains?): sent2
        1.1.1. (What is the first disadvantage?): sent3
        1.1.2. (What is the second disadvantage?): sent4
    1.2. (How do recent studies address the limitation of manually creating reasoning chains?): sent5
        1.2.1. (What are the two main aspects recent studies focus on?): sent9
            1.2.1.1. (What are process-based approaches?): sent9
                1.2.1.1.1. (What is least-to-most prompting?): sent10
                1.2.1.1.2. (What is an alternative solution using large language models?): sent11
                    1.2.1.1.2.1. (How is the computation handled in this alternative solution?): sent12
                1.2.1.1.3. (What is Chameleon?): sent13
            1.2.1.2. (What are outcome-based approaches?): sent13
                1.2.1.2.1. (What is self-consistency?): sent14
                1.2.1.2.2. (What is the complementary solution proposed by Li et al. (2022a)?): sent15",What are the disadvantages of manually creating reasoning chains?,"sent2, sent3, sent4","However, manually creating reasoning chains has two disadvantages.

First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks.

Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer.","Manually creating reasoning chains has two disadvantages.

First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks.

Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer.","Questions:

What are the disadvantages of manually creating reasoning chains?

Answer:

Manually creating reasoning chains has two disadvantages.

First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks.

Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer."
A Survey of Deep Learning for Mathematical Reasoning,High-quality Reasoning Chains,"Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

6 Discussion and Findings

Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

6 Discussion and Findings","# QUA Analysis:

1. (What is the early chain of thought work?): sent1
    1.1. (What are the disadvantages of manually creating reasoning chains?): sent2
        1.1.1. (What is the first disadvantage?): sent3
        1.1.2. (What is the second disadvantage?): sent4
    1.2. (How do recent studies address the limitation of manually creating reasoning chains?): sent5
        1.2.1. (What are the two main aspects recent studies focus on?): sent9
            1.2.1.1. (What are process-based approaches?): sent9
                1.2.1.1.1. (What is least-to-most prompting?): sent10
                1.2.1.1.2. (What is an alternative solution using large language models?): sent11
                    1.2.1.1.2.1. (How is the computation handled in this alternative solution?): sent12
                1.2.1.1.3. (What is Chameleon?): sent13
            1.2.1.2. (What are outcome-based approaches?): sent13
                1.2.1.2.1. (What is self-consistency?): sent14
                1.2.1.2.2. (What is the complementary solution proposed by Li et al. (2022a)?): sent15",How do recent studies address the limitation of manually creating reasoning chains?,"sent5, sent9, sent10, sent11, sent12, sent13, sent14, sent15","To address this limitation, recent studies mainly fo-

(Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks.

In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems.

Similarly , an alternative solution that uses large language models to express the reasoning process as a program.

The computation is then relegated to an external computer, which executes the generated programs to derive the answer.

A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a).

Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer.

In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.","Recent studies mainly focus on addressing the limitation of manually creating reasoning chains.

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks (Zhou et al., 2023; Chen et al., 2022b).

In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems.

Similarly, an alternative solution uses large language models to express the reasoning process as a program.

The computation is then relegated to an external computer, which executes the generated programs to derive the answer.

A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning. Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023; Li et al., 2022a).

Self-consistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model and marginalizes out the reasoning paths by choosing the most common answer.

In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""self-teaching"" as a complementary solution to produce a higher degree of diversity.","Questions:

How do recent studies address the limitation of manually creating reasoning chains?

Answer:

Recent studies mainly focus on addressing the limitation of manually creating reasoning chains.

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks (Zhou et al., 2023; Chen et al., 2022b).

In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems.

Similarly, an alternative solution uses large language models to express the reasoning process as a program.

The computation is then relegated to an external computer, which executes the generated programs to derive the answer.

A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning. Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023; Li et al., 2022a).

Self-consistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model and marginalizes out the reasoning paths by choosing the most common answer.

In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""self-teaching"" as a complementary solution to produce a higher degree of diversity."
A Survey of Deep Learning for Mathematical Reasoning,High-quality Reasoning Chains,"Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

6 Discussion and Findings

Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

6 Discussion and Findings","# QUA Analysis:

1. (What is the early chain of thought work?): sent1
    1.1. (What are the disadvantages of manually creating reasoning chains?): sent2
        1.1.1. (What is the first disadvantage?): sent3
        1.1.2. (What is the second disadvantage?): sent4
    1.2. (How do recent studies address the limitation of manually creating reasoning chains?): sent5
        1.2.1. (What are the two main aspects recent studies focus on?): sent9
            1.2.1.1. (What are process-based approaches?): sent9
                1.2.1.1.1. (What is least-to-most prompting?): sent10
                1.2.1.1.2. (What is an alternative solution using large language models?): sent11
                    1.2.1.1.2.1. (How is the computation handled in this alternative solution?): sent12
                1.2.1.1.3. (What is Chameleon?): sent13
            1.2.1.2. (What are outcome-based approaches?): sent13
                1.2.1.2.1. (What is self-consistency?): sent14
                1.2.1.2.2. (What is the complementary solution proposed by Li et al. (2022a)?): sent15",What are process-based approaches in improving chain-of-thought reasoning quality?,"sent9, sent10, sent11, sent12, sent13","(Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks.

In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems.

Similarly , an alternative solution that uses large language models to express the reasoning process as a program.

The computation is then relegated to an external computer, which executes the generated programs to derive the answer.

A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a).","Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks (Zhou et al., 2023; Chen et al., 2022b).

One such approach is least-to-most prompting (Zhou et al., 2023), where the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems.

Similarly, an alternative solution uses large language models to express the reasoning process as a program.

The computation is then relegated to an external computer, which executes the generated programs to derive the answer.

A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning. Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023; Li et al., 2022a).","Questions:

What are process-based approaches in improving chain-of-thought reasoning quality?

Answer:

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks (Zhou et al., 2023; Chen et al., 2022b).

One such approach is least-to-most prompting (Zhou et al., 2023), where the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems.

Similarly, an alternative solution uses large language models to express the reasoning process as a program.

The computation is then relegated to an external computer, which executes the generated programs to derive the answer.

A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning. Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023; Li et al., 2022a)."
A Survey of Deep Learning for Mathematical Reasoning,High-quality Reasoning Chains,"Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

6 Discussion and Findings

Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).

Process-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.

Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.

6 Discussion and Findings","# QUA Analysis:

1. (What is the early chain of thought work?): sent1
    1.1. (What are the disadvantages of manually creating reasoning chains?): sent2
        1.1.1. (What is the first disadvantage?): sent3
        1.1.2. (What is the second disadvantage?): sent4
    1.2. (How do recent studies address the limitation of manually creating reasoning chains?): sent5
        1.2.1. (What are the two main aspects recent studies focus on?): sent9
            1.2.1.1. (What are process-based approaches?): sent9
                1.2.1.1.1. (What is least-to-most prompting?): sent10
                1.2.1.1.2. (What is an alternative solution using large language models?): sent11
                    1.2.1.1.2.1. (How is the computation handled in this alternative solution?): sent12
                1.2.1.1.3. (What is Chameleon?): sent13
            1.2.1.2. (What are outcome-based approaches?): sent13
                1.2.1.2.1. (What is self-consistency?): sent14
                1.2.1.2.2. (What is the complementary solution proposed by Li et al. (2022a)?): sent15",What are outcome-based approaches in improving chain-of-thought reasoning quality?,"sent13, sent14, sent15","A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a).

Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer.

In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""selfteaching"", as a complementary solution to produce a higher degree of diversity.","Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path and instead use multiple reasoning paths (Wang et al., 2023; Li et al., 2022a).

Self-consistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model and marginalizes out the reasoning paths by choosing the most common answer.

In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""self-teaching"" as a complementary solution to produce a higher degree of diversity.","Questions:

What are outcome-based approaches in improving chain-of-thought reasoning quality?

Answer:

Outcome-based approaches acknowledge the potential incorrectness of an individual reasoning path and instead use multiple reasoning paths (Wang et al., 2023; Li et al., 2022a).

Self-consistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model and marginalizes out the reasoning paths by choosing the most common answer.

In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through ""self-teaching"" as a complementary solution to produce a higher degree of diversity."
Towards Reasoning in Large Language Models: A Survey,What is Reasoning?,"Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018). Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information. Although ""reasoning"" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things. To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:

Deductive reasoning. Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises. In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true. For example:

• Premise: All mammals have kidneys. • Premise: All whales are mammals. • Conclusion: All whales have kidneys.

Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence. The conclusion is likely to be true based on the available evidence, but it is not necessarily certain. For example:

• Observation: Every time we see a creature with wings, it is a bird. • Observation: We see a creature with wings. • Conclusion: The creature is likely to be a bird.

Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations. The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain. For example:

• Observation: The car cannot start and there is a puddle of liquid under the engine. • Conclusion: The most likely explanation is that the car has a leak in the radiator.

Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.

Formal Reasoning vs Informal Reasoning. Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic. Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life. Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable. We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.

Reasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails. In the literature, the term ""reasoning"" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia). Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia). In this paper, we encompass various forms of reasoning, with a particular focus on ""informal deductive reasoning"" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.","1. (What is reasoning?): sent1
    1.1. (What does reasoning involve?): sent2
    1.2. (Is reasoning a commonly used term?): sent3
    1.3. (How can we better understand the concept of reasoning?): sent4
        1.3.1. (What is deductive reasoning?): sent5
            1.3.1.1. (What is the nature of deductive reasoning?): sent6
            1.3.1.2. (Can you provide an example of deductive reasoning?): sent7, sent8, sent9
        1.3.2. (What is inductive reasoning?): sent10
            1.3.2.1. (What is the nature of inductive reasoning?): sent11
            1.3.2.2. (Can you provide an example of inductive reasoning?): sent12, sent13, sent14
        1.3.3. (What is abductive reasoning?): sent15
            1.3.3.1. (What is the nature of abductive reasoning?): sent16
            1.3.3.2. (Can you provide an example of abductive reasoning?): sent17, sent18
        1.3.4. (Are there other types of reasoning?): sent19
    1.4. (What is the difference between formal and informal reasoning?): sent20
        1.4.1. (What is formal reasoning?): sent21
        1.4.2. (What is informal reasoning?): sent22
        1.4.3. (How do formal and informal reasoning compare?): sent23
        1.4.4. (Where can I find more information on the distinction between formal and informal reasoning?): sent24
2. (What is reasoning in language models?): sent25
    2.1. (How is the term ""reasoning"" used in the literature regarding language models?): sent26
    2.2. (What forms of reasoning are used in language models?): sent27
    2.3. (What is the focus of this paper regarding reasoning in language models?): sent28",What are the main categories of reasoning commonly recognized?,"sent4, sent5, sent10, sent15, sent19","To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:Deductive reasoning.

Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises.

Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence.

Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations.

Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.","Several main categories of reasoning that are commonly recognized include deductive reasoning.

Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises.

Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence.

Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations.

Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.","Questions:

What are the main categories of reasoning commonly recognized?

Answer:

Several main categories of reasoning that are commonly recognized include deductive reasoning.

Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises.

Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence.

Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations.

Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes."
Towards Reasoning in Large Language Models: A Survey,What is Reasoning?,"Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018). Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information. Although ""reasoning"" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things. To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:

Deductive reasoning. Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises. In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true. For example:

• Premise: All mammals have kidneys. • Premise: All whales are mammals. • Conclusion: All whales have kidneys.

Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence. The conclusion is likely to be true based on the available evidence, but it is not necessarily certain. For example:

• Observation: Every time we see a creature with wings, it is a bird. • Observation: We see a creature with wings. • Conclusion: The creature is likely to be a bird.

Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations. The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain. For example:

• Observation: The car cannot start and there is a puddle of liquid under the engine. • Conclusion: The most likely explanation is that the car has a leak in the radiator.

Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.

Formal Reasoning vs Informal Reasoning. Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic. Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life. Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable. We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.

Reasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails. In the literature, the term ""reasoning"" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia). Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia). In this paper, we encompass various forms of reasoning, with a particular focus on ""informal deductive reasoning"" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.","1. (What is reasoning?): sent1
    1.1. (What does reasoning involve?): sent2
    1.2. (Is reasoning a commonly used term?): sent3
    1.3. (How can we better understand the concept of reasoning?): sent4
        1.3.1. (What is deductive reasoning?): sent5
            1.3.1.1. (What is the nature of deductive reasoning?): sent6
            1.3.1.2. (Can you provide an example of deductive reasoning?): sent7, sent8, sent9
        1.3.2. (What is inductive reasoning?): sent10
            1.3.2.1. (What is the nature of inductive reasoning?): sent11
            1.3.2.2. (Can you provide an example of inductive reasoning?): sent12, sent13, sent14
        1.3.3. (What is abductive reasoning?): sent15
            1.3.3.1. (What is the nature of abductive reasoning?): sent16
            1.3.3.2. (Can you provide an example of abductive reasoning?): sent17, sent18
        1.3.4. (Are there other types of reasoning?): sent19
    1.4. (What is the difference between formal and informal reasoning?): sent20
        1.4.1. (What is formal reasoning?): sent21
        1.4.2. (What is informal reasoning?): sent22
        1.4.3. (How do formal and informal reasoning compare?): sent23
        1.4.4. (Where can I find more information on the distinction between formal and informal reasoning?): sent24
2. (What is reasoning in language models?): sent25
    2.1. (How is the term ""reasoning"" used in the literature regarding language models?): sent26
    2.2. (What forms of reasoning are used in language models?): sent27
    2.3. (What is the focus of this paper regarding reasoning in language models?): sent28",What is deductive reasoning and how does it work?,"sent5, sent6, sent7, sent8, sent9","Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises.

In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true.

For example:• Premise: All mammals have kidneys.

• Premise: All whales are mammals.

• Conclusion: All whales have kidneys.","Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises.

In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true.

For example, if the premise is that all mammals have kidneys, then the conclusion must follow that any specific mammal also has kidneys.

For instance, consider the premise: all whales are mammals.

The conclusion would be that all whales have kidneys.","Questions:

What is deductive reasoning and how does it work?

Answer:

Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises.

In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true.

For example, if the premise is that all mammals have kidneys, then the conclusion must follow that any specific mammal also has kidneys.

For instance, consider the premise: all whales are mammals.

The conclusion would be that all whales have kidneys."
Towards Reasoning in Large Language Models: A Survey,What is Reasoning?,"Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018). Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information. Although ""reasoning"" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things. To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:

Deductive reasoning. Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises. In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true. For example:

• Premise: All mammals have kidneys. • Premise: All whales are mammals. • Conclusion: All whales have kidneys.

Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence. The conclusion is likely to be true based on the available evidence, but it is not necessarily certain. For example:

• Observation: Every time we see a creature with wings, it is a bird. • Observation: We see a creature with wings. • Conclusion: The creature is likely to be a bird.

Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations. The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain. For example:

• Observation: The car cannot start and there is a puddle of liquid under the engine. • Conclusion: The most likely explanation is that the car has a leak in the radiator.

Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.

Formal Reasoning vs Informal Reasoning. Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic. Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life. Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable. We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.

Reasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails. In the literature, the term ""reasoning"" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia). Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia). In this paper, we encompass various forms of reasoning, with a particular focus on ""informal deductive reasoning"" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.","1. (What is reasoning?): sent1
    1.1. (What does reasoning involve?): sent2
    1.2. (Is reasoning a commonly used term?): sent3
    1.3. (How can we better understand the concept of reasoning?): sent4
        1.3.1. (What is deductive reasoning?): sent5
            1.3.1.1. (What is the nature of deductive reasoning?): sent6
            1.3.1.2. (Can you provide an example of deductive reasoning?): sent7, sent8, sent9
        1.3.2. (What is inductive reasoning?): sent10
            1.3.2.1. (What is the nature of inductive reasoning?): sent11
            1.3.2.2. (Can you provide an example of inductive reasoning?): sent12, sent13, sent14
        1.3.3. (What is abductive reasoning?): sent15
            1.3.3.1. (What is the nature of abductive reasoning?): sent16
            1.3.3.2. (Can you provide an example of abductive reasoning?): sent17, sent18
        1.3.4. (Are there other types of reasoning?): sent19
    1.4. (What is the difference between formal and informal reasoning?): sent20
        1.4.1. (What is formal reasoning?): sent21
        1.4.2. (What is informal reasoning?): sent22
        1.4.3. (How do formal and informal reasoning compare?): sent23
        1.4.4. (Where can I find more information on the distinction between formal and informal reasoning?): sent24
2. (What is reasoning in language models?): sent25
    2.1. (How is the term ""reasoning"" used in the literature regarding language models?): sent26
    2.2. (What forms of reasoning are used in language models?): sent27
    2.3. (What is the focus of this paper regarding reasoning in language models?): sent28",Can you provide an example of inductive reasoning?,"sent12, sent13, sent14","For example:• Observation: Every time we see a creature with wings, it is a bird.

• Observation: We see a creature with wings.

• Conclusion: The creature is likely to be a bird.","An example of inductive reasoning is the observation that every time a creature with wings is seen, it is a bird.

An observation is made when a creature with wings is seen.

The conclusion drawn is that the creature is likely to be a bird.","Questions:

Can you provide an example of inductive reasoning?

Answer:

An example of inductive reasoning is the observation that every time a creature with wings is seen, it is a bird.

An observation is made when a creature with wings is seen.

The conclusion drawn is that the creature is likely to be a bird."
Towards Reasoning in Large Language Models: A Survey,What is Reasoning?,"Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018). Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information. Although ""reasoning"" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things. To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:

Deductive reasoning. Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises. In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true. For example:

• Premise: All mammals have kidneys. • Premise: All whales are mammals. • Conclusion: All whales have kidneys.

Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence. The conclusion is likely to be true based on the available evidence, but it is not necessarily certain. For example:

• Observation: Every time we see a creature with wings, it is a bird. • Observation: We see a creature with wings. • Conclusion: The creature is likely to be a bird.

Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations. The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain. For example:

• Observation: The car cannot start and there is a puddle of liquid under the engine. • Conclusion: The most likely explanation is that the car has a leak in the radiator.

Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.

Formal Reasoning vs Informal Reasoning. Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic. Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life. Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable. We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.

Reasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails. In the literature, the term ""reasoning"" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia). Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia). In this paper, we encompass various forms of reasoning, with a particular focus on ""informal deductive reasoning"" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.","1. (What is reasoning?): sent1
    1.1. (What does reasoning involve?): sent2
    1.2. (Is reasoning a commonly used term?): sent3
    1.3. (How can we better understand the concept of reasoning?): sent4
        1.3.1. (What is deductive reasoning?): sent5
            1.3.1.1. (What is the nature of deductive reasoning?): sent6
            1.3.1.2. (Can you provide an example of deductive reasoning?): sent7, sent8, sent9
        1.3.2. (What is inductive reasoning?): sent10
            1.3.2.1. (What is the nature of inductive reasoning?): sent11
            1.3.2.2. (Can you provide an example of inductive reasoning?): sent12, sent13, sent14
        1.3.3. (What is abductive reasoning?): sent15
            1.3.3.1. (What is the nature of abductive reasoning?): sent16
            1.3.3.2. (Can you provide an example of abductive reasoning?): sent17, sent18
        1.3.4. (Are there other types of reasoning?): sent19
    1.4. (What is the difference between formal and informal reasoning?): sent20
        1.4.1. (What is formal reasoning?): sent21
        1.4.2. (What is informal reasoning?): sent22
        1.4.3. (How do formal and informal reasoning compare?): sent23
        1.4.4. (Where can I find more information on the distinction between formal and informal reasoning?): sent24
2. (What is reasoning in language models?): sent25
    2.1. (How is the term ""reasoning"" used in the literature regarding language models?): sent26
    2.2. (What forms of reasoning are used in language models?): sent27
    2.3. (What is the focus of this paper regarding reasoning in language models?): sent28",What is abductive reasoning and how does it work?,"sent15, sent16, sent17, sent18","Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations.

The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain.

For example:• Observation: The car cannot start and there is a puddle of liquid under the engine.

• Conclusion: The most likely explanation is that the car has a leak in the radiator.","Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations.

The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain.

For example, consider the observation that the car cannot start and there is a puddle of liquid under the engine.

The most likely explanation is that the car has a leak in the radiator.","Questions:

What is abductive reasoning and how does it work?

Answer:

Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations.

The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain.

For example, consider the observation that the car cannot start and there is a puddle of liquid under the engine.

The most likely explanation is that the car has a leak in the radiator."
Towards Reasoning in Large Language Models: A Survey,What is Reasoning?,"Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018). Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information. Although ""reasoning"" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things. To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:

Deductive reasoning. Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises. In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true. For example:

• Premise: All mammals have kidneys. • Premise: All whales are mammals. • Conclusion: All whales have kidneys.

Inductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence. The conclusion is likely to be true based on the available evidence, but it is not necessarily certain. For example:

• Observation: Every time we see a creature with wings, it is a bird. • Observation: We see a creature with wings. • Conclusion: The creature is likely to be a bird.

Abductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations. The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain. For example:

• Observation: The car cannot start and there is a puddle of liquid under the engine. • Conclusion: The most likely explanation is that the car has a leak in the radiator.

Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.

Formal Reasoning vs Informal Reasoning. Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic. Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life. Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable. We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.

Reasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails. In the literature, the term ""reasoning"" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia). Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia). In this paper, we encompass various forms of reasoning, with a particular focus on ""informal deductive reasoning"" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.","1. (What is reasoning?): sent1
    1.1. (What does reasoning involve?): sent2
    1.2. (Is reasoning a commonly used term?): sent3
    1.3. (How can we better understand the concept of reasoning?): sent4
        1.3.1. (What is deductive reasoning?): sent5
            1.3.1.1. (What is the nature of deductive reasoning?): sent6
            1.3.1.2. (Can you provide an example of deductive reasoning?): sent7, sent8, sent9
        1.3.2. (What is inductive reasoning?): sent10
            1.3.2.1. (What is the nature of inductive reasoning?): sent11
            1.3.2.2. (Can you provide an example of inductive reasoning?): sent12, sent13, sent14
        1.3.3. (What is abductive reasoning?): sent15
            1.3.3.1. (What is the nature of abductive reasoning?): sent16
            1.3.3.2. (Can you provide an example of abductive reasoning?): sent17, sent18
        1.3.4. (Are there other types of reasoning?): sent19
    1.4. (What is the difference between formal and informal reasoning?): sent20
        1.4.1. (What is formal reasoning?): sent21
        1.4.2. (What is informal reasoning?): sent22
        1.4.3. (How do formal and informal reasoning compare?): sent23
        1.4.4. (Where can I find more information on the distinction between formal and informal reasoning?): sent24
2. (What is reasoning in language models?): sent25
    2.1. (How is the term ""reasoning"" used in the literature regarding language models?): sent26
    2.2. (What forms of reasoning are used in language models?): sent27
    2.3. (What is the focus of this paper regarding reasoning in language models?): sent28",What are the differences between formal and informal reasoning?,"sent20, sent21, sent22, sent23","Formal Reasoning vs Informal Reasoning.

Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic.

Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life.

Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable.","Formal reasoning involves structured, rule-based processes, whereas informal reasoning relies on more flexible, context-dependent approaches.

Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic.

Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life.

Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable.","Questions:

What are the differences between formal and informal reasoning?

Answer:

Formal reasoning involves structured, rule-based processes, whereas informal reasoning relies on more flexible, context-dependent approaches.

Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic.

Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life.

Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable."
Towards Reasoning in Large Language Models: A Survey,Findings and Implications,"In this section, we summarize the important findings and implications of studies on reasoning in large language models.

Reasoning seems an emergent ability of LLMs. Wei et al. (2022a,b);  show that reasoning ability appears to emerge only in large language models like GPT-3 175B, as evidenced by significant improvements in performance on reasoning tasks at a certain scale (e.g., 100 billion parameters). This suggests that it may be more effective to utilize large models for general reasoning problems rather than training small models for specific tasks. However, the reason for this emergent ability is not yet fully understood. We refer the reader to Wei et al. (2022a);Fu et al. (2022a) for some potential explanations.

Chain of thought elicits ""reasoning"" of LLMs. The use of chain-of-thought (CoT) prompts (Wei et al., 2022b) has been shown to improve the performance of LLMs on various reasoning tasks, as demonstrated in the experiments of Wei et al. (2022a,b); . Additionally, Saparov and He (2022) ( §4.2) find that, when using CoT prompts, LLMs are able to produce valid individual proof steps, even when the synthetic ontology is fictional or counterfactual. However, they may sometimes choose the wrong steps when multiple options are available, leading to incomplete or incorrect proofs. Moreover, for many reasoning tasks where the performance of standard prompting grows smoothly with model scale, chain-of-thought prompting can lead to dramatic performance improvement. In addition to these benefits, the use of CoT prompts has been shown to improve the out-ofdistribution robustness of LLMs (Wei et al., 2022b;Zhou et al., 2022a;Anil et al., 2022, inter alia), an advantage that is not typically observed with standard prompting or fully supervised finetuning paradigms.

LLMs show human-like content effects on reasoning. According to Dasgupta et al. (2022), LLMs exhibit reasoning patterns that are similar to those of humans as described in the cognitive literature. For example, the models' predictions are influenced by both prior knowledge and abstract reasoning, and their judgments of logical validity are impacted by the believability of the conclusions. These findings suggest that, although language models may not always perform well on reasoning tasks, their failures often occur in situations that are challenging for humans as well. This provides some evidence that language models may ""reason"" in a way that is similar to human reasoning.

LLMs are still unskilled at complex reasoning. Although LLMs seem to possess impressive reasoning capabilities with the techniques described in §3, they still struggle with more complex reasoning tasks or those involving implicature, according to studies such as Valmeekam et al. (2022);Han et al. (2022a); Ruis et al. (2022). For instance, Valmeekam et al. (2022) find that even in relatively simple commonsense planning domains that humans would have no trouble navigating, LLMs such as GPT-3 (Brown et al., 2020) and BLOOM (Scao et al., 2022) struggle to perform effectively. These findings suggest that existing benchmarks may be too simple to accurately gauge the true reasoning abilities of LLMs, and that more challenging tasks may be needed to fully evaluate their abilities in this regard.","# QUA Analysis:

1. (What is the purpose of this section?): sent1
    1.1. (What are the important findings about reasoning in LLMs?): sent2
        1.1.1. (What evidence supports the emergence of reasoning in LLMs?): sent3
        1.1.2. (What is the implication of reasoning emerging in large models?): sent4
        1.1.3. (Is the reason for the emergent ability understood?): sent5
            1.1.3.1. (Where can one find potential explanations for the emergent ability?): sent6
    1.2. (What technique elicits reasoning in LLMs?): sent7
        1.2.1. (How does chain-of-thought (CoT) prompting affect LLM performance?): sent8
        1.2.2. (What do Saparov and He (2022) find about CoT prompts?): sent9
        1.2.3. (What is a limitation of CoT prompts?): sent10
        1.2.4. (What is another benefit of CoT prompts?): sent11
        1.2.5. (How does CoT prompting compare to other paradigms?): sent12
    1.3. (Do LLMs show human-like reasoning patterns?): sent13
        1.3.1. (What evidence supports human-like reasoning in LLMs?): sent14
        1.3.2. (How do LLMs' predictions and judgments compare to humans?): sent15
        1.3.3. (What do these findings suggest about LLMs' reasoning?): sent16
            1.3.3.1. (What evidence supports the similarity between LLM and human reasoning?): sent17
    1.4. (Are LLMs skilled at complex reasoning tasks?): sent18
        1.4.1. (What do studies show about LLMs' performance on complex reasoning tasks?): sent19
        1.4.2. (What is an example of LLMs struggling with reasoning tasks?): sent20
        1.4.3. (What do these findings suggest about existing benchmarks?): sent21",What evidence supports the emergence of reasoning abilities in large language models?,"sent2, sent3, sent4, sent5","Reasoning seems an emergent ability of LLMs.

Wei et al. (2022a,b);  show that reasoning ability appears to emerge only in large language models like GPT-3 175B, as evidenced by significant improvements in performance on reasoning tasks at a certain scale (e.g., 100 billion parameters).

This suggests that it may be more effective to utilize large models for general reasoning problems rather than training small models for specific tasks.

However, the reason for this emergent ability is not yet fully understood.","Reasoning appears to be an emergent ability of large language models.

Wei et al. (2022a,b) show that reasoning ability appears to emerge only in large language models like GPT-3 175B, as evidenced by significant improvements in performance on reasoning tasks at a certain scale (e.g., 100 billion parameters).

This suggests that it may be more effective to utilize large models for general reasoning problems rather than training small models for specific tasks.

However, the reason for this emergent ability is not yet fully understood.","Questions:

What evidence supports the emergence of reasoning abilities in large language models?

Answer:

Reasoning appears to be an emergent ability of large language models.

Wei et al. (2022a,b) show that reasoning ability appears to emerge only in large language models like GPT-3 175B, as evidenced by significant improvements in performance on reasoning tasks at a certain scale (e.g., 100 billion parameters).

This suggests that it may be more effective to utilize large models for general reasoning problems rather than training small models for specific tasks.

However, the reason for this emergent ability is not yet fully understood."
Towards Reasoning in Large Language Models: A Survey,Findings and Implications,"In this section, we summarize the important findings and implications of studies on reasoning in large language models.

Reasoning seems an emergent ability of LLMs. Wei et al. (2022a,b);  show that reasoning ability appears to emerge only in large language models like GPT-3 175B, as evidenced by significant improvements in performance on reasoning tasks at a certain scale (e.g., 100 billion parameters). This suggests that it may be more effective to utilize large models for general reasoning problems rather than training small models for specific tasks. However, the reason for this emergent ability is not yet fully understood. We refer the reader to Wei et al. (2022a);Fu et al. (2022a) for some potential explanations.

Chain of thought elicits ""reasoning"" of LLMs. The use of chain-of-thought (CoT) prompts (Wei et al., 2022b) has been shown to improve the performance of LLMs on various reasoning tasks, as demonstrated in the experiments of Wei et al. (2022a,b); . Additionally, Saparov and He (2022) ( §4.2) find that, when using CoT prompts, LLMs are able to produce valid individual proof steps, even when the synthetic ontology is fictional or counterfactual. However, they may sometimes choose the wrong steps when multiple options are available, leading to incomplete or incorrect proofs. Moreover, for many reasoning tasks where the performance of standard prompting grows smoothly with model scale, chain-of-thought prompting can lead to dramatic performance improvement. In addition to these benefits, the use of CoT prompts has been shown to improve the out-ofdistribution robustness of LLMs (Wei et al., 2022b;Zhou et al., 2022a;Anil et al., 2022, inter alia), an advantage that is not typically observed with standard prompting or fully supervised finetuning paradigms.

LLMs show human-like content effects on reasoning. According to Dasgupta et al. (2022), LLMs exhibit reasoning patterns that are similar to those of humans as described in the cognitive literature. For example, the models' predictions are influenced by both prior knowledge and abstract reasoning, and their judgments of logical validity are impacted by the believability of the conclusions. These findings suggest that, although language models may not always perform well on reasoning tasks, their failures often occur in situations that are challenging for humans as well. This provides some evidence that language models may ""reason"" in a way that is similar to human reasoning.

LLMs are still unskilled at complex reasoning. Although LLMs seem to possess impressive reasoning capabilities with the techniques described in §3, they still struggle with more complex reasoning tasks or those involving implicature, according to studies such as Valmeekam et al. (2022);Han et al. (2022a); Ruis et al. (2022). For instance, Valmeekam et al. (2022) find that even in relatively simple commonsense planning domains that humans would have no trouble navigating, LLMs such as GPT-3 (Brown et al., 2020) and BLOOM (Scao et al., 2022) struggle to perform effectively. These findings suggest that existing benchmarks may be too simple to accurately gauge the true reasoning abilities of LLMs, and that more challenging tasks may be needed to fully evaluate their abilities in this regard.","# QUA Analysis:

1. (What is the purpose of this section?): sent1
    1.1. (What are the important findings about reasoning in LLMs?): sent2
        1.1.1. (What evidence supports the emergence of reasoning in LLMs?): sent3
        1.1.2. (What is the implication of reasoning emerging in large models?): sent4
        1.1.3. (Is the reason for the emergent ability understood?): sent5
            1.1.3.1. (Where can one find potential explanations for the emergent ability?): sent6
    1.2. (What technique elicits reasoning in LLMs?): sent7
        1.2.1. (How does chain-of-thought (CoT) prompting affect LLM performance?): sent8
        1.2.2. (What do Saparov and He (2022) find about CoT prompts?): sent9
        1.2.3. (What is a limitation of CoT prompts?): sent10
        1.2.4. (What is another benefit of CoT prompts?): sent11
        1.2.5. (How does CoT prompting compare to other paradigms?): sent12
    1.3. (Do LLMs show human-like reasoning patterns?): sent13
        1.3.1. (What evidence supports human-like reasoning in LLMs?): sent14
        1.3.2. (How do LLMs' predictions and judgments compare to humans?): sent15
        1.3.3. (What do these findings suggest about LLMs' reasoning?): sent16
            1.3.3.1. (What evidence supports the similarity between LLM and human reasoning?): sent17
    1.4. (Are LLMs skilled at complex reasoning tasks?): sent18
        1.4.1. (What do studies show about LLMs' performance on complex reasoning tasks?): sent19
        1.4.2. (What is an example of LLMs struggling with reasoning tasks?): sent20
        1.4.3. (What do these findings suggest about existing benchmarks?): sent21",How does chain-of-thought (CoT) prompting affect the performance of large language models on reasoning tasks?,"sent7, sent8, sent9, sent10, sent11, sent12","Chain of thought elicits ""reasoning"" of LLMs.

The use of chain-of-thought (CoT) prompts (Wei et al., 2022b) has been shown to improve the performance of LLMs on various reasoning tasks, as demonstrated in the experiments of Wei et al. (2022a,b); .

Additionally, Saparov and He (2022) ( §4.2) find that, when using CoT prompts, LLMs are able to produce valid individual proof steps, even when the synthetic ontology is fictional or counterfactual.

However, they may sometimes choose the wrong steps when multiple options are available, leading to incomplete or incorrect proofs.

Moreover, for many reasoning tasks where the performance of standard prompting grows smoothly with model scale, chain-of-thought prompting can lead to dramatic performance improvement.

In addition to these benefits, the use of CoT prompts has been shown to improve the out-ofdistribution robustness of LLMs (Wei et al., 2022b;Zhou et al., 2022a;Anil et al., 2022, inter alia), an advantage that is not typically observed with standard prompting or fully supervised finetuning paradigms.","Chain of thought prompting elicits ""reasoning"" in large language models.

The use of chain-of-thought (CoT) prompts (Wei et al., 2022b) has been shown to improve the performance of LLMs on various reasoning tasks, as demonstrated in the experiments of Wei et al. (2022a,b).

Additionally, Saparov and He (2022) find that, when using CoT prompts, LLMs are able to produce valid individual proof steps, even when the synthetic ontology is fictional or counterfactual.

However, they may sometimes choose the wrong steps when multiple options are available, leading to incomplete or incorrect proofs.

Moreover, for many reasoning tasks where the performance of standard prompting grows smoothly with model scale, chain-of-thought prompting can lead to dramatic performance improvement.

In addition to these benefits, the use of CoT prompts has been shown to improve the out-of-distribution robustness of LLMs (Wei et al., 2022b; Zhou et al., 2022a; Anil et al., 2022), an advantage that is not typically observed with standard prompting or fully supervised finetuning paradigms.","Questions:

How does chain-of-thought (CoT) prompting affect the performance of large language models on reasoning tasks?

Answer:

Chain of thought prompting elicits ""reasoning"" in large language models.

The use of chain-of-thought (CoT) prompts (Wei et al., 2022b) has been shown to improve the performance of LLMs on various reasoning tasks, as demonstrated in the experiments of Wei et al. (2022a,b).

Additionally, Saparov and He (2022) find that, when using CoT prompts, LLMs are able to produce valid individual proof steps, even when the synthetic ontology is fictional or counterfactual.

However, they may sometimes choose the wrong steps when multiple options are available, leading to incomplete or incorrect proofs.

Moreover, for many reasoning tasks where the performance of standard prompting grows smoothly with model scale, chain-of-thought prompting can lead to dramatic performance improvement.

In addition to these benefits, the use of CoT prompts has been shown to improve the out-of-distribution robustness of LLMs (Wei et al., 2022b; Zhou et al., 2022a; Anil et al., 2022), an advantage that is not typically observed with standard prompting or fully supervised finetuning paradigms."
Towards Reasoning in Large Language Models: A Survey,Findings and Implications,"In this section, we summarize the important findings and implications of studies on reasoning in large language models.

Reasoning seems an emergent ability of LLMs. Wei et al. (2022a,b);  show that reasoning ability appears to emerge only in large language models like GPT-3 175B, as evidenced by significant improvements in performance on reasoning tasks at a certain scale (e.g., 100 billion parameters). This suggests that it may be more effective to utilize large models for general reasoning problems rather than training small models for specific tasks. However, the reason for this emergent ability is not yet fully understood. We refer the reader to Wei et al. (2022a);Fu et al. (2022a) for some potential explanations.

Chain of thought elicits ""reasoning"" of LLMs. The use of chain-of-thought (CoT) prompts (Wei et al., 2022b) has been shown to improve the performance of LLMs on various reasoning tasks, as demonstrated in the experiments of Wei et al. (2022a,b); . Additionally, Saparov and He (2022) ( §4.2) find that, when using CoT prompts, LLMs are able to produce valid individual proof steps, even when the synthetic ontology is fictional or counterfactual. However, they may sometimes choose the wrong steps when multiple options are available, leading to incomplete or incorrect proofs. Moreover, for many reasoning tasks where the performance of standard prompting grows smoothly with model scale, chain-of-thought prompting can lead to dramatic performance improvement. In addition to these benefits, the use of CoT prompts has been shown to improve the out-ofdistribution robustness of LLMs (Wei et al., 2022b;Zhou et al., 2022a;Anil et al., 2022, inter alia), an advantage that is not typically observed with standard prompting or fully supervised finetuning paradigms.

LLMs show human-like content effects on reasoning. According to Dasgupta et al. (2022), LLMs exhibit reasoning patterns that are similar to those of humans as described in the cognitive literature. For example, the models' predictions are influenced by both prior knowledge and abstract reasoning, and their judgments of logical validity are impacted by the believability of the conclusions. These findings suggest that, although language models may not always perform well on reasoning tasks, their failures often occur in situations that are challenging for humans as well. This provides some evidence that language models may ""reason"" in a way that is similar to human reasoning.

LLMs are still unskilled at complex reasoning. Although LLMs seem to possess impressive reasoning capabilities with the techniques described in §3, they still struggle with more complex reasoning tasks or those involving implicature, according to studies such as Valmeekam et al. (2022);Han et al. (2022a); Ruis et al. (2022). For instance, Valmeekam et al. (2022) find that even in relatively simple commonsense planning domains that humans would have no trouble navigating, LLMs such as GPT-3 (Brown et al., 2020) and BLOOM (Scao et al., 2022) struggle to perform effectively. These findings suggest that existing benchmarks may be too simple to accurately gauge the true reasoning abilities of LLMs, and that more challenging tasks may be needed to fully evaluate their abilities in this regard.","# QUA Analysis:

1. (What is the purpose of this section?): sent1
    1.1. (What are the important findings about reasoning in LLMs?): sent2
        1.1.1. (What evidence supports the emergence of reasoning in LLMs?): sent3
        1.1.2. (What is the implication of reasoning emerging in large models?): sent4
        1.1.3. (Is the reason for the emergent ability understood?): sent5
            1.1.3.1. (Where can one find potential explanations for the emergent ability?): sent6
    1.2. (What technique elicits reasoning in LLMs?): sent7
        1.2.1. (How does chain-of-thought (CoT) prompting affect LLM performance?): sent8
        1.2.2. (What do Saparov and He (2022) find about CoT prompts?): sent9
        1.2.3. (What is a limitation of CoT prompts?): sent10
        1.2.4. (What is another benefit of CoT prompts?): sent11
        1.2.5. (How does CoT prompting compare to other paradigms?): sent12
    1.3. (Do LLMs show human-like reasoning patterns?): sent13
        1.3.1. (What evidence supports human-like reasoning in LLMs?): sent14
        1.3.2. (How do LLMs' predictions and judgments compare to humans?): sent15
        1.3.3. (What do these findings suggest about LLMs' reasoning?): sent16
            1.3.3.1. (What evidence supports the similarity between LLM and human reasoning?): sent17
    1.4. (Are LLMs skilled at complex reasoning tasks?): sent18
        1.4.1. (What do studies show about LLMs' performance on complex reasoning tasks?): sent19
        1.4.2. (What is an example of LLMs struggling with reasoning tasks?): sent20
        1.4.3. (What do these findings suggest about existing benchmarks?): sent21",What evidence suggests that large language models exhibit human-like reasoning patterns?,"sent13, sent14, sent15, sent16, sent17","LLMs show human-like content effects on reasoning.

According to Dasgupta et al. (2022), LLMs exhibit reasoning patterns that are similar to those of humans as described in the cognitive literature.

For example, the models' predictions are influenced by both prior knowledge and abstract reasoning, and their judgments of logical validity are impacted by the believability of the conclusions.

These findings suggest that, although language models may not always perform well on reasoning tasks, their failures often occur in situations that are challenging for humans as well.

This provides some evidence that language models may ""reason"" in a way that is similar to human reasoning.","Large language models (LLMs) show human-like content effects on reasoning.

According to Dasgupta et al. (2022), LLMs exhibit reasoning patterns that are similar to those of humans as described in the cognitive literature.

For example, the models' predictions are influenced by both prior knowledge and abstract reasoning, and their judgments of logical validity are impacted by the believability of the conclusions.

These findings suggest that, although language models may not always perform well on reasoning tasks, their failures often occur in situations that are challenging for humans as well.

This provides some evidence that language models may ""reason"" in a way that is similar to human reasoning.","Questions:

What evidence suggests that large language models exhibit human-like reasoning patterns?

Answer:

Large language models (LLMs) show human-like content effects on reasoning.

According to Dasgupta et al. (2022), LLMs exhibit reasoning patterns that are similar to those of humans as described in the cognitive literature.

For example, the models' predictions are influenced by both prior knowledge and abstract reasoning, and their judgments of logical validity are impacted by the believability of the conclusions.

These findings suggest that, although language models may not always perform well on reasoning tasks, their failures often occur in situations that are challenging for humans as well.

This provides some evidence that language models may ""reason"" in a way that is similar to human reasoning."
Towards Reasoning in Large Language Models: A Survey,Findings and Implications,"In this section, we summarize the important findings and implications of studies on reasoning in large language models.

Reasoning seems an emergent ability of LLMs. Wei et al. (2022a,b);  show that reasoning ability appears to emerge only in large language models like GPT-3 175B, as evidenced by significant improvements in performance on reasoning tasks at a certain scale (e.g., 100 billion parameters). This suggests that it may be more effective to utilize large models for general reasoning problems rather than training small models for specific tasks. However, the reason for this emergent ability is not yet fully understood. We refer the reader to Wei et al. (2022a);Fu et al. (2022a) for some potential explanations.

Chain of thought elicits ""reasoning"" of LLMs. The use of chain-of-thought (CoT) prompts (Wei et al., 2022b) has been shown to improve the performance of LLMs on various reasoning tasks, as demonstrated in the experiments of Wei et al. (2022a,b); . Additionally, Saparov and He (2022) ( §4.2) find that, when using CoT prompts, LLMs are able to produce valid individual proof steps, even when the synthetic ontology is fictional or counterfactual. However, they may sometimes choose the wrong steps when multiple options are available, leading to incomplete or incorrect proofs. Moreover, for many reasoning tasks where the performance of standard prompting grows smoothly with model scale, chain-of-thought prompting can lead to dramatic performance improvement. In addition to these benefits, the use of CoT prompts has been shown to improve the out-ofdistribution robustness of LLMs (Wei et al., 2022b;Zhou et al., 2022a;Anil et al., 2022, inter alia), an advantage that is not typically observed with standard prompting or fully supervised finetuning paradigms.

LLMs show human-like content effects on reasoning. According to Dasgupta et al. (2022), LLMs exhibit reasoning patterns that are similar to those of humans as described in the cognitive literature. For example, the models' predictions are influenced by both prior knowledge and abstract reasoning, and their judgments of logical validity are impacted by the believability of the conclusions. These findings suggest that, although language models may not always perform well on reasoning tasks, their failures often occur in situations that are challenging for humans as well. This provides some evidence that language models may ""reason"" in a way that is similar to human reasoning.

LLMs are still unskilled at complex reasoning. Although LLMs seem to possess impressive reasoning capabilities with the techniques described in §3, they still struggle with more complex reasoning tasks or those involving implicature, according to studies such as Valmeekam et al. (2022);Han et al. (2022a); Ruis et al. (2022). For instance, Valmeekam et al. (2022) find that even in relatively simple commonsense planning domains that humans would have no trouble navigating, LLMs such as GPT-3 (Brown et al., 2020) and BLOOM (Scao et al., 2022) struggle to perform effectively. These findings suggest that existing benchmarks may be too simple to accurately gauge the true reasoning abilities of LLMs, and that more challenging tasks may be needed to fully evaluate their abilities in this regard.","# QUA Analysis:

1. (What is the purpose of this section?): sent1
    1.1. (What are the important findings about reasoning in LLMs?): sent2
        1.1.1. (What evidence supports the emergence of reasoning in LLMs?): sent3
        1.1.2. (What is the implication of reasoning emerging in large models?): sent4
        1.1.3. (Is the reason for the emergent ability understood?): sent5
            1.1.3.1. (Where can one find potential explanations for the emergent ability?): sent6
    1.2. (What technique elicits reasoning in LLMs?): sent7
        1.2.1. (How does chain-of-thought (CoT) prompting affect LLM performance?): sent8
        1.2.2. (What do Saparov and He (2022) find about CoT prompts?): sent9
        1.2.3. (What is a limitation of CoT prompts?): sent10
        1.2.4. (What is another benefit of CoT prompts?): sent11
        1.2.5. (How does CoT prompting compare to other paradigms?): sent12
    1.3. (Do LLMs show human-like reasoning patterns?): sent13
        1.3.1. (What evidence supports human-like reasoning in LLMs?): sent14
        1.3.2. (How do LLMs' predictions and judgments compare to humans?): sent15
        1.3.3. (What do these findings suggest about LLMs' reasoning?): sent16
            1.3.3.1. (What evidence supports the similarity between LLM and human reasoning?): sent17
    1.4. (Are LLMs skilled at complex reasoning tasks?): sent18
        1.4.1. (What do studies show about LLMs' performance on complex reasoning tasks?): sent19
        1.4.2. (What is an example of LLMs struggling with reasoning tasks?): sent20
        1.4.3. (What do these findings suggest about existing benchmarks?): sent21",What are the limitations of large language models in performing complex reasoning tasks?,"sent18, sent19, sent20, sent21","LLMs are still unskilled at complex reasoning.

Although LLMs seem to possess impressive reasoning capabilities with the techniques described in §3, they still struggle with more complex reasoning tasks or those involving implicature, according to studies such as Valmeekam et al. (2022);Han et al. (2022a); Ruis et al. (2022).

For instance, Valmeekam et al. (2022) find that even in relatively simple commonsense planning domains that humans would have no trouble navigating, LLMs such as GPT-3 (Brown et al., 2020) and BLOOM (Scao et al., 2022) struggle to perform effectively.

These findings suggest that existing benchmarks may be too simple to accurately gauge the true reasoning abilities of LLMs, and that more challenging tasks may be needed to fully evaluate their abilities in this regard.","Large language models (LLMs) are still unskilled at complex reasoning.

Although LLMs seem to possess impressive reasoning capabilities, they still struggle with more complex reasoning tasks or those involving implicature, according to studies such as Valmeekam et al. (2022), Han et al. (2022a), and Ruis et al. (2022).

For instance, Valmeekam et al. (2022) find that even in relatively simple commonsense planning domains that humans would have no trouble navigating, LLMs such as GPT-3 (Brown et al., 2020) and BLOOM (Scao et al., 2022) struggle to perform effectively.

These findings suggest that existing benchmarks may be too simple to accurately gauge the true reasoning abilities of LLMs, and that more challenging tasks may be needed to fully evaluate their abilities in this regard.","Questions:

What are the limitations of large language models in performing complex reasoning tasks?

Answer:

Large language models (LLMs) are still unskilled at complex reasoning.

Although LLMs seem to possess impressive reasoning capabilities, they still struggle with more complex reasoning tasks or those involving implicature, according to studies such as Valmeekam et al. (2022), Han et al. (2022a), and Ruis et al. (2022).

For instance, Valmeekam et al. (2022) find that even in relatively simple commonsense planning domains that humans would have no trouble navigating, LLMs such as GPT-3 (Brown et al., 2020) and BLOOM (Scao et al., 2022) struggle to perform effectively.

These findings suggest that existing benchmarks may be too simple to accurately gauge the true reasoning abilities of LLMs, and that more challenging tasks may be needed to fully evaluate their abilities in this regard."
Towards Reasoning in Large Language Models: A Survey,End Task Performance,"One way to measure reasoning abilities of LLMs is to report their performance, e.g., accuracy, on end tasks that require reasoning. We list some common benchmarks as follows.

Arithmetic Reasoning. Arithmetic reasoning is the ability to understand and apply mathematical concepts and principles in order to solve problems involving arithmetic operations. This involves using logical thinking and mathematical principles to determine the correct course of action when solving mathematical problems. Representative benchmarks for arithmetic reasoning include GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), AS-Div (Miao et al., 2020), AQuA (Ling et al., 2017), and MAWPS (Roy and Roth, 2015). It is worth mentioning that Anil et al. (2022)  Others. In practice, there are many benchmarks that can be used to evaluate reasoning abilities of LLMs (indirectly), as long as the downstream task involves reasoning. BIG-bench (Srivastava et al., 2022), for example, includes over 200 tasks that test a range of reasoning skills, including tasks like Date Understanding, Word Sorting, and Causal Judgement. Other benchmarks, such as SCAN (Lake and Baroni, 2018) and the one proposed by Anil et al. (2022), focus on evaluating generalization ability. LLMs can also be tested on their table reasoning abilities using benchmarks such as WikiTableQA (Pasupat and Liang, 2015), FetaQA (Nan et al., 2022), as suggested by Chen (2022). In addition, there are benchmarks for evaluating LLMs' generative relational reasoning abilities, such as CommonGen (Lin et al., 2020;Liu et al., 2022a) and Open Relation Modeling (Huang et al., 2022b,d).","1. How can the reasoning abilities of LLMs be measured?
    1.1. What is one way to measure reasoning abilities of LLMs?
        - sent1: One way to measure reasoning abilities of LLMs is to report their performance, e.g., accuracy, on end tasks that require reasoning.
    1.2. What are some common benchmarks for measuring reasoning abilities of LLMs?
        - sent2: We list some common benchmarks as follows.
        1.2.1. What is arithmetic reasoning?
            - sent3: Arithmetic Reasoning. Arithmetic reasoning is the ability to understand and apply mathematical concepts and principles in order to solve problems involving arithmetic operations.
            - sent4: This involves using logical thinking and mathematical principles to determine the correct course of action when solving mathematical problems.
        1.2.2. What are some representative benchmarks for arithmetic reasoning?
            - sent5: Representative benchmarks for arithmetic reasoning include GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), AS-Div (Miao et al., 2020), AQuA (Ling et al., 2017), and MAWPS (Roy and Roth, 2015).
        1.2.3. What are other benchmarks for evaluating reasoning abilities of LLMs?
            - sent7: In practice, there are many benchmarks that can be used to evaluate reasoning abilities of LLMs (indirectly), as long as the downstream task involves reasoning.
            1.2.3.1. What is an example of a benchmark that includes a range of reasoning skills?
                - sent8: BIG-bench (Srivastava et al., 2022), for example, includes over 200 tasks that test a range of reasoning skills, including tasks like Date Understanding, Word Sorting, and Causal Judgement.
            1.2.3.2. What are some benchmarks that focus on evaluating generalization ability?
                - sent9: Other benchmarks, such as SCAN (Lake and Baroni, 2018) and the one proposed by Anil et al. (2022), focus on evaluating generalization ability.
            1.2.3.3. What benchmarks can be used to test table reasoning abilities of LLMs?
                - sent10: LLMs can also be tested on their table reasoning abilities using benchmarks such as WikiTableQA (Pasupat and Liang, 2015), FetaQA (Nan et al., 2022), as suggested by Chen (2022).
            1.2.3.4. What benchmarks are available for evaluating generative relational reasoning abilities of LLMs?
                - sent11: In addition, there are benchmarks for evaluating LLMs' generative relational reasoning abilities, such as CommonGen (Lin et al., 2020; Liu et al., 2022a) and Open Relation Modeling (Huang et al., 2022b,d).",What is arithmetic reasoning and how is it evaluated in LLMs?,"sent3, sent4, sent5","Arithmetic Reasoning. Arithmetic reasoning is the ability to understand and apply mathematical concepts and principles in order to solve problems involving arithmetic operations.

This involves using logical thinking and mathematical principles to determine the correct course of action when solving mathematical problems.

Representative benchmarks for arithmetic reasoning include GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), AS-Div (Miao et al., 2020), AQuA (Ling et al., 2017), and MAWPS (Roy and Roth, 2015).","Arithmetic reasoning is the ability to understand and apply mathematical concepts and principles to solve problems involving arithmetic operations.

This involves using logical thinking and mathematical principles to determine the correct course of action when solving mathematical problems.

Representative benchmarks for arithmetic reasoning include GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), AS-Div (Miao et al., 2020), AQuA (Ling et al., 2017), and MAWPS (Roy and Roth, 2015).","Questions:

What is arithmetic reasoning and how is it evaluated in LLMs?

Answer:

Arithmetic reasoning is the ability to understand and apply mathematical concepts and principles to solve problems involving arithmetic operations.

This involves using logical thinking and mathematical principles to determine the correct course of action when solving mathematical problems.

Representative benchmarks for arithmetic reasoning include GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), AS-Div (Miao et al., 2020), AQuA (Ling et al., 2017), and MAWPS (Roy and Roth, 2015)."
Towards Reasoning in Large Language Models: A Survey,End Task Performance,"One way to measure reasoning abilities of LLMs is to report their performance, e.g., accuracy, on end tasks that require reasoning. We list some common benchmarks as follows.

Arithmetic Reasoning. Arithmetic reasoning is the ability to understand and apply mathematical concepts and principles in order to solve problems involving arithmetic operations. This involves using logical thinking and mathematical principles to determine the correct course of action when solving mathematical problems. Representative benchmarks for arithmetic reasoning include GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), AS-Div (Miao et al., 2020), AQuA (Ling et al., 2017), and MAWPS (Roy and Roth, 2015). It is worth mentioning that Anil et al. (2022)  Others. In practice, there are many benchmarks that can be used to evaluate reasoning abilities of LLMs (indirectly), as long as the downstream task involves reasoning. BIG-bench (Srivastava et al., 2022), for example, includes over 200 tasks that test a range of reasoning skills, including tasks like Date Understanding, Word Sorting, and Causal Judgement. Other benchmarks, such as SCAN (Lake and Baroni, 2018) and the one proposed by Anil et al. (2022), focus on evaluating generalization ability. LLMs can also be tested on their table reasoning abilities using benchmarks such as WikiTableQA (Pasupat and Liang, 2015), FetaQA (Nan et al., 2022), as suggested by Chen (2022). In addition, there are benchmarks for evaluating LLMs' generative relational reasoning abilities, such as CommonGen (Lin et al., 2020;Liu et al., 2022a) and Open Relation Modeling (Huang et al., 2022b,d).","1. How can the reasoning abilities of LLMs be measured?
    1.1. What is one way to measure reasoning abilities of LLMs?
        - sent1: One way to measure reasoning abilities of LLMs is to report their performance, e.g., accuracy, on end tasks that require reasoning.
    1.2. What are some common benchmarks for measuring reasoning abilities of LLMs?
        - sent2: We list some common benchmarks as follows.
        1.2.1. What is arithmetic reasoning?
            - sent3: Arithmetic Reasoning. Arithmetic reasoning is the ability to understand and apply mathematical concepts and principles in order to solve problems involving arithmetic operations.
            - sent4: This involves using logical thinking and mathematical principles to determine the correct course of action when solving mathematical problems.
        1.2.2. What are some representative benchmarks for arithmetic reasoning?
            - sent5: Representative benchmarks for arithmetic reasoning include GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), AS-Div (Miao et al., 2020), AQuA (Ling et al., 2017), and MAWPS (Roy and Roth, 2015).
        1.2.3. What are other benchmarks for evaluating reasoning abilities of LLMs?
            - sent7: In practice, there are many benchmarks that can be used to evaluate reasoning abilities of LLMs (indirectly), as long as the downstream task involves reasoning.
            1.2.3.1. What is an example of a benchmark that includes a range of reasoning skills?
                - sent8: BIG-bench (Srivastava et al., 2022), for example, includes over 200 tasks that test a range of reasoning skills, including tasks like Date Understanding, Word Sorting, and Causal Judgement.
            1.2.3.2. What are some benchmarks that focus on evaluating generalization ability?
                - sent9: Other benchmarks, such as SCAN (Lake and Baroni, 2018) and the one proposed by Anil et al. (2022), focus on evaluating generalization ability.
            1.2.3.3. What benchmarks can be used to test table reasoning abilities of LLMs?
                - sent10: LLMs can also be tested on their table reasoning abilities using benchmarks such as WikiTableQA (Pasupat and Liang, 2015), FetaQA (Nan et al., 2022), as suggested by Chen (2022).
            1.2.3.4. What benchmarks are available for evaluating generative relational reasoning abilities of LLMs?
                - sent11: In addition, there are benchmarks for evaluating LLMs' generative relational reasoning abilities, such as CommonGen (Lin et al., 2020; Liu et al., 2022a) and Open Relation Modeling (Huang et al., 2022b,d).",What are some benchmarks used to evaluate the reasoning abilities of LLMs?,"sent7, sent8, sent9, sent10, sent11","In practice, there are many benchmarks that can be used to evaluate reasoning abilities of LLMs (indirectly), as long as the downstream task involves reasoning.

BIG-bench (Srivastava et al., 2022), for example, includes over 200 tasks that test a range of reasoning skills, including tasks like Date Understanding, Word Sorting, and Causal Judgement.

Other benchmarks, such as SCAN (Lake and Baroni, 2018) and the one proposed by Anil et al. (2022), focus on evaluating generalization ability.

LLMs can also be tested on their table reasoning abilities using benchmarks such as WikiTableQA (Pasupat and Liang, 2015), FetaQA (Nan et al., 2022), as suggested by Chen (2022).

In addition, there are benchmarks for evaluating LLMs' generative relational reasoning abilities, such as CommonGen (Lin et al., 2020;Liu et al., 2022a) and Open Relation Modeling (Huang et al., 2022b,d).","Many benchmarks can be used to evaluate the reasoning abilities of LLMs, as long as the downstream task involves reasoning.

For example, BIG-bench (Srivastava et al., 2022) includes over 200 tasks that test a range of reasoning skills, including tasks like Date Understanding, Word Sorting, and Causal Judgement.

Other benchmarks, such as SCAN (Lake and Baroni, 2018) and the one proposed by Anil et al. (2022), focus on evaluating generalization ability.

LLMs can also be tested on their table reasoning abilities using benchmarks such as WikiTableQA (Pasupat and Liang, 2015) and FetaQA (Nan et al., 2022), as suggested by Chen (2022).

In addition, there are benchmarks for evaluating LLMs' generative relational reasoning abilities, such as CommonGen (Lin et al., 2020; Liu et al., 2022a) and Open Relation Modeling (Huang et al., 2022b,d).","Questions:

What are some benchmarks used to evaluate the reasoning abilities of LLMs?

Answer:

Many benchmarks can be used to evaluate the reasoning abilities of LLMs, as long as the downstream task involves reasoning.

For example, BIG-bench (Srivastava et al., 2022) includes over 200 tasks that test a range of reasoning skills, including tasks like Date Understanding, Word Sorting, and Causal Judgement.

Other benchmarks, such as SCAN (Lake and Baroni, 2018) and the one proposed by Anil et al. (2022), focus on evaluating generalization ability.

LLMs can also be tested on their table reasoning abilities using benchmarks such as WikiTableQA (Pasupat and Liang, 2015) and FetaQA (Nan et al., 2022), as suggested by Chen (2022).

In addition, there are benchmarks for evaluating LLMs' generative relational reasoning abilities, such as CommonGen (Lin et al., 2020; Liu et al., 2022a) and Open Relation Modeling (Huang et al., 2022b,d)."
Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,Task Evaluation:,"Another key limitation of existing work is the lack of a comprehensive evaluation. Prior work majorly focused on objective metrics which only provides a limited view of the model performance. A comprehensive evaluation is challenging since it must consider partner perception along with objective outcomes. Building user simulators could potentially alleviate this problem (Li et al., 2016;Jain et al., 2018;. Most existing simulators are developed for task-oriented systems which follow a certain agenda. Future research should study how to use partner modeling to build social influence user simulators for more efficient and accurate task evaluation (He et al., 2018;Yang et al., 2020). For instance, one could potentially design different user personalities and simulate the change in user's beliefs, opinions, and attitudes accordingly (Yang et al., 2021).

Multimodal systems: Being a core function of human communication, social influence occurs not just through text, but through all possible modalities. Schulman and Bickmore (2009) showed that embodied agents achieve better persuasion results than text-only agents. Other studies have recognized the importance of emotion in social influence tasks (Asai et al., 2020;Chawla et al., 2021a). Nguyen et al. (2021) proposed a speech dataset in debates and study the influence of spoken tactics on persuasiveness across genders. Given these findings, we encourage interdisciplinary efforts in the future to explore the developement of multimodal social influence agents.

Knowledge-enriched systems: Social influence tasks often involve constantly-changing world knowledge such as organization facts and news. Often, the system's internal state (e.g., the change of task setting from one set of products to a different set) needs to be updated. Retraining the entire system is costly to maintain after the initial development. Recent work has proposed to augment the dialogue system with internet-search ability to generate more factual and updated responses in open-domain dialogues (Komeili et al., 2021). Future efforts in this direction will benefit social influence dialogue systems as well.","# QUA Analysis:

1. What is a key limitation of existing work in social influence dialogue systems?
    1.1. Why is the lack of a comprehensive evaluation a limitation? 
        - sent1
        1.1.1. What has prior work focused on in terms of evaluation?
            - sent2
        1.1.2. Why is a comprehensive evaluation challenging?
            - sent3
        1.1.3. How could building user simulators alleviate the problem of limited evaluation?
            - sent4
            1.1.3.1. What could be a potential approach to designing user simulators?
                - sent5

2. How does social influence occur in human communication?
    2.1. What is the role of multimodal systems in social influence?
        - sent6
        2.1.1. What evidence supports the effectiveness of multimodal systems in social influence?
            - sent7
        2.1.2. What other factors are important in social influence tasks?
            - sent8
        2.1.3. What recent study has been conducted on the influence of spoken tactics on persuasiveness?
            - sent9
        2.1.4. What is encouraged for future research in multimodal social influence agents?
            - sent10

3. What is the role of knowledge-enriched systems in social influence tasks?
    3.1. Why is constantly-changing world knowledge important in social influence tasks?
        - sent11
        3.1.1. What needs to be updated in the system's internal state?
            - sent12
        3.1.2. Why is retraining the entire system costly?
            - sent13
        3.1.3. What recent work has been proposed to address the need for updated responses?
            - sent14
        3.1.4. How will future efforts in this direction benefit social influence dialogue systems?
            - sent15",What are the challenges and potential solutions for comprehensive evaluation in social influence dialogue systems?,"sent1, sent2, sent3, sent4, sent5","Another key limitation of existing work is the lack of a comprehensive evaluation.

Prior work majorly focused on objective metrics which only provides a limited view of the model performance.

A comprehensive evaluation is challenging since it must consider partner perception along with objective outcomes.

Building user simulators could potentially alleviate this problem (Li et al., 2016;Jain et al., 2018;. Most existing simulators are developed for task-oriented systems which follow a certain agenda. Future research should study how to use partner modeling to build social influence user simulators for more efficient and accurate task evaluation (He et al., 2018;Yang et al., 2020).

For instance, one could potentially design different user personalities and simulate the change in user's beliefs, opinions, and attitudes accordingly (Yang et al., 2021).","A key limitation of existing work is the lack of a comprehensive evaluation.

Prior work has primarily focused on objective metrics, which only provide a limited view of the model performance.

A comprehensive evaluation is challenging since it must consider partner perception along with objective outcomes.

Building user simulators could potentially alleviate this problem (Li et al., 2016; Jain et al., 2018). Most existing simulators are developed for task-oriented systems which follow a certain agenda. Future research should study how to use partner modeling to build social influence user simulators for more efficient and accurate task evaluation (He et al., 2018; Yang et al., 2020).

For instance, one could potentially design different user personalities and simulate the change in users' beliefs, opinions, and attitudes accordingly (Yang et al., 2021).","Questions:

What are the challenges and potential solutions for comprehensive evaluation in social influence dialogue systems?

Answer:

A key limitation of existing work is the lack of a comprehensive evaluation.

Prior work has primarily focused on objective metrics, which only provide a limited view of the model performance.

A comprehensive evaluation is challenging since it must consider partner perception along with objective outcomes.

Building user simulators could potentially alleviate this problem (Li et al., 2016; Jain et al., 2018). Most existing simulators are developed for task-oriented systems which follow a certain agenda. Future research should study how to use partner modeling to build social influence user simulators for more efficient and accurate task evaluation (He et al., 2018; Yang et al., 2020).

For instance, one could potentially design different user personalities and simulate the change in users' beliefs, opinions, and attitudes accordingly (Yang et al., 2021)."
Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,Task Evaluation:,"Another key limitation of existing work is the lack of a comprehensive evaluation. Prior work majorly focused on objective metrics which only provides a limited view of the model performance. A comprehensive evaluation is challenging since it must consider partner perception along with objective outcomes. Building user simulators could potentially alleviate this problem (Li et al., 2016;Jain et al., 2018;. Most existing simulators are developed for task-oriented systems which follow a certain agenda. Future research should study how to use partner modeling to build social influence user simulators for more efficient and accurate task evaluation (He et al., 2018;Yang et al., 2020). For instance, one could potentially design different user personalities and simulate the change in user's beliefs, opinions, and attitudes accordingly (Yang et al., 2021).

Multimodal systems: Being a core function of human communication, social influence occurs not just through text, but through all possible modalities. Schulman and Bickmore (2009) showed that embodied agents achieve better persuasion results than text-only agents. Other studies have recognized the importance of emotion in social influence tasks (Asai et al., 2020;Chawla et al., 2021a). Nguyen et al. (2021) proposed a speech dataset in debates and study the influence of spoken tactics on persuasiveness across genders. Given these findings, we encourage interdisciplinary efforts in the future to explore the developement of multimodal social influence agents.

Knowledge-enriched systems: Social influence tasks often involve constantly-changing world knowledge such as organization facts and news. Often, the system's internal state (e.g., the change of task setting from one set of products to a different set) needs to be updated. Retraining the entire system is costly to maintain after the initial development. Recent work has proposed to augment the dialogue system with internet-search ability to generate more factual and updated responses in open-domain dialogues (Komeili et al., 2021). Future efforts in this direction will benefit social influence dialogue systems as well.","# QUA Analysis:

1. What is a key limitation of existing work in social influence dialogue systems?
    1.1. Why is the lack of a comprehensive evaluation a limitation? 
        - sent1
        1.1.1. What has prior work focused on in terms of evaluation?
            - sent2
        1.1.2. Why is a comprehensive evaluation challenging?
            - sent3
        1.1.3. How could building user simulators alleviate the problem of limited evaluation?
            - sent4
            1.1.3.1. What could be a potential approach to designing user simulators?
                - sent5

2. How does social influence occur in human communication?
    2.1. What is the role of multimodal systems in social influence?
        - sent6
        2.1.1. What evidence supports the effectiveness of multimodal systems in social influence?
            - sent7
        2.1.2. What other factors are important in social influence tasks?
            - sent8
        2.1.3. What recent study has been conducted on the influence of spoken tactics on persuasiveness?
            - sent9
        2.1.4. What is encouraged for future research in multimodal social influence agents?
            - sent10

3. What is the role of knowledge-enriched systems in social influence tasks?
    3.1. Why is constantly-changing world knowledge important in social influence tasks?
        - sent11
        3.1.1. What needs to be updated in the system's internal state?
            - sent12
        3.1.2. Why is retraining the entire system costly?
            - sent13
        3.1.3. What recent work has been proposed to address the need for updated responses?
            - sent14
        3.1.4. How will future efforts in this direction benefit social influence dialogue systems?
            - sent15",How do multimodal systems enhance social influence in human communication?,"sent6, sent7, sent8, sent9, sent10","Multimodal systems: Being a core function of human communication, social influence occurs not just through text, but through all possible modalities.

Schulman and Bickmore (2009) showed that embodied agents achieve better persuasion results than text-only agents.

Other studies have recognized the importance of emotion in social influence tasks (Asai et al., 2020;Chawla et al., 2021a).

Nguyen et al. (2021) proposed a speech dataset in debates and study the influence of spoken tactics on persuasiveness across genders.

Given these findings, we encourage interdisciplinary efforts in the future to explore the developement of multimodal social influence agents.","Social influence, a core function of human communication, occurs not just through text but through all possible modalities.

Schulman and Bickmore (2009) showed that embodied agents achieve better persuasion results than text-only agents.

Other studies have recognized the importance of emotion in social influence tasks (Asai et al., 2020; Chawla et al., 2021a).

Nguyen et al. (2021) proposed a speech dataset in debates and studied the influence of spoken tactics on persuasiveness across genders.

Given these findings, interdisciplinary efforts are encouraged to explore the development of multimodal social influence agents.","Questions:

How do multimodal systems enhance social influence in human communication?

Answer:

Social influence, a core function of human communication, occurs not just through text but through all possible modalities.

Schulman and Bickmore (2009) showed that embodied agents achieve better persuasion results than text-only agents.

Other studies have recognized the importance of emotion in social influence tasks (Asai et al., 2020; Chawla et al., 2021a).

Nguyen et al. (2021) proposed a speech dataset in debates and studied the influence of spoken tactics on persuasiveness across genders.

Given these findings, interdisciplinary efforts are encouraged to explore the development of multimodal social influence agents."
Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,Task Evaluation:,"Another key limitation of existing work is the lack of a comprehensive evaluation. Prior work majorly focused on objective metrics which only provides a limited view of the model performance. A comprehensive evaluation is challenging since it must consider partner perception along with objective outcomes. Building user simulators could potentially alleviate this problem (Li et al., 2016;Jain et al., 2018;. Most existing simulators are developed for task-oriented systems which follow a certain agenda. Future research should study how to use partner modeling to build social influence user simulators for more efficient and accurate task evaluation (He et al., 2018;Yang et al., 2020). For instance, one could potentially design different user personalities and simulate the change in user's beliefs, opinions, and attitudes accordingly (Yang et al., 2021).

Multimodal systems: Being a core function of human communication, social influence occurs not just through text, but through all possible modalities. Schulman and Bickmore (2009) showed that embodied agents achieve better persuasion results than text-only agents. Other studies have recognized the importance of emotion in social influence tasks (Asai et al., 2020;Chawla et al., 2021a). Nguyen et al. (2021) proposed a speech dataset in debates and study the influence of spoken tactics on persuasiveness across genders. Given these findings, we encourage interdisciplinary efforts in the future to explore the developement of multimodal social influence agents.

Knowledge-enriched systems: Social influence tasks often involve constantly-changing world knowledge such as organization facts and news. Often, the system's internal state (e.g., the change of task setting from one set of products to a different set) needs to be updated. Retraining the entire system is costly to maintain after the initial development. Recent work has proposed to augment the dialogue system with internet-search ability to generate more factual and updated responses in open-domain dialogues (Komeili et al., 2021). Future efforts in this direction will benefit social influence dialogue systems as well.","# QUA Analysis:

1. What is a key limitation of existing work in social influence dialogue systems?
    1.1. Why is the lack of a comprehensive evaluation a limitation? 
        - sent1
        1.1.1. What has prior work focused on in terms of evaluation?
            - sent2
        1.1.2. Why is a comprehensive evaluation challenging?
            - sent3
        1.1.3. How could building user simulators alleviate the problem of limited evaluation?
            - sent4
            1.1.3.1. What could be a potential approach to designing user simulators?
                - sent5

2. How does social influence occur in human communication?
    2.1. What is the role of multimodal systems in social influence?
        - sent6
        2.1.1. What evidence supports the effectiveness of multimodal systems in social influence?
            - sent7
        2.1.2. What other factors are important in social influence tasks?
            - sent8
        2.1.3. What recent study has been conducted on the influence of spoken tactics on persuasiveness?
            - sent9
        2.1.4. What is encouraged for future research in multimodal social influence agents?
            - sent10

3. What is the role of knowledge-enriched systems in social influence tasks?
    3.1. Why is constantly-changing world knowledge important in social influence tasks?
        - sent11
        3.1.1. What needs to be updated in the system's internal state?
            - sent12
        3.1.2. Why is retraining the entire system costly?
            - sent13
        3.1.3. What recent work has been proposed to address the need for updated responses?
            - sent14
        3.1.4. How will future efforts in this direction benefit social influence dialogue systems?
            - sent15",Why is it important to integrate constantly-changing world knowledge in social influence dialogue systems?,"sent11, sent12, sent13, sent14, sent15","Knowledge-enriched systems: Social influence tasks often involve constantly-changing world knowledge such as organization facts and news.

Often, the system's internal state (e.g., the change of task setting from one set of products to a different set) needs to be updated.

Retraining the entire system is costly to maintain after the initial development.

Recent work has proposed to augment the dialogue system with internet-search ability to generate more factual and updated responses in open-domain dialogues (Komeili et al., 2021).

Future efforts in this direction will benefit social influence dialogue systems as well.","Integrating constantly-changing world knowledge, such as organization facts and news, is important in social influence dialogue systems.

The system's internal state, such as the change of task setting from one set of products to a different set, often needs to be updated.

Retraining the entire system is costly to maintain after the initial development.

Recent work has proposed augmenting the dialogue system with internet-search ability to generate more factual and updated responses in open-domain dialogues (Komeili et al., 2021).

Future efforts in this direction will benefit social influence dialogue systems as well.","Questions:

Why is it important to integrate constantly-changing world knowledge in social influence dialogue systems?

Answer:

Integrating constantly-changing world knowledge, such as organization facts and news, is important in social influence dialogue systems.

The system's internal state, such as the change of task setting from one set of products to a different set, often needs to be updated.

Retraining the entire system is costly to maintain after the initial development.

Recent work has proposed augmenting the dialogue system with internet-search ability to generate more factual and updated responses in open-domain dialogues (Komeili et al., 2021).

Future efforts in this direction will benefit social influence dialogue systems as well."
Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,Methodological Progress,"Having summarized the datasets that capture social influence, we now discuss the modeling approaches developed for social influence dialogue systems. Most domains have seen efforts in analyzing human dialogue behaviors and their impact on task outcomes. Examples include analyzing deception in games (Peskov et al., 2020), the impact of persuasive strategies and dialogue acts on charity donations ( In addition, researchers have targeted various domain-specific subtasks that can be crucial for the eventual development of dialogue systems in this space. This involves research in lie detection methods (Yeh and Ku, 2021;Yu et al., 2015), discourse parsing (Shi and Huang, 2019;Ouyang et al., 2021), strategy prediction (Chawla et al., 2021b;, breakdown detection (Yamaguchi et al., 2021), outcome prediction (Sinha and Dasgupta, 2021;Chawla et al., 2020;Dutt et al., 2020), and argument mining (Dutta et al., 2022).

Research that directly targets the development of dialogue systems in this space is still nascent. Among other challenges like limited cross-cultural diversity and relatively smaller dataset size, social influence dialogue settings pose a unique challenge: an average human often exhibits sub-optimal strategic behaviors in social influence tasks (Wunderle, 2007;Babcock and Laschever, 2009). This means that standard seq2seq approaches trained on these collected datasets using supervised learning are fundamentally insufficient for developing effective dialogue systems with influence capabilities. Hence, prior work has put a special attention to the system strategy, employing different ways to model the strategy and language together.

We design a taxonomy of methods developed for social influence tasks, assisting readers to comprehend the progress and reflect on future directions. We organize them based on the system strategy, language generation, partner model, architecture, learning process, and the use of pretrained language models. We present annotations for all the surveyed methods in Table 2 and discuss the common categories in brief below.","1. (What is the focus of this section?): sent1
    1.1. (What efforts have been made in analyzing human dialogue behaviors and their impact on task outcomes?): sent2
        1.1.1. (What are some examples of these efforts?): sent3
    1.2. (What is the current state of research targeting the development of dialogue systems in this space?): sent4
        1.2.1. (What are the challenges faced in social influence dialogue settings?): sent5
            1.2.1.1. (Why are standard seq2seq approaches insufficient for developing effective dialogue systems with influence capabilities?): sent6
        1.2.2. (What has prior work focused on to address these challenges?): sent7
    1.3. (How is the taxonomy of methods for social influence tasks designed?): sent8
        1.3.1. (How are the methods organized in the taxonomy?): sent9
        1.3.2. (What additional information is provided about the surveyed methods?): sent10",What are the challenges faced in social influence dialogue settings?,"sent4, sent5, sent6","Research that directly targets the development of dialogue systems in this space is still nascent.

Among other challenges like limited cross-cultural diversity and relatively smaller dataset size, social influence dialogue settings pose a unique challenge: an average human often exhibits sub-optimal strategic behaviors in social influence tasks (Wunderle, 2007;Babcock and Laschever, 2009).

This means that standard seq2seq approaches trained on these collected datasets using supervised learning are fundamentally insufficient for developing effective dialogue systems with influence capabilities.","Research that directly targets the development of dialogue systems in social influence settings is still nascent.

Among other challenges like limited cross-cultural diversity and relatively smaller dataset size, social influence dialogue settings pose a unique challenge: an average human often exhibits sub-optimal strategic behaviors in social influence tasks (Wunderle, 2007; Babcock and Laschever, 2009).

Standard seq2seq approaches trained on these collected datasets using supervised learning are fundamentally insufficient for developing effective dialogue systems with influence capabilities.","Questions:

What are the challenges faced in social influence dialogue settings?

Answer:

Research that directly targets the development of dialogue systems in social influence settings is still nascent.

Among other challenges like limited cross-cultural diversity and relatively smaller dataset size, social influence dialogue settings pose a unique challenge: an average human often exhibits sub-optimal strategic behaviors in social influence tasks (Wunderle, 2007; Babcock and Laschever, 2009).

Standard seq2seq approaches trained on these collected datasets using supervised learning are fundamentally insufficient for developing effective dialogue systems with influence capabilities."
Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,Methodological Progress,"Having summarized the datasets that capture social influence, we now discuss the modeling approaches developed for social influence dialogue systems. Most domains have seen efforts in analyzing human dialogue behaviors and their impact on task outcomes. Examples include analyzing deception in games (Peskov et al., 2020), the impact of persuasive strategies and dialogue acts on charity donations ( In addition, researchers have targeted various domain-specific subtasks that can be crucial for the eventual development of dialogue systems in this space. This involves research in lie detection methods (Yeh and Ku, 2021;Yu et al., 2015), discourse parsing (Shi and Huang, 2019;Ouyang et al., 2021), strategy prediction (Chawla et al., 2021b;, breakdown detection (Yamaguchi et al., 2021), outcome prediction (Sinha and Dasgupta, 2021;Chawla et al., 2020;Dutt et al., 2020), and argument mining (Dutta et al., 2022).

Research that directly targets the development of dialogue systems in this space is still nascent. Among other challenges like limited cross-cultural diversity and relatively smaller dataset size, social influence dialogue settings pose a unique challenge: an average human often exhibits sub-optimal strategic behaviors in social influence tasks (Wunderle, 2007;Babcock and Laschever, 2009). This means that standard seq2seq approaches trained on these collected datasets using supervised learning are fundamentally insufficient for developing effective dialogue systems with influence capabilities. Hence, prior work has put a special attention to the system strategy, employing different ways to model the strategy and language together.

We design a taxonomy of methods developed for social influence tasks, assisting readers to comprehend the progress and reflect on future directions. We organize them based on the system strategy, language generation, partner model, architecture, learning process, and the use of pretrained language models. We present annotations for all the surveyed methods in Table 2 and discuss the common categories in brief below.","1. (What is the focus of this section?): sent1
    1.1. (What efforts have been made in analyzing human dialogue behaviors and their impact on task outcomes?): sent2
        1.1.1. (What are some examples of these efforts?): sent3
    1.2. (What is the current state of research targeting the development of dialogue systems in this space?): sent4
        1.2.1. (What are the challenges faced in social influence dialogue settings?): sent5
            1.2.1.1. (Why are standard seq2seq approaches insufficient for developing effective dialogue systems with influence capabilities?): sent6
        1.2.2. (What has prior work focused on to address these challenges?): sent7
    1.3. (How is the taxonomy of methods for social influence tasks designed?): sent8
        1.3.1. (How are the methods organized in the taxonomy?): sent9
        1.3.2. (What additional information is provided about the surveyed methods?): sent10",How is the taxonomy of methods for social influence tasks designed?,"sent8, sent9, sent10","We design a taxonomy of methods developed for social influence tasks, assisting readers to comprehend the progress and reflect on future directions.

We organize them based on the system strategy, language generation, partner model, architecture, learning process, and the use of pretrained language models.

We present annotations for all the surveyed methods in Table 2 and discuss the common categories in brief below.","A taxonomy of methods developed for social influence tasks is designed to assist readers in comprehending the progress and reflecting on future directions.

The methods are organized based on the system strategy, language generation, partner model, architecture, learning process, and the use of pretrained language models.

Annotations for all the surveyed methods are provided, and the common categories are discussed briefly.","Questions:

How is the taxonomy of methods for social influence tasks designed?

Answer:

A taxonomy of methods developed for social influence tasks is designed to assist readers in comprehending the progress and reflecting on future directions.

The methods are organized based on the system strategy, language generation, partner model, architecture, learning process, and the use of pretrained language models.

Annotations for all the surveyed methods are provided, and the common categories are discussed briefly."
Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,Strategy Representation,"Implicit: The most obvious way to represent the system strategy is implicitly, without any intended decoupling between system strategy and response realization. This corresponds to the usual sequenceto-sequence framework that has been a standard baseline for the methods developed in this space. An important example is the work by Lewis et al. (2017), who were one of the first works to train endto-end dialogue models that exhibit social influence. The authors employed a neural network based on GRUs, one for encoding the negotiation context, one to encode the dialogue utterances, and two recurrent units to generate the output agreement in a bidirectional manner. Latent vectors: Yarats and Lewis (2018) explored latent vectors to decouple utterance semantics from its linguistic aspects. Their hierarchical approach first constructs a latent vector from the input message, which is then used for response generation and planning. These latent vectors are trained to maximize the likelihood of future dialogue messages and actions, which enables the decoupling between semantics and realization. Dialogue Acts (DAs): Dialogue Acts, such as greeting, offer propose, agreement, or disagreement, are effective at capturing a high-level structure of the dialogue flow in social influence settings, reducing the model strategy to first predicting the dialogue act for the next response. The use of DAs makes it convenient to apply reinforcement learning approaches (Zhang et al., 2020b;Yang et al., 2021), while also aiding in developing a modular dialogue system design (He et al., 2018). Semantic Strategies: The structural properties ex-  ","# QUA Analysis:

1. (What is the implicit way to represent the system strategy?): sent1
    1.1. (How is the implicit strategy representation typically implemented?): sent2
    1.2. (What is an important example of implicit strategy representation?): sent3
        1.2.1. (How did Lewis et al. (2017) implement their model?): sent4
2. (What is the latent vector approach to strategy representation?): sent5
    2.1. (How does the hierarchical approach of latent vectors work?): sent6
    2.2. (What is the purpose of training latent vectors?): sent7
3. (What are Dialogue Acts (DAs) and their role in strategy representation?): sent8
    3.1. (What are the benefits of using DAs in dialogue systems?): sent9",How is the implicit strategy representation typically implemented in dialogue systems?,"sent1, sent2, sent3, sent4","Implicit: The most obvious way to represent the system strategy is implicitly, without any intended decoupling between system strategy and response realization.

This corresponds to the usual sequenceto-sequence framework that has been a standard baseline for the methods developed in this space.

An important example is the work by Lewis et al. (2017), who were one of the first works to train endto-end dialogue models that exhibit social influence.

The authors employed a neural network based on GRUs, one for encoding the negotiation context, one to encode the dialogue utterances, and two recurrent units to generate the output agreement in a bidirectional manner.","The most obvious way to represent the system strategy is implicitly, without any intended decoupling between system strategy and response realization.

This corresponds to the usual sequence-to-sequence framework that has been a standard baseline for the methods developed in this space.

An important example is the work by Lewis et al. (2017), who were among the first to train end-to-end dialogue models that exhibit social influence.

Lewis et al. (2017) employed a neural network based on GRUs, with one for encoding the negotiation context, one to encode the dialogue utterances, and two recurrent units to generate the output agreement in a bidirectional manner.","Questions:

How is the implicit strategy representation typically implemented in dialogue systems?

Answer:

The most obvious way to represent the system strategy is implicitly, without any intended decoupling between system strategy and response realization.

This corresponds to the usual sequence-to-sequence framework that has been a standard baseline for the methods developed in this space.

An important example is the work by Lewis et al. (2017), who were among the first to train end-to-end dialogue models that exhibit social influence.

Lewis et al. (2017) employed a neural network based on GRUs, with one for encoding the negotiation context, one to encode the dialogue utterances, and two recurrent units to generate the output agreement in a bidirectional manner."
Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,Strategy Representation,"Implicit: The most obvious way to represent the system strategy is implicitly, without any intended decoupling between system strategy and response realization. This corresponds to the usual sequenceto-sequence framework that has been a standard baseline for the methods developed in this space. An important example is the work by Lewis et al. (2017), who were one of the first works to train endto-end dialogue models that exhibit social influence. The authors employed a neural network based on GRUs, one for encoding the negotiation context, one to encode the dialogue utterances, and two recurrent units to generate the output agreement in a bidirectional manner. Latent vectors: Yarats and Lewis (2018) explored latent vectors to decouple utterance semantics from its linguistic aspects. Their hierarchical approach first constructs a latent vector from the input message, which is then used for response generation and planning. These latent vectors are trained to maximize the likelihood of future dialogue messages and actions, which enables the decoupling between semantics and realization. Dialogue Acts (DAs): Dialogue Acts, such as greeting, offer propose, agreement, or disagreement, are effective at capturing a high-level structure of the dialogue flow in social influence settings, reducing the model strategy to first predicting the dialogue act for the next response. The use of DAs makes it convenient to apply reinforcement learning approaches (Zhang et al., 2020b;Yang et al., 2021), while also aiding in developing a modular dialogue system design (He et al., 2018). Semantic Strategies: The structural properties ex-  ","# QUA Analysis:

1. (What is the implicit way to represent the system strategy?): sent1
    1.1. (How is the implicit strategy representation typically implemented?): sent2
    1.2. (What is an important example of implicit strategy representation?): sent3
        1.2.1. (How did Lewis et al. (2017) implement their model?): sent4
2. (What is the latent vector approach to strategy representation?): sent5
    2.1. (How does the hierarchical approach of latent vectors work?): sent6
    2.2. (What is the purpose of training latent vectors?): sent7
3. (What are Dialogue Acts (DAs) and their role in strategy representation?): sent8
    3.1. (What are the benefits of using DAs in dialogue systems?): sent9",What is the latent vector approach to strategy representation in dialogue systems?,"sent5, sent6, sent7","Latent vectors: Yarats and Lewis (2018) explored latent vectors to decouple utterance semantics from its linguistic aspects.

Their hierarchical approach first constructs a latent vector from the input message, which is then used for response generation and planning.

These latent vectors are trained to maximize the likelihood of future dialogue messages and actions, which enables the decoupling between semantics and realization.","Latent vectors were explored by Yarats and Lewis (2018) to decouple utterance semantics from its linguistic aspects.

Their hierarchical approach first constructs a latent vector from the input message, which is then used for response generation and planning.

These latent vectors are trained to maximize the likelihood of future dialogue messages and actions, which enables the decoupling between semantics and realization.","Questions:

What is the latent vector approach to strategy representation in dialogue systems?

Answer:

Latent vectors were explored by Yarats and Lewis (2018) to decouple utterance semantics from its linguistic aspects.

Their hierarchical approach first constructs a latent vector from the input message, which is then used for response generation and planning.

These latent vectors are trained to maximize the likelihood of future dialogue messages and actions, which enables the decoupling between semantics and realization."
"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems",Adaptive objectives to recommendation,"Numerous pre-training or fine-tuning objectives draw inspiration from LM objectives and have been effectively applied to specific downstream tasks based on the input data types and recommendation goals. In sequential recommendations, there is a common interest in modelling an ordered input sequence in an auto-regressive manner from left to right.

Analogous to text sentences,  and Xiao et al. (2022) treated the user's clicked news history as input text and proposed to model user behavior in an auto-regressive manner for next-click prediction. However, as the sequential dependency may not always hold strictly in terms of user preference for recommendations (Yuan et al., 2020a), MLM objectives can be modified accordingly. Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training. Autoregressive learning tasks can also be adapted to other types of data. Geng et al. (2022b) modeled a series of paths sampled from a knowledge graph in an auto-regressive manner for recommendation by generating the end node from the pretrained model. Zhao (2022) proposed pre-training the Rearrange Sequence Prediction task to learn the sequence-level information of the user's entire interaction history by predicting whether the user interaction history had been rearranged, which is similar to Permuted Language Modelling (PerLM) (Yang et al., 2019).

MLM, also known as Cloze Prediction, can be adapted to learn graph representations for different recommendation purposes.  proposed pre-training a transformer model on a reconstructed subgraph from a user-item-attribute heterogeneous graph, using Masked Node Prediction (MNP), Masked Edge Prediction (MEP), and meta-path type prediction as objectives. Specifically, MNP was performed by randomly masking a proportion of nodes in a heterogeneous subgraph and then predicting the masked nodes based on the remaining contexts by maximizing the distance between the masked node and the irrelevant node. Similarly, MEP was used to recover the masked edge of two adjacent nodes based on the surrounding context. Apart from that, MLM can also be adapted to multi-modal data called Masked Multimodal Modelling (MMM) (Wu et al., 2022a). MMM was performed by predicting the semantics of masked news and news image regions given the unmasked inputs and indicating whether a news image and news content segment correspond to each other for news recommendation purposes.

The NSP/SOP can be adapted for CTR prediction as Next K Behaviors Prediction (NBP). NBP was proposed to learn user representations in the pre-training stage by inferring whether a candidate behavior is the next i-th behavior of the target user based on their past N behaviors. NBP can also capture the relatedness between past and multiple future behaviors.","1. (What are the adaptive objectives to recommendation?): sent1
    1.1. (How are sequential recommendations modeled?): sent2
        1.1.1. (How is user behavior modeled in sequential recommendations?): sent3
        1.1.2. (What are the limitations of sequential dependency in recommendations?): sent4
            1.1.2.1. (How can MLM objectives be modified for recommendations?): sent5
    1.2. (How can autoregressive learning tasks be adapted to other data types?): sent6
        1.2.1. (How is autoregressive learning applied to knowledge graphs?): sent7
        1.2.2. (What is the Rearrange Sequence Prediction task?): sent8
    1.3. (How can MLM be adapted for graph representations?): sent9
        1.3.1. (What is Masked Node Prediction (MNP) and how is it performed?): sent10
        1.3.2. (What is Masked Edge Prediction (MEP) and how is it performed?): sent11
    1.4. (How can MLM be adapted to multi-modal data?): sent12
        1.4.1. (What is Masked Multimodal Modelling (MMM) and how is it performed?): sent13
    1.5. (How can NSP/SOP be adapted for CTR prediction?): sent14
        1.5.1. (What is Next K Behaviors Prediction (NBP) and how is it performed?): sent15
        1.5.2. (What additional capability does NBP provide?): sent16",How are sequential recommendations modeled in an auto-regressive manner?,"sent2, sent3, sent4, sent5","In sequential recommendations, there is a common interest in modelling an ordered input sequence in an auto-regressive manner from left to right.

Analogous to text sentences,  and Xiao et al. (2022) treated the user's clicked news history as input text and proposed to model user behavior in an auto-regressive manner for next-click prediction.

However, as the sequential dependency may not always hold strictly in terms of user preference for recommendations (Yuan et al., 2020a), MLM objectives can be modified accordingly.

Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training.","In sequential recommendations, an ordered input sequence is commonly modeled in an auto-regressive manner from left to right.

Analogous to text sentences, Xiao et al. (2022) treated the user's clicked news history as input text and proposed to model user behavior in an auto-regressive manner for next-click prediction.

However, as the sequential dependency may not always hold strictly in terms of user preference for recommendations (Yuan et al., 2020a), MLM objectives can be modified accordingly.

Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training.","Questions:

How are sequential recommendations modeled in an auto-regressive manner?

Answer:

In sequential recommendations, an ordered input sequence is commonly modeled in an auto-regressive manner from left to right.

Analogous to text sentences, Xiao et al. (2022) treated the user's clicked news history as input text and proposed to model user behavior in an auto-regressive manner for next-click prediction.

However, as the sequential dependency may not always hold strictly in terms of user preference for recommendations (Yuan et al., 2020a), MLM objectives can be modified accordingly.

Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training."
"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems",Adaptive objectives to recommendation,"Numerous pre-training or fine-tuning objectives draw inspiration from LM objectives and have been effectively applied to specific downstream tasks based on the input data types and recommendation goals. In sequential recommendations, there is a common interest in modelling an ordered input sequence in an auto-regressive manner from left to right.

Analogous to text sentences,  and Xiao et al. (2022) treated the user's clicked news history as input text and proposed to model user behavior in an auto-regressive manner for next-click prediction. However, as the sequential dependency may not always hold strictly in terms of user preference for recommendations (Yuan et al., 2020a), MLM objectives can be modified accordingly. Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training. Autoregressive learning tasks can also be adapted to other types of data. Geng et al. (2022b) modeled a series of paths sampled from a knowledge graph in an auto-regressive manner for recommendation by generating the end node from the pretrained model. Zhao (2022) proposed pre-training the Rearrange Sequence Prediction task to learn the sequence-level information of the user's entire interaction history by predicting whether the user interaction history had been rearranged, which is similar to Permuted Language Modelling (PerLM) (Yang et al., 2019).

MLM, also known as Cloze Prediction, can be adapted to learn graph representations for different recommendation purposes.  proposed pre-training a transformer model on a reconstructed subgraph from a user-item-attribute heterogeneous graph, using Masked Node Prediction (MNP), Masked Edge Prediction (MEP), and meta-path type prediction as objectives. Specifically, MNP was performed by randomly masking a proportion of nodes in a heterogeneous subgraph and then predicting the masked nodes based on the remaining contexts by maximizing the distance between the masked node and the irrelevant node. Similarly, MEP was used to recover the masked edge of two adjacent nodes based on the surrounding context. Apart from that, MLM can also be adapted to multi-modal data called Masked Multimodal Modelling (MMM) (Wu et al., 2022a). MMM was performed by predicting the semantics of masked news and news image regions given the unmasked inputs and indicating whether a news image and news content segment correspond to each other for news recommendation purposes.

The NSP/SOP can be adapted for CTR prediction as Next K Behaviors Prediction (NBP). NBP was proposed to learn user representations in the pre-training stage by inferring whether a candidate behavior is the next i-th behavior of the target user based on their past N behaviors. NBP can also capture the relatedness between past and multiple future behaviors.","1. (What are the adaptive objectives to recommendation?): sent1
    1.1. (How are sequential recommendations modeled?): sent2
        1.1.1. (How is user behavior modeled in sequential recommendations?): sent3
        1.1.2. (What are the limitations of sequential dependency in recommendations?): sent4
            1.1.2.1. (How can MLM objectives be modified for recommendations?): sent5
    1.2. (How can autoregressive learning tasks be adapted to other data types?): sent6
        1.2.1. (How is autoregressive learning applied to knowledge graphs?): sent7
        1.2.2. (What is the Rearrange Sequence Prediction task?): sent8
    1.3. (How can MLM be adapted for graph representations?): sent9
        1.3.1. (What is Masked Node Prediction (MNP) and how is it performed?): sent10
        1.3.2. (What is Masked Edge Prediction (MEP) and how is it performed?): sent11
    1.4. (How can MLM be adapted to multi-modal data?): sent12
        1.4.1. (What is Masked Multimodal Modelling (MMM) and how is it performed?): sent13
    1.5. (How can NSP/SOP be adapted for CTR prediction?): sent14
        1.5.1. (What is Next K Behaviors Prediction (NBP) and how is it performed?): sent15
        1.5.2. (What additional capability does NBP provide?): sent16",How can MLM objectives be adapted to learn graph representations for recommendations?,"sent9, sent10, sent11","proposed pre-training a transformer model on a reconstructed subgraph from a user-item-attribute heterogeneous graph, using Masked Node Prediction (MNP), Masked Edge Prediction (MEP), and meta-path type prediction as objectives.

Specifically, MNP was performed by randomly masking a proportion of nodes in a heterogeneous subgraph and then predicting the masked nodes based on the remaining contexts by maximizing the distance between the masked node and the irrelevant node.

Similarly, MEP was used to recover the masked edge of two adjacent nodes based on the surrounding context.","Pre-training a transformer model on a reconstructed subgraph from a user-item-attribute heterogeneous graph, using Masked Node Prediction (MNP), Masked Edge Prediction (MEP), and meta-path type prediction as objectives, has been proposed.

Specifically, MNP involves randomly masking a proportion of nodes in a heterogeneous subgraph and then predicting the masked nodes based on the remaining contexts by maximizing the distance between the masked node and the irrelevant node.

Similarly, MEP is used to recover the masked edge between two adjacent nodes based on the surrounding context.","Questions:

How can MLM objectives be adapted to learn graph representations for recommendations?

Answer:

Pre-training a transformer model on a reconstructed subgraph from a user-item-attribute heterogeneous graph, using Masked Node Prediction (MNP), Masked Edge Prediction (MEP), and meta-path type prediction as objectives, has been proposed.

Specifically, MNP involves randomly masking a proportion of nodes in a heterogeneous subgraph and then predicting the masked nodes based on the remaining contexts by maximizing the distance between the masked node and the irrelevant node.

Similarly, MEP is used to recover the masked edge between two adjacent nodes based on the surrounding context."
"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems",Adaptive objectives to recommendation,"Numerous pre-training or fine-tuning objectives draw inspiration from LM objectives and have been effectively applied to specific downstream tasks based on the input data types and recommendation goals. In sequential recommendations, there is a common interest in modelling an ordered input sequence in an auto-regressive manner from left to right.

Analogous to text sentences,  and Xiao et al. (2022) treated the user's clicked news history as input text and proposed to model user behavior in an auto-regressive manner for next-click prediction. However, as the sequential dependency may not always hold strictly in terms of user preference for recommendations (Yuan et al., 2020a), MLM objectives can be modified accordingly. Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training. Autoregressive learning tasks can also be adapted to other types of data. Geng et al. (2022b) modeled a series of paths sampled from a knowledge graph in an auto-regressive manner for recommendation by generating the end node from the pretrained model. Zhao (2022) proposed pre-training the Rearrange Sequence Prediction task to learn the sequence-level information of the user's entire interaction history by predicting whether the user interaction history had been rearranged, which is similar to Permuted Language Modelling (PerLM) (Yang et al., 2019).

MLM, also known as Cloze Prediction, can be adapted to learn graph representations for different recommendation purposes.  proposed pre-training a transformer model on a reconstructed subgraph from a user-item-attribute heterogeneous graph, using Masked Node Prediction (MNP), Masked Edge Prediction (MEP), and meta-path type prediction as objectives. Specifically, MNP was performed by randomly masking a proportion of nodes in a heterogeneous subgraph and then predicting the masked nodes based on the remaining contexts by maximizing the distance between the masked node and the irrelevant node. Similarly, MEP was used to recover the masked edge of two adjacent nodes based on the surrounding context. Apart from that, MLM can also be adapted to multi-modal data called Masked Multimodal Modelling (MMM) (Wu et al., 2022a). MMM was performed by predicting the semantics of masked news and news image regions given the unmasked inputs and indicating whether a news image and news content segment correspond to each other for news recommendation purposes.

The NSP/SOP can be adapted for CTR prediction as Next K Behaviors Prediction (NBP). NBP was proposed to learn user representations in the pre-training stage by inferring whether a candidate behavior is the next i-th behavior of the target user based on their past N behaviors. NBP can also capture the relatedness between past and multiple future behaviors.","1. (What are the adaptive objectives to recommendation?): sent1
    1.1. (How are sequential recommendations modeled?): sent2
        1.1.1. (How is user behavior modeled in sequential recommendations?): sent3
        1.1.2. (What are the limitations of sequential dependency in recommendations?): sent4
            1.1.2.1. (How can MLM objectives be modified for recommendations?): sent5
    1.2. (How can autoregressive learning tasks be adapted to other data types?): sent6
        1.2.1. (How is autoregressive learning applied to knowledge graphs?): sent7
        1.2.2. (What is the Rearrange Sequence Prediction task?): sent8
    1.3. (How can MLM be adapted for graph representations?): sent9
        1.3.1. (What is Masked Node Prediction (MNP) and how is it performed?): sent10
        1.3.2. (What is Masked Edge Prediction (MEP) and how is it performed?): sent11
    1.4. (How can MLM be adapted to multi-modal data?): sent12
        1.4.1. (What is Masked Multimodal Modelling (MMM) and how is it performed?): sent13
    1.5. (How can NSP/SOP be adapted for CTR prediction?): sent14
        1.5.1. (What is Next K Behaviors Prediction (NBP) and how is it performed?): sent15
        1.5.2. (What additional capability does NBP provide?): sent16",How can NSP/SOP be adapted for CTR prediction?,"sent14, sent15, sent16","The NSP/SOP can be adapted for CTR prediction as Next K Behaviors Prediction (NBP).

NBP was proposed to learn user representations in the pre-training stage by inferring whether a candidate behavior is the next i-th behavior of the target user based on their past N behaviors.

NBP can also capture the relatedness between past and multiple future behaviors.","NSP/SOP can be adapted for CTR prediction as Next K Behaviors Prediction (NBP).

NBP was proposed to learn user representations in the pre-training stage by inferring whether a candidate behavior is the next i-th behavior of the target user based on their past N behaviors.

NBP can also capture the relatedness between past and multiple future behaviors.","Questions:

How can NSP/SOP be adapted for CTR prediction?

Answer:

NSP/SOP can be adapted for CTR prediction as Next K Behaviors Prediction (NBP).

NBP was proposed to learn user representations in the pre-training stage by inferring whether a candidate behavior is the next i-th behavior of the target user based on their past N behaviors.

NBP can also capture the relatedness between past and multiple future behaviors."
A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,Keyphrase Extraction Dataset,"Since the first shared task on KE (Turney, 1999), many shared tasks and benchmark datasets for KE have been created. Specifically, OpenKP (Xiong et al., 2019), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010, Se-mEval2017 (Augenstein et al., 2017), and KP20k (Meng et al., 2017) were created from scientific articles in English.

Compared with other datasets, KP20k contains a large amount of annotation data, so it is often used as the dataset to train the neural-based KE models recently. Meanwhile, in recent papers (Sun et al., 2020a;Song et al., 2021), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010, and SemEval2017 (Augenstein et al., 2017)  datasets are often used as the zero-shot test sets to verify the robustness of the KE models trained by the KP20k dataset. Furthermore, KE tasks have also been organized on newswire articles in English, e.g., DUC2001 (Wan and Xiao, 2008b). Table 1 summarizes the statistics of several commonly used benchmark datasets.","# QUA Analysis:

1. (What is the historical context and development of KE datasets?): sent1
    1.1. (What are some specific KE datasets created from scientific articles in English?): sent2
        1.1.1. (Which dataset contains a large amount of annotation data and is often used to train neural-based KE models?): sent3
        1.1.2. (Which datasets are often used as zero-shot test sets to verify the robustness of KE models trained by KP20k?): sent4
    1.2. (Have KE tasks been organized on other types of articles?): sent5
2. (What does Table 1 summarize?): sent6",What are some specific KE datasets created from scientific articles in English?,"sent2, sent3, sent4","Specifically, OpenKP (Xiong et al., 2019), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010, Se-mEval2017 (Augenstein et al., 2017), and KP20k (Meng et al., 2017) were created from scientific articles in English.

Compared with other datasets, KP20k contains a large amount of annotation data, so it is often used as the dataset to train the neural-based KE models recently.

Meanwhile, in recent papers (Sun et al., 2020a;Song et al., 2021), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010, and SemEval2017 (Augenstein et al., 2017)  datasets are often used as the zero-shot test sets to verify the robustness of the KE models trained by the KP20k dataset.","Specifically, OpenKP (Xiong et al., 2019), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010), SemEval2017 (Augenstein et al., 2017), and KP20k (Meng et al., 2017) were created from scientific articles in English.

Compared with other datasets, KP20k contains a large amount of annotation data, so it is often used as the dataset to train neural-based KE models recently.

Meanwhile, in recent papers (Sun et al., 2020a; Song et al., 2021), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010), and SemEval2017 (Augenstein et al., 2017) datasets are often used as the zero-shot test sets to verify the robustness of the KE models trained by the KP20k dataset.","Questions:

What are some specific KE datasets created from scientific articles in English?

Answer:

Specifically, OpenKP (Xiong et al., 2019), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010), SemEval2017 (Augenstein et al., 2017), and KP20k (Meng et al., 2017) were created from scientific articles in English.

Compared with other datasets, KP20k contains a large amount of annotation data, so it is often used as the dataset to train neural-based KE models recently.

Meanwhile, in recent papers (Sun et al., 2020a; Song et al., 2021), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010), and SemEval2017 (Augenstein et al., 2017) datasets are often used as the zero-shot test sets to verify the robustness of the KE models trained by the KP20k dataset."
A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,Two-Stage Supervised Keyphrase Extraction Models,"Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously. Therefore, to obtain sufficient candidates, the recent supervised models (Xiong et (Xiong et al., 2019) formulates keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, which incorporates pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations. BLING-KPE achieves significant improvement over previous models. To leverage external knowledge to assist keyphrase extraction, SMART-KPE 9  also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction. Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents. SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context. JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases. To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space. Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.  ","# QUA Analysis:

1. (What are the characteristics of two-stage supervised keyphrase extraction models?): sent1
    1.1. (How do recent supervised models obtain sufficient candidates?): sent2
        1.1.1. (What is the performance of BLING-KPE?): sent3
    1.2. (How do models leverage external knowledge for keyphrase extraction?): sent4
    1.3. (What modifications have been made to improve keyphrase extraction performance for long documents?): sent5
    1.4. (What are the specific approaches proposed for keyphrase extraction?): sent6, sent7
        1.4.1. (What is the approach of JointKPE?): sent7
        1.4.2. (What is the approach of KIEMP?): sent7
    1.5. (How does HyperMatch propose to extract more relevant keyphrases?): sent8
        1.5.1. (How does HyperMatch model the relevance between candidate phrases and the document?): sent9",What are the characteristics of two-stage supervised keyphrase extraction models?,"sent1, sent2, sent4, sent5, sent6, sent7, sent8, sent9","Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously.

Therefore, to obtain sufficient candidates, the recent supervised models (Xiong et (Xiong et al., 2019) formulates keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, which incorporates pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations.

To leverage external knowledge to assist keyphrase extraction, SMART-KPE 9  also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction.

Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents.

SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context.

JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.","Supervised two-stage keyphrase extraction models generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guiding the whole model to rank and extract keyphrases through annotated data and optimizing the two stages simultaneously.

Recent supervised models, such as Xiong et al. (2019), formulate keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, incorporating pre-trained embeddings like ELMo (Peters et al., 2018) into a convolutional transformer network to model n-gram representations.

To leverage external knowledge to assist keyphrase extraction, SMART-KPE also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction.

Later, Ainslie et al. (2020) replace the full self-attention of Transformers with local-global attention, significantly boosting keyphrase extraction performance for long documents.

SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context.

JointKPE (Sun et al., 2020a) proposes an open-domain keyphrase extraction approach built on pre-trained language models (Devlin et al., 2019), which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introduces a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.","Questions:

What are the characteristics of two-stage supervised keyphrase extraction models?

Answer:

Supervised two-stage keyphrase extraction models generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guiding the whole model to rank and extract keyphrases through annotated data and optimizing the two stages simultaneously.

Recent supervised models, such as Xiong et al. (2019), formulate keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, incorporating pre-trained embeddings like ELMo (Peters et al., 2018) into a convolutional transformer network to model n-gram representations.

To leverage external knowledge to assist keyphrase extraction, SMART-KPE also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction.

Later, Ainslie et al. (2020) replace the full self-attention of Transformers with local-global attention, significantly boosting keyphrase extraction performance for long documents.

SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context.

JointKPE (Sun et al., 2020a) proposes an open-domain keyphrase extraction approach built on pre-trained language models (Devlin et al., 2019), which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introduces a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases."
A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,Two-Stage Supervised Keyphrase Extraction Models,"Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously. Therefore, to obtain sufficient candidates, the recent supervised models (Xiong et (Xiong et al., 2019) formulates keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, which incorporates pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations. BLING-KPE achieves significant improvement over previous models. To leverage external knowledge to assist keyphrase extraction, SMART-KPE 9  also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction. Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents. SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context. JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases. To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space. Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.  ","# QUA Analysis:

1. (What are the characteristics of two-stage supervised keyphrase extraction models?): sent1
    1.1. (How do recent supervised models obtain sufficient candidates?): sent2
        1.1.1. (What is the performance of BLING-KPE?): sent3
    1.2. (How do models leverage external knowledge for keyphrase extraction?): sent4
    1.3. (What modifications have been made to improve keyphrase extraction performance for long documents?): sent5
    1.4. (What are the specific approaches proposed for keyphrase extraction?): sent6, sent7
        1.4.1. (What is the approach of JointKPE?): sent7
        1.4.2. (What is the approach of KIEMP?): sent7
    1.5. (How does HyperMatch propose to extract more relevant keyphrases?): sent8
        1.5.1. (How does HyperMatch model the relevance between candidate phrases and the document?): sent9",How do recent supervised models obtain sufficient candidates for keyphrase extraction?,"sent2, sent4, sent5, sent6, sent7, sent8, sent9","Therefore, to obtain sufficient candidates, the recent supervised models (Xiong et (Xiong et al., 2019) formulates keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, which incorporates pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations.

To leverage external knowledge to assist keyphrase extraction, SMART-KPE 9  also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction.

Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents.

SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context.

JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.","To obtain sufficient candidates, recent supervised models (Xiong et al., 2019) formulate keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, incorporating pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations.

Incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction, as demonstrated by SMART-KPE.

Later, Ainslie et al. (2020) replace the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents.

SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context.

JointKPE (Sun et al., 2020a) proposes an open-domain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019), which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introduces a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.","Questions:

How do recent supervised models obtain sufficient candidates for keyphrase extraction?

Answer:

To obtain sufficient candidates, recent supervised models (Xiong et al., 2019) formulate keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, incorporating pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations.

Incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction, as demonstrated by SMART-KPE.

Later, Ainslie et al. (2020) replace the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents.

SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context.

JointKPE (Sun et al., 2020a) proposes an open-domain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019), which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introduces a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases."
A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,Two-Stage Supervised Keyphrase Extraction Models,"Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously. Therefore, to obtain sufficient candidates, the recent supervised models (Xiong et (Xiong et al., 2019) formulates keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, which incorporates pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations. BLING-KPE achieves significant improvement over previous models. To leverage external knowledge to assist keyphrase extraction, SMART-KPE 9  also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction. Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents. SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context. JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases. To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space. Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.  ","# QUA Analysis:

1. (What are the characteristics of two-stage supervised keyphrase extraction models?): sent1
    1.1. (How do recent supervised models obtain sufficient candidates?): sent2
        1.1.1. (What is the performance of BLING-KPE?): sent3
    1.2. (How do models leverage external knowledge for keyphrase extraction?): sent4
    1.3. (What modifications have been made to improve keyphrase extraction performance for long documents?): sent5
    1.4. (What are the specific approaches proposed for keyphrase extraction?): sent6, sent7
        1.4.1. (What is the approach of JointKPE?): sent7
        1.4.2. (What is the approach of KIEMP?): sent7
    1.5. (How does HyperMatch propose to extract more relevant keyphrases?): sent8
        1.5.1. (How does HyperMatch model the relevance between candidate phrases and the document?): sent9",How do models leverage external knowledge for keyphrase extraction?,"sent4, sent5, sent6, sent7, sent8, sent9","To leverage external knowledge to assist keyphrase extraction, SMART-KPE 9  also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction.

Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents.

SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context.

JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.","To leverage external knowledge for keyphrase extraction, incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction.

Ainslie et al. (2020) replace the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents.

SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context.

JointKPE (Sun et al., 2020a) proposes an open-domain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019), which can capture both local phraseness and global informativeness when extracting keyphrases. KIEMP (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introduces a matching module to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.","Questions:

How do models leverage external knowledge for keyphrase extraction?

Answer:

To leverage external knowledge for keyphrase extraction, incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction.

Ainslie et al. (2020) replace the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents.

SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context.

JointKPE (Sun et al., 2020a) proposes an open-domain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019), which can capture both local phraseness and global informativeness when extracting keyphrases. KIEMP (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introduces a matching module to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases."
A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,Two-Stage Supervised Keyphrase Extraction Models,"Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously. Therefore, to obtain sufficient candidates, the recent supervised models (Xiong et (Xiong et al., 2019) formulates keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, which incorporates pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations. BLING-KPE achieves significant improvement over previous models. To leverage external knowledge to assist keyphrase extraction, SMART-KPE 9  also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction. Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents. SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context. JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases. To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space. Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.  ","# QUA Analysis:

1. (What are the characteristics of two-stage supervised keyphrase extraction models?): sent1
    1.1. (How do recent supervised models obtain sufficient candidates?): sent2
        1.1.1. (What is the performance of BLING-KPE?): sent3
    1.2. (How do models leverage external knowledge for keyphrase extraction?): sent4
    1.3. (What modifications have been made to improve keyphrase extraction performance for long documents?): sent5
    1.4. (What are the specific approaches proposed for keyphrase extraction?): sent6, sent7
        1.4.1. (What is the approach of JointKPE?): sent7
        1.4.2. (What is the approach of KIEMP?): sent7
    1.5. (How does HyperMatch propose to extract more relevant keyphrases?): sent8
        1.5.1. (How does HyperMatch model the relevance between candidate phrases and the document?): sent9",What modifications have been made to improve keyphrase extraction performance for long documents?,"sent5, sent6, sent7, sent8, sent9","Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents.

SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context.

JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.","Ainslie et al. (2020) replaced the full self-attention of Transformers with local-global attention, significantly boosting keyphrase extraction performance for long documents.

SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context.

JointKPE (Sun et al., 2020a) proposes an open-domain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019), which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introduces a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.","Questions:

What modifications have been made to improve keyphrase extraction performance for long documents?

Answer:

Ainslie et al. (2020) replaced the full self-attention of Transformers with local-global attention, significantly boosting keyphrase extraction performance for long documents.

SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context.

JointKPE (Sun et al., 2020a) proposes an open-domain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019), which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introduces a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases."
A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,Two-Stage Supervised Keyphrase Extraction Models,"Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously. Therefore, to obtain sufficient candidates, the recent supervised models (Xiong et (Xiong et al., 2019) formulates keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, which incorporates pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations. BLING-KPE achieves significant improvement over previous models. To leverage external knowledge to assist keyphrase extraction, SMART-KPE 9  also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction. Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents. SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context. JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases. To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space. Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.  ","# QUA Analysis:

1. (What are the characteristics of two-stage supervised keyphrase extraction models?): sent1
    1.1. (How do recent supervised models obtain sufficient candidates?): sent2
        1.1.1. (What is the performance of BLING-KPE?): sent3
    1.2. (How do models leverage external knowledge for keyphrase extraction?): sent4
    1.3. (What modifications have been made to improve keyphrase extraction performance for long documents?): sent5
    1.4. (What are the specific approaches proposed for keyphrase extraction?): sent6, sent7
        1.4.1. (What is the approach of JointKPE?): sent7
        1.4.2. (What is the approach of KIEMP?): sent7
    1.5. (How does HyperMatch propose to extract more relevant keyphrases?): sent8
        1.5.1. (How does HyperMatch model the relevance between candidate phrases and the document?): sent9",What are the specific approaches proposed for keyphrase extraction?,"sent6, sent7, sent8, sent9","SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context.

JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.","SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to capture the relationships between candidates and the document in context.

JointKPE (Sun et al., 2020a) proposes an open-domain keyphrase extraction approach built on pretrained language models, which can capture both local phraseness and global informativeness when extracting keyphrases. KIEMP (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introduces a matching module to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.","Questions:

What are the specific approaches proposed for keyphrase extraction?

Answer:

SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to capture the relationships between candidates and the document in context.

JointKPE (Sun et al., 2020a) proposes an open-domain keyphrase extraction approach built on pretrained language models, which can capture both local phraseness and global informativeness when extracting keyphrases. KIEMP (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introduces a matching module to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases."
A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,Two-Stage Supervised Keyphrase Extraction Models,"Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously. Therefore, to obtain sufficient candidates, the recent supervised models (Xiong et (Xiong et al., 2019) formulates keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, which incorporates pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations. BLING-KPE achieves significant improvement over previous models. To leverage external knowledge to assist keyphrase extraction, SMART-KPE 9  also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction. Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents. SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context. JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases. To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space. Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.  ","# QUA Analysis:

1. (What are the characteristics of two-stage supervised keyphrase extraction models?): sent1
    1.1. (How do recent supervised models obtain sufficient candidates?): sent2
        1.1.1. (What is the performance of BLING-KPE?): sent3
    1.2. (How do models leverage external knowledge for keyphrase extraction?): sent4
    1.3. (What modifications have been made to improve keyphrase extraction performance for long documents?): sent5
    1.4. (What are the specific approaches proposed for keyphrase extraction?): sent6, sent7
        1.4.1. (What is the approach of JointKPE?): sent7
        1.4.2. (What is the approach of KIEMP?): sent7
    1.5. (How does HyperMatch propose to extract more relevant keyphrases?): sent8
        1.5.1. (How does HyperMatch model the relevance between candidate phrases and the document?): sent9",What is the approach of JointKPE for keyphrase extraction?,"sent7, sent8, sent9","JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.","JointKPE (Sun et al., 2020a) proposes an open-domain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019), which can capture both local phraseness and global informativeness when extracting keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.","Questions:

What is the approach of JointKPE for keyphrase extraction?

Answer:

JointKPE (Sun et al., 2020a) proposes an open-domain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019), which can capture both local phraseness and global informativeness when extracting keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases."
A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,Two-Stage Supervised Keyphrase Extraction Models,"Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously. Therefore, to obtain sufficient candidates, the recent supervised models (Xiong et (Xiong et al., 2019) formulates keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, which incorporates pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations. BLING-KPE achieves significant improvement over previous models. To leverage external knowledge to assist keyphrase extraction, SMART-KPE 9  also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction. Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents. SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context. JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases. To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space. Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.  ","# QUA Analysis:

1. (What are the characteristics of two-stage supervised keyphrase extraction models?): sent1
    1.1. (How do recent supervised models obtain sufficient candidates?): sent2
        1.1.1. (What is the performance of BLING-KPE?): sent3
    1.2. (How do models leverage external knowledge for keyphrase extraction?): sent4
    1.3. (What modifications have been made to improve keyphrase extraction performance for long documents?): sent5
    1.4. (What are the specific approaches proposed for keyphrase extraction?): sent6, sent7
        1.4.1. (What is the approach of JointKPE?): sent7
        1.4.2. (What is the approach of KIEMP?): sent7
    1.5. (How does HyperMatch propose to extract more relevant keyphrases?): sent8
        1.5.1. (How does HyperMatch model the relevance between candidate phrases and the document?): sent9",What is the approach of KIEMP for keyphrase extraction?,"sent7, sent8, sent9","JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.","KIEMP (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introduces a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.","Questions:

What is the approach of KIEMP for keyphrase extraction?

Answer:

KIEMP (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introduces a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases.

To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space.

HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases."
Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Discussion,"If the heuristics or QG are properly designed, NR models trained from their supervision can even match the fully-supervised performance Ren et al., 2022). The biggest challenge is the difficulty to pick the most suitable heuristics or QG when we face a new domain. A general solution is to automatically select good pseudo pairs with reinforcement learning (RL) when minimal target-domain annotations are available , so as avoiding the need to manually fixing the WS signals, but this would bring significant computational overhead. In practice hyperlink-based approaches often perform the best among the heuristics as they have additional reference information to leverage, which makes them most similar to the actual relevance annotations. However, hyperlink information is not available in most domains and thereby limits its use cases . QG-based WS signals are often preferred over heuristics-based ones as they can produce naturally-sound questions themselves without relying on the chance to find good pseudo questions in the documents. Nonetheless, obtaining a high-performing QG can also be non-trivial. One big challenge comes from the one-to-many mapping relations between questions and documents. Under this situation, standard supervised learning tends to produce safe questions with less diversity and high lexical overlap with the document. For example, Shinoda et al. (2021) found that QG reinforces the model bias towards high lexical overlap. We will need more sophisticated training techniques such as latent-variable models (Shen and Su, 2018; and reinforcement learning (Yuan et al., 2017;Zhang and Bansal, 2019;Shen et al., 2019a) to alleviate the model bias towards safe questions.","1. (How can NR models trained with heuristics or QG match fully-supervised performance?): sent1
    1.1. (What is the biggest challenge in using heuristics or QG for new domains?): sent2
        1.1.1. (What is a general solution to the challenge of selecting suitable heuristics or QG?): sent3
    1.2. (Which heuristics often perform the best in practice?): sent4
        1.2.1. (What is the limitation of hyperlink-based approaches?): sent5
    1.3. (Why are QG-based WS signals often preferred over heuristics-based ones?): sent6
        1.3.1. (What is a challenge in obtaining high-performing QG?): sent7
            1.3.1.1. (What is one big challenge related to QG?): sent8
                1.3.1.1.1. (What happens under the one-to-many mapping relations between questions and documents?): sent9
                    1.3.1.1.1.1. (What did Shinoda et al. (2021) find about QG?): sent10
                    1.3.1.1.1.2. (What techniques are needed to alleviate the model bias towards safe questions?): sent11",What is the biggest challenge in using heuristics or QG for new domains?,"sent2, sent3, sent5","The biggest challenge is the difficulty to pick the most suitable heuristics or QG when we face a new domain.

A general solution is to automatically select good pseudo pairs with reinforcement learning (RL) when minimal target-domain annotations are available , so as avoiding the need to manually fixing the WS signals, but this would bring significant computational overhead.

However, hyperlink information is not available in most domains and thereby limits its use cases .","The biggest challenge is the difficulty in selecting the most suitable heuristics or QG when encountering a new domain.

A general solution is to automatically select good pseudo pairs with reinforcement learning (RL) when minimal target-domain annotations are available, avoiding the need to manually fix the weak supervision signals, but this approach introduces significant computational overhead.

However, hyperlink information is not available in most domains, thereby limiting its use cases.","Questions:

What is the biggest challenge in using heuristics or QG for new domains?

Answer:

The biggest challenge is the difficulty in selecting the most suitable heuristics or QG when encountering a new domain.

A general solution is to automatically select good pseudo pairs with reinforcement learning (RL) when minimal target-domain annotations are available, avoiding the need to manually fix the weak supervision signals, but this approach introduces significant computational overhead.

However, hyperlink information is not available in most domains, thereby limiting its use cases."
Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Discussion,"If the heuristics or QG are properly designed, NR models trained from their supervision can even match the fully-supervised performance Ren et al., 2022). The biggest challenge is the difficulty to pick the most suitable heuristics or QG when we face a new domain. A general solution is to automatically select good pseudo pairs with reinforcement learning (RL) when minimal target-domain annotations are available , so as avoiding the need to manually fixing the WS signals, but this would bring significant computational overhead. In practice hyperlink-based approaches often perform the best among the heuristics as they have additional reference information to leverage, which makes them most similar to the actual relevance annotations. However, hyperlink information is not available in most domains and thereby limits its use cases . QG-based WS signals are often preferred over heuristics-based ones as they can produce naturally-sound questions themselves without relying on the chance to find good pseudo questions in the documents. Nonetheless, obtaining a high-performing QG can also be non-trivial. One big challenge comes from the one-to-many mapping relations between questions and documents. Under this situation, standard supervised learning tends to produce safe questions with less diversity and high lexical overlap with the document. For example, Shinoda et al. (2021) found that QG reinforces the model bias towards high lexical overlap. We will need more sophisticated training techniques such as latent-variable models (Shen and Su, 2018; and reinforcement learning (Yuan et al., 2017;Zhang and Bansal, 2019;Shen et al., 2019a) to alleviate the model bias towards safe questions.","1. (How can NR models trained with heuristics or QG match fully-supervised performance?): sent1
    1.1. (What is the biggest challenge in using heuristics or QG for new domains?): sent2
        1.1.1. (What is a general solution to the challenge of selecting suitable heuristics or QG?): sent3
    1.2. (Which heuristics often perform the best in practice?): sent4
        1.2.1. (What is the limitation of hyperlink-based approaches?): sent5
    1.3. (Why are QG-based WS signals often preferred over heuristics-based ones?): sent6
        1.3.1. (What is a challenge in obtaining high-performing QG?): sent7
            1.3.1.1. (What is one big challenge related to QG?): sent8
                1.3.1.1.1. (What happens under the one-to-many mapping relations between questions and documents?): sent9
                    1.3.1.1.1.1. (What did Shinoda et al. (2021) find about QG?): sent10
                    1.3.1.1.1.2. (What techniques are needed to alleviate the model bias towards safe questions?): sent11",Why are QG-based WS signals often preferred over heuristics-based ones?,"sent6, sent7, sent8, sent9, sent10","QG-based WS signals are often preferred over heuristics-based ones as they can produce naturally-sound questions themselves without relying on the chance to find good pseudo questions in the documents.

Nonetheless, obtaining a high-performing QG can also be non-trivial.

One big challenge comes from the one-to-many mapping relations between questions and documents.

Under this situation, standard supervised learning tends to produce safe questions with less diversity and high lexical overlap with the document.

For example, Shinoda et al. (2021) found that QG reinforces the model bias towards high lexical overlap.","QG-based WS signals are often preferred over heuristics-based ones because they can produce naturally-sound questions without relying on the chance to find good pseudo questions in the documents.

Nonetheless, obtaining a high-performing QG can also be non-trivial.

One significant challenge arises from the one-to-many mapping relations between questions and documents.

Under this situation, standard supervised learning tends to produce safe questions with less diversity and high lexical overlap with the document.

For example, Shinoda et al. (2021) found that QG reinforces the model bias towards high lexical overlap.","Questions:

Why are QG-based WS signals often preferred over heuristics-based ones?

Answer:

QG-based WS signals are often preferred over heuristics-based ones because they can produce naturally-sound questions without relying on the chance to find good pseudo questions in the documents.

Nonetheless, obtaining a high-performing QG can also be non-trivial.

One significant challenge arises from the one-to-many mapping relations between questions and documents.

Under this situation, standard supervised learning tends to produce safe questions with less diversity and high lexical overlap with the document.

For example, Shinoda et al. (2021) found that QG reinforces the model bias towards high lexical overlap."
Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Discussion,"If the heuristics or QG are properly designed, NR models trained from their supervision can even match the fully-supervised performance Ren et al., 2022). The biggest challenge is the difficulty to pick the most suitable heuristics or QG when we face a new domain. A general solution is to automatically select good pseudo pairs with reinforcement learning (RL) when minimal target-domain annotations are available , so as avoiding the need to manually fixing the WS signals, but this would bring significant computational overhead. In practice hyperlink-based approaches often perform the best among the heuristics as they have additional reference information to leverage, which makes them most similar to the actual relevance annotations. However, hyperlink information is not available in most domains and thereby limits its use cases . QG-based WS signals are often preferred over heuristics-based ones as they can produce naturally-sound questions themselves without relying on the chance to find good pseudo questions in the documents. Nonetheless, obtaining a high-performing QG can also be non-trivial. One big challenge comes from the one-to-many mapping relations between questions and documents. Under this situation, standard supervised learning tends to produce safe questions with less diversity and high lexical overlap with the document. For example, Shinoda et al. (2021) found that QG reinforces the model bias towards high lexical overlap. We will need more sophisticated training techniques such as latent-variable models (Shen and Su, 2018; and reinforcement learning (Yuan et al., 2017;Zhang and Bansal, 2019;Shen et al., 2019a) to alleviate the model bias towards safe questions.","1. (How can NR models trained with heuristics or QG match fully-supervised performance?): sent1
    1.1. (What is the biggest challenge in using heuristics or QG for new domains?): sent2
        1.1.1. (What is a general solution to the challenge of selecting suitable heuristics or QG?): sent3
    1.2. (Which heuristics often perform the best in practice?): sent4
        1.2.1. (What is the limitation of hyperlink-based approaches?): sent5
    1.3. (Why are QG-based WS signals often preferred over heuristics-based ones?): sent6
        1.3.1. (What is a challenge in obtaining high-performing QG?): sent7
            1.3.1.1. (What is one big challenge related to QG?): sent8
                1.3.1.1.1. (What happens under the one-to-many mapping relations between questions and documents?): sent9
                    1.3.1.1.1.1. (What did Shinoda et al. (2021) find about QG?): sent10
                    1.3.1.1.1.2. (What techniques are needed to alleviate the model bias towards safe questions?): sent11",What techniques are needed to alleviate the model bias towards safe questions?,"sent9, sent10, sent11","Under this situation, standard supervised learning tends to produce safe questions with less diversity and high lexical overlap with the document.

For example, Shinoda et al. (2021) found that QG reinforces the model bias towards high lexical overlap.

We will need more sophisticated training techniques such as latent-variable models (Shen and Su, 2018; and reinforcement learning (Yuan et al., 2017;Zhang and Bansal, 2019;Shen et al., 2019a) to alleviate the model bias towards safe questions.","Standard supervised learning tends to produce safe questions with less diversity and high lexical overlap with the document.

For example, Shinoda et al. (2021) found that question generation (QG) reinforces the model bias towards high lexical overlap.

More sophisticated training techniques such as latent-variable models (Shen and Su, 2018) and reinforcement learning (Yuan et al., 2017; Zhang and Bansal, 2019; Shen et al., 2019a) are needed to alleviate the model bias towards safe questions.","Questions:

What techniques are needed to alleviate the model bias towards safe questions?

Answer:

Standard supervised learning tends to produce safe questions with less diversity and high lexical overlap with the document.

For example, Shinoda et al. (2021) found that question generation (QG) reinforces the model bias towards high lexical overlap.

More sophisticated training techniques such as latent-variable models (Shen and Su, 2018) and reinforcement learning (Yuan et al., 2017; Zhang and Bansal, 2019; Shen et al., 2019a) are needed to alleviate the model bias towards safe questions."
Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Resource: Documents + Questions,"This section includes WS signals that require additional access to a question set Q. In practice, annotating question-document relations usually requires domain experts to read long documents and careful sampling strategies to ensure enough positive samples, while unlabeled questions are much easier to obtain either through real user-generated content or simulated traffic. Therefore, it is common to have a predominance of unlabeled questions. The crucial point is to establish the missing relevance labels. Suppose a WS method can provide the missing label WS(q, d) for a question-document pair (q, d), then we can use it to supervise the NR model by:

where L is the loss function that encourages similarity between R(q, d) and WS(q, d).

There are three popular types models that can provide such WS signal here: (1) sparse retriever, (2) pre-trained language model and (3) supervised teacher model.

Sparse Retriever (SR) Recent research finds that NR and SR models are complementary. NR models are better at semantic matching while SRs are better at capturing exact match and handling long documents . SRs are also more robust across domains (Thakur et al., 2021;Chen et al., 2022). This motivates the use of unsupervised sparse retrievers like BM25 as WS signals. Pre-trained Language Model (PLM) As PLMs already encode significant linguistic knowledge, there have also been attempts at using promptbased PLMs to provide WS signals for question-document relations (Smith et al., 2022;Zeng et al., 2022). Similar as in question generation, we can use prompts like ""Please write a question based on this passage"", concatenate the document and question, then use the probability assigned by the PLM to auto-label question-document pairs. To maximize the chances of finding positive document, normally we first obtain a set of candidate documents by BM25, then apply PLM to auto-label the candidate set (Sachan et al., 2022). This can further exploit the latent knowledge inside PLMs that has been honed through pre-training, so it often shows better performance compared with weak supervision only using BM25 Singh Sachan et al., 2022).

Supervised Teacher Model A very common choice is using a supervised teacher model to provide WS signals. The teacher model is ""supervised"" because it is explicitly fine-tuned on annotated question-document pairs. When in-domain annotations are not sufficient, we can leverage outof-domain (OOD) annotations, if available, to train the teacher model. The teacher model usually employs a more powerful architecture such as with more complex interactions or larger sizes. It may not be directly applicable in downstream tasks due to the latency constraints, but can be useful in providing WS signals for training the NR model. For example, previous research has shown that models with larger sizes or late/cross-interaction structures generalize much better on OOD data Ni et al., 2021;Rosa et al., 2022;Muennighoff, 2022;. After training a teacher model on OOD annotations, applying it to provide WS signals through targetdomain question and document collections can significantly improve the in-domain performance of the NR model (Hofstätter et al., 2021;Lin et al., 2021b;Lu et al., 2022). Kim et al. (2022) further show that we can even use the same architecture and capacity to obtain a good teacher model. They expand the question with centroids of word embeddings from top retrieved passages (using BM25), and then use the expanded query for self knowledge distillation. Similar ideas of reusing the same architecture to provide WS signals have also been explored by Yu et al. (2021a); Kulshreshtha et al. (2021); Zhuang and Zuccon (2022).","1. What does this section include? (sent1)
    1.1. Why is it common to have a predominance of unlabeled questions? (sent2)
    1.2. What is the crucial point in dealing with unlabeled questions? (sent3)
    1.3. How can a WS method provide the missing relevance labels? (sent4)
        1.3.1. What are the popular types of models that can provide WS signals? (sent5)
            1.3.1.1. What is a Sparse Retriever (SR)? (sent6)
                1.3.1.1.1. How do NR and SR models complement each other? (sent7)
                1.3.1.1.2. What is another advantage of SRs? (sent8)
                1.3.1.1.3. What motivates the use of unsupervised sparse retrievers like BM25? (sent9)
            1.3.1.2. What is a Pre-trained Language Model (PLM)? (sent10)
                1.3.1.2.1. How can PLMs be used to provide WS signals? (sent11)
                1.3.1.2.2. How can prompts be used with PLMs for auto-labeling question-document pairs? (sent12)
                1.3.1.2.3. How can we maximize the chances of finding positive documents using PLMs? (sent13)
                1.3.1.2.4. Why does using PLMs often show better performance compared to using only BM25? (sent14)
            1.3.1.3. What is a Supervised Teacher Model? (sent15)
                1.3.1.3.1. Why is the teacher model ""supervised""? (sent16)
                1.3.1.3.2. What can be done when in-domain annotations are not sufficient? (sent17)
                1.3.1.3.3. What kind of architecture does the teacher model usually employ? (sent18)
                1.3.1.3.4. Why might the teacher model not be directly applicable in downstream tasks? (sent19)
                1.3.1.3.5. How do larger or more complex models perform on OOD data? (sent20)
                1.3.1.3.6. How can training a teacher model on OOD annotations improve in-domain performance? (sent21)
                1.3.1.3.7. What did Kim et al. (2022) show about obtaining a good teacher model? (sent22)
                1.3.1.3.8. What similar ideas have been explored by other researchers? (sent23, sent24)",What are the popular types of models that can provide weak supervision (WS) signals for question-document pairs?,"sent5, sent6, sent10, sent15","There are three popular types models that can provide such WS signal here: (1) sparse retriever, (2) pre-trained language model and (3) supervised teacher model.

Sparse Retriever (SR) Recent research finds that NR and SR models are complementary.

Pre-trained Language Model (PLM)

Supervised Teacher Model A very common choice is using a supervised teacher model to provide WS signals.","There are three popular types of models that can provide weak supervision (WS) signals for question-document pairs: (1) sparse retriever, (2) pre-trained language model, and (3) supervised teacher model.

Recent research finds that sparse retriever (SR) models and neural retriever (NR) models are complementary.

Pre-trained language models (PLMs) are another type of model that can provide weak supervision signals for question-document pairs.

A very common choice is using a supervised teacher model to provide WS signals.","Questions:

What are the popular types of models that can provide weak supervision (WS) signals for question-document pairs?

Answer:

There are three popular types of models that can provide weak supervision (WS) signals for question-document pairs: (1) sparse retriever, (2) pre-trained language model, and (3) supervised teacher model.

Recent research finds that sparse retriever (SR) models and neural retriever (NR) models are complementary.

Pre-trained language models (PLMs) are another type of model that can provide weak supervision signals for question-document pairs.

A very common choice is using a supervised teacher model to provide WS signals."
Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Resource: Documents + Questions,"This section includes WS signals that require additional access to a question set Q. In practice, annotating question-document relations usually requires domain experts to read long documents and careful sampling strategies to ensure enough positive samples, while unlabeled questions are much easier to obtain either through real user-generated content or simulated traffic. Therefore, it is common to have a predominance of unlabeled questions. The crucial point is to establish the missing relevance labels. Suppose a WS method can provide the missing label WS(q, d) for a question-document pair (q, d), then we can use it to supervise the NR model by:

where L is the loss function that encourages similarity between R(q, d) and WS(q, d).

There are three popular types models that can provide such WS signal here: (1) sparse retriever, (2) pre-trained language model and (3) supervised teacher model.

Sparse Retriever (SR) Recent research finds that NR and SR models are complementary. NR models are better at semantic matching while SRs are better at capturing exact match and handling long documents . SRs are also more robust across domains (Thakur et al., 2021;Chen et al., 2022). This motivates the use of unsupervised sparse retrievers like BM25 as WS signals. Pre-trained Language Model (PLM) As PLMs already encode significant linguistic knowledge, there have also been attempts at using promptbased PLMs to provide WS signals for question-document relations (Smith et al., 2022;Zeng et al., 2022). Similar as in question generation, we can use prompts like ""Please write a question based on this passage"", concatenate the document and question, then use the probability assigned by the PLM to auto-label question-document pairs. To maximize the chances of finding positive document, normally we first obtain a set of candidate documents by BM25, then apply PLM to auto-label the candidate set (Sachan et al., 2022). This can further exploit the latent knowledge inside PLMs that has been honed through pre-training, so it often shows better performance compared with weak supervision only using BM25 Singh Sachan et al., 2022).

Supervised Teacher Model A very common choice is using a supervised teacher model to provide WS signals. The teacher model is ""supervised"" because it is explicitly fine-tuned on annotated question-document pairs. When in-domain annotations are not sufficient, we can leverage outof-domain (OOD) annotations, if available, to train the teacher model. The teacher model usually employs a more powerful architecture such as with more complex interactions or larger sizes. It may not be directly applicable in downstream tasks due to the latency constraints, but can be useful in providing WS signals for training the NR model. For example, previous research has shown that models with larger sizes or late/cross-interaction structures generalize much better on OOD data Ni et al., 2021;Rosa et al., 2022;Muennighoff, 2022;. After training a teacher model on OOD annotations, applying it to provide WS signals through targetdomain question and document collections can significantly improve the in-domain performance of the NR model (Hofstätter et al., 2021;Lin et al., 2021b;Lu et al., 2022). Kim et al. (2022) further show that we can even use the same architecture and capacity to obtain a good teacher model. They expand the question with centroids of word embeddings from top retrieved passages (using BM25), and then use the expanded query for self knowledge distillation. Similar ideas of reusing the same architecture to provide WS signals have also been explored by Yu et al. (2021a); Kulshreshtha et al. (2021); Zhuang and Zuccon (2022).","1. What does this section include? (sent1)
    1.1. Why is it common to have a predominance of unlabeled questions? (sent2)
    1.2. What is the crucial point in dealing with unlabeled questions? (sent3)
    1.3. How can a WS method provide the missing relevance labels? (sent4)
        1.3.1. What are the popular types of models that can provide WS signals? (sent5)
            1.3.1.1. What is a Sparse Retriever (SR)? (sent6)
                1.3.1.1.1. How do NR and SR models complement each other? (sent7)
                1.3.1.1.2. What is another advantage of SRs? (sent8)
                1.3.1.1.3. What motivates the use of unsupervised sparse retrievers like BM25? (sent9)
            1.3.1.2. What is a Pre-trained Language Model (PLM)? (sent10)
                1.3.1.2.1. How can PLMs be used to provide WS signals? (sent11)
                1.3.1.2.2. How can prompts be used with PLMs for auto-labeling question-document pairs? (sent12)
                1.3.1.2.3. How can we maximize the chances of finding positive documents using PLMs? (sent13)
                1.3.1.2.4. Why does using PLMs often show better performance compared to using only BM25? (sent14)
            1.3.1.3. What is a Supervised Teacher Model? (sent15)
                1.3.1.3.1. Why is the teacher model ""supervised""? (sent16)
                1.3.1.3.2. What can be done when in-domain annotations are not sufficient? (sent17)
                1.3.1.3.3. What kind of architecture does the teacher model usually employ? (sent18)
                1.3.1.3.4. Why might the teacher model not be directly applicable in downstream tasks? (sent19)
                1.3.1.3.5. How do larger or more complex models perform on OOD data? (sent20)
                1.3.1.3.6. How can training a teacher model on OOD annotations improve in-domain performance? (sent21)
                1.3.1.3.7. What did Kim et al. (2022) show about obtaining a good teacher model? (sent22)
                1.3.1.3.8. What similar ideas have been explored by other researchers? (sent23, sent24)",How do NR and SR models complement each other in neural ranking?,"sent6, sent7, sent8","Sparse Retriever (SR) Recent research finds that NR and SR models are complementary.

NR models are better at semantic matching while SRs are better at capturing exact match and handling long documents .

SRs are also more robust across domains (Thakur et al., 2021;Chen et al., 2022).","Recent research finds that NR and SR models are complementary.

NR models are better at semantic matching, while SR models excel at capturing exact matches and handling long documents.

SR models are also more robust across domains (Thakur et al., 2021; Chen et al., 2022).","Questions:

How do NR and SR models complement each other in neural ranking?

Answer:

Recent research finds that NR and SR models are complementary.

NR models are better at semantic matching, while SR models excel at capturing exact matches and handling long documents.

SR models are also more robust across domains (Thakur et al., 2021; Chen et al., 2022)."
Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Resource: Documents + Questions,"This section includes WS signals that require additional access to a question set Q. In practice, annotating question-document relations usually requires domain experts to read long documents and careful sampling strategies to ensure enough positive samples, while unlabeled questions are much easier to obtain either through real user-generated content or simulated traffic. Therefore, it is common to have a predominance of unlabeled questions. The crucial point is to establish the missing relevance labels. Suppose a WS method can provide the missing label WS(q, d) for a question-document pair (q, d), then we can use it to supervise the NR model by:

where L is the loss function that encourages similarity between R(q, d) and WS(q, d).

There are three popular types models that can provide such WS signal here: (1) sparse retriever, (2) pre-trained language model and (3) supervised teacher model.

Sparse Retriever (SR) Recent research finds that NR and SR models are complementary. NR models are better at semantic matching while SRs are better at capturing exact match and handling long documents . SRs are also more robust across domains (Thakur et al., 2021;Chen et al., 2022). This motivates the use of unsupervised sparse retrievers like BM25 as WS signals. Pre-trained Language Model (PLM) As PLMs already encode significant linguistic knowledge, there have also been attempts at using promptbased PLMs to provide WS signals for question-document relations (Smith et al., 2022;Zeng et al., 2022). Similar as in question generation, we can use prompts like ""Please write a question based on this passage"", concatenate the document and question, then use the probability assigned by the PLM to auto-label question-document pairs. To maximize the chances of finding positive document, normally we first obtain a set of candidate documents by BM25, then apply PLM to auto-label the candidate set (Sachan et al., 2022). This can further exploit the latent knowledge inside PLMs that has been honed through pre-training, so it often shows better performance compared with weak supervision only using BM25 Singh Sachan et al., 2022).

Supervised Teacher Model A very common choice is using a supervised teacher model to provide WS signals. The teacher model is ""supervised"" because it is explicitly fine-tuned on annotated question-document pairs. When in-domain annotations are not sufficient, we can leverage outof-domain (OOD) annotations, if available, to train the teacher model. The teacher model usually employs a more powerful architecture such as with more complex interactions or larger sizes. It may not be directly applicable in downstream tasks due to the latency constraints, but can be useful in providing WS signals for training the NR model. For example, previous research has shown that models with larger sizes or late/cross-interaction structures generalize much better on OOD data Ni et al., 2021;Rosa et al., 2022;Muennighoff, 2022;. After training a teacher model on OOD annotations, applying it to provide WS signals through targetdomain question and document collections can significantly improve the in-domain performance of the NR model (Hofstätter et al., 2021;Lin et al., 2021b;Lu et al., 2022). Kim et al. (2022) further show that we can even use the same architecture and capacity to obtain a good teacher model. They expand the question with centroids of word embeddings from top retrieved passages (using BM25), and then use the expanded query for self knowledge distillation. Similar ideas of reusing the same architecture to provide WS signals have also been explored by Yu et al. (2021a); Kulshreshtha et al. (2021); Zhuang and Zuccon (2022).","1. What does this section include? (sent1)
    1.1. Why is it common to have a predominance of unlabeled questions? (sent2)
    1.2. What is the crucial point in dealing with unlabeled questions? (sent3)
    1.3. How can a WS method provide the missing relevance labels? (sent4)
        1.3.1. What are the popular types of models that can provide WS signals? (sent5)
            1.3.1.1. What is a Sparse Retriever (SR)? (sent6)
                1.3.1.1.1. How do NR and SR models complement each other? (sent7)
                1.3.1.1.2. What is another advantage of SRs? (sent8)
                1.3.1.1.3. What motivates the use of unsupervised sparse retrievers like BM25? (sent9)
            1.3.1.2. What is a Pre-trained Language Model (PLM)? (sent10)
                1.3.1.2.1. How can PLMs be used to provide WS signals? (sent11)
                1.3.1.2.2. How can prompts be used with PLMs for auto-labeling question-document pairs? (sent12)
                1.3.1.2.3. How can we maximize the chances of finding positive documents using PLMs? (sent13)
                1.3.1.2.4. Why does using PLMs often show better performance compared to using only BM25? (sent14)
            1.3.1.3. What is a Supervised Teacher Model? (sent15)
                1.3.1.3.1. Why is the teacher model ""supervised""? (sent16)
                1.3.1.3.2. What can be done when in-domain annotations are not sufficient? (sent17)
                1.3.1.3.3. What kind of architecture does the teacher model usually employ? (sent18)
                1.3.1.3.4. Why might the teacher model not be directly applicable in downstream tasks? (sent19)
                1.3.1.3.5. How do larger or more complex models perform on OOD data? (sent20)
                1.3.1.3.6. How can training a teacher model on OOD annotations improve in-domain performance? (sent21)
                1.3.1.3.7. What did Kim et al. (2022) show about obtaining a good teacher model? (sent22)
                1.3.1.3.8. What similar ideas have been explored by other researchers? (sent23, sent24)",How can pre-trained language models (PLMs) be used to provide WS signals for question-document relations?,"sent11, sent12, sent13, sent14","As PLMs already encode significant linguistic knowledge, there have also been attempts at using promptbased PLMs to provide WS signals for question-document relations (Smith et al., 2022;Zeng et al., 2022).

Similar as in question generation, we can use prompts like ""Please write a question based on this passage"", concatenate the document and question, then use the probability assigned by the PLM to auto-label question-document pairs.

To maximize the chances of finding positive document, normally we first obtain a set of candidate documents by BM25, then apply PLM to auto-label the candidate set (Sachan et al., 2022).

This can further exploit the latent knowledge inside PLMs that has been honed through pre-training, so it often shows better performance compared with weak supervision only using BM25 Singh Sachan et al., 2022).","Pre-trained language models (PLMs) have been used to provide WS signals for question-document relations by leveraging their encoded linguistic knowledge (Smith et al., 2022; Zeng et al., 2022).

One approach involves using prompts like ""Please write a question based on this passage,"" concatenating the document and question, and then using the probability assigned by the PLM to auto-label question-document pairs.

To maximize the chances of finding positive documents, a set of candidate documents is first obtained by BM25, and then the PLM is applied to auto-label the candidate set (Sachan et al., 2022).

This approach can further exploit the latent knowledge inside PLMs that has been honed through pre-training, often showing better performance compared with weak supervision only using BM25 (Singh Sachan et al., 2022).","Questions:

How can pre-trained language models (PLMs) be used to provide WS signals for question-document relations?

Answer:

Pre-trained language models (PLMs) have been used to provide WS signals for question-document relations by leveraging their encoded linguistic knowledge (Smith et al., 2022; Zeng et al., 2022).

One approach involves using prompts like ""Please write a question based on this passage,"" concatenating the document and question, and then using the probability assigned by the PLM to auto-label question-document pairs.

To maximize the chances of finding positive documents, a set of candidate documents is first obtained by BM25, and then the PLM is applied to auto-label the candidate set (Sachan et al., 2022).

This approach can further exploit the latent knowledge inside PLMs that has been honed through pre-training, often showing better performance compared with weak supervision only using BM25 (Singh Sachan et al., 2022)."
Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Resource: Documents + Questions,"This section includes WS signals that require additional access to a question set Q. In practice, annotating question-document relations usually requires domain experts to read long documents and careful sampling strategies to ensure enough positive samples, while unlabeled questions are much easier to obtain either through real user-generated content or simulated traffic. Therefore, it is common to have a predominance of unlabeled questions. The crucial point is to establish the missing relevance labels. Suppose a WS method can provide the missing label WS(q, d) for a question-document pair (q, d), then we can use it to supervise the NR model by:

where L is the loss function that encourages similarity between R(q, d) and WS(q, d).

There are three popular types models that can provide such WS signal here: (1) sparse retriever, (2) pre-trained language model and (3) supervised teacher model.

Sparse Retriever (SR) Recent research finds that NR and SR models are complementary. NR models are better at semantic matching while SRs are better at capturing exact match and handling long documents . SRs are also more robust across domains (Thakur et al., 2021;Chen et al., 2022). This motivates the use of unsupervised sparse retrievers like BM25 as WS signals. Pre-trained Language Model (PLM) As PLMs already encode significant linguistic knowledge, there have also been attempts at using promptbased PLMs to provide WS signals for question-document relations (Smith et al., 2022;Zeng et al., 2022). Similar as in question generation, we can use prompts like ""Please write a question based on this passage"", concatenate the document and question, then use the probability assigned by the PLM to auto-label question-document pairs. To maximize the chances of finding positive document, normally we first obtain a set of candidate documents by BM25, then apply PLM to auto-label the candidate set (Sachan et al., 2022). This can further exploit the latent knowledge inside PLMs that has been honed through pre-training, so it often shows better performance compared with weak supervision only using BM25 Singh Sachan et al., 2022).

Supervised Teacher Model A very common choice is using a supervised teacher model to provide WS signals. The teacher model is ""supervised"" because it is explicitly fine-tuned on annotated question-document pairs. When in-domain annotations are not sufficient, we can leverage outof-domain (OOD) annotations, if available, to train the teacher model. The teacher model usually employs a more powerful architecture such as with more complex interactions or larger sizes. It may not be directly applicable in downstream tasks due to the latency constraints, but can be useful in providing WS signals for training the NR model. For example, previous research has shown that models with larger sizes or late/cross-interaction structures generalize much better on OOD data Ni et al., 2021;Rosa et al., 2022;Muennighoff, 2022;. After training a teacher model on OOD annotations, applying it to provide WS signals through targetdomain question and document collections can significantly improve the in-domain performance of the NR model (Hofstätter et al., 2021;Lin et al., 2021b;Lu et al., 2022). Kim et al. (2022) further show that we can even use the same architecture and capacity to obtain a good teacher model. They expand the question with centroids of word embeddings from top retrieved passages (using BM25), and then use the expanded query for self knowledge distillation. Similar ideas of reusing the same architecture to provide WS signals have also been explored by Yu et al. (2021a); Kulshreshtha et al. (2021); Zhuang and Zuccon (2022).","1. What does this section include? (sent1)
    1.1. Why is it common to have a predominance of unlabeled questions? (sent2)
    1.2. What is the crucial point in dealing with unlabeled questions? (sent3)
    1.3. How can a WS method provide the missing relevance labels? (sent4)
        1.3.1. What are the popular types of models that can provide WS signals? (sent5)
            1.3.1.1. What is a Sparse Retriever (SR)? (sent6)
                1.3.1.1.1. How do NR and SR models complement each other? (sent7)
                1.3.1.1.2. What is another advantage of SRs? (sent8)
                1.3.1.1.3. What motivates the use of unsupervised sparse retrievers like BM25? (sent9)
            1.3.1.2. What is a Pre-trained Language Model (PLM)? (sent10)
                1.3.1.2.1. How can PLMs be used to provide WS signals? (sent11)
                1.3.1.2.2. How can prompts be used with PLMs for auto-labeling question-document pairs? (sent12)
                1.3.1.2.3. How can we maximize the chances of finding positive documents using PLMs? (sent13)
                1.3.1.2.4. Why does using PLMs often show better performance compared to using only BM25? (sent14)
            1.3.1.3. What is a Supervised Teacher Model? (sent15)
                1.3.1.3.1. Why is the teacher model ""supervised""? (sent16)
                1.3.1.3.2. What can be done when in-domain annotations are not sufficient? (sent17)
                1.3.1.3.3. What kind of architecture does the teacher model usually employ? (sent18)
                1.3.1.3.4. Why might the teacher model not be directly applicable in downstream tasks? (sent19)
                1.3.1.3.5. How do larger or more complex models perform on OOD data? (sent20)
                1.3.1.3.6. How can training a teacher model on OOD annotations improve in-domain performance? (sent21)
                1.3.1.3.7. What did Kim et al. (2022) show about obtaining a good teacher model? (sent22)
                1.3.1.3.8. What similar ideas have been explored by other researchers? (sent23, sent24)",What is a supervised teacher model and how does it provide WS signals?,"sent15, sent16, sent17, sent18, sent19, sent20, sent21, sent22, sent23, sent24","Supervised Teacher Model A very common choice is using a supervised teacher model to provide WS signals.

The teacher model is ""supervised"" because it is explicitly fine-tuned on annotated question-document pairs.

When in-domain annotations are not sufficient, we can leverage outof-domain (OOD) annotations, if available, to train the teacher model.

The teacher model usually employs a more powerful architecture such as with more complex interactions or larger sizes.

It may not be directly applicable in downstream tasks due to the latency constraints, but can be useful in providing WS signals for training the NR model.

For example, previous research has shown that models with larger sizes or late/cross-interaction structures generalize much better on OOD data Ni et al., 2021;Rosa et al., 2022;Muennighoff, 2022;.

After training a teacher model on OOD annotations, applying it to provide WS signals through targetdomain question and document collections can significantly improve the in-domain performance of the NR model (Hofstätter et al., 2021;Lin et al., 2021b;Lu et al., 2022).

Kim et al. (2022) further show that we can even use the same architecture and capacity to obtain a good teacher model.

They expand the question with centroids of word embeddings from top retrieved passages (using BM25), and then use the expanded query for self knowledge distillation.

Similar ideas of reusing the same architecture to provide WS signals have also been explored by Yu et al. (2021a); Kulshreshtha et al. (2021); Zhuang and Zuccon (2022).","A very common choice is using a supervised teacher model to provide WS signals.

The teacher model is considered ""supervised"" because it is explicitly fine-tuned on annotated question-document pairs.

When in-domain annotations are not sufficient, out-of-domain (OOD) annotations, if available, can be leveraged to train the teacher model.

The teacher model usually employs a more powerful architecture, such as with more complex interactions or larger sizes.

The teacher model may not be directly applicable in downstream tasks due to latency constraints, but it can be useful in providing WS signals for training the NR model.

For example, previous research has shown that models with larger sizes or late/cross-interaction structures generalize much better on OOD data (Ni et al., 2021; Rosa et al., 2022; Muennighoff, 2022).

After training a teacher model on OOD annotations, applying it to provide WS signals through target-domain question and document collections can significantly improve the in-domain performance of the NR model (Hofstätter et al., 2021; Lin et al., 2021b; Lu et al., 2022).

Kim et al. (2022) further show that using the same architecture and capacity can also yield an effective teacher model.

For instance, Kim et al. (2022) expand the question with centroids of word embeddings from top retrieved passages (using BM25), and then use the expanded query for self-knowledge distillation.

Similar ideas of reusing the same architecture to provide WS signals have also been explored by Yu et al. (2021a); Kulshreshtha et al. (2021); Zhuang and Zuccon (2022).","Questions:

What is a supervised teacher model and how does it provide WS signals?

Answer:

A very common choice is using a supervised teacher model to provide WS signals.

The teacher model is considered ""supervised"" because it is explicitly fine-tuned on annotated question-document pairs.

When in-domain annotations are not sufficient, out-of-domain (OOD) annotations, if available, can be leveraged to train the teacher model.

The teacher model usually employs a more powerful architecture, such as with more complex interactions or larger sizes.

The teacher model may not be directly applicable in downstream tasks due to latency constraints, but it can be useful in providing WS signals for training the NR model.

For example, previous research has shown that models with larger sizes or late/cross-interaction structures generalize much better on OOD data (Ni et al., 2021; Rosa et al., 2022; Muennighoff, 2022).

After training a teacher model on OOD annotations, applying it to provide WS signals through target-domain question and document collections can significantly improve the in-domain performance of the NR model (Hofstätter et al., 2021; Lin et al., 2021b; Lu et al., 2022).

Kim et al. (2022) further show that using the same architecture and capacity can also yield an effective teacher model.

For instance, Kim et al. (2022) expand the question with centroids of word embeddings from top retrieved passages (using BM25), and then use the expanded query for self-knowledge distillation.

Similar ideas of reusing the same architecture to provide WS signals have also been explored by Yu et al. (2021a); Kulshreshtha et al. (2021); Zhuang and Zuccon (2022)."
Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Discussion,"The three WS signals listed above work directly on actual questions instead of pseudo pairs as in §3 so that the NR model can adapt better to the target-domain question distribution. The bottleneck is the quality of the WS signals. SRs and PLMs are unsupervised, which could be more robust when we face a completely different domain (Dai et al., 2022). Otherwise, if we already have certain amounts relevance annotations from the target or similar domains, usually using a supervised teacher model is preferred. Nevertheless, these WS signals inevitably contain noise, and can harm the downstream performance if the noise is significant. There are two main strategies to reduce the noise effects: (1) Apply less strict margin-based loss such as the hinge loss (Dehghani et al., 2017;Xu et al., 2019) and MarginMSE loss (Hofstätter et al., 2020;, then models have fewer chances of overfitting to the exact labels, and (2) Apply noise-resistant training methods such as confidence-based filtering (Mukherjee and Awadallah, 2020;Yu et al., 2021b) and metalearning-based refinement (Ren et al., 2018;Zhu et al., 2022). Another potential issue is that the amount of training data in this section relies on the amount of questions we have. Unlike the document set which we can obtain for free, the question set takes time to collect and are often orders of magnitudes smaller. If no sufficient questions are available, we can use synthetic questions from question generation, then apply same WS signals in this section, which has been shown to perform on par with using real questions in certain domains Thakur et al., 2022).","1. (How can NR models trained with heuristics or QG match fully-supervised performance?): sent1
    1.1. (What is the biggest challenge in using heuristics or QG for new domains?): sent2
        1.1.1. (What is a general solution to the challenge of selecting suitable heuristics or QG?): sent3
    1.2. (Which heuristics often perform the best in practice?): sent4
        1.2.1. (What is the limitation of hyperlink-based approaches?): sent5
    1.3. (Why are QG-based WS signals often preferred over heuristics-based ones?): sent6
        1.3.1. (What is a challenge in obtaining high-performing QG?): sent7
            1.3.1.1. (What is one big challenge related to QG?): sent8
                1.3.1.1.1. (What happens under the one-to-many mapping relations between questions and documents?): sent9
                    1.3.1.1.1.1. (What did Shinoda et al. (2021) find about QG?): sent10
                    1.3.1.1.1.2. (What techniques are needed to alleviate the model bias towards safe questions?): sent11",How can the noise effects of WS signals be reduced in NR models?,"sent6, sent7, sent8","There are two main strategies to reduce the noise effects: (1)

Apply less strict margin-based loss such as the hinge loss (Dehghani et al., 2017;Xu et al., 2019) and MarginMSE loss (Hofstätter et al., 2020;, then models have fewer chances of overfitting to the exact labels, and (2)

Apply noise-resistant training methods such as confidence-based filtering (Mukherjee and Awadallah, 2020;Yu et al., 2021b) and metalearning-based refinement (Ren et al., 2018;Zhu et al., 2022).","Two main strategies can be employed to reduce the noise effects of WS signals in NR models: (1)

Apply less strict margin-based loss such as the hinge loss (Dehghani et al., 2017; Xu et al., 2019) and MarginMSE loss (Hofstätter et al., 2020), then models have fewer chances of overfitting to the exact labels, and (2)

Apply noise-resistant training methods such as confidence-based filtering (Mukherjee and Awadallah, 2020; Yu et al., 2021b) and metalearning-based refinement (Ren et al., 2018; Zhu et al., 2022).","Questions:

How can the noise effects of WS signals be reduced in NR models?

Answer:

Two main strategies can be employed to reduce the noise effects of WS signals in NR models: (1)

Apply less strict margin-based loss such as the hinge loss (Dehghani et al., 2017; Xu et al., 2019) and MarginMSE loss (Hofstätter et al., 2020), then models have fewer chances of overfitting to the exact labels, and (2)

Apply noise-resistant training methods such as confidence-based filtering (Mukherjee and Awadallah, 2020; Yu et al., 2021b) and metalearning-based refinement (Ren et al., 2018; Zhu et al., 2022)."
Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,Discussion,"The three WS signals listed above work directly on actual questions instead of pseudo pairs as in §3 so that the NR model can adapt better to the target-domain question distribution. The bottleneck is the quality of the WS signals. SRs and PLMs are unsupervised, which could be more robust when we face a completely different domain (Dai et al., 2022). Otherwise, if we already have certain amounts relevance annotations from the target or similar domains, usually using a supervised teacher model is preferred. Nevertheless, these WS signals inevitably contain noise, and can harm the downstream performance if the noise is significant. There are two main strategies to reduce the noise effects: (1) Apply less strict margin-based loss such as the hinge loss (Dehghani et al., 2017;Xu et al., 2019) and MarginMSE loss (Hofstätter et al., 2020;, then models have fewer chances of overfitting to the exact labels, and (2) Apply noise-resistant training methods such as confidence-based filtering (Mukherjee and Awadallah, 2020;Yu et al., 2021b) and metalearning-based refinement (Ren et al., 2018;Zhu et al., 2022). Another potential issue is that the amount of training data in this section relies on the amount of questions we have. Unlike the document set which we can obtain for free, the question set takes time to collect and are often orders of magnitudes smaller. If no sufficient questions are available, we can use synthetic questions from question generation, then apply same WS signals in this section, which has been shown to perform on par with using real questions in certain domains Thakur et al., 2022).","1. (How can NR models trained with heuristics or QG match fully-supervised performance?): sent1
    1.1. (What is the biggest challenge in using heuristics or QG for new domains?): sent2
        1.1.1. (What is a general solution to the challenge of selecting suitable heuristics or QG?): sent3
    1.2. (Which heuristics often perform the best in practice?): sent4
        1.2.1. (What is the limitation of hyperlink-based approaches?): sent5
    1.3. (Why are QG-based WS signals often preferred over heuristics-based ones?): sent6
        1.3.1. (What is a challenge in obtaining high-performing QG?): sent7
            1.3.1.1. (What is one big challenge related to QG?): sent8
                1.3.1.1.1. (What happens under the one-to-many mapping relations between questions and documents?): sent9
                    1.3.1.1.1.1. (What did Shinoda et al. (2021) find about QG?): sent10
                    1.3.1.1.1.2. (What techniques are needed to alleviate the model bias towards safe questions?): sent11",What are the strategies to reduce noise effects in WS signals?,"sent6, sent7, sent8","There are two main strategies to reduce the noise effects: (1)

Apply less strict margin-based loss such as the hinge loss (Dehghani et al., 2017;Xu et al., 2019) and MarginMSE loss (Hofstätter et al., 2020;, then models have fewer chances of overfitting to the exact labels, and (2)

Apply noise-resistant training methods such as confidence-based filtering (Mukherjee and Awadallah, 2020;Yu et al., 2021b) and metalearning-based refinement (Ren et al., 2018;Zhu et al., 2022).","There are two main strategies to reduce the noise effects in WS signals: (1)

Apply less strict margin-based loss such as the hinge loss (Dehghani et al., 2017; Xu et al., 2019) and MarginMSE loss (Hofstätter et al., 2020), then models have fewer chances of overfitting to the exact labels, and (2)

Apply noise-resistant training methods such as confidence-based filtering (Mukherjee and Awadallah, 2020; Yu et al., 2021b) and metalearning-based refinement (Ren et al., 2018; Zhu et al., 2022).","Questions:

What are the strategies to reduce noise effects in WS signals?

Answer:

There are two main strategies to reduce the noise effects in WS signals: (1)

Apply less strict margin-based loss such as the hinge loss (Dehghani et al., 2017; Xu et al., 2019) and MarginMSE loss (Hofstätter et al., 2020), then models have fewer chances of overfitting to the exact labels, and (2)

Apply noise-resistant training methods such as confidence-based filtering (Mukherjee and Awadallah, 2020; Yu et al., 2021b) and metalearning-based refinement (Ren et al., 2018; Zhu et al., 2022)."
Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,Decoding with Feedback Models,"As mentioned, feedback models have the advantage that they can be queried cheaply for feedback once trained. Perhaps for this reason, most approaches that leverage feedback models by sampling a large number of candidate generations, and reranking them according to the feedback model:

whereĥ ϕ is a trained (numerical) feedback model and C is a set of S candidate generations given specific regularization terms, such as the KL terms in PPO (Schulman et al., 2017). by the model (for example, by sampling from its distribution multiple times).

In machine translation, Fernandes et al. (2022) andFreitag et al. (2022a) build upon recent advances in automatic quality estimation and evaluation via feedback model training to improve generation. Their framework comprises a candidate generation stage followed by a ranking stage, in which the candidates are scored using quality metrics trained to regress on human assessments (reward models) (Rei et al., 2020a,b) via N -best list reranking or minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2002). The highest-scoring candidate is then chosen as the final translation. Li et al. (2022) collected a dataset of both numerical and natural language feedback for responses from a QA system, and finetuned a pretrained model to predict both kinds of feedback, using the predicted scores from this feedback model to re-rank the predictions from the model. Gao et al. (2022) also used this approach to study the scaling properties of feedback models and the problem of ""overoptimization"" (see below).

Additionally, there are several works combining MT and APE systems at decoding time, in which the output of an MT system is further improved by an APE system (Bhattacharyya et al., 2022).

Feedback Model Overoptimization One problem that arises when optimizing a system with a feedback model is that this model is only an imperfect proxy for the ground truth human feedback, therefore, ""overoptimizing"" for them can lead to systems that receive good feedback from the model, but not humans. This problem is known as the overoptimization problem, and is the main reason for the regularization term in Equation 11 Gao et al. (2022) studies the overoptimization problem in preference models, by both optimizing against it with reinforcement learning (training) and reranking outputs with it (decoding). They found that both using preference models during training or decoding led to similar levels of overoptimization, and that the scale of the generation model helps little with this problem.","1. (What are the advantages of feedback models?): sent1
    1.1. (Why do most approaches leverage feedback models by sampling and reranking?): sent2
        1.1.1. (How do models sample candidate generations?): sent3
    1.2. (How are feedback models used in machine translation?): sent4
        1.2.1. (What is the framework used by Fernandes et al. (2022) and Freitag et al. (2022a)?): sent5
            1.2.1.1. (How is the final translation chosen?): sent6
    1.3. (How is feedback used in QA systems?): sent7
    1.4. (What other studies have used this approach?): sent8
    1.5. (What other systems combine MT and APE at decoding time?): sent9
2. (What is the overoptimization problem in feedback models?): sent10
    2.1. (Why is overoptimization a problem?): sent11
        2.1.1. (What did Gao et al. (2022) find about overoptimization?): sent12",How do feedback models improve machine translation?,"sent4, sent5, sent6","In machine translation, Fernandes et al. (2022) andFreitag et al. (2022a) build upon recent advances in automatic quality estimation and evaluation via feedback model training to improve generation.

Their framework comprises a candidate generation stage followed by a ranking stage, in which the candidates are scored using quality metrics trained to regress on human assessments (reward models) (Rei et al., 2020a,b) via N -best list reranking or minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2002).

The highest-scoring candidate is then chosen as the final translation.","In machine translation, Fernandes et al. (2022) and Freitag et al. (2022a) build upon recent advances in automatic quality estimation and evaluation via feedback model training to improve generation.

Their framework includes a candidate generation stage followed by a ranking stage, where candidates are scored using quality metrics trained to regress on human assessments (reward models) (Rei et al., 2020a,b) via N-best list reranking or minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2002).

The highest-scoring candidate is then chosen as the final translation.","Questions:

How do feedback models improve machine translation?

Answer:

In machine translation, Fernandes et al. (2022) and Freitag et al. (2022a) build upon recent advances in automatic quality estimation and evaluation via feedback model training to improve generation.

Their framework includes a candidate generation stage followed by a ranking stage, where candidates are scored using quality metrics trained to regress on human assessments (reward models) (Rei et al., 2020a,b) via N-best list reranking or minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2002).

The highest-scoring candidate is then chosen as the final translation."
Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,Decoding with Feedback Models,"As mentioned, feedback models have the advantage that they can be queried cheaply for feedback once trained. Perhaps for this reason, most approaches that leverage feedback models by sampling a large number of candidate generations, and reranking them according to the feedback model:

whereĥ ϕ is a trained (numerical) feedback model and C is a set of S candidate generations given specific regularization terms, such as the KL terms in PPO (Schulman et al., 2017). by the model (for example, by sampling from its distribution multiple times).

In machine translation, Fernandes et al. (2022) andFreitag et al. (2022a) build upon recent advances in automatic quality estimation and evaluation via feedback model training to improve generation. Their framework comprises a candidate generation stage followed by a ranking stage, in which the candidates are scored using quality metrics trained to regress on human assessments (reward models) (Rei et al., 2020a,b) via N -best list reranking or minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2002). The highest-scoring candidate is then chosen as the final translation. Li et al. (2022) collected a dataset of both numerical and natural language feedback for responses from a QA system, and finetuned a pretrained model to predict both kinds of feedback, using the predicted scores from this feedback model to re-rank the predictions from the model. Gao et al. (2022) also used this approach to study the scaling properties of feedback models and the problem of ""overoptimization"" (see below).

Additionally, there are several works combining MT and APE systems at decoding time, in which the output of an MT system is further improved by an APE system (Bhattacharyya et al., 2022).

Feedback Model Overoptimization One problem that arises when optimizing a system with a feedback model is that this model is only an imperfect proxy for the ground truth human feedback, therefore, ""overoptimizing"" for them can lead to systems that receive good feedback from the model, but not humans. This problem is known as the overoptimization problem, and is the main reason for the regularization term in Equation 11 Gao et al. (2022) studies the overoptimization problem in preference models, by both optimizing against it with reinforcement learning (training) and reranking outputs with it (decoding). They found that both using preference models during training or decoding led to similar levels of overoptimization, and that the scale of the generation model helps little with this problem.","1. (What are the advantages of feedback models?): sent1
    1.1. (Why do most approaches leverage feedback models by sampling and reranking?): sent2
        1.1.1. (How do models sample candidate generations?): sent3
    1.2. (How are feedback models used in machine translation?): sent4
        1.2.1. (What is the framework used by Fernandes et al. (2022) and Freitag et al. (2022a)?): sent5
            1.2.1.1. (How is the final translation chosen?): sent6
    1.3. (How is feedback used in QA systems?): sent7
    1.4. (What other studies have used this approach?): sent8
    1.5. (What other systems combine MT and APE at decoding time?): sent9
2. (What is the overoptimization problem in feedback models?): sent10
    2.1. (Why is overoptimization a problem?): sent11
        2.1.1. (What did Gao et al. (2022) find about overoptimization?): sent12",What is the overoptimization problem in feedback models?,"sent10, sent11, sent12","Feedback Model Overoptimization One problem that arises when optimizing a system with a feedback model is that this model is only an imperfect proxy for the ground truth human feedback, therefore, ""overoptimizing"" for them can lead to systems that receive good feedback from the model, but not humans.

This problem is known as the overoptimization problem, and is the main reason for the regularization term in Equation 11 Gao et al. (2022) studies the overoptimization problem in preference models, by both optimizing against it with reinforcement learning (training) and reranking outputs with it (decoding).

They found that both using preference models during training or decoding led to similar levels of overoptimization, and that the scale of the generation model helps little with this problem.","One problem that arises when optimizing a system with a feedback model is that this model is only an imperfect proxy for the ground truth human feedback, therefore, ""overoptimizing"" for them can lead to systems that receive good feedback from the model, but not humans.

This problem is known as the overoptimization problem, and Gao et al. (2022) study it in preference models by both optimizing against it with reinforcement learning and reranking outputs with it.

Gao et al. (2022) found that both using preference models during training or decoding led to similar levels of overoptimization, and that the scale of the generation model helps little with this problem.","Questions:

What is the overoptimization problem in feedback models?

Answer:

One problem that arises when optimizing a system with a feedback model is that this model is only an imperfect proxy for the ground truth human feedback, therefore, ""overoptimizing"" for them can lead to systems that receive good feedback from the model, but not humans.

This problem is known as the overoptimization problem, and Gao et al. (2022) study it in preference models by both optimizing against it with reinforcement learning and reranking outputs with it.

Gao et al. (2022) found that both using preference models during training or decoding led to similar levels of overoptimization, and that the scale of the generation model helps little with this problem."
Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,Collecting and Using Human Feedback,"Collecting human feedback can be rather expensive and may present i ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethical considerations in the use and collection of human feedback.

In future, richer types of feedback may be collected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007;Amershi et al., 2014a;Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022;Gehrmann et al., 2022b).","1. Why is collecting human feedback important and challenging? (Question 1): sent1
    1.1. What are the existing resources and considerations for collecting human feedback? (Question 1.1): sent2
        1.1.1. What ethical considerations are involved in the use and collection of human feedback? (Question 1.1.1): sent3
    1.2. What future directions exist for collecting and using human feedback? (Question 1.2): sent4
        1.2.1. What types of feedback do humans prefer to provide? (Question 1.2.1): sent5
        1.2.2. What is the variability between human annotators? (Question 1.2.2): sent6",What types of feedback do humans prefer to provide?,"sent1, sent4, sent5","Collecting human feedback can be rather expensive and may present i ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully.

In future, richer types of feedback may be collected and we may find ways to make use of this signal.

For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007;Amershi et al., 2014a;Ghai et al., 2021).","Collecting human feedback can be rather expensive and may present issues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully.

In the future, richer types of feedback may be collected, and ways to utilize this signal could be developed.

For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labeling (Stumpf et al., 2007; Amershi et al., 2014a; Ghai et al., 2021).","Questions:

What types of feedback do humans prefer to provide?

Answer:

Collecting human feedback can be rather expensive and may present issues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully.

In the future, richer types of feedback may be collected, and ways to utilize this signal could be developed.

For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labeling (Stumpf et al., 2007; Amershi et al., 2014a; Ghai et al., 2021)."
Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,Considerations in Data Collection,"There are multiple facets to consider when collecting human feedback data for a generation task; a non-exhaustive list of axes along which data collection can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008;Sheng et al., 2008;Clark et al., 2021;Gillick and Liu, 2010;Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020;Bai et al., 2022a;Freitag et al., 2021). 3. Collection method: Data can be gathered explicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018;Freitag et al., 2021). 4. Collection platform: Common platforms include Amazon Mechanical Turk, Upwork, and Scale AI. 5. Annotator demographics: Different groups may have varying opinions on quality generations; demographics may be collected during data collection. There is generally a trade-off between the effort needed to create the datasets and the reliability of judgments collected. For higher-stakes applications in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment.","1. What are the considerations when collecting human feedback data for a generation task? (sent1)
    1.1. What are the axes along which data collection can vary? (sent1)
        1.1.1. What is the role of annotator expertise in data collection? (sent2)
        1.1.2. How does the length of engagement affect data collection? (sent3)
        1.1.3. What are the methods of data collection? (sent4)
        1.1.4. What platforms are commonly used for data collection? (sent5)
        1.1.5. How do annotator demographics influence data collection? (sent6)
2. What is the trade-off in data collection for human feedback? (sent7)
    2.1. When is it worth the effort to consult expert annotators? (sent8)
    2.2. When is it prudent to recruit a diverse group of annotators? (sent9)",What are the key considerations when collecting human feedback data for a generation task?,"sent1, sent2, sent3, sent4, sent5, sent6, sent7, sent8, sent9","There are multiple facets to consider when collecting human feedback data for a generation task; a non-exhaustive list of axes along which data collection can vary is presented below.

1. Annotator expertise: Depending on task and training (Snow et al., 2008;Sheng et al., 2008;Clark et al., 2021;Gillick and Liu, 2010;Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models.

2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020;Bai et al., 2022a;Freitag et al., 2021).

3. Collection method: Data can be gathered explicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018;Freitag et al., 2021).

4. Collection platform: Common platforms include Amazon Mechanical Turk, Upwork, and Scale AI.

5. Annotator demographics: Different groups may have varying opinions on quality generations; demographics may be collected during data collection.

There is generally a trade-off between the effort needed to create the datasets and the reliability of judgments collected.

For higher-stakes applications in specific domains, it may be worth the effort to consult expert annotators in an extended partnership.

For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment.","There are multiple facets to consider when collecting human feedback data for a generation task.

One key consideration is annotator expertise: depending on the task and training (Snow et al., 2008; Sheng et al., 2008; Clark et al., 2021; Gillick and Liu, 2010; Freitag et al., 2021), annotators can range from domain experts to crowdworkers or even models.

Another key consideration is the length of engagement, which can involve one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020; Bai et al., 2022a; Freitag et al., 2021).

Another important consideration is the collection method: data can be gathered explicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018; Freitag et al., 2021).

Another consideration is the collection platform: common platforms include Amazon Mechanical Turk, Upwork, and Scale AI.

Another consideration is annotator demographics: different groups may have varying opinions on quality generations, and demographics may be collected during data collection.

There is generally a trade-off between the effort needed to create the datasets and the reliability of judgments collected.

For higher-stakes applications in specific domains, it may be worth the effort to consult expert annotators in an extended partnership.

For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment.","Questions:

What are the key considerations when collecting human feedback data for a generation task?

Answer:

There are multiple facets to consider when collecting human feedback data for a generation task.

One key consideration is annotator expertise: depending on the task and training (Snow et al., 2008; Sheng et al., 2008; Clark et al., 2021; Gillick and Liu, 2010; Freitag et al., 2021), annotators can range from domain experts to crowdworkers or even models.

Another key consideration is the length of engagement, which can involve one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020; Bai et al., 2022a; Freitag et al., 2021).

Another important consideration is the collection method: data can be gathered explicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018; Freitag et al., 2021).

Another consideration is the collection platform: common platforms include Amazon Mechanical Turk, Upwork, and Scale AI.

Another consideration is annotator demographics: different groups may have varying opinions on quality generations, and demographics may be collected during data collection.

There is generally a trade-off between the effort needed to create the datasets and the reliability of judgments collected.

For higher-stakes applications in specific domains, it may be worth the effort to consult expert annotators in an extended partnership.

For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment."
Large Language Models Meet NL2Code: A Survey,Benchmarks and Metrics,"To evaluate the NL2Code task, high-quality benchmarks and reliable metrics are fundamental and essential. In this section, we provide a brief overview of current benchmarks and metrics, as well as our observations and the open challenges.

We summarize 17 well-studied NL2Code benchmarks in Table 3, where we can find that each of these benchmarks has its own characteristics regarding size, language, complexity, and scenario. We observe that most benchmarks contain a limited number of instances. For example, the widely used HumanEval and MBPP have 164 and 974 instances, respectively. This is because these benchmarks are typically hand-written to ensure that LLMs have not seen them during training. In the era of large language models, it is crucial to avoid data leak-age when creating new benchmarks. Additionally, most current benchmarks have their problem descriptions in English and code solutions in Python.

Recently, several multi-lingual benchmarks have been proposed, such as MBXP (Athiwaratkun et al., 2022), HumanEvalX (Zheng et al., 2023), and Mul-tiPL (Cassano et al., 2022), which cover multiple programming languages, and ODEX (Wang et al., 2022c), which covers multiple natural languages. Details of multi-lingual benchmarks are listed in Appendix Table 7. Furthermore, benchmarks have been proposed for other practical scenarios, such as data science (Lai et al., 2022), public library (Zan et al., 2022b), private library (Zan et al., 2022a), multi-turn program synthesis (Nijkamp et al., 2023), and code security (Siddiq and msiddiq, 2022). For execution-based benchmarks, comprehensive test cases with complete coverage of the generated program can ensure the trustworthiness of evaluation results. As a reference, the average number of test cases for each benchmark, as well as the length statistics of the problem descriptions and solutions are also provided in Table 3.

Manually evaluating the generated code is impractical, which calls for the need for automatic metrics. The above mentioned benchmarks all provide test cases for execution-based evaluation, where metrics such as pass@k (Chen et al., 2021), n@k (Li et al., 2022b), test case aver-age (Hendrycks et al., 2021), and execution accuracy (Rajkumar et al., 2022) can be used. However, this approach has stringent requirements for the quality of test cases and can only evaluate executable code. For non-executable code, metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), andCodeBLEU (Ren et al., 2020) are used, while they can not precisely evaluate the correctness of the code. So far, there are many open challenges in designing metrics to evaluate various aspects of code, such as vulnerability, maintainability, clarity, execution complexity, and stability.","1. (What is the importance of benchmarks and metrics in evaluating the NL2Code task?): sent1
    1.1. (What will be covered in this section?): sent2
        1.1.1. (What are the current benchmarks and their characteristics?): sent3
            1.1.1.1. (What is a common observation about the benchmarks?): sent4
                1.1.1.1.1. (What are examples of benchmarks with limited instances?): sent5
                1.1.1.1.2. (Why do these benchmarks have a limited number of instances?): sent6
                    1.1.1.1.2.1. (Why is it crucial to avoid data leakage in the era of large language models?): sent7
            1.1.1.2. (What are the common languages used in current benchmarks?): sent8
                1.1.1.2.1. (What are examples of multi-lingual benchmarks?): sent9
                    1.1.1.2.1.1. (Where can details of multi-lingual benchmarks be found?): sent10
            1.1.1.3. (What other practical scenarios have benchmarks been proposed for?): sent11
        1.1.2. (What ensures the trustworthiness of execution-based benchmarks?): sent12
            1.1.2.1. (What additional information is provided in Table 3?): sent13
        1.1.3. (Why is there a need for automatic metrics?): sent14
            1.1.3.1. (What metrics are used for execution-based evaluation?): sent15
                1.1.3.1.1. (What are the limitations of execution-based evaluation?): sent16
            1.1.3.2. (What metrics are used for non-executable code?): sent17
                1.1.3.2.1. (What is a limitation of these metrics for non-executable code?): sent17
        1.1.4. (What are the open challenges in designing metrics for code evaluation?): sent18",What are the characteristics and limitations of current NL2Code benchmarks?,"sent3, sent4, sent5, sent6, sent8","We summarize 17 well-studied NL2Code benchmarks in Table 3, where we can find that each of these benchmarks has its own characteristics regarding size, language, complexity, and scenario.

We observe that most benchmarks contain a limited number of instances.

For example, the widely used HumanEval and MBPP have 164 and 974 instances, respectively.

This is because these benchmarks are typically hand-written to ensure that LLMs have not seen them during training.

Additionally, most current benchmarks have their problem descriptions in English and code solutions in Python.","Each of the 17 well-studied NL2Code benchmarks has its own characteristics regarding size, language, complexity, and scenario.

Most benchmarks contain a limited number of instances.

For example, the widely used HumanEval and MBPP have 164 and 974 instances, respectively.

This is because these benchmarks are typically hand-written to ensure that LLMs have not seen them during training.

Additionally, most current benchmarks have their problem descriptions in English and code solutions in Python.","Questions:

What are the characteristics and limitations of current NL2Code benchmarks?

Answer:

Each of the 17 well-studied NL2Code benchmarks has its own characteristics regarding size, language, complexity, and scenario.

Most benchmarks contain a limited number of instances.

For example, the widely used HumanEval and MBPP have 164 and 974 instances, respectively.

This is because these benchmarks are typically hand-written to ensure that LLMs have not seen them during training.

Additionally, most current benchmarks have their problem descriptions in English and code solutions in Python."
Large Language Models Meet NL2Code: A Survey,Large Language Models for NL2Code,"Given a natural language problem description, the NL2Code task aims to automatically generate the demanded code. To illustrate this task visually, we provide a Python programming problem as an example in Figure 1, while different NL2Code benchmarks may vary in terms of language or 2 We summarize the related surveys in Appendix A.  problem domain. Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus. For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens. During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters. There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance. We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task. Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility. For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes. One trend observed is that these large language models are consistently growing in size as the research field advances. Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.

Early works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation. Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus. Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code. While they show surprisingly good performance on NL2Code, most of them are not readily accessible. At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code. Besides, recent studies have proposed various approaches to address specific NL2Code scenarios. For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages. Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code. As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing. Details of the website can be found in Appendix B.

These models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023). One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions. Other notable products include CodeGeeX 3 and CodeWhisperer 4 . A summary of 10 products can be found in Appendix Table 5. Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users. There is still room for improvement before LLMs can be fully practical and capable of coding like humans.","1. What is the NL2Code task?
    1.1. How can the NL2Code task be illustrated? sent2
        1.1.1. What is provided to illustrate the task visually? sent2
        1.1.2. Where can related surveys be found? sent3
2. What are existing large language models for the NL2Code task based on? sent4
    2.1. What objective do most LLMs employ for better code generation performance? sent5
    2.2. How can an LLM tackle NL2Code problems during inference? sent6
    2.3. What other methods are used to boost performance? sent7
3. What investigation is conducted in the paper? sent8
    3.1. What details are summarized in Table 1? sent9
    3.2. How are the models presented for better visualization? sent10
        3.2.1. What trend is observed in the size of these models? sent11
        3.2.2. Which architecture is favored for larger pre-trained models? sent12
4. How do early works compare to large-scale models in terms of zero-shot code generation? sent13
    4.1. What is the limitation of large-scale models like GPT-Neo and GPT-J? sent14
5. What recent powerful LLMs have been proposed for NL2Code? sent15
    5.1. What is the accessibility of these powerful LLMs? sent16
6. What excellent open-source models have been proposed recently? sent17
7. What approaches have been proposed to address specific NL2Code scenarios? sent18
    7.1. What is an example of a model designed for Jupyter Notebooks? sent19
    7.2. What models support multiple natural or programming languages? sent19
    7.3. What models support infilling arbitrary regions of code? sent20
8. How is the rapid evolution of LLMs for NL2Code being tracked? sent21
    8.1. Where can details of the website be found? sent22
9. How are these models applied in real-world products? sent22
    9.1. What is an example of a programming assistance tool using Codex? sent23
    9.2. What other notable products are mentioned? sent24
    9.3. Where can a summary of 10 products be found? sent25
    9.4. What have recent studies shown about these products? sent26
10. What is the current state of LLMs in terms of practical coding capabilities? sent27",What are the foundational models and training objectives for large language models in the NL2Code task?,"sent4, sent5, sent6, sent7","Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus.

For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens.

During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters.

There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance.","Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code-related unlabelled corpus.

For better code generation performance, most large language models, whether encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens.

During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters.

There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance.","Questions:

What are the foundational models and training objectives for large language models in the NL2Code task?

Answer:

Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code-related unlabelled corpus.

For better code generation performance, most large language models, whether encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens.

During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters.

There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance."
Large Language Models Meet NL2Code: A Survey,Large Language Models for NL2Code,"Given a natural language problem description, the NL2Code task aims to automatically generate the demanded code. To illustrate this task visually, we provide a Python programming problem as an example in Figure 1, while different NL2Code benchmarks may vary in terms of language or 2 We summarize the related surveys in Appendix A.  problem domain. Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus. For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens. During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters. There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance. We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task. Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility. For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes. One trend observed is that these large language models are consistently growing in size as the research field advances. Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.

Early works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation. Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus. Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code. While they show surprisingly good performance on NL2Code, most of them are not readily accessible. At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code. Besides, recent studies have proposed various approaches to address specific NL2Code scenarios. For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages. Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code. As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing. Details of the website can be found in Appendix B.

These models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023). One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions. Other notable products include CodeGeeX 3 and CodeWhisperer 4 . A summary of 10 products can be found in Appendix Table 5. Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users. There is still room for improvement before LLMs can be fully practical and capable of coding like humans.","1. What is the NL2Code task?
    1.1. How can the NL2Code task be illustrated? sent2
        1.1.1. What is provided to illustrate the task visually? sent2
        1.1.2. Where can related surveys be found? sent3
2. What are existing large language models for the NL2Code task based on? sent4
    2.1. What objective do most LLMs employ for better code generation performance? sent5
    2.2. How can an LLM tackle NL2Code problems during inference? sent6
    2.3. What other methods are used to boost performance? sent7
3. What investigation is conducted in the paper? sent8
    3.1. What details are summarized in Table 1? sent9
    3.2. How are the models presented for better visualization? sent10
        3.2.1. What trend is observed in the size of these models? sent11
        3.2.2. Which architecture is favored for larger pre-trained models? sent12
4. How do early works compare to large-scale models in terms of zero-shot code generation? sent13
    4.1. What is the limitation of large-scale models like GPT-Neo and GPT-J? sent14
5. What recent powerful LLMs have been proposed for NL2Code? sent15
    5.1. What is the accessibility of these powerful LLMs? sent16
6. What excellent open-source models have been proposed recently? sent17
7. What approaches have been proposed to address specific NL2Code scenarios? sent18
    7.1. What is an example of a model designed for Jupyter Notebooks? sent19
    7.2. What models support multiple natural or programming languages? sent19
    7.3. What models support infilling arbitrary regions of code? sent20
8. How is the rapid evolution of LLMs for NL2Code being tracked? sent21
    8.1. Where can details of the website be found? sent22
9. How are these models applied in real-world products? sent22
    9.1. What is an example of a programming assistance tool using Codex? sent23
    9.2. What other notable products are mentioned? sent24
    9.3. Where can a summary of 10 products be found? sent25
    9.4. What have recent studies shown about these products? sent26
10. What is the current state of LLMs in terms of practical coding capabilities? sent27",How are the 27 representative LLMs for the NL2Code task investigated and presented in the paper?,"sent8, sent9, sent10, sent11, sent12","We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task.

Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility.

For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes.

One trend observed is that these large language models are consistently growing in size as the research field advances.

Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.","A comprehensive investigation of 27 representative LLMs for the NL2Code task is conducted.

Details of each model, including variations in architecture, size, and accessibility, are summarized.

For better visualization, these models are presented in chronological order, plotting the largest model sizes.

One trend observed is that these large language models are consistently growing in size as the research field advances.

Additionally, the decoder-only architecture is favored for pre-trained models with larger sizes.","Questions:

How are the 27 representative LLMs for the NL2Code task investigated and presented in the paper?

Answer:

A comprehensive investigation of 27 representative LLMs for the NL2Code task is conducted.

Details of each model, including variations in architecture, size, and accessibility, are summarized.

For better visualization, these models are presented in chronological order, plotting the largest model sizes.

One trend observed is that these large language models are consistently growing in size as the research field advances.

Additionally, the decoder-only architecture is favored for pre-trained models with larger sizes."
Large Language Models Meet NL2Code: A Survey,Large Language Models for NL2Code,"Given a natural language problem description, the NL2Code task aims to automatically generate the demanded code. To illustrate this task visually, we provide a Python programming problem as an example in Figure 1, while different NL2Code benchmarks may vary in terms of language or 2 We summarize the related surveys in Appendix A.  problem domain. Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus. For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens. During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters. There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance. We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task. Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility. For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes. One trend observed is that these large language models are consistently growing in size as the research field advances. Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.

Early works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation. Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus. Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code. While they show surprisingly good performance on NL2Code, most of them are not readily accessible. At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code. Besides, recent studies have proposed various approaches to address specific NL2Code scenarios. For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages. Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code. As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing. Details of the website can be found in Appendix B.

These models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023). One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions. Other notable products include CodeGeeX 3 and CodeWhisperer 4 . A summary of 10 products can be found in Appendix Table 5. Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users. There is still room for improvement before LLMs can be fully practical and capable of coding like humans.","1. What is the NL2Code task?
    1.1. How can the NL2Code task be illustrated? sent2
        1.1.1. What is provided to illustrate the task visually? sent2
        1.1.2. Where can related surveys be found? sent3
2. What are existing large language models for the NL2Code task based on? sent4
    2.1. What objective do most LLMs employ for better code generation performance? sent5
    2.2. How can an LLM tackle NL2Code problems during inference? sent6
    2.3. What other methods are used to boost performance? sent7
3. What investigation is conducted in the paper? sent8
    3.1. What details are summarized in Table 1? sent9
    3.2. How are the models presented for better visualization? sent10
        3.2.1. What trend is observed in the size of these models? sent11
        3.2.2. Which architecture is favored for larger pre-trained models? sent12
4. How do early works compare to large-scale models in terms of zero-shot code generation? sent13
    4.1. What is the limitation of large-scale models like GPT-Neo and GPT-J? sent14
5. What recent powerful LLMs have been proposed for NL2Code? sent15
    5.1. What is the accessibility of these powerful LLMs? sent16
6. What excellent open-source models have been proposed recently? sent17
7. What approaches have been proposed to address specific NL2Code scenarios? sent18
    7.1. What is an example of a model designed for Jupyter Notebooks? sent19
    7.2. What models support multiple natural or programming languages? sent19
    7.3. What models support infilling arbitrary regions of code? sent20
8. How is the rapid evolution of LLMs for NL2Code being tracked? sent21
    8.1. Where can details of the website be found? sent22
9. How are these models applied in real-world products? sent22
    9.1. What is an example of a programming assistance tool using Codex? sent23
    9.2. What other notable products are mentioned? sent24
    9.3. Where can a summary of 10 products be found? sent25
    9.4. What have recent studies shown about these products? sent26
10. What is the current state of LLMs in terms of practical coding capabilities? sent27",How do early works compare to recent large-scale models in terms of zero-shot code generation capabilities?,"sent13, sent14, sent15, sent16","Early works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation.

Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus.

Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code.

While they show surprisingly good performance on NL2Code, most of them are not readily accessible.","Early works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation.

Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus.

Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), and PaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code.

While these models show surprisingly good performance on NL2Code, most of them are not readily accessible.","Questions:

How do early works compare to recent large-scale models in terms of zero-shot code generation capabilities?

Answer:

Early works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation.

Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus.

Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), and PaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code.

While these models show surprisingly good performance on NL2Code, most of them are not readily accessible."
Large Language Models Meet NL2Code: A Survey,Large Language Models for NL2Code,"Given a natural language problem description, the NL2Code task aims to automatically generate the demanded code. To illustrate this task visually, we provide a Python programming problem as an example in Figure 1, while different NL2Code benchmarks may vary in terms of language or 2 We summarize the related surveys in Appendix A.  problem domain. Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus. For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens. During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters. There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance. We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task. Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility. For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes. One trend observed is that these large language models are consistently growing in size as the research field advances. Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.

Early works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation. Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus. Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code. While they show surprisingly good performance on NL2Code, most of them are not readily accessible. At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code. Besides, recent studies have proposed various approaches to address specific NL2Code scenarios. For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages. Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code. As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing. Details of the website can be found in Appendix B.

These models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023). One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions. Other notable products include CodeGeeX 3 and CodeWhisperer 4 . A summary of 10 products can be found in Appendix Table 5. Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users. There is still room for improvement before LLMs can be fully practical and capable of coding like humans.","1. What is the NL2Code task?
    1.1. How can the NL2Code task be illustrated? sent2
        1.1.1. What is provided to illustrate the task visually? sent2
        1.1.2. Where can related surveys be found? sent3
2. What are existing large language models for the NL2Code task based on? sent4
    2.1. What objective do most LLMs employ for better code generation performance? sent5
    2.2. How can an LLM tackle NL2Code problems during inference? sent6
    2.3. What other methods are used to boost performance? sent7
3. What investigation is conducted in the paper? sent8
    3.1. What details are summarized in Table 1? sent9
    3.2. How are the models presented for better visualization? sent10
        3.2.1. What trend is observed in the size of these models? sent11
        3.2.2. Which architecture is favored for larger pre-trained models? sent12
4. How do early works compare to large-scale models in terms of zero-shot code generation? sent13
    4.1. What is the limitation of large-scale models like GPT-Neo and GPT-J? sent14
5. What recent powerful LLMs have been proposed for NL2Code? sent15
    5.1. What is the accessibility of these powerful LLMs? sent16
6. What excellent open-source models have been proposed recently? sent17
7. What approaches have been proposed to address specific NL2Code scenarios? sent18
    7.1. What is an example of a model designed for Jupyter Notebooks? sent19
    7.2. What models support multiple natural or programming languages? sent19
    7.3. What models support infilling arbitrary regions of code? sent20
8. How is the rapid evolution of LLMs for NL2Code being tracked? sent21
    8.1. Where can details of the website be found? sent22
9. How are these models applied in real-world products? sent22
    9.1. What is an example of a programming assistance tool using Codex? sent23
    9.2. What other notable products are mentioned? sent24
    9.3. Where can a summary of 10 products be found? sent25
    9.4. What have recent studies shown about these products? sent26
10. What is the current state of LLMs in terms of practical coding capabilities? sent27",What recent open-source models have been proposed to advance the NL2Code task?,"sent17, sent18, sent19, sent20","At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code.

Besides, recent studies have proposed various approaches to address specific NL2Code scenarios.

For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages.

Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code.","Several excellent open-source models have been proposed, including CodeParrot (Huggingface, 2021), PolyCoder, GPT-NeoX (Black et al., 2022), and SantaCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code.

Recent studies have proposed various approaches to address specific NL2Code scenarios.

For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), and BLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages.

Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction but also allow for infilling arbitrary regions of code.","Questions:

What recent open-source models have been proposed to advance the NL2Code task?

Answer:

Several excellent open-source models have been proposed, including CodeParrot (Huggingface, 2021), PolyCoder, GPT-NeoX (Black et al., 2022), and SantaCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code.

Recent studies have proposed various approaches to address specific NL2Code scenarios.

For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), and BLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages.

Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction but also allow for infilling arbitrary regions of code."
Large Language Models Meet NL2Code: A Survey,Large Language Models for NL2Code,"Given a natural language problem description, the NL2Code task aims to automatically generate the demanded code. To illustrate this task visually, we provide a Python programming problem as an example in Figure 1, while different NL2Code benchmarks may vary in terms of language or 2 We summarize the related surveys in Appendix A.  problem domain. Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus. For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens. During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters. There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance. We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task. Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility. For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes. One trend observed is that these large language models are consistently growing in size as the research field advances. Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.

Early works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation. Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus. Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code. While they show surprisingly good performance on NL2Code, most of them are not readily accessible. At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code. Besides, recent studies have proposed various approaches to address specific NL2Code scenarios. For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages. Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code. As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing. Details of the website can be found in Appendix B.

These models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023). One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions. Other notable products include CodeGeeX 3 and CodeWhisperer 4 . A summary of 10 products can be found in Appendix Table 5. Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users. There is still room for improvement before LLMs can be fully practical and capable of coding like humans.","1. What is the NL2Code task?
    1.1. How can the NL2Code task be illustrated? sent2
        1.1.1. What is provided to illustrate the task visually? sent2
        1.1.2. Where can related surveys be found? sent3
2. What are existing large language models for the NL2Code task based on? sent4
    2.1. What objective do most LLMs employ for better code generation performance? sent5
    2.2. How can an LLM tackle NL2Code problems during inference? sent6
    2.3. What other methods are used to boost performance? sent7
3. What investigation is conducted in the paper? sent8
    3.1. What details are summarized in Table 1? sent9
    3.2. How are the models presented for better visualization? sent10
        3.2.1. What trend is observed in the size of these models? sent11
        3.2.2. Which architecture is favored for larger pre-trained models? sent12
4. How do early works compare to large-scale models in terms of zero-shot code generation? sent13
    4.1. What is the limitation of large-scale models like GPT-Neo and GPT-J? sent14
5. What recent powerful LLMs have been proposed for NL2Code? sent15
    5.1. What is the accessibility of these powerful LLMs? sent16
6. What excellent open-source models have been proposed recently? sent17
7. What approaches have been proposed to address specific NL2Code scenarios? sent18
    7.1. What is an example of a model designed for Jupyter Notebooks? sent19
    7.2. What models support multiple natural or programming languages? sent19
    7.3. What models support infilling arbitrary regions of code? sent20
8. How is the rapid evolution of LLMs for NL2Code being tracked? sent21
    8.1. Where can details of the website be found? sent22
9. How are these models applied in real-world products? sent22
    9.1. What is an example of a programming assistance tool using Codex? sent23
    9.2. What other notable products are mentioned? sent24
    9.3. Where can a summary of 10 products be found? sent25
    9.4. What have recent studies shown about these products? sent26
10. What is the current state of LLMs in terms of practical coding capabilities? sent27","How are LLMs for NL2Code being applied in real-world products, and what are some examples?","sent22, sent23, sent24, sent25, sent26","Details of the website can be found in Appendix B.These models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023).

One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions.

Other notable products include CodeGeeX 3 and CodeWhisperer 4 .

A summary of 10 products can be found in Appendix Table 5.

Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users.","Large language models (LLMs) for NL2Code are not only attractive in academia (Chen et al., 2021; Nijkamp et al., 2023; Li et al., 2022b), but are also applied in real-world products to improve programming efficiency (Sobania et al., 2022a; Barke et al., 2023).

One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide real-time code suggestions.

Other notable products include CodeGeeX and CodeWhisperer.

A summary of 10 products highlights the diverse applications of LLMs for NL2Code in real-world scenarios.

Recent studies (Sobania et al., 2022b; Pearce et al., 2022; Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users.","Questions:

How are LLMs for NL2Code being applied in real-world products, and what are some examples?

Answer:

Large language models (LLMs) for NL2Code are not only attractive in academia (Chen et al., 2021; Nijkamp et al., 2023; Li et al., 2022b), but are also applied in real-world products to improve programming efficiency (Sobania et al., 2022a; Barke et al., 2023).

One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide real-time code suggestions.

Other notable products include CodeGeeX and CodeWhisperer.

A summary of 10 products highlights the diverse applications of LLMs for NL2Code in real-world scenarios.

Recent studies (Sobania et al., 2022b; Pearce et al., 2022; Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users."
Large Language Models Meet NL2Code: A Survey,Large and Premium Data,"As the sizes of LLMs increase in the field of NL2Code, the scale of the corpus used for training also increases. This highlights the importance of selecting and pre-processing high-quality data. In this section, we will discuss various commonly used data sources and pre-processing strategies that are essential for training LLMs.

Early models were trained using manually annotated data pairs of NL and code, and the data sources include CodeSearchNet (Husain et al., 2019), CoST (Zhu et al., 2022b), and XL-CoST (Zhu et al., 2022a). However, manual annotation is labour-intensive and time-consuming. There are also models like GPT-3 (Brown et al., 2020), GPT-Neo (Black et al., 2021), and GPT-J (Wang and Komatsuzaki, 2021)  In order to ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data. We carefully review the data pre-processing methods of five powerful LLMs, including Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), CodeGen (Nijkamp et al., 2023), In-Coder (Fried et al., 2023), and PyCodeGPT (Zan et al., 2022b), and identify several commonalities. One is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless. Additionally, specific rules are employed to filter out uncommon code files. These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate. In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature.","1. (What happens as the sizes of LLMs increase in the field of NL2Code?): sent1
    1.1. (Why is the selection and pre-processing of high-quality data important?): sent2
    1.2. (What will be discussed in this section?): sent3
2. (What were the early models trained on?): sent4
    2.1. (What is a drawback of using manually annotated data?): sent5
3. (What are some examples of models that do not rely on manual annotation?): sent6
4. (How is the quality of the training corpus ensured?): sent7
    4.1. (What data pre-processing methods are reviewed?): sent8
        4.1.1. (What is one common pre-processing method?): sent9
        4.1.2. (What additional rules are employed to filter out uncommon code files?): sent10
            4.1.2.1. (What factors are included in these rules?): sent11
5. (What is the goal of these pre-processing strategies?): sent12",What are the common data pre-processing strategies used to ensure the quality of the training corpus for LLMs?,"sent7, sent8, sent9, sent10, sent11, sent12","In order to ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data.

We carefully review the data pre-processing methods of five powerful LLMs, including Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), CodeGen (Nijkamp et al., 2023), In-Coder (Fried et al., 2023), and PyCodeGPT (Zan et al., 2022b), and identify several commonalities.

One is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless.

Additionally, specific rules are employed to filter out uncommon code files.

These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate.

In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature.","To ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data.

Several common data pre-processing methods have been identified in powerful LLMs, including Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), CodeGen (Nijkamp et al., 2023), In-Coder (Fried et al., 2023), and PyCodeGPT (Zan et al., 2022b).

One common method is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless.

Additionally, specific rules are employed to filter out uncommon code files.

These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate.

In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature.","Questions:

What are the common data pre-processing strategies used to ensure the quality of the training corpus for LLMs?

Answer:

To ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data.

Several common data pre-processing methods have been identified in powerful LLMs, including Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), CodeGen (Nijkamp et al., 2023), In-Coder (Fried et al., 2023), and PyCodeGPT (Zan et al., 2022b).

One common method is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless.

Additionally, specific rules are employed to filter out uncommon code files.

These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate.

In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature."
A Survey on Zero Pronoun Translation,Data-Level Methods Do Not Change Model,"Architecture. This is more friendly to NMT. Some researchers targeted making better usage of the limited training data (Tan et al., 2019;Ohtani et al., 2019;Tan et al., 2021). They trained an external model on the ZP data to recover the ZP information in the input sequence of the MT model (Tan et al., 2019;Ohtani et al., 2019;Tan et al., 2021) or correct the errors in the translation outputs (Voita et al., 2019). Others aimed to up-sample the training data for the ZPT task (Sugiyama and Yoshinaga, 2019;Kimura et al., 2019;Ri et al., 2021). They preferred to  improve the ZPT performance via a data augmentation without modifying the MT architecture (Wang et al., 2016a;Sugiyama and Yoshinaga, 2019). Kimura et al. (2019); Ri et al. (2021) verified that the performance can be further improved by denoising the pseudo data. 4. Multitask and Multi-Lingual Learning. ZPT is a hard task to be done alone, researchers are investigating how to leverage other related NLP tasks to improve ZPT by training models to perform multiple tasks simultaneously (Wang et al., 2018a). Since ZPT is a cross-lingual problem, researchers are exploring techniques for training models that can work across multiple languages, rather than being limited to a single language (Aloraini and Poesio, 2020).

6 Evaluation Methods","# QUA Analysis:

1. (What is the general topic of this section?): sent1
    1.1. (What is the specific focus within the topic?): sent2
        1.1.1. (How did some researchers use the limited training data?): sent3
        1.1.2. (What did other researchers aim to do with the training data?): sent4
            1.1.2.1. (How did they aim to improve ZPT performance?): sent5
                1.1.2.1.1. (What additional method was verified to improve performance?): sent6
2. (What is the next topic in the section?): sent7
    2.1. (What is the challenge in ZPT and how are researchers addressing it?): sent8
    2.2. (What is another approach researchers are exploring for ZPT?): sent9
3. (What is the subsequent section title?): sent10",What methods did researchers use to up-sample training data for the ZPT task?,"sent4, sent5, sent6","Others aimed to up-sample the training data for the ZPT task (Sugiyama and Yoshinaga, 2019;Kimura et al., 2019;Ri et al., 2021).

They preferred to  improve the ZPT performance via a data augmentation without modifying the MT architecture (Wang et al., 2016a;Sugiyama and Yoshinaga, 2019).

Kimura et al. (2019); Ri et al. (2021) verified that the performance can be further improved by denoising the pseudo data.","Researchers aimed to up-sample the training data for the ZPT task (Sugiyama and Yoshinaga, 2019; Kimura et al., 2019; Ri et al., 2021).

They preferred to improve the ZPT performance via data augmentation without modifying the MT architecture (Wang et al., 2016a; Sugiyama and Yoshinaga, 2019).

Kimura et al. (2019) and Ri et al. (2021) verified that the performance can be further improved by denoising the pseudo data.","Questions:

What methods did researchers use to up-sample training data for the ZPT task?

Answer:

Researchers aimed to up-sample the training data for the ZPT task (Sugiyama and Yoshinaga, 2019; Kimura et al., 2019; Ri et al., 2021).

They preferred to improve the ZPT performance via data augmentation without modifying the MT architecture (Wang et al., 2016a; Sugiyama and Yoshinaga, 2019).

Kimura et al. (2019) and Ri et al. (2021) verified that the performance can be further improved by denoising the pseudo data."
Beyond Words: A Comprehensive Survey of Sentence Representations,Alternative Methods,"Researchers have explored various other methods for obtaining positive samples for unsupervised sentence representations. One approach involves using other spans of text that either overlap with the given span or come from the same document (Giorgi et al., 2021). Another strategy is to use related entities as positives instead of text spans (Nishikawa et al., 2022). This approach is particularly useful for cross-lingual sentence representation learning, as information about the same entity is often available in different languages.

Other approaches use large language models that are capable of performing various tasks from observing few examples, even for tasks that they have never encountered before. Generally, larger models with more parameters tend to perform better (Brown et al., 2020). Moreover, these models can be directed to perform tasks by providing them with specific instructions-a technique known as ""prompting"". This prompting paradigm has become a popular approach in natural language processing. Researchers have used prompts to obtain better sentence representations, as demonstrated in studies such as , which employs the ""[X] means [MASK]"" prompt to extract sentence representations from the representation of the ""[MASK]"" token in a sentence. Another study by (Zeng et al., 2022) combines prompt-derived sentence representations with contrastive learning to improve the quality of the representations.","1. What methods have researchers explored for obtaining positive samples for unsupervised sentence representations? (sent1)
    1.1. What is one approach for obtaining positive samples? (sent2)
    1.2. What is another strategy for obtaining positive samples? (sent3)
        1.2.1. Why is this strategy particularly useful for cross-lingual sentence representation learning? (sent4)
    1.3. What other approaches are there for obtaining positive samples? (sent5)
        1.3.1. How do larger models perform in these approaches? (sent6)
        1.3.2. How can these models be directed to perform tasks? (sent7)
            1.3.2.1. What is the popularity and application of the prompting paradigm? (sent8)
            1.3.2.2. How have researchers used prompts to obtain better sentence representations? (sent9)
            1.3.2.3. How does combining prompt-derived sentence representations with contrastive learning improve the quality of the representations? (sent10)",What methods have researchers explored for obtaining positive samples for unsupervised sentence representations?,"sent1, sent2, sent3, sent4, sent5","Researchers have explored various other methods for obtaining positive samples for unsupervised sentence representations.

One approach involves using other spans of text that either overlap with the given span or come from the same document (Giorgi et al., 2021).

Another strategy is to use related entities as positives instead of text spans (Nishikawa et al., 2022).

This approach is particularly useful for cross-lingual sentence representation learning, as information about the same entity is often available in different languages.

Other approaches use large language models that are capable of performing various tasks from observing few examples, even for tasks that they have never encountered before.","Researchers have explored various methods for obtaining positive samples for unsupervised sentence representations.

One approach involves using other spans of text that either overlap with the given span or come from the same document (Giorgi et al., 2021).

Another strategy is to use related entities as positives instead of text spans (Nishikawa et al., 2022).

This approach is particularly useful for cross-lingual sentence representation learning, as information about the same entity is often available in different languages.

Other approaches involve using large language models that can perform various tasks from observing few examples, even for tasks they have never encountered before.","Questions:

What methods have researchers explored for obtaining positive samples for unsupervised sentence representations?

Answer:

Researchers have explored various methods for obtaining positive samples for unsupervised sentence representations.

One approach involves using other spans of text that either overlap with the given span or come from the same document (Giorgi et al., 2021).

Another strategy is to use related entities as positives instead of text spans (Nishikawa et al., 2022).

This approach is particularly useful for cross-lingual sentence representation learning, as information about the same entity is often available in different languages.

Other approaches involve using large language models that can perform various tasks from observing few examples, even for tasks they have never encountered before."
Beyond Words: A Comprehensive Survey of Sentence Representations,Model Level,"Another approach to generating positive examples is by leveraging the distinctive characteristics of the backbone model utilized in contrastive learning. These characteristics might be the architectural choices or using representation from certain components of the model.

Dropout is a regularization technique used in deep learning to prevent overfitting of a model. During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times. These different representations can be used as positive examples for sentence representations. Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy. Several other works have also incorporated this technique and improved upon it: promoting decorrelation between different dimensions (Klein and Nabi, 2022) and adding it as a part of the transformation arsenal (Wu et al., 2022a,d).

Specific components of language models can be trained to generate semantically similar representations. One example is the use of prefix modules (Li and Liang, 2021), which are small, trainable modules added to a pretrained language model. In a recent study, Wang and Lu (2022) trained prefix modules on NLI data before using them to generate positive examples for unsupervised sentence representations. a","# QUA Analysis:

1. (How can positive examples be generated using the backbone model in contrastive learning?): sent1
    1.1. (What are the distinctive characteristics of the backbone model?): sent2
    1.2. (How can dropout be used in generating positive examples?): sent3
        1.2.1. (What is dropout?): sent3
        1.2.2. (How does dropout work during training?): sent4
        1.2.3. (How can different representations from dropout be used?): sent5
        1.2.4. (What studies support the use of dropout as an augmentation strategy?): sent6
        1.2.5. (What other works have incorporated and improved dropout?): sent7
2. (How can specific components of language models generate semantically similar representations?): sent8
    2.1. (What is an example of a specific component used in language models?): sent9
    2.2. (How have prefix modules been used in recent studies?): sent10",How can dropout be used to generate positive examples in sentence representations?,"sent3, sent4, sent5, sent6, sent7","Dropout is a regularization technique used in deep learning to prevent overfitting of a model.

During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times.

These different representations can be used as positive examples for sentence representations.

Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy.

Several other works have also incorporated this technique and improved upon it: promoting decorrelation between different dimensions (Klein and Nabi, 2022) and adding it as a part of the transformation arsenal (Wu et al., 2022a,d).","Dropout is a regularization technique used in deep learning to prevent overfitting of a model.

During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times.

These different representations can be used as positive examples for sentence representations.

Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy.

Several other works have also incorporated this technique and improved upon it, such as promoting decorrelation between different dimensions (Klein and Nabi, 2022) and adding it as a part of the transformation arsenal (Wu et al., 2022a,d).","Questions:

How can dropout be used to generate positive examples in sentence representations?

Answer:

Dropout is a regularization technique used in deep learning to prevent overfitting of a model.

During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times.

These different representations can be used as positive examples for sentence representations.

Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy.

Several other works have also incorporated this technique and improved upon it, such as promoting decorrelation between different dimensions (Klein and Nabi, 2022) and adding it as a part of the transformation arsenal (Wu et al., 2022a,d)."
Beyond Words: A Comprehensive Survey of Sentence Representations,Model Level,"Another approach to generating positive examples is by leveraging the distinctive characteristics of the backbone model utilized in contrastive learning. These characteristics might be the architectural choices or using representation from certain components of the model.

Dropout is a regularization technique used in deep learning to prevent overfitting of a model. During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times. These different representations can be used as positive examples for sentence representations. Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy. Several other works have also incorporated this technique and improved upon it: promoting decorrelation between different dimensions (Klein and Nabi, 2022) and adding it as a part of the transformation arsenal (Wu et al., 2022a,d).

Specific components of language models can be trained to generate semantically similar representations. One example is the use of prefix modules (Li and Liang, 2021), which are small, trainable modules added to a pretrained language model. In a recent study, Wang and Lu (2022) trained prefix modules on NLI data before using them to generate positive examples for unsupervised sentence representations. a","# QUA Analysis:

1. (How can positive examples be generated using the backbone model in contrastive learning?): sent1
    1.1. (What are the distinctive characteristics of the backbone model?): sent2
    1.2. (How can dropout be used in generating positive examples?): sent3
        1.2.1. (What is dropout?): sent3
        1.2.2. (How does dropout work during training?): sent4
        1.2.3. (How can different representations from dropout be used?): sent5
        1.2.4. (What studies support the use of dropout as an augmentation strategy?): sent6
        1.2.5. (What other works have incorporated and improved dropout?): sent7
2. (How can specific components of language models generate semantically similar representations?): sent8
    2.1. (What is an example of a specific component used in language models?): sent9
    2.2. (How have prefix modules been used in recent studies?): sent10",How do specific components of language models generate semantically similar representations?,"sent8, sent9, sent10","Specific components of language models can be trained to generate semantically similar representations.

One example is the use of prefix modules (Li and Liang, 2021), which are small, trainable modules added to a pretrained language model.

In a recent study, Wang and Lu (2022) trained prefix modules on NLI data before using them to generate positive examples for unsupervised sentence representations.","Specific components of language models can be trained to generate semantically similar representations.

One example is the use of prefix modules (Li and Liang, 2021), which are small, trainable modules added to a pretrained language model.

In a recent study, Wang and Lu (2022) trained prefix modules on NLI data before using them to generate positive examples for unsupervised sentence representations.","Questions:

How do specific components of language models generate semantically similar representations?

Answer:

Specific components of language models can be trained to generate semantically similar representations.

One example is the use of prefix modules (Li and Liang, 2021), which are small, trainable modules added to a pretrained language model.

In a recent study, Wang and Lu (2022) trained prefix modules on NLI data before using them to generate positive examples for unsupervised sentence representations."
Beyond Words: A Comprehensive Survey of Sentence Representations,Alternative Loss and Objectives,"In § 2, we discuss Contrastive loss, which is widely used in machine learning. However, this loss suffers from several limitations: for instance it only considers binary relationships between instances, lacks a mechanism to incorporate ""hard negatives"". To overcome these drawbacks, researchers have explored supplementary losses that can be used in conjunction with the Contrastive loss. Moreover, they have proposed alterations to the loss function that make it more effective for learning, as well as alternative loss functions for contrastive learning of sentence representations. This section provides an overview of these approaches.

To improve the learning of sentence representations using the Contrastive loss, researchers have introduced several supplementary losses. These include: (1) the hinge loss proposed by Jiang et al. (2022b), which enhances discrimination between positive and negative pairs; (2) losses suggested by Wu et al. (2022b) for reconstructing the original sentence from its representation to better capture sentence semantics; (3) a loss developed by Chuang et al. (2022) to identify masked words and improve sensitivity to meaningless semantic transformations; and (4) a loss proposed by Chen et al. (2022a) to minimize redundant information from transformations by minimizing information entropy.

Besides such supplementary losses, some studies have suggested modifications to the original contrastive loss for better effectiveness. For instance, Wu et al. (2022c) have proposed an additional term that incorporates random noise from a Gaussian distribution as negative instances. In the same vein, Zhang et al. (2022d) have introduced two lossesthe angular loss and margin-based triplet loss-to better address the intricacies of similarity between pairs of examples.

However, recent research has moved away from using the contrastive loss and has employed different loss functions to learn sentence representations. For instance, Zhang et al. (2020) maximize the mutual information between a local and a global representation of a sentence using .  identify an alternative sub-manifold within the sentence representation space that considers the geometric structure of sentences to compute semantic similarity.","# QUA Analysis:

1. (What is discussed in § 2?): sent1
    1.1. (What are the limitations of Contrastive loss?): sent2
    1.2. (How can the drawbacks of Contrastive loss be overcome?): sent3
        1.2.1. (What supplementary losses can be used with Contrastive loss?): sent6
            1.2.1.1. (What are the specific supplementary losses introduced by researchers?): sent7, sent8
        1.2.2. (What alterations have been proposed to the Contrastive loss function?): sent4, sent9
            1.2.2.1. (What specific modifications have been suggested to the original Contrastive loss?): sent10, sent11
2. (What does this section provide?): sent5
3. (What is the recent trend in research regarding loss functions for sentence representations?): sent12
    3.1. (What are the alternative loss functions employed in recent research?): sent13, sent14",What supplementary losses can be used with the Contrastive loss to improve sentence representations?,"sent6, sent7, sent8","To improve the learning of sentence representations using the Contrastive loss, researchers have introduced several supplementary losses.

These include: (1) the hinge loss proposed by Jiang et al. (2022b), which enhances discrimination between positive and negative pairs; (2) losses suggested by Wu et al. (2022b) for reconstructing the original sentence from its representation to better capture sentence semantics;

(3) a loss developed by Chuang et al. (2022) to identify masked words and improve sensitivity to meaningless semantic transformations; and (4) a loss proposed by Chen et al. (2022a) to minimize redundant information from transformations by minimizing information entropy.","To improve the learning of sentence representations using the Contrastive loss, researchers have introduced several supplementary losses.

These include the hinge loss proposed by Jiang et al. (2022b), which enhances discrimination between positive and negative pairs, and losses suggested by Wu et al. (2022b) for reconstructing the original sentence from its representation to better capture sentence semantics.

Another supplementary loss is developed by Chuang et al. (2022) to identify masked words and improve sensitivity to meaningless semantic transformations. Additionally, Chen et al. (2022a) proposed a loss to minimize redundant information from transformations by minimizing information entropy.","Questions:

What supplementary losses can be used with the Contrastive loss to improve sentence representations?

Answer:

To improve the learning of sentence representations using the Contrastive loss, researchers have introduced several supplementary losses.

These include the hinge loss proposed by Jiang et al. (2022b), which enhances discrimination between positive and negative pairs, and losses suggested by Wu et al. (2022b) for reconstructing the original sentence from its representation to better capture sentence semantics.

Another supplementary loss is developed by Chuang et al. (2022) to identify masked words and improve sensitivity to meaningless semantic transformations. Additionally, Chen et al. (2022a) proposed a loss to minimize redundant information from transformations by minimizing information entropy."
Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,Factors Affecting Knowledge Retention,"PLMs are diverse with respect to their architectures, pre-training objectives and their pre-training data.A compelling question is: how do all these factors affect knowledge retention in PLMs?

Large language models are known to perform generally better and hold more knowledge (Brown et al., 2020;Roberts et al., 2020).However, the model's architecture and pre-training objectives are more decisive for knowledge retention than its size (Li et al., 2022a).For example, pre-training with the Salient Span Masking objective (Guu et al., 2020) helps PLMs to absorb more facts (Roberts et al., 2020;Cole et al., 2023).Similarly, Xiong et al. (2020) demonstrate that training the model to predict if the original entities in the text have been replaced with other entities is beneficial for fact retrieval.More generally, Ye et al. (2021) conclude that a masking strategy matching the downstream task, positively affects the performance on that task.

A larger pre-training corpus with an encoderonly model (Liu et al., 2020) leads to higher knowledge retention (Zhang et al., 2021), but with an encoder-decoder model (Lewis et al., 2020), a larger corpus negatively affects knowledge retention Wang et al. (2021a).Recency (Chiang et al., 2020) and frequency (Kandpal et al., 2023), i.e., when and how often the data is observed at training, are also essential for knowledge retention.

Larger models and more pre-training data can improve knowledge retention if combined with the right choices for architecture and pre-training objective(s).However, scaling might not be sufficient (Kandpal et al., 2023).Even though many works propose new architectures and pre-training objectives to increase factual knowledge retention in PLMs and their robustness to prompts (Févry et al., 2020;Hosseini et al., 2021;Sadeq et al., 2022;Whitehouse et al., 2022;Min et al., 2023;Zhong et al., 2023), this is a promising future work direction, as there is more room for improvement.","1. (What are the factors affecting knowledge retention in PLMs?): sent1
    1.1. (How do these factors affect knowledge retention in PLMs?): sent2
        1.1.1. (What is the impact of model size on knowledge retention?): sent3
        1.1.2. (What is the impact of model architecture and pre-training objectives on knowledge retention?): sent3
            1.1.2.1. (How does the Salient Span Masking objective affect knowledge retention?): sent3
            1.1.2.2. (How does training the model to predict entity replacement affect knowledge retention?): sent3
        1.1.3. (What is the general conclusion about masking strategies and their impact on task performance?): sent4
        1.1.4. (What is the impact of pre-training corpus size and model type on knowledge retention?): sent5
            1.1.4.1. (How does a larger pre-training corpus with an encoder-only model affect knowledge retention?): sent5
            1.1.4.2. (How does a larger pre-training corpus with an encoder-decoder model affect knowledge retention?): sent5
            1.1.4.3. (What is the impact of recency and frequency of data observation on knowledge retention?): sent5
        1.1.5. (Can larger models and more pre-training data improve knowledge retention?): sent6
            1.1.5.1. (What are the conditions under which larger models and more pre-training data improve knowledge retention?): sent6
            1.1.5.2. (Is scaling alone sufficient for improving knowledge retention?): sent6
            1.1.5.3. (What is the future direction for improving factual knowledge retention in PLMs?): sent6",How do model architecture and pre-training objectives impact knowledge retention in PLMs?,"sent3, sent4, sent5, sent6","Large language models are known to perform generally better and hold more knowledge (Brown et al., 2020;Roberts et al., 2020).However, the model's architecture and pre-training objectives are more decisive for knowledge retention than its size (Li et al., 2022a).For example, pre-training with the Salient Span Masking objective (Guu et al., 2020) helps PLMs to absorb more facts (Roberts et al., 2020;Cole et al., 2023).Similarly, Xiong et al. (2020) demonstrate that training the model to predict if the original entities in the text have been replaced with other entities is beneficial for fact retrieval.

More generally, Ye et al. (2021) conclude that a masking strategy matching the downstream task, positively affects the performance on that task.

A larger pre-training corpus with an encoderonly model (Liu et al., 2020) leads to higher knowledge retention (Zhang et al., 2021), but with an encoder-decoder model (Lewis et al., 2020), a larger corpus negatively affects knowledge retention Wang et al. (2021a).Recency (Chiang et al., 2020) and frequency (Kandpal et al., 2023), i.e., when and how often the data is observed at training, are also essential for knowledge retention.

Larger models and more pre-training data can improve knowledge retention if combined with the right choices for architecture and pre-training objective(s).However, scaling might not be sufficient (Kandpal et al., 2023).Even though many works propose new architectures and pre-training objectives to increase factual knowledge retention in PLMs and their robustness to prompts (Févry et al., 2020;Hosseini et al., 2021;Sadeq et al., 2022;Whitehouse et al., 2022;Min et al., 2023;Zhong et al., 2023), this is a promising future work direction, as there is more room for improvement.","The model's architecture and pre-training objectives are more decisive for knowledge retention than its size (Li et al., 2022a).

More generally, Ye et al. (2021) conclude that a masking strategy matching the downstream task positively affects the performance on that task.

A larger pre-training corpus with an encoder-only model (Liu et al., 2020) leads to higher knowledge retention (Zhang et al., 2021), but with an encoder-decoder model (Lewis et al., 2020), a larger corpus negatively affects knowledge retention (Wang et al., 2021a). Recency (Chiang et al., 2020) and frequency (Kandpal et al., 2023), i.e., when and how often the data is observed during training, are also essential for knowledge retention.

Larger models and more pre-training data can improve knowledge retention if combined with the right choices for architecture and pre-training objectives. However, scaling might not be sufficient (Kandpal et al., 2023). Even though many works propose new architectures and pre-training objectives to increase factual knowledge retention in PLMs and their robustness to prompts (Févry et al., 2020; Hosseini et al., 2021; Sadeq et al., 2022; Whitehouse et al., 2022; Min et al., 2023; Zhong et al., 2023), this remains a promising future work direction, as there is more room for improvement.","Questions:

How do model architecture and pre-training objectives impact knowledge retention in PLMs?

Answer:

The model's architecture and pre-training objectives are more decisive for knowledge retention than its size (Li et al., 2022a).

More generally, Ye et al. (2021) conclude that a masking strategy matching the downstream task positively affects the performance on that task.

A larger pre-training corpus with an encoder-only model (Liu et al., 2020) leads to higher knowledge retention (Zhang et al., 2021), but with an encoder-decoder model (Lewis et al., 2020), a larger corpus negatively affects knowledge retention (Wang et al., 2021a). Recency (Chiang et al., 2020) and frequency (Kandpal et al., 2023), i.e., when and how often the data is observed during training, are also essential for knowledge retention.

Larger models and more pre-training data can improve knowledge retention if combined with the right choices for architecture and pre-training objectives. However, scaling might not be sufficient (Kandpal et al., 2023). Even though many works propose new architectures and pre-training objectives to increase factual knowledge retention in PLMs and their robustness to prompts (Févry et al., 2020; Hosseini et al., 2021; Sadeq et al., 2022; Whitehouse et al., 2022; Min et al., 2023; Zhong et al., 2023), this remains a promising future work direction, as there is more room for improvement."
Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,Datasets for Factual Probing,"We found a variety of datasets (44 in our corpus) that have been proposed or used for probing factual knowledge in PLMs: 18 datasets for probing general knowledge, 8 for domain-specific knowledge and 18 datasets that target other aspects, e.g, consistency of PLMs (cf.Table 2).

Datasets for general knowledge probing are used to quantify generic factual knowledge in PLMs with the most prominent being LAMA (Petroni et al., 2019).WIKI-UNI (Cao et al., 2021) is similar to LAMA, but with a uniform distribution of object entities.LAMA-UHN (Poerner et al., 2020) is a subset of LAMA without easy-to-guess examples.DLAMA (Keleg and Magdy, 2023) targets culturally diverse facts.While 16 datasets are solely English, there are three multilingual datasets (mLAMA (Kassner et al., 2021), X-FACTR (Jiang et al., 2020a) and DLAMA (Keleg and Magdy, 2023)).In-dicGLUE (Kakwani et al., 2020) contains 11 Indic languages.Most datasets consist of cloze prompts, while QA datasets (WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017), NQ (Kwiatkowski et al., 2019)), PopQA and Enti-tyQuestions (Mallen et al., 2023) are also used to quantify factual knowledge (Roberts et al., 2020).Wang et al. (2021a) adapt SQuAD (Rajpurkar et al., 2018) for closed-book question answering.

6 out of 8 datasets used for probing domainspecific knowledge target the biomedical domain (e.g., MedQA (Jin et al., 2021), Bio-LAMA (Sung et al., 2021) and MedLAMA (Meng et al., 2022b)).The multilingual dataset EX-AMS (Hardalov et al., 2020) focuses on scientific QA, whereas LEFT (Ciosici et al., 2021) contains questions from humanities and social sciences.

The community has constructed further datasets to investigate other aspects of using PLMs as knowledge bases.PARAREL (Elazar et al., 2021) and its multilingual counterpart mPARA-REL (Fierro and Søgaard, 2022)","1. What datasets have been proposed or used for probing factual knowledge in PLMs? (sent1)
    1.1. What are the datasets for probing general knowledge? (sent1)
        1.1.1. What is LAMA? (sent1)
        1.1.2. What is WIKI-UNI? (sent1)
        1.1.3. What is LAMA-UHN? (sent2)
        1.1.4. What is DLAMA? (sent3)
    1.2. What are the multilingual datasets? (sent4)
        1.2.1. What are the names of the multilingual datasets? (sent4)
        1.2.2. What languages does In-dicGLUE contain? (sent4)
    1.3. What types of prompts do most datasets consist of? (sent5)
        1.3.1. What are the QA datasets used to quantify factual knowledge? (sent5)
        1.3.2. How is SQuAD adapted for closed-book question answering? (sent5)
    1.4. What datasets are used for probing domain-specific knowledge? (sent6)
        1.4.1. What domain do most of these datasets target? (sent6)
        1.4.2. What is the focus of the multilingual dataset EX-AMS? (sent6)
        1.4.3. What does LEFT contain? (sent6)
    1.5. What other aspects of using PLMs as knowledge bases are investigated by further datasets? (sent7)
        1.5.1. What are PARAREL and its multilingual counterpart? (sent8)",What datasets have been proposed or used for probing factual knowledge in pre-trained language models (PLMs)?,"sent1, sent2, sent3, sent4, sent5, sent6, sent7, sent8","We found a variety of datasets (44 in our corpus) that have been proposed or used for probing factual knowledge in PLMs: 18 datasets for probing general knowledge, 8 for domain-specific knowledge and 18 datasets that target other aspects, e.g, consistency of PLMs (cf.Table 2).Datasets for general knowledge probing are used to quantify generic factual knowledge in PLMs with the most prominent being LAMA (Petroni et al., 2019).WIKI-UNI (Cao et al., 2021) is similar to LAMA, but with a uniform distribution of object entities.

LAMA-UHN (Poerner et al., 2020) is a subset of LAMA without easy-to-guess examples.

DLAMA (Keleg and Magdy, 2023) targets culturally diverse facts.

While 16 datasets are solely English, there are three multilingual datasets (mLAMA (Kassner et al., 2021), X-FACTR (Jiang et al., 2020a) and DLAMA (Keleg and Magdy, 2023)).In-dicGLUE (Kakwani et al., 2020) contains 11 Indic languages.

Most datasets consist of cloze prompts, while QA datasets (WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017), NQ (Kwiatkowski et al., 2019)), PopQA and Enti-tyQuestions (Mallen et al., 2023) are also used to quantify factual knowledge (Roberts et al., 2020).Wang et al. (2021a) adapt SQuAD (Rajpurkar et al., 2018) for closed-book question answering.

6 out of 8 datasets used for probing domainspecific knowledge target the biomedical domain (e.g., MedQA (Jin et al., 2021), Bio-LAMA (Sung et al., 2021) and MedLAMA (Meng et al., 2022b)).The multilingual dataset EX-AMS (Hardalov et al., 2020) focuses on scientific QA, whereas LEFT (Ciosici et al., 2021) contains questions from humanities and social sciences.

The community has constructed further datasets to investigate other aspects of using PLMs as knowledge bases.

PARAREL (Elazar et al., 2021) and its multilingual counterpart mPARA-REL (Fierro and Søgaard, 2022)","A variety of datasets have been proposed or used for probing factual knowledge in PLMs, including 18 datasets for probing general knowledge, 8 for domain-specific knowledge, and 18 datasets targeting other aspects, such as the consistency of PLMs.

LAMA-UHN (Poerner et al., 2020) is a subset of LAMA without easy-to-guess examples.

DLAMA (Keleg and Magdy, 2023) targets culturally diverse facts.

While 16 datasets are solely English, there are three multilingual datasets: mLAMA (Kassner et al., 2021), X-FACTR (Jiang et al., 2020a), and DLAMA (Keleg and Magdy, 2023). In-dicGLUE (Kakwani et al., 2020) contains 11 Indic languages.

Most datasets consist of cloze prompts, while QA datasets such as WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017), NQ (Kwiatkowski et al., 2019), PopQA, and EntityQuestions (Mallen et al., 2023) are also used to quantify factual knowledge (Roberts et al., 2020). Wang et al. (2021a) adapt SQuAD (Rajpurkar et al., 2018) for closed-book question answering.

Six out of eight datasets used for probing domain-specific knowledge target the biomedical domain, such as MedQA (Jin et al., 2021), Bio-LAMA (Sung et al., 2021), and MedLAMA (Meng et al., 2022b). The multilingual dataset EX-AMS (Hardalov et al., 2020) focuses on scientific QA, whereas LEFT (Ciosici et al., 2021) contains questions from humanities and social sciences.

The community has constructed further datasets to investigate other aspects of using PLMs as knowledge bases.

PARAREL (Elazar et al., 2021) and its multilingual counterpart mPARA-REL (Fierro and Søgaard, 2022) are examples of datasets constructed to investigate other aspects of using PLMs as knowledge bases.","Questions:

What datasets have been proposed or used for probing factual knowledge in pre-trained language models (PLMs)?

Answer:

A variety of datasets have been proposed or used for probing factual knowledge in PLMs, including 18 datasets for probing general knowledge, 8 for domain-specific knowledge, and 18 datasets targeting other aspects, such as the consistency of PLMs.

LAMA-UHN (Poerner et al., 2020) is a subset of LAMA without easy-to-guess examples.

DLAMA (Keleg and Magdy, 2023) targets culturally diverse facts.

While 16 datasets are solely English, there are three multilingual datasets: mLAMA (Kassner et al., 2021), X-FACTR (Jiang et al., 2020a), and DLAMA (Keleg and Magdy, 2023). In-dicGLUE (Kakwani et al., 2020) contains 11 Indic languages.

Most datasets consist of cloze prompts, while QA datasets such as WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017), NQ (Kwiatkowski et al., 2019), PopQA, and EntityQuestions (Mallen et al., 2023) are also used to quantify factual knowledge (Roberts et al., 2020). Wang et al. (2021a) adapt SQuAD (Rajpurkar et al., 2018) for closed-book question answering.

Six out of eight datasets used for probing domain-specific knowledge target the biomedical domain, such as MedQA (Jin et al., 2021), Bio-LAMA (Sung et al., 2021), and MedLAMA (Meng et al., 2022b). The multilingual dataset EX-AMS (Hardalov et al., 2020) focuses on scientific QA, whereas LEFT (Ciosici et al., 2021) contains questions from humanities and social sciences.

The community has constructed further datasets to investigate other aspects of using PLMs as knowledge bases.

PARAREL (Elazar et al., 2021) and its multilingual counterpart mPARA-REL (Fierro and Søgaard, 2022) are examples of datasets constructed to investigate other aspects of using PLMs as knowledge bases."
Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,Obstacles to Adopting PLMs as KBs,"Consistency.A challenge to relying on PLMs as knowledge bases is their sensitivity to the input queries (Fierro and Søgaard, 2022).PLMs rely on shallow surface features and lexical correlations (Kassner and Schütze, 2020;Misra et al., 2020;Poerner et al., 2020;Rogers et al., 2020;Li et al., 2022b), which explains their high sensitivity to the way queries are formulated.Current solutions (Elazar et al., 2021;Newman et al., 2022) train PLMs to be robust to variations in inputs, but further improvements are needed to make PLMs reliable knowledge bases.PLMs are known to be highly sensitive to prompts, especially in languages other than English (Fierro and Søgaard, 2022), where less resources are available.Making PLMs more robust to prompts in non-English languages is a promising future work direction.

Interpretability.Identifying where facts are stored and how they are retrieved is essential to adopt PLMs as trustworthy knowledge sources.Several approaches locate knowledge in PLMs (Wallat et al., 2020;Podkorytov et al., 2021;Alkhaldi et al., 2022;Dai et al., 2022;Meng et al., 2022a), with different conclusions depending on the architecture (e.g., knowledge is located in the middle layers of GPT-like models (Meng et al., 2022a), or in the upper layers in BERT-like models (Dai et al., 2022)).Another line of work focuses on the data aspect, showing the dependence of PLMs on word co-occurrences and positionally close words (Li et al., 2022b), or tracing back predictions to training data (Akyurek et al., 2022;Park et al., 2023).Knowing how PLMs retrieve facts remains challenging, but necessary to make PLMs transparent fact retrievers.The introduction of a fact tracing benchmark (Akyurek et al., 2022) opens the door for works in this direction.

Updating Knowledge.PLMs come with a fixed set of pre-trained parameters that encode knowledge about the world.As time passes, this knowledge becomes partially outdated.Hence, editing existing knowledge in PLMs and augmenting them with new knowledge is crucial for their use as knowledge bases (Zini and Awad, 2022).

One line of research locates the modules responsible for factual predictions and modifies these to update the corresponding facts (Dai et al., 2022;De Cao et al., 2021;Meng et al., 2022a).Other lines of research keep the original PLM unchanged, but augment it with additional parameters to induce the desired changes (Wang et al., 2021b;Lee et al., 2022), or encode facts with time stamps in PLMs to make them ""time-aware"" (Dhingra et al., 2022).

When updating facts in PLMs, it is crucial that only the targeted facts are affected and that these facts are retrievable using different paraphrases (De Cao et al., 2021;Hase et al., 2023).However, current methods for facts editing (Meng et al., 2022a(Meng et al., , 2023) ) still do not fulfill these requirements (Hoelscher-Obermaier et al., 2023).Methods that introduce additional parameters should be made more scalable (Jang et al., 2022b).","1. What are the obstacles to adopting PLMs as knowledge bases?
    1.1. What is the challenge related to consistency in PLMs?
        1.1.1. Why is consistency a challenge for PLMs as knowledge bases? (sent1)
        1.1.2. What are the current solutions to improve consistency in PLMs? (sent2, sent3)
        1.1.3. How are PLMs sensitive to prompts, especially in non-English languages? (sent4)
        1.1.4. What is a promising future work direction for improving PLMs' robustness to prompts in non-English languages? (sent5)
    1.2. What is the challenge related to interpretability in PLMs?
        1.2.1. Why is interpretability essential for adopting PLMs as trustworthy knowledge sources? (sent6)
        1.2.2. What approaches are used to locate knowledge in PLMs? (sent7)
        1.2.3. What is the significance of the fact tracing benchmark? (sent8)
    1.3. What is the challenge related to updating knowledge in PLMs?
        1.3.1. Why is updating knowledge in PLMs crucial? (sent9, sent10, sent11)
        1.3.2. What are the methods for updating knowledge in PLMs? (sent12)
        1.3.3. What are the requirements and current limitations for updating facts in PLMs? (sent13)",Why is updating knowledge in PLMs crucial?,"sent9, sent10, sent11","Updating Knowledge. PLMs come with a fixed set of pre-trained parameters that encode knowledge about the world.

As time passes, this knowledge becomes partially outdated.

Hence, editing existing knowledge in PLMs and augmenting them with new knowledge is crucial for their use as knowledge bases (Zini and Awad, 2022).","Updating knowledge in PLMs is crucial because they come with a fixed set of pre-trained parameters that encode knowledge about the world.

As time passes, this knowledge becomes partially outdated.

Hence, editing existing knowledge in PLMs and augmenting them with new knowledge is crucial for their use as knowledge bases (Zini and Awad, 2022).","Questions:

Why is updating knowledge in PLMs crucial?

Answer:

Updating knowledge in PLMs is crucial because they come with a fixed set of pre-trained parameters that encode knowledge about the world.

As time passes, this knowledge becomes partially outdated.

Hence, editing existing knowledge in PLMs and augmenting them with new knowledge is crucial for their use as knowledge bases (Zini and Awad, 2022)."
Transformers as Recognizers of Formal Languages: A Survey on Expressivity,First-order logic with majority,"Merrill and Sabharwal (2023a) further tighten the L-uniform TC 0 upper bound of Merrill and Sabharwal (2023b) to DLOGTIME-uniform TC 0 , and therefore FOM [BIT].The proof constructs subroutines to answer queries about the types of nodes and connectivity of pairs of nodes in the computation graph of a transformer, and shows that these queries can be translated to queries for a TC 0 circuit family with  (log ) time overhead.which is first-order logic with counting quantifiers, using two sorts for positions and counts (Immerman, 1999, p. 185-187), where positions have the MOD predicate (but not < or =), and counts have <, +, and =, capturing the fact that transformers can add and compare activations, but not positions.They show that this logic is intermediate in expressivity between  (1)-precision and infinite-precision transformers.The lower-bound proof makes use of a normal form that eliminates quantifiers over counts and makes quantifiers over positions have depth 1; a perhaps surprising consequence is that  (1)-precision transformers are no more powerful than 2-layer uniform-attention transformers.Weiss et al. (2021) define a programming language called RASP (Restricted Access Sequence Programming Language) and show that it can be compiled to transformers with average-hard attention and two extensions: • Attention weights are directly computed from the previous layer without being confined to dotproducts of query and key vectors.• Position-wise FFNs compute arbitrary computable functions.Lindner et al. (2023) describe a RASP compiler that outputs standard transformers.It compiles RASP selectors to dot-product attention, with syntactic restrictions on selectors and a maximum string length.Element-wise operations are approximately compiled to ReLU FFNs.Friedman et al. (2023) define Transformer Programs, a restricted class of transformers that can be translated into RASP programs.","# QUA Analysis:

1. (What did Merrill and Sabharwal (2023a) achieve in their work?): sent1
    1.1. (What is the significance of the DLOGTIME-uniform TC 0 upper bound?): sent1
    1.2. (How does the proof construct subroutines to answer queries?): sent1
    1.3. (What is the expressivity of the logic shown by Merrill and Sabharwal (2023a)?): sent2
        1.3.1. (How does this logic compare to other transformers?): sent2
        1.3.2. (What is the lower-bound proof about?): sent3
            1.3.2.1. (What is the normal form used in the lower-bound proof?): sent3
            1.3.2.2. (What is the consequence of the normal form?): sent3
2. (What did Weiss et al. (2021) define and show?): sent4
    2.1. (What is RASP?): sent4
    2.2. (What are the extensions of RASP?): sent4
3. (What did Lindner et al. (2023) describe?): sent5
    3.1. (What does the RASP compiler output?): sent5
    3.2. (How does the RASP compiler work?): sent6
        3.2.1. (How are RASP selectors compiled?): sent6
        3.2.2. (What are the restrictions on selectors and string length?): sent6
    3.3. (How are element-wise operations compiled?): sent7
4. (What did Friedman et al. (2023) define?): sent8
    4.1. (What are Transformer Programs?): sent8
    4.2. (How can Transformer Programs be translated?): sent8",What did Merrill and Sabharwal (2023a) achieve in their work on transformers?,"sent1, sent2, sent3","Merrill and Sabharwal (2023a) further tighten the L-uniform TC 0 upper bound of Merrill and Sabharwal (2023b) to DLOGTIME-uniform TC 0 , and therefore FOM [BIT].The proof constructs subroutines to answer queries about the types of nodes and connectivity of pairs of nodes in the computation graph of a transformer, and shows that these queries can be translated to queries for a TC 0 circuit family with  (log ) time overhead.which is first-order logic with counting quantifiers, using two sorts for positions and counts (Immerman, 1999, p. 185-187), where positions have the MOD predicate (but not < or =), and counts have <, +, and =, capturing the fact that transformers can add and compare activations, but not positions.

They show that this logic is intermediate in expressivity between  (1)-precision and infinite-precision transformers.

The lower-bound proof makes use of a normal form that eliminates quantifiers over counts and makes quantifiers over positions have depth 1; a perhaps surprising consequence is that  (1)-precision transformers are no more powerful than 2-layer uniform-attention transformers.","Merrill and Sabharwal (2023a) further tighten the L-uniform TC^0 upper bound of Merrill and Sabharwal (2023b) to DLOGTIME-uniform TC^0, and therefore FOM [BIT].

They show that this logic is intermediate in expressivity between 1-precision and infinite-precision transformers.

The lower-bound proof demonstrates that 1-precision transformers are no more powerful than 2-layer uniform-attention transformers by using a normal form that eliminates quantifiers over counts and restricts quantifiers over positions to depth 1.","Questions:

What did Merrill and Sabharwal (2023a) achieve in their work on transformers?

Answer:

Merrill and Sabharwal (2023a) further tighten the L-uniform TC^0 upper bound of Merrill and Sabharwal (2023b) to DLOGTIME-uniform TC^0, and therefore FOM [BIT].

They show that this logic is intermediate in expressivity between 1-precision and infinite-precision transformers.

The lower-bound proof demonstrates that 1-precision transformers are no more powerful than 2-layer uniform-attention transformers by using a normal form that eliminates quantifiers over counts and restricts quantifiers over positions to depth 1."
