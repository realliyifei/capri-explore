# Efficient High-Resolution Deep Learning: A Survey

CorpusID: 251066965
 
tags: #Environmental_Science, #Computer_Science

URL: [https://www.semanticscholar.org/paper/5169a986f8f30418f239372e6f9b59de832aaac2](https://www.semanticscholar.org/paper/5169a986f8f30418f239372e6f9b59de832aaac2)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

Efficient High-Resolution Deep Learning: A Survey


Arian Bakhtiarnia 
Qi Zhang 
Alexandros Iosifidis 
Efficient High-Resolution Deep Learning: A Survey
1Index Terms-high-resolution deep learningefficient deep learningvision transformercomputer vision
Cameras in modern devices such as smartphones, satellites and medical equipment are capable of capturing very high resolution images and videos. Such high-resolution data often need to be processed by deep learning models for cancer detection, automated road navigation, weather prediction, surveillance, optimizing agricultural processes and many other applications. Using high-resolution images and videos as direct inputs for deep learning models creates many challenges due to their high number of parameters, computation cost, inference latency and GPU memory consumption. Simple approaches such as resizing the images to a lower resolution are common in the literature, however, they typically significantly decrease accuracy. Several works in the literature propose better alternatives in order to deal with the challenges of high-resolution data and improve accuracy and speed while complying with hardware limitations and time restrictions. This survey describes such efficient high-resolution deep learning methods, summarizes realworld applications of high-resolution deep learning, and provides comprehensive information about available high-resolution datasets.

## I. INTRODUCTION

Many modern devices such as smartphones, drones, augmented reality headsets, vehicles and other Internet of Things (IoT) devices are equipped with high-quality cameras that can capture high-resolution images and videos. With the help of image stitching techniques, camera arrays [1], [2], gigapixel acquisition robots [3] and whole-slide scanners [4], capture resolutions can be increased to billions of pixels (commonly referred to as gigapixels), such as the image depicted in Figure 1. One could attempt to define high-resolution based on the capabilities of human visual system. However, many deep learning tasks rely on data captured by equipment which behaves very differently compared to the human eye, such as microscopes, satellite imagery and infrared cameras. Furthermore, utilizing more detail than the eye can sense is beneficial in many deep learning tasks, such as in the applications discussed in Section II. The amount of detail that can be captured and is useful if processed varies greatly from task to task. Therefore, the definition of high-resolution is taskdependent. For instance, in image classification and computed tomography (CT) scan processing, a resolution of 512×512 pixels is considered to be high [5], [6]. In visual crowd counting, datasets with High-Definition (HD) resolutions or higher are common [7], and whole-slide images (WSIs) in histopathology, which is the study of diseases of the tissues, or remote sensing data, which are captured by aircrafts or satellites, can easily reach gigapixel resolutions [8], [9].

Moreover, with the constant advancement of hardware and methodologies, what deep learning literature considers highresolution has shifted over time. For instance, in the late 1990s, processing the 32×32-pixel MNIST images with neural networks was an accomplishment [10], whereas in early 2010s, the 256×256-pixel images in ImageNet were considered highresolution [11]. This trend can also be seen in the consistent increase of the average resolution of images in popular deep learning datasets, such as crowd counting [7] and anomaly detection [12] datasets. Therefore, the definition of highresolution is also period-dependent. Based on the task-and period-dependence properties, it is clear that the term "highresolution" is technical, not fundamental or universal. Therefore, instead of trying to derive such a definition, we shift our focus to resolutions that create technical challenges in deep learning at the time of this writing.

Using high-resolution images and videos directly as inputs to deep learning models creates challenges during both training and inference phases. With the exception of fullyconvolutional networks (FCNs), the number of parameters in deep learning models typically increases with larger input sizes. Moreover, the amount of computation, which is commonly measured in terms of floating point operations (FLOPs), and therefore inference/training time, as well as GPU memory consumption increase with higher-resolution inputs, as shown in Figure 2. This issue is especially problematic in Vision Transformer (ViT) architectures, which use the selfattention mechanism, where the inference speed and number of parameters scale quadratically with input size [6], [15]. These issues are exacerbated when the training or inference needs to be done on resource-constrained devices, such as smartphones, that have limited computational capabilities compared to highend computing equipment, such as workstations or servers.

Even though methods such as model parallelism can be used to split the model between multiple GPUs during both the training [17], [18] and inference [19] phases, and thus avoid memory and latency issues, these methods require a large amount of resources, such as a large number of GPUs and servers, which can incur high costs, especially when working with extreme resolutions such as gigapixel images. Furthermore, in many applications, such as self-driving cars and drone image processing, there is a limit for the hardware that can be mounted, and offloading the computation to external servers is not always possible because of unreliability of the network connection due to movement and the timecritical nature of the application. Therefore, the most common approach for deep learning training and inference is to load the full model on each single GPU instance. Multi-GPU setups are (a) (b) Fig. 1: Example of a gigapixel image, taken from the PANDA-Crowd dataset [13], captured using an array of micro-cameras; (a) original image with a size of 26,558×14,828 pixels, and (b) zoomed in to the location specified by the red rectangle in the original image, with a size of 2,516×1,347 pixels, which is more than 100 times smaller than the original image, yet still approximately 5 times larger than the image size processed by state-of-the-art deep learning models for crowd counting such as SASNet [14], which is 1024×768, and around 50 times larger than the standard image size processed by image classification models, which is 224×224.  [16]; and (d) the number of parameters in the ViT-B16 [6] architecture. The last layer of EfficientNet-B7 was removed to form a fully-convolutional feature extractor. Since accuracy is not considered in these figures, there is no need to use real images, thus randomly generated images are given to the models as input. All experiments were conducted on an Nvidia A6000 GPU.

instead typically used to speed up the training by increasing the overall batch size, to test multiple sets of hyper-parameters in parallel or to distribute the inference load. Consequently, in many cases, there is an effective maximum resolution that can be processed by deep learning models. As an example, the maximum resolution for inference using SASNet [14], which is the state-of-the-art model for crowd counting on the Shanghai Tech dataset [20] at the time of this writing, is around 1024×768 (less than HD) on Nvidia 2080 Ti GPUs which have 11 GBs of video memory.

Although newer generations of GPUs are getting faster and have more memory available, the resolution of images and videos captured by devices is also increasing. Figure 3 shows this trend across recent years for multiple types of devices. Therefore, the aforementioned issues will likely persist even with advances in computation hardware technology. Furthermore, current imaging technologies are nowhere near the physical limits of image resolutions, which is estimated to be in petapixels [21].

Whether or not capturing and processing a higher resolution leads to improvements depends on the particular problem at hand. For instance, in image classification, it is unlikely that increasing the resolution for images of objects or animals to gigapixels would reveal more beneficial details and improve the accuracy. On the other hand, if the goal is to count the total number of people in scenes such as the one presented in Figure 1, using an HD resolution instead of gigapixels would mean that several people could be represented by a single pixel, which significantly increases the error. Similarly, it has been shown that using higher resolutions in histopathology can lead to better results [22].

Assuming there is an effective maximum resolution for a particular problem due to hardware limitations or latency requirements, there are two simple baseline approaches for processing the original captured inputs which are commonly used in deep learning literature [23]- [25]. The popularity of  these baselines can be attributed to the simplicity of their implementation. The first one is to resize (downsample) the original input to the desired resolution, however, this will lead to a lower accuracy if any important details for the problem at hand are lost. This approach is called uniform downsampling (UD) since the quality is reduced uniformly throughout the image. The second approach is to cut up the original input into smaller patches that each have a maximum resolution, process the patches independently, and aggregate the results, for instance, by summing them up for regression problems and majority voting for classification problems. We call this approach cutting into patches (CIP). There are two issues with this approach. First, many deep learning models rely on global features which will be lost since features extracted from each patch will not be shared with other patches, leading to decreased accuracy. For instance, crowd counting methods typically heavily rely on global information such as perspective or illumination [7], and in object detection, objects near the boundaries may be split between multiple patches. Secondly, since multiple passes of inference are performed, that is, one pass for each patch, inference will take much longer. This issue is worse when patches overlap.

To highlight these issues, we test the two baseline approaches (UD and CIP) on the Shanghai Tech Part B dataset [20] for crowd counting, which contains images of size 1024×768 pixels. We reduce the original image size by factors of 4 and 16 and measure the mean absolute error (MAE) for both baselines. To test UD, we take a SASNet model [14] pre-trained on the Shanghai Tech Part B dataset [20] with input size of 1024×768, and fine-tune it for the target input size using the AdamW optimizer [26] with a learning rate of 10 −5 and weight decay of 10 −4 . Note that the original SASNet paper uses the Adam optimizer [27] with a learning rate of 10 −5 . We train the model for 100 epochs with batch size of 12 per GPU instance using 3×Nvidia A6000 GPUs. We empirically found that fine-tuning does not improve the accuracy of cutting into patches, therefore, we cut the original image into 4 and 16 patches, and obtain the count for each patch using the pre-trained SASNet mentioned above, then aggregate the results by summing up the predicted count for each patch.

The results of these experiments are shown in Table I. It can be observed that uniform downsampling significantly increases the error compared to processing the original input size. Keep in mind that even though the increase in error is not as drastic with cutting into patches, the inference time of this approach is increased by the same factor (i.e., 4 and 16) since we assumed we are using the effective maximum resolution possible for our hardware, and thus patches cannot be processed in parallel as the entire hardware is required to process a single patch. Since these baseline approaches are far from ideal, in recent years, several alternative methods have been proposed in the literature in order to improve accuracy and speed while complying with the maximum resolution limitation caused either by memory limitations or speed requirements. The goal of this survey is to summarize and categorize these contributions. To the best of our knowledge, no other survey on the topic of high-resolution deep learning exists. However, there are some surveys that include aspects relevant to this topic. A survey on methods for reducing the computational complexity of Transformer architectures is provided in [15], which discusses the issues related to the quadratic time and memory complexity of self-attention and analyzes various aspects of efficiency including memory footprint and computational cost. While reducing the computational complexity of Transformer models can contribute to efficient processing of high-resolution inputs, in this survey, we only include Vision Transformer methods that explicitly focus on high-resolution images. Some application-specific surveys include high-resolution datasets and methods that operate on such data. For instance, a survey on deep learning for histopathology, which mentions challenges with processing the giga-resolution of WSIs, is provided in [28]; a survey of methods that achieve greater spatial resolution in computed tomography (CT) is provided in [29], which highlights improved diagnostic accuracy with ultra high-resolution CT, and briefly discusses deep learning methods for noise reduction and reconstruction; a survey on crowd counting where many of the available datasets are high-resolution is provided in [7]; a survey on deep learning methods for land cover classification and object detection in high-resolution remote sensing imagery is provided in [30]; and a survey on deep learning-based change detection in highresolution remote sensing images is provided in [31].

It is important to mention that some methods operate on high-resolution inputs, yet do not make any effort to address the aforementioned challenges. For instance, multi-column (also known as multi-scale) networks [7] incorporate multiple columns of layers in their architecture, where each column is responsible for processing a specific scale as shown in Figure  4. However, since the columns process the same resolution as the original input, most of these methods in fact require even more memory and computation compared to the case where only the original scale is processed. The primary goal of these methods is instead to increase the accuracy by taking into account the scale variances that occur in high-resolution images, although there are some multi-scale methods that improve both accuracy and efficiency [32]- [34]. Therefore, these methods do not fall within the scope of this survey, unless they explicitly address the efficiency aspect for highresolution inputs. ZoomCount [35], Locality-Aware Crowd Counting [36], RAZ-Net [37] and Learn to Scale [38] are all examples of multi-scale methods in crowd counting, and DMMN [39] and KGZNet [40] in medical image processing. If the original input to the DNN is a patch taken from a larger image, such as in [36], in addition to zooming in, it is also possible to zoom out.

The primary purpose of this survey is to collect and describe methods that exist in deep learning literature, which can be used in situations where the high resolution of input images and videos create the aforementioned technical challenges regarding memory, computation and time. The rest of this paper is organized as follows: Section II lists applications where high-resolution images and videos are processed using deep learning. Section III categorizes efficient methods for high-resolution deep learning into five general categories and provides several examples for each category. This section also briefly discusses alternative approaches for solving the memory and processing time issues caused by high-resolution inputs. Section IV lists existing high-resolution datasets for various deep learning problems and provides details for each of them. Section V discusses the advantages and disadvantages of using efficient high-resolution methods belonging to different categories and provides recommendations about which method to use in different situations. Finally, Section VI concludes the paper by summarizing the current state and trends in high-resolution deep learning as well as suggestions for future research. The code for experiments conducted in this survey is available at https://gitlab.au.dk/ maleci/high-resolution-deep-learning.


## II. APPLICATIONS OF HIGH-RESOLUTION DEEP LEARNING

In this section, we list some real-world applications where high-resolution images are processed with deep learning. Most of these methods do not focus on the efficiency angle, however, some of the methods address issues encountered with highresolution images. For instance, [41] mentions that "it was not possible to train the model with the original 6000×4000 pixels images because of GPU memory limitation" and [42], which uses the cutting into patches approach, states that "a raw remote image has millions of pixels and is difficult to process directly".


## A. Medical and Biomedical Image Analysis

Multi-gigapixel whole-slide pathology images can be processed with deep learning in order to detect breast cancer [43], skin cancer [44], [45], prostate cancer [45], lung cancer [45], cervical cancer [46] and cancer in the digestive tract [47]. Some methods are even able to detect the cancer subtypes [45] or detect the spread of cancer to lymph nodes (metastasis) [48]. Semantic segmentation of such images can be useful in neuropathology [49], which is the study of diseases of the nervous system, and identifying tissue components such as tumor, muscle, and debris in medical images [50].

Moreover, the processing of high-resolution computed tomography (CT) scans with deep learning is becoming more prevalent. The studies in [51] and [5] detect COVID-19 in high-resolution CT scans of the lung, and the study in [52] uses deep learning to improve the quality of captured ultra-high-resolution CT scans. In addition, the study in [53] performs semantic segmentation on high-resolution electron microscopy images from hearts and brains of mice, which is useful for fundamental biomedical research. Additionally, high-resolution deep learning can be used for reconstruction of CT images and reduction of image noise, which has been shown to obtain results similar to other conventional methods with clinically feasible speed [54], [55].

Even though medical image analysis methods primarily focus on improving the accuracy of particular tasks, inference speed can be crucial in some applications, for instance, speed might be a requirement in clinical practice [48]. Furthermore, real-time augmented reality under microscopes can provide suitable human-computer interaction for AI-assisted slide screening [46]. Finally, there might be situations where the speed for processing a single input is acceptable, however, the sheer number of input data is so high that inputs collectively cannot be processed within a deadline. For instance, 55,000 high-resolution images are taken during the examination of a single patient using wireless capsule endoscopy, where a tiny wireless camera is swallowed to take pictures of the digestive tract, which can be used to detect lesions and inflammation [56].


## B. Remote Sensing

Processing high-resolution aerial and satellite imagery with deep learning has various applications [57], such as detecting buildings [58], which is useful for urban planning and monitoring; detecting airplanes [59], which can be used for defense and military applications as well as airport surveillance; extracting road networks [42], which has applications in automated road navigation with unmanned vehicles, urban planning and real-time updating of geospatial databases; detecting areas in a forest that are damaged due to natural disasters such as storms [60]; identifying weed plants, which can be used for targeted spraying of pesticides in agricultural fields; semantic segmentation of satellite data which can help with crop monitoring, natural resource management and digital mapping [61]; and remote sensing image captioning which is useful for applications such as image retrieval and military intelligence generation [62]. Moreover, significant accuracy improvements can be obtained by taking low-resolution weather data as input and interpolating high-resolution data using super-resolution [63]. The motivation behind this approach is that high-resolution data are only available with a few days delay, and this method can be used to more accurately process low-resolution but up-to-date data.


## C. Surveillance

Capturing and processing gigapixel images for surveillance is becoming increasingly widespread, and such images can be processed with deep learning for searching and identifying people [64], [65] as well as detecting pedestrians [66], [67] which can be used for human behavior analysis and intelligent video surveillance such as enforcing social distancing restrictions during a pandemic [68], [69]. It should be noted that capturing gigapixel images for surveillance has several advantages over capturing lower resolutions with multiple cameras at different locations of the scene. First, cameras in a multi-camera setup typically have some overlap in their fields of view to avoid blindspots. This may result in errors for many applications, such as crowd counting, due to duplicates, as shown in Figure 5. Reducing this error is not an easy task, since it requires information about the geometry of the scene and the use of re-identification methods for identifying and deduplicating people in multiple views of the same scene. Secondly, tracking the trajectory of people, vehicles and other moving objects is difficult with multiple cameras, since it also requires identifying them in multiple views of the scene. Finally, in many deep learning applications such as crowd counting, incorporating global information from the entire scene such as illumination and perspective improves the accuracy of the task [7]. Note that images captured from drastically different locations and perspectives, such as the ones in in Figure 5, cannot be stitched together to form a single image.


## D. Other Applications

High-resolution deep learning can be beneficial in many other applications and various domains of science. For instance, the study in [41] estimates the density of wheat ears, which are the grain-bearing parts of the plant, from highresolution images taken from grain fields, which aids plant breeders in optimizing their yield; and the study in [70] introduces a deep learning method for segmentation of highresolution electron microscopy images, which has applications in material science such as understanding the degradation process of industrial catalysts. [71] proposes a method for realtime high-resolution background replacement, which is useful in video calls and conferencing.


## III. METHODS FOR EFFICIENT PROCESSING OF HIGH-RESOLUTION INPUTS WITH DEEP LEARNING


## A. Non-Uniform Downsampling

Non-uniform downsampling (NUD) is based on the idea that for any deep learning task, some locations of an input image are more important than others. For instance, in gaze estimation, where the goal is to detect where a person is looking given an image including the person's face, the image locations depicting the person's eyes are much more important than other parts of the image. Therefore, when reducing the resolution of the image, it might be beneficial to sample more pixels from salient areas and less pixels from non-salient locations, resulting in a warped and distorted image. This operation requires salient areas to be determined before introducing the downsampled image to the task DNN. Therefore, a small saliency detection network is utilized in order to obtain this saliency map. Figure 6 provides a schematic illustration of the non-uniform downsampling approach. Note that nonuniform downsampling is a broad process that encompasses any method that downsamples the input image in any manner other than uniform. [23] further subdivides non-uniform downsampling into three categories: attention mechanisms, saliency-based methods and adaptive image sampling methods. However, as the authors point out, there is a lot of overlap between these categories and it is difficult to draw a clear border between them.

Formally, the saliency map S can be obtained by applying saliency detection network f s (·) on a uniformly downsampled image I l , that is, S = f s (I l ). The input to the saliency detection network is downsampled in order to keep the overhead of the saliency detection process low. The nonuniformly downsampled image J can the be obtained based on J = g(I, S), where g(·) is the non-uniform resampler and I is the original image. Essentially, the resampler should compute a mapping J(x, y) = I(u c (x, y), v c (x, y)) from the original Fig. 6: Schematic illustration of the non-uniform downsampling approach. The saliency detector detects the cat's right eye as a salient area, therefore, the non-uniform resampler samples more pixels from that area.

image to the downsampled one. Functions u c (·) and v c (·) need to map pixels proportionally to the weight assigned to them in the saliency map. Assuming the saliency map is normalized and ∀x, y : 0 ≤ u c (x, y) ≤ 1 and ∀x, y : 0 ≤ v c (x, y) ≤ 1, this problem can be written as
uc(x,y) 0 vc(x,y) 0 S(x , y )dx dy = xy.(1)
However, methods for determining this transformation based on Eq. 1 are not efficient [23]. An alternative approach is to presume each pixel (x , y ) is pulling all other pixels with a force proportional to its saliency S(x , y ), which can be formulated as
u c (x, y) = x ,y S(x , y )k((x, y), (x , y ))x x ,y S(x , y )k((x, y), (x , y )) ,(2)v c (x, y) = x ,y S(x , y )k((x, y), (x , y ))y x ,y S(x , y )k((x, y), (x , y )) ,(3)
where k((x, y), (x , y )) is a distance kernel, for instance, the Gaussian kernel. Using this formulation, salient areas will be sampled more since they attract more pixels. Moreover, based on this formulation, u c (·) and v c (·) can be computed with simple convolutions. Therefore, this operation can be easily plugged into neural network architectures as a layer, and has the added benefit of preserving the differentiability which is a requirement for training neural networks with the backpropagation algorithm. The overall result is that the entire module including the saliency detection network and the task network can be trained end-to-end. The method in [23] uses this approach to improve the performance of gaze estimation as well as fine-grained classification, which is the task of differentiating between hard-to-distinguish objects such as different species of animals. The method in [72] applies the idea of non-uniform downsampling to semantic segmentation. If the input image I = I ij has a size H × W and must be downsampled to size h × w, the first step is to generate ideal sampling tensors from ground truth (GT) labels based on
E(φ) = i,j φ ij − b(u ij ) 2 + λ |i−i |+|j−j |=1 φ ij − φ i j 2 ,(4)
where φ ∈ [0, 1] h×w×2 is the sampling tensor to be determined, E(φ) is the (energy) cost function to minimize, u ∈ [0, 1] h×w×2 is the uniform downsampling tensor and b(u ij ) is the coordinates of the closest point to pixel u ij on semantic boundaries in the GT labels. Eq. 4 corresponds to a least squares problem with convex constraints that can be efficiently solved using a set of sparse linear equations. The first term in Eq. 4 ensures the sampling locations are close to the semantic boundaries, and the second term ensures that the distortion is not excessive by forcing the transformations of adjacent pixels to be similar. Eq. 4 is also subject to covering constraints that ensure the sampled locations cover the whole image. The contribution of the second term is controlled by a parameter λ which is empirically set to 1. The next step is to train a neural network to generate sampling tensors from input images. The images are then downsampled based on the output of this neural network and introduced to the task network. Finally, the segmentation output is upsampled to remove distortions and match the original resolution.

Similarly, the method in [73] utilizes non-uniform downsampling for semantic segmentation. However, in contrast with the previous method, the saliency detector in this method is optimized based on the performance of semantic segmentation rather than external supervision signals. This method is similar to [23], however, applying a straightforward adaptation of [23] to semantic segmentation does not perform well. To improve the performance, an edge loss is added as a regularization term, which is calculated by using the mean squared error (MSE) between the deformation map d obtained by the saliency detector and target deformation map d t calculated based on segmentation labels. To combat trivial solutions, the target deformation map has denser sampling around object boundaries and is formulated by d t = f edge (f gauss (Y lr )), where Y lr is the uniformly downsampled segmentation label, f edge is an edge detection filter by convolution with a specific 3 × 3 kernel, and f gauss is Gaussian blur with σ = 1.

Since the distortions caused by the customized grids defined in Eqs. 2 and 3 can be severe, the method in [56] introduces structured grids that can be combined with customized grids to obtain a more subtle spatial distortion effect for wireless capsule endoscopy (WCE) image classification. These structured grids ensure that pixels that were in the same row/column in the input image are also in the same row/column in the output image, and can be obtained by
u(x) = x S(x )k(x, x )x x S(x )k(x, x ) ,(5)v(y) = y S(y )k(y, y )x y S(y )k(y, y ) ,(6)
where S(x) = max y S(x, y) and S(y) = max x S(x, y). u(x) and v(y) are then copied and stacked to form
u s (x, y) = u(x)
and v s (x, y) = v(y). Finally, the combined deformation grids can be computed by
u(x, y) = λu s (x, y) + (1 − λ)u c (x, y), (7) v(x, y) = λv s (x, y) + (1 − λ)v c (x, y),(8)
where parameter λ is empirically set to 0.5. Similarly, FOVEA [74] discards custom grids and solely relies on structured grids for object detection in autonomous driving use cases. It also introduces anti-cropping regularization to combat cropping which may result in missing objects, by using reflect padding on the saliency map. In [23], the saliency detector is trained end-to-end along with the task network, however, as mentioned, finding saliency maps in object detection is more difficult. Therefore FOVEA uses intermediate supervision to train the saliency detection network.

Even though the primary goal of the spatial transformer module in spatial transformer networks (STNs) [75] is to learn invariance to translation, scale, rotation and warping in order to improve performance, in the special case where the module is the first layer of the network, it can learn to crop the raw high-resolution input to a lower resolution and increase computational efficiency, thus it could be considered a form of NUD. Figure 7 shows the architecture of the spatial transformer module, where the localization network determines the parameters θ for the transformation τ θ from input features U . τ θ (·) can be a 2D affine transformation, a more constrained transformation such as
A θ = s 0 t x 0 s t y ,(9)
which only allows cropping, translation and scaling, or a more general transformation such as plane projective transformation with 8 parameters, piecewise affine, thin plate spline [76], or any transformation as long as it is differentiable with respect to its parameters. Fig. 7: Architecture of the spatial transformer module [75].

SALISA [77] uses spatial transformer modules to perform non-uniform downsampling for object detection in highresolution videos. In SALISA, the output of a video frame is used to determine the saliency map for the next frame. Figure  8 shows this method, where the first frame is introduced to a high-performing detector without any downsampling. The detected objects are subsequently used to create a saliency map, which is then given to the resampling module. The resampling module contains a spatial transformer module with a thin plate spline transformation, where the localization network receives the saliency map as input. The downsampled image provided by the resampling module is then introduced to a lightweight detector. Since the lightweight detector detects objects in the warped image, the detected bounding boxes need to be transformed back into the original grid. Therefore an inverse transformation is applied before generating the saliency map. To prevent cascading errors, the method is reset to use the original high-resolution frame and high-performing detector every few frames. Fig. 8: Overview of SALISA [77]. The second frame is slightly different from the first frame (in this case, slightly rotated clockwise), therefore, the detection result obtained from the first frame can be used to estimate the saliency of objects in the second frame.


## B. Selective Zooming and Skipping

Selective zooming and skipping (SZS) methods take a more efficient approach to cutting into patches by only zooming into regions of the input image that are important. The zoom level may differ across different patches, and some patches may be entirely skipped. Reinforced Auto-Zoom Net (RAZN) [78] uses reinforcement learning to determine where to zoom in WSIs for the task of breast cancer segmentation. RAZN assumes the zoom-in action can be performed at most m times and the zooming rate is a constant r. At each zoom level i, there is a different segmentation network f θi and a different policy network g θi . Initially, policy network g θ0 takes a cropped image x 0 ∈ R H×W ×3 as input and determines whether to zoom-in or to break. If there is no need to zoom in, x 0 is given as input to segmentation network f θ0 which produces the output, otherwise, a higher-resolution imagex 0 ∈ R rH×rW ×3 is sampled from the same area and will be cut into r 2 patches of size H × W × 3. Each patch is then given to policy network g θ1 and this process is recursively repeated until all policy networks break or the maximum zoom level is reached. RAZN achieves an improved performance over other stateof-the-art methods while reducing the inference time by a factor of ∼2. Similarly, the methods in [79] and [80] use reinforcement learning for efficient object detection and aerial image classification, respectively.

Instead of reinforcement learning, the method in [81] uses a hierarchichal graph neural network to classify whether a mammogram (X-ray image of a breast) is normal/benign (contains a tumor that is not cancerous) or malignant (contains a tumor that is cancerous). At each zoom level i, the graph G i is defined by the adjacency matrix A i ∈ R Ni×Ni where there is an edge between each zoomed-in patch and its original image. The feature matrix of the graph is defined as X i ∈ R Ni×D×D , and the maximum zoom level is R. The features on the nodes are zoomed-in regions of the input image, resized to D ×D. A pre-trained CNN is used to extract feature vectors H i ∈ R Ni×H from X i . GAT node (·) is a graph attention network [82] used to classify whether to zoom in for each node. Therefore, the output of the i-th level in the hierarchical graph is
P i = 1, i = 1, softmax(GAT node (A i , H i )), 1 < i < R,(10)
where P i ∈ R Ni×2 represents the decision to zoom or not for each node of the i-th level. At the final zoom level R, another graph attention network GAT graph (·) is used to perform the final classification for the entire mammogram based on
Y = softmax(GAT graph (A R , H R )W ),
where W is a trainable weight matrix. The loss function contains both node losses and graph losses, with the zoom labels for nodes being obtained from lesion segmentation labels. This method achieves an accuracy comparable to the state-of-the-art, however, it is unclear how much it improves the inference speed. GigaDet [83] achieves near real-time object detection in gigapixel videos. At the core of GigaDet is the Patch Generation Network (PGN). PGN takes a uniformly downsampled image as input and outputs a dense regression map which counts the number of objects that are completely contained within the corresponding area in the image, referred to as the patch candidate. PGN is applied at different scales in order to obtain patch candidates of varying scales. The patch candidates selected by the PGN go through post-processing which includes non-maximum suppression (NMS), and are subsequently sorted based on their count. The top K patch candidates are then selected to be processed by the Decorated Detector (DecDet) to detect objects. VGG [84] and YOLO [85] are used for the PGN and DecDet networks, respectively. Given gigapixel videos, GigaDet is capable of running 5 FPS on a single Nvidia 2080 Ti GPU, which is 50× faster than Faster RCNN [86], yet obtains a comparable performance in terms of average precision.

REMIX [87] detects pedestrians in high-resolution videos within a latency budget given by the user. The input frame is partitioned into several blocks, where more salient blocks are processed using a computationally expensive but accurate network whereas less salient blocks are processed using a computationally cheap network or even skipped, as shown in Figure 9. REMIX uses historical frames to determine the object distribution, and determines the optimal partition using a dynamic programming algorithm that takes into account the given latency budget, the estimated object distribution, as well as the accuracy and speed of available neural networks for object detection. REMIX achieves up to 8.1× inference speedup with an accuracy comparable to the state-of-the-art methods. Fig. 9: Partitioning in REMIX [87]. Some parts of the image are skipped, some processed by computationally cheap DNNs and some by computationally expensive DNNs.


## C. Lightweight Scanner Networks

Lightweight scanner networks (LSNs) are lightweight fully convolutional neural networks (FCNs) that efficiently scan the entire high-resolution input. To achieve a lightweight architecture, LSNs are typically designed and trained for very specific tasks. Moreover, as opposed to the cutting into patches approach, FCNs are inherently efficient in a sliding-window setting since they share the computation in overlapping regions [88].

VGG-720p and VGG-1080p [89], [90] are LSNs capable of running in real-time on drones and provide heatmaps for input images of size 1280×720 and 1920×1080 pixels, respectively, that specify whether or not there are people, faces, or bicycles at each location in the input image. Both models take patches of size 32×32 or 64×64 pixels as input. The architectures of VGG-720p and VGG-1080, shown in Tables II and III, respectively, contain only 5 convolutional layers with only 2 to 24 output channels. In contrast, the original VGG architectures have 11 to 19 layers with up to 512 output channels in some layers [84].  Similarly, the study in [91] proposes an architecture with 6 convolutional layers for the same problem of generating a crowd heatmap from high-resolution images. The study in [92] proposes lightweight FCNs for face detection with 7 convolutional layers and 76K parameters, for facial parts detection (such as eyes, nose and mouth) with 4 convolutional layers and 20K parameters, and for combined face and parts detection with 9 convolutional layers and 101K parameters.

You only look twice (YOLT) [9] is a method that detects objects of different scales in DigitalGlobe satellite images which have a size of over 250 megapixels. The architecture of YOLT is based on the YOLO architecture [85], however, it reduces the number of layers from the original 30 down to 22. Furthermore, YOLT trains two separate models: one which processes images that correspond to areas of 200×200m 2 for detecting relatively small objects such as cars, airplanes, boats and buildings; and another which processes images that correspond to areas of 2500×2500m 2 for detecting large objects such as airports. YOLT has an inference speed of 32km 2 /min for the former model and 6000km 2 /min for the latter on an Nvidia Titan X GPU.

Fast ScanNet [48] converts VGG16 [84] to a fully convolutional network by replacing the last fully-connected layers in VGG16 with convolutional layers of kernel size 1×1. Fast ScanNet is applied to patches of size 2800×2800 pixels, a size which is dictated by GPU memory limitations, taken from WSIs, which have ∼400 patches on average. It takes about one minute for Fast ScanNet to process a WSI on a workstation with 8×Nvidia Titan X GPUs.

ICNet [34] takes advantage of both the efficiency of processing lower resolutions and the accuracy of processing higher ones by uniformly downsampling the input image to two smaller scales, processing each scale separately, and fusing the result of processing lower resolutions with higher ones. Lower resolutions are processed with more convolution layers and higher resolutions with less, which makes the entire architecture efficient, as shown in Figure 10. In addition, some of the layers share weights in order to increase the efficiency. ICNet is able to perform semantic segmentation on 2048×1024 images at 30 frames per second with high accuracy on a Titan X GPU. Even though ICNet does not obtain state-ofthe-art accuracy, it is ∼ 15× faster than methods with similar performance.

ESPNet [93] relies on efficient spatial pyramid (ESP) modules which reduce the amount of computation by decomposing standard convolutions with n × n kernels into two steps. The first step applies a 1×1 convolution to project feature maps with dimension N to feature maps with dimension N K . The second step applies K dilated convolutions with kernel size n × n and dilation rates 2 k−1 , k ∈ {1, . . . , K} to the new feature maps simultaneously, and combines the results. Concatenating the outputs of dilated convolutions creates checkerboard artifacts, therefore, a simple solution is used where the outputs of dilated convolutions are hierarchically added to each other before concatenation. ESPNet can perform semantic segmentation on 2048×1024 images at 54 frames per second with an accuracy comparable to the state-of-the-art.

Neural architecture search (NAS) techniques can be used for designing better LSNs. Since LSNs need to be lightweight and contain few layers and parameters, the search space is relatively small, making NAS easier. HR-NAS [94] is one such method that searches for network architectures that can contain both convolutions and lightweight Transformers, and may have parallel branches. HR-NAS obtains state-of-the-art results in the trade-off between efficiency and accuracy in semantic segmentation, human pose estimation and 3D object detection tasks with high-resolution inputs.


## D. Task-Oriented Input Compression

Task-oriented input compression (TOIC) methods compress the high-resolution inputs into lightweight representations. These representations are then given to the task DNN as input instead of the high-resolution images or videos. The exact nature of the lightweight representations and the compression procedure varies from method to method and is often highly dependent on the underlying task.

There is an important distinction between this approach and neural image compression methods such as SlimCAE [95]. The goal of neural image compression is to learn optimal compression algorithms for the task at hand, in order to reduce the size of stored or transmitted data. Therefore, the network that compresses and decompresses this data may be very large and inefficient. Moreover, neural image compression aims to reconstruct the input from the compressed representations, whereas TOIC does not reconstruct the input data and strives to extract compact representations that are suitable for the second part of the network which is responsible for performing the task.

Slide Graph [96] recognizes the loss of visual context that comes with using the cutting into patches method, and fixes this issue by building and processing a compact graph representation of the cellular architecture in breast cancer WSIs in order to predict the status of human epidermal growth factor receptor 2 (HER2) and progesterone receptor (PR), which are proteins that promote the growth of cancer cells. Slide Graph has four stages: The first stage uses a HoVer-Net [97], which is a CNN for segmentation and classification of cellular nuclei, trained on the PanNuke dataset [98] to extract features of the tissue cells. The second stage uses agglomerative clustering [99] to group neighboring nuclei to further reduce the computational cost. The third stage constructs a graph where each vertex corresponds to a cluster and contains features extracted in the previous stage. Graph edges are constructed based on Delauney triangulation where vertices are represented by the geometric center of their corresponding cluster, which results in a planar graph. In the final stage, HER2 and PR status predictions are obtained from the constructed graph using a graph convolutional network (GCN) [100]. Slide graph is more accurate than state-of-theart methods and reduces the average inference time from 1.2 seconds of the baseline down to 0.4 milliseconds. However, these measurements do not include the graph construction phase. Therefore, the end-to-end improvement in efficiency obtained by Slide Graph is unclear.

The method in [101], shown in Figure 11, compresses gigapixel histopathology WSIs down to a size that can be processed with a CNN on a single GPU. This compression is obtained by training an autoencoder (either VAE [102] or bidirectional GAN [103]) on image patches of size P × P × 3. The WSI image of size M × N × 3 is then cut into patches of the aforementioned size, and compressed embeddings of size 1×1×C are obtained from the patches using the encoder part of the autoencoder. These embeddings are then concatenated to form a compressed image of size M P × N P × C, which can be given as input to the CNN. In experiments where M = N = 50, 000 and P = C = 128, the input size is reduced by a factor of ∼43. MCAT [104] uses a combination of WSIs and genomics data for cancer survival outcome prediction. At the core of MCAT is the Genomic-Guided Co-Attention (GCA) layer which reduces the spatial complexity of processing WSIs. MCAT processes the input in data structures known as bags, which are unordered sets of objects of varying size without individual labels. MCAT constructs one bag (H bag ) from multiple WSIs in order to utilize the entire tissue microenvironment, and another bag (G bag ) from genomic features. H bag is constructed by cutting the WSIs into non-overlapping 256 × 256 pixel patches and processing each patch with a ResNet50 CNN [105] pre-trained on the ImageNet dataset [106] to obtain d k -dimensional feature embeddings. G bag is constructed by categorizing genes into N different sets based on similarity and applying a fully-connected (FC) layer to obtain genomic embeddings. GCA then takes these two bags as input and performs the co-attention operation by
CoAttn G→H (G, H) = softmax QK T √ d k V (11) = softmax W q GH T W T k √ d k W v H,
where Q = W q G is the query matrix, K = W k H is the key matrix, V = W v H is the value matrix, and W q , W k , W v ∈ R d k ×d k are trainable weights. The output of this operation, as shown in Figure 12, has a dimension of N × d k . Therefore, the subsequent self-attention layers in the MCAT network are quadratic with respect to N instead of M . Since on average M = 15, 231 and N = 6, this results in a massive reduction in complexity. A subcategory of TOIC methods are frequency-domain DNNs, which convert input RGB pixels to frequency domain representations with the help of operations such as discrete cosine transform (DCT) or wavelet transform. The intuition behind this approach is that the first few layers in CNNs often learn filters that resemble such transforms. Therefore, not only are image representations more compact in the frequency domain, but also a lower number of layers is required for processing such representations.

The method in [107] uses the DCT coefficients obtained in the middle of JPEG encoding as inputs to a modified ResNet50 CNN [105] for the image classification task. JPEG encoding consists of three stages. The first stage converts the input 3- 

The luma component (Y) represents the brightness, and the chroma components (Cb and Cr) represent color. The resolution of chroma components is then reduced by a factor of 2 due to the fact that the human eye is less sensitive to fine color detail than fine brightness. Figure 13 shows an example image and its corresponding Y, Cb and Cr components. The second stage is a blockwise DCT, where each of the three components is partitioned into 8 × 8 blocks that undergo a 2D DCT. The amplitude values of the frequency domain are the input representations used by this method. The DCT representations of Cb and Cr are upsampled by a factor of two and concatenated with the DCT representation of Y before being given as input to the task DNN, as shown in Figure 14. The rest of the JPEG encoding process contains the quantization of these representations as well as lossless compression techniques such as Huffman coding. However, this method uses the representations obtained before quantization and lossless compression. With the help of these input representations, this method obtains DNNs that are both more accurate and up to 1.77× faster than ResNet50. Moreover, [107] includes experiments attempting to learn convolutions behaving like DCT, however, they find that this learned DCT transform leads to higher error compared to the conventional DCT transform.

The method in [108] uses the same idea for image classification and semantic segmentation tasks using ResNet50 and MobileNetV2 architectures. However, this method also prunes the 192 DCT channels with the help of a gating module that generates a binary decision for each channel. Furthermore, this study discovers that some channels are consistently pruned regardless of the particular task, and develops a static frequency channel selection scheme based on these results. This scheme prunes up to 87.5% of the channels with little accuracy drop, if any. The method in [109] uses the same approach for image classification, however, it uses several variants of discrete wavelet transform (DWT) instead of DCT. The advantage of DWT over DCT is that it can obtain a better compression ratio without loss of information, however, it is more computationally expensive [110]. Experiments show that using DWT instead of DCT can lead to higher accuracy, however, the impact of DWT on inference time is unclear.

Finally, similar to images, DNNs can directly process the compressed representations obtained by video compression formats. MMNet [111] performs efficient object detection on H.264/MPEG-4 Part 10 compressed videos [112], one of the most commonly used video compression formats, by taking advantage of the motion information already embedded in the video compression format. It only runs the complete feature extractor DNN on few reference frames in the video and aggregates the visual information from the subsequent frames with the help of an LSTM [113]. H.264 has two types of frames: I-frames which contain a complete image, and Pframes, also known as delta frames, which store the offset to previous frames using motion vectors and residual errors. In MMNet, the extracted motion vectors and residual errors for each P-frame following an I-frame are passed on to the LSTM. MMNet is 3× to 10× faster than competing models with minor loss in accuracy.


## E. High-Resolution Vision Transformers

As previously mentioned, the self-attention operation in Transformers has a high complexity that increases in a quadratic fashion with respect to the number of input tokens. This operation is formulated by
Z = softmax QK T √ d k V,(13)
where query Q = XW Q ∈ R n×dq , key K = XW K ∈ R n×d k and value V = XW V ∈ R n×dv are obtained from sequence of input tokens X = (x 1 , . . . , x n ) ∈ R n×d , and W Q , W K and W V are learnable weight matrices. Due to this quadratic complexity, naive approaches, such as ViT [6], that create a long sequence of input tokens from a high-resolution image will lead to massive complexity. On the other hand, if X contains few tokens, each input token represents a large area of the original image, leading to loss of detailed information that might be crucial to some applications.

Vision Longformer (ViL) [114] is a variant of Longformer [115] which has a linear complexity with respect to the number of input tokens, and is capable of processing high-resolution images. This linear complexity is achieved by adding n g global tokens, which include the classification token cls, that serve as global memory by attending to all input tokens. Input tokens are only allowed to attend to the global tokens as well as their neighbors within a 2D window. If the number of input tokens are n l and the 2D window size is w, then the memory complexity is O(n g (n g + n l ) + n l w 2 ). When n g n l , the complexity is significantly reduced from the original n 2 l in ViT. By using ViL in a multi-scale architecture, multi-scale Vision Longformer is able to obtain superior performance compared to the state-of-the-art in image classification, object detection and semantic segmentation while requiring less computation in terms of FLOPs in some cases.

High-Resolution Transformer (HRFormer) [116] reduces the computational complexity of self-attention by partitioning the input representations into non-overlapping patches, and performing the self-attention only within each patch. Figure  15 shows the building block of HRFormer, which contains a depth-wise convolution that facilitates information exchange between patches. By utilizing this augmented self-attention in a multi-scale architecture, HRFormer obtains superior performance in human pose estimation and semantic segmentation with fewer parameters and FLOPs. Multi-Scale High-Resolution Vision Transformer (HRViT) [117] uses cross-shaped self-attention [118] and parameter sharing to decrease the computational cost of self-attention. Cross-shaped self-attention, shown in Figure 16, splits the K self-attention heads present in multi-head attention into two groups: {h 1 , . . . , h K 2 } and {h K 2 +1 , . . . , h K }. These groups perform self-attention in horizontal and vertical strips in parallel. Strip width sw can be adjusted to achieve a trade-off between efficiency and performance. The linear projections for key and value tensors are shared in HRViT's blocks to save in computation and parameters. In addition to efficient self-attention, HRViT employs a convolutional stem to reduce the spatial dimension of the input. HRViT achieves the best performance-efficiency trade-off compared to state-of-the-art models for semantic segmentation.

Instead of restricting self-attention to patches that are neighbors in the 2D grid, Glance and Gaze Transformer (GG-Transformer) [119], shown in Figure 17, performs the self-attention within dilated partitions. Since these dilations create holes in the receptive field, a parallel branch containing depth-wise convoluion is added to compensate for the local interactions with negligible cost. GG-Transformer achieves superior performance in image classification, object detection and semantic segmentation and reduces the parameters or FLOPs in some cases. Hierarchical Image Pyramid Transformer (HIPT) [120] processes gigapixel WSIs for the task of cancer subtyping and survival prediction. Since the input WSIs are as large as 150,000×150,000 pixels, processing them with a normal ViT and small patch size, such as 16×16, results in a massive number of parameters and computational cost requirements, and using large patch sizes such as 4096×4096 pixels directly would result in loss of cellular information. Therefore, HIPT takes a hierarchical approach, shown in Figure 18, where an initial ViT processes patches of 16×16 in an area of size 256×256 pixels. A second ViT then takes the aggregated tokens from the previous ViT and processes an area of size 4096×4096 pixels. A final ViT takes the aggregated tokens from the second ViT and processes the entire image. Table IV lists popular datasets used in the high-resolution deep learning literature and provides information about their attributes, such as which is the deep learning application they have been primarily used for, the number of images/videos in the dataset and their resolution, the type of available annotations, whether they specify training/validation/test set splits, the year of publication, and whether they are publicly available. It is important to note that studies reported in some papers create customized datasets. For instance, [79] constructs a dataset from YFCC100M [121]; [89] constructs datasets from AFLW [122], MTFL [123] and WIDER FACE [124]; and [9] constructs datasets from DigitalGlobe satellites, Planet satellites, and aerial platforms.


## IV. HIGH-RESOLUTION DATASETS

The Cancer Genome Atlas (TCGA) program is a collaboration between National Cancer Institute (NCI) and National Human Genome Research (NHGRI) 1 . Since 2006, TCGA has generated over 2.5 petabytes of publicly available data which has led to improvements in cancer diagnosis, treatment, and prevention. Among efficient high-resolution deep learning methods, the most widely used subset of this data is the breast invasive carcinoma (BRCA), which is outlined in Table IV. However, TCGA provides data for many other types of cancer, such as bladder urothelial carcinoma (BLCA), glioblastoma and lower grade glioma (GBMLGG), lung adenocarcinoma (LUAD), and uterine corpus endometrial carcinoma (UCEC). These are used in some studies, and have properties similar to that of BRCA.


## V. DISCUSSION AND OPEN ISSUES

Each of the approaches introduced in Section III has its advantages and disadvantages and is useful in certain situations. NUD (Section III-A) works well in cases where the salient area is small compared to the entire image, and thus, it is possible to sample many pixels from such areas. This requirement is satisfied in gaze estimation or object detection problems. Our conjecture is that it would also work well in problems such as hand gesture detection and non-cropped facial expression recognition, although these tasks are not yet explored in the literature in combination with NUD. However, when the salient area is large, for instance densely populated scenes in visual crowd counting or a scene fully covered with objects in object detection, the quality gain obtained by sampling from salient areas will be negligible, and the result of NUD will be similar that of uniform downsampling [77].

Similarly, SZS methods (Section III-B) require the salient area to be small, otherwise they zoom everywhere and save little time and computation. This also means that the effectiveness of NUD and SZS methods may vary based on the specific input. For instance, the more people there are in an image processed for crowd counting, or the more tumors there are in cancer detection, the less efficient such methods will be, unless there are specific safeguards that prevent them from performing an enormous number of computations, such as GigaDet [83] which processes at most K patch candidates.

Furthermore, NUD methods are not effective when the resulting resolution is extremely smaller compared to the input resolution, for instance, when gigapixel inputs need to be resized down to HD, as this would result in highly distorted images which makes it difficult for the task DNN to perform well. Even when the gap between the two resolutions is not extremely large, NUD can lead to high distortions in some cases, for instance, it may completely distort and change the shape of the edges of a gastrointestinal lesion, making it difficult for the task network to detect useful features. This may reduce accuracy despite the fact that more pixels are sampled from salient areas. As explained in Section III-A, some methods try to mitigate the distortion by using structured grids. However, this may limit the benefits obtained by NUD.

In addition, since NUD is enlarging some parts of the image compared to uniform downsampling, some areas of the resulting image will be smaller than they would be with uniform downsampling. Thus, if the saliency map is not of high quality, unimportant areas will be enlarged and the ones important for the final task will shrink, resulting in accuracy loss. This is directly at odds with the requirement that the saliency detection method should be low-overhead, creating another trade-off that needs to be carefully balanced. Moreover, as explained in Section III-A, some variations of NUD require an external supervision signal or regularization term to train the saliency detection network, which can be difficult to design. In NUD or SZS methods that detect saliency in videos based on the results obtained from previous frames, such as SALISA [77] and REMIX [87], when the difference between subsequent frames is high, the method needs to be reset to processing the entire high-resolution image. When this occurs frequently, the obtained benefits are diminished.

As mentioned in Section III-C, LSNs need to designed, trained and well optimized for the specific problem at hand, which is not an easy task. Furthermore, since LSNs produce an output for each scanned area of the input, they are suitable for tasks where the output has the form of a map, such as dense classification or dense regression problems. Moreover, the scanning nature of LSNs means that all areas of the image are treated similarly, therefore, they are better suited for situations where there is no perspective and objects of the same type have the same size regardless of their location, such as WSIs and remote sensing, as opposed to surveillance and crowd counting where people close to the camera are larger than people far away.

Since TOIC methods extract representations that are both Public § A frame is a single image in a sequence representing a video * The locaion for the center of each human head in the image is specified † A measure of the number of cells in a tumor that are dividing ‡ https://github.com/supervisely-ecosystem/persons compressed and suitable for the task at hand, they often need to be tailored to the specific problem, which requires high domain knowledge. Both Slide Graph [96] and MCAT [104] presented in Section III-D are based on domain knowledge about cellular structure of tissues and biological function of genes, respectively. Almost all frequency-domain DNNs try to preserve the architecture of the CNNs they are based on. However, since the interpretation of features in frequencydomain is different, and they have certain properties such as being non-negative, it might be better to customize the architectural elements for the frequency domain, as CS-Fnet [152] does.

Most high-resolution Vision Transformer methods try to reduce the quadratic cost of self-attention to linear, and then compensate the accuracy loss by learning data transformations using convolutions. To keep the overhead of convolutions low, depth-wise convolution is typically used. Additionally, most high-resolution ViTs utilize a multi-scale architecture in order to capture features of various scales. High-resolution ViTs are more general purpose than other high-resolution deep learning methods and are often used for a large variety of tasks.

VI. CONCLUSION AND OUTLOOK Processing high-resolution images and videos with deep learning is crucial in various domains of science and technology. However, few methods exist that address the computational challenges. Among existing methods, the trend of designing solutions specifically for the problem at hand is clearly visible. This can be an issue in tasks for which high-resolution datasets are not available. Similar to model compression approaches, both modifying existing methods and designing an efficient high-resolution method from scratch are viable approaches.

Efficient high-resolution deep learning is in its infancy and there is a lot of room for improvement. For instance, a number of attention-free MLP-based methods have been recently proposed as lightweight alternatives for Transformers [153], which try to mimic the global receptive field of Transformers without the self-attention mechanism. Exploiting such architectures for efficient processing of high-resolution inputs would be an interesting research direction. Furthermore, the multimodal co-attention in MCAT [104] can be applied to many other multimodal tasks, especially the ones with audio, vision and language modalities. Moreover, frequency-domain representations can be explored as inputs to ViTs, which can lead to more efficiency compared to frequency-domain CNNs. For instance, ViTs can take separate patches from DCT-Cb, DCT-Cr and DCT-Y components, bypassing the need to upsample DCT-Cb and DCT-Cr to match the dimensions of DCT-Y.

The combination of efficient high-resolution deep learning with other efficient deep learning methods, such as model compression [154], dynamic inference [155], collaborative inference [156] and continual inference [157], is an unexplored area of research. For instance, if the saliency detection network is a lightweight version of the task network, NUD can be combined with early exiting, where the output of the saliency detection network would be a fast, but less accurate, early result. This is simple to implement in dense regression problems such as depth estimation and crowd counting, where the output of the task can be interpreted as a form of saliency.

Moreover, with the adoption of edge and cloud computing, transmission of high-resolution inputs to servers for processing is a real challenge. As a solution, efficient high-resolution deep learning methods can be combined with edge computing paradigms. For instance, the downsampled images in NUD and compressed representation in TOIC can be transmitted instead of the original inputs. This would be a form of split computing (also known as collaborative intelligence) [158], [159], where the initial portion of computation is performed on a resource-constrained end-device, and the compact intermediate representation is then transmitted to a server where the rest of the computation is carried out. A study using this idea for high-resolution images captured by drones is reported in [160].

## Fig. 2 :
2As the resolution of the input image increases, so does (a) the amount of computation, (b) inference time, and (c) GPU memory usage in the EfficientNet-B7

## Fig. 4 :
4Schematic illustration of a multi-column architecture.

## Fig. 5 :
5Overlap in the field of view for multi-camera setups, which can result in duplicates in tasks such as crowd counting.

## Fig. 10 :
10ICNet architecture. CFF blocks perform the fusion operation and consist of convolution and upsample layers. CFF blocks get supervision signals using downsampled annotations during the training process.

## Fig. 11 :
11A method based on neural image compression for gigapixel histopathology images.

## Fig. 12 :
12Genomic-Guided Co-Attention (GCA) layer.

## Fig. 13 :
13(a) Original color image, taken from the Shanghai Tech Part B dataset [20]; (b) luma component Y, which is essentially a grayscale version of the color image; (c) chroma component Cb; and (d) chroma component Cr. channel 24-bit RGB image to the YCbCr color space by

## Fig. 14 :
14Initial stages of JPEG encoding, used by[107] to obtain frequency-domains representations of the RGB input.

## Fig. 15 :
15HRFormer block. Multi-head self-attention (MHSA) is applied only within each patch. The patches are then concatenated and followed by a depth-wise (DW) convolution.

## Fig. 16 :
16Cross-shaped self-attention.

## Fig. 17 :
17GG-Transformer block.

## Fig. 18 :
18Hierarchical Image Pyramid Transformer (HIPT). The notation ViT L − l means a Vision Transformer that operates on size L × L with patch size of l × l. ViT WSI operates on the entire WSI.

## TABLE I :
IPerformance of baseline approaches on the Shanghai Tech Part B dataset.Input Size 
Original MAE 
UD  *  MAE 
CIP  † MAE 
1024×768 (original) 
6.31 
-
-
512×384 (reduced 4×) 
-
9.01 (+43%) 
6.40 (+1%) 
256×192 (reduced 16×) 
-
16.06 (+155%) 
6.67 (+6%) 
 *  Uniform Downsampling 
 † Cutting into Patches 



## TABLE II :
IIArchitecture of VGG-720p. Zero padding * X and Y represent the horizontal and vertical axesLayer 
Kernel Stride Pad  † (X/Y)  *  
Max Pool (X/Y) Channels 
conv1 1 
3×3 
1/1 
1/1 
-/ -
16 
conv1 2 
3×3 
1/1 
1/1 
/ -
16 
conv2 1 
3×3 
1/1 
1/1 
-/ -
24 
conv2 2 
3×3 
1/4 
1/1 
/ 
16 
conv last 
8×8 
1/1 
0/0 
-/ -
2 
 † 

## TABLE III :
IIIArchitecture of VGG-1080p. Zero padding * X and Y represent the horizontal and vertical axesLayer 
Kernel Stride Pad  † (X/Y)  *  
Max Pool (X/Y) Channels 
conv1 1 
3×3 
2/1 
0/0 
-/ -
8 
conv1 2 
3×3 
1/2 
0/0 
/ -
8 
conv2 1 
3×3 
1/1 
0/0 
-/ -
6 
conv2 2 
3×3 
1/2 
0/0 
-/ -
6 
conv last 
8×8 
1/1 
0/0 
-/ -
2 
 † 

## TABLE IV :
IVList of Popular High-Resolution DatasetsName 
Applications 
Resolution (Pixels) 
# of Samples 
Annotations 
Splits 
Year 
Availability 

Supervisely Persons  ‡ 
Person Segmentation 
800×1116 to 9933×6622 
5711 images 
Pixel Mask 
None 
2018 
Public 
PANDA [13] 
Person Detection 
>25K×14K 
555 frames  § 
Person Bounding Box None 
2020 
Upon Request 
UCF CC 50 [125] 
Crowd Counting 
2888×2101 on average 
50 images 
Head Annotations  *  
None 
2013 
Public 
Shanghai Tech Part A [126] 
Crowd Counting 
868×589 
482 images 
Head Annotations 
Train & Test 
2016 
Public 
Shanghai Tech Part B [126] 
Crowd Counting 
1024×768 
716 images 
Head Annotations 
Train & Test 
2016 
Public 
UCF-QNRF [127] 
Crowd Counting 
2902×2013 on average 
1535 images 
Head Annotations 
Train & Test 
2018 
Public 
PANDA Crowd [13] 
Crowd Counting 
25,151×14,151 to 26,908×15,024 45 images 
Person Bounding Box None 
2020 
Upon Request 
JHU-CROWD++ [128] 
Crowd Counting 
1430×910 on average 
4372 images 
Head Annotations 
Train, Val & Test 
2020 
Public 
NWPU-Crowd [129] 
Crowd Counting 
3209×2191 on average 
5109 images 
Head Annotations 
Train, Val & Test 
2020 
Public 
DISCO [130] 
Audio-Visual Crowd Counting 
1920×1080 (Full HD) 
1935 images 
Head Annotations 
Train & Test 
2020 
Public 
CityScapes [131] 
Autonomous Driving 
2048×1024 
5K images 
Pixel Mask 
Train, Val & Test 
2016 
Upon Request 
SYNTHIA-RAND [132] 
Autonomous Driving 
1280×720 (HD) 
∼13K images 
Pixel Mask 
Train & Test 
2016 
Public 
ApolloScape [133] 
Autonomous Driving 
3384×2710 
∼113K images 
Pixel Mask 
Train & Test 
2020 
Upon Request 
Argoverse-HD [134] 
Autonomous Driving 
1920×1200 
89 videos 
Bounding Box 
Train, Val & Test 
2020 
Public 
BDD100K [135] 
Autonomous Driving 
1280×720 (HD) 
100K videos 
Bounding Box 
Train, Val & Test 
2020 
Upon Request 
PASCAL-Context [136] 
Scene Understanding 
500×375 to 500×500 
10,103 images 
Pixel Mask 
Train & Test 
2014 
Public 
ADE20K [137] 
Scene Understanding 
683×512 to 2100×2100 
27,574 images 
Pixel Mask 
Train & Test 
2017 
Upon Request 
COCO-Stuff 10K [138] 
Scene Understanding 
∼640×480 
10K images 
Pixel Mask 
Train & Test 
2018 
Public 
DeepGlobe [139] 
Land Cover Classification 
2448×2448 
1146 images 
Pixel Mask 
Train, Val & Test 
2018 
Public 
Copernicus [140] 
Land Cover Classification 
20,160×20,160 
94 images 
Pixel Mask 
None 
2015-2019 Public 
fMoW [141] 
Aerial Image Classification 
up to 16,032×14,840 
1,047,691 images Classes 
Train, Val & Test 
2018 
Public 
KID [142] 
Capsule Endoscopy 
360×360 
∼2500 frames 
Pixel Mask 
None 
2017 
Public (N/A) 
CAD-CAP [143] 
Capsule Endoscopy 
576×576 
25,124 frames 
Pixel Mask 
Train & Test 
2020 
Upon Request 
CAMELYON16 [124] 
Pathology 
up to 200,000×100,000 
400 images 
Pixel Mask 
Train & Test 
2016 
Public 
TUPAC16 [144] 
Pathology 
∼50,000×50,000 
821 images 
Proliferation Score  † 
Train & Test 
2016 
Public 
BACH Part B [145] 
Pathology 
(39,980-62,952)×(27,972-44,889) 
40 images 
Pixel Mask 
Train & Test 
2019 
Public 
TCGA-BRCA [146] 
Pathology 
up to 150,000×100,000 
709 images 
Classes 
None 
2020 
Public 
PCa-Histo [73] 
Pathology 
(1968±216)×(9392±4794) 
266 images 
Pixel Mask 
Train, Val & Test 
2021 
Private 
INbreast [147] 
Breast Cancer Detection 
2560×3328 to 3328×4084 
410 images 
Pixel Mask 
Train & Test 
2012 
Public 
UA-DETRAC [148] 
Video Object Detection 
960×540 
140K frames 
Bounding Box 
Train & Test 
2015 
Public 
ImageNet-VID [149] 
Video Object Detection 
176×132 to 1280×720 (HD) 
5354 videos 
Bounding Box 
Train, Val & Test 
2015 
Public 
FAIR1M [150] 
Fine-Grained Object Detection 600×600 to 10,000×10,000 
40,000 images 
Bounding Box 
Train & Test 
2021 
Public (N/A) 

COCO [151] 
Object Detection 
Human Pose Estimation 
∼640×480 
>200K images 
Pixel Mask 
Keypoints 
Train, Val & Test 
2014 

https://www.cancer.gov/about-nci/organization/ccg/research/ structural-genomics/tcga

Gigapixel behavioral and neural activity imaging with a novel multi-camera array microscope. E Thomson, M Harfouche, bioRxivE. Thomson, M. Harfouche et al., "Gigapixel behavioral and neural activity imaging with a novel multi-camera array microscope," bioRxiv, 2021.

Multiscale gigapixel video: A cross resolution image matching and warping approach. X Yuan, L Fang, IEEE International Conference on Computational Photography. X. Yuan, L. Fang et al., "Multiscale gigapixel video: A cross reso- lution image matching and warping approach," in IEEE International Conference on Computational Photography, 2017, pp. 1-9.

Timelapse gigapan: Capturing, sharing, and exploring timelapse gigapixel imagery. R Sargent, C Bartley, Fine International Conference on Gigapixel Imaging for Science. R. Sargent, C. Bartley et al., "Timelapse gigapan: Capturing, sharing, and exploring timelapse gigapixel imagery," in Fine International Conference on Gigapixel Imaging for Science, 2010.

Whole slide imaging in pathology: advantages, limitations, and emerging perspectives. N Farahani, A V Parwani, Pathology and Laboratory Medicine International. 7234321N. Farahani, A. V. Parwani et al., "Whole slide imaging in pathology: advantages, limitations, and emerging perspectives," Pathology and Laboratory Medicine International, vol. 7, no. 23-33, p. 4321, 2015.

Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography. J Chen, L Wu, Scientific Reports. 10119196J. Chen, L. Wu et al., "Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomogra- phy," Scientific Reports, vol. 10, no. 1, p. 19196, 2020.

An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, International Conference on Learning Representations. A. Dosovitskiy, L. Beyer et al., "An image is worth 16x16 words: Transformers for image recognition at scale," in International Conference on Learning Representations, 2021. [Online]. Available: https://openreview.net/forum?id=YicbFdNTTy

Cnn-based density estimation and crowd counting: A survey. G Gao, J Gao, arXiv:2003.12783arXiv preprintG. Gao, J. Gao et al., "Cnn-based density estimation and crowd counting: A survey," arXiv preprint arXiv:2003.12783, 2020.

Deep learning in histopathology: the path to the clinic. J Van Der Laak, G Litjens, F Ciompi, Nature Medicine. 275J. van der Laak, G. Litjens, and F. Ciompi, "Deep learning in histopathology: the path to the clinic," Nature Medicine, vol. 27, no. 5, pp. 775-784, 2021.

You only look twice: Rapid multi-scale object detection in satellite imagery. A Van Etten, arXiv:1805.09512arXiv preprintA. Van Etten, "You only look twice: Rapid multi-scale object detection in satellite imagery," arXiv preprint arXiv:1805.09512, 2018.

Gradient-based learning applied to document recognition. Y Lecun, L Bottou, Proceedings of the IEEE. 8611Y. Lecun, L. Bottou et al., "Gradient-based learning applied to docu- ment recognition," Proceedings of the IEEE, vol. 86, no. 11, pp. 2278- 2324, 1998.

Imagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Proceedings of the 25th International Conference on Neural Information Processing Systems. the 25th International Conference on Neural Information Processing SystemsRed Hook, NY, USACurran Associates Inc1ser. NIPS'12A. Krizhevsky, I. Sutskever, and G. E. Hinton, "Imagenet classification with deep convolutional neural networks," in Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1, ser. NIPS'12. Red Hook, NY, USA: Curran Associates Inc., 2012, p. 1097-1105.

A survey of singlescene video anomaly detection. B Ramachandra, M J Jones, R R Vatsavai, IEEE Transactions on Pattern Analysis and Machine Intelligence. 445B. Ramachandra, M. J. Jones, and R. R. Vatsavai, "A survey of single- scene video anomaly detection," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 5, pp. 2293-2312, 2022.

Panda: A gigapixel-level human-centric video dataset. X Wang, X Zhang, IEEE/CVF conference on computer vision and pattern recognition. X. Wang, X. Zhang et al., "Panda: A gigapixel-level human-centric video dataset," in IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 3268-3278.

To choose or to fuse? scale selection for crowd counting. Q Song, C Wang, AAAI Conference on Artificial Intelligence. 35Q. Song, C. Wang et al., "To choose or to fuse? scale selection for crowd counting," in AAAI Conference on Artificial Intelligence, vol. 35, no. 3, 2021, pp. 2576-2583.

Efficient transformers: A survey. Y Tay, M Dehghani, arXiv:2009.06732arXiv preprintY. Tay, M. Dehghani et al., "Efficient transformers: A survey," arXiv preprint arXiv:2009.06732, 2020.

Efficientnet: Rethinking model scaling for convolutional neural networks. M Tan, Q Le, International Conference on Machine Learning. M. Tan and Q. Le, "Efficientnet: Rethinking model scaling for con- volutional neural networks," in International Conference on Machine Learning, 2019, pp. 6105-6114.

Megatron-lm: Training multi-billion parameter language models using model parallelism. M Shoeybi, M Patwary, arXiv:1909.08053arXiv preprintM. Shoeybi, M. Patwary et al., "Megatron-lm: Training multi-billion parameter language models using model parallelism," arXiv preprint arXiv:1909.08053, 2019.

Techniques for training large neural networks. L Weng, G Brockman, L. Weng and G. Brockman. (2022) Techniques for training large neural networks. [Online]. Available: https://openai.com/blog/ techniques-for-training-large-neural-networks/

Model parallelism optimization for distributed inference via decoupled cnn structure. J Du, X Zhu, IEEE Transactions on Parallel and Distributed Systems. 327J. Du, X. Zhu et al., "Model parallelism optimization for distributed inference via decoupled cnn structure," IEEE Transactions on Parallel and Distributed Systems, vol. 32, no. 7, pp. 1665-1676, 2020.

Single-image crowd counting via multicolumn convolutional neural network. Y Zhang, D Zhou, IEEE conference on computer vision and pattern recognition. Y. Zhang, D. Zhou et al., "Single-image crowd counting via multi- column convolutional neural network," in IEEE conference on com- puter vision and pattern recognition, 2016, pp. 589-597.

Petapixel photography and the limits of camera information capacity. D J Brady, D L Marks, Computational Imaging XI. D. J. Brady, D. L. Marks et al., "Petapixel photography and the limits of camera information capacity," in Computational Imaging XI, 2013, pp. 87-93.

Capturing cellular topology in multigigapixel pathology images. W Lu, S Graham, IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. W. Lu, S. Graham et al., "Capturing cellular topology in multi- gigapixel pathology images," in IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2020, pp. 260-261.

Learning to zoom: a saliencybased sampling layer for neural networks. A Recasens, P Kellnhofer, European Conference on Computer Vision. A. Recasens, P. Kellnhofer et al., "Learning to zoom: a saliency- based sampling layer for neural networks," in European Conference on Computer Vision, 2018, pp. 51-66.

Remote sensing image scene classification meets deep learning: Challenges, methods, benchmarks, and opportunities. G Cheng, X Xie, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 13G. Cheng, X. Xie et al., "Remote sensing image scene classification meets deep learning: Challenges, methods, benchmarks, and opportu- nities," IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 13, pp. 3735-3756, 2020.

Deep learning for whole slide image analysis: An overview. N Dimitriou, O Arandjelović, P D Caie, https:/www.frontiersin.org/articles/10.3389/fmed.2019.00264Frontiers in Medicine. 6N. Dimitriou, O. Arandjelović, and P. D. Caie, "Deep learning for whole slide image analysis: An overview," Frontiers in Medicine, vol. 6, 2019. [Online]. Available: https://www.frontiersin.org/articles/ 10.3389/fmed.2019.00264

Decoupled weight decay regularization. I Loshchilov, F Hutter, International Conference on Learning Representations. I. Loshchilov and F. Hutter, "Decoupled weight decay regularization," in International Conference on Learning Representations, 2019.

Adam: A method for stochastic optimization. D P Kingma, J Ba, International Conference on Learning Representations. D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," in International Conference on Learning Representations, 2015.

Deep neural network models for computational histopathology: A survey. C L Srinidhi, O Ciga, A L Martel, Medical Image Analysis. 67101813C. L. Srinidhi, O. Ciga, and A. L. Martel, "Deep neural network models for computational histopathology: A survey," Medical Image Analysis, vol. 67, p. 101813, 2021.

Ct imaging with ultra-high-resolution: Opportunities for cardiovascular imaging in clinical practice. J D Schuijf, J A Lima, Journal of Cardiovascular Computed Tomography. 165J. D. Schuijf, J. A. Lima et al., "Ct imaging with ultra-high-resolution: Opportunities for cardiovascular imaging in clinical practice," Journal of Cardiovascular Computed Tomography, vol. 16, no. 5, pp. 388-396, 2022. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S1934592522000235

How well do deep learning-based methods for land cover classification and object detection perform on high resolution remote sensing imagery?. X Zhang, L Han, Remote Sensing. 123X. Zhang, L. Han et al., "How well do deep learning-based methods for land cover classification and object detection perform on high resolution remote sensing imagery?" Remote Sensing, vol. 12, no. 3, 2020.

A survey on deep learning-based change detection from high-resolution remote sensing images. H Jiang, M Peng, Remote Sensing. 1472022H. Jiang, M. Peng et al., "A survey on deep learning-based change de- tection from high-resolution remote sensing images," Remote Sensing, vol. 14, no. 7, 2022.

A unified multi-scale deep convolutional neural network for fast object detection. Z Cai, Q Fan, Computer Vision -ECCV 2016. B. Leibe, J. Matas et al.Springer International PublishingZ. Cai, Q. Fan et al., "A unified multi-scale deep convolutional neural network for fast object detection," in Computer Vision -ECCV 2016, B. Leibe, J. Matas et al., Eds. Cham: Springer International Publishing, 2016, pp. 354-370.

Deep high-resolution representation learning for visual recognition. J Wang, K Sun, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4310J. Wang, K. Sun et al., "Deep high-resolution representation learning for visual recognition," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 10, pp. 3349-3364, 2021.

Icnet for real-time semantic segmentation on high-resolution images. H Zhao, X Qi, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)H. Zhao, X. Qi et al., "Icnet for real-time semantic segmentation on high-resolution images," in Proceedings of the European Conference on Computer Vision (ECCV), September 2018.

Zoomcount: A zooming mechanism for crowd counting in static images. U Sajid, H Sajid, IEEE Transactions on Circuits and Systems for Video Technology. 30U. Sajid, H. Sajid et al., "Zoomcount: A zooming mechanism for crowd counting in static images," IEEE Transactions on Circuits and Systems for Video Technology, vol. 30, no. 10, pp. 3499-3512, 2020.

Locality-aware crowd counting. J T Zhou, L Zhang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 447J. T. Zhou, L. Zhang et al., "Locality-aware crowd counting," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 7, pp. 3602-3613, 2021.

Recurrent attentive zooming for joint crowd counting and precise localization. C Liu, X Weng, Y Mu, IEEE/CVF Conference on Computer Vision and Pattern Recognition. C. Liu, X. Weng, and Y. Mu, "Recurrent attentive zooming for joint crowd counting and precise localization," in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.

Learn to scale: Generating multipolar normalized density maps for crowd counting. C Xu, K Qiu, IEEE/CVF International Conference on Computer Vision. C. Xu, K. Qiu et al., "Learn to scale: Generating multipolar normal- ized density maps for crowd counting," in IEEE/CVF International Conference on Computer Vision, 2019.

Deep multi-magnification networks for multi-class breast cancer image segmentation. D J Ho, D V Yarlagadda, Computerized Medical Imaging and Graphics. 88101866D. J. Ho, D. V. Yarlagadda et al., "Deep multi-magnification networks for multi-class breast cancer image segmentation," Computerized Med- ical Imaging and Graphics, vol. 88, p. 101866, 2021.

KGZNet: knowledge-guided deep zoom neural networks for thoracic disease classification. K Wang, X Zhang, S Huang, IEEE International Conference on Bioinformatics and Biomedicine (BIBM). K. Wang, X. Zhang, and S. Huang, "KGZNet: knowledge-guided deep zoom neural networks for thoracic disease classification," in IEEE International Conference on Bioinformatics and Biomedicine (BIBM), 2019, pp. 1396-1401.

Ear density estimation from high resolution rgb imagery using deep learning technique. S Madec, X Jin, Agricultural and Forest Meteorology. 264S. Madec, X. Jin et al., "Ear density estimation from high resolution rgb imagery using deep learning technique," Agricultural and Forest Meteorology, vol. 264, pp. 225-234, 2019.

Road extraction from high-resolution remote sensing imagery using deep learning. Y Xu, Z Xie, Remote Sensing. 1091461Y. Xu, Z. Xie et al., "Road extraction from high-resolution remote sensing imagery using deep learning," Remote Sensing, vol. 10, no. 9, p. 1461, 2018.

Detecting cancer metastases on gigapixel pathology images. Y Liu, K Gadepalli, arXiv:1703.02442arXiv preprintY. Liu, K. Gadepalli et al., "Detecting cancer metastases on gigapixel pathology images," arXiv preprint arXiv:1703.02442, 2017.

Automated identification of malignancy in whole-slide pathological images: identification of eyelid malignant melanoma in gigapixel pathological slides using deep learning. L Wang, L Ding, British Journal of Ophthalmology. 1043L. Wang, L. Ding et al., "Automated identification of malignancy in whole-slide pathological images: identification of eyelid malignant melanoma in gigapixel pathological slides using deep learning," British Journal of Ophthalmology, vol. 104, no. 3, pp. 318-323, 2020.

Beyond classification: Whole slide tissue histopathology analysis by end-to-end part learning. C Xie, H Muhammad, Conference on Medical Imaging with Deep Learning. C. Xie, H. Muhammad et al., "Beyond classification: Whole slide tissue histopathology analysis by end-to-end part learning," in Conference on Medical Imaging with Deep Learning, 2020, pp. 843-856.

Robust whole slide image analysis for cervical cancer screening using deep learning. S Cheng, S Liu, Nature Communications. 1215639S. Cheng, S. Liu et al., "Robust whole slide image analysis for cer- vical cancer screening using deep learning," Nature Communications, vol. 12, no. 1, p. 5639, 2021.

Texture-based deep learning for effective histopathological cancer image classification. N Z Tsaku, S C Kosaraju, IEEE International Conference on Bioinformatics and Biomedicine (BIBM). N. Z. Tsaku, S. C. Kosaraju et al., "Texture-based deep learning for effective histopathological cancer image classification," in IEEE International Conference on Bioinformatics and Biomedicine (BIBM), 2019, pp. 973-977.

Fast scannet: Fast and dense analysis of multigigapixel whole-slide images for cancer metastasis detection. H Lin, H Chen, IEEE Transactions on Medical Imaging. 388H. Lin, H. Chen et al., "Fast scannet: Fast and dense analysis of multi- gigapixel whole-slide images for cancer metastasis detection," IEEE Transactions on Medical Imaging, vol. 38, no. 8, pp. 1948-1958, 2019.

Joint semi-supervised and active learning for segmentation of gigapixel pathology images with cost-effective labeling. Z Lai, C Wang, IEEE/CVF International Conference on Computer Vision Workshops. Z. Lai, C. Wang et al., "Joint semi-supervised and active learning for segmentation of gigapixel pathology images with cost-effective labeling," in IEEE/CVF International Conference on Computer Vision Workshops, October 2021, pp. 591-600.

Deep multiresolution cellular communities for semantic segmentation of multi-gigapixel histology images. S Javed, A Mahmood, IEEE/CVF International Conference on Computer Vision Workshops. S. Javed, A. Mahmood et al., "Deep multiresolution cellular communi- ties for semantic segmentation of multi-gigapixel histology images," in IEEE/CVF International Conference on Computer Vision Workshops, 2019.

Deep learning for detecting corona virus disease 2019 (covid-19) on high-resolution computed tomography: a pilot study. S Yang, L Jiang, Annals of Translational Medicine. 87S. Yang, L. Jiang et al., "Deep learning for detecting corona virus disease 2019 (covid-19) on high-resolution computed tomography: a pilot study," Annals of Translational Medicine, vol. 8, no. 7, pp. 450- 450, 2020.

Deep learning reconstruction improves image quality of abdominal ultra-high-resolution ct. M Akagi, Y Nakamura, European Radiology. 2911M. Akagi, Y. Nakamura et al., "Deep learning reconstruction improves image quality of abdominal ultra-high-resolution ct," European Radi- ology, vol. 29, no. 11, pp. 6163-6171, 2019.

EM-stellar: benchmarking deep learning for electron microscopy image segmentation. A Khadangi, T Boudier, V Rajagopal, Bioinformatics. 371A. Khadangi, T. Boudier, and V. Rajagopal, "EM-stellar: benchmarking deep learning for electron microscopy image segmentation," Bioinfor- matics, vol. 37, no. 1, pp. 97-106, 2021.

Evaluation of moyamoya disease in ct angiography using ultra-high-resolution computed tomography: Application of deep learning reconstruction. Y Fukushima, Y Fushimi, European Journal of Radiology. 151110294Y. Fukushima, Y. Fushimi et al., "Evaluation of moyamoya disease in ct angiography using ultra-high-resolution computed tomography: Application of deep learning reconstruction," European Journal of Radiology, vol. 151, p. 110294, 2022. [Online]. Available: https: //www.sciencedirect.com/science/article/pii/S0720048X22001449

The future of ct: deep learning reconstruction. C Mcleavy, M Chunara, Clinical Radiology. 766C. McLeavy, M. Chunara et al., "The future of ct: deep learning reconstruction," Clinical Radiology, vol. 76, no. 6, pp. 407-415, 2021. [Online]. Available: https://www.sciencedirect.com/science/article/pii/ S0009926021000672

Zoom in lesions for better diagnosis: Attention guided deformation network for wce image classification. X Xing, Y Yuan, M Q , -H Meng, IEEE Transactions on Medical Imaging. 3912X. Xing, Y. Yuan, and M. Q.-H. Meng, "Zoom in lesions for better diagnosis: Attention guided deformation network for wce image classi- fication," IEEE Transactions on Medical Imaging, vol. 39, no. 12, pp. 4047-4059, 2020.

Comprehensive survey of deep learning in remote sensing: theories, tools, and challenges for the community. J E Ball, D T Anderson, C S Chan Sr, Journal of applied remote sensing. 11442609J. E. Ball, D. T. Anderson, and C. S. Chan Sr, "Comprehensive survey of deep learning in remote sensing: theories, tools, and challenges for the community," Journal of applied remote sensing, vol. 11, no. 4, p. 042609, 2017.

Building detection in very high resolution multispectral data with deep learning features. M Vakalopoulou, K Karantzalos, IEEE International Geoscience and Remote Sensing Symposium. M. Vakalopoulou, K. Karantzalos et al., "Building detection in very high resolution multispectral data with deep learning features," in IEEE International Geoscience and Remote Sensing Symposium, 2015, pp. 1873-1876.

Comparative research on deep learning approaches for airplane detection from very high-resolution satellite images. U Alganci, M Soydas, E Sertel, Remote Sensing. 123458U. Alganci, M. Soydas, and E. Sertel, "Comparative research on deep learning approaches for airplane detection from very high-resolution satellite images," Remote Sensing, vol. 12, no. 3, p. 458, 2020.

Forest damage assessment using deep learning on high resolution remote sensing data. Z M Hamdi, M Brandmeier, C Straub, Remote Sensing. 11171976Z. M. Hamdi, M. Brandmeier, and C. Straub, "Forest damage assess- ment using deep learning on high resolution remote sensing data," Remote Sensing, vol. 11, no. 17, p. 1976, 2019.

Looking outside the window: Wide-context transformer for the semantic segmentation of high-resolution remote sensing images. L Ding, D Lin, IEEE Transactions on Geoscience and Remote Sensing. 60L. Ding, D. Lin et al., "Looking outside the window: Wide-context transformer for the semantic segmentation of high-resolution remote sensing images," IEEE Transactions on Geoscience and Remote Sens- ing, vol. 60, pp. 1-13, 2022.

High-resolution remote sensing image captioning based on structured attention. R Zhao, Z Shi, Z Zou, IEEE Transactions on Geoscience and Remote Sensing. 60R. Zhao, Z. Shi, and Z. Zou, "High-resolution remote sensing image captioning based on structured attention," IEEE Transactions on Geo- science and Remote Sensing, vol. 60, pp. 1-14, 2022.

Deepdownscale: A deep learning strategy for high-resolution weather forecast. E Rocha Rodrigues, I Oliveira, IEEE International Conference on e-Science. E. Rocha Rodrigues, I. Oliveira et al., "Deepdownscale: A deep learning strategy for high-resolution weather forecast," in IEEE In- ternational Conference on e-Science, 2018, pp. 415-422.

Where's wally: A gigapixel image study for face recognition in crowds. C B R Ferreira, H Pedrini, Advances in Visual Computing. C. B. R. Ferreira, H. Pedrini et al., "Where's wally: A gigapixel image study for face recognition in crowds," in Advances in Visual Computing, 2020, pp. 386-397.

Fast and lightweight online person search for large-scale surveillance systems. A Specker, L Moritz, IEEE/CVF Winter Conference on Applications of Computer Vision Workshops. A. Specker, L. Moritz et al., "Fast and lightweight online person search for large-scale surveillance systems," in IEEE/CVF Winter Conference on Applications of Computer Vision Workshops, 2022, pp. 570-580.

Fast pedestrian detection for realworld crowded scenarios on embedded gpu. M Cormier, S Wolf, IEEE International Conference on Smart Technologies. M. Cormier, S. Wolf et al., "Fast pedestrian detection for real- world crowded scenarios on embedded gpu," in IEEE International Conference on Smart Technologies, 2021, pp. 40-44.

Region nms-based deep network for gigapixel level pedestrian detection with two-step cropping. L Li, X Guo, Neurocomputing. 468L. Li, X. Guo et al., "Region nms-based deep network for gigapixel level pedestrian detection with two-step cropping," Neurocomputing, vol. 468, pp. 482-491, 2022.

Single image human proxemics estimation for visual social distancing. M Aghaei, M Bustreo, IEEE Winter Conference on Applications of Computer Vision. M. Aghaei, M. Bustreo et al., "Single image human proxemics esti- mation for visual social distancing," in IEEE Winter Conference on Applications of Computer Vision, 2021.

A deep learning-based social distance monitoring framework for covid-19. I Ahmed, M Ahmad, Sustainable Cities and Society. 65102571I. Ahmed, M. Ahmad et al., "A deep learning-based social distance monitoring framework for covid-19," Sustainable Cities and Society, vol. 65, p. 102571, 2021.

Understanding important features of deep learning models for segmentation of high-resolution transmission electron microscopy images. J P Horwath, D N Zakharov, NPJ Computational Materials. 61108J. P. Horwath, D. N. Zakharov et al., "Understanding important features of deep learning models for segmentation of high-resolution trans- mission electron microscopy images," NPJ Computational Materials, vol. 6, no. 1, p. 108, 2020.

Real-time high-resolution background matting. S Lin, A Ryabtsev, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)S. Lin, A. Ryabtsev et al., "Real-time high-resolution background matting," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021, pp. 8762-8771.

Efficient segmentation: Learning downsampling near semantic boundaries. D Marin, Z He, IEEE/CVF International Conference on Computer Vision. D. Marin, Z. He et al., "Efficient segmentation: Learning downsampling near semantic boundaries," in IEEE/CVF International Conference on Computer Vision, 2019, pp. 2131-2141.

Learning to downsample for segmentation of ultra-high resolution images. C Jin, R Tanno, arXiv:2109.11071arXiv preprintC. Jin, R. Tanno et al., "Learning to downsample for segmentation of ultra-high resolution images," arXiv preprint arXiv:2109.11071, 2021.

Fovea: Foveated image magnification for autonomous navigation. C Thavamani, M Li, IEEE/CVF International Conference on Computer Vision. C. Thavamani, M. Li et al., "Fovea: Foveated image magnification for autonomous navigation," in IEEE/CVF International Conference on Computer Vision, 2021, pp. 15 539-15 548.

Spatial transformer networks. M Jaderberg, K Simonyan, Advances in Neural Information Processing Systems. 28M. Jaderberg, K. Simonyan et al., "Spatial transformer networks," Advances in Neural Information Processing Systems, vol. 28, 2015.

Splines minimizing rotation-invariant semi-norms in sobolev spaces. J Duchon, Constructive Theory of Functions of Several Variables. J. Duchon, "Splines minimizing rotation-invariant semi-norms in sobolev spaces," in Constructive Theory of Functions of Several Variables, 1977, pp. 85-100.

Salisa: Saliency-based input sampling for efficient video object detection. B E Bejnordi, A Habibian, arXiv:2204.02397arXiv preprintB. E. Bejnordi, A. Habibian et al., "Salisa: Saliency-based in- put sampling for efficient video object detection," arXiv preprint arXiv:2204.02397, 2022.

Reinforced auto-zoom net: towards accurate and fast breast cancer segmentation in whole-slide images. N Dong, M Kampffmeyer, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. N. Dong, M. Kampffmeyer et al., "Reinforced auto-zoom net: towards accurate and fast breast cancer segmentation in whole-slide images," in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, 2018, pp. 317-325.

Dynamic zoom-in network for fast object detection in large images. M Gao, R Yu, IEEE Conference on Computer Vision and Pattern Recognition. M. Gao, R. Yu et al., "Dynamic zoom-in network for fast object detection in large images," in IEEE Conference on Computer Vision and Pattern Recognition, 2018.

Learning when and where to zoom with deep reinforcement learning. B Uzkent, S Ermon, IEEE/CVF Conference on Computer Vision and Pattern Recognition. B. Uzkent and S. Ermon, "Learning when and where to zoom with deep reinforcement learning," in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.

Zoom in to where it matters: a hierarchical graph based model for mammogram analysis. H Du, J Feng, M Feng, arXiv:1912.07517arXiv preprintH. Du, J. Feng, and M. Feng, "Zoom in to where it matters: a hierarchical graph based model for mammogram analysis," arXiv preprint arXiv:1912.07517, 2019.

Graph attention networks. P Velickovic, G Cucurull, International Conference on Learning Representations. P. Velickovic, G. Cucurull et al., "Graph attention networks," in International Conference on Learning Representations, 2018.

Towards real-time object detection in gigapixel-level video. K Chen, Z Wang, Neurocomputing. 477K. Chen, Z. Wang et al., "Towards real-time object detection in gigapixel-level video," Neurocomputing, vol. 477, pp. 14-24, 2022.

Very deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556arXiv preprintK. Simonyan and A. Zisserman, "Very deep convolutional networks for large-scale image recognition," arXiv preprint arXiv:1409.1556, 2014.

You only look once: Unified, real-time object detection. J Redmon, S Divvala, IEEE Conference on Computer Vision and Pattern Recognition. J. Redmon, S. Divvala et al., "You only look once: Unified, real-time object detection," in IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 779-788.

Faster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, Advances in Neural Information Processing Systems. 28S. Ren, K. He et al., "Faster r-cnn: Towards real-time object detec- tion with region proposal networks," Advances in Neural Information Processing Systems, vol. 28, 2015.

Flexible high-resolution object detection on edge devices with tunable latency. S Jiang, Z Lin, Annual International Conference on Mobile Computing and Networking. S. Jiang, Z. Lin et al., "Flexible high-resolution object detection on edge devices with tunable latency," in Annual International Conference on Mobile Computing and Networking, 2021, p. 559-572.

Overfeat: Integrated recognition, localization and detection using convolutional networks. P Sermanet, D Eigen, arXiv:1312.6229arXiv preprintP. Sermanet, D. Eigen et al., "Overfeat: Integrated recognition, lo- calization and detection using convolutional networks," arXiv preprint arXiv:1312.6229, 2013.

Class-specific discriminant regularization in real-time deep cnn models for binary classification problems. M Tzelepi, A Tefas, Neural Processing Letters. 512M. Tzelepi and A. Tefas, "Class-specific discriminant regularization in real-time deep cnn models for binary classification problems," Neural Processing Letters, vol. 51, no. 2, pp. 1989-2005, 2020.

Improving the performance of lightweight cnns for binary classification using quadratic mutual information regularization. M Tzelepi, A Tefas, Pattern Recognition. 106107407M. Tzelepi and A. Tefas, "Improving the performance of lightweight cnns for binary classification using quadratic mutual information reg- ularization," Pattern Recognition, vol. 106, p. 107407, 2020.

Graph embedded convolutional neural networks in human crowd detection for drone flight safety. M Tzelepi, A Tefas, IEEE Transactions on Emerging Topics in Computational Intelligence. 52M. Tzelepi and A. Tefas, "Graph embedded convolutional neural networks in human crowd detection for drone flight safety," IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 5, no. 2, pp. 191-204, 2021.

Fast deep convolutional face detection in the wild exploiting hard sample mining. D Triantafyllidou, P Nousi, A Tefas, Big Data Research. 11D. Triantafyllidou, P. Nousi, and A. Tefas, "Fast deep convolutional face detection in the wild exploiting hard sample mining," Big Data Research, vol. 11, pp. 65-76, 2018.

Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation. S Mehta, M Rastegari, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)S. Mehta, M. Rastegari et al., "Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation," in Proceedings of the European Conference on Computer Vision (ECCV), September 2018.

Hr-nas: Searching efficient high-resolution neural architectures with lightweight transformers. M Ding, X Lian, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)M. Ding, X. Lian et al., "Hr-nas: Searching efficient high-resolution neural architectures with lightweight transformers," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021, pp. 2982-2992.

Slimmable compressive autoencoders for practical neural image compression. F Yang, L Herranz, IEEE/CVF Conference on Computer Vision and Pattern Recognition. F. Yang, L. Herranz et al., "Slimmable compressive autoencoders for practical neural image compression," in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 4998-5007.

Capturing cellular topology in multigigapixel pathology images. W Lu, S Graham, IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. W. Lu, S. Graham et al., "Capturing cellular topology in multi- gigapixel pathology images," in IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2020.

Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images. S Graham, Q D Vu, Medical Image Analysis. 58101563S. Graham, Q. D. Vu et al., "Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images," Medical Image Analysis, vol. 58, p. 101563, 2019.

Pannuke: an open pan-cancer histology dataset for nuclei instance segmentation and classification. J Gamper, N A Koohbanani, European Congress on Digital Pathology. J. Gamper, N. A. Koohbanani et al., "Pannuke: an open pan-cancer histology dataset for nuclei instance segmentation and classification," in European Congress on Digital Pathology, 2019, pp. 11-19.

Modern hierarchical, agglomerative clustering algorithms. D Müllner, arXiv:1109.2378arXiv preprintD. Müllner, "Modern hierarchical, agglomerative clustering algo- rithms," arXiv preprint arXiv:1109.2378, 2011.

Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, International Conference on Learning Representations. T. N. Kipf and M. Welling, "Semi-supervised classification with graph convolutional networks," in International Conference on Learning Representations, 2017.

Neural image compression for gigapixel histopathology image analysis. D Tellez, G Litjens, IEEE Transactions on Pattern Analysis and Machine Intelligence. 432D. Tellez, G. Litjens et al., "Neural image compression for gigapixel histopathology image analysis," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 2, pp. 567-578, 2021.

Auto-encoding variational bayes. D P Kingma, M Welling, arXiv:1312.6114arXiv preprintD. P. Kingma and M. Welling, "Auto-encoding variational bayes," arXiv preprint arXiv:1312.6114, 2013.

Adversarial feature learning. J Donahue, P Krähenbühl, T Darrell, arXiv:1605.09782arXiv preprintJ. Donahue, P. Krähenbühl, and T. Darrell, "Adversarial feature learn- ing," arXiv preprint arXiv:1605.09782, 2016.

Multimodal co-attention transformer for survival prediction in gigapixel whole slide images. R J Chen, M Y Lu, IEEE/CVF International Conference on Computer Vision. R. J. Chen, M. Y. Lu et al., "Multimodal co-attention transformer for survival prediction in gigapixel whole slide images," in IEEE/CVF International Conference on Computer Vision, 2021, pp. 4015-4025.

Deep residual learning for image recognition. K He, X Zhang, IEEE Conference on Computer Vision and Pattern Recognition. K. He, X. Zhang et al., "Deep residual learning for image recognition," in IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770-778.

Imagenet: A large-scale hierarchical image database. J Deng, W Dong, IEEE Conference on Computer Vision and Pattern Recognition. J. Deng, W. Dong et al., "Imagenet: A large-scale hierarchical im- age database," in IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 248-255.

Faster neural networks straight from jpeg. L Gueguen, A Sergeev, Advances in Neural Information Processing Systems. 31L. Gueguen, A. Sergeev et al., "Faster neural networks straight from jpeg," Advances in Neural Information Processing Systems, vol. 31, 2018.

Learning in the frequency domain. K Xu, M Qin, IEEE/CVF Conference on Computer Vision and Pattern Recognition. K. Xu, M. Qin et al., "Learning in the frequency domain," in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 1740-1749.

Image classification using convolutional neural network with wavelet domain inputs. L Wang, Y Sun, IET Image Processing. 168L. Wang and Y. Sun, "Image classification using convolutional neural network with wavelet domain inputs," IET Image Processing, vol. 16, no. 8, pp. 2037-2048, 2022.

Comparative analysis between dct & dwt techniques of image compression. A Katharotiya, S Patel, M Goyani, Journal of Information Engineering and Applications. 12A. Katharotiya, S. Patel, and M. Goyani, "Comparative analysis between dct & dwt techniques of image compression," Journal of Information Engineering and Applications, vol. 1, no. 2, pp. 9-17, 2011.

Fast object detection in compressed video. S Wang, H Lu, Z Deng, IEEE/CVF International Conference on Computer Vision. S. Wang, H. Lu, and Z. Deng, "Fast object detection in compressed video," in IEEE/CVF International Conference on Computer Vision, 2019.

264 and MPEG-4 video compression: video coding for next-generation multimedia. I E Richardson, H , John Wiley & SonsI. E. Richardson, H. 264 and MPEG-4 video compression: video coding for next-generation multimedia. John Wiley & Sons, 2004.

Long short-term memory. S Hochreiter, J Schmidhuber, Neural Computation. 98S. Hochreiter and J. Schmidhuber, "Long short-term memory," Neural Computation, vol. 9, no. 8, pp. 1735-1780, 1997.

Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. P Zhang, X Dai, IEEE/CVF International Conference on Computer Vision. P. Zhang, X. Dai et al., "Multi-scale vision longformer: A new vision transformer for high-resolution image encoding," in IEEE/CVF International Conference on Computer Vision, 2021, pp. 2998-3008.

Longformer: The longdocument transformer. I Beltagy, M E Peters, A Cohan, arXiv:2004.05150arXiv preprintI. Beltagy, M. E. Peters, and A. Cohan, "Longformer: The long- document transformer," arXiv preprint arXiv:2004.05150, 2020.

Hrformer: High-resolution vision transformer for dense predict. Y Yuan, R Fu, Advances in Neural Information Processing Systems. 34Y. YUAN, R. Fu et al., "Hrformer: High-resolution vision transformer for dense predict," in Advances in Neural Information Processing Systems, vol. 34, 2021, pp. 7281-7293.

Multi-scale high-resolution vision transformer for semantic segmentation. J Gu, H Kwon, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)12J. Gu, H. Kwon et al., "Multi-scale high-resolution vision transformer for semantic segmentation," in Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 12 094-12 103.

Cswin transformer: A general vision transformer backbone with cross-shaped windows. X Dong, J Bao, IEEE/CVF Conference on Computer Vision and Pattern Recognition. X. Dong, J. Bao et al., "Cswin transformer: A general vision trans- former backbone with cross-shaped windows," in IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, 2022, pp. 12 124- 12 134.

Glance-and-gaze vision transformer. Q Yu, Y Xia, Advances in Neural Information Processing Systems. 343Q. Yu, Y. Xia et al., "Glance-and-gaze vision transformer," in Advances in Neural Information Processing Systems, vol. 34, 2021, pp. 12 992- 13 003.

Scaling vision transformers to gigapixel images via hierarchical self-supervised learning. R J Chen, C Chen, IEEE/CVF Conference on Computer Vision and Pattern Recognition. R. J. Chen, C. Chen et al., "Scaling vision transformers to gigapixel images via hierarchical self-supervised learning," in IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, 2022, pp. 16 144- 16 155.

Yfcc100m: The new data in multimedia research. B Thomee, D A Shamma, Communications of the ACM. 592B. Thomee, D. A. Shamma et al., "Yfcc100m: The new data in multimedia research," Communications of the ACM, vol. 59, no. 2, pp. 64-73, 2016.

Annotated Facial Landmarks in the Wild: A Large-scale, Real-world Database for Facial Landmark Localization. P M R Martin Koestinger, Paul Wohlhart, H Bischof, IEEE International Workshop on Benchmarking Facial Image Analysis Technologies. P. M. R. Martin Koestinger, Paul Wohlhart and H. Bischof, "Annotated Facial Landmarks in the Wild: A Large-scale, Real-world Database for Facial Landmark Localization," in IEEE International Workshop on Benchmarking Facial Image Analysis Technologies, 2011.

Facial landmark detection by deep multitask learning. Z Zhang, P Luo, European Conference on Computer Vision. Z. Zhang, P. Luo et al., "Facial landmark detection by deep multi- task learning," in European Conference on Computer Vision, 2014, pp. 94-108.

Wider face: A face detection benchmark. S Yang, P Luo, IEEE Conference on Computer Vision and Pattern Recognition. S. Yang, P. Luo et al., "Wider face: A face detection benchmark," in IEEE Conference on Computer Vision and Pattern Recognition, 2016.

Multi-source multi-scale counting in extremely dense crowd images. H Idrees, I Saleemi, IEEE Conference on Computer Vision and Pattern Recognition. H. Idrees, I. Saleemi et al., "Multi-source multi-scale counting in extremely dense crowd images," in IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 2547-2554.

Single-image crowd counting via multicolumn convolutional neural network. Y Zhang, D Zhou, IEEE Conference on Computer Vision and Pattern Recognition. Y. Zhang, D. Zhou et al., "Single-image crowd counting via multi- column convolutional neural network," in IEEE Conference on Com- puter Vision and Pattern Recognition, 2016, pp. 589-597.

Composition loss for counting, density map estimation and localization in dense crowds. H Idrees, M Tayyab, in European conference on computer visionH. Idrees, M. Tayyab et al., "Composition loss for counting, density map estimation and localization in dense crowds," in European con- ference on computer vision, 2018, pp. 532-546.

Jhu-crowd++: Large-scale crowd counting dataset and a benchmark method. V A Sindagi, R Yasarla, V M Patel, Technical ReportV. A. Sindagi, R. Yasarla, and V. M. Patel, "Jhu-crowd++: Large-scale crowd counting dataset and a benchmark method," Technical Report, 2020.

Nwpu-crowd: A large-scale benchmark for crowd counting and localization. Q Wang, J Gao, IEEE Transactions on Pattern Analysis and Machine Intelligence. 436Q. Wang, J. Gao et al., "Nwpu-crowd: A large-scale benchmark for crowd counting and localization," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 6, pp. 2141-2149, 2020.

Ambient sound helps: Audiovisual crowd counting in extreme conditions. D Hu, L Mou, arXiv:2005.07097arXiv preprintD. Hu, L. Mou et al., "Ambient sound helps: Audiovisual crowd counting in extreme conditions," arXiv preprint arXiv:2005.07097, 2020.

The cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, IEEE Conference on Computer Vision and Pattern Recognition. M. Cordts, M. Omran et al., "The cityscapes dataset for semantic urban scene understanding," in IEEE Conference on Computer Vision and Pattern Recognition, 2016.

The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. G Ros, L Sellart, IEEE Conference on Computer Vision and Pattern Recognition. G. Ros, L. Sellart et al., "The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes," in IEEE Conference on Computer Vision and Pattern Recognition, 2016.

The apolloscape open dataset for autonomous driving and its application. X Huang, P Wang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4210X. Huang, P. Wang et al., "The apolloscape open dataset for au- tonomous driving and its application," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 10, pp. 2702-2719, 2020.

Towards streaming perception. M Li, Y.-X Wang, D Ramanan, European Conference on Computer Vision. M. Li, Y.-X. Wang, and D. Ramanan, "Towards streaming perception," in European Conference on Computer Vision, 2020, pp. 473-488.

Bdd100k: A diverse driving dataset for heterogeneous multitask learning. F Yu, H Chen, IEEE/CVF Conference on Computer Vision and Pattern Recognition. F. Yu, H. Chen et al., "Bdd100k: A diverse driving dataset for hetero- geneous multitask learning," in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.

The role of context for object detection and semantic segmentation in the wild. R Mottaghi, X Chen, IEEE Conference on Computer Vision and Pattern Recognition. R. Mottaghi, X. Chen et al., "The role of context for object detection and semantic segmentation in the wild," in IEEE Conference on Computer Vision and Pattern Recognition, 2014.

Scene parsing through ade20k dataset. B Zhou, H Zhao, IEEE Conference on Computer Vision and Pattern Recognition. B. Zhou, H. Zhao et al., "Scene parsing through ade20k dataset," in IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 633-641.

Coco-stuff: Thing and stuff classes in context. H Caesar, J Uijlings, V Ferrari, IEEE Conference on Computer Vision and Pattern Recognition. H. Caesar, J. Uijlings, and V. Ferrari, "Coco-stuff: Thing and stuff classes in context," in IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 1209-1218.

Deepglobe 2018: A challenge to parse the earth through satellite images. I Demir, K Koperski, IEEE Conference on Computer Vision and Pattern Recognition Workshops. I. Demir, K. Koperski et al., "Deepglobe 2018: A challenge to parse the earth through satellite images," in IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2018, pp. 172-181.

Copernicus global land service. M Buchhorn, B Smets, 2020Land cover 100m: collection 3: epoch 2019: Globe," Version V3. 0.1)[Data setM. Buchhorn, B. Smets et al., "Copernicus global land service: Land cover 100m: collection 3: epoch 2019: Globe," Version V3. 0.1)[Data set], 2020.

Functional map of the world. G Christie, N Fendley, IEEE Conference on Computer Vision and Pattern Recognition. G. Christie, N. Fendley et al., "Functional map of the world," in IEEE Conference on Computer Vision and Pattern Recognition, 2018.

KID project: an internet-based digital video atlas of capsule endoscopy for research purposes. A Koulaouzidis, D Iakovidis, Endoscopy International Open. 56A. Koulaouzidis, D. Iakovidis et al., "KID project: an internet-based digital video atlas of capsule endoscopy for research purposes," En- doscopy International Open, vol. 5, no. 6, pp. E477-E483, 2017.

CAD-CAP: a 25,000-image database serving the development of artificial intelligence for capsule endoscopy. R Leenhardt, C Li, Endosc Int Open. 83R. Leenhardt, C. Li et al., "CAD-CAP: a 25,000-image database serv- ing the development of artificial intelligence for capsule endoscopy," Endosc Int Open, vol. 8, no. 3, pp. E415-E420, 2020.

Predicting breast tumor proliferation from whole-slide images: The tupac16 challenge. M Veta, Y J Heng, Medical Image Analysis. 54M. Veta, Y. J. Heng et al., "Predicting breast tumor proliferation from whole-slide images: The tupac16 challenge," Medical Image Analysis, vol. 54, pp. 111-121, 2019.

Bach: Grand challenge on breast cancer histology images. G Aresta, T Araújo, Medical Image Analysis. 56G. Aresta, T. Araújo et al., "Bach: Grand challenge on breast cancer histology images," Medical Image Analysis, vol. 56, pp. 122-139, 2019.

Comprehensive molecular portraits of human breast tumours. D C Koboldt, R S Fulton, Nature. 4907418D. C. Koboldt, R. S. Fulton et al., "Comprehensive molecular portraits of human breast tumours," Nature, vol. 490, no. 7418, pp. 61-70, 2012.

INbreast. I C Moreira, I Amaral, Academic Radiology. 192I. C. Moreira, I. Amaral et al., "INbreast," Academic Radiology, vol. 19, no. 2, pp. 236-248, 2012.

UA-DETRAC: A new benchmark and protocol for multi-object detection and tracking. L Wen, D Du, Computer Vision and Image Understanding. 193102907L. Wen, D. Du et al., "UA-DETRAC: A new benchmark and protocol for multi-object detection and tracking," Computer Vision and Image Understanding, vol. 193, p. 102907, 2020.

Imagenet large scale visual recognition challenge. O Russakovsky, J Deng, International Journal of Computer Vision. 1153O. Russakovsky, J. Deng et al., "Imagenet large scale visual recognition challenge," International Journal of Computer Vision, vol. 115, no. 3, pp. 211-252, 2015.

Fair1m: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery. X Sun, P Wang, ISPRS Journal of Photogrammetry and Remote Sensing. 184X. Sun, P. Wang et al., "Fair1m: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery," ISPRS Journal of Photogrammetry and Remote Sensing, vol. 184, pp. 116-130, 2022. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S0924271621003269

Microsoft coco: Common objects in context. T.-Y Lin, M Maire, European Conference on Computer Vision. T.-Y. Lin, M. Maire et al., "Microsoft coco: Common objects in context," in European Conference on Computer Vision, 2014, pp. 740- 755.

Cs-fnet: A compressive sampling frequency neural network for simultaneous image compression and recognition. R Ma, Q Hao, IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems. R. Ma and Q. Hao, "Cs-fnet: A compressive sampling frequency neural network for simultaneous image compression and recognition," in IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, 2021, pp. 1-6.

Can attention enable mlps to catch up with cnns?. M.-H Guo, Z.-N Liu, Computational Visual Media. 73M.-H. Guo, Z.-N. Liu et al., "Can attention enable mlps to catch up with cnns?" Computational Visual Media, vol. 7, no. 3, pp. 283-288, 2021.

Model compression and acceleration for deep neural networks: The principles, progress, and challenges. Y Cheng, D Wang, IEEE Signal Processing Magazine. 351Y. Cheng, D. Wang et al., "Model compression and acceleration for deep neural networks: The principles, progress, and challenges," IEEE Signal Processing Magazine, vol. 35, no. 1, pp. 126-136, 2018.

Dynamic neural networks: A survey. Y Han, G Huang, arXiv:2102.04906arXiv preprintY. Han, G. Huang et al., "Dynamic neural networks: A survey," arXiv preprint arXiv:2102.04906, 2021.

Massively parallel video networks. J Carreira, V Patraucean, European Conference on Computer Vision. J. Carreira, V. Patraucean et al., "Massively parallel video networks," in European Conference on Computer Vision, 2018.

Continual inference: A library for efficient online inference with deep neural networks in pytorch. L Hedegaard, A Iosifidis, arXiv:2204.03418arXiv preprintL. Hedegaard and A. Iosifidis, "Continual inference: A library for efficient online inference with deep neural networks in pytorch," arXiv preprint: arXiv:2204.03418, 2022.

Split computing and early exiting for deep learning applications: Survey and research challenges. Y Matsubara, M Levorato, F Restuccia, ACM Computing Surveys. Y. Matsubara, M. Levorato, and F. Restuccia, "Split computing and early exiting for deep learning applications: Survey and research challenges," ACM Computing Surveys, 2021.

Dynamic split computing for efficient deep edge intelligence. A Bakhtiarnia, N Milošević, arXiv:2205.11269arXiv preprintA. Bakhtiarnia, N. Milošević et al., "Dynamic split computing for efficient deep edge intelligence," arXiv preprint arXiv:2205.11269, 2022.

Onboard ROI selection for aerial surveillance using a high resolution, high framerate camera. N Boehrer, A Gabriel, Mobile Multimedia/Image Processing, Security, and Applications. 11399N. Boehrer, A. Gabriel et al., "Onboard ROI selection for aerial surveillance using a high resolution, high framerate camera," in Mobile Multimedia/Image Processing, Security, and Applications, vol. 11399, 2020, pp. 76 -95.

Device Camera Year Resolution (MP) Source Apple iPhone Rear Camera. Details for device camera resolutions. All links were accessed at 26. 1st generation. 1st generation. 3rd generationTABLE V: Details for device camera resolutions. All links were accessed at 26 July 2022. Device Camera Year Resolution (MP) Source Apple iPhone Rear Camera 2007 2 https://en.wikipedia.org/wiki/IPhone (1st generation) 2008 2 https://en.wikipedia.org/wiki/IPhone 3G 2009 3 https://en.wikipedia.org/wiki/IPhone 3GS 2010 5 https://en.wikipedia.org/wiki/IPhone 4 2011 8 https://en.wikipedia.org/wiki/IPhone 4S 2012 8 https://en.wikipedia.org/wiki/IPhone 5 2013 8 https://en.wikipedia.org/wiki/IPhone 5S 2014 8 https://en.wikipedia.org/wiki/IPhone 6 2015 12 https://en.wikipedia.org/wiki/IPhone 6S 2016 12.2 https://en.wikipedia.org/wiki/IPhone SE (1st generation) 2017 12 https://en.wikipedia.org/wiki/IPhone X 2018 12 https://en.wikipedia.org/wiki/IPhone XS 2019 12 https://en.wikipedia.org/wiki/IPhone 11 Pro 2020 12 https://en.wikipedia.org/wiki/IPhone 12 Pro 2021 12 https://en.wikipedia.org/wiki/IPhone 13 Pro 2022 12 https://en.wikipedia.org/wiki/IPhone SE (3rd generation)

. Samsung Galaxy S Rear, Camera, Samsung Galaxy S Rear Camera 2010 5 https://en.wikipedia.org/wiki/Samsung Galaxy S 2011 8 https://en.wikipedia.org/wiki/Samsung Galaxy S II 2012 8 https://en.wikipedia.org/wiki/Samsung Galaxy S III 2013 13 https://en.wikipedia.org/wiki/Samsung Galaxy S4 2014 16 https://en.wikipedia.org/wiki/Samsung Galaxy S5 2015 16 https://en.wikipedia.org/wiki/Samsung Galaxy S6 2016 12 https://en.wikipedia.org/wiki/Samsung Galaxy S7 2017 12 https://en.wikipedia.org/wiki/Samsung Galaxy S8 2018 12 https://en.wikipedia.org/wiki/Samsung Galaxy S9 2019 16 https://en.wikipedia.org/wiki/Samsung Galaxy S10 2020 108 https://en.wikipedia.org/wiki/Samsung Galaxy S20 2021 108 https://en.wikipedia.org/wiki/Samsung Galaxy S21 2022 108 https://en.wikipedia.org/wiki/Samsung Galaxy S22

Microsoft HoloLens Camera. Microsoft HoloLens Camera 2016 2.4 https://docs.microsoft.com/en-us/hololens/hololens1-hardware 2019 8 https://www.microsoft.com/en-us/hololens/hardware

Raspberry Pi Camera. UAV)#Current Phantom. Raspberry Pi Camera 2013 2.1 https://en.wikipedia.org/wiki/Raspberry Pi#Accessories 2016 8 https://en.wikipedia.org/wiki/Raspberry Pi#Accessories 2020 12.3 https://en.wikipedia.org/wiki/Raspberry Pi#Accessories DJI Phantom Camera 2012 12 https://en.wikipedia.org/wiki/GoPro#HERO3 (White/Silver/Black) 2013 14 https://www.dji.com/dk/phantom-2-vision 2014 14 https://www.dji.com/dk/phantom-2-vision-plus 2015 12.4 https://www.dji.com/dk/phantom-3-pro 2016 20 https://en.wikipedia.org/wiki/Phantom (UAV)#Current Phantom drones 2017 20 https://en.wikipedia.org/wiki/Phantom (UAV)#Current Phantom drones 2018 20 https://en.wikipedia.org/wiki/Phantom (UAV)#Current Phantom drones