# Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition

CorpusID: 231925325 - [https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23](https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23)

Fields: Computer Science, Engineering

## (s3) A. RNN-based encoder-decoder architecture
(p3.0) Sequence-to-sequence RNN-based ASR models are based on an encoder-decoder architecture. The encoder is an RNN which takes input sequence and converts it into hidden states. The decoder is also an RNN which takes the last encoder hidden state as input and process it to decoder hidden states which in turn used for output predictions. This traditional encoder-decoder structure has some limitations:

(p3.1) • The encoder hidden state, h T (last one) which is fed to the decoder has the entire input sequence information compressed into it. For longer input sequences, it may cause information loss as h T may not capture long-range dependencies effectively. • There is no alignment between the input sequence frames and the output. For predicting each output symbol, instead The above issues can be overcome by letting the decoder to access all the encoder hidden states (instead of the last one) and at each decoder time step, relevant input frames are given higher priorities than others. It is achieved by incorporating attention mechanism to the encoder-decoder model. As a part of sequence-to-sequence modelling, attention mechanism was introduced in [71] for machine translation. Inspired by the effectiveness in [71], the attention mechanism was introduced to ASR in [11]. An earlier version of this work has been presented in [10].

(p3.2) The model in [11] is named as attention-based recurrent sequence generator (ASRG). The graphical representation of this model is shown in Figure 1. The encoder of ASRG processes the input audio frames to encoder hidden states which are then used to predict output phonemes. By focusing on the relevant encoder hidden states, at i th decoder time step, prediction of phoneme y i is given by (1)

(p3.3) where c i is the context given by (2) generated by attention mechanism at the i th decoder time step. s i given by (3) is the decoder hidden state at i th time step. It is the output of a recurrent function like LSTM or GRU. Spell(., .) is a feedforward neural network with softmax output activation.

(p3.4) where h j is the encoder hidden state at the j th encoder time step. α i,j given by (4) is the attention probability belonging to the j th encoder hidden state for the output prediction at i th decoder time step. In other words, α i,j captures the importance of the j th input speech frame (or encoder hidden state) for decoding the i th output word (or phoneme or character). α i values are also considered as the alignment of encoder hidden states (h j∈[1,··· ,L] ) to predict an output at i th decoder time step. Therefore, c i is the sum of the products (SOP) of attention probabilities and the hidden states belonging to all encoder time steps at the i th decoder time step and it provides a context to the decoder to decode (or predict) the corresponding output.
## (s5) A. Global Attention with RNN
(p5.0) Global attention is computed over the entire encoder hidden states at every decoder time step. The mechanism illustrated in Section III-A as per [11] is an example of global attention. Since [11], a lot of progress has been made by many researchers.

(p5.1) The authors of [24] presented a global attention mechanism in their Listen, Attend and Spell (LAS) model. Here, Spell function takes inputs as current decoder state s i and the context c i . y i = Spell(s i , c i ). s i is computed using a recurrent function which takes inputs as previous decoder state (s i−1 ), previous output prediction (y i−1 ) and previous context (c i−1 ).
## (s6) B. Local attention with RNN
(p6.0) In global attention model, each encoder hidden states are attended at each decoder time step. This results in a quadratic computation complexity. In addition, the prediction of a particular decoder output mostly depends on a small number of encoder hidden states. Therefore, it is not necessary to attend the entire set of encoder hidden states at each decoder time step. The application of local attention fulfils the requirement of reducing the computation complexity by focusing on relevant encoder hidden states. Local attention mechanism is mostly popular in streaming speech recognition but, it has been applied to offline speech recognition as well. The core idea of local attention is to attend a set of encoder hidden states within a window or range at each decoder time step instead of attending the entire set of encoder hidden states. Local attention was introduced in [74] for machine translation and thereafter, it has been applied to ASR as well.
## (s8) D. RNN-free Transformer-based models
(p8.0) Self-attention is a mechanism to capture the dependencies within a sequence. It allows to compute the similarity between different frames in the same sequence. In other words, selfattention finds to what extent different positions of a sequence relate to each other. Transformer network [20] is entirely built using self-attention for seq2seq processing and has been successfully used in ASR as well.
