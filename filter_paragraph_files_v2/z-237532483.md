# A Survey on Temporal Sentence Grounding in Videos

CorpusID: 237532483 - [https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453](https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453)

Fields: Computer Science

## (s1) METHOD OVERVIEW
(p1.0) We establish the taxonomy of existing approaches based on their characteristics. As shown in Fig. 2, early works adopt a two-stage architecture, i.e., they first scan the whole video and pre-cut various candidate segments (i.e., proposals or moments) via sliding window strategy or proposal generation network, and then rank the candidates according to the ranking scores produced by the cross-modal matching module. However, such a scan-and-localize pipeline is time-consuming due to too much redundant computation of overlapping candidate segments, and the individual pairwise segment-query matching may also neglect the contextual video information.

(p1.1) Considering the above concerns, some researchers start to solve TSGV in an end-to-end manner. It is unnecessary for such end-to-end models to pre-cut candidate moments as the inputs of the model. Instead, multi-scale candidate moments ended at each time step are maintained by LSTM sequentially or convolutional neural networks hierarchically, and such end-to-end methods are named anchor-based methods. Some other end-to-end methods predict the probabilities for each video unit (i.e., frame-level or clip-level) being the start and end point of the target segment, or straightforwardly regress the target start and end coordinates based on the multimodal feature of the providing video and sentence query. These methods do not depend on any candidate proposal generation process, and are named anchor-free methods.

(p1.2) Besides, it is worth noting that some works resort to deep reinforcement learning techniques to address TSGV, taking the sentence localization problem as a sequential decision process, which are also of anchor-free. To reduce intensive labor for annotating the boundaries of groundtruth moments, weakly supervised methods with only video-level annotated descriptions have also emerged. In the following, we will present all the approaches above and perform a deep analysis of the characteristics for each type.   frameworks, two pioneer works that firstly present TSGV task. CTRL uses a joint representation to get the final alignment score and refines the temporal boundaries by location regressor, while MCN tries to minimize the ℓ 2 distance between the language and video representation vectors, figures from [16] and [23].
## (s3) proposal-generated.
(p3.0) Considering the inevitable drawbacks of sliding window-based methods, some approaches devote to reduce the number of proposal candidates, namely proposal-generated method. Such proposal-generated methods still adopt a two-stage scheme but avoid densely sliding window sampling through different kinds of proposal networks.

(p3.1) QSPN [67] relieves such a computation burden by proposing temporal segments conditioned on the query so as to reduce the number of candidate segments (c.f ., Fig. 4). Specifically, QSPN comprises of a query-guided segment proposal network (SPN) to propose query-specific candidate segments, a fine-grained early-fused similarity model for retrieval and a multi-tasking loss combining retrieval task with an auxiliary captioning task.

(p3.2) As shown in Fig. 4a, the query-guided SPN first incorporates the query embeddings into the video features to get the attention weight for each temporal location, and further integrates the temporal attention weights into the convolutional process for video encoding to propose queryaware representations of candidate segments. Then as shown in Fig. 4b, the generated proposal visual feature from Fig. 4a is incorporated into the sentence embedding process at each time step of the second layer of the two-layer LSTM in a early fusion way. Then QSPN devises a triplet-based retrieval loss which is similar to MCN:

(p3.3) where ( , ) is the positive (sentence, segment) pair while ′ is the sampled negative segment. QSPN also devises an auxiliary captioning task which re-generate the query sentence from the retrieved video segment. The loss for captioning is as follows:
## (s9) Datasets
(p9.0) Several datasets for TSGV from different scenarios with their distinct characteristics have been proposed in the past few years. There is no doubt that the effort of creating these datasets and designing corresponding evaluation metrics do promote the development of TSGV. Table 1 provides an overview about the statistics of public datasets, indicating the trend of involving more complicated activities and not being constrained in a narrow and specific scene (e.g., kitchen). We will introduce them more concretely in the following.

(p9.1) DiDeMo [23]. This dataset is collected from Flickr, and consists of various human activities uploaded by personal users. Hendricks et al. [23] split and label video segments from original untrimmed videos by aggregating five-second clip units, which means the lengths of groundtruth segments are times of five seconds. They claim that this trick is for avoiding ambiguity of labeling and accelerating the validation process. However, such a length-fixed issue makes the retrieval task easier since it compresses the searching space into a set with limited candidates. The data split is also provided by [23], with 33008, 4180, and 4022 video-sentence pairs for training, validation, and test, respectively.
## (s10) words per sentence).
(p10.0) ActivityNet Captions [31]. ActivityNet Captions is originally proposed for dense video captioning, and the sentence-segment pairs in this dataset can naturally be utilized for TSGV. ActivityNet Captions contains the largest amount of videos, and it aligns videos with a series of temporally annotated sentence descriptions. On average, each of the 20k videos contains 3.65 temporally localized sentences, resulting in a total of 100k sentences. Each sentence has an average length of 13.48 words. The sentence length is also normally distributed. Since the official test set is withheld for competitions, most TSGV works merge the two available validation subsets "val1" and "val2" as the test set. In summary, there are 10,009 videos and 37,421 sentence-segment pairs in the training set, and 4,917 videos and 34,536 sentence-segment pairs in the test set.
## (s11) Metrics
(p11.0) There are two types of metrics for TSGV, i.e., R@ ,IoU@ and mIoU, both of which are first introduced for TSGV in [16]. Since IoU (Intersection over Union) is widely used in object detection to measure the similarity between two bounding boxes, similarly for TSGV, as illustrated in Fig. 16, many TSGV methods adopt temporal IoU to measure the similarity between the groundtruth moment and the predicted one. The ratio of intersection area over union area ranges from 0 to 1, and it will be equal to 1 when these two moments are totally overlapped.

(p11.1) Thereby, one of the metrics is mIoU (i.e., mean IoU), a simple way to evaluate the results through averaging temporal IoUs of all samples. The other commonly-used metric is R@ , IoU@ [25]. As for sample , it is accounted as positive when there exists one segment out of top retrieved segments whose temporal IoU with the groundtruth segment is over , which can be denoted as ( , , ) = 1. Otherwise, ( , , ) = 0. R@ , IoU@ is the percentage of positive samples over all samples:

(p11.2) The community is accustomed to setting ∈ {1, 5, 10} and ∈ {0.3, 0.5, 0.7}. Usually, = 1 when the method adopts a proposal-free manner (i.e., belongs to either anchor-free or RL-based frameworks). Moreover, it is worth noting that MCN [23] adopts a particular metric with the IoU  threshold = 1 since the groundtruth segments in DiDeMo is generated by aggregating the clip units of 5 seconds, and MCN also employs a matching-based method thus the predicted moment has chance to fully coincide with the target moment, satisfying such a extremely high IoU threshold.
## (s17) Spatio-temporal localization.
(p17.0) Spatial-temporal sentence grounding in videos is another extension from TSGV which mainly localizes the referring object/instance as a continuing spatialtemporal tube (i.e., a sequence of bounding boxes) extracted from an untrimmed video via a natural language description. Since fine-grained labeling process of localizing a tube (i.e., annotate a spatial region for each frame in videos) for STSGV is labor-intensive and complicated, Chen et al. [13] propose to solve this task in a weakly-supervised manner which only needs video-level descriptions, with a newly-constructed VID-sentence dataset. Besides, VOGNet [50] commits to address the task of video object grounding, which grounds objects in videos referred to the natural language descriptions, and constructs a new dataset called ActivityNet-SRL. Tang et al. [56] employ visual transformer to solve a similar task which aims to localize a spatio-temporal tube of the target person from an untrimmed video based on a given textural description with a newly-constructed HC-STVG dataset.
