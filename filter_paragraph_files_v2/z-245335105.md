# A Comprehensive Analytical Survey on Unsupervised and Semi-Supervised Graph Representation Learning Methods

CorpusID: 245335105 - [https://www.semanticscholar.org/paper/b415ecb687941e1e9ef68e04a4a1c68c73483d51](https://www.semanticscholar.org/paper/b415ecb687941e1e9ef68e04a4a1c68c73483d51)

Fields: Computer Science, Mathematics

## (s19) Performance Metrics
(p19.0) A key focus of this survey is to understand the practical performance such as runtime, memory utilization, and scalability of various graph embedding and GNN methods on different graphs and hardware platforms. We also measure the accuracy of different methods when performing various machine learning tasks. We use F1-micro and F1-macro scores for the multilabel classification task, accuracy for the binary prediction task, and the modularity score to compare the results of clustering. We give a brief description of these measures below:

(p19.1) • Accuracy: When the predicted class is positive (negative) and the ground truth class is also positive (negative), it is termed as a true positive (negative). But when the predicted class is positive (negative) whereas ground truth class is negative (positive) then we term this as a false positive (negative). We represent an absolute number of true positive, false positive, true negative and false negative as TP, FP, TN and FN, respectively. In any type of binary classification or link prediction, we define accuracy as the following:

(p19.2) • F1-micro: For the multi-class classification task, the F1micro score aggregates the contributions of all classes to calculate the average value of final F1 score. In terms of precision-recall we can define this for a set of classes C as follows:

(p19.3) • F1-macro: Unlike the F1-micro score, the F1-macro score will compute the score independently for each class and finally take their average F1 score. In terms of 20 http://socialcomputing.asu.edu/pages/datasets precision-recall, we can define this for a set of classes C as follows:

(p19.4) • Modularity: The modularity score is a well-known measure to evaluate the effectiveness of any graph clustering technique. It computes the fraction of the edges that are within a given cluster minus the expected fraction, if edges are distributed randomly [74]. We can compute this score by using following Equation:

(p19.5) Here, A is the adjacency matrix of the graph, m is the number of edges, k i is the degree of the vertex v i . The membership of vertex v i belonging to a cluster is represented by c i , and δ(c i , c j ) = 1, if i and j are in the same cluster; otherwise, δ(c i , c j ) = 0.
## (s23) Training time for GNN methods
(p23.0) In the recent years, GNNs emerge as a popular option for graph representation learning, especially when nodes are partially labeled to facilitate semi-supervised predictions. In this section, we analyze the training time of some popular GNN methods. We measure the training time of GCN, GraphSAGE and FastGCN using their original source codes which are publicly available (Category 1). To measure the runtime of GAT, ClusterGCN and GraphSAINT, we use source codes available in the PyTorch Geometric (PyG) framework (Category 2) [27]. We ran all these methods on the Intel Skylake server and reported their training time in Table 10. We observe that GCN is the fastest GNN method for smaller graphs, but FastGCN runs faster than other methods for bigger graphs. The benefit of FastGCN stems from a sampling approach that is used to accelerate the training process while keeping the accuracy competitive to GCN and GraphSAGE. ClusterGCN and GraphSAINT apply a clustering technique and a sampling technique, respectively, as a pre-processing step before starting the training procedure. These expensive reprocessing steps make ClusterGCN and GraphSAINT slower than some of their peers.
## (s29) Operator
(p29.0) Notation Definition Hadamard We show notational definitions of three vector operators in Table 12. All these operators have been used previously for link prediction tasks [34]. We show the experimental results of unsupervised methods on this task in Figs. 17 (a), 17 (b), and 17 (c). As link prediction is a binary classification problem, we only report accuracy. We can see while using the Hadamard operator, Force2Vec and VERSE show competitive performance and outperform other methods. They achieve almost 99% accuracy in all datasets. For weighted L1 and L2 operators, DeepWalk performs better than other methods. For all cases, struc2vec is the worst performer as this tool is mainly designed to capture structural equivalence in the network. We also conducted experiments for link prediction task using HOPE and RolX. We observe that HOPE achieves accuracy of 79.8% and 80% for the Cora and the Pubmed datasets, respectively, using the Hadamard vector operator. On the other hand, RolX achieves accuracy of 79.7% and 74.8% for the Cora and the Pubmed datasets, respectively, using the Hadamard vector operator. These values of accuracy are lower than that of Force2Vec, VERSE and DeepWalk.
## (s34) Effect of Dimensions
(p34.0) Some previous studies have shown that the performance on the prediction task may vary if we choose different values for hyper-parameters [79,34,100,80]. For example, after reaching a certain value for dimensionality, the accuracy of prediction starts to drop when we increase it further. Most of the previous studies suggest using dimensional embedding. To summarize the results, we conduct experiments varying the dimensions of the output embedding for some shallow network-based methods. We set different parameters as described in Section 4.3 and take 20% of the dataset to train the logistic regression model while the rest of the samples in the dataset are used for the classification. We report the results of the F1-micro scores for the Pubmed dataset in Figs. 21 (a). We observe that Force2Vec, DeepWalk, and HARP perform better than other methods for various dimensional embedding. We also notice that, for lower dimensions, the F1-micro scores are not that much less compared to higher dimensions. In fact, the VERSE tool shows better performance for 16-dimensional embedding for the Pubmed dataset. RolX shows high sensitivity for different dimensions. It shows the lowest performance for 16-dimensional embedding. Then, with the increase of dimension, the F1-micro score also increases until 128-dimension. Then, it falls a little for 256-dimensional embedding. The LINE method shows similar sensitivity to the VERSE method though its F1-micro scores are lower than the VERSE.
