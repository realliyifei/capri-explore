# A Survey on Practical Applications of Multi-Armed and Contextual Bandits

CorpusID: 128358546 - [https://www.semanticscholar.org/paper/b24e6b0539d6e27d82c60fa7c53a1d0905e41a19](https://www.semanticscholar.org/paper/b24e6b0539d6e27d82c60fa7c53a1d0905e41a19)

Fields: Computer Science, Mathematics, Business

## (s4) Dynamic Pricing
(p4.0) Online retailer companies are often faced with the dynamic pricing problem: the company must decide on real-time prices for each of its multiple products. The company can run price experiments (make frequent price changes) to learn about demand and maximize long-run profits. The authors in [Misra et al., 2018] propose a dynamic price experimentation policy, where the company has only incomplete demand information. For this general setting, authors derive a pricing algorithm that balances earning an immediate profit vs. learning for future profits. The approach combines multi-armed bandit with partial identification of consumer demand from economic theory. Similar to [Misra et al., 2018], authors in [Mueller et al., 2018] consider high-dimensional dynamic multi-product pricing with an evolving lowdimensional linear demand model. They show that the revenue maximization problem reduces to an online bandit convex optimization with side information given by the observed demands. The approach applies a bandit convex optimization algorithm in a projected low-dimensional space spanned by the latent product features, while simultaneously learning this span via online singular value decomposition of a carefully-crafted matrix containing the observed demands.
## (s8) Dialogue Systems
(p8.0) Dialogue response selection. Dialogue response selection is an important step towards natural response generation in conversational agents. Existing work on conversational models mainly focuses on offline supervised learning using a large set of context-response pairs. In [Liu et al., 2018] authors focus on online learning of response selection in dialog systems. They propose a contextual multi-armed bandit model with a nonlinear reward function that uses distributed representation of text for online response selection. A bidirectional LSTM is used to produce the distributed representations of dialog context and responses, which serve as the input to a contextual bandit. They propose a customized Thompson sampling method that is applied to a polynomial feature space in approximating the reward.
## (s14) Hyperparameter Optimization
(p14.0) Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select optimal hyperparameter configurations, they rather focus on speeding up random search through adaptive resource allocation and early-stopping. [Li et al., 2016] formulated hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resources, such as iterations, data samples, or features are allocated to randomly sampled configurations. This work introduced a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, Hyperband wascmpared with popular Bayesian optimization methods on a suite of hyperparameter optimization problems; it was observed that Hyperband can provide more than an orderof-magnitude speedup over its competitors on a variety of deep-learning and kernel-based learning problems.
## (s18) Reinforcement learning
(p18.0) Autonomous cyber-physical systems play a large role in our lives. To ensure that agents behave in ways aligned with the values of the societies in which they operate, we must develop techniques that allow these agents to not only maximize their reward in an environment, but also to learn and follow the implicit constraints assumed by the society. In [Noothigattu et al., 2018], the authors study a setting where an agent can observe traces of behavior of members of the society but has no access to the explicit set of constraints that give rise to the observed behavior. Instead, inverse reinforcement learning is used to learn such constraints, that are then combined with a possibly orthogonal value function through the use of a contextual bandit-based orchestrator that picks a contextually-appropriate choice between the two policies (constraint-based and environment reward-based) when taking actions. The contextual bandit orchestrator allows the agent to mix policies in novel ways, taking the best actions from either a reward maximizing or constrained policy. The [Laroche and FÃ©raud, 2017] tackles the problem of online RL algorithm selection. A metaalgorithm is given for input a portfolio constituted of several off-policy RL algorithms. It then determines at the beginning of each new trajectory, which algorithm in the portfolio is in control of the behaviour during the next trajectory, in order to maximise the return. A novel metaalgorithm, called Epochal Stochastic Bandit Algorithm Selection. Its principle is to freeze the policy updates at each epoch, and to leave a rebooted stochastic bandit in charge of the algorithm selection.

(p18.1) 3.7 Bandit for Machine Learning:  Table 2 summarizes the types of bandit problems used to solve the machine-learning problems mentioned above. We see, for example, that contextual bandit was not used in feature selection or hyperparameter optimization. This observation could point into a direction for future work, where side information could be employed in feature selection. Also, non-stationary bandit was rarely considered in these problem settings, which is also suggesting possible extensions of current work. For instance, the non-stationary contextual bandit could be useful in the non-stationary feature selection setting, where finding the right features is time-dependent and contextdependent when the environment keeps changing. Our main observation is also that each technique is solving just one machine learning problem at a time; thus, the question is whether a bandit setting and algoritms can be developed to solve multiple machine learning problems simultaneously, and whether transfer and continual learning can be achieved in this setting. One solution could be to model all these problems in a combinatorial bandit framework, where the bandit algorithm would find the optimal solution for each problem at each iteration; thus, combinatorial bandit could be further used as a tool for advancing automated machine learning.
