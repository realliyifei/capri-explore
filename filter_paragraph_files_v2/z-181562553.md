# A Systematic Literature Review on Image Captioning

CorpusID: 181562553 - [https://www.semanticscholar.org/paper/92ccf5a39c63cb5e1639be518e6db2e357acd58e](https://www.semanticscholar.org/paper/92ccf5a39c63cb5e1639be518e6db2e357acd58e)

Fields: Computer Science, Linguistics

## (s6) Results
(p6.0) After reading all the articles and inspired by another SLR [6] we achieved a good understanding on the key aspects in image captioning. To present the summarized results in a convenient way, a comprehensive comparison table (Table A1 in Appendix A ) of all articles found with the methods used was made together with the results on the most used datasets for testing. The structure is presented below:
## (s10) Decoder-LSTM
(p10.0) LSTM (long-short-term memory) was developed from RNN, with the intention to work with sequential data. It is now considered as the most popular method for image captioning due to its effectiveness in memorizing long term dependencies through a memory cell. Undoubtedly this requires a lot of storage and is complex to build and maintain. There have been intentions to replace it with CNN [52], but as we can see from the number of times this method is used in most of the articles found during this SLR (68 of 78), scientists always come back to LSTM. LSTM works by generating a caption by making one word at every time step conditioned on a context vector, together with the previous hidden state and the earlier generated words.
## (s14) Comparison of Results
(p14.0) This study found many articles in which the results of their models had been compared with state of the art models, such as refs. [2][3][4][5]. As these models were built some years ago, they have been more cited so are easier to find during a search on the digital libraries. For example, ref. [5] has been cited 2855 times, according to Google Scholar from Google, while most of the newest articles found have not been cited at all yet, or the ones written in 2018 have usually been cited less than 10 times. Not surprisingly the newer the articles are, the further at the bottom of the search they appear, so most researchers might not even find them if not enough time has been dedicated for a literature review. Figures 1-6 confirm that results are not steadily increasing-there are many results which are not higher than the ones from a year ago. This can undoubtedly be due to the topic difficulty, but also lack of details can lower the goals of researchers so they do not improve knowing that there are higher results already even though a very important part for researchers is to compare their work results with similar approaches. In this study the results from the newest models are presented so upcoming researchers can compare their models with regard to the newest achievements. We hope this research will help further researchers to save their time on detailed literature reviews and to keep in mind the importance of checking for the newest articles.
