# Rank-based Decomposable Losses in Machine Learning: A Survey

CorpusID: 250627000 - [https://www.semanticscholar.org/paper/59ceacd001ee4a0e7984207040275e5e9e657d9b](https://www.semanticscholar.org/paper/59ceacd001ee4a0e7984207040275e5e9e657d9b)

Fields: Computer Science, Medicine, Mathematics

## (s22) Average Top-k (AT k ) Aggregate Loss
(p22.0) The median loss can solve the issue of outliers, but it cannot address imbalanced data situations, and its learning objective is often non-convex. To mitigate these drawbacks, the average top-k (AT k ) loss [8] is proposed, which is the average of the largest k individual losses, that is defined as:

(p22.1) where 1 ≤ k ≤ n. We can find that the AT k loss generalizes the average loss (k = n) and the maximum loss (k = 1). Therefore, it can adapt to imbalanced and/or multi-modal data distributions better than the average loss and is less sensitive to outliers than the maximum loss. Several typical loss functions are shown in Table 4.

(p22.2) Since the AT k loss involved the sorting operation, it will bring a high time complexity in the training when directly optimizing it. Therefore, [8] proposes a reformulation of the AT k loss as the minimization on the average of the individual losses over all training examples transformed by a hinge function:
## (s26) Average of Ranked Range (AoRR) Aggregate Loss
(p26.0) As we mentioned, the average loss is insensitive to minority sub-groups, while the maximum loss is sensitive to outliers. The AT K can dilute but not exclude the influences of the outliers. The AB k is also insensitive to minority sub-group data since they focus on more samples with lower loss values. To this end, the average of ranked range (AoRR) loss is proposed in [13]. Unlike previous aggregate losses, the AoRR loss is robust to imbalanced data and can completely eliminate the influence of outliers if their proportion in training data is known. It can be defined as:
## (s34) Top-k Individual Loss
(p34.0) Class overlap, multi-label nature of samples, and class ambiguity problems appear when the number of classes increases in image classification. Top-k error is explored and studied when a predictor allows k guesses instead of one and is not penalized for k − 1 mistakes. It is regarded as a robust evaluation measure in the current research and competition, such as the top-1 to top-5 performances are evaluated in ImageNet challenge [150]. In such a case, topk accuracy is an important metric that estimates whether the candidates include correct targets, which limits total performance. Therefore, the top-k guided individual loss is proposed and can be defined as:
