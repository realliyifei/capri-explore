# A survey on GANs for computer vision: Recent research, analysis and taxonomy

CorpusID: 247597155 - [https://www.semanticscholar.org/paper/4876459cc2abb2189c41a4e2ec23c6407048920e](https://www.semanticscholar.org/paper/4876459cc2abb2189c41a4e2ec23c6407048920e)

Fields: Computer Science

## (s10) Evaluation metrics
(p10.0) Due to GAN's particularity, there is not an unique metric to measure the quality of the synthesized data [190]. One of the reasons of why there is no consensus among researches is the particularity of each GAN application. As mentioned in previous sections, GANs can be used to replicate any data distribution, but it depends on the particular problem how to measure the differences between the origin and synthesized distributions [17].
## (s21) Auxiliary Classifier GAN (ACGAN)
(p21.0) ACGAN [130] modifies the CGAN structure. The D of the ACGAN does not receive the class label c as an input, instead D is used to classify the probability of the image class. To train the model, the loss function must be modified, dividing the objective function in two parts, one for the correct source of data and the other for the class label. ACGAN loss function can be denoted as:
## (s29) Self Attention GAN (SAGAN)
(p29.0) SAGAN [199]  SAGAN uses self attention layers [174], these layers are capable to capture structural and geometric features of multiclass datasets. The feature maps of each convolution are split into a 1 Ã— 1 convolution in query, key and value, then they are multiplied to construct the output of the layer. This way the network can learn long-range dependencies. The structure of the self-attention layer can be seen in Fig.5. 
## (s38) WGAN-GP
(p38.0) In the original paper of WGAN, the authors suggest that weight clipping is "a terrible way to enforce Lipschitz constraints". Weigh clipping is one problem that the original WGAN had, but it worked well enough and its implementation was easy. The WGAN-GP [55] proposes a new technique to substitute the weight clipping that leads to the WGAN with undesired behavior.

(p38.1) The proposed change involves constraining the critic gradient norm output regarding to the input of the network. The constraint is softened via a penalty on the gradient norm. say that the new loss function is denoted as follows:

(p38.2) The new change makes the WGAN-GP optimize its training, stabilizing it with almost no hyperparameter tuning. The new loss function also improves the quality of the generated images over WGAN and converges faster.
## (s40) Least Square GAN (LSGAN)
(p40.0) The new loss function presented in LSGAN [116] aims to reduce the vanishing gradient problem. The main objective of the LSGAN is to punish the synthesized samples that are far from the real data but still in the correct side of the decision boundary. The least squares loss function is denoted as follows:

(p40.1) where a and b are the labels for fake and real data respectively and c is the label that G wants D to believe is real data. It should be noted that the square of both equations is responsible for punishing far from the decision boundary samples.

(p40.2) The LSGAN tries to generate more gradients while penalizing samples that lie a long way from the decision boundary. This way the gradients are forced to be higher, preventing the gradient vanishing problem. Compared to the classical sigmoid cross entropy loss function of GANs, the new least squares loss is flat only at one point as we can see in Fig.8. 
## (s50) GAN applications
(p50.0) As mentioned before, GANs are one of the most popular applications of machine learning of the last years. GANs models can achieve results in fields where previous models could not, in other cases, GANs improve the previous results significantly.

(p50.1) In this section, we will review the most important fields where GAN architectures are applied, paying a special attention to the GAN models related to computer vision tasks and we will compare the different architecture results.

(p50.2) Most of the last researches focus on how to apply GANs to generate new synthesized data, replicating a data distribution. But, as we will review in this section GANs can be applied to other fields, e.g. video game creation [80].
