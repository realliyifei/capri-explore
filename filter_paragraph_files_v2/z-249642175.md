# Multimodal Learning with Transformers: A Survey

CorpusID: 249642175 - [https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7](https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7)

Fields: Computer Science, Medicine

## (s0) INTRODUCTION
(p0.0) The initial inspiration of Artificial Intelligence (AI) is to imitate human perception, e.g., seeing, hearing, touching, smelling. In general, a modality is often associated with a specific sensor that creates a unique communication channel, such as vision and language [1]. In humans, a fundamental mechanism in our sensory perception is the ability to leverage multiple modalities of perception data collectively in order to engage ourselves properly with the world under dynamic unconstrained circumstances, with each modality serving as a distinct information source characterized by different statistical properties. For example, an image gives the visual appearance of an "elephants playing in water" scene via thousands of pixels, whilst the corresponding text describes this moment with a sentence using discrete words. Fundamentally, a multimodal AI system needs to ingest, interpret, and reason about multimodal information sources to realize similar human level perception abilities. Multimodal learning (MML) is a general approach to building AI models that can extract and relate information from multimodal data [1].
## (s7) Vanilla Transformer
(p7.0) Vanilla Transformer has an encoder-decoder structure and is the origin of the Transformer-based research field. It takes tokenized input (see Section 3.1.1). Both its encoder and decoder are stacked by the Transformer layers/blocks, as demonstrated in Figure 1. Each block has two sub-layers, i.e., a multi-head self-attention (MHSA) layer (see Section 3.1.2) and a position-wise fully-connected feed-forward network (FFN) (see Section 3.1.3). To help the back propagation of the gradient, both MHSA and FFN use Residual Connection [159] (given an input x, the residual connection of any mapping f (·) is defined as x ← f (x) + x), followed by normalization layer. Thus, assuming that the input tensor is Z, the output of MHSA and FFN sub-layers can be formulated as:
## (s8) Input Tokenization
(p8.0) Tokenization Vanilla Transformer was originally proposed for machine translation as a sequence-to-sequence model, thus it is straightforward to take the vocabulary sequences as input. As mentioned previously, the original selfattention can model an arbitrary input as a fully-connected graph, independently of modalities. Specifically, both Vanilla and variant Transformers take in the tokenized sequences, where each token can be regarded as a node of the graph. 1. In this survey, "multimodal Transformer" means "Transformer in multimodal learning context".
## (s38) Efficiency
(p38.0) Multimodal Transformers suffer from two major efficiency issues: (1) Due to the large model parameter capacity, they are data hungry and thus dependent on huge scale training datasets. (2) They are limited by the time and memory complexities that grow quadratically with the input sequence length, which are caused by the self-attention. In multimodal contexts, calculation explosion will become worse due to jointly high dimension representations. These two bottlenecks are interdependent and should be considered together.

(p38.1) To improve the training and/or inferring efficiency for multimodal Transformers, recent efforts have attempted to find various solutions, to use fewer training data and/or parameters. The main ideas can be summarized as the follows.
