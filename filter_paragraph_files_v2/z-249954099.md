# Synthetic Data-Based Simulators for Recommender Systems: A Survey

CorpusID: 249954099 - [https://www.semanticscholar.org/paper/74aa80b9c39724146682a55642de59002ed2460c](https://www.semanticscholar.org/paper/74aa80b9c39724146682a55642de59002ed2460c)

Fields: Computer Science

## (s18) Models for generating user-item responses
(p18.0) The In UserSim Zhao et al. (2021), a user response model is based on a generative adversarial network, where the generator is used to generate synthetic logs based on the historical data, while the discriminator is utilized to predict users' behavior. The generative adversarial network is also used in CLL Chen et al. (2019) for the simulation of user's behavior (in the form of sequential choices) and learning of the user's reward function.
## (s25) Unrealistic metrics
(p25.0) To evaluate an RS offline, a typical procedure consists of dividing the available data into training and test sets. The RS is trained on the training set, part of which is usually used for validation. After training, RS is used for making predictions on the test set and comparing predictions with real data using some metrics. These metrics are called accuracy-based since they are computed on a test set using the true responses. The most frequent ones are Precision@K, Recall@K, AUC@K, nDCG@K, MAP@K. However, it has been noticed that the users' behavior is more complex than these metrics would imply. In particular, users would like to discover new content, and value diverse and surprising recommendations. Hence, there are beyond accuracy metrics proposed to evaluate an RS output: Novelty, Serendipity, Unexpectedness, Diversity and others Silveira et al. (2019).
## (s73) Models for generating user-item responses
(p73.0) The In UserSim Zhao et al. (2021), a user response model is based on a generative adversarial network, where the generator is used to generate synthetic logs based on the historical data, while the discriminator is utilized to predict users' behavior. The generative adversarial network is also used in CLL Chen et al. (2019) for the simulation of user's behavior (in the form of sequential choices) and learning of the user's reward function.
## (s80) Unrealistic metrics
(p80.0) To evaluate an RS offline, a typical procedure consists of dividing the available data into training and test sets. The RS is trained on the training set, part of which is usually used for validation. After training, RS is used for making predictions on the test set and comparing predictions with real data using some metrics. These metrics are called accuracy-based since they are computed on a test set using the true responses. The most frequent ones are Precision@K, Recall@K, AUC@K, nDCG@K, MAP@K. However, it has been noticed that the users' behavior is more complex than these metrics would imply. In particular, users would like to discover new content, and value diverse and surprising recommendations. Hence, there are beyond accuracy metrics proposed to evaluate an RS output: Novelty, Serendipity, Unexpectedness, Diversity and others Silveira et al. (2019).
