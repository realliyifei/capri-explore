# Visuo-Haptic Object Perception for Robots: An Overview

CorpusID: 247596914 - [https://www.semanticscholar.org/paper/49579115424853a2e479678023ef592fddaca003](https://www.semanticscholar.org/paper/49579115424853a2e479678023ef592fddaca003)

Fields: Computer Science, Engineering

## (s3) Prehension of Objects
(p3.0) Object perception benefits greatly from performing exploratory procedures (EPs) on an object of interest, to observe different sides of an object or perceive non-visual features for instance. For that, we first reach towards that object, i.e., move our hand close to its location, and then grasp it, which involves pre-shaping our hand to the object's physical properties and selecting the optimal grip type. The capacity to reach and grasp objects is also more generally referred to as prehension (Turella and Lingnau, 2014).
## (s28) Transfer Learning
(p28.0) One of the challenges of transfer learning (colearning) is that machine learning models are based on the assumption that both training and test data are drawn from the same distribution. However, such an assumption does not hold when transferring knowledge between different robots or sensor modalities. A possible solution is domain adaptation, a.k.a. transfer learning, (e.g., Daum√© III and Marcu, 2006;Wang and Deng, 2018). Here, training samples from a source dataset are adapted to fit a target distribution.
## (s30) Multimodal Object Perception for Manipulation
(p30.0) Robotic manipulation has a huge impact in many industrial and service applications; visuo-tactile perception has been actively studied to improve the performance of robots, for instance, by allowing more secure object grasping and handling with a lower risk of damaging delicate objects. In the multimodal setting, visual perception is predominantly used for planning reaching trajectories and identifying grasp type and orientation, while haptic perception is typically used for slippage prevention and compliant grasping. The classical way of tackling the problem of grasping has been with model-based, i.e., analytical approaches, and examples of such multimodal perception for grasping and manipulation in the literature are abundant. However, as seen in other fields, recently, there has been a tendency to move from model-based approaches to data-driven ones. In this section, we outline the importance of using both the visual and haptic modality for grasping and manipulation tasks by presenting several recent approaches whose results show that multimodal variants are outperforming the uni-modal ones; see Bohg et al (2014) for an in-depth survey of older data-driven grasping approaches.
## (s32) Grasping
(p32.0) Once an object is reached, the robot can grip the object and lift it. At this stage, it is crucial to find a good gripper configuration and to apply an adequate force such that the grasp is successful. Calandra et al (2018) presented a data-driven action-conditioned approach for predicting grasp success that can be used to determine the most promising grasping action based on raw visuotactile information. Given an action consisting of 3D motion, in-plane rotation and change of force applied by the gripper, the proposed model uses a midst-mapping fusion strategy to combine the different modalities and predict the grasp outcome. First, the visual input from a Kinect v2 camera and the tactile input from two GelSight sensors attached to the fingers of a Weiss WSG-50 gripper are separately processed by CNNs, while an MLP processes the action channel. Then the latent features were concatenated and fed to an MLP that outputs the probability of successful grasp. The results show that the multimodal variant outperformed uni-modal or hard-coded baselines when grasping previously unseen objects. Furthermore, the qualitative analysis shows that the model learned meaningful grasping strategies for positioning the gripper and what amount of force to apply for successful grasping.
## (s33) Maintaining Grasping
(p33.0) Once the object is grasped and lifted, slip detection is essential for maintaining a successful grasp. For instance, the gripper force can be adjusted to prevent objects from dropping when a slip is detected. In this direction, Li et al (2018) proposed a datadriven visuo-tactile model for slip detection of grasped objects based on DNN architecture. Their model uses a sequence of eight consecutive GelSight and corresponding camera image pairs during a grasp-and-lift attempt. Each modality undergoes a separate feature extraction step through a pretrained CNN, after which the latent features for both modalities are concatenated (midst-mapping) and passed through an additional FC layer. LSTM layers are used on top of the FC layer, and a final FC layer provides the probability that a slip occurred for the duration covered by the image sequence. During the experimental evaluation, several conditions were tested, like the type of image input (raw vs difference images), type of feature extractor (different off-the-shelf CNN models) or the type of information (visual, tactile or visuotactile). The best performing model used combined visuo-tactile information, significantly outperforming the unimodal approaches and achieving 88% accuracy in detecting slips on a test dataset of unseen objects.
## (s37) Biologically-inspired Approaches
(p37.0) Regarding biological inspiration, the question for robotics is which and in what proportion bioinspired sensory and data processing principles can help us improve multimodal object recognition in its multiple application areas. Sensor technologies are largely bio-inspired, and there are efforts to incorporate other capabilities, such as measuring humidity, hardness, and viscosity, as well as mimicking other skin properties such as self-healing (Oh et al, 2019). On the contrary, perception models in artificial agents are still largely detached from their biological counterparts. While some biological principles have been explicitly studied, like integration strategies (e.g., Toprak et al, 2018), others like hierarchical processing and input-driven self-organization or processing of object properties rather than sensory modality are some of the promising directions that should be further explored.
