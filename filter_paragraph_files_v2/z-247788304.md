# A Survey on Quantum Computational Finance for Derivatives Pricing and VaR

CorpusID: 247788304 - [https://www.semanticscholar.org/paper/37e273264c38589693c54d332d827a9f57b85739](https://www.semanticscholar.org/paper/37e273264c38589693c54d332d827a9f57b85739)

Fields: Computer Science, Economics

## (s6) Numerical Methods for Options
(p6.0) In order to compute the value of European options with one underlying asset, we can use Girsanov's theorem 4 to work in the risk neutral probability measure Q. Mathematical models for options pricing must take into account the absence of arbitrage hypothesis. For this purpose, the discounted prices of financial products must satisfy the martingale property under certain probability measure, this measure is the so called risk neutral probability measure Q. The dynamics of the underlying asset under this measure is obtained by replacing the drift by the risk free rate r in (5). Under the measure Q, the value of the option is a martingale process, that is, the conditional expected value of the discounted

(p6.1) value of the derivative is constant through time. Next, we use Ito's lemma 5 and the martingale property, so that the option value v t = V(t, S t ) can be obtained as a conditional expectation of the form:
## (s13) 3
(p13.0) factors of the systemic part, the model can be classified into the one-or multi-factor class. The power of factor models is that they allow us to reduce the number of parameters needed to capture the portfolio correlations. In the following section we briefly describe some of the most commonly employed models.

(p13.1) One-Factor Models In the one-factor model setting, the credit quality (which in the Merton model is defined as the logarithmic return of the asset value) of counterparty j, X j , at time T is represented by a common, standard normally distributed single factor Y component and an idiosyncratic Gaussian noise component j . The dependence structure between these two latent random variables can be set using copula 6 functions. Thus, these models are also called onefactor copula models. Two models are usually considered in practice. The Gaussian copula model is given by:

(p13.2) where Y and j are i.i.d. standard normal random variables for all j = 1, … , J . Alternatively, as an extension of the model in Eq. (40), the t-copula model was introduced to take into account tail dependence,

(p13.3) , W follows a chi-square distribution 2 ( ) with degrees of freedom and 1 , ⋯ , J , Y and W are mutually independent. Scaling the model in Equation (40) by the factor √ ∕W transforms standard Gaussian random variables into t-distributed random variables with degrees of freedom. For both models, the parameters 1 , … , J ∈ (0, 1) are the correlation coefficients. In the case that j = , for all j = 1, … , J , the parameter is called the common asset correlation.

(p13.4) According to the Merton model described above, counterparty j defaults when the value of its assets falls below the barrier j . The barrier is therefore defined by j ∶= Φ −1 (P j ) or j ∶= Φ −1 (P j ) for the Gaussian and t-copula models respectively, where Φ −1 denotes the inverse of the standard normal cumulative distribution function and Φ −1 is the corresponding inverse distribution function of the t-distribution (with degrees of freedom).

(p13.5) Multi-factor Models Multi-factor models aim to capture more realistic correlation structures, e.g. counterparties in similar industrial sectors and geographies would typically be more correlated. For this, we consider the extension to multiple dimensions of the models presented in Sect. 2.2.2,

(p13.6) i.e., the multi-factor Gaussian copula model and the multifactor t-copula model. The d-factor Gaussian copula model assumes that the covariance structure of [V 1 , … , V J ] is determined by the multi-factor model,

(p13.7) T denotes the systematic risk factors. Note that we represent vectors by bold symbols. Here, a j = a j1 , a j2 , … , a jd T represents the factor loadings satisfying a T j a j < 1 , and j are standard normally distributed random variables representing the idiosyncratic risks, independent of each other and independent of Y . The constant b j , being the factor loading of the idiosyncratic risk factor, is ch o s e n s o t h a t V j h a s u n i t va r i a n c e , i . e . ,

(p13.8) The incentive for considering the multi-factor version of the Gaussian copula model becomes clear when one rewrites it in matrix form, While each j represents the idiosyncratic factor affecting only counterparty j, the common factors Y 1 , Y 2 … , Y d , may affect all (or a certain group of) counterparties. Although the systematic factors are sometimes given economic interpretations (as industry or regional risk factors, for example), their key role is that they allow us to model complicated correlation structures in a non-homogeneous portfolio.
## (s19) Original Amplitude Estimation
(p19.0) The path suggested in the original paper [15] is to adopt an inverse quantum Fourier transform, see Fig. 3. In the picture, the oracle A of Sect. 3.1.1 corresponds to R(P ⊗ ) acting on the first n qubits. The first n qubits are the physical register upon which the block P loads the probability distribution. The block R applies the function whose expected value we want to compute, this is assumed to require one auxiliary qubit. The last m qubits constitute an auxiliary register used for amplitude estimation; it controls different powers of the Grover amplification block Q. Eventually, an inverse of the Quantum Fourier transform on the auxiliary register provides the phase estimation, from which one recovers the amplitude.

(p19.1) The inverse Quantum Fourier transform adopted by the original amplitude estimation algorithm [15] is resource demanding and thus constitutes a serious bottleneck, especially in relation to current or near-future technologies. Said otherwise, Quantum Fourier transform requires a deep and wide quantum circuit. For this reason, some algorithms which need less resources have been proposed.
## (s21) Applications to Option Pricing
(p21.0) As presented in Eq. (8), the option pricing problem can be formulated as the computation of the expected value of a payoff function with respect to a probability distribution: 10 where the domain Ω can be multi-dimensional. In pure computational terms, the problem can be reduced to that of computing an expression like:

(p21.1) where Here we have implicitly defined p, f and X, which are proper discretized versions of p , f and Ω respectively. Recall that, upon the discretization procedure (which we are not specifying to keep the treatment general), the sum in (53) corresponds to the discretized version of the original integral we needed to compute. The problem of defining a suitable (and
## (s31) Discussion
(p31.0) In this section we discuss some of the open problems in quantum computing for option pricing and VaR. As QAMC approaches to solve pricing and VaR problems are the most widely adopted ones in the literature and the ones taking more attention, we focus on them. The main implicit assumption in QAMC is the existence of an efficient oracle which loads the probability distribution. Both for pricing and VaR, loading the distribution means that it is necessary to create a circuit for a unitary operation P such that:

(p31.1) In the case of VaR, the cost of creating such a unitary can be mitigated to some extent using the techniques from Sect. 3.4.1. In the case of pricing, it is much more critical as we discuss below.

(p31.2) In pricing, the distribution to be loaded has to be previously generated through the simulation of a SDE. As it was discussed in the first part of this survey, the simulation of the SDE consumes most of the computing resources. When assessing the overall performance of the QAMC one must take into account this step. Otherwise, the latter comparison of the QAMC and the CMC would be unfair. Indeed, the claims of a quadratic speed-up of the QAMC over the CMC for financial applications-in general-do not take into account the generation step. If we compare both QAMC and CMC under the same conditions, with the approaches proposed in the literature, we will find that there is no rigorous evidence for the quadratic speed-up.

(p31.3) If we assume that we have the probability distribution in the classical case (as it is done for the quantum one), the problem of pricing is reduced to computing the following expectation where p is the probability distribution, f de payoff function and x i are the points where we know the probability

(p31.4) distribution. In this case the number of operations performed in the classical computer is of order N and there is no quadratic speedup for the QAMC. In fact, when adding the costs of loading the probability distribution and the payoff into the quantum state we might end up with a clear disadvantage.

(p31.5) These problems provide concrete examples about possible issues encountered in designing full quantum algorithms able to reach a quantum advantage. Almost any speed-up concentrated in a subroutine of an overarching inefficient algorithm, however interesting, is clearly not sufficient to reach quantum advantage.

(p31.6) We are here implicitly referring (as it often happens) to quantum advantage in terms of scaling of the execution time. This is only a part of a bigger picture which needs to involve other variables such as the energy consumption and cost. Strangely enough, this wider picture is usually not analyzed in the quantum finance literature.

(p31.7) Many ideal algorithms studied in the literature are not viable on current or near-future quantum technology 14 . They usually require either an exceedingly large number of qubits or involve too deep a circuit with respect to the realistic coherence time, or both. The theoretical analysis of algorithms should be always accompanied by a critically explored awareness of current and future technological limitations. In this perspective, an important (negative) claim has been described in [8], where it is argued that a quadratic speed-up is not sufficient to obtain a quantum advantage, mainly due to the-constant but large-resource overheads (needed for error correction). An important overarching suggestion emerging from [8] is that the complexity scaling is in general not enough to properly define an actual threshold for quantum advantage.
