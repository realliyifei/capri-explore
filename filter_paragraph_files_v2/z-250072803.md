# Self-supervised Learning in Remote Sensing: A Review

CorpusID: 250072803 - [https://www.semanticscholar.org/paper/cfd94ae8dd48c695cd0d0d63cd67573bd5310f87](https://www.semanticscholar.org/paper/cfd94ae8dd48c695cd0d0d63cd67573bd5310f87)

Fields: Computer Science, Environmental Science

## (s1) A. Supervision: Data vs. Labels
(p1.0) A significant fraction of real-world applications of machine/ deep learning methodologies calls for massive amounts of training data. Fortunately, the increasing adoption of open access in earth observation provides remote sensing researchers with a plethora of such. However, annotation of such vast amounts of earth observation data implies significant human interaction with frequent updates needed every single day. For example, apart from the large amounts of sampled data to label, a single large-scale dataset may require separate annotation depending on downstream applications. Moreover, Fig. 1. The number of recent publications related to self-supervised learning (SSL). While a clear trend of increased efforts to advance SSL is observed, activity in remote sensing lags behind. datasets sensing the physical world get outdated over time due to both natural and man-made changes. Both aspects significantly amplify the efforts in annotating remotely sensed data of the Earth.

(p1.1) Another big challenge of machine learning in remote sensing is label noise. Due to the quality of remotely sensed images and the complexity of various specific applications, it is very difficult to generate perfect labels during large-scale data annotation. Therefore, there always exists a trade-off between the number of annotations and their quality: large but noisy labeled datasets can bias the model, whereas small amounts of good-quality labels usually lead to overfitting.

(p1.2) In addition, it has become popular to execute pre-training on established benchmark datasets before fine-tuning deeply learned models for downstream tasks that do not have enough labels. This procedure is commonly referred to as transfer learning [16]. However, the performance of supervised pretraining depends on the domain difference between source and target data. A good pre-trained model renders well on a similar dataset but is not as useful on a very different one.

(p1.3) All the above challenges emphasize the growing gap between an increasing amount of remote sensing data and the shortage of good-quality labels, calling for techniques to exploit the corpus of unlabeled data to learn valuable information that easily transfers to multiple applications. Self-supervised learning offers a paradigm to approach this dilemma. B. Self-supervision: Learning Task-agnostic Representations of Data Fig. 2 schematically depicts the general underlying principle of self-supervised learning. Based on a certain self-produced objective (the so-called self-supervision), a large amount of unlabeled data is exploited to train a model f 1 by optimizing this objective without requiring any manual annotation. With a carefully designed self-supervised problem, the model f gets the ability to capture high-level representations of the input data. Afterward, the model f can be further transferred to supervised downstream tasks for real-world applications. The most common strategies to design such self-supervision typically exploit three types of objectives: (1) reconstructing input data x, f (x) → x, (2) predicting a self-produced label c which usually comes from contextual information and data augmentation (e.g., predicting the rotation angle of a rotated image), f (x) → c, (3) contrasting semantically similar inputs x 1 and x 2 (e.g., the encoded features of two augmented views of the same image should be identical), |f (x 1 ) − f (x 2 )| → 0.

(p1.4) Given self-supervised training succeeded, the pre-trained model f may get transferred to downstream tasks. As Opposed to supervised pre-training, models pre-trained by selfsupervision bear the potential to leverage more general representations and offer a paradigm to mitigate the shortcomings of supervised learning: (1) no human annotation is needed for pre-training, (2) small amounts of labels are sufficient for good performance on downstream tasks, (3) little domain gap between pre-training and downstream dataset can be ensured by collecting unlabeled data from the target application.

(p1.5) Link to semi-supervised learning. Self-supervised learning algorithms may be considered as part of semi-supervised learning-a branch of machine learning concerned with labeled and unlabelled data. Van Engelen et al. [4] proposed a comprehensive taxonomy of semi-supervised classification algorithms based on conceptual and methodological aspects for processing unlabelled data. Within that scheme, selfsupervised learning may get categorized as unsupervised preprocessing: in the first step, unlabeled data gets transformed to extract feature representations; labeled data is then utilized to adapt the model to specific tasks.
## (s5) B. Predictive Methods
(p5.0) While most generative methods perform pixel-level reconstruction or generation, predictive self-supervised methods are based on auto-generated labels. In fact, one may argue that being able to generate very high dimensional data points, such as images, is not necessary to learn useful representations for many downstream tasks. Instead, one can focus on predicting specific properties of the data, which is the general idea behind predictive methods. To this end, the so-called pretext tasks 2 get utilized. A predictive method firstly designs a suitable pretext task for the dataset, prepares self-generated labels, and trains a model to predict such labels and learn data representations.

(p5.1) Predictive self-supervised learning targets two possible drawbacks associated with generative methods that perform pixel-level reconstruction: (1) pixel-level loss functions may overly focus on low-level details whereas in practice such details are irrelevant for a human to recognize the contents of an image, (2) pixel-based reconstruction typically do not involve pixel-to-pixel (long-range) correlations that can be important for image understanding. Based on the assumption that providing the network with relevant high-level pretext tasks, the network may learn high-level semantic information.

(p5.2) In general, the design of different pretext tasks harnesses various context information of the input data. According to distinct context attributes, we categorize pretext tasks as follows: spatial context, spectral context, temporal context, and other semantic contexts.

(p5.3) 1) Spatial Context: Images contain rich spatial information for designing self-supervised pretext tasks. Doersch et al. [62] proposed the first example of such methods, predicting the relative position of pairs of randomly cropped image patches drawn from a given input sample. It is assumed that doing well on this task requires a global understanding of the scene and its objects contained. Accordingly, a valuable visual representation is expected to extract the composition of objects in order to reason about their relative spatial location.
