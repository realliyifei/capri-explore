# Latent Variable Modelling Using Variational Autoencoders: A Survey

CorpusID: 249889169 - [https://www.semanticscholar.org/paper/6cd09476afab33bbefff5703c493a8404c909b6a](https://www.semanticscholar.org/paper/6cd09476afab33bbefff5703c493a8404c909b6a)

Fields: Computer Science, Mathematics

## (s10) Hierarchical inference
(p10.0) Learning the complex distribution over the data was largely simplified by the addition of latent variables as discussed in 2.1, we use the same idea to learn a complex approximate posterior by adding latent variables to the approximate posterior. This method was introduced by [33] in the context of variational inference, this section discusses the method in the context of variational autoencoders.

(p10.1) We can add latent variables λ to the approximate posterior q(Z) and learn q(Z) by marginalizing over simpler distributions q(Z|λ) and q(λ), this is the core idea behind hierarchical inference. The approximate posterior obtained through marginalization is given as:

(p10.2) Learning the approximate posterior through marginalization becomes intractable when the number of latent states λ is too large, this is a standard problem in latent variable models solved using VAEs. So we learn another VAE by maximizing the lower bound on the approximate posterior q(Z). This second VAE infers the posterior q(λ|Z) and is seen as performing auxiliary inference. Thus, adding latent variables to latent variables creates additional inference steps, hence the name hierarchical inference.
## (s22) Unbiased estimates of gradients
(p22.0) Relaxing the objective allows training however the training is biased towards the continuous latent states. In this section, we discuss REBAR [48] a method that allows us to construct control variates using the biased gradient estimator and the score function gradient estimator. This control variate is added to the objective to obtain unbiased estimates of the gradients.
## (s46) Hierarchical inference
(p46.0) Learning the complex distribution over the data was largely simplified by the addition of latent variables as discussed in 2.1, we use the same idea to learn a complex approximate posterior by adding latent variables to the approximate posterior. This method was introduced by [33] in the context of variational inference, this section discusses the method in the context of variational autoencoders.

(p46.1) We can add latent variables λ to the approximate posterior q(Z) and learn q(Z) by marginalizing over simpler distributions q(Z|λ) and q(λ), this is the core idea behind hierarchical inference. The approximate posterior obtained through marginalization is given as:

(p46.2) Learning the approximate posterior through marginalization becomes intractable when the number of latent states λ is too large, this is a standard problem in latent variable models solved using VAEs. So we learn another VAE by maximizing the lower bound on the approximate posterior q(Z). This second VAE infers the posterior q(λ|Z) and is seen as performing auxiliary inference. Thus, adding latent variables to latent variables creates additional inference steps, hence the name hierarchical inference.
## (s58) Unbiased estimates of gradients
(p58.0) Relaxing the objective allows training however the training is biased towards the continuous latent states. In this section, we discuss REBAR [48] a method that allows us to construct control variates using the biased gradient estimator and the score function gradient estimator. This control variate is added to the objective to obtain unbiased estimates of the gradients.
