# Survey on Automated Machine Learning

CorpusID: 139106022 - [https://www.semanticscholar.org/paper/e7e6684e5e73eae210c6d7adfc57e245f0fa0fdf](https://www.semanticscholar.org/paper/e7e6684e5e73eae210c6d7adfc57e245f0fa0fdf)

Fields: Computer Science, Mathematics

## (s20) Multi-Armed Bandit Learning
(p20.0) In Section 4.3 the general problem of exploitation versus exploration was shortly introduced. An extensively studied model for this problem is the multi-armed bandit problem (Robbins, 1952). A gambler has access to K different slot machines and each machine provides a reward based on an unknown distribution. The gambler is allowed to pull a fixed number of arms of any machine he wants to use. To maximize his reward, the gambler has to find the arm with the highest probability of reward as soon as possible and then keep using this arm.
## (s35) Grid Search
(p35.0) Grid search is the classic approach for HPO with many different implementations. For the experiments the existing GridSearchCV implementation from scikit-learn (Pedregosa et al., 2011) is utilized. Besides a parallelization to evaluate several configuration instances at the same time on a single machine, the scikit-learn implementation does not provide any performance improvements. To ensure fair results, a mechanism for stopping the optimization after a fixed number of iterations has been added. For each configuration instance, the performance is calculated using cross-validation.

(p35.1) By design, GridSearchCV is limited to HPO for a fixed algorithm. To extend this implementation for algorithm selection, a distinct GridSearchCV instance is created for each available ML algorithm. This allows sequential evaluation of all available ML algorithms while also reducing the search space significantly by eliminating redundant configurations, for example trying different number of neighbors for k nearest neighbors while using a SVM for classification. When all grid search instances have finished, the best result of all instances is returned. 
## (s36) Random Search
(p36.0) The other classic approach for HPO is random search. This algorithm also has many different implementations, but again the scikit-learn (Pedregosa et al., 2011) implementation RandomizedSearchCV is used. RandomizedSearchCV tests a fixed number of random configurations in parallel on a single machine. For each tested configuration, the performance is calculated using cross-validation. Similar to GridSearchCV, RandomizedSearchCV is also designed to optimize only a single estimator. Therefore, the ability to also select a random algorithm has been added. The RandomizedSearchCV code is wrapped to first select an algorithm and the according configuration space Λ (i) and then passed to the scikit-learn implementation.
## (s37) Spearmint
(p37.0) Spearmint (Snoek et al., 2012) uses SMBO with a Gaussian process as a surrogate model for proposing configurations. As mentioned in Section 4.3.1, the selection of the mean function m and covariance function k are crucial for the regression performance. Spearmint uses a constant mean function m(λ) = c and an adjusted Matérn kernel. It is important to note, that this configuration contains meta-hyperparameters itself that are not subject to the CASH optimization: the constant mean c, the length scales θ 1:d of the kernel and the observation noise ν. In combination with the expected improvement in Equation (5) as the acquisition function, these meta-hyperparameters are optimized via the marginalization of the integrated expected improvement a(λ, D) = E max 0, f − f (λ) · p (θ, ν, c | D 1:n ) dθ.
## (s42) BOHB
(p42.0) BOHB  is a composed solver for the CASH problem. It is a combination of Bayesian optimization and hyperband (Li et al., 2018). A limitation of hyperband is the random generation of the tested configurations. BOHB replaces this random selection by a SMBO procedure. All function evaluations are stored in and modeled by a TPE. New configurations are drawn from l(λ) in Equation (8)   candidate configurations is sampled at random to comply with the theoretical guarantees of hyperband (Li et al., 2018). For each function evaluation, BOHB passes the current budget and a configuration instance to the objective function. The interpretation of the budget is conferred to the user, meaning it can represent basically anything, e.g., the fraction of training data to use, available runtime or number of iterations.
## (s48) hyperopt-sklearn
(p48.0) hyperopt-sklearn (Komer et al., 2014) is a framework for fitting classification and regression pipelines. The pipeline shape is fixed to exactly one preprocessor and one classification or regression algorithm; all algorithms are taken from scikit-learn. Those two algorithms are selected and configured via hyperopt. In general, hyperopt-sklearn only provides a thin wrapper around hyperopt by introducing the fixed pipeline shape and adding a configuration space definition for each implemented algorithm. Besides the addition of a time budget per evaluation, no other performance improvements are implemented. To limit the effects of overfitting, cross-validation is used to evaluate the performance of a single configuration. hyperopt-sklearn stops the optimization after a fixed number of iterations.
