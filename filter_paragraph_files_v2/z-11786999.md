# Siamese Learning Visual Tracking: A Survey

CorpusID: 11786999 - [https://www.semanticscholar.org/paper/754fa133a250d824c50b4c3b9c73975059954f41](https://www.semanticscholar.org/paper/754fa133a250d824c50b4c3b9c73975059954f41)

Fields: Computer Science, Mathematics

## (s11) Disucssion
(p11.0) The literature shows the enormous success of using machine learning to improve the robustness of tracking. While recent work and initiatives try to establish community platforms, evaluation protocols and allow new insights into tracking, only a few works consider the problem of initialisation. Vagueness, complexity and computability of tracking are strongly intertwined and suggest a common machine learning approach as principled solution. However, it is important to point out that although machine learning is very promising to control vagueness by fully exploiting video information, learning will fail in the most general case of uncertainty, as learning assume priors of the underlying random processes, a constraining assumption in case of real disjuncture between known random processes and new unknown processes with unknown statistics. [33] emphasised recently this an other problems as robustness to distributional shift.
## (s17) SiamFC
(p17.0) [53] propose two identical branches inherited from AlexNet with five conv layers, max-pooling following the first two conv layers and ReLUs after every conv layer except for conv5. A novel cross-correlation layer links the two conv5 layers. By waiving padding the whole network is fullyconvolutional. The output is an unbounded correlation map with high values at pixels indicating object presence. As for YCNN and SINT, the branches can be seen as spatial description of increasing complexity which is embedded in a metric space where cross-correlation is used as similarity function. Like SINT, SiamFC learns discriminating solely the features with triplets of template, search region and corresponding prediction map. Values isotropically within a radius of the centre count correctly to the object's position, hence are labeled positively whereas all other values are labeled negatively. Training is done on videos of objects from ImageNet [2]. Augmentation considers scale but not translation, because of the fully-convolutional network property. Training minimises a discriminative mean logistic loss by using SGD, mini-batches, Xavier initialisation and annealing of the learning rate. Tracking computes the position via the up-sampled prediction map for a given template. The tracker handles scale by searching over five different scale variations and updates scale by linear interpolation.
## (s22) Connection of Branches
(p22.0) All proposed methods except SINT connect the branches, SiamFC and CFNet with a single crosscorrelation layer, YCNN and GOTURN with three fc layers. SINT omits the concatenating layer by using normalisation layers at the end of both branches. Fig. 2 illustrates these three variations of network architecture. This Siamese network architecture of SiamFC, CFNet and SINT in combination with parameter sharing limit the feature selection to the spatial image domain. Instead, YCNN and GO-TURN enable additional learning of spatiotemporal features in the concatenating layers, as argued by [52] such as "relationships between an object's appearance change and its motion" which seems very general for different categories of objects. Parameter sharing has the consequence that all methods require appearance constancy between template and search region, hence [51]'s argument that YCNN's deep features show "superiority of recognising an object with varying appearance" is questionable. SiamFC, Theoretically, the network of GOTURN generalises over the network of SiamFC which allows capturing features beyond sole visual features of the exemplar image and which allows regression of the bounding box instead of convoluting a final score map capturing potential positions of the exemplar image within the search image. The author's argue that GOTURN learns a generic relationship between arbitrary motion and visual features, however this is not clear yet. Due to the more general Y-shaped architecture it might learn features beyond pure visual such as motion and their relationship in the fully-connected layers, however the network might also be able to learn context features as well.

(p22.1) CFNet and SINT assume a specific function of similarity and the idea is to solely learn visual features to best match the given similarity. SINT even expresses similarity by the training loss which might have advantages in generalisation as particular different functions of similarity and training loss might derive adversary optimisation problems. This is not a problem for YCNN and GOTURN, as similarity and features are jointly learned, however, the interference with the particular training loss is unclear.
## (s23) Network Training
(p23.0) Training is a crucial for sufficient performance. All methods describe basically two training phases, (i) a pre-training phase to transfer-learn generic features of objects from labeled datasets and (ii) a fine-tuning phase to adapt features to given video sets. The cross-correlation layer has here advantages as crosscorrelation preserves the convolutional property of the whole network which introduces invariance to object translation. Therefore training samples must not contain translated versions which reduces significantly the effort for training. Less augmentation of training data is needed. The training loss and its choice has significant influence on the training result. [52] argue that L 1 is superior to L 2 as it penalises more harshly small errors near zero which increases substantially accuracy of the predicted bounding box. This argument is an exception, as none of the other studies show some insights into this important problem. [52] chose also different inputs for training and studied their influence on the mean error derived from VOT accuracy and robustness measures. They show that feeding the network with whole frames instead of template and search region pairs, the frames' contexts are exploited which reduces significantly the mean error, especially in cases of occlusion. SINT is the only method following this insight but without any hints of their awareness. The reason is that their motivation comes from image retrieval where processing of frames is common.
