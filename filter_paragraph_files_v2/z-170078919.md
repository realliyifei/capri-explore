# A survey of Object Classification and Detection based on 2D/3D data

CorpusID: 170078919 - [https://www.semanticscholar.org/paper/b2c768b7ddc9a038111a1361ef25828adfdcbdf8](https://www.semanticscholar.org/paper/b2c768b7ddc9a038111a1361ef25828adfdcbdf8)

Fields: Computer Science, Engineering

## (s9) 3D-image based systems
(p9.0) In this section, 3D-image based systems will be introduced. The main highlevel tasks in 3D vision, 3D image data representation and methods used to do the 3D classification and object detection are briefly described. Several import papers in this area are surveyed. Figure 27: A 3D shape reconstruction example: 3D-RecGAN reconstructs a full 3D shape from a single 2.5D depth view [42] Similar to the 2D systems, the main tasks also include 3D object classification, 3D object detection, 3D semantic segmentation and 3D instance segmentation. Also, as 3D images commonly suffer from occlusion such as self occlusion and inter object occlusion. In order to address this issue, a 3D shape reconstruction task is also actively researched in the 3D vision understanding community. This part is important, however, it will not be surveyed. An example of 3D shape reconstruction is shown in Figure 27 3.2 3D image data representation 3D images can have multiple data representations, such as multi-view RGB-D images, volumetric images, polygonal meshes and point clouds.    3D images are becoming more and more important and are widely used in reconstructing architectural models of buildings, navigation of self-driving cars, detection face (such as face ID for iPhone X), preservation of at-risk historical sites, and recreation of virtual environments for film and video game industries. Mainly, there are two basic kinds of hardware available for the 3D data generation in outdoor and indoor environments. For outdoors, one typical hardware is LiDAR( Light Detection and Ranging). The coverage of this equipment can achieve to hundreds and even thousands meters. Most selfdriving cars use a LiDAR scanner. For indoors, in recent years the availability of low-cost sensors such as the Microsoft Kinect have enabled the acquisition of short-range indoor 3D data at the consumer level. Meanwhile, smart phone such as iPhone X a equipped will a depth camera. In Figure 29, one example of the 3D data collected from the outdoor urban LiDAR scanner is shown. In Figure 28, depth map generated by a Kinect camera is provided. Capturing a 3D environment by a robot is shown in Figure 30. A RGB-D camera based object detection system is used to help a robot to grasp objects as shown in Figure 31. Velodyne's HDL-64E LiDAR sensor is commonly used for Self-driving cars. There is a new LiDAR sensor with 128 laser beams released from Velodyne in 2017. The photos of both the 64 beams sensor and the 128 beams sensors is given in Figure 32. The sensor's vertical scan range is shown in Figure 33 while the horizontal scan range is shown in Figure 34. Raw data collected by Velodyne's HDL-64E LiDAR sensor is shown in Figure 35.
## (s16) Novel view point models
(p16.0) RotationNet is an extension of MVCNN [60]. In this paper, multiple views from different angles are explored. Three models of camera views are proposed as shown in Figure 37. The performance of case(i) (the same view points model as MVCNN [60]) and case(ii) are compared. Case (ii) achieves a better performance based on the ModelNet40 task. For the ModelNet40, the case(iii) model is not used.
## (s18) PointNet [48]
(p18.0) Applications of PointNet PointNet is a new work which is performing 3D vision understanding directly on the raw cloud point data. Applications of PointNet are shown in Figure 40. PointNet consumes raw point clouds (set of points) without voxelization or rendering. It is a unified architecture that learns both global and local point features, providing a simple, efficient and effective approach for a number of 3D classification tasks. Figure 41: PointNet Architecture. The figure is from [48]. Here the "mlp" means multi-layer perceptron.
## (s23) Detection
(p23.0) Similar to 2D-image based object systems, most 3D systems are also using the two-stage methods to do the 3d object detection: first, generate proposals and then do detection. At the same time, the unique properties of the 3D systems, such as different data representation and the availability of both 2D and 3D images, make the 3D detection framework more complicated and more interesting. We will discuss the datasets used for detection and main works in indoor and outdoor scenarios next.     For 3D understanding, the detection output is more complicated than the 2D case. It contains the output of the class and the bounding box. The class output is similar to the 2D detection task. For the bounding box detection, it can output the image plane bounding box, bird's eye view(BEV) bounding box and 3D bounding box. Meanwhile, for the 2D object detection, the bounding boxes are axis aligned, however, the BEV and 3D bounding boxes are not axis aligned. Finally, the orientation of BEV and 3D bounding box will also be detected in some tasks. The detection outputs are summarized in Table 8. Examples of each output are shown in Figure 48. In this survey, we mainly focus on the 3D bounding box detection.  Most frameworks are based on two-stage methods where the proposals are firstly generated and then the detection will be done based on the proposals. F-PointNet [75] is a special case. It has three stages: first, get a frustum proposal from the detected 2D bounding based on the RGB image. The difference of the frustum proposal with the proposal from two-stage methods is it has a class label. second, 3D Instance segmentation is done based on the proposal. Finally, a 3D box is estimated based on the segmentation results. In order to represent a 3D bounding box, different methods are proposed, such as 8-corner method, 3D center offset method and 4-corner-2-height method as shown in Figure 49.  Different input data can be used to detect the 3D bounding box, such as the Monocular image, Stereo image and depth/LiDAR image. The detection system organized by the input data type is given in Table 10. Generally speaking, the 2D image only system including both the monocular and stereo image will perform worse than the 3D only or 2D+3D system. The comparison of the 2D image only and the 2D+3D system is provided in Figure 66.
## (s30) Deep Sliding Shapes [71]
(p30.0) The TSDF 3D Representation A directional Truncated Signed Distance Function(TSDF) is used to encode 3D shapes. The 3D space is divided into 3D voxel grid and the value in each voxel is defined to be the shortest distance between the voxel center and the surface from the input depth map. To encode the direction of the surface point, a directional TSDF stores a three-dimensional vector [dx, dy, dz] in each voxel in order to record the distance in three directions to the closest surface point instead of a single distance value [71]. The example of the directional TSDF for the RPN and detection network is shown in Figure 50.

(p30.1) The 3D RPN Figure 51: 3D Region Proposal Network: Taking a 3D volume from depth as input, a fully convolutional 3D network extracts 3D proposals at two scales with different receptive fields. Figure and Caption are from [71].

(p30.2) Deep Sliding Shapes [71] is inspired by the Faster RCNN [23] framework. The proposals are generated by a CNN based RPN. The RPN is shown in Figure 51. The network structure shown in Figure 51:
## (s35) FV features for MV3D
(p35.0) MV3D projects the FV into a cylinder plane to generate a dense front view map as in VeloFCN [84]. The front view map is encoded with threechannel features, which are height, distance and intensity as shown in Figure  64. Since KITTI uses a 64-beam Velodyne laser scanner, the size of map for the front view is 64 × 512.  [72] The performance of MV3D is evaluated based on the outdoor KITTI dataset. The performance of 3D object detection based on the test set can be found from the leaderboard. The performance of 3D object detection based on validation dataset is shown in Figures 65 and 66. It only provides the car detection results. Detection results for the pedestrians and cyclists are not provided.   The framework of AVOD is shown in Figure 68. AVOD is using the same encoding method as MV3D for the BEV. In AVOD, the value of M is set as 5 and the range of the LiDAR is [0, 70] × [−40, 40] × [0, 2.5] meters. So the size of the input feature for the BEV is 700 × 800 × 7. AVOD is using both the BEV and image to do the region proposals which is the main difference to the MV3D work.  VoxelNet architecture is shown in Figure 71. The feature learning network takes a raw point cloud as input, partitions the space into voxels, and transforms points within each voxel to a vector representation characterizing the shape information. The space is represented as a sparse 4D tensor. The convolutional middle layers processes the 4D tensor to aggregate spatial context. Finally, a RPN generates the 3D detection. A fixed number, T , of points from voxels containing more than T points are randomly sampled. For each point, a 7-feature is used which is (x, y, z, r, x− v x , y − v y , z − v z ) where x, y, z are the XY Z coordinates for each point. r is the received reflectance and (v x , v y , v z ) is the centroid of points in the voxel. Voxel Feature Encoding is proposed in VoxelNet. The 7-feature for each point is fed into the Voxel feature encoding layer as shown in Figure 72. Fully connected networks are used in the VFE network with element-wise MaxPooling for each point and concatenation between each point and the element-wise MaxPooling output. The input of the VFE is T × 7 and the output will be T × C where C depends on the FC layers of the VFE itself and depends on the whole VFE layers network used. Finally, an element-wise MaxPooling is used again and change the dimension of the output to 1 × C. Then for each voxel we have a one vector with C elements as shown in Figure 71. For the whole framework, we will have an input data with shape of C ×D ×H ×W .
## (s45) Data representation methods summary for 3D system
(p45.0) From the surveyed 3D systems, we can see the importance of the data representation to the performance. Here the pros and cons of different data representation methods to the classification and detection tasks is summarized:

(p45.1) • using Projected multiple view RGB images -Pros : It is a similar to the human being's recognization process for a 3D object by looking from different views. Since it can encode the multiple view info into 2D RGB image, it can take the advantage of well developed 2D image recognization system such as 2D CNN to further classify or detect 3D objects.

(p45.2) -Cons : Sometime, not all the desired multiple-view RGB images are available. In some papers which use multiple views such as RotationNet [48] and MVCNN [60], views from different angles are used to do the classification. This is possible when the whole object's CAD model is available. However, in the real application such as the autonomous cars scenario, from the self-driving cars' perspective, it can only take multiple-view of objects from one side during the driving process. The other side of the objects cannot be observed due to self-occultations. The partial availability of all views will reduce the performance of the algorithms based on all views.
