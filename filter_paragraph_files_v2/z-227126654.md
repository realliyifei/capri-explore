# Time Series Data Imputation: A Survey on Deep Learning Approaches

CorpusID: 227126654 - [https://www.semanticscholar.org/paper/c88ee84b150ce0d3be164fd4cbaeeda7d151e3b3](https://www.semanticscholar.org/paper/c88ee84b150ce0d3be164fd4cbaeeda7d151e3b3)

Fields: Computer Science

## (s5) GRU-D
(p5.0) GRU-D is proposed by [7] as one of the early attempts to impute time series with deep learning models. It is the first one among the 5 researched paper to systematically model missing patterns into RNN for time series classification problems. It is also the first research to exploit that, RNN can model multivariable time series with the informativeness from the time series. Former works like [23,8] attempted to impute missing values with RNN by concatenating timestamps and raw data, i.e., regard timestamps as one attribute of raw data. But in [7], the concept time lag is first proposed. In this paper, Gated Recurrent Unit (GRU) is first adopted to generate missing values. In each layer of GRU, since the input can contain missing values, they replace the input x j ti with a combination of 4   The main contribution of this paper is the GRU based model GRU-D and the proposition of decay rate. To address the imputation of the missing values, they discover that • The missing variables tend to be close to some default value if its last observation happens a long time ago.
## (s7) BRITS
(p7.0) Unlike former methods, BRITS [6] is totally based on RNN structure and proposes imputation with unidirectional dynamics. Time lag (corresponding to "time gaps" in [6]) is also employed since the time series may be irregular. Similar to the idea of decay rate γ from GRU-D introduced in Section 4.2, they propose temporal decay  factor γ t = exp (−max (0, W γ δ t + b γ )). Compared to GRU-D where the time lags are considered in input and serve as the decay rate, in BRITS the hidden states update with the decay rate γ. It means when updating the hidden state, the old hidden state decays according to the time duration recorded in the time lags. Hence, the model is updated by:
## (s12) Attention Mechanism Enhanced
(p12.0) In recent years, the attention mechanism has been shown successful in deep learning society, especially in NLP fields. When adopted in RNN, the attention mechanism allocates weights for each hidden state to draw information from the sequence. With such mechanism, the model is improved to capture latent patterns in historical data, thus may benefit time series imputation. Compared to existing RNN models (e.g., LSTM and GRU) which already take long-term dependencies into consideration, the attention mechanism for instance temporal attention enables the model to see features and status globally. However, LSTM and GRU will still lose long-term information due to the forget gate unit.

(p12.1) Recently, pure attention models are proposed without RNN. The Transformer proposed in [47] is one of the popular frameworks. In the proposed Transformer framework, it only adopts an attention layer called selfattention, which is computed as:

(p12.2) where Q, K, V are queries, keys and values respectively, and d k is the dimension of the input. Accepting a single sequence as input, the self-attention mechanism relates different positions of the input and tries to compute a representation of the sequence. Without applying RNN, the Transformer relies entirely on the selfattention layers to former an encoder-decoder structure, which is similar to the auto-encoder introduced in Section 4.1. Such a structure provides the ability to extract high-dimensional features for reconstructing, which benefits tasks like machine translation introduced in [47].

(p12.3) For improving the performance of data imputation, due to the effectiveness of the attention mechanisms, models based on attention mechanisms may also address the time series imputation problems. And two aforementioned categories of the attention mechanisms including temporal attention and self-attention are both potential techniques which may benefit the time series imputation. Moreover, with the idea of removing RNN and leveraging only attention mechanisms, structures like the Transformer may contribute to a new framework for the imputation tasks.

(p12.4) To summary, two categories of attention mechanisms including temporal attention and self-attention may bring future opportunities on time series imputation. And the pure attention frameworks are also new directions to model time series.
