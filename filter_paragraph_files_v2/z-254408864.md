# A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches

CorpusID: 254408864 - [https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8](https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8)

Fields: Computer Science, Linguistics

## (s0) INTRODUCTION
(p0.0) Machine reading comprehension (MRC) is one of the most important and long-standing topics in the Natural Language Machine reading comprehension (MRC) is one of the most important and long-standing topics in Natural Language Processing (NLP). MRC provides a way to evaluate an NLP system's capability for natural language understanding. An MRC task, in brief, refers to the ability of a computer to read and understand natural language context and then find the answer to questions about that context. The emergence of large-scale single-document MRC datasets, such as SQuAD [1], CNN/Daily mail [2], has led to increased attention to this topic and different models have been proposed to address the MRC problem, such as (Chen, Bolton, and Manning, 2016) Adriana Trigiani is an Italian American best-selling author of sixteen books, television writer, film director, and entrepreneur based in Greenwich Village, New York City. (2) Based on Trigiani's 2000 best-selling novel of the same name, the story is set in the actual Virginia town of Big Stone Gap circa 1970s. The first attempt to improve the simple single-hop MRC task happened with emerging of some datasets like TriviaQA [15] and NarrativeQA [16]. These datasets addressed more challenges by introducing multiple passages per each question and also presenting a more complex kind of questions that couldn't be answered with one single sentence. Although this kind of question was more complex than single-hop questions, they still could be answered by a few nearby sentences within one passage, which means they mostly do not need multi-hop reasoning. They are generally known as the multi-passage or multi-document dataset that
## (s11) Recurrent reasoning-based technique
(p11.0) The sequence models have been first used for single-hop MRC tasks, and most of them are based on Recurrent Neural Networks (RNNs), some studies focus on using them in multi-hop MRC. It can be called state-based reasoning models and they are closer to a standard attention-based RC model with an additional "state" representation that is iteratively updated. The changing state representation results in the model focusing on different parts of the passage during each iteration, allowing it to combine information from different parts of the passage [28]. Most models, presented in this section, have used advanced neural network concepts, such as attention mechanism and network memory for multi-hop reasoning. In the following the models which use this technique will be reviewed in detail including the architecture alongside the superiority and the motivation of them.
## (s13) Commonsense Algorithm consists of Commonsense Selection Representation to select useful relational knowledge paths and
(p13.0) Commonsense Model Incorporation to fill the gaps of reasoning between hops of inference using Necessary and Optional Information Cell (NOIC).  [29] QFE: Nishida et al. [30] proposed a model for explainable multi-hop QA named Query Focused Extractor (QFE) which is based on a summarization idea. They use the multi-task learning of the QA model for answer selection and QFE for evidence extraction.

(p13.1) QFE as the main part of this model adaptively determines the number of evidence sentences by considering the dependency among the evidence sentences and the coverage of the question. Unlike other approaches that extract each evidence sentence separately, QFE uses an RNN and attention mechanism to consider important information in the question and the relationships between sentences. This query-aware recurrent structure enables QFE to consider the dependency among the evidence sentences and cover the important information in the question sentence. In brief, the main goal of QFE is to summarize the context according to the question. Query-focused summarization is the task of summarizing the source document with regard to the given query. The multitask learning with QFE is general in the sense that it can be combined with any QA model. The overview of QFE is shown in Figure 11. is the current summarization vector, is the query vector considering the current summarization, is the extracted sentence, updates the RNN state. Figure 11: Overview of Query Focused Extractor at step t [30] TAP: Bhargav et al. [31] proposed a deep neural architecture, called TAP (Translucent Answer Prediction) cover of two main ideas: (1) Local Interaction: Each sentence should be understood in the context of its neighboring sentences and the question, (2) Global Interaction: A global (inter-passage) interaction among sentences must be identified and used for supporting facts. TAP is a hierarchical architecture that tries to capture the local and global interactions between the sentences and consists of two main parts: (Figure 12)

(p13.2) â€¢ Local and Global Interaction eXtractor (LoGIX) with three layers: local layer to obtain intra-passage dependencies, Global Layer to obtain inter-passage dependencies, and Supporting Facts Prediction Layer to calculate the probability that a sentence is a supporting fact.
## (s14) Path-based:
(p14.0) In the following, we review models that focus on finding the right path in information to find the answer. As multi-hop MRC faces more information and complex questions, finding the right path has become more important and difficult, so many models in this technique are path-based. One of the important advantages of path-based models is that they are interpretable because they can provide the evidence chain to the final answer.

(p14.1) EPAr: Jiang et al. [33] proposed an interpretable model named Explore-Propose-Assemble reader (EPAr) to mimic the coarseto-fine-grained reasoning behavior of humans when facing multiple long documents. The main idea is to construct a reasoning tree according to the documents like a hierarchical memory network and use the path in this tree to extract the final answer. This model has three components as shown in Figure 14:
## (s22) HDE:
(p22.0) Tu et al. [46] proposed a more complex graph named Heterogeneous Document-Entity (HDE) graph with different types of nodes and edges. This graph can cover different granularity levels of information in context and also enables rich information interaction among different types of nodes for accurate reasoning. The nodes in the HDE graph are candidates, documents, and entities. Besides, it has seven types of edges: 1) between a document node and a candidate node that appears in the same document.

(p22.1) 2) between a document node and its entity node. 3) between a candidate node and its entity node. 4) between two entity nodes from the same document. 5) between two entity nodes from different documents but they are mentions of the same candidate or query subject. 6) All candidate nodes connect with each other. 7) Entity nodes that do not meet previous conditions are connected as well. Figure 30 is an example of an HDE graph. In this figure, green nodes indicate documents, yellow nodes denote candidates, and blue nodes stand for entities. In addition, dash lines indicate type 1 edges, dash-dotted lines denote type 2 edges, square dot lines indicate type 3 edges, the red line denotes type 4 edge, the purple line indicates type 5 edge, and black lines indicate type 6 edges. The type 7 edge is not shown in this figure. As Figure 31 shows This model can be categorized into three parts: initializing HDE graph nodes with co-attention and self-attention-based context encoding, and reasoning over HDE graph with GNN-based message passing algorithms and score accumulation from updated HDE graph nodes representations.

(p22.2) However, [35] have shown that if the number of inferences increases, the complexity of models will rise sharply due to the iteration of cumbersome message passing algorithm, resulting in low efficiency.  [46] SAE: Tu et al. [47] proposed an interpretable system named Select, Answer, and Explain (SAE) with multi-hop reasoning graphs based on GNN with contextual sentence embeddings as nodes. This kind of sentence graph makes lead to an interpretable model because it can directly output supporting sentences with the answer prediction. The edges capture the global information presented within each document and also the cross-document reasoning path. Also, the contextual sentence embedding used in GNN is summarized over token representations based on a novel mixed attentive pooling mechanism. The attention weight is calculated from both answer span logits and self-attention output on token representations. This attention-based interaction enables the exploitation of complementary information between "answer" and "explain" tasks. The SAE system first filters unrelated documents, and selects gold documents using a document classifier trained with a novel pairwise learning-to-rank loss function.

(p22.3) The gold documents are then fed into a model to predict the answer and supporting sentences. The model is a multi-task learning process which means while the answer prediction is accomplished at the token level, the support sentence is predicted as a node classification task at the sentence level. As it is shown in Figure 32, the selection module consists of:
## (s28) Models Performance:
(p28.0) In this section we will show the performance of the models. This investigation is helpful in several ways; it will determine the stat-of-the-result, and also shows which models and techniques has achieved the best result. On the other hand, it can show the overall performance and effectiveness of each technique in multi-hop MRC To evaluate the results of the models we need to use the evaluation metrics of the datasets. HotpotQA [11] and Wikihop [55], are two populare datasets among the reviewed studies as it has been clear in Figure 46 in which shows the percentage of use of two datasets among the reviewed models from 2018 to 2022. Then they provide a proper situation for evaluating the model's performance. 
## (s29) HotpotQA
(p29.0) In this section, the performance of models on HotpotQA [11] will be investigated. Exact-match (EM) and F1 metrics have been used in this study to evaluate the model performance. EM is used to show whether the predicated answer and the ground-truth answer are exactly the same. F1 measures the average overlap between the predicted answer and the ground-truth answer. In addition, there are three sets of metrics: 1) Answer EM/F1 to evaluate the answer span prediction, 2) Support EM/F1 to evaluate the supporting facts prediction, and 3) Joint EM/F1 to combine the two previous ones.
## (s30) WikiHop
(p30.0) The papers that have used the WikiHop [55] dataset is investigated in this section. WikiHop consists of 51k questions, answers, and context where each context consists of several documents from Wikipedia .Each question in WikiHop is a tuple, which denotes two entities, and their relationship, then the answers in the WikiHop dataset are a single entity. Accuracy is a popular and fairly common metric to evaluate the performance of multiple-choice and Cloze-style MRC tasks. In the multiple-choice task, it is required to check whether the correct answer has been selected from the candidate answers. In contrast, in the Cloze-style task, it is required to check whether the correct words have been selected for the missing words

(p30.1) Since the answer type of this model is multiple-choice then accuracy is the evaluation metric on this dataset which is obtained for both the test and development set. For each paper, the year of publication, the technique along with the results are shown in Table 3. The best result is for ChainEX [38] which has used the recurrent reasoning technique. Besides that, the graph-based models could achieve the good result in this dataset too. 
