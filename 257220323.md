# Placental Vessel Segmentation and Registration in Fetoscopy: Literature Review and MICCAI FetReg2021 Challenge Findings

CorpusID: 257220323
 
tags: #Computer_Science, #Medicine, #Engineering

URL: [https://www.semanticscholar.org/paper/9774087e7ffba606d4d70f080fb29763e56bbd02](https://www.semanticscholar.org/paper/9774087e7ffba606d4d70f080fb29763e56bbd02)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Placental Vessel Segmentation and Registration in Fetoscopy: Literature Review and MICCAI FetReg2021 Challenge Findings
26 Feb 2023

Sophia Bano sophia.bano@ucl.ac.uk 
Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS)
Department of Computer Science
University College London
UK

Alessandro Casella 
Department of Advanced Robotics
Istituto Italiano di Tecnologia
Italy

Department of Electronics, Information and Bioengineering
Politecnico di Milano
Italy

Francisco Vasconcelos 
Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS)
Department of Computer Science
University College London
UK

Abdul Qayyum 
UMR CNRS 6285 LabSTICC
ENIB
29238France

Abdesslam Benzinou 
UMR CNRS 6285 LabSTICC
ENIB
29238France

Moona Mazher 
Department of Computer Engineering and Mathematics
University Rovira i Virgili
Spain

Fabrice Meriaudeau 
ImViA Laboratory
University of Bourgogne Franche-Comté
France

Chiara Lena 
Department of Electronics, Information and Bioengineering
Politecnico di Milano
Italy

Ilaria Anita Cintorrino 
Department of Electronics, Information and Bioengineering
Politecnico di Milano
Italy

Romana Gaia 
De Paolis 
Department of Electronics, Information and Bioengineering
Politecnico di Milano
Italy

Jessica Biagioli 
Department of Electronics, Information and Bioengineering
Politecnico di Milano
Italy

Daria Grechishnikova 
Physics Department
Lomonosov Moscow State University
Russia

Jing Jiao 
Fudan University
China

Bizhe Bai 
Department of Mathematical and Computational Sciences
Medical Computer Vision and Robotics Group (Group) Co., Ltd, p NepAL Applied Mathematics and Informatics Institute for Research, Nepal q Redev Technology
University of Toronto
Canada, UK

Yanyan Qiao 
Binod Bhattarai 
Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS)
Department of Computer Science
University College London
UK

Rebati Raman Gaire 
Ronast Subedi 
Eduard Vazquez 
Szymon Płotka 
Sano Center for Computational Medicine
Poland

Quantitative Healthcare Analysis Group, Informatics Institute
University of Amsterdam
AmsterdamThe Netherlands

Aneta Lisowska 
Sano Center for Computational Medicine
Poland

Arkadiusz Sitek 
Sano Center for Computational Medicine
Poland

Center for Advanced Medical Computing and Simulation
Massachusetts General Hospital
Harvard Medical School
BostonMAUnited States of America

George Attilakos 
Fetal Medicine Unit
Elizabeth Garrett Anderson Wing
University College London Hospital
UK

Faculty of Population Health Sciences
EGA Institute for Women's Health
University College London
UK

Ruwan Wimalasundera 
Fetal Medicine Unit
Elizabeth Garrett Anderson Wing
University College London Hospital
UK

Faculty of Population Health Sciences
EGA Institute for Women's Health
University College London
UK

Anna L David 
Fetal Medicine Unit
Elizabeth Garrett Anderson Wing
University College London Hospital
UK

Faculty of Population Health Sciences
EGA Institute for Women's Health
University College London
UK

Department of Development and Regeneration
University Hospital Leuven
Belgium

Dario Paladini 
Department of Fetal and Perinatal Medicine
Istituto "Giannina Gaslini"
Italy

Jan Deprest 
Faculty of Population Health Sciences
EGA Institute for Women's Health
University College London
UK

Department of Development and Regeneration
University Hospital Leuven
Belgium

Elena De Momi 
Department of Electronics, Information and Bioengineering
Politecnico di Milano
Italy

Leonardo S Mattos 
Department of Advanced Robotics
Istituto Italiano di Tecnologia
Italy

Sara Moccia 
The BioRobotics Institute and Department of Excellence in Robotics and AI
Scuola Superiore Sant'Anna
Italy

Danail Stoyanov 
Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS)
Department of Computer Science
University College London
UK

O Shanghai 
Microport Medbot 
Placental Vessel Segmentation and Registration in Fetoscopy: Literature Review and MICCAI FetReg2021 Challenge Findings
26 Feb 2023Preprint submitted to Elsevier February 28, 2023FetoscopyPlacental scene segmentationVideo mosaickingSurgical data science
Fetoscopy laser photocoagulation is a widely adopted procedure for treating Twin-to-Twin Transfusion Syndrome (TTTS). The procedure involves photocoagulation pathological anastomoses to restore a physiological blood exchange among twins. The procedure is particularly challenging, from the surgeon's side, due to the limited field of view, poor manoeuvrability of the fetoscope, poor visibility due to amniotic fluid turbidity, and variability in illumination. These challenges may lead to increased surgery time and incomplete ablation of pathological anastomoses, resulting in persistent TTTS. Computer-assisted intervention (CAI) can provide TTTS surgeons with decision support and context awareness by identifying key structures in the scene and expanding the fetoscopic field of view through video * Corresponding author. * * Equal contribution.mosaicking. Research in this domain has been hampered by the lack of high-quality data to design, develop and test CAI algorithms. Through the Fetoscopic Placental Vessel Segmentation and Registration (FetReg2021) challenge, which was organized as part of the MICCAI2021 Endoscopic Vision (EndoVis) challenge, we released the first largescale multi-center TTTS dataset for the development of generalized and robust semantic segmentation and video mosaicking algorithms with a focus on creating drift-free mosaics from long duration fetoscopy videos. For this challenge, we released a dataset of 2060 images, pixel-annotated for vessels, tool, fetus and background classes, from 18 in-vivo TTTS fetoscopy procedures and 18 short video clips of an average length of 411 frames for developing placental scene segmentation and frame registration for mosaicking techniques. Seven teams participated in this challenge and their model performance was assessed on an unseen test dataset of 658 pixel-annotated images from 6 fetoscopic procedures and 6 short clips. The challenge provided an opportunity for creating generalized solutions for fetoscopic scene understanding and mosaicking. In this paper, we present the findings of the FetReg2021 challenge alongside reporting a detailed literature review for CAI in TTTS fetoscopy. Through this challenge, its analysis and the release of multi-center fetoscopic data, we provide a benchmark for future research in this field.

## Introduction

Twin-to-Twin Transfusion Syndrome (TTTS) is a severe complication of monochorionic twin pregnancies. TTTS is characterized by an unbalanced and chronic blood transfer from one twin (the donor twin) to the other (the recipient twin) through placental anastomoses (Baschat et al., 2011). This shared circulation is responsible for serious complications, which may lead to profound fetal hemodynamic and cardiovascular disturbances (Lewi et al., 2013). In 2004, a randomized, controlled trial demonstrated that fetoscopic laser ablation of placental anastomoses in TTTS had a higher survival rate for at least one twin than other treatments, such as serial amnioreduction. Laser ablation further showed a lower incidence of complications, such as cystic periventricular leukomalacia and neurologic complications (Senat et al., 2004). The trial included pregnancy at 16-26 weeks' gestation. Such results were confirmed for pregnancy before 17 and after 26 weeks' gestation (Baud et al., 2013). A description of all the steps that brought laser surgery for coagulation of placental anastomoses to be the elective treatment for TTTS can be found in (Deprest et al., 2010).

Fetoscopic laser photocoagulation involves the ultrasound-guided insertion of a fetoscope into the amniotic sac. Through fetoscopic camera, the surgeon identifies abnormal anastomoses and laser ablate them to regulate the blood flow between the two fetuses (as illustrated in Fig. 1(a)). First attempts at laser coagulation included laser ablating all vessels that looked like anastomoses (a non-reproducible and operator-dependent technique), and laser ablating all vessels crossing the inter-fetus membrane (an approach that relies on the assumption that all vessels crossing the dividing membrane are pathological anastomoses) (Quintero et al., 2007). Today, the recognized elective treatment is the selective laser photocoagulation, which consists of the precise identification and lasering of placental pathological anastomoses. The selective treatment relies on the identification of the anastomoses (shown in Fig. 1(b)) and their classification into Arterio-Venous (from donor to recipient, AVDR, or from the recipient to donor, AVRD), Arterio-Arterial (AA) or Veno-Venous (VV) anastomoses. The identified AVDR anastomoses are laser ablated to regulate the blood flow between the two fetuses.

Despite all the advancements in instrumentation and imaging for TTTS (Cincotta and Kumar, 2016;Maselli and Badillo, 2016), residual anastomoses after monochorionic placentas treated with fetoscopic laser surgery still represent an issue (Lopriore et al., 2007). This may be explained considering challenges from the surgeon's side, such as limited field of view (FoV), poor visibility and high inter-subject variability. In this complex scenario, computer-assisted-intervention (CAI) and surgical-datascience (SDS) methodologies may be exploited to provide surgeons with context awareness and decision support. However, the research in this field is still in its infancy, and several challenges still have to be tackled (Pratt et al., 2015). These include dynamically changing views with In the placenta, conversely from body circulatory system, arteries carries deoxygenated blood (in blue), and veins carries oxygenated blood (in red).

poor texture visibility, low image resolution, non-planar view, especially in the case of the anterior placenta, occlusions due to the fetus and surgical tools, fluid turbidity and specular highlights.

In the context of TTTS fetoscopy, approaches to anatomical landmark segmentation (inter-fetus membrane, vessel) (Casella et al., 2020Sadda et al., 2019;Bano et al., 2020a), event detection (Vasconcelos et al., 2018;Bano et al., 2020c) and mosaicking (Gaisser et al., 2018;Tella-Amo et al., 2019;Peter et al., 2018;Bano et al., 2020a,b) exist (Sec. 2). Even though fetoscopic videos have large inter-and intra-procedure variability, the majority of the segmentation and event detection approaches are validated on a small subset of invivo TTTS videos. Existing mosaicking approaches are validated only on a small subset of ex-vivo , in-vivo Bano et al., 2020a) or underwater phantom sequences (Gaisser et al., 2018). Intensity-based image registration (Bano et al., 2020a;Li et al., 2021) methods relies on placental vessel segmentation maps for registration which facilitated in overcoming some visibility challenges (e.g., floating particles, poor illumination), however, such method fails when the predicted segmentation map is inaccurate, or the vessels are inconsistent across frames or are absent from the view. Deep learning-based flow-field matching for mosaicking (Alabi et al., 2022) has also been proposed, which results in accurate registration even in regions with poor or weak vessels but such an approach fails when the fetoscopic scene is homogenous having poor texture.

In fetoscopy, a major effort is needed to collect large, high-quality, multi-center datasets that can capture the variability of fetoscopic video. This reflects a well-known problem in the medical-image analysis community (Litjens et al., 2017) that is currently addressed by organizing international initiatives such as Grand Challenge 1 .


### Our Contributions

Placental Vessel Segmentation and Registration for Mosaicking (FetReg2021) 2 challenge is a crowdsourcing initiative to address key problems in fetoscopy towards developing CAI techniques for providing TTTS with decision support and context awareness. With FetReg2021, we collected a large multi-center dataset to better capture not only inter-and intra-procedure variability but also inter-domain (data captured in two different clinical sites) variability. The FetReg2021 dataset can support developing robust and generalised models, paving the way for the translation of deep-learning methodologies in the actual surgical practice. The dataset is available to the research community 3 , under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International license (CC BY-NC-SA 4.0), to foster research in the field. FetReg2021 was organized as part of the MIC-CAI 2021 Endoscopic Vision (EndoVis) 4 challenge, and aimed at solving two tasks: placental scene segmentation and frame registration for mosaicking.

In this paper, we present the results and findings of the FetReg2021 challenge, in which 7 teams participated. We further provide a detailed review of the relevant literature on CAI for fetoscopy. To conclude, we benchmark Fe-tReg2021 participants methods against the existing stateof-the-art in fetoscopic scene segmentation and mosaicking method.


## Related work

This section surveys the most relevant CAI methods developed in the field of TTTS surgery. This includes anatomical structure segmentation (Sec. 2.1), mosaicking and navigation (Sec. 2.2), and surgical event recognition (Sec. 2.3).


### Anatomical structure segmentation

Image segmentation is one of the most explored task in medical image analysis. Segmentation from intraoperative images aims at supporting surgeons' by enhancing the visibility of relevant structures (e.g., blood vessels) but presents additional challenges over anatomical image analysis due to poor texture and uncertain contours. Segmentation algorithms for TTTS partition mainly focuses on vessel (Sec. 2.1.1) and placenta (Sec. 2.1.2) segmentation, as reference anatomical structures to provide surgeons with context awareness.


#### Placental vessel segmentation

Since the abnormal distribution of the anastomoses on the placenta is responsible for TTTS, exploration of its vascular network is crucial during the photocoagulation procedure. The work presented by Almoussa et al. (2011) is among the first in the field. The work, developed and tested with ex-vivo images, combined Hessian-based filtering and a custom neural network trained on handcrafted features. The approach was improved by Chang et al. (2013), which introduced a vessel enhancement filter that combined multi-scale and curvilinear filter matching. The multi-scale filter extends the Hessian filter, introducing two scaling parameters to tune vesselness sensitivity. The curvilinear filter matching refined vessel segmentation, preserving all the structures that fit in the vessel shape template defined by a curvilinear function. The main limitation of both methods (Almoussa et al., 2011;Chang et al., 2013) lies in the analysis of ex-vivo images, which present different characteristics than in-vivo ones. More importantly, Hessian-based methods have been proven to perform poorly in the case of tortuous and irregular vessels (Moccia et al., 2018).

More recently, researchers have focused their attention on Convolutional Neural Networks (CNNs) to tackle the variability of intra-operative TTTS frames. Sadda et al. (2019) used U-Net, achieving segmentation performance in terms of Dice Similarity Coefficient (DSC) on a dataset of 345 in-vivo fetoscopic frames of 0.55 ± 0.22. U-Net is further explored by Bano et al. (2020a), which used segmented vessels as a prior for fetoscopic mosaicking (Sec. 2.2.3). The authors tested several versions of U-Net, including the original version by Ronneberger et al. (2015), and U-Net with different backbones (i.e. VGG16, ResNet50 and ResNet101). The segmentation performance was evaluated on a dataset of 483 in-vivo images from six TTTS surgery, the first publicly available 6 .

Despite the advances introduced by CNNs, the stateof-the-art methods cannot tackle the high variability of intraoperative images. From one side, encoder-decoder architectures trained to minimize cross-entropy and DSC loss fail in segmenting poor contrasted vessels and vessels with uneven margins. Furthermore, the datasets used to train these algorithms are small and the challenges of intra-operative images, as listed in Sec. 1, are not always represented.

Research in this field is strongly limited by the low availability of comprehensive expert-annotated datasets collected in different surgical settings that could encode such variability. This is mainly due to the low incidence of TTTS, which make systematic data collection difficult, and the lack of annotators with sufficient domain expertise 6 Fetoscopy placenta dataset: https://www. ucl.ac.uk/interventional-surgical-sciences/ fetoscopy-placenta-data to ensure clinically correct groundtruth.


#### Inter-fetus membrane segmentation

At the beginning of the surgical treatment, due to the very limited FoV and poor image quality, the surgeon finds a reference for orientation within the amniotic cavity. The structure identified for this purpose is the interfetus membrane. The visibility of this membrane can be very variable, depending on the chorion characteristics, in addition to the challenges described so far in fetoscopic images. Once located, the surgeon refers to the inter-fetus membrane as a navigation reference during placental vascular network exploration.

Automatic inter-fetus membrane segmentation has been introduced by Casella et al. (2020) where an adversarial segmentation network based on ResNet was proposed to enforce placenta-shape constraining. The method was tested on a dataset of 900 intraoperative frames from 6 TTTS patients with an average DSC of 91.91%. Despite the promising results, this method suffered when illumination was too high or low, so the membrane is barely visible in such conditions.

The work by Casella et al. (2020) was extended  by exploiting dense connectivity and spatio-temporal information to improve membrane segmentation accuracy and tackle high illumination variability. The segmentation performance outperformed the method previously proposed when tested on the first publicly available dataset 7 of 2000 in-vivo images from 20 TTTS surgeries. Despite the promising results achieved in the literature, the task of inter-fetus membrane segmentation remains poorly explored, and requires further research for performance improvement and generalization.


### Fetoscopic Mosaicking and Navigation

Video mosaicking aims at generating an expanded FoV image of the scene by registering and stitching overlapping video frames. Video mosaicking of high-resolution images has been extensively used as navigation guidance in the context of aerial, underwater, and street view imaging and also in consumer photography to build panorama shots. However, the outputs from off-the-shelf mosaicking methods have significantly poorer quality or fail completely when applied to fetoscopy videos due to the added visibility challenges of intra-operative images. Nevertheless, fetoscopy video mosaicking remains an active research topic within the context of computer-assisted intervention. Such a technique can facilitate the surgeon during the procedure in better localization of the anastomotic sites, which can improve the procedural outcomes.

Mosaicking for FoV expansion in fetoscopy has been explored using handcrafted feature-based and hybrid methods (Sec. 2.2.1), intensity-based (Sec. 2.2.2), and deep learning-based (Sec. 2.2.3) methods. These methods are either devised for synthetic placental images, ex-vivo placental images/videos or in-vivo videos.


#### Handcrafted feature-based and hybrid methods

Feature-based methods involve detecting and matching features across adjacent or overlapping frames, followed by estimating the transformation between the image pairs. On the other hand, hybrid methods utilize multimodal data (combination of image and electromagnetic tracking data) or a combination of feature-based and intensitybased methods.

Early approaches focused on accomplishing fetoscopic mosaicking from videos or overlapping a pair of images only for image registration and mosaicking. Reeff et al. (2006) proposed a hybrid method that used classical feature detection and matching approach for first estimating the transformation of each image with respect to a reference frame, followed by global optimization by minimizing the sum of the squared differences of pixel intensities between two images. Multi-band blending was applied for seamless stitching. For testing the hybrid method, the authors recorded one ex-vivo placenta fixed in a hemispherical receptacle submerged in water to mimic an invivo imaging scenario. Such an experiment also allowed capturing camera calibration to remove lens distortion. A short sequence of 40 frames sampled at 3 frames per second was used for the evaluation. The matched feature correspondences were visually analyzed to mark them as correct or incorrect, which is a labor-intensive task. The generated mosaic with and without global optimization was shown for qualitative comparison.

Handcrafted feature-based methods, similar to what is commonly used in high-resolution image stitching in computer vision, were also explored for fetoscopic mosaicking. Daga et al. (2016) presented the first approach toward generating real-time mosaics. The approach considered using SIFT for feature detection and matching. For real-time computation, texture memory was used on GPU for computing extremes of the difference of Gaussian (DoG) that describes SIFT features. Planar images of ex-vivo phantom placenta recorded by mounting a fetoscope to a KUKA robotic arm were used for validating the approach. The robot was programmed to follow a spiral path that facilitated qualitative evaluation. Yang et al. (2016) proposed a SURF feature detection and matching based approach for generating mosaics from 100 frames long sequences that captured ex-vivo phantom and monkey placentas. Additionally, pair of images correspondence failure approach was proposed based on the statistical attributes of the feature distribution and an adaptive updating mechanism for parameter tuning to recover registration failures. Gaisser et al. (2017) used different keypoint descriptors (SIFT, SURF, ORB) along with Least Median of Squares (LMedS) for estimating the transformation between overlapping pairs of images.

Through experiments on both ex-vivo and in-water phantom sequences, the authors showed that handcrafted features returns either no features or low confidence features due to texture paucity and dynamically changing visual conditions. This leads to inaccurate or poor transformation estimation. Sadda et al. (2018) proposed a feature-based method that relied on extracting AGAST corner detector (Mair et al., 2010), SIFT as descriptor and grid-based motion statistics (GMS) (Bian et al., 2017) for refining feature matching for homography estimation. The validation was performed on 22 in-vivo fetoscopic image pairs. Additionally, in a hybrid approach by Sadda et al. (2019), vessel segmentation masks were also used for selecting AGAST features only around the vessel regions. However, the reported error was large mainly because of linear and single vessels in the 22 image pairs under analysis. Using handcrafted feature descriptors such as SIFT shows poor performance in the case of in-vivo placental videos due to the added challenges introduced by poor visibility, texture paucity and low resolution imaging.

A few approaches used an additional electromagnetic tracker in an ex-vivo setting to design a feature-based method for improved mosaicking. Tella et al. (2016) and Tella-Amo et al. (2018) assumed the placenta to be planar and static and used a combination of visual and electromagnetic tracker information for generating robust and drift-free mosaics. Mosaicking performance was increased by Tella-Amo et al. (2019), where the pruning of overlapping frames and generation of a super frame for reducing computational time was proposed. An Aurora electromagnetic tracker (EMT) was mounted on the tip of a laparoscope to obtain camera pose measurements. Using this setup, a data sequence of 701 frames was captured from a phantom (i.e., a printed image of a placenta). Additionally, a synthetic sequence of 273 frames following only planar motion was also generated for quantitative evaluation. The camera pose measurements from the EMT were incorporated with frame-based visual information using a probabilistic model to obtain globally consistent sequential mosaics. It is worth mentioning that laparoscopic cameras used are considerably better than fetoscopic cameras. However, current clinical regulations and the limited form factor of the fetoscope hinder the use of such a tracker in intraoperative settings.


#### Intensity-based methods

Intensity-based image registration is an iterative process that uses raw pixel values for direct registration through first selecting features, such as edges, contours, followed by a metric, such as mutual information, crosscorrelation, the sum of squared difference, absolute difference, for describing how similar two overlapping input images are and an optimizer for obtaining the best alignment through fitting a spatial transformation model.

The use of direct pixel-wise alignment of oriented image gradients for creating a mosaic was proposed by Peter et al. (2018) that was validated on only one in-vivo fetoscopic sequence of 600 frames. An offline bag of words was used to improve the global consistency of the generated mosaic. Bano et al. (2020a) proposed a placental vessel-based direct registration approach. A U-Net model was trained on a dataset of 483 vessel annotated images from 6 invivo fetoscopy for segmenting vessels. The vessel maps from consecutive frames were registered, estimating the affine transformation between the frames. Testing was performed on 6 additional in-vivo fetoscopy video clips. The approach facilitated overcoming visibility challenges, such as floating particles and varying illumination. How-ever, the method failed when the predicted segmentation map is inaccurate or in views with thin or no vessels. Li et al. (2021) further extended this approach to propose a graph-based globally optimal image mosaicking method. The method detected loop closures with a bad-of-words scheme followed by direct image registration. Only 3 out of 6 in-invivo videos had loop closures present in them. Global refinement in alignment is then performed through G2O framwork (Kümmerle et al., 2011).


#### Deep learning-based methods

Existing deep learning-based methods for fetoscopic mosaicking mainly focused on training a CNN network (Bano et al., 2019(Bano et al., , 2020b for directly estimating homography between adjacent frames, extracting stable regions (Gaisser et al., 2016) in a view, or relying on flow fields (Alabi et al., 2022) for robust pair-wise images registration.

A deep learning-based feature extractor was proposed by Gaisser et al. (2016) that used similarity learning using contrastive loss when training a Siamese convolutional neural network (CNN) architecture between pairs of similar and dissimilar small patches extracted from ex-vivo placental images. The learned feature extractor was used for extracting features from pairs of overlapping images, followed by using LMedS for the transformation estimation. Due to motion blur and texture paucity that affected the feature extractor performance, the method was validated only on a short sequence (26 frames) that captured an ex-vivo phantom placenta. Gaisser et al. (2018) extended their similarity learning approach (Gaisser et al., 2016) for detecting stable regions on the vessels of the placenta. These stable regions' representation is used as features for placental image registration in an inwater phantom setting. The obtained homography estimation did not result in highly accurate registration, as the learned regions were not robust to visual variability in underwater placental scenes.

Methods for estimating 4-point homography using direct registration with deep learning exist in computer vision literature (DeTone et al., 2016;Nguyen et al., 2018). Bano et al. (2019, 2020b) extended (DeTone et al., 2016 to propose one of the first homography-based methods for fetoscopic mosaicking, which was tested on 5 diverse placental sequences, namely, synthetic sequence of 811 frames, ex-vivo placenta planar sequence of 404 frames, ex-vivo phantom placenta sequence of 681 frames, invivo phantom placenta sequence of 350 frames and invivo TTTS fetoscopic video of 150 frames. In (Bano et al., 2019(Bano et al., , 2020b, a VGG-like model was trained to estimate 4-point homography between two patches extracted from the same image with known transformation. Controlled data augmentation was applied to the two patches for network training. Filtering is then applied during testing to obtain the most consistent homography estimation. The proposed approach led to advancing the literature on fetoscopic mosaicking, although the network mainly focused on estimating rigid transformation (rotation and translation) between adjacent frames due to controlled data augmentation. As a result, the generated mosaics in non-planar sequences accumulated drift over time.

More recently, deep learning-based optical flow combined with inconsistent motion filtering for robust fetoscopy mosaicking has been proposed (Alabi et al., 2022). Their method relied on FlowNet-v2 (Ilg et al., 2017) for obtaining dense correspondence between adjacent frames, robust estimation using RANSAC and local refinement for removing the effect of floating particles and specularities for improved registration. Unlike Bano et al. (2020a) which used placental vessel prediction to drive mosaicking, Alabi et al. (2022) did not rely on vessels, as a result, it managed to generate robust and consistent mosaic for longer duration of fetoscopic videos. Their approach was tested on the extended fetoscopy placenta dataset from (Bano et al., 2020a).

Recent computer vision literate has also introduced deep learning-based interest point descriptors (DeTone et al., 2018;Sarlin et al., 2020) and detector-free dense feature matching (Sun et al., 2021) techniques. These techniques have shown robustness in multiview feature matching. Inspired from DeTone et al. (2018), Casella et al. (2022) proposed a learning-based keypoint proposal network and an encoding strategy for filtering irrelevant keypoints based on fetoscopic image segmentation and inconsistent homographies for producing robust and driftfree fetoscopic mosaics. Bano et al. (2022) proposed a placental vessel-guided hybrid framework for mosaicking that relies of best of (Bano et al., 2020a) and (Sun et al., 2021). The framework combines these two methods through a selection mechanism based on appearance consistency of placental vessels and photometric error minimization for choosing the best homography estimation between consecutive frames. Casella et al. (2022) and Bano et al. (2022) methods have been validated using the extended fetoscopy placenta dataset from (Bano et al., 2020a).

While these approach significantly improved fetoscopic mosaicking, further analysis is needed to investigate its performance in low-textured and highly nonplanar placental regions.


### Surgical event recognition

TTTS laser therapy has a relatively simple workflow with an initial inspection of the vasculature and placenta surface to identify and visualize photocoagulation targets. Fetoscopic laser therapy is conducted by photocoagulation of each identified target in sequence. Automatic identification of these surgical phases and surgical events is an essential step towards general scene understanding and tracking of the photocoagulation targets. This identification can provide temporal context for tasks such as segmentation and mosaicking. It could also provide prior to finding the most reliable images for registration (before ablation) or identify changes in the appearance of the scene (after ablation).

The CAI literature has hardly explored event detection or workflow analysis methods. Vasconcelos et al. (2018) used a ResNet encoder to detect ablation in TTTS procedures, additionally also indicating when the surgeon is ready for ablating the target vessel. The method was validated on 5 in-vivo fetoscopic videos. Bano et al. (2020c) combined CNNs and recurrent networks for the spatiotemporal identification of fetoscopic events, including clear view, occlusion (i.e., fetus or working channel port in the FoV), laser tool presence, and ablating laser tool present. The method was effective in identifying clear view segments (Bano et al., 2020c) suitable for mosaicking and was validated on 7 in-vivo fetoscopic videos. Due to inter-and intra-case variability present in fetosopic videos, evaluation on a larger dataset is needed to validate the generalization capabilities of the current surgical event recognition methods.


## The FetReg Challenge: Dataset, Submission, Evaluation

In this section, we present the dataset of the EndoVis FetReg 2021 challenge and its tasks (Sec. 3.1), the evalu-  ID (I -UCLH, II -IGG), image resolution, the number of annotated frames (for the segmentation task), the occurrence of each class per frame and the average number of pixels per class per frame are presented. For the registration task, the number of unlabeled frames in each video clip is provided. Key: BG -background. ation protocol designed to assess the performance of the participating methods (Sec. 3.2) and an overview of the challenge setup and submission protocol(Sec. 3.3).


### Dataset and Challenge Tasks

The EndoVis FetReg 2021 challenge aims at advancing the current state-of-the-art in placental vessel segmentation and mosaicking (Bano et al., 2020a) by providing a benchmark multi-center large-scale dataset that captured variability across different patients and different clinical institutions. We also aimed to perform out-of-sample testing to validate the generalization capabilities of trained models. The participants were required to complete two sub-tasks which are critical in fetoscopy, namely:

• Task 1: Placental semantic segmentation: The participants were required to segment four classes, namely, background, vessels, tool (ablation instrument, i.e. the tip of the laser probe) and fetus, on the provided dataset. Fetoscopic frames from 24 TTTS procedures collected in two different centers were annotated for the four classes that commonly occur during the procedure. This task was evaluated on unseen test data (6 videos) independent of the training data (18 videos). The segmentation task aimed to assess the generalization capability of segmentation models on unseen fetoscopic video frames.

• Task 2: Registration for Mosaicking: The par-ticipants were required to perform the registration of consecutive frames to create an expanded FoV image of the fetoscopic environment. Fetoscopic video clips from 18 multi-center fetoscopic procedures were provided as the training data. No registration annotations were provided as it is not possible to get the groundtruth registration during the in-vivo clinical fetoscopy. The task was evaluated on 6 unseen video clips extracted from fetoscopic procedure videos, which were not part of the training data. The registration task aimed to assess the robustness and performance of registration methods for creating a drift-free mosaic from unseen data.

The EndoVis FetReg 2021 dataset is unique as it is the first large-scale fetoscopic video dataset of 24 different TTTS fetoscopic procedures. The videos contained in this dataset are collected from two fetal surgery centers across Europe, namely,

• Center I: Fetal Medicine Unit, University College London Hospital (UCLH), London, UK,

• Center II: Department of Fetal and Perinatal Medicine, Istituto "Giannina Gaslini" (IGG), Genoa, Italy, Both centers contributed with 12 TTTS fetoscopic laser photocoagulation videos each. A total of 9 videos from each center (18 videos in total) form the training set, while 3 videos from each center (6 videos in total) form the test set. Alongside capturing the intra-case and inter-case variability, the multi-center data collection allowed capturing the variability that arises due to different clinical settings and imaging equipment at different clinical sites. At UCLH, the data collection was carried out as part of the GIFT-Surg 8 project. The requirement for formal ethical approval was waived, as the data were fully anonymized in the corresponding clinical centers before being transferred to the organizers of the EndoVis FetReg 2021 challenge. Table 2 summarizes EndoVis FetReg 2021 dataset characteristics and also indicates the center from which it is acquired. Videos from the two centers varied in terms 8 GIFT-Surg project: https://www.gift-surg.ac.uk/ of the resolution, imaging device and light source. The videos from UCLH are of higher resolution (minimum resolution: 470 × 470, maximum resolution: 720 × 720) with majority videos having 720p resolution compared to IGG (minimum resolution: 320 × 320, maximum resolution: 622 × 622) videos with majority having 400p or lower resolution. From Fig. 4 and Fig. 5, we can observe that most of the IGG center videos have a dominant red spotlight light visible with most views appearing to be very close to the placental surface. On the other hand, no domain light reflection is visible in any of the UCLH center videos and the imaging device captured relatively wider view compared to the IGG videos. Additionally, the frame appearance and quality changes in each video due to the large variation in intra-operative environment among different cases. Amniotic fluid turbidity resulting in poor visibility, artefacts introduced due to spotlight light source, low resolution, texture paucity, non-planar views due to anterior placenta imaging, are some of the major factors that contribute to increase the variability in the data from both centers. Large intra-case variations can also be observed from Fig. 4 and Fig. 5. All these factors contribute toward limiting the performance of the existing placental image segmentation and registration methods (Bano et al., 2020a(Bano et al., , 2019(Bano et al., , 2020b. The EndoVis Fe-tReg 2021 challenge provided an opportunity to make advancements in the current literature by designing and contributing novel segmentation and registration methods that are robust even in the presence of the above-mentioned challenges. Further details about the segmentation and registration datasets are provided in following sections.


#### Dataset for placental semantic segmentation

Fetoscopy videos acquired from the two different fetal medicine centers were first decomposed into frames, and excess black background was cropped to obtain squared images capturing mainly the fetoscope FoV. From each video, a subset of non-overlapping informative frames (in the range 100-150) is selected and manually annotated. All pixels in each image are labelled with background (class 0), placental vessel (1), ablation tool (2) or fetus class (3). Labels are mutually exclusive.

Annotation of 7 out of 24 videos was performed by four academic researchers and staff members with a solid background in fetoscopic imaging. Additionally, annotation services are obtained from Humans in the Loop (HITL) 9 for a subset of videos (17 out of 24 videos), who provided annotators with clinical background. Each image was annotated once following a defined annotation protocol. All annotations were then verified by two academic researchers for their correctness and consistency. Finally, two fetal medicine specialists verified all the annotations to confirm the correctness and consistency of 9 Humans in the Loop: https://humansintheloop.org/ the labels. The publicly available Supervisely 10 platform was used for annotating the dataset.

The FetReg train and test dataset for the segmentation task contains 2060 and 658 annotated images from 18 and 6 different in-vivo TTTS fetoscopic procedures, respectively. Figure 2(a) and Fig. 2(b) show the overall class occurrence per frame and class occurrence in average pixels per frame on the training dataset. The same for test dataset is shown in Figure. 3(a) and Fig. 3(b). Note that the frames present different resolutions as the fetoscopic videos are captured at different centers with different facilities (e.g., device, light scope). The dataset is highly unbalanced: Vessel is the most frequent class while Tool and Fetus are presented only in a small subset of images corresponding to 28% and 14%, respectively of the training dataset and 48% and 13% of the test dataset. When observing the class occurrence in average pixels per image, the Background class is the most dominant, with Vessel, Tool and Fetus occur 10%, 0.13% and 0.16% in train dataset and 11%, 0.22%, and 0.20% in test dataset, respectively. Figure 4 shows some representative annotated frames from each video. Note that the frame appearance and quality change in each video due to the large variation in the intra-operative environment among different cases. Amniotic fluid turbidity resulting in poor visibility, artifacts introduced due to spotlight light source and reddish reflection introduced by the laser tool, low resolution, texture paucity, and non-planar views due to anterior placenta imaging are some of the major factors that contribute to increase the variability in the data. Large intracase variations can also be observed from these representative images. All these factors contribute toward limiting the performance of the existing placental image segmentation and registration methods (Bano et al., 2020a(Bano et al., , 2019(Bano et al., , 2020b. The EndoVis FetReg 2021 challenge provided an opportunity to make advancements in the current literature by designing and contributing novel segmentation and registration methods that are robust even in the presence of the above-mentioned challenges. 


## Video011-II


#### Dataset for registration for mosaicking

A typical TTTS fetoscopy surgery takes approximately 30 minutes. Only a sub-set of fetoscopic frames is suitable for frame registration and mosaicking because fetuses, laser ablation fibre, and working channel port can occlude the field-of-view of the fetoscope. Mosaicking is mainly required in occlusion-free video segments that capture the surface of the placenta (Bano et al., 2020c) as these are the segments in which the surgeon is exploring the intraoperative environment to identify abnormal vascular connections. Expanding the FoV through mosaicking in these video segments can facilitate the procedure by providing better visualization of the environment.

For the registration for the mosaicking task, we have provided one video clip per video for all 18 procedures in the training dataset. Likewise, one clip per video from all 6 procedures in the test dataset is selected for testing and validation. These frames are neither annotated with segmentation labels nor have registration groundtruth. The number of frames in each video clip is reported in Table 2 for training and test dataset. Representative frames from each clip are shown in 5.

Representative frames every 2 seconds from some video clips are shown in Fig. 5. Observe the variability in the appearance, lighting conditions and image quality in all video clips. Even though there is no noticeable deformation in fetoscopic videos, which is usually thought to occur due to breathing motion, the views can be non-  I  I  I   I   II  II  II   I  I   II   I  II   II   I   II   I  II  II II II I I II I Figure 5: Representative frames from training and test datasets at every 2 seconds. These clips are unannotated and the length of each clip mentioned in Table 2. Center ID is also marked on each video sequence (I -UCLH, II -IGG) for visual comparison of the data from the two different centers. planar as the placenta can be anterior or posterior. Moreover, there is no groundtruth camera motion and scene geometry that can be used to evaluate video registration approaches for in-vivo fetoscopy. In Section 3.2.2, we detail how this challenge is addressed with an evaluation metric that is correlated with good quality, consistent, and complete mosaics (Bano et al., 2020a).


### Evaluation protocol 3.2.1. Segmentation Evaluation

Intersection over union (IoU) is another most commonly used metric for evaluating segmentation algorithms which measure the spatial overlap between the predicted and groundtruth segmentation masks as:
IoU = TP TP + FP + FN(1)
where TP are the correctly classified pixels belonging to a class, FP are the pixels incorrectly predicted in a specific class, and FN are the pixels in a class incorrectly classified as not belonging to it. For evaluating the performance of segmentation models (Task 1), we compute for each frame provided in the test set the mean Intersection over Union (mIoU) per class between the prediction and the manually annotated segmentation masks. Overall mean mIoU over all classes and all test samples is also computed and used for ranking different methods under comparison.


#### Frame Registration and Mosaicking Evaluation

For evaluating homographies and mosaics (Task 2), we use the evaluation metric presented by Bano et al. (2020a) in the absence of groundtruth. The metric that we referred as -frame structural similarity index measure (SSIM) aims to evaluate the consistency in the adjacent frames. A visual illustration of the -frame SSIM metric is presented in Fig. 6. Given consecutive frames and a set of − 1 homographies { 1 , 2 , ..., −1 }, we evaluate the consistency between them. The ultimate clinical goal of Figure 6: Illustration of the N-frame SSIM evaluation metric from Bano et al. (2020a) fetoscopic registration is to generate consistent, comprehensible and complete mosaics that map the placental surface and guide the surgeon. Considering adjacent frames will have a large overlap along them, we evaluate the registration consistency between pairs of non-consecutive frames frames apart that have a large overlap in the FoV and present a clear view of the placental surface. Consider a source image , a target image + , and a homography transformation → + between them, we define the consistency between these two images as:
→ + = sim( (˜, → + ),˜+ )(2)
where sim is an image similarity metric that is computed based on the target image and warped source image, and is a smoothed version of the image . Smoothing˜is obtained by applying a 9 × 9 Gaussian filter with a standard deviation of 2 to the original image . This is fundamental to make the similarity metric robust to small outlier (e.g., particles) and image discretization artifacts. For computing the similarity, we start by determining the overlap region between the target˜and the warped source (˜, → + ), taking into account their circular edges. If the overlap contains less than 25% of˜, we consider that the registration failed, as there will be no such cases in the evaluation pool. A rectangular crop fits the overlap, and the SSIM is calculated between the image pairs after being smoothed, warped, and cropped.


### Challenge Organization and Timeline

The FetReg 2021 challenge is a crowdsourcing initiative that was organized by Sophia Bano (University Col- The challenge timeline and submission statistics are presented in Fig. 7. The challenge was announced on April 1st 2021, through the FetReg2021 Synapse 2 website. The training dataset for task 1 and task 2 was released on May 1st and 29th, respectively. No restrictions were imposed on using additional publicly available datasets for training. A challenge description paper  that also included baseline method evaluation was also published on June 10th. All the details regarding the baseline methods (i.e., architecture, algorithms, and training settings) for segmentation and registration have been publicly disclosed along with its release. Additionally, a slack support forum was launched for faster communication with the participants.

Docker submission was opened on August 20th 2021, followed by the team registration deadline of September 10th, and the final submission deadline was set to September 17th. Members of the organizers' department may participate in the challenge, but were not eligible for awards.

Through the FegReg website, it was announced since the start of the challenge that the top three performing methods will be announced publicly during the challenge day, and the top method for each task will be awarded with a prize from the sponsors. The remaining teams could decide whether their identity should be publicly revealed or not (e.g., in the challenge publication). All participating team, whose method achieved an overall mIoU of over 0.25 were included in this joint publication. Only one team was excluded as their method resulted in an extremely low mIoU of 0.060 on the test set (see Section 4.) The test dataset was not made available to the challenge participants to keep the comparison fair and avoid misuse of the test data during training. Each participating team was required to make submissions as a docker container that accepts a path to a folder containing video frames from a patient as input and outputs a segmentation mask as an image (for task 1) or a text file with relative homography matrix (for task 2). Only fully automatic algorithms are allowed to participate in the challenge.

The teams could submit multiple docker dockers during the submission time (from August 20th to September 17th 2021) to check the validity of the docker. We provided the participants with docker examples for both tasks along with detailed submission guidelines through FetReg2021 GitHub repository 11 . The docker submission protocol is illustrated in Fig. 8. Each participating team submitted their docker through the Synapse platform. The submitted docker was verified for the validity of their output structure, i.e., they follow the same output format as requested and needed for the evaluation. Each participating team was then informed whether their submission passed the 11 FetReg2021 GitHub: https://github.com/sophiabano/ EndoVis-FetReg2021 validity test. Each team was allowed to submit multiple dockers. However, only the last valid docker submission was used in the final evaluation.

We received 33 challenge registration requests from 16 different countries. A total of 13 team registration requests with a total number of 22 team members were received. For task 1, final submissions were received from 7 teams having 16 participants. For task 2, one submission was received, probably because of the challenging nature of this task.


## Summary of methods proposed by participating teams

In total, 7 teams participated in the challenge. Out of these, one team did not qualify to be included in this article as the achieved performance was extremely low with a mIoU of 0.060. In this section, we summarize the methodology proposed by each participating team.  (France), University Rovira i Virgili (Spain) and University of Bourgogne (France). The method proposed by AQ-ENIB implemented a model made by a recursive dense encoder followed by a non-dense decoder. Dense encoder is chosen to enable efficient features reuse, facilitating training convergence. The dense encoder consists of 5 dense blocks, each consisting of 6 dense layers followed by a transition layer. Each dense layer consists of 2 convolutional layers with batch normalization (BN) and ReLU activation functions. The first convolutional layer uses 1 × 1 kernels, while the second uses 3 × 3 kernels. The transition layers consist of a BN layer, a 1 × 1 convolutional layer, and a 2 × 2 average pooling layer. The transition layer helps to reduce feature-map size. The dense blocks in the encoder have an increasing number of feature maps at each encoder stage. The model is trained using 5-fold cross-validation. To compute the final prediction, test time augmentation (TTA) is performed. This means that the model is fed with raw images and their augmented versions (using flipping and rotation with different angles). The model predicts, for each input, a segmentation mask. All the segmentation masks are ensembled using maximum majority voting.


### AQ-ENIB

The recursive dense architecture proposed by AQ-ENIB enables improved feature learning on the small training dataset, attenuating the chance of overfitting. Test time augmentation allows the team to increase the variability of the test set. A graphical schema of the method has been provided in Fig. 9(a) 


### BioPolimi

The team BioPolmini from Politecnico di Milano (Italy) are Chiara Lena, Ilaria Anita Cintorrino, Gaia Romana De Paolis and Jessica Biagioli. The model proposed by BioPolimi has a ResNet50 (He et al., 2016) backbone followed by the U-Net (Ronneberger et al., 2015) decoder for segmentation. The model is trained for 700 epochs with 6-fold cross-validation, using learning rate and batch size of 10 −3 and 32, respectively. To be consistent with the FetReg Challenge baseline, training images are resized to 448 × 448 pixels. Data augmentation, consisting of random crop with size 256 × 256 pixels, random rotation (in range (−45 • , +45 • )), horizontal and vertical flip and random variation in brightness (in range (−20%, +20%)), is applied to the training data. During inference, testing images are cropped in patches of dimension 256×256 pixels. The final prediction is obtained by overlapping the prediction obtained for each patch with a stride equal to 8.

BioPolimi enhances the baseline architecture by incorporating handcrafted features to address the issue of low contrast. The Histogram of Oriented Gradients (HoG) is specifically combined with features from ResNet50 to strengthen the recognition of anatomical contours, thereby supplying the decoder with a spatial prior of the features. A graphical schema of the method has been provided in Fig. 9(b).


### GRECHID

Team GRECHID is Daria Grechishnikova from Moscow State University (Russia). The method proposed by GRECHID consists of a U-Net model with SERes-NeXt50 backbone (Hu et al., 2018) trained sequentially for each class (i.e., vessels, fetus and surgical tools). The SEResNeXt50 backbone contains Squeeze-and-Excitation (SE) blocks, which allow the model to weigh adaptively each channel of SE blocks. Before training, exact and OOF (f)


## EfficientNet-b0 UNet++


## AQ-ENIB (a)

BioPolimi ( 


## GRECHID (d)


## SEResNeXt -UNet

Model 0


## SEResNeXt -UNet

Model 1 


## SEResNeXt -UNet


## SANO (e)


## FPN ResNet152

Fold 01


## FPN ResNet152

Fold 02


## FPN ResNet152

Fold 03


## FPN ResNet152

Fold 04


## FPN ResNet152

Fold 05


## Ensemble Mean


## FPN ResNet152

Fold 06  near-duplicates were removed using an online software 12 , obtaining 783 unique images from the original training dataset. Multi-label stratification split is performed to allocate images into train, test, and validation sets. All the images are resized to 224 × 224 pixels. To improve model generalization, data augmentation is performed using horizontal and vertical flip, random rotation and flipping. The model is trained using Adam optimizer and cosine annealing with restart as learning rate scheduler, with a loss that combines Dice and modified cross-entropy losses. The 12 https://github.com/idealo/imagededup modified cross-entropy loss has additional parameters to penalize either false positives and false negatives. Training is carried out in two stages. During the first stage, the model is trained for 30 epochs with a higher learning rate of 10 −3 , then the learning rate is lowered to 10 −5 . Cosine annealing with restart scheduling is used until best convergence.


## RREB (c)

A triple threshold-based post-processing is applied on the model output to remove spurious pixels.

GRECHID proposes the use of a ResNeXt encoder for feature extraction. This approach aims to address the challenges of large intra-class variability and poor im-age quality by providing a better representation of features. Additionally, the per-class model ensemble and triple threshold post-processing help manage the high data imbalance. A graphical schema of the method has been provided in Fig. 9(d).


### OOF -Overoverfitting

Team OOF are Jing Jiao, Bizhe Bai and Yanyan Qiao from Fudan University (China), University of Toronto (Canada) and MicroPort Robotics. Team OOF used U-Net++ (Zhou et al., 2018) as the segmentation model. EfficientNetb-0 (Tan and Le, 2019) pre-trained on the Im-ageNet dataset is used as U-Net++ encoder. To tackle illumination variability, median blur and Contrast Limited Adaptive Histogram Equalization (CLAHE) are applied to the images before feeding them to the model. Data augmentation, including random rotation, flip, and elastic transform, is applied during training. Adam optimizer with an initial learning rate of 10 −4 is used. The learning rate increases exponentially with 5 warm-up epochs.

OOF addresses the issue of low contrast in images by applying the Contrast Limited Adaptive Histogram Equalization (CLAHE) technique to enhance the visibility of vessel borders. Along with visual challenges, the team encountered moiré patterns in some images that could pose difficulties in identifying the vessels. To better learn features from a small and unbalanced dataset, various configurations of EfficientNet were used as feature extractors, combined with a U-Net++ architecture and trained using standard data augmentation techniques. After evaluating the results, the team determined that the EfficientNet-b0 configuration was the best option to submit, as deeper architectures did not result in improved performance during validation. A graphical schema of the method has been provided in Fig. 9(f).


### RREB

Team RREB are Binod Bhattarai, Rebati Raman Gaire, Ronast Subedi and Eduard Vazquez from University College London (UK), NepAL Applied Mathematics and Informatics Institute for Research (Nepal) and Redev Technology (UK). The model proposed by RREB uses 2 -Net (Qin et al., 2020) as the segmentation network. A regressor branch is added on top of each decoder layer to learn the Histogram of Oriented Gradients (HoG) at different scales. The loss L minimized during the training is defined as:
L = CE seg + MSE HoG(3)
where = 1, CE seg is the cross-entropy loss for semantic segmentation, = 1 and MSE HoG is the mean-squared error of the HoG regressor. All the images are resized to 448 × 448 pixels, and random crops of 256 × 256 are extracted. Random rotation between (−45 • , +45 • ), cropping at different corners and centers, and flipping are applied as data augmentation. The entire model is trained for 200000 iterations using Adam optimizer with 1 = 0.9 and 2 = 0.999 and a batch size of 16. The initial learning rate is set to 0.0002 and then is halved at 75000, 125000, 175000 iterations. The proposed model is validated through cross-validation.

RREB team proposes the use of 2 -Net to enhance the learning of multi-scale features in fetoscopic images. They believe that combining handcrafted features with semantic segmentation and detection can better represent the structure of interest without incurring extra costs. To achieve this, RREB's network learns HoG descriptors as an auxiliary task, by adding regression heads to 2 -Net at each scale. A graphical schema of the method has been provided in Fig. 9(c).


### SANO

Team SANO from Sano Center for Computational Medicine (Poland) are Szymon Płotka, Aneta Lisowska and Arkadiusz Sitek. This is the only team that participated in both tasks. Segmentation. The model proposed by SANO is a Feature Pyramid Network (FPN) ) that uses ResNet-152 (He et al., 2016) with pre-trained weights as backbone. The first convolutional layer has a 3-input channel, = 64 feature maps, 7 × 7 kernel with stride = 2, and padding = 3. The following three convolutional blocks have 2 ,4 and 32 feature maps. Our bottleneck consists of three convolutional blocks with BN. During training, the images are resized to 448 × 448 pixels and following augmentations are applied:  • horizontal and vertical flip.

The overall framework is trained with cross-entropy loss using a batch size of 4, Adam as optimizer with an initial learning rate of 10 −4 , weight decay and step learning rate by 0.1, and cross-entropy loss. Validation is performed with 6-fold cross-validation. SANO propose to use a deeper feature encoder ResNet-152, to increase the number of features extracted, on top of a FPN architecture to tackle image complexity and improve segmentation performance. A graphical schema of the strategy proposed by SANO team for Task 1 is shown in Fig. 9(e).

Registration. The algorithm uses the channel corresponding to the placental vessel (PV) from the segmentation network and the original RGB images. The algorithm only models translation with the precision of 1 pixel. If frames are indexed by = 1, . . . , , . . . , , the algorithm finds − 1 translations between neighboring frames. To compute the placenta vasculature (PV) image, softmax is applied to the raw output of the segmentation. The PV channel is extracted and multiplied by 255. A mask of non-zero pixels is computed from the raw image and applied to the PV image. The homography is then computed in two steps: The shift between PV images and + 1 is computed using masked Fast Fourier Transform. Then, the rotation matrix between and the shifted + 1 image + 1 is computed by minimizing the mean square error.


### Baseline

As the baseline model, we trained a U-Net (Ronneberger et al., 2015) with ResNet50 (He et al., 2016) backbone as described in Bano et al. (2020a). Softmax activation is used at the final layer. Cross-entropy loss is computed and back propagated during training. Before training, the images are first resized to 448×448 pixels. To perform data augmentation, at each iteration step, a patch of 256 × 256 pixels is extracted at a random position in the image. Each of the extracted patches is augmented by applying a random rotation in range (−45 • , +45 • ), horizontal and vertical flip, scaling with a factor in the range of (−20%, +20%) and random variation in brightness (−20%, +20%) and contrast (−10%, +10%). Segmentation results are obtained by inference using 448 × 446 pixels resized input image. The baseline model is trained for 300 epochs on the training dataset. We create 6 folds, where each fold contains 3 procedures, to preserve as much variability as possible while keeping the number of samples in each fold approximately balanced. The final model is trained on the entire dataset, splitting videos in 80% for training and 20% for validation. The data is distributed to represent the same amount of variability in both subsets.


## Quantitative and Qualitative Evaluation Results


### Data variability contribution

To assess data variability contribution from the multicenter dataset, we compute the performance of our baseline model when trained on data from one surgical center and tested on data from the other one. Table 3 shows the mIoU over each of the 6 test video samples and the overall mIoU over all videos with the baseline model trained on dataset from a single center. Figure 10 shows (a) the qualitative comparison of mean performance over each test video for baseline model trained with data from only one center. When training the model on data from Center I ,  I   II  I+II  I  II  I+II  I  II  I+II  I  II  I+II  I  II  I+II  I  the baseline performance on all test videos are generally lower compared to the one trained on data from Center II, except for Video025, which obtained an average mIoU of 0.1102 and 0.1761 respectively.

The difference in baseline model performance is mainly due to the variability and size of the dataset. In Center I, the images are higher quality and have well-visible structures. Although this is beneficial for clinicians, it needs to provide more information for the model learning process, which may lead to overfitting and poor segmentation performance. In contrast, data from Center II is more diverse, with various cases treated (e.g., different placenta positions and gestational weeks) and various imaging setups (e.g., straight or 30-degree fetoscope, brightness, FoV size). The increased image variability from these factors enables the model to generalize better to test images. Another crucial factor is that dividing the two datasets reduces the training set to about 900 images.

It can also be observed that when trained on individual center data, the model is not generalizable on the other center data due to data variability. However, combining the datasets (I+II) enhances the baseline model performance (average mIoU of 0.6763) and generalization capabilities, as it introduces a more extensive collection of images with higher variability.


### Placental Scene Segmentation Task

We perform both quantitative and qualitative comparison to evaluate the performance of the submitted placental scene segmentation methods. Table 4 shows the mIoU for each team individually over each of the 6 test video samples and the overall mIoU over all videos. To test the rank stability, the total number of times a team is ranked 1st on a video is also reported. Figure 11(a) shows the qualitative comparison of each team on each video, and Fig. 11(b) shows the comparison of each team on individual segmentation classes.

The qualitative results for the placental scene segmentation task are presented in Fig. 12. Among the challenge participants, the best performing approach is that of RREB, which achieved an overall mIoU of 0.6411. RREB obtained the best performance for all videos, but Video010 and Video012, where AQ-ENIB and GRECHID were the best, respectively. RREB performed the best among participants for all the three classes, with median IoU for vessel and surgical tools that overcome 60%. However, RREB obtained poor results for fetus segmentation, with a median IoU lower than 40% with a large dispersion among images. As shown in Fig. 12(c) and (d), RREB meets challenges in presence of fetus and surgical tools. In the first case, RREB does not segment the fetus, while in the second the tool is segmented as fetus.

GRECHID scored second among all the participants, with a mIoU of 0.5865. As for RREB, GRECHID grants the best and lowest performance for surgical tools and vessels, respectively. Figure 12(b) and (f) show that GRECHID wrongly identifies and segments the fetus when it is not present in the FoV, while in Fig. 12(c), where the fetus is present, GRECHID does not segment it.

With an overall mIoU of 0.5741, SANO scored third, with the best performance achieved for vessels. SANO Intesertion-over-Union shows high variability in the IoU computed among frames for both fetus and surgical tools. Despite the generalized good visual performance among videos, SANO tends to underestimate the areas.

AQ-ENIB obtained an overall mIoU of 0.5503 with the least performance obtained with fetus segmentation. Despite the good performance for vessel segmentation, vessel area is often underestimated as shown in Fig. 12(b), (e) and (f).

BioPolimi and OOF show the least performance with an mIoU of 0.3443 and 0.2526, respectively. OOF also faced challenges in images where one single vessel is present in the FoV, as shown in Fig. 12(b). Despite the low overall performance of BioPolimi, especially in tool and fetus segmentation, vessels are correctly segmented when visible and continuous (i.e., particles or specularities does not interrupt vessels surface), as shown in Fig. 12(d).

The baseline method is the best performing method achieving an overall mIoU of 0.6763, overcoming the performance of the challenge participants for all videos but Video024 and Video025 where RREB is the best performing method.


### Registration for Mosaicking Task

Quantitative and qualitative results for the mosaicking task are presented in Table 5, Fig. 14 and Fig. 13.

The mosaics from the baseline and SANO methods and their 5-frame SSIM metric for every pair of images 5 frames apart in a sequence are shown in Fig. 13 for all 6 test video clips. Both methods utilized placental vessel maps for estimating the transformation between adjacent frames. From the mosaic of Video010, we observe that both methods followed different strategies for registration. SANO utilized translation registration having fewer degrees of freedom, while baseline performed affine registration of vessel having more degrees of freedom. Therefore, baseline is able to deal with perspective warpings while SANO's approach is unable to deal with perspective changes and overestimates translation to compensate such changes. As a result, the 5-frame SSIM for SANO is lower compared to the baseline in Video001. On Video012, both methods struggled to generate a meaningful mosaic, but overall the baseline resulted in better 5-frame SSIM metric compared to SANO (see Table 5). Video015 is an anterior placenta case in which the placen- Figure 12: Qualitative comparison of the 7 methods under analysis. Both baseline and RREB better generalize over the placental scene dataset. Baseline achieved better segmentation than RREB in (c), (d) and (e). OOF is the least performing as it failed to generalize, wrongly segmenting vessels and missed the fetus class. White markers on the input and groundtruth images indicate regions where observations can be drawn between the seven methods under comparison. Through rank stability test, we found that baseline performance was better in 5 out of 6 videos (see Table 5). Figure. 13 shows the qualitative comparison using 1 to 5 frame SSIM metric. We observe that with increasing frame distance, the error becomes large. In the case of SANO, Video010 and Video015 results in large drift even at 2-frame distance. As SANO used a translation trans- formation estimation, its error becomes very large in all videos when observing from 1 to 5 frames SSIM. The baseline followed an affine transformation estimation, as a result, its errors appear to be relatively smaller than SANO, which mainly occurred when no visible vessels were present in the scene.


## Discussion

An accurate placental semantic segmentation is necessary for better understanding and visualization of the fetoscopic environment; as a result, this may facilitate surgeons in improved localization of the anastomoses and better surgical outcome. However, the high intra and inter-procedure variability remain a key challenge, as only a small subset of images from each procedure were manually annotated for model training. Additionally, datasets captured from different clinical centers varies in terms of the resolution, imaging device and light source, making model generalization even more challenging. From the segmentation model results on individual 6 test videos, we observed large variability in the mIoU values of all methods (see Table 4). Note that Video010, 012 and 024 are from Center II and the remaining were acquired from Center I. The performance of RREB, i.e., the winning team, may be explained by the use of a multi-task approach to segment anatomical structures while regressing the HoG. We hypothesize that training a CNN to regress multi-scale HoG from labels enhance borders and may help the network in segmenting poorly contrasted regions. RREB remains the best performaning team on the tool class. Despite HoG helps in better understanding the contours and thus producing smoother segmentation masks, this does not improve the performance with non-uniform texture, as for reflections on vessel surface (Fig. 12(e) which can cause holes in the final segmentation mask, and fetus. The poor performance of RREB for fetus segmentation may be attributed to the low number (293, i.e. 14.22% of training frames) and high texture variability of fetus frames used for training. Baseline method, which is also the top performing method, was relatively low on Center II videos (average mIoU of 0.5130) compared to Center I videos (average mIoU of 0.6910). Team RREB, the second-best method, also performed poorly on Video012 (mIoU of 0.3765).

The runner-up team, GRECHID, achieved the best performance in vessel segmentation close to RREB and baseline, with some issues in segmenting the fetus ( Fig. 12(b,c,f)). GRECHID network architecture is rather similar to the baseline, but the adoption a per-class network configuration was chosen to achieve one-vs-all pixel classification and, thus eased data distribution learning for each class. While we cannot speculate whether this design actually improves the performance, it would be interesting to assess the data reduction impact on segmentation performance.

AQ-ENIB (average mIoU of 0.5503) and SANO (average mIoU of 0.5741) share the same segmentation strategy with only minor differences as also reflected from the comparable performance (ΔmIoU of 4.32%). Overall, both models perform well and have the same weakness producing no or under segmentation in case of reflections ( Fig. 12(c)), small vessels ( Fig. 12(d)) and poor contrast ( Fig. 12(f)). Test-Time Augmentation in AQ-ENIB can provide some help in fetus segmentation but also cause false positive as in Fig. 12(d). Considering the low difference in performance, we can analyze the models footprint and a positive aspect of AQ-ENIB is that DenseNet has lower parameters number (around 20 millions) compared to SANO ResNet152 (around 60 millions).

BioPolimi uses the same architecture of the baseline but achieved way lower performance (average mIoU of 0.3443). The integration of HoG features computed on the image seemed to have a negative impact on segmentation. We hypothesize that computing HoG features on the input frame does not provide a strong reference to help network encoder to manage for low contrast, compared to HoG computed on groundtruth and multi-task as in RREB. Even though, it is worth to mention that BioPolimi method achieved the best vessel segmentation for Fig. 12(d).

OOF method is the least performing (average mIoU of 0.2526) on all the test set and produced several segmentation error as shown in Fig. 12. We think that the additional preprocessing generates image with high contrast, thus polarizing the network in learning non-realistic features.

There was no single method that outperformed on all 6 test samples. This suggests that the proposed methods did not fully generalize to the dataset distributions from the two centers. Nonetheless, it is worth to consider that some of the strategies presented by the participants are complementary and can be combined to effectively tackle some of the challenges and boost the segmentation performance. Further can be performed to assess the performance by combining RREB method with other architectures, like DenseNet (AQ-ENIB) or ResNet (SANO) with GRECHID data reduction and AQ-ENIB Test-Time Augmentation. However, as stated in Sec. 3.3, participants have been requested to provide their inference algorithms as Docker container thus we do not have access to their training code.

To better model the variability in the dataset, more annotated images would be needed for supervised learning. Limited annotation problems can also be addressed through pseudo labelling using semi-supervised learning techniques. A reliable and consistent mosaic is needed for visualizing an increased FoV image of the placental environment. The two methods under comparison relied on accurate placental vessel segmentation for mosaicking. However, during fetoscopy, the placenta regions might appear either with very thin and weak vessels or no vessels at all. A segmentation algorithm may fail in these scenarios, especially when no vessels are visible, leading to failure in consecutive frames registration for mosaicking. This suggests that a registration algorithm should not solely rely on vessel segmentation predictions. More recent deep learning-based keypoint and matching approaches (DeTone et al., 2018;Sarlin et al., 2020;Sun et al., 2021) could be useful in improving placental frame registration for mosaicking. Some recent works (Casella et al., 2022;Bano et al., 2022) in mosaicking have already shifted interests towards exploiting learning-based keypoints and matching approaches.


## Conclusion

SDS has the potential to enhance intraoperative imaging by providing better visualization of the surgical environment with increased FoV to support the surgeon's decision during the procedure. Deep learning-based semantic segmentation algorithms can help in better understanding the fetoscopic placental scene during fetoscopy. How-ever, large labelled datasets are required for training robust segmentation models. Through the FetReg2021 challenge, which was part of the MICCAI2021 Endoscopic vision challenge, we contributed a large scale multi-center fetoscopy dataset containing data from 18 fetoscopy procedures for training and 6 fetoscopy procedures for testing. The test data was hidden from the challenge participants but followed similar distribution to the training dataset. The challenge focused on solving the task of placental semantic segmentation and fetoscopy video frame registration for mosaicking. The segmentation solutions presented by the participating teams achieved promising results though they were unable to beat the baseline method. Achieving generalizability remained an open question, and none of the methods outperformed in all test video samples. The contributed mosaicking approaches relied on accurate vessel segmentation and the presence of vessels in the fetoscopic placental view. Through the Fe-tReg2021 challenge, we contributed a benchmark dataset for advancing the research in fetoscopic mosaicking.

## Figure 1 :
1Illustrations of Twin-to-Twin Transfusion Syndrome. (a) shows the fetoscopic laser photocoagulation procedure, where the field of view of the fetoscope is extremely narrow. (b) shows the types of anastomoses (i) A-V: arterio-venous, (ii) V-V: veno-venous, and (iii) A-A: arterioarterial.

## Figure 2 :Figure 3 :
23Training dataset distribution: (a) and (b) segmentation classes and their overall distribution in the segmentation data. Testing dataset distribution: (a) and (b) segmentation classes and their overall distribution n the segmentation data.

## Figure 4 :
4Representative images from training and test datasets along with the segmentation annotations (groundtruth). Each center ID is also indicated next to video name (I -UCLH, II -IGG) for visual comparison of variabilities between the two centers.


lege London, London, UK), Alessandro Casella (Istituto Italiano di Tecnologia and Politecnico di Milano, Italy), Francisco Vasconcelos (University College London, London, UK), Sara Moccia (Scuola Superiore Sant'Anna, Italy) and Danail Stoyanov (University College London, London, UK). The FetReg 2021 challenge was organized as part of the EndoVis challenge series, which is led by Stefanie Speidel (German Cancer Research Center, Heidelberg, Germany), Lena Maier-Hein (German Cancer Research Center, Heidelberg, Germany) and Danail Stoyanov (University College London, London, UK). The FetReg challenge was organized according to The Biomedical Image Analysis Challenges (BIAS) (Maier-Hein et al., 2020) reporting guideline to enhance the quality and transparency of health research.

## Figure 7 :
7FetReg2021 timeline and challenge participation statistics.

## Figure 8 :
8FetReg2021 submission protocol illustrating the docker image verification protocol.

## Figure 9 :
9Graphical overview of the participants' methodologies for Task 1 as described in Sec. 4 (Key: -input frame; -groundtruth;ˆprediction). AQ-ENIB (a) proposed an ensemble of DenseNet models with Test Time Augment (TTA). BioPolimi (b) combined ResNet50 features with Histogram of oriented Gradients (HoG) computed on . RREB (c) proposed a multi-task 2 for segmentation and multi-scale regression of HoG features (ĤoG 0 ,ĤoG 1 ,...) computed on (HoG 0 ,HoG 1 ,...). GRECHID (d) used 3 SEResNeXt-UNet models individually trained on each class ensembled by thresholding, where pixels HighConfidence are pixels predicted with high confidence and count threshold is the empirical threshold. SANO (e) proposed a mean ensemble of Feature Pyramid Network (FPN) with ResNet152 backbone. OOF (f) used an EfficientNet UNet++, preprocessing images with contrast limited adaptive histogram equalization (CLAHE) and median filter.

## •
color jitter (brightness = [0.8, 1.2], contrast = [0.8, 1.2], saturation = [0.8, 1.2], and hue = [−0.1, 0.1])

## Figure 10 :
10Qualitative comparison showing results for baseline model when trained on single center data and multi-center data. mIoU over each test video for the baseline model trained with data from one center (I -UCLH, II -IGG). Bar colors from left to right indicate Centre I, II and I+II results.

## Figure 11 :
11Qualitative comparison showing (a) mIoU for each team on each video, and (b) overall mIoU for each team per segmentation class. Bar colors from left to right in (a) indicate team in alphabetical order and in (b) indicate vessel, tool, fetus classes.

## Figure 14 :
14Quantitative comparison of the Baseline(Bano et al., 2020a) and SANO methods using the -frame SSIM metric.

## Table 1 :
1Overview of the existing segmentation (Sec. 2.1-2.1.2, fetoscopic event detection (Sec. 2.3) and video mosaicking methods (Sec. 2.2). The type of dataset used in each method is also reported. Key: IFM -inter-fetus membrane; GMS -grid-based motion statistics; EMT -electromagnetic tracker.Reference 
Task 
Methodology 
Imaging type 

Almoussa et al. (2011) 
Vessel segmentation Hessian filter and Neural Network trained on handcrafted features 
Ex-vivo 
Chang et al. (2013) 
Vessel segmentation Combined Enhancement Filters 
Ex-vivo (150 images) 
Sadda et al. (2019) 
Vessel segmentation Convolutional Neural Network (U-Net) 
In-vivo (345 frames from 10 TTTS procedures) 
Bano et al. (2019) 
Vessel segmentation Convolutional Neural Network 
In-vivo (483 frames from 6 TTTS procedures) 
Casella et al. (2020) 
IFM segmentation 
Adversarial Neural Network (ResNet) 
In-vivo (900 frames from 6 TTTS procedures) 
Casella et al. (2021) 
IFM segmentation 
Spatio-temporal Adversarial Neural Network (3D DenseNet) 
In-vivo (2000 frames from 20 TTTS procedures) 5 

Reeff et al. (2006) 
Mosaicking 
Hybrid feature and intensity-based 
In water ex-vivo placenta 
Daga et al. (2016) 
Mosaicking 
Feature-based with GPU for real time computation 
Ex-vivo, Phantom placenta 
Tella et al. (2016) 
Mosaicking 
Combined EM and visual tracking probablistic model 
Ex-vivo w/laparoscope& EMT 
Gaisser et al. (2016) 
Mosaicking 
Deep-learned features through contrastive loss 
Ex-vivo and Phantom placenta video frames 
Yang et al. (2016) 
Mosaicking 
SURF features matching and RANSAC for transformation estimation Ex-vivo and monkey placentas w/laparoscope 
Gaisser et al. (2017) 
Mosaicking 
Handcrafted features and LMedS for tranformation estimation 
Ex-vivo, In water placenta phantom 
Tella-Amo et al. (2018) 
Mosaicking 
Combined EM and visual tracking with bundle adjustment 
Ex-vivo placenta w/laparoscope & EMT 
Gaisser et al. (2018) 
Mosaicking 
Extended Gaisser et al. (2016) to detect stable vessel regions 
In water placenta phantom 
Sadda et al. (2018) 
Mosaicking 
AGAST detector with SIFT followed by GMS matching 
In-vivo (# frames/clips) 
Peter et al. (2018) 
Mosaicking 
Direct pixel-wise alignment of image gradient orientations 
In-vivo (# frames/clips) 
Tella-Amo et al. (2019) 
Mosaicking 
Pruning through EM and super frame generation 
Ex-vivo placenta w/laparoscope & EMT 
Bano et al. (2019, 2020a) Mosaicking 
Deep learning-based four point registration in consecutive images 
Synthetic, Ex-vivo, Phantom, In-vivo phantom) 
Bano et al. (2020a) 
Mosaicking 
Direct aligment of predicted vessel maps 
In-vivo fetoscopy placenta data (6 procedures) 6 
Li et al. (2021) 
Mosaicking 
Direct aligment of predicted vessel with graph optimisation 
In-vivo fetoscopy placenta data (3 procedures) 6 
Alabi et al. (2022) 
Mosaicking 
FlowNet 2.0 with robust estimation for direct registration 
Extended in-vivo fetoscopy placenta data (6 procedures) 6 
Casella et al. (2022) 
Mosaicking 
Learning-based keypoint matching for registration 
Extended in-vivo fetoscopy placenta data (6 procedures) 6 
Bano et al. (2022) 
Mosaicking 
Placental vessel-guided detector-free matching for registration 
Extended in-vivo fetoscopy placenta data (6 procedures) 6 

Vasconcelos et al. (2018) Ablation detection 
Binary classification using ResNet 
In-vivo fetoscopy videos (5 procedures) 
Bano et al. (2020c) 
Event detection 
Spatio-temporal model for multi-label classification 
In-vivo fetoscopy videos (7 procedures) 



## Table 2 :
2Summary of the EndoVis FetReg 2021 training and testing dataset. For each video, center


Team AQ-ENIB are Abdul Qayyum, Abdesslam Benzinou, Moona Mazher and Fabrice Meriaudeau from ENIBParticipating 
Team 

Docker Veri cation 
(testing for 
valid output) 

Docker image 
evaluation on test 
data 

Fail 

Pass 

Noti cation 
Noti cation 

Docker Submission 
FetReg Synapse 




b)ResNet50 
encoder 

HoG 

UNet 
decoder 
Concat 

DenseNet 
Fold 01 

DenseNet 
Fold 02 

DenseNet 
Fold 03 

DenseNet 
Fold 04 

DenseNet 
Fold 05 

TTA 

TTA 

TTA 

TTA 

TTA 

Ensemble 
Majority Voting 



## Table 3 :
3Results of segmentation on the test set for the Task 1 by training the baseline on videos only from one center. Each center ID is also indicated (I -UCLH, II -IGG) for performance comparison between the two centers.Train Dataset Video010 Video012 Video015 Video020 Video024 Video025 Overall mIoU 
Center ID 
II 
II 
I 
I 
II 
I 

I+II 
0.5750 
0.4122 
0.6923 
0.6757 
0.5514 
0.7045 
0.6763 
I 
0.0109 
0.0092 
0.1012 
0.0754 
0.0056 
0.2180 
0.1102 
II 
0.1968 
0.2630 
0.1525 
0.1562 
0.3545 
0.1907 
0.1761 

• random affine transformation (rotation = [−90, 90], 
translation = [0.2, 0.2], scale = [1, 2], shear = 
[−10, 10]) 



## Table 4 :
4Performance of participating methods for the Task 1 (segmentation) on on the test dataset. Each center ID is also indicated (I -UCLH, II -IGG) for performance comparison between the two centers.Team name 
Video010 Video012 Video015 Video020 Video024 Video025 Overall mIoU # Video won 
Center ID 
II 
II 
I 
I 
II 
I 

AQ-ENIB 
0.5611 
0.2745 
0.4855 
0.4848 
0.3342 
0.6414 
0.5503 
0 
Baseline (Bano et al., 2020a) 
0.5750 
0.4122 
0.6923 
0.6757 
0.5514 
0.7045 
0.6763 
4 
BioPolimi 
0.3891 
0.2806 
0.2718 
0.2606 
0.3666 
0.3943 
0.3443 
0 
GRECHID 
0.4768 
0.3792 
0.5884 
0.5744 
0.3097 
0.6534 
0.5865 
0 
OOF 
0.1874 
0.1547 
0.2745 
0.2074 
0.0872 
0.3724 
0.2526 
0 
RREB 
0.5449 
0.3765 
0.6823 
0.6191 
0.6443 
0.7585 
0.6411 
2 
SANO 
0.4682 
0.3277 
0.5201 
0.5863 
0.4132 
0.6609 
0.5741 
0 

Video010 Video012 Video015 Video020 Video024 Video025 
0.00 

0.25 

0.50 

0.75 

1.00 



## Table 5 :
5Results of Registration for the Task 2 using test video clips. Mean and Median of 5-frame-SSIM metric over individual video clips is reported.tal surface is not fronto-parallel to the camera. As a result, there is large perspective warping across different frames. SANO's approach failed in Video015 as it estimated only translation transformation. On the other hand, the baseline successfully estimated the warping through affine transformation, resulting in better 5-frame SSIM metric. Qualitatively, SANO performed better on Video020 compared to the baseline, especially in regions where vessels are visible and the mosaic remained bounded due to only translation transformation estimation. However, the errorFigure 13: Qualitative comparison of the Baseline(Bano et al., 2020a) and SANO methods showing (first column) generated mosaics from the Baseline method, (2nd column) generated mosaics from the SANO method, and (3rd column) 5-frame SSIM per frame for both methods. Baseline performance is better in all videos except Video020. between 5 frames is particularly large for SANO as the warpings are not accurate. Video024 and Video025 show interesting cases where in some frames there are no distinguishable structures like vessels (frame 90 in Video024 and frame 148 in Video025), hence both methods lost tracking intermediately. Quantitatively, SANO's performance is slightly better than the baseline on Video024.Team name 
Video010 Video012 Video015 Video020 Video024 Video025 Overall # Video won 
Center ID 
II 
II 
I 
I 
II 
I 

Baseline (Bano et al., 2020a) 
Mean 
0.9048 
0.9204 
0.9695 
0.9169 
0.9336 
0.9558 
0.9348 
5 
Median 
0.9303 
0.9330 
0.9767 
0.9301 
0.9478 
0.9712 
0.9524 

SANO 
Mean 
0.8231 
0.9164 
0.9588 
0.8276 
0.9420 
0.9234 
0.9019 
1 
Median 
0.8837 
0.9289 
0.9746 
0.8825 
0.9563 
0.9608 
0.9434 


https://grand-challenge.org/ 2 FetReg challenge website: https://www.synapse.org/#! Synapse:syn25313156/ 3 FetReg dataset: https://www.ucl.ac.uk/ interventional-surgical-sciences/weiss-open-research/ weiss-open-data-server/fetreg
EndoVis Challenges: https://endovis.grand-challenge. org/
Inter-Fetus Membrane Segmentation Dataset: https://zenodo. org/record/7259050
Supervisely: a web-based annotation tool: https://supervise. ly/
AcknowledgmentWe are grateful to NVIDIA, Medtronic and E4 Computing for sponsoring the FetReg2021 challenge. This work was supported by the Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS) at UCL (203145Z/16/Z), the Engineering and Physical Sciences Research Council (EP/P027938/1, EP/R004080/1,EP/P012841/1, NS/A000027/1), the Royal Academy of Engineering Chair in Emerging Technologies Scheme, Horizon 2020 FET Open (863146) and Wellcome [WT101957]. Anna L. David is supported by the NIHR UCLH Biomedical Research Center. For the purpose of open access, the author has applied a CC BY public copyright license to any author accepted manuscript version arising from this submission.
Robust Fetoscopic Mosaicking from Deep Learned Flow Fields. Oluwatosin Alabi, S Andbano, F Vasconcelos, L David, A Deprest, J Stoyanov, D , International Journal of Computer Assisted Radiology and Surgery. Alabi, Oluwatosin andBano, S., Vasconcelos, F., L. David, A., Deprest, J., Stoyanov, D., 2022. Ro- bust Fetoscopic Mosaicking from Deep Learned Flow Fields. International Journal of Computer Assisted Ra- diology and Surgery .

Automated vasculature extraction from placenta images. N Almoussa, B Dutra, B Lampe, P Getreuer, T Wittman, C Salafia, L Vese, Medical Imaging 2011: Image Processing. 79621Almoussa, N., Dutra, B., Lampe, B., Getreuer, P., Wittman, T., Salafia, C., Vese, L., 2011. Automated vasculature extraction from placenta images, in: Medi- cal Imaging 2011: Image Processing, SPIE. p. 79621L.

Fetreg: placental vessel segmentation and registration in fetoscopy challenge dataset. S Bano, A Casella, F Vasconcelos, S Moccia, G Attilakos, R Wimalasundera, A L David, D Paladini, J Deprest, E De Momi, arXiv:2106.05923arXiv preprintBano, S., Casella, A., Vasconcelos, F., Moccia, S., Atti- lakos, G., Wimalasundera, R., David, A.L., Paladini, D., Deprest, J., De Momi, E., et al., 2021. Fetreg: pla- cental vessel segmentation and registration in fetoscopy challenge dataset. arXiv preprint arXiv:2106.05923 .

Placental vessel-guided hybrid framework for fetoscopic mosaicking. S Bano, F Vasconcelos, A L David, J Deprest, D Stoyanov, Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization. Bano, S., Vasconcelos, F., David, A.L., Deprest, J., Stoy- anov, D., 2022. Placental vessel-guided hybrid frame- work for fetoscopic mosaicking. Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization , 1-6.

Deep placental vessel segmentation for fetoscopic mosaicking. S Bano, F Vasconcelos, L M Shepherd, E Vander Poorten, T Vercauteren, S Ourselin, A L David, J Deprest, D Stoyanov, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerBano, S., Vasconcelos, F., Shepherd, L.M., Van- der Poorten, E., Vercauteren, T., Ourselin, S., David, A.L., Deprest, J., Stoyanov, D., 2020a. Deep placen- tal vessel segmentation for fetoscopic mosaicking, in: International Conference on Medical Image Comput- ing and Computer-Assisted Intervention, Springer. pp. 763-773.

Deep sequential mosaicking of fetoscopic videos. S Bano, F Vasconcelos, M Tella Amo, G Dwyer, C Gruijthuijsen, J Deprest, S Ourselin, E Vander Poorten, T Vercauteren, D Stoyanov, Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). Bano, S., Vasconcelos, F., Tella Amo, M., Dwyer, G., Gruijthuijsen, C., Deprest, J., Ourselin, S., Van- der Poorten, E., Vercauteren, T., Stoyanov, D., 2019. Deep sequential mosaicking of fetoscopic videos, in: Lecture Notes in Computer Science (including sub- series Lecture Notes in Artificial Intelligence and Lec- ture Notes in Bioinformatics), pp. 311-319.

Deep learning-based fetoscopic mosaicking for field-of-view expansion. S Bano, F Vasconcelos, M Tella-Amo, G Dwyer, C Gruijthuijsen, E Vander Poorten, T Vercauteren, S Ourselin, J Deprest, D Stoyanov, International Journal of Computer Assisted Radiology and Surgery. Bano, S., Vasconcelos, F., Tella-Amo, M., Dwyer, G., Gruijthuijsen, C., Vander Poorten, E., Vercauteren, T., Ourselin, S., Deprest, J., Stoyanov, D., 2020b. Deep learning-based fetoscopic mosaicking for field-of-view expansion. International Journal of Computer Assisted Radiology and Surgery .

FetNet: a recurrent convolutional network for occlusion identification in fetoscopic videos. S Bano, F Vasconcelos, E Vander Poorten, T Vercauteren, S Ourselin, J Deprest, D Stoyanov, International Journal of Computer Assisted Radiology and Surgery. 15Bano, S., Vasconcelos, F., Vander Poorten, E., Ver- cauteren, T., Ourselin, S., Deprest, J., Stoyanov, D., 2020c. FetNet: a recurrent convolutional network for occlusion identification in fetoscopic videos. Inter- national Journal of Computer Assisted Radiology and Surgery 15, 791-801.

Twin-to-twin transfusion syndrome (TTTS). A Baschat, R H Chmait, J Deprest, E Gratacós, K Hecher, E Kontopoulos, R Quintero, D W Skupski, D V Valsky, Y Ville, Journal of Perinatal Medicine. 39Baschat, A., Chmait, R.H., Deprest, J., Gratacós, E., Hecher, K., Kontopoulos, E., Quintero, R., Skupski, D.W., Valsky, D.V., Ville, Y., 2011. Twin-to-twin transfusion syndrome (TTTS). Journal of Perinatal Medicine 39, 107-112.

Fetoscopic laser therapy for twin-twin transfusion syndrome before 17 and after 26 weeks' gestation. D Baud, R Windrim, J Keunen, E N Kelly, P Shah, T Van Mieghem, P G R Seaward, G Ryan, American Journal of Obstetrics and Gynecology. 208Baud, D., Windrim, R., Keunen, J., Kelly, E.N., Shah, P., Van Mieghem, T., Seaward, P.G.R., Ryan, G., 2013. Fetoscopic laser therapy for twin-twin transfusion syn- drome before 17 and after 26 weeks' gestation. Ameri- can Journal of Obstetrics and Gynecology 208, 1-197.

Gms: Grid-based motion statistics for fast, ultra-robust feature correspondence. J Bian, W Y Lin, Y Matsushita, S K Yeung, T D Nguyen, M M Cheng, IEEE Conference on Computer Vision and Pattern Recognition. Bian, J., Lin, W.Y., Matsushita, Y., Yeung, S.K., Nguyen, T.D., Cheng, M.M., 2017. Gms: Grid-based motion statistics for fast, ultra-robust feature correspondence, in: IEEE Conference on Computer Vision and Pattern Recognition, pp. 4181-4190.

Learningbased keypoint registration for fetoscopic mosaicking. A Casella, S Bano, F Vasconcelos, A L David, D Paladini, J Deprest, E De Momi, L S Mattos, S Moccia, D Stoyanov, 10.48550/ARXIV.2207.13185Casella, A., Bano, S., Vasconcelos, F., David, A.L., Paladini, D., Deprest, J., De Momi, E., Mattos, L.S., Moccia, S., Stoyanov, D., 2022. Learning- based keypoint registration for fetoscopic mosaick- ing. URL: https://arxiv.org/abs/2207.13185, doi:10.48550/ARXIV.2207.13185.

Inter-foetus Membrane Segmentation for TTTS Using Adversarial Networks. A Casella, S Moccia, E Frontoni, D Paladini, E De Momi, L S Mattos, Annals of Biomedical Engineering. 48Casella, A., Moccia, S., Frontoni, E., Paladini, D., De Momi, E., Mattos, L.S., 2020. Inter-foetus Mem- brane Segmentation for TTTS Using Adversarial Net- works. Annals of Biomedical Engineering 48, 848- 859.

A shape-constraint adversarial framework with instance-normalized spatiotemporal features for inter-fetal membrane segmentation. A Casella, S Moccia, D Paladini, E Frontoni, E De Momi, L Mattos, Medical Image Analysis. 70Casella, A., Moccia, S., Paladini, D., Frontoni, E., De Momi, E., Mattos, L., 2021. A shape-constraint ad- versarial framework with instance-normalized spatio- temporal features for inter-fetal membrane segmenta- tion. Medical Image Analysis 70, 102008.

Vessel enhancement with multiscale and curvilinear filter matching for placenta images. J M Chang, N Huynh, M Vazquez, C Salafia, International Conference on Systems, Signals, and Image Processing. IEEE Computer SocietyChang, J.M., Huynh, N., Vazquez, M., Salafia, C., 2013. Vessel enhancement with multiscale and curvilinear filter matching for placenta images, in: International Conference on Systems, Signals, and Image Process- ing, IEEE Computer Society. pp. 125-128.

Future Directions in the Management of Twin-to-Twin Transfusion Syndrome. R Cincotta, S Kumar, Twin Research and Human Genetics. 19Cincotta, R., Kumar, S., 2016. Future Directions in the Management of Twin-to-Twin Transfusion Syndrome. Twin Research and Human Genetics 19, 285-291.

Real-time mosaicing of fetoscopic videos using SIFT. P Daga, F Chadebecq, D I Shakir, L C G Herrera, M Tella, G Dwyer, A L David, J Deprest, D Stoyanov, T Vercauteren, S Ourselin, Medical Imaging 2016: Image-Guided Procedures, Robotic Interventions, and Modeling. 97861Daga, P., Chadebecq, F., Shakir, D.I., Herrera, L.C.G., Tella, M., Dwyer, G., David, A.L., Deprest, J., Stoy- anov, D., Vercauteren, T., Ourselin, S., 2016. Real-time mosaicing of fetoscopic videos using SIFT, in: Medi- cal Imaging 2016: Image-Guided Procedures, Robotic Interventions, and Modeling, SPIE. p. 97861R.

The making of fetal surgery. J A Deprest, A W Flake, E Gratacos, Y Ville, K Hecher, K Nicolaides, M P Johnson, F I Luks, N S Adzick, M R Harrison, Deprest, J.A., Flake, A.W., Gratacos, E., Ville, Y., Hecher, K., Nicolaides, K., Johnson, M.P., Luks, F.I., Adz- ick, N.S., Harrison, M.R., 2010. The making of fetal surgery.

D Detone, T Malisiewicz, A Rabinovich, arXiv:1606.03798Deep image homography estimation. arXiv preprintDeTone, D., Malisiewicz, T., Rabinovich, A., 2016. Deep image homography estimation. arXiv preprint arXiv:1606.03798 .

Superpoint: Self-supervised interest point detection and description. D Detone, T Malisiewicz, A Rabinovich, IEEE Conference on Computer Vision and Pattern Recognition. DeTone, D., Malisiewicz, T., Rabinovich, A., 2018. Su- perpoint: Self-supervised interest point detection and description, in: IEEE Conference on Computer Vision and Pattern Recognition, pp. 224-236.

Image Registration for Placenta Reconstruction. F Gaisser, P P Jonker, T Chiba, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. IEEE Computer SocietyGaisser, F., Jonker, P.P., Chiba, T., 2016. Image Registra- tion for Placenta Reconstruction, in: IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, IEEE Computer Society. pp. 473-480.

Fetoscopic panorama reconstruction: Moving from ex-vivo to in-vivo. F Gaisser, S H Peeters, B Lenseigne, P P Jonker, D Oepkes, Communications in Computer and Information Science. 723Springer International PublishingGaisser, F., Peeters, S.H., Lenseigne, B., Jonker, P.P., Oepkes, D., 2017. Fetoscopic panorama reconstruc- tion: Moving from ex-vivo to in-vivo. volume 723 of Communications in Computer and Information Sci- ence. Springer International Publishing.

Stable image registration for in-vivo fetoscopic panorama reconstruction. F Gaisser, S H Peeters, B A Lenseigne, P P Jonker, D Oepkes, Journal of Imaging. 4Gaisser, F., Peeters, S.H., Lenseigne, B.A., Jonker, P.P., Oepkes, D., 2018. Stable image registration for in-vivo fetoscopic panorama reconstruction. Journal of Imag- ing 4.

Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, IEEE Conference on Computer Vision and Pattern Recognition. He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition, in: IEEE Conference on Computer Vision and Pattern Recognition, pp. 770- 778.

Squeeze-and-excitation networks. J Hu, L Shen, G Sun, 10.1109/CVPR.2018.007452018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Hu, J., Shen, L., Sun, G., 2018. Squeeze-and-excitation networks, in: 2018 IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pp. 7132-7141. doi:10.1109/CVPR.2018.00745.

Flownet 2.0: Evolution of optical flow estimation with deep networks. E Ilg, N Mayer, T Saikia, M Keuper, A Dosovitskiy, T Brox, IEEE Conference on Computer Vision and Pattern Recognition. Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy, A., Brox, T., 2017. Flownet 2.0: Evolution of optical flow estimation with deep networks, in: IEEE Confer- ence on Computer Vision and Pattern Recognition, pp. 2462-2470.

G2O: A general framework for graph optimization. R Kümmerle, G Grisetti, H Strasdat, K Konolige, W Burgard, 2011 IEEE International Conference on Robotics and Automation. IEEEKümmerle, R., Grisetti, G., Strasdat, H., Konolige, K., Burgard, W., 2011. G2O: A general framework for graph optimization, in: 2011 IEEE International Con- ference on Robotics and Automation, IEEE. pp. 3607- 3613.

The vascular anastomoses in monochorionic twin pregnancies and their clinical consequences. L Lewi, J Deprest, K Hecher, Lewi, L., Deprest, J., Hecher, K., 2013. The vascular anastomoses in monochorionic twin pregnancies and their clinical consequences.

Globally optimal fetoscopic mosaicking based on pose graph optimisation with affine constraints. L Li, S Bano, J Deprest, A L David, D Stoyanov, F Vasconcelos, IEEE Robotics and Automation Letters. 6Li, L., Bano, S., Deprest, J., David, A.L., Stoyanov, D., Vasconcelos, F., 2021. Globally optimal fetoscopic mo- saicking based on pose graph optimisation with affine constraints. IEEE Robotics and Automation Letters 6, 7831-7838.

Feature pyramid networks for object detection. T Y Lin, P Dollár, R Girshick, K He, B Hariharan, S Belongie, IEEE Conference on Computer Vision and Pattern Recognition. Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S., 2017. Feature pyramid networks for ob- ject detection, in: IEEE Conference on Computer Vi- sion and Pattern Recognition, pp. 2117-2125.

A survey on deep learning in medical image analysis. G Litjens, T Kooi, B E Bejnordi, A A A Setio, F Ciompi, M Ghafoorian, J A Van Der Laak, B Van Ginneken, C I Sánchez, Medical Image Analysis. 42Litjens, G., Kooi, T., Bejnordi, B.E., Setio, A.A.A., Ciompi, F., Ghafoorian, M., Van Der Laak, J.A., Van Ginneken, B., Sánchez, C.I., 2017. A survey on deep learning in medical image analysis. Medical Im- age Analysis 42, 60-88.

Residual Anastomoses After Fetoscopic Laser Surgery in Twinto-Twin Transfusion Syndrome: Frequency, Associated Risks and Outcome. E Lopriore, J M Middeldorp, D Oepkes, F J Klumper, F J Walther, F P Vandenbussche, Placenta. 28Lopriore, E., Middeldorp, J.M., Oepkes, D., Klumper, F.J., Walther, F.J., Vandenbussche, F.P., 2007. Residual Anastomoses After Fetoscopic Laser Surgery in Twin- to-Twin Transfusion Syndrome: Frequency, Associated Risks and Outcome. Placenta 28, 204-208.

Bias: Transparent reporting of biomedical image analysis challenges. L Maier-Hein, A Reinke, M Kozubek, A L Martel, T Arbel, M Eisenmann, A Hanbury, P Jannin, H Müller, S Onogur, J Saez-Rodriguez, B Van Ginneken, A Kopp-Schneider, B A Landman, 10.1016/j.media.2020.101796Medical Image Analysis. 66Maier-Hein, L., Reinke, A., Kozubek, M., Mar- tel, A.L., Arbel, T., Eisenmann, M., Hanbury, A., Jannin, P., Müller, H., Onogur, S., Saez- Rodriguez, J., van Ginneken, B., Kopp-Schneider, A., Landman, B.A., 2020. Bias: Transparent re- porting of biomedical image analysis challenges. Medical Image Analysis 66, 101796. URL: https://www.sciencedirect.com/science/ article/pii/S1361841520301602, doi:https: //doi.org/10.1016/j.media.2020.101796.

Adaptive and generic corner detection based on the accelerated segment test. E Mair, G D Hager, D Burschka, M Suppa, G Hirzinger, European Conference on Computer Vision. SpringerMair, E., Hager, G.D., Burschka, D., Suppa, M., Hirzinger, G., 2010. Adaptive and generic corner de- tection based on the accelerated segment test, in: Eu- ropean Conference on Computer Vision, Springer. pp. 183-196.

Advances in fetal surgery. K M Maselli, A Badillo, Maselli, K.M., Badillo, A., 2016. Advances in fetal surgery.

Blood vessel segmentation algorithms-review of methods, datasets and evaluation metrics. S Moccia, E De Momi, S El Hadji, L S Mattos, Computer Methods and Programs in Biomedicine. 158Moccia, S., De Momi, E., El Hadji, S., Mattos, L.S., 2018. Blood vessel segmentation algorithms-review of methods, datasets and evaluation metrics. Computer Methods and Programs in Biomedicine 158, 71-91.

Unsupervised deep homography: A fast and robust homography estimation model. T Nguyen, S W Chen, S S Shivakumar, C J Taylor, V Kumar, IEEE Robotics and Automation Letters. 3Nguyen, T., Chen, S.W., Shivakumar, S.S., Taylor, C.J., Kumar, V., 2018. Unsupervised deep homography: A fast and robust homography estimation model. IEEE Robotics and Automation Letters 3, 2346-2353.

Retrieval and registration of longrange overlapping frames for scalable mosaicking of in vivo fetoscopy. L Peter, M Tella-Amo, D I Shakir, G Attilakos, R Wimalasundera, J Deprest, S Ourselin, T Vercauteren, International Journal of Computer Assisted Radiology and Surgery. 13Peter, L., Tella-Amo, M., Shakir, D.I., Attilakos, G., Wimalasundera, R., Deprest, J., Ourselin, S., Ver- cauteren, T., 2018. Retrieval and registration of long- range overlapping frames for scalable mosaicking of in vivo fetoscopy. International Journal of Computer As- sisted Radiology and Surgery 13, 713-720.

Computer-assisted surgical planning and intraoperative guidance in fetal surgery: A systematic review. R Pratt, J Deprest, T Vercauteren, S Ourselin, A L David, Pratt, R., Deprest, J., Vercauteren, T., Ourselin, S., David, A.L., 2015. Computer-assisted surgical planning and intraoperative guidance in fetal surgery: A systematic review.

U2-net: Going deeper with nested u-structure for salient object detection. X Qin, Z Zhang, C Huang, M Dehghan, O R Zaiane, M Jagersand, Pattern Recognition. 106107404Qin, X., Zhang, Z., Huang, C., Dehghan, M., Zaiane, O.R., Jagersand, M., 2020. U2-net: Going deeper with nested u-structure for salient object detection. Pattern Recognition 106, 107404.

Sequential selective laser photocoagulation of communicating vessels in twin-twin transfusion syndrome. R A Quintero, K Ishii, R H Chmait, P W Bornick, M H Allen, E V Kontopoulos, Journal of Maternal-Fetal and Neonatal Medicine. 20Quintero, R.A., Ishii, K., Chmait, R.H., Bornick, P.W., Allen, M.H., Kontopoulos, E.V., 2007. Sequential se- lective laser photocoagulation of communicating ves- sels in twin-twin transfusion syndrome. Journal of Maternal-Fetal and Neonatal Medicine 20, 763-768.

Mosaicing of endoscopic placenta images. M Reeff, F Gerhard, P Cattin, G Székely, INFOR-MATIK 2006 -Informatik fur Menschen, Beitrage der 36. Jahrestagung der Gesellschaft fur Informatik e.V. (GI). Reeff, M., Gerhard, F., Cattin, P., Székely, G., 2006. Mo- saicing of endoscopic placenta images, in: INFOR- MATIK 2006 -Informatik fur Menschen, Beitrage der 36. Jahrestagung der Gesellschaft fur Informatik e.V. (GI), pp. 467-474.

U-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerRonneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks for biomedical image segmen- tation, in: International Conference on Medical Im- age Computing and Computer-Assisted Intervention, Springer. pp. 234-241.

Deep-learned placental vessel segmentation for intraoperative video enhancement in fetoscopic surgery. P Sadda, M Imamoglu, M Dombrowski, X Papademetris, M O Bahtiyar, J Onofrey, Sadda, P., Imamoglu, M., Dombrowski, M., Pa- pademetris, X., Bahtiyar, M.O., Onofrey, J., 2019. Deep-learned placental vessel segmentation for intra- operative video enhancement in fetoscopic surgery.

Better feature matching for placental panorama construction. P Sadda, J A Onofrey, M O Bahtiyar, X Papademetris, Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics. Springer VerlagSadda, P., Onofrey, J.A., Bahtiyar, M.O., Papademetris, X., 2018. Better feature matching for placental panorama construction, in: Lecture Notes in Computer Science (including subseries Lecture Notes in Artifi- cial Intelligence and Lecture Notes in Bioinformatics), Springer Verlag. pp. 128-137.

Superglue: Learning feature matching with graph neural networks. P E Sarlin, D Detone, T Malisiewicz, A Rabinovich, IEEE Conference on Computer Vision and Pattern Recognition. Sarlin, P.E., DeTone, D., Malisiewicz, T., Rabinovich, A., 2020. Superglue: Learning feature matching with graph neural networks, in: IEEE Conference on Com- puter Vision and Pattern Recognition, pp. 4938-4947.

Endoscopic Laser Surgery versus Serial Amnioreduction for Severe Twin-to-Twin Transfusion Syndrome. M V Senat, J Deprest, M Boulvain, A Paupe, N Winer, Y Ville, New England Journal of Medicine. 351Senat, M.V., Deprest, J., Boulvain, M., Paupe, A., Winer, N., Ville, Y., 2004. Endoscopic Laser Surgery versus Serial Amnioreduction for Severe Twin-to-Twin Trans- fusion Syndrome. New England Journal of Medicine 351, 136-144.

LoFTR: Detector-free local feature matching with transformers. J Sun, Z Shen, Y Wang, H Bao, X Zhou, IEEE Conference on Computer Vision and Pattern Recognition. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X., 2021. LoFTR: Detector-free local feature matching with transformers, in: IEEE Conference on Computer Vision and Pattern Recognition, pp. 8922-8931.

Efficientnet: Rethinking model scaling for convolutional neural networks. M Tan, Q Le, PMLR.International Conference on Machine Learning. Tan, M., Le, Q., 2019. Efficientnet: Rethinking model scaling for convolutional neural networks, in: Interna- tional Conference on Machine Learning, PMLR. pp. 6105-6114.

A Combined em and Visual Tracking Probabilistic Model for Robust Mosaicking: Application to Fetoscopy. M Tella, P Daga, F Chadebecq, S Thompson, D I Shakir, G Dwyer, R Wimalasundera, J Deprest, D Stoyanov, T Vercauteren, S Ourselin, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. IEEE Computer SocietyTella, M., Daga, P., Chadebecq, F., Thompson, S., Shakir, D.I., Dwyer, G., Wimalasundera, R., Deprest, J., Stoy- anov, D., Vercauteren, T., Ourselin, S., 2016. A Combined em and Visual Tracking Probabilistic Model for Robust Mosaicking: Application to Fetoscopy, in: IEEE Computer Society Conference on Computer Vi- sion and Pattern Recognition Workshops, IEEE Com- puter Society. pp. 524-532.

Probabilistic visual and electromagnetic data fusion for robust drift-free sequential mosaicking: application to fetoscopy. M Tella-Amo, L Peter, D I Shakir, J Deprest, D Stoyanov, J E Iglesias, T Vercauteren, S Ourselin, Journal of Medical Imaging. 5Tella-Amo, M., Peter, L., Shakir, D.I., Deprest, J., Stoy- anov, D., Iglesias, J.E., Vercauteren, T., Ourselin, S., 2018. Probabilistic visual and electromagnetic data fu- sion for robust drift-free sequential mosaicking: appli- cation to fetoscopy. Journal of Medical Imaging 5, 1.

Pruning strategies for efficient online globally consistent mosaicking in fetoscopy. M Tella-Amo, L Peter, D I Shakir, J Deprest, D Stoyanov, T Vercauteren, S Ourselin, Journal of Medical Imaging. 6Tella-Amo, M., Peter, L., Shakir, D.I., Deprest, J., Stoy- anov, D., Vercauteren, T., Ourselin, S., 2019. Pruning strategies for efficient online globally consistent mo- saicking in fetoscopy. Journal of Medical Imaging 6, 1.

Towards computer-assisted TTTS: Laser ablation detection for workflow segmentation from fetoscopic video. F Vasconcelos, P Brandão, T Vercauteren, S Ourselin, J Deprest, D Peebles, D Stoyanov, International Journal of Computer Assisted Radiology and Surgery. 13Vasconcelos, F., Brandão, P., Vercauteren, T., Ourselin, S., Deprest, J., Peebles, D., Stoyanov, D., 2018. To- wards computer-assisted TTTS: Laser ablation detec- tion for workflow segmentation from fetoscopic video. International Journal of Computer Assisted Radiology and Surgery 13, 1661-1670.

Towards scene adaptive image correspondence for placental vasculature mosaic in computer assisted fetoscopic procedures. L Yang, J Wang, T Ando, A Kubota, H Yamashita, I Sakuma, T Chiba, E Kobayashi, 10.1002/rcs.1700The International Journal of Medical Robotics and Computer Assisted Surgery. 12Yang, L., Wang, J., Ando, T., Kubota, A., Yamashita, H., Sakuma, I., Chiba, T., Kobayashi, E., 2016. To- wards scene adaptive image correspondence for pla- cental vasculature mosaic in computer assisted feto- scopic procedures. The International Journal of Medi- cal Robotics and Computer Assisted Surgery 12, 375- 386. doi:10.1002/rcs.1700.

Unet++: A nested u-net architecture for medical image segmentation, in: Deep learning in medical image analysis and multimodal learning for clinical decision support. Z Zhou, M M Rahman Siddiquee, N Tajbakhsh, J Liang, SpringerZhou, Z., Rahman Siddiquee, M.M., Tajbakhsh, N., Liang, J., 2018. Unet++: A nested u-net architec- ture for medical image segmentation, in: Deep learning in medical image analysis and multimodal learning for clinical decision support. Springer, pp. 3-11.