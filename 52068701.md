# A Survey of Standardized Approaches towards the Quality of Experience Evaluation for Video Services: An ITU Perspective

CorpusID: 52068701
 
tags: #Engineering, #Computer_Science

URL: [https://www.semanticscholar.org/paper/0f3dfd09e6dfd8124c90f9ba306d4e24a0cec534](https://www.semanticscholar.org/paper/0f3dfd09e6dfd8124c90f9ba306d4e24a0cec534)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

A Survey of Standardized Approaches towards the Quality of Experience Evaluation for Video Services: An ITU Perspective
27 May 2018

Debajyoti Pal debajyoti.pal@gmail.com 0000-0002-7305-1015
School of Information Technology
IP Communications Laboratory
King Mongkut's University of Technology Thonburi
10140BangkokThailand

Tuul Triyason 
School of Information Technology
IP Communications Laboratory
King Mongkut's University of Technology Thonburi
10140BangkokThailand

A Survey of Standardized Approaches towards the Quality of Experience Evaluation for Video Services: An ITU Perspective
27 May 20186E7168825DCF286B53CA195942A7A63B10.1155/2018/1391724Received 29 January 2018; Revised 8 March 2018; Accepted 21 March 2018;3G4GWiFi Region I Production network Region II Distribution network Region III Consumer network
Over the past few years there has been an exponential increase in the amount of multimedia data being streamed over the Internet.At the same time, we are also witnessing a change in the way quality of any particular service is interpreted, with more emphasis being given to the end-users.Thus, silently there has been a paradigm shift from the traditional Quality of Service approach (QoS) towards a Quality of Experience (QoE) model while evaluating the service quality.A lot of work that tries to evaluate the quality of audio, video, and multimedia services over the Internet has been done.At the same time, research is also going on trying to map the two different domains of quality metrics, i.e., the QoS and QoE domain.Apart from the work done by individual researchers, the International Telecommunications Union (ITU) has been quite active in this area of quality assessment.This is obvious from the large number of ITU standards that are available for different application types.The sheer variety of techniques being employed by ITU as well as other researchers sometimes tends to be too complex and diversified.Although there are survey papers that try to present the current state of the art methodologies for video quality evaluation, none has focused on the ITU perspective.In this work, we try to fill up this void by presenting up-to-date information on the different measurement methods that are currently being employed by ITU for a video streaming scenario.We highlight the outline of each method with sufficient detail and try to analyze the challenges being faced along with the direction of future research.

## Introduction

There has been a rapid advance in various video services and its applications, like video telephony, High-Definition (HD) and Ultrahigh-Definition (UHD) television, Internet protocol television (IPTV), and mobile multimedia streaming in recent years.Thus, quality assessment of videos that are being streamed and watched online has become an area of active research.As per a report published in [1][2][3], video streaming over the Internet is becoming increasingly popular and accounts for more than 55% of the overall traffic.A lot of work has been done by several researchers towards the quality assessment of streaming multimedia services [4][5][6][7][8].At the same time, organizations like the International Telecommunication Union (ITU) also have in place different models and standardization efforts towards the perceived video quality evaluation under a variety of application scenarios.The main objective of this paper is to provide an up-to-date review of this research field from a standard ITU perspective.

Figure 1 shows a typical video streaming scenario over the Internet.Broadly, three distinct regions are identified as the production network (head-end), the distribution network (carrier), and the consumer network (tail end).Relevant contents are created, edited, encoded, and stored in suitable multimedia databases ready to be transported to the endusers (consumer network) over the Internet with the help of streaming servers.This multimedia traffic has to pass through the unreliable Internet (distribution network) where they are fragmented into various IP segments and ultimately delivered to the consumer end where they are displayed on a variety of devices like television, computers, or mobile phones.The inherent unreliable service provided by the Internet necessitates the use of perceptual quality evaluation schemes for such video traffic.We segregate the multimedia streaming scenario presented in Figure 1 to two different types based upon the ownership use case of the Internet as the Internet protocol television (IPTV) service and over-the-top (OTT) streaming service.YouTube, Netflix, Hulu, etc. are prime examples of the OTT service.IPTV runs on a private, fully controlled network and hence has the advantage of tight control and guaranteed (overprovisioned) bandwidth [9].IPTV typically uses the User Datagram Protocol (UDP) at the transport layer, and hence in case of any packet loss retransmission does not happen.Still, the reliability of IPTV service is generally high because the video traffic is being carried over a fully controlled network (usually private).On the contrary, in case of OTT services, the contents are streamed over the open and unmanaged public Internet.Thus, IPTV services utilize a network that guarantees a Quality of Service (QoS), which differentiates them from the other OTT services.Quality of Experience (QoE) provisioning for OTT services is a far more challenging job as compared to IPTV services.Hence, for this work we focus only on those ITU standards that do not include IPTV services.More specifically, we focus on video streaming over the public Internet only.

The main goal of this article is to summarize the current and other emerging approaches of video quality evaluation of a streaming service within the scope of ITU.Often due to the sheer variety of the different ITU standards, it becomes difficult for a new researcher to select a suitable method.This work aims to bridge the aforementioned gap by carefully analyzing the relevant ITU standards in detail and giving suitable recommendations as to which standard to choose for a specific context.

We begin by presenting the concepts related to QoS and QoE in Section 2 along with the interrelationship between them.Sections 3 and 4 present the review of subjective and objective methods, respectively.In Section 5, we discuss the current challenges in video quality measurement and the future trends.Finally, Section 6 provides the conclusion.


## QoS and QoE

We begin the survey process by explaining the key concepts of QoS and QoE explicitly highlighting their differences.


### QoS Concepts

2.1.1.QoS Definition.QoS has been defined by ITU-T as "totality of characteristics of a telecommunications service that bear on its ability to satisfy stated and implied needs of the user of the service" [15].This definition of QoS is extremely generic in nature and needs to be reapplied in a specific application context.Figure 2 shows the concept of end-to-end QoS that is commonly prevalent in almost all scenarios.Terminal equipment refers to the devices that are used either by the service provider or by the consumer in order to provide/avail a particular service.Access network is a combination of the access medium and technology used for a particular service (e.g., wireless, cable, ADSL).Access network generally belongs to a specific service provider.Core network refers to the IP backbone network, which is usually controlled by different stakeholders.The QoS contribution of the core network is governed by the technology used (digital multiplexing, IP, etc.) and transmission media (air, cable, optical, etc.) along with other factors.While specifying the end-to-end QoS, it is necessary to state the specified operating conditions in which a service is supported over a connection (connectionless or connection-oriented) scheme.QoS is also affected by factors like traffic and routing [16].Each of the elements presented in Figure 2 affects the QoS in its own way.In addition, it is evident that QoS comprises both network performance (NP) and non-network-related factors.Bit-error rate, latency, and jitter are some of the NP related factors, while tariff levels, service-repair time, etc. are the non-network parameters.Four different angles from which QoS can be viewed are discussed next.


#### Viewpoints of QoS and Their Interrelationship.

We can classify the different perspectives of QoS into four different types as shown in Figure 3.

(i) Customer's QoS requirement refers to the quality level of any application that is expected by the end-users and expressed in nontechnical terms.The customer is not bothered about how a service is offered or about the internals of the network/application design; rather the focus is on the overall end-to-end quality.

(ii) QoS offered by the provider refers to the level of service quality that the provider is expected to provide to the customers.The level of quality is expressed by values assigned to QoS parameters.Primarily this is used for planning purpose and framing of Service Level Agreements (SLA) between the provider and the customer.

(iii) QoS achieved by the provider refers to the quality level of the service that the provider actually delivers to the customer, which ideally should be the same as the QoS offered by the provider.In reality, the values are different and the performance is compared across the two groups over a certain period.

(iv) QoS perceived by the customer refers to the satisfaction level that the customer "believes" to have experienced.This is usually assessed from data gathered through customer surveys or individual assessment by a customer for the service.

The four viewpoints are interconnected as shown in Figure 3. Logically, the process starts at the customer's QoS requirement stage.These requirements act as input suggestions to the service provider who plans to offer the desired level of quality.Most of the time, the planned level of service quality is not met due to several factors.As discussed before, these factors are primarily NP related ones like packet loss, jitter, latency, and throughput.A tradeoff between the cost incurred to deliver the ideal quality and the viability of the overall business model has to be done, which affects the service quality in general.The service is ultimately delivered to the customers who perceive the real quality that is achieved by the provider.

From the above discussion, it is clear that the customer viewpoint is the most important one for any service to be successful.This is exactly the reason why ITU has a separate recommendation in [11] that defines a model for multimedia QoS categories from an end-user viewpoint.Next, a brief overview of this recommendation is provided.


#### QoS Requirements of Different Application

Types.Different types of applications are identified like voice, video, and web browsing, with each having different performance requirements for achieving a good perceived quality.Figure 4 shows a classification based upon the overall requirements of the applications in terms of two important QoS parameters, namely, packet loss and one-way delay.

The applications have been classified into eight distinct groups.Some applications such as conversational voice and video are sensitive to delay, but can tolerate a certain extent of packet loss.On the other hand, applications like Fax are sensitive to packet loss, but can withstand delay to a certain extent.Other interactive applications like online gaming are extremely sensitive to both packet loss and delay.These facts are presented in a more clear fashion in Figure 5.The figure shows four distinct delay types depending upon the extent of user interaction involved.

The recommended range of QoS values for some important applications have been provided in Table 1 [11].The target values of certain applications like audio streaming, videophone, and video streaming are outdated as of 2018.For example, in case of video streaming the typical data rates


## Non-critical (delay >> 10 s)

Figure 5: QoS requirements for different applications [11].can easily shoot up to the order of tens of Mbps instead of 384 kbps due to an increase in the network throughput as well as the video resolutions [17].Similarly, with the advent of modern techniques like dynamic adaptive streaming over HTTP (DASH based streaming), the upper and lower bounds of the other QoS parameters like jitter, one-way delay, and packet loss also need to be updated.


### QoE Concepts

2.2.1.QoE Definition.QoE is defined as the degree of delight or annoyance of the user of an application or service [18,19].The concept of QoE is closely related to the human auditory and visual system (HAS and HVS, respectively) and the overall satisfaction that the end-user has in using such a service.Thus, QoE also refers to a complete end-to-end experience that has been shown previously in Figure 2. It is obvious that for any service to succeed, it must provide a good experience to the end-users.A lot of work is being done by ITU towards the quality assessment of various application types.For this article, however, we concentrate only on the video streaming applications.Next, a general overview of the different QoE assessment methodologies being employed by ITU has been provided.


#### QoE Assessment Methodologies.

Confining the scope of this work to video streaming only, Figure 6 shows an overview of the different QoE assessment methodologies being currently used by ITU.Irrespective of the methodology used, the QoE assessment technique must be valid and reliable.The concept of validity versus reliability has been shown in Figure 7. Validity describes how well a method measures what it is intended to measure, while reliability refers to the accuracy of a method in terms of scattering of results (for example, when a test assessment is repeated) [20].

The end-user experience can be measured using two broad techniques: subjective and objective tests [12].Subjective tests that involve human subjects are considered the most accurate means of quality estimation.Objective tests on the  other hand use some mathematical formulae or algorithms to predict the quality.Despite the accuracy of the objective methods being lesser than the subjective ones, they are preferred in many situations as they are automatic, i.e., easy and faster to be carried out and much cheaper than the subjective tests.One way to categorize the objective methods is by a general approach, which lists down the different application scenarios in which a particular objective model can be used.There are three specific use-cases as mentioned below:

(i) Monitoring: in which a particular objective model is used for live quality assessment of a video application.This is a real time usage scenario that assesses the video quality, e.g., ITU-T P.1202.

(ii) Planning: in which an objective model can be used for network planning before an actual service startup.underlying network is subjected to packet loss, e.g., ITU-T G.1071.(iii) Lab testing: in which quality assessment is done in a typical laboratory setup.This type of approach is used when commercially it is not feasible to assess the quality or in certain situations that require the presence of the original source signal for the purpose of quality measurement or during the development and testing of particular equipment, e.g., ITU-T J.341.

In the second approach, the objective methods are classified based upon the type of measurement used as follows:

(i) Media layer model: this uses actual audio/video signals as their input.They also take into account codec compression and the channel characteristics.These types of models can further be subdivided into three different types depending upon the extent of the original reference signal that they have for quality assessment:

(1) Full reference (FR) methods in which a reference video is compared frame-by-frame with a distorted video sequence in order to obtain the quality.The comparison can be from many aspects like color processing, spatial and temporal features, contrast features, etc.  layer and packet layer models.Figure 10 shows the conceptual view of a bitstream layer model.These are ideal for live quality monitoring purpose, e.g., ITU-T P.1202.

A third approach to QoE assessment known as the hybrid method uses a combination of the subjective and objective techniques [21,22].In this method, typically at the beginning a subjective test is carried out to gather the opinion from the people regarding the quality of the test video sequences under consideration.These test-videos are impaired by one or more QoS factors (NP or non-NP related) depending upon the experimental scenario and requirements.Thereafter, mathematical techniques like linear or nonlinear regression, different types of neural networks, or other machine learning algorithms are used for creating a quality prediction model based upon the subjective scores.This approach tries to take into account the advantages of both the subjective and objective techniques [23], e.g., ITU-T G.1070.


### The Relationship between QoS and QoE.

After an elaborate explanation of QoS and QoE from an ITU perspective, now we present the interdependence between them.A possible relationship between the two has been shown in Figure 11.The QoS-QoE relationship has been separated into three distinct zones.Zone I (marked in green) shows the ideal region where the perceived video QoE should be.


## The users experience an excellent viewing quality. A certain

QoS level needs to be maintained (corresponding to point "a" on the graph) in order to achieve this QoE.This point "a" represents the ideal threshold QoS level (in terms of packet loss, jitter, network throughput, or other factors) that should be maintained theoretically by all the concerned stakeholders.Zone II shows a diminishing QoE region where further deterioration in the QoS values results in a sharp drop in QoE.The point "b" on the graph represents the actual threshold value below which the user will probably stop using the service.There is no exact relationship that models this region of diminishing QoE [24,25].However, a number of ITU recommendations like ITU-T G.1070, ITU-T G.1071, and ITU-T P.1201 attempt to model this scenario.Zone 3 (marked in red) shows the region where the QoE is extremely poor and should be avoided under all circumstances.

The taxonomy of all the ITU recommendations related to video streaming that have been covered in this survey is shown in Figure 12.


## Subjective Methodologies

In this section, we present the relevant subjective methods that are used for video streaming applications.


### ITU-T Recommendation P.910.

Noninteractive subjective assessment methods for evaluating the one-way overall video quality of multimedia applications such as videoconferencing, storage, and retrieval applications have been covered in ITU Recommendation P.910 [26].The number of subjects in the tests varies from 4 to 40.


#### Overall Experiment Design.

The test is usually carried out in a recording environment that has sufficient lighting.The lighting conditions should be representative of a typical office scenario rather than studio lighting.Specifically, the ambient lighting of the room should be between 100 lux and 10,000 lux.

The reference video sequences that are used for showing to the human subjects are extremely important.Perceived video quality depends largely on the type of video content [27][28][29][30].Hence, while selecting the reference sequences, spatial information (SI) and temporal information (TI) are two critical factors that must be taken into account.SI gives an indication to the amount of spatial details that each frame has and it has a higher value for more spatially complex scenes.The SI value for every video frame is calculated by filtering each one of them using the Sobel filter followed by computing the standard deviation.The maximum value in the frame represents the SI content of the scene.Similarly, TI values give an indication of the amount of temporal changes in a particular video sequence and it has a higher value for sequences having greater amount of motion.Equations ( 1) and ( 2) show the calculation of the SI and TI values, respectively:
SI = max time {std space [Sobel (𝐹 𝑛 )]}(1)TI = max time {std space [𝐹 𝑛 (𝑖, 𝑗) − 𝐹 𝑛−1 (𝑖, 𝑗)]} ,(2)
where   is the video frame at time , std space the standard deviation across all the pixels for each filtered frame, and max time the corresponding maximum value in the considered time interval.

Figure 13 shows the SI and TI values of some commonly used video sequences [14].The publicly available video database of VQEG is used most frequently while selecting the reference videos [31].The relevant video details are given in Table 2. Table 3 summarizes the viewing conditions that must be satisfied.Normally, at-least 4 different types of video sequences should be used in a particular test.

Next, we present a brief overview of the different methods that are used by this recommendation.(i) ACR method: here the distorted test sequences are presented one at a time and the users give opinion scores (typically on a scale of 1 to 5), which are averaged into a Mean Opinion Score (MOS) [32].Table 4 shows the MOS scale.The timing diagram of the stimulus presentation has been shown in Figure 14(a).The users are shown video sequences, which typically last for 10 seconds followed by a voting time interval of 10 seconds approximately, wherein the subjects need to enter their opinion in the form of MOS scores.The video presentation time can be increased or decreased depending on the test sequences.

(ii) ACR-HR method: it is similar to the ACR method, with an exception that the reference version of each presented distorted test sequence is also shown to the subjects.This is referred to as the hidden reference condition.The subjects give their opinion in the form of MOS scores.However, for final quality assessment a differential quality score (DMOS) is computed for each distorted sequence and its corresponding reference one as per the following equation:
DMOS = MOS 𝑆 − MOS 𝑅 + 5,(3)
where MOS  represents the MOS of a particular distorted video sequence and MOS  represents the   MOS of its corresponding reference sequence.DMOS is also measured on a scale of 1 to 5 identical to MOS.If the distorted video sequence has a better quality than its corresponding reference one, the DMOS value will be greater than 5, which is valid and indicative of an excellent quality (better than the reference one).Similarly, when the values of MOS  and MOS  are the same, the DMOS value is maximum, i.e., 5, indicating no perceptual difference in quality between the distorted and the reference video sequences.The timing diagram is the same as the ACR method.
！ Ｃ ！ j ＂ Ｅ ＂ Ｆ (c)
(iii) DCR method: in this type, the test sequences are presented in pairs.In a pair, the reference sequence is always shown first followed by the distorted sequence.

The timing diagram for this type of method has been shown in Figure 14  and stop at the same frame.In this case, the subjects are asked to rate the distorted sequences with respect to the reference on a 5-point scale.Table 5 presents the 5-level opinion scale.

(iv) PC method: in this method, the test sequences are presented in pairs like DCR.However, none of the sequences in the pair is a reference sequence.Instead, all the distorted sequences are combined in all possible combinations and then presented in pairs to the subjects.After each presentation, a judgment is made by the subject on which is the preferred sequence in the pair.The timing diagram has been shown in Figure 14(c).


#### Comparison of the Test Methods.

The most crucial decision is to choose the right technique for a particular application.Normally, the choice is between applications that require or do not require the presence of the reference sequences.The DCR method should be chosen when testing the fidelity of transmission with respect to the reference signal.ACR is easy, fast to implement, and hence commonly used.The basic advantage of ACR-HR over ACR is that the memory effect of the reference sequences can be removed from the subjective scores.PC method should be used when a high discriminatory power is required on the subjective scores.  of double stimulus methods, wherein both the reference and distorted video sequences must be presented simultaneously.


### ITU-R Recommendation

(i) SSCQE method: this is a single stimulus method that enables a continuous evaluation of the distorted video sequences on a scale that has been shown in Table 6.

The items are normalized in a range of 0 to 100.Generally, each video sequence lasts for at-least 5 minutes.

(ii) DSIS method: this is a type of cyclic method in the sense that the subject is at first presented with the original sequence and then with the same impaired sequence.Each sequence is generally reproduced either one (variant 1) or two times (variant 2), after which the subject evaluates the distorted video sequence using an opinion scale that has been shown in Table 5. Interpretations for both the DCR and DSIS methods are the same.The timing diagrams for variants 1 and 2 are shown in Figures 15 and  16, respectively.For both the variants, the subjects need to watch the video sequence during the time slots  1 and  3 and voting is permitted only in  4 .Time slots  1 and  3 are approximately of 10 seconds duration each, with  2 being around 3-second pause/gap period and  4 lasting for 5-11 seconds. 1 time slot shows the reference sequence, followed by the distorted sequence in  3 .

(iii) DSCQS method: this is also a type of cyclic method in which the subject is asked to view a pair of video sequences consecutively, with both of them being from the same source, but one being the original reference sequence and the other one the distorted version of the same source.The subjects assess the quality of both the sequences on a continuous scale that has been shown in Table 6.In this case, the subjects do not know that whether a particular sequence is a reference one or the distorted version.The  general timing diagram of the stimulus presentation for DSCQS method is the same as the second variant of the DSIS method (shown in Figure 16).However, the interpretations of the time slots  1 ,  2 ,  3 , and  4 are different.In time slots  1 and  3 the test sequences are presented (in no particular order and generally changed across different sequences in a pseudorandom fashion) while in slot  4 the voting is done. 2 represents a short gap period between  1 and  3 .Recommended values for the four time slots are the same as those in the case of the DSIS method.

(iv) SDSCE method: in this procedure, the subjects are allowed to watch two video sequences simultaneously, where one sequence is the reference and the other one its distorted counterpart.Generally, both the sequences are shown side by side and the subjects know which is the reference sequence and which is its distorted version.This method is generally used for judging the fidelity of the video information.It is recommended when the video sequences are of longer duration (at-least 5 minutes) and uses the same scale that has been presented in Table 6.

(v) SCACJ method: this is an example of a stimulus comparison method and similar to the double stimulus methods discussed above.However, the only difference is that the reference sequences are not shown in this case and only the distorted sequences are presented to the subjects.The subject has to rate the quality of the second video in comparison to the first one based upon the scale which has been shown in Table 7. Reliability of subjects is one of the crucial factors that affect the quality of the results obtained from these subjective techniques.Human perception is often influence by factors like ambient room conditions, emotional and mental state of the subjects, personal profile (age, gender, etc.) that can affect the results obtained [35,36].


### ITU-T Recommendation

It is obvious from the above discussion that a number of different subjective techniques are available.Hence, for a new researcher it becomes rather confusing which method to select out of the numerous alternatives.In Table 8 we try to provide a guideline to the best subjective technique that should be considered depending upon certain requirements like video duration, presence/absence of reference videos, and need for video repetition.


## Objective Methodologies

In this section, we provide an overview of the objective models that are used for video streaming and listed in Figure 12.For each model, the overall methodology is discussed along with the mathematical relationships and algorithms wherever necessary.


### ITU-T Recommendation G.1070. This recommendation

proposes an algorithm that estimates the videophone quality and is specifically useful for the QoS/QoE planners [37].This multimedia model takes input from the network and application layers of the TCP/IP protocol stack.


#### Overall Model Framework.

The overall framework of the model has been shown in Figure 17.Certain video and speech quality parameters are given as inputs to the model and there are three main outputs:   (  ),   (  ), and MM  .  (  ) refers to the video quality influenced by the speech quality,   (  ) refers to the speech quality influenced by the  4 Table 4 Table 5 -Table 6 Table 5 Table 6 Table 6 Table 7 Common assumptions video quality, and MM  refers to the overall multimedia quality outputted by the model.In this survey for every recommendation, which produces a multimedia quality as output, we concentrate only on the video quality evaluation part.Therefore, our discussion will focus only on the video quality   .Packet loss rate and jitter are the factors considered from the network layer, while bitrate, frame rate, codec type, and video format are the application layer factors.


## Speech related assumptions


## Video-related assumption


## Video quality parameters


## Speech quality parameters


## Video quality estimation function


## Speech quality estimation function


## Multimedia


#### General Model Equations. The overall video quality predicted by the model is given by
𝑉 𝑞 = 1 + 𝐼 Coding exp (− 𝑃 𝑝𝑙 𝑉 𝐷 𝑃𝑝𝑙 𝑉 ) ,(4)
where  Coding represents the basic video quality affected by the coding distortion,    expresses the degree of video quality robustness due to packet loss, and    denotes the packet loss percentage. Coding is further expressed as
𝐼 Coding = 𝐼 𝑂 𝑓𝑟 exp { { { − (ln (𝐹 𝑟𝑉 ) − ln (𝑂 𝑓𝑟 )) 2 2𝐷 𝐹𝑟𝑉 2 } } } ,(5)
where   is an optimal frame rate that maximizes the video quality at each video bitrate   and is expressed as
𝑂 𝑓𝑟 = V 1 + V 2 𝐵 𝑟𝑉 , 1 ≤ 𝑂 𝑓𝑟 ≤ 30, V 1 and V 2 constants,(6)
where   =   ,  Coding =   ,   represents maximum video quality at each video bitrate   and is expressed as
𝐼 𝑂𝑓𝑟 = V 3 − V 3 1 + (𝐵 𝑟𝑉 /V 4 ) V 5 , 0 ≤ 𝐼 𝑂𝑓𝑟 ≤ 4, V 3 , V 4 and V 5 constants.(7)
  represents the degree of video quality robustness due to frame rate   and is expressed as
𝐷 𝐹𝑟𝑉 = V 6 + V 7 𝐵 𝑟𝑉 , 0 < 𝐷 𝐹𝑟𝑉 , V 6 and V 7 constants. (8)
The packet loss robustness factor    introduced in ( 4) is expressed as
𝐷 𝑃𝑝𝑙 𝑉 = V 10 + V 11 exp (− 𝐹 𝑟𝑉 V 8 ) + V 12 exp (− 𝐵 𝑟𝑉 V 9 ) , 0 < 𝐷 𝑃𝑝𝑙 𝑉 .(9)
All the coefficients V 1 to V 12 are dependent on the codec type, the video format, and the video display size and need to be found out by carrying suitable subjective tests.Equation ( 4) highlights the fact that ITU-T G.1070 takes into account factors from the network as well as the application layer when evaluating the video quality.Therefore, this method is suitable when any new codec is to be tested for judging their performance.All the equations from (4) to ( 9) are generic in nature and show how this technique can be ported to a specific context (like evaluating the performance of a new codec along with the network QoS factors) by evaluating the coefficients V 1 to V 12 .ITU has validated this model only for a limited number of codecs (MPEG-2 and MPEG-4) across VGA, QVGA, and QQVGA resolutions [38,39].However, following the procedure that has been outlined through (4)- (9), this model has been extended to other recent codecs like H.265/HEVC and VP9 also [40,41].[42].Two application areas are addressed by this objective technique: a high-resolution area including IPTV and a low-resolution area including services like mobile TV.For reasons that we discussed previously, this survey presents only the mobile streaming application that is an IP based service.This algorithmic model tries to estimate the impact of typical IP layer impairments on the end-user QoE over transport formats such as Real Time Transport Protocol (RTP) over User Datagram Protocol (UDP), Motion Picture Experts Group-2 Transport Stream (MPEG2-TS) over UDP or RTP/UDP, and 3rd Generation Partnership Project Packet-Switched Steaming Service (3GPP-PSS) over RTP.Dynamic adaptive streaming over HTTP or DASH streaming that is currently being used by commercial services like YouTube and Netflix is not taken into account by this model.


### ITU-T Recommendation G.1071. This recommendation provides an opinion model for network planning of video and audio streaming applications


#### Overall Model

Framework.The overall model framework has been shown in Figure 18.The general way by which this model works is similar to [43] with an exception in the input that it takes.While as input this model takes into account different network planning parameters like the video bitrate, video codec type, video resolution, and the packet loss rate, the one described in [43] uses the IP packet header information to extract relevant parameters for predicting the video quality.Since the primary video quality estimation block is the same for both models, a conversion rule is applied for those planning parameters that are not taken into account by [43] in order to make it compatible.As output, this model provides three parameters:

(i) audiovisual Quality (  ) on a scale of 1-5, (ii) video only MOS (  ) on a scale of 1-5 (without audio stream), (iii) audio only MOS (  ) on scale of 1-5 (without video stream).

Here


## MOS VR is video MOS in case of no packet loss but only rebuffering (video quality due to rebuffering).

An elaborate methodology to calculate the three different types of MOS  has been provided in [42].However, in order to highlight the factors that this model takes into consideration and motivate the readers to port this for codecs that have not been tested by ITU yet, we present a snapshot of the calculation process by introducing three different algorithms.The same procedure can be applied to any other codec for evaluating the video quality.This ITU model is primarily used for planning purposes only.Since it does not take into account any reference video, it is an example of a NR scheme.

MOS VC is calculated as per Algorithm 1.For this algorithm,  CCF represents the video content complexity factor, i.e., the spatiotemporal complexity of the video sequence and International Journal of Digital Multimedia Broadcasting (1) set
𝑀𝑂𝑆 𝑀𝐴𝑋 = 5 (2) set 𝑀𝑂𝑆 𝑀𝐼𝑁 = 1 (3) set 𝑉 𝐷𝐶 = 0 (4) if (V𝑖𝑑𝑒𝑜𝐹𝑟𝑎𝑚𝑒𝑅𝑎𝑡𝑒 ≥ 24) (5) compute 𝑉 𝐷𝐶 = 𝑀𝑂𝑆 𝑀𝐴𝑋 − 𝑀𝑂𝑆 𝑀𝐼𝑁 1 + (𝑉 𝑁𝐵𝑅 /V 3 × 𝑉 𝐶𝐶𝐹 + V 4 ) (V 5 ×𝑉 𝐶𝐶𝐹 +V 6 ) (6) compute 𝑀𝑂𝑆 𝑉𝐶 = 𝑀𝑂𝑆 𝑀𝐴𝑋 − 𝑉 𝐷𝐶 (7) else (8) compute 𝑉 DC = 𝑀𝑂𝑆 𝑀𝐴𝑋 − 𝑀𝑂𝑆 𝑀𝐼𝑁 1 + (𝑉 𝑁𝐵𝑅 /V 3 × 𝑉 𝐶𝐶𝐹 + V 4 ) (V 5 ×𝑉 𝐶𝐶𝐹 +V 6 ) (9) compute 𝑀𝑂𝑆 𝑉𝐶 = (𝑀𝑂𝑆 𝑀𝐴𝑋 − 𝑉 𝐷𝐶 ) × (1 + V 1 × 𝑉 𝐶𝐶𝐹 − V 2 × 𝑉 𝐶𝐶𝐹 × log ( 1000 V𝑖𝑑𝑒𝑜𝐹𝑟𝑎𝑚𝑒𝑅𝑎𝑡𝑒
))
(10) end if
Algorithm 1: Calculation of MOS VC .

(
) set 𝑀𝑂𝑆 𝑀𝐼𝑁 = 1 (2) set 𝑉 𝐷𝑃 = 0 (3) denote 𝑠𝑐𝑒𝑛𝑒 = (𝑠𝑙𝑖𝑐𝑖𝑛𝑔 𝑂𝑅 𝑓𝑟𝑒𝑒𝑧𝑖𝑛𝑔) (4) if (𝑠𝑐𝑒𝑛𝑒 = 𝑠𝑙𝑖𝑐𝑖𝑛𝑔) (5) compute 𝑉 𝐷𝑃 = (𝑀𝑂𝑆 𝑉𝐶 − 𝑀𝑂𝑆 𝑀𝐼𝑁 ) × (𝑉 𝐴𝐼𝑅𝐹×𝑉 𝐼𝑅 / (V 7 × 𝑉 𝐶𝐶𝐹 + V 8 )) V 9 × (𝑉 𝑃𝐿𝐸𝐹 / (V 10 × 𝑉 𝐶𝐶𝐹 + V 11 )) V 12 1 + (𝑉 𝐴𝐼𝑅𝐹×𝑉 𝐼𝑅 / (V 7 × 𝑉 𝐶𝐶𝐹 + V 8 )) V 9 × (𝑉 𝑃𝐿𝐸𝐹 / (V 10 × 𝑉 𝐶𝐶𝐹 + V 11 )) V 12 (6) else (7) compute 𝑉 𝐷𝑃 = (𝑀𝑂𝑆 𝑉𝐶 − 𝑀𝑂𝑆 𝑀𝐼𝑁 ) × (𝑉 𝐼𝑅 / (V 7 × 𝑉 𝐶𝐶𝐹 + V 8 )) V 9 × (𝑉 𝑃𝐿𝐸𝐹 / (V 10 × 𝑉 𝐶𝐶𝐹 + V 11 )) V 12 1 + (𝑉 𝐼𝑅 / (V 7 × 𝑉 𝐶𝐶𝐹 + V 8 )) V 9 × (𝑉 𝑃𝐿𝐸𝐹 / (V 10 × 𝑉 𝐶𝐶𝐹 + V 11 )) V 12 (8) end if (9) compute 𝑀𝑂𝑆 𝑉𝑃 = 𝑀𝑂𝑆 𝑉𝐶 − 𝑉 𝐷𝑃 Algorithm 2: Calculation of MOS VP .1
it can vary from an initial default value of 0.5 to a maximum value of 1.It has to be calculated for every sequence used. NBR represents the normalized video bitrate in kbps and depends upon the video frame rate.The coefficients V 1 to V 6 are provided by ITU for H.264 and MPEG4 encoded video sequences at QCIF, QVGA, and HVGA resolutions only.

The procedure for calculating MOS VP is given in Algorithm 2.  DP represents the video quality distortion due to packet loss, which can lead to either a slicing or video freezing scenario.Depending upon the scenario  DP is calculated appropriately. AIRF represents the average impairment rate of the video frames whereas  IR represents the impairment rate of the entire video stream itself.Both of these values lie between 0 and 1, with 0 depicting the best and 1 the worst case. PLEF represents the video packet loss event frequency, which is incremented by 1 each time a slicing or freezing event occurs.V 7 to V 12 are the coefficients provided by ITU for the same set of conditions as discussed before.

Algorithm 3 summarizes the procedure for calculation of MOS VR .NRE represents the number of rebuffering events, ARL represents the average rebuffering length, and MREEF represents the multiple rebuffering events effect factor.The coefficients V 13 to V 18 are obtained in the same fashion as discussed before for the other coefficients.


### ITU-T Recommendation P

.1201/P.1201.1.This recommendation provides a parametric nonintrusive assessment of audiovisual media streaming quality [43].This is a nonintrusive model based upon the packet header information, which provides certain algorithms for evaluating the audiovisual quality of IP based video services.The packet header information is fed to the algorithm in a Packet Capture Format (PCAP).

This model has 2 subparts: ITU-T P.1201.1 and ITU-T P.1201.2[44,45].While the first one is intended for lowresolution application areas like mobile TV, the second one targets a high-resolution IPTV service.As output, the algorithm estimates the audio, video, and combined audiovisual quality in terms of the 5-point MOS scale.

Primarily, these models are used for in-service monitoring of perceived transmission quality or for maintenance purpose.As such they can be deployed either at the endpoints of the transmission system, i.e., the service provider or customers premises, or in the middle of the network as monitoring points.This model works only for a UDP based streaming service.An alternative version has been proposed in [46] that uses TCP for a nonadaptive and progressive download type media streaming.Table 9 summarizes the (1)  main input types and scope of this model.For any other specific factor or technology used that has not been mentioned in Table 9, the model needs to be retrained and revalidated.
set 𝑀𝑂𝑆 𝑀𝐼𝑁 = 1 (2) set 𝑉 𝐷𝑅 = 0 (3) set 𝑉𝑖𝑑𝑒𝑜 𝑄𝑢𝑎𝑙𝑖𝑡𝑦 = 0 (4) denote 𝑠𝑐𝑒𝑛𝑒 = (𝑟𝑒𝑏𝑢𝑓𝑓 𝐴𝑁𝐷 𝑝𝑎𝑐𝑘𝑒𝑡 𝑙𝑜𝑠𝑠) 𝑂𝑅 𝑟𝑒𝑏𝑢𝑓𝑓 (5) if (𝑠𝑐𝑒𝑛𝑒 = 𝑟𝑒𝑏𝑢𝑓𝑓 𝐴𝑁𝐷 𝑝𝑎𝑐𝑘𝑒𝑡 𝑙𝑜𝑠𝑠) (6) set 𝑉𝑖𝑑𝑒𝑜 𝑄𝑢𝑎𝑙𝑖𝑡𝑦 = 𝑀𝑂𝑆 𝑉𝑃 (7) else (8) set 𝑉𝑖𝑑𝑒𝑜 𝑄𝑢𝑎𝑙𝑖𝑡𝑦 = 𝑀𝑂𝑆 𝑉𝐶 (9) end if (10) compute 𝑉 𝐷𝑅 = (𝑉𝑖𝑑𝑒𝑜 𝑄𝑢𝑎𝑙𝑖𝑡𝑦 − 𝑀𝑂𝑆 𝑀𝐼𝑁 ) × (𝑁𝑅𝐸/V 13 ) V 14 × (𝐴𝑅𝐿/V 15 ) V 16 × (𝑀𝑅𝐸𝐸𝐹/V 17 ) V 18 1 + (𝑁𝑅𝐸/V 13 ) V 14 × (𝐴𝑅𝐿/V 15 ) V 16 × (𝑀𝑅𝐸𝐸𝐹/V 17 ) V 18 (11) compute 𝑀𝑂𝑆 𝑉𝑅 = 𝑉𝑖𝑑𝑒𝑜 𝑄𝑢𝑎𝑙𝑖𝑡𝑦 − 𝑉 𝐷𝑅 Algorithm 3: Calculation of MOS VR .
An overview of the model inputs and outputs has been shown in Figure 19.

The packet header information is obtained dynamically from the transport layer in a PCAP file format (interface I.2).Since this model is used for monitoring the video quality in real time, the transport layer input information (in the form of transport header) is dynamic by nature.Relevant information from this PCAP file is filtered out by interface I.3 and fed to the core MOS estimation module.Additional information about the media stream and the decoder behavior is taken out of band in a static manner with certain predefined values.This is the function of the interface I.1.Interface I.4 provides information about the rebuffering information that is extracted and measured at the end-points and provided as an input to the core MOS estimation module.

Three model outputs are provided: MOS A , MOS  , and MOS  referring to the audio only, video only, and combined audiovisual quality all in a MOS scale of 1-5.The overall block diagram of the ITU-T P.1201.1 model has been shown in Figure 20.

The parameter extraction modules for audio, video, and audiovisual scenarios are labeled as PEA, PEV, and PER, respectively.The procedure for calculating the overall video quality MOS  is the same that has been presented previously in Algorithms 1, 2, and 3.For video, only MOS  of the model attains a RMSE value of 0.535 (based on 1430 samples) and PCC value of 0.830.4.4.ITU-T Recommendation P.1202/P.1202.1.This recommendation is similar to the ITU-T P.1201 discussed above.However, in order to evaluate the perceived quality, this algorithm takes into account the bitstream information also, as well as the packet header information that has been used in the previous case [47].Similar to the previous algorithm, in this case also the model can be subdivided into two parts: ITU-T P.1202.1, which is targeted towards low-resolution areas like mobile video streaming, and ITU-T P.1202.2, which is targeted towards high-resolution IPTV application [48,49].Since this model parses information from both the IP header and the payload, it is more accurate when compared to the previous algorithm but requires more computational effort.Also for this model to work, the payload data must be in an unencrypted form.There is another striking difference between this model and ITU-T P.1201 with reference to the number of outputs.P.1202 provides only 1 video MOS as the output, whereas P.1201 provides 3 outputs (audio only, video only, and audiovisual MOS).A summary of the application    The model description in a block diagram format has been shown in Figure 22.H.264 encoded video bitstream, along with other side information (error concealment type, rebuffering, etc.), is taken as input; relevant parameters are extracted out and then aggregated, which are then used to predict the video QoE.

Compression, slicing, freezing, and rebuffering are the four different types of artifacts considered by this model and included in the final video MOS.Each of them is calculated separately and they are finally aligned together to the same level (MOS) by using suitable mapping functions.This model attains a RMSE value of 0.357 (across 982 sequences) and a PCC value of 0.918.


### ITU-T Recommendation J.247.

This recommendation provides guidelines on the selection of an appropriate video quality measurement method when a full reference is available [50].Presently this model has 4 different flavors: Video Quality Expert Group (VQEG) Proponent A (NTT, Japan), VQEG Proponent B (OPTICOM, Germany), VQEG Proponent C (Psytechnics, UK), and VQEG Proponent D (Yonsei University, South Korea).All these 4 models have been tested across video sequences having resolution of VGA, CIF, and QCIF only.All of them take the same inputs and provide the same output in terms of the video MOS (outperforming the commonly used Peak Signal to Noise Ratio (PSNR) model) [51].Depending upon the operational requirement, these models can predict the quality of videos that have been impaired by codec related factors only, network transmission related factors, or a combination of both.The performance overview for the 4 different models across the 3 different resolutions has been shown in Table 12.The PCC values are obtained by comparing the objective scores across the three different resolutions against the subjective data from 984 end-users.Figure 23 shows the comparison of the model performances (in terms of PCC values only).The outlier ratio is obtained by using the standard error of the mean as per the formulae given in outlier ratio (OR) = (total no of outliers)  , (10) where an outlier is a point for which
󵄨 󵄨 󵄨 󵄨 󵄨 (MOS Subjective − MOS Objective ) 󵄨 󵄨 󵄨 󵄨 󵄨 > 𝐶 1 × 𝜎 (DMOS (𝑖)) √𝑁 Subjects .(11)
In (11),  1 is a constant that depends on the nature of the score distribution (Gaussian, exponential, etc.), (DMOS()) represents the standard deviation of the individual scores associated with the th video clip, and  Subjects is the number of viewers per video clip .  13 shows a summary of the inputs that this model can take across its different variants.

Figures 24-26 show the hybrid NR, RR, and FR models (for both encrypted and nonencrypted video data).While the NR models have access to the bitstream and the PVS data, the RR models have access to the bitstream data and the source video sequences having some reduced set of features, and the FR models have full access to the bitstream data along with the entire source video sequences.For all the versions, the encrypted model does not have access to the video payload data and operates without parsing the packet payload.

Table 14 enlists the various parameters for which the models have been tested.The model performance summary has been shown in Table 15.PCC and RMSE values have been used for calculating the model performance statistics.For each of the models, relevant subjective tests are carried out, the results of which are fitted using a third order monotonic polynomial function.In case of the NR models, MOS values are used (obtained from the ACR subjective technique), while for the RR and FR models DMOS values are used (obtained from the ACR-HR subjective technique) for evaluating the model accuracy.

From the above discussion it is clear that a variety of objective techniques that can be used in a number of different scenarios are available.For evaluating the video quality, while some models take in account the presence of reference video signals (FR and RR methods), others do not have this requirement.Similarly, each of the models has been tested for specific codecs only corresponding to specific resolutions.In order to generalize them for different codecs and other factors like resolution and content complexity, we have provided a snapshot of the relevant methodologies.The video sequences that are used for testing the models also vary in terms of the video duration, content complexity, etc.Some of the ITU models are best suited for network monitoring (ITU-T P.1201 series), whereas some are used for network or QoS/QoE planning (ITU-T G.1070, ITU-T G.1071), while the others are used for laboratory testing purpose (ITU-T J.247).Due to this wide variety of objective ITU techniques, it becomes International Journal of Digital Multimedia Broadcasting  can be made depending upon the research context.Table 16 highlights the network and application parameters that each of the models takes into account along with their intended purpose.Therefore, based upon the parameters of interest for quality prediction and the application scenario, it will be easy to choose a particular reference model.


## Current Limitations and Challenges

A lot of work is going on within ITU to assess the quality of video streaming services.However, a number of shortcomings exist especially for the quality evaluation of videos that are streamed to mobile devices.We enlist here the challenges that are being faced and should be addressed.The primary dilemma is in the existence of numerous models, the basic aim of which is to measure the video QoE and the varied type of inputs that they take in predicting the quality.Each model takes a different input based upon either network parameters (packet loss, delay, jitter, etc.) or video characteristics (bitrate, frame rate, resolution, content type, etc.) or a combination of both.There can be variations among the network parameters itself.For example, the packet loss pattern may be random or bursty by nature.Similar situations can arise in case of delay also.

The assessment methodologies are also different in terms of the subjective, objective, and hybrid methods.To make the situation even more complex, the different QoS factors (network or application level) as outlined in this survey are not sufficient in predicting the QoE accurately.QoE is strongly influenced by external factors like the type of device used in viewing, the surrounding environmental conditions, and other factors.For majority of the models, the video sequences that are selected from the VQEG database are very short in duration (roughly 10 s only) and hence their ability to portray a real life-streaming scenario is questionable.In addition, the effect of using videos lesser or greater than 10 seconds on the subjective quality assessment has not been accounted for [59,60].

When streaming is done on mobile devices, the characteristics of the device itself should be taken into account because the viewing experience is quite different on small form factor mobile screens and conventional televisions [61,62].There are several limitations to the mobile devices in terms of the variety of screen sizes, display resolution, limited battery backup, limited storage, and other connectivity problems [63].Currently, none of the existing ITU models considers the peculiarities that are unique to a mobile streaming environment.Despite the fact that more than 55% of the overall Internet traffic is generated by some form of multimedia streaming over a mobile device, lack of a model that particularly addresses this scenario leaves a great void and a lot of scope for further research into this aspect [3].

In a mobile video streaming environment, the inherent unreliable nature of the wireless networks should also be kept in mind.A detailed analysis of the video QoE over a WiFi network and other mobile networks like 2G, 3G, and 4G should be carried out with sufficient detail.Often the low speeds that are associated with mobile networks result in a poor video QoE, which has prompted companies like Google to release a new version of the most popular YouTube application named as YouTube Go that is supposed to work in low speed networks [64].Thus, ITU should have in place models that simulate these wireless environments in detail and are targeted towards mobile devices considering the recent trend of watching videos online.Most of the ITU models use videos having low resolutions of VGA, HVGA, CIF, and QCIF only.Practically, only the J.343 series take into account HD resolution.This is in sharp contrast to the current trend where 4K is gaining in popularity.In fact streaming services like YouTube and Netflix have contents that can be streamed in 4K.However, ITU does not provide any model that is dedicated towards such high-resolution videos.Recent advances in virtual reality (VR) and augmented reality (AR) platforms coupled with the availability of mobile devices that can support these have carved out a new way in which videos are being watched by the users.These recent trends and changing viewing habits should be incorporated into future ITU models.


## Conclusion

Video streaming has become extremely popular these days, which allows the users to watch videos anytime and anywhere.However, for the success of such a service, the quality provided to the end-users must be excellent.There are a number of challenges being faced particularly in a mobile streaming environment.The QoE should be calculated keeping in mind not only the network QoS factors like packet loss, jitter, delay, and throughput and application QoS factors like bitrate, frame rate, and content complexity, but also the nature and characteristics of the mobile devices being used together with the surrounding environment.

In this article, we have presented an in-depth review of the standardized approaches being followed by ITU towards the video quality evaluation.Proper definitions of QoS and QoE have been provided along with the interrelationship between the two.Taxonomy of all the ITU models has been provided based on a general approach and the measurement methodology used.The basic overview and working of all the objective models are provided with suitable diagrams and algorithms/mathematical formulae.Finally, the current drawbacks are discussed along with the scope of future work.

## Figure 1 :
1
Figure 1: Typical video streaming scenario.


## Figure 2 :Figure 3 :
23
Figure 2: The concept of end-to-end QoS.




High reliability, High validityHigh reliability, Low validity Low reliability, High validity Low reliability, Low validity


## Figure 7 :
7
Figure 7: Concept of validity versus reliability [12].


## Figure 8 :
8
Figure 8: Conceptual view of media layer model: (a) FR methods, (b) RR methods, (c) NR methods.


## Figure 9 :Figure 10 :Figure 11 :
91011
Figure 9: Conceptual view of packet layer model.


## 6 Figure 12 :Figure 13 :
61213
Figure 12: Taxonomy of ITU recommendations related to video streaming.


## 3. 1 . 2 .
12
Different Test Methods.Four different types of methods are used in this recommendation and they are classified as Absolute Category Rating (ACR), Absolute Category Rating with Hidden Reference (ACR-HR), Degradation Category Rating (DCR), and Pair Comparison (PC) method.Each of these techniques is discussed next.


## Figure 14 :
14
Figure 14: Timing diagram for stimulus presentation: (a) ACR/ACR-HR, (b) DCR, (c) PC.




(b).The two sequences should be perfectly synchronized; i.e., both of them must start 10 International Journal of Digital Multimedia Broadcasting


## Figure 16 :
16
Figure 16: Timing diagram for stimulus presentation (variant 2).


## Figure 18 :
18
Figure 18: Overall G.1071 model framework.


## Figure 19 :
19
Figure 19: Overview of model inputs and output.


## Figure 20 :
20
Figure 20: Overall block diagram of ITU-T P.1201.1 model.


## Figure 21 :
21
Figure 21: Overview of ITU-P.1202model interfaces.


## Figure 25 :Figure 26 :
2526
Figure 25: Block diagram of hybrid RR models.


## Table 1 :
1
Performance target for different applications.
ApplicationTypical Data RatesOne-way DelayPerformance Parameters and Target Values JitterPacket LossConversational Voice4-64 kbps<150 ms (preferred) <400 ms (limit)<1 ms<3%Voice Messaging4-32 kbps<1 s (playback) <2 s (record)<1 ms<3%Audio Streaming16-128 kbps<10 s<1 ms<1%Videophone16-384 kbps<150 ms (preferred) <400 ms (limit)-<1%Video Streaming16-384 kbps<10 s-<1%Web Browsing and HTMLNA<2 sNAZeroE-commerce ServicesNA<2 sNAZeroInteractive GamesNA<200 msNAZero
NA: not applicable.




, ！ Ｄ Sequence A under Ｃ ＮＢ and Ｄ ＮＢ test condition respectively ＂ Ｅ , ＂ Ｆ Sequence B under Ｅ ＮＢ and Ｆ ＮＢ test condition respectively
Pict.！ ＣGreyPict.＂ ＤGreyPict.＃ Ｅ！ ＬGrey！ ＣGrey＂ ＬGrey＂ Ｄ~10 sVoting ≤ 10 s~10 sVoting ≤ 10 s~10 sVoting~10 s 2 s~10 sVoting ≤ 10 s~10 s2 s~10 sVoting！ Ｃ Sequence A under test condition i ＃ Ｅ Sequence C under test condition k ＂ Ｄ Sequence B under test condition j！ Ｃ Sequence A under test condition i ＂ Ｄ Sequence B under test condition j ！ Ｌ , ＂ Ｌ Sequence A and B respectively in the reference source format(a)(b)GreyGreyGrey~10 s 2 s~10 s≤ 10 s~10 s2 s~10 sVotingVoting！ Ｃ

## Table 2 :
2
Relevant video details.
Seq No.Seq NameFrame RateChroma FormatContent Complexity1H a r b o r6 0 f p s4 . 2 . 01 0 1 42Ice60 fps4.2.07563DucksTakeOff50 fps4.2.027284ParkJoy50 fps4.2.024505C r e w6 0 f p s4 . 2 . 01 0 5 36C r o w d R u n5 0 f p s4 . 2 . 02 6 8 87Akiyo30 fps4.2.02558Soccer60 fps4.2.027049Foreman30 fps4.2.0114010Football30 fps4.2.0276011News30 fps4.2.01470

## Table 3 :
3
Summary of viewing conditions.
ParameterSettingsViewing distance1-8 times picture heightPeak luminance of screen100-200 cd/mRatio of luminance of inactive screen to peak luminance≤0.05Ratio of luminance of screen, when displayingonly black level in a complete dark room to a peak≤0.1whiteBackground room illumination≤20 lux

## Table 4 :
4
MOS scale.
RatingMeaning5Excellent4Good3F a i r2P o o r1B a d

## Table 5 :
5
DCR 5 level opinion scale.
RatingMeaning5I m p e r c e p t i b l e4Perceptible but not annoying3Slightly annoying2Annoying1Very annoying



[33] recommendation gives different methodologies for assessing the picture and video quality for any generic application scenario, not only restricting to a video streaming case[33].Considering the popularity of the methods that have been outlined in this recommendation, we chose to include them as a part of this survey.The subjects can be experts or nonexperts depending upon the objectives of the assessment.Minimum 15 observers must be present with no limits on the upper bound.Next, the different test methodologies that are enumerated in this recommendation are presented.
Different Test Methods. Five different types of test proceduresare described. They are the Single Stimulus ContinuousQuality Evaluation (SSCQE) method, Double Stimulus Con-
tinuous Quality Scale (DSCQS) method, Double Stimulus Impairment Scale (DSIS) method, Simultaneous Double Stimulus for Continuous Evaluation (SDSCE) method, and the Stimulus Comparison Adjectival Categorical Judgment (SCACJ) method.The first one is an example of a single stimulus technique, while all the remaining four are examples


## Table 6 :
6
Continuous quality scale.
RatingMeaning80-100Excellent60-80Good40-60Fair20-40Poor0-20BadT1T2T3T4VoteFigure 15: Timing diagram for stimulus presentation (variant 1).

## Table 7 :
7
SCACJ quality scale.
RatingMeaning−3M u c h w o r s e−2W o r s e−1S l i g h t l y w o r s e0Th e s a m e+1Slightly better+2Better+3Much better



discussed in the previous sections.Theonly difference is in the stimulus type that is shown to the users.In this case video sequences are shown which have an audio counterpart.Therefore, the subjects evaluate the overall multimedia quality.However, in case of the previous recommendations, the videos normally do not have any audio portion.Next, a brief summary of the subjective methods discussed above and their shortcomings is presented.3.4.Summary of Subjective Methods.Subjective methods are more accurate in gauging the user opinion when compared to the objective ones.A variety of techniques is available and a proper one should be chosen based upon the time available and application requirement.If time is not a constraint, then any of the methods discussed above can be used.For time critical conditions, generally ACR or ACR-HR method is preferred.Similarly, presence or absence of reference content also affects the choice of a particular technique.Sometimes, the duration of the video sequence that needs to be evaluated also plays a judgmental role in deciding which technique is to be chosen.For longer video sequences, normally SSCQE or SDSCE is used.Requirements related to certain specific quality aspects can also sometimes dictate a specific choice.

[34]1.This recommendation presents the different subjective quality assessment methods for multimedia applications[34].The number of subjects varies from 6 to 40.It uses four different techniques, namely, ACR, DCR, PC, and SSCQE.All these techniques have already been


## Table 8 :
8
Guidelines to choose a proper subjective approach.
Parameter

## quality integration function Coefficient database Coefficient database End-to-end delay End-to-end delay
Terminal typeTerminal characteristicsMonitor sizeMonitorVideo-alonequality VqVideo qualityVq(Sq)Multimediaquality MMqSpeech qualitySq(Vq)Speech-alonequality Sq

## resolution Ambient illuminance Conversational task Packet loss pattern Terminal factors Loudness rating etc. Environmental factors Ambient noise Network factors Packet-loss pattern End-to-end delay Video codec Codec type, Video format, Keyframe interval, Video display size Bit rate, Frame rate, Packet-loss rate End-to-end delay Speech codec Codec type, Bit rate Packet-loss rate TELR
Figure 17: G.1070 model framework.



we discuss only MOS  .The overall video quality   can be classified into three different types,   , MOS  , and MOS VR , where MOS VC is video MOS in case of no packet loss and no rebuffering (video quality due to compression), MOS VP is video MOS in case of packet loss but no rebuffering (video quality due to packet loss),
When compared againstsimilar subjective tests, this model attains a Root MeanSquare Error (RMSE) value of 0.60 and a Pearson CorrelationCoefficient (PCC) value of 0.78 across 1430 different samplevideo types.4.2.2. General Model Algorithm.

## Table 9 :
9
Application areas, test factors, and technology used by the ITU-T P.1201.1 model.
Type

## Table 10 :
10
Application areas, test factors, and technology used by the ITU-T P.1202.1 model.
Type

## Table 11 :
11
Table 11 lists down the factors for which this model has been evaluated.Application areas, test factors, and technology used by the ITU-T J.247 model.
International Journal of Digital Multimedia Broadcasting

## Table 12 :
12
Model performance overview.
ModelResolutionPCCRMSEOutlier RatioNTT0.7860.6210.523OPTICOM0.8250.5710.502PsytechnicsVGA0.8220.5660.524Yonsei0.8050.5930.542PSNR0.7130.7140.615NTT0.7770.6040.538OPTICOM0.8080.5620.513PsytechnicsCIF0.8360.5260.507Yonsei0.7850.5940.522PSNR0.6560.7200.632NTT0.8190.5510.497OPTICOM0.8410.5160.461PsytechnicsQCIF0.8300.5170.458Yonsei0.7560.6170.523PSNR0.6620.7210.596

## Table 13 :
13
Model input across different variants.
Model TypeModel NameRequired InputsHybrid NR (encrypted)RST-V model YHyNRe modelProcessed video sequence (PVS) and encrypted bitstream PVS and encrypted bitstreamHybrid NRYHyNR modelPVS and non-encrypted bitstreamHybrid RR (encrypted)YHyRRe modelPVS, extracted features from source reference channel (SRC) and encrypted bitstreamHybrid RRYHyRR modelPVS, features extracted from SRC and non-encrypted bitstreamHybrid FR (encrypted)PEVQ-S (e) YHyFRe modelPVS, SRC and encrypted bitstream PVS, SRC and encrypted bitstreamHybrid FRPEVQ-S YHyFR modelPVS, SRC and non-encrypted bitstream PVS, SRC and non-encrypted bitstream4.6. ITU-T Recommendation J.343. This recommendationspecifies objective methods that use bitstream data in addi-tion to the processed video sequences [52]. As this is abitstream model, it has additional information about thepayload data like codec type, bitrate, frame rate, spatial, andtemporal shifts apart from the transmission errors like delayand packet loss. Six different application areas are addressedby it through [53-58]. This model can work in FR, RR, and NRmodes for both encrypted and unencrypted video payloaddata. Table

## Table 14 :
14
Application areas, test factors, and technology used by the ITU-T J.343 model series.
Type
confusing for a researcher to select an appropriate method depending upon the requirements.Therefore, in order to make the model selection process easier, we list down certain factors in Table16that can serve as the baseline for selecting the most appropriate model under a specific circumstance.Once a particular model is selected, the necessary changes


## Table 15 :
15
Model performance summary.
ModelResolutionPCCRMSEHybrid NR YHyNR modelVGA WVGA0.78 0.810.59 0.56FHD0.850.52Hybrid NRVGA0.760.61(encrypted)WVGA0.790.59RST-V modelFHD0.770.64Hybrid NRVGA0.720.66(encrypted)WVGA0.770.62YHyNRe modelFHD0.780.61Hybrid RR YHyRR modelVGA WVGA0.80 0.840.57 0.52FHD0.860.52Hybrid RRVGA0.790.58(encrypted)WVGA0.840.53YHyRRe modelFHD0.840.55Hybrid FR PEVQ-SVGA WVGA0.81 0.830.57 0.55FHD0.880.48Hybrid FR YHyFR modelVGA WVGA0.80 0.840.66 0.61FHD0.860.52Hybrid FRVGA0.810.57(encrypted)WVGA0.830.55PEVQ-S (e)FHD0.880.48Hybrid FR YHyFRe modelVGA WVGA0.72 0.790.58 0.52FHD0.840.55

## Table 16 :
16
Objective model selection criterion.
Intended PurposeQoS/QoEPlanningNetworkPlanningNetworkMonitoringNetworkMonitoringLaboratoryTesting, NetworkMonitoringNetworkMonitoringService CategoryVideo TelephonyVideo StreamingVideo StreamingVideo StreamingVideo Telephony,Video StreamingVideo StreamingVideo Resolution Packet Loss Type Model TypeVGA, QVGA and Random QQVGA NRQCIF, QVGA, Uniform, Bursty HVGA NRQCIF, QVGA, Random, Bursty HVGA NRQCIF, QVGA, Random, Bursty HVGA NRQCIF, CIF, VGA × FRVGA, WVGA, HD Random, Bursty NR, RR, FRVideo CodecMPEG-2, MPEG-4MPEG-4, ITU-TH.264MPEG-4, ITU-TH.264ITU-TH.264H.264/AVC, RV 10,WM 9, MPEG-4(Part 2)H.264/AVCVideo SequenceDuration10 seconds8-24 seconds8-24 seconds10-16 seconds××Application FactorsBR, FR, CT, VRBR, FR, CT, VR×BR, FR, CT, VRBR, FR, CTBR, FR, CT, VRNetworkFactorsPL, JPL, JPL, J, TPL, J, TPLPL, J, TModelITU-TG.1070ITU-TG.1071ITU-TP.1201SeriesITU-TP.1202SeriesITU-TJ.247SeriesITU-TJ.343Series
PL: packet loss, J: jitter; T: throughput, BR: bitrate; FR: frame rate, CT: video content type; VR: video resolution.

AcknowledgmentsThe authors would like to thank Dr. Borworn Papasratorn from the School of Information Technology, KMUTT, for sharing his long expertise in telecommunications research and providing the guidelines for writing an effective review paper.Conflicts of InterestThe authors declare that there are no conflicts of interest regarding the publication of this paper.
Index-TV is no longer a single screen in your Living Room. Ooyala Corp. 2013Video. 2013

Synthetic structure of industrial. G O Young, Plastics, J. Peters1964McGraw-Hill3New York, NY, USA2nd edition

Cisco Global Mobile Data Traffic Forecast Update Report. Cisco Corp, USA2014-2019. 2016

A unified QoE prediction framework for HEVC encoded video streaming over wireless networks. Z Cheng, L Ding, W Huang, F Yang, L Qian, Proceedings of the 12th IEEE International Symposium on Broadband Multimedia Systems and Broadcasting. the 12th IEEE International Symposium on Broadband Multimedia Systems and BroadcastingCagliari, Italy2017. June 2017

QoE-aware quality selection method for adaptive video streaming with scalable video coding. S Mori, M Bandai, Proceedings of the 2018 IEEE International Conference on Consumer Electronics (ICCE). the 2018 IEEE International Conference on Consumer Electronics (ICCE)Las Vegas, NV, USAJanuary 2018

QoE-Driven dynamic adaptive video streaming strategy with future information. L Yu, T Tillo, J Xiao, IEEE Transactions on Broadcasting. 6332017

A holistic modeling for QoE estimation in live video streaming applications over LTE Advanced technologies with Full and Non Reference approaches. M García-Pineda, J Segura-García, S Felici-Castell, Computer Communications. 1172018

The impact of network impairment on quality of experience (QoE) in H.265/HEVC video streaming. J Nightingale, Q Wang, C Grecos, S Goma, IEEE Transactions on Consumer Electronics. 6022014

A study on a QoS/QoE correlation model for QoE evaluation on IPTV service. H J Kim, S G Choi, Proceedings of the 12th International Conference on Advanced Communication Technology: ICT for Green Growth and Sustainable Development, ICACT 2010. the 12th International Conference on Advanced Communication Technology: ICT for Green Growth and Sustainable Development, ICACT 2010Phoenix Park, KoreaFebruary 2010

Communication Quality of Service: A Framework and Definition. ITU-T Recommendation G.1000. November 2001

End-user Multimedia QoS Categories. ITU-T Recommendation G.1010. November 2001

Reference Guide to Quality of Experience Assessment Methodologies. ITU-T Recommendation G. 1011. July 2016

A generic quantitative relationship between quality of experience and quality of service. M Fiedler, T Hossfeld, P Tran-Gia, IEEE Network. 2422010

A no-reference modular video quality prediction model for H.265/HEVC and VP9 codecs on a mobile device. D , V Vanijja, ID 8317590Advances in Multimedia. 20172017

Definitions of terms related to Quality of Service. ITU-T Recommendation E.800. September 2008

Video streaming in content-centric mobile networks: challenges and solutions. C Xu, P Zhang, S Jia, M Wang, G.-M Muntean, IEEE Wireless Communications Magazine. 2452017

Toward QoE-Assured 4K Video-on-Demand Delivery Through Mobile Edge Virtualization with Adaptive Prefetching. C Ge, N Wang, G Foster, M Wilson, IEEE Transactions on Multimedia. 19102017

Amendment 5: New Definitions for inclusion in Recommendation. ITU-T P.10/G.100ITU-T Recommendation P.10/G.100 Amendment 5. July 2016

Definitions on Quality of Experience, Qualinet White Paper from the 5th Qualinet Meeting. March 2013

Challenges of future multimedia QoE monitoring for internet service providers. W Robitza, A Ahmad, P A Kara, Multimedia Tools and Applications. 201776

The evolution of video quality measurement: from PSNR to hybrid metrics. S Winkler, P Mohandas, IEEE Transactions on Broadcasting. 5432008

A concise review of the quality of experience assessment for video streaming. O B Maia, H C Yehia, L De Errico, Computer Communications. 572015

Hybrid QoE assessment is wellsuited for Multiple Description Coding video streaming in overlay networks. M Ghareeb, C Viho, Proceedings of the 8th Annual Conference on Communication Networks and Services Research, CNSR 2010. the 8th Annual Conference on Communication Networks and Services Research, CNSR 2010Montreal, CanadaMay 2010

A brief synthesis of QoS-QoE methodologies. H Rifaï, S Mohammed, A Mellouk, Proceedings of the 10th International Symposium on Programming and Systems, ISPS' 2011. the 10th International Symposium on Programming and Systems, ISPS' 2011Algiers, AlgeriaApril 2011

QoE assessment model for multimedia streaming services using QoS parameters. H J Kim, S G Choi, Multimedia Tools and Applications. 2013

Subjective Video Quality Assessment Methods for Multimedia Applications. ITU-T Recommendation P.910. April 2008

Noreference quality assessment of screen content pictures. K Gu, J Zhou, J.-F Qiao, G Zhai, W Lin, A C Bovik, IEEE Transactions on Image Processing. 2682017

Content clustering based video quality prediction model for MPEG4 video streaming over wireless networks. A Khan, L Sun, E Ifeachor, Proceedings of the 2009 IEEE. the 2009 IEEEGermany2009. June 2009

Content-based subjective quality prediction in stereoscopic videos with machine learning. H Malekmohamadi, W A C Fernando, A M Kondoz, IEEE Electronics Letters. 48212012

Content-Based Video Quality Prediction Using Random Neural Networks for Video Streaming over LTE Networks. T Ghalut, H Larijani, A Shahrabi, Procceedings of the IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable. ceedings of the IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; DependableLiverpool, England2015

VQEG Standard Database. 

Methods for Objective and Subjective Assessment of Speech and Video Quality. ITU-T Recommendation P.800. July 2016

Methodology for the Subjective Assessment of the Quality of Television Pictures. January 2012ITU-R Recommendation BT

ITU-T Recommendation P. Subjective Audiovisual Quality Assessment Methods for Multimedia Applications. December 1998911

A survey of QoE assurance in converged networks. R Stankiewicz, A Jajszczyk, Computer Networks. 5572011

Study of subjective and objective quality assessment of video. K Seshadrinathan, R Soundararajan, A C Bovik, L K Cormack, IEEE Transactions on Image Processing. 1962010

Opinion Model for Video-telephony Applications. ITU-T Recommendation G.1070. July 2012

Extending G.1070 for video quality monitoring. N D Narvekar, T Liu, D Zou, J A Bloom, Proceedings of the 2011 12th IEEE International Conference on Multimedia and Expo, ICME 2011. the 2011 12th IEEE International Conference on Multimedia and Expo, ICME 2011Barcelona, SpainJuly 2011

Extension of the G.1070 video quality function for the MPEG2 video codec. B Belmudez, S Möller, Proceedings of the 2010 2nd International Workshop on Quality of Multimedia Experience. the 2010 2nd International Workshop on Quality of Multimedia ExperienceTrondheim, NorwayIEEEQoMEX 2010. June 2010

Extending the ITU-T G.1070 opinion model to support current generation H.265/HEVC video codec. D Pal, T Triyason, V Vanijja, Proceedings of the International Conference on Computational Science and Its Applications. the International Conference on Computational Science and Its ApplicationsBeijing, China20169787

1070 model extension at full HD resolution for VP9/HEVC codec. D , V Vanijja, Journal of Telecommunication, Electronic and Computer Engineering. 892016

Opinion Model for Network Planning of Video and Audio Streaming Applications. ITU-T Recommendation G. 1071. November 2016

ITU-T Recommendation P.1201. Parametric Non-Intrusive Assessment of Audiovisual Media Streaming Quality. October 2012

Parametric non-intrusive Assessment of Audiovisual Media Streaming Quality: Lower Resolution Application Area. ITU-T Recommendation P.1201.1October 2012

Parametric non-intrusive Assessment of Audiovisual Media Streaming Quality: Higher Resolution Application Area. ITU- T Recommendation P.1201.2October 2012

Use of ITU-T P.1201 for Non-adaptive, Progressive Download type Media Streaming. ITU-T Recommendation P.1201 Amendment 2:New Appendix III. December 2013

Parametric non-intrusive Bitstream Assessment of Video Media Streaming Quality. October 2012ITU-T Recommendation P.1202

Parametric non-intrusive Bitstream Assessment of Video Media Streaming Quality-Lower Resolution Application Area. ITU-T Recommendation P.1202.1October 2012

Parametric non-intrusive Bitstream Assessment of Video Media Streaming Quality-Higher Resolution Application Area. ITU-T Recommendation P.1202.2May 2013

Objective Perceptual Multimedia Video Quality Measurement in the Presence of a Full Reference. ITU-T Recommendation J. 247August 2008

Perceptual Visual Quality Measurement Techniques for Multimedia Services over Digital Cable Television Networks in the Presence of a Reduced Bandwidth Reference. ITU-T J.246August 2008

Hybrid Perceptual Bitstream Models for Objective Video Quality Measurements. ITU-T Recommendation J. 343November 2014

Hybrid-NRe Objective Perceptual Video Quality Measurement for HDTV and Multimedia IP-based Video Services in the Presence of Encrypted Bitstream Data. November 2014343ITU-T Recommendation J

Hybrid-NR Objective Perceptual Video Quality Measurement for HDTV and Multimedia IP-based Video Services in the Presence of Non-encrypted Bitstream Data. J. 3432November 2014ITU-T Recommendation

Hybrid-RRe Objective Perceptual Video Quality Measurement for HDTV and Multimedia IP-based Video Services in the Presence of a Reduced Reference Signal and Encrypted Bitstream Data. ITU-T Recommendation J. 3433November 2014

Hybrid-RR Objective Perceptual Video Quality Measurement for HDTV and Multimedia IP-based Video Services in the Presence of a Reduced Reference Signal and Non-encrypted Bitstream Data. ITU-T Recommendation J. 343November 2014

Hybrid-FRe Objective Perceptual Video Quality Measurement for HDTV and Multimedia IP-based Video Services in the Presence of a Full Reference Signal and Encrypted Bitstream Data. ITU-T Recommendation J. 343November 2014

Hybrid-FR Objective Perceptual Video Quality Measurement for HDTV and Multimedia IP-based Video Services in the Presence of a Full Reference Signal and Non-encrypted Bitstream Data. ITU-T Recommendation J. 3436November 2014

On the optimal presentation duration for subjective video quality assessment. F M Moss, K Wang, F Zhang, R Baddeley, D R Bull, IEEE Transactions on Circuits and Systems for Video Technology. 201626

Study of temporal effects on subjective video quality of experience. C G Bampis, Z Li, A K Moorthy, I Katsavounidis, IEEE Transactions on Image Processing. 26112017

Exploration and optimization of user experience in viewing videos on a mobile phone. W Song, D Tjondronegoro, M Docherty, International Journal of Software Engineering and Knowledge Engineering. 2082010

Can small be beautiful? assessing image resolution requirements for mobile TV. H Knoche, J D Mccarthy, M A Sasse, Proceedings of the 13th ACM International Conference on Multimedia, MM 2005. the 13th ACM International Conference on Multimedia, MM 2005SingaporeNovember 2005

Mobile IPTV: Approaches, challenges, standards, and QoS support. S Park, S.-H Jeong, IEEE Internet Computing. 1332009

YouTube Go application. 14th September, 2017