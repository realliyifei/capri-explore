# Nonparametric additive model-assisted estimation for survey data

CorpusID: 437245
 
tags: #Mathematics, #Computer_Science

URL: [https://www.semanticscholar.org/paper/eaf2ecc3c7774681cf368c44d2ec464f2959767b](https://www.semanticscholar.org/paper/eaf2ecc3c7774681cf368c44d2ec464f2959767b)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Nonparametric additive model-assisted estimation for survey data
4 Jan 2011

Li Wang 
Suojin Wang 

Department of Statistics
Department of Statistics
University of Georgia
30602AthensGeorgia


Texas A&M University
77843College Station, Texas

Nonparametric additive model-assisted estimation for survey data
4 Jan 2011CalibrationHorvitz-Thompson estimatorlocal linear regressionmodel-assisted estimationsplinesuperpopulation 2000 MSC: primary62D05; secondary62G08
An additive model-assisted nonparametric method is investigated to estimate the finite population totals of massive survey data with the aid of auxiliary information. A class of estimators is proposed to improve the precision of the well known Horvitz-Thompson estimators by combining the spline and local polynomial smoothing methods. These estimators are calibrated, asymptotically design-unbiased, consistent, normal and robust in the sense of asymptotically attaining the Godambe-Joshi lower bound to the anticipated variance. A consistent model selection procedure is further developed to select the significant auxiliary variables. The proposed method is sufficiently fast to analyze large survey data of high dimension within seconds. The performance of the proposed method is assessed empirically via simulation studies.

## Introduction

Auxiliary information is often available in many surveys for all elements of the population of interest. For instance, in many countries, administrative registers provide extensive sources of auxiliary information. Complete registers can give access to variables such as sex, age, income and country of birth. Studies of labor force characteristics or household expenditure patterns, for example, might benefit from these auxiliary data. Another example is the satellite images or GPS data used in spatial sampling. These data are often collected at the population level, which are often available at little or no extra cost, especially compared to the cost of collecting the survey data.

If no information other than the inclusion probabilities is used to estimate the population total, a well-known design unbiased estimator is the Horvitz-Thompson (HT) estimator. Nowadays, "cheap" auxiliary information can be regularly used to obtain higher precision estimates for the unknown finite population quantities. For instance, post-stratification, calibration and regression estimation are different design-based approaches used to improve the precision of estimators. Auxiliary information can also be used to increase the accuracy of the finite population distribution function; see, for example, [30]. Model-assisted estimation ( [21]) provides a convenient way to incorporate auxiliary variables to develop more efficient survey estimators. By model-assisted, it is meant that a superpopulation model is adopted (for example, model (1) below), in which the finite population is modeled conditionally on the auxiliary information; see, for instance, [4,6,8,9]. The traditional parametric model-assisted approach assumes that the superpopulation model is fully described by a finite set of parameters, e.g., the regression estimator introduced in [21]. However, survey data now being collected by many government, health and social science organizations have more complex design features. It is difficult to obtain any prior model information to address various hypotheses. In this sense, preselected parametric model is too restricted to fit unexpected features. In contrast, nonparametric regression provides a useful tool for studying the dependence of variables of interest on auxiliary information without constraining the dependence to a fixed form with few parameters. The flexibility of nonparametric regression is extremely helpful to capture the complicated relationship between variables as well as in obtaining robust predictions; see [10,12] for details.

Breidt and Opsomer [1] first proposed a nonparametric model-assisted estimator based on local polynomial regression, which generalizes the parametric framework in survey sampling and improves the precision of the survey estimators immensely. Their investigation is only based on one auxiliary variable. Most surveys, however, involve more than one study variables, perhaps, many; see [22]. For example, the remote sensing data which provide a wide and growing range of variables to be employed. In this context, when the dimension of the auxiliary information vector is high, one unavoidable issue is the "curse of dimensionality", which refers to the poor convergence rate of nonparametric estimation of general multivariate functions. One solution is regression in the form of additive model; see [13].

Estimation and inference for additive models have been well studied in the literature; see, for example, the classic backfitting estimators of [13], the marginal integration estimators of [16], the smoothing backfitting estimators of [17], the spline estimators of Stone ( [25,26]) and the spline-backfitted kernel estimators of [29]. In survey sampling context, [2] discussed a semiparametric possible extension to multiple auxiliary variables via using the penalized splines; [19] applied the generalized additive models (GAMs) in an interaction model for the estimation of variables from forest inventory and analysis surveys; and [3] proposed a special case of the GAMs with an identity link function. For large and high dimensional survey data, it is important that estimation and inference methods are efficient and computationally easily implemented. However, few methods are theoretically justified and computational efficient when there are multiple nonparametric terms. The kernel based backfitting and marginal integration approaches are computationally expensive, limiting their use for high dimensional data; see [18] for some numerical comparisons of these methods. Spline methods, on the other hand, provide only convergence rates but no asymptotic distributions, so no measures of confidence can be assigned to the estimators.

Challenged by these demands, we propose approximating the nonparametric components by using the spline-backfitted local polynomial: spline does a quick initial estimation of all additive components and removes them all except the ones of interest; kernel smoothing is then applied to the cleaned univariate data to estimate with the asymptotic distribution. This two-step estimator is both computationally expedient for analyzing large and high dimensional survey data, and theoretically reliable as the estimator is uniformly oracle with asymptotic confidence intervals. The resulting estimator of population total can therefore be easily calculated, and more importantly allow for formal derivation of the asymptotic properties of the estimator.

In practice, a large number of variables may be collected and some of the insignificant ones should be excluded from the final model in order to enhance the predictability. The selection of auxiliary variables is a fundamental issue for model-assisted survey sampling methods. In this paper, we propose a consistent variable selection method for the additive model-assisted survey sampling based on the Bayes information criterion (BIC). A comprehensive Monte Carlo study demonstrates superior performance of the proposed methods.

The rest of the paper is organized as follows. Section 2 gives details of the superpopulation model and proposed method of estimation. Section 3 describes the weighting, calibration and asymptotic properties of the proposed estimator. Section 4 describes the auxiliary variable selection procedure for the superpopulation model under simple random sampling design (SRS). Section 5 reports the findings in an extensive simulation study. Lengthy technical arguments are given in the Appendix.


## Superpopulation Model and Proposed Estimator

In what follows, let U N = {1, ..., i, ..., N} be the finite population of N elements, called the target population, and i represents the ith element of the population. Let x i = {x i1 , ..., x id } be a d-dimensional auxiliary variable vector, i ∈ U N . We are interested in the estimation of the population total t y = i∈U N y i , where y i is the value of the study variable, y, for the ith element. To this end, a sample s of size n N is drawn from U N according to a fixed sampling design p N (·), where p N (s) is the probability of drawing the sample s. The inclusion probabilities, known for all i ∈ U N , are π iN ≡ π i = Pr {i ∈ s} = s∋i p N (s). In addition to the π i , denote π ijN ≡ π ij = Pr {i, j ∈ s} = s∋i,j p N (s) the inclusion probability for both elements i, j ∈ U N .

Let {(x i , y i )} i∈U N be a realization of (X, Y ) from an infinite superpopulation, ξ, satisfying
Y = m (X) + σ (X) ε,(1)
in which the unknown d-variate function m has a simpler form of
m (X) = c + d α=1 m α (X α ) , E ξ [m α (X α )] ≡ 0, 1 ≤ α ≤ d,(2)
the function σ(·) is the unknown standard deviation function and the standard error ε satisfies that E ξ (ε |X ) = 0 and E ξ (ε 2 |X) = 1. In the following, we assume the auxiliary variable X α is distributed on a compact interval [a α , b α ], α = 1, ..., d. Without loss of generality, we take all intervals [a α , b α ] = [0, 1].

To estimate the additive components in (2), we employ a two-stage procedure based on the spline-backfitted local polynomial smoothing. For any α = 1, ..., d, we introduce a knot sequence with J interior knots k 0α = 0 < k 1α < ... < k Jα < 1 = k (J+1)α , where J ≡ J N increases when n N increases, and the precise order is given in Assumption (A5). Denote the piecewise linear truncated power spline basis
Γ (x) ≡ 1, x α , (x α − k 1α ) + , . . . , (x α − k Jα ) + , α = 1, ..., d T ,(3)
where (a) + = a if a > 0 and 0 otherwise. For the local linear smoothing, let
K h (x) = h −1 K (x/h),
where K denotes a kernel function and h = h N is the bandwidth; see Assumption (A6) below. We now describe our two-stage estimator for the population total t y . At the first stage, we apply the spline smoothing to obtain a quick initial estimator of
m (x i ),m (x i ) =b 0 + d α=1b 0,α x iα + d α=1 J j=1b j,α (x iα − k jα ) + ,
whereb 0 andb j,α , j = 0, 1, .., J, α = 1, ..., d are the minimizes of the following
i∈s π −1 i y i − b 0 − d α=1 b 0,α x α − d α=1 J j=1 b j,α (x iα − k jα ) + 2 (4) over a G d ≡ 1 + (J + 1)d dimensional vector. Because the components m α (x α )
can only be identified up to an additive constants, we center the estimator of m α (x α ) and define the centered pilot estimator of the αth component aŝ
m α (x α ) =b 0,α x α + J j=1b j,α (x α − k jα ) + −ĉ α ,(5)whereĉ α = N −1 i∈s π −1 i b 0,α x iα + J j=1b j,α (x iα − k jα ) + .
The above pilot estimators in (5) are then used to construct the new pseudo-responseŝ
y iα = y i − N −1t y − β =αm β (x iα ), i ∈ s, α = 1, ..., d,(6)
wheret y is the well-known HT estimator. At the second stage, a local polynomial smoothing is applied to the cleaned univariate data {x iα ,ŷ iα } i∈s to achieve the "oracle" property in [29]. To be specific, considering the local linear smoothing, for any α = 1, ..., d, we minimize
i∈s π −1 i {ŷ iα − a 0,α − a 1,α (x iα − x) K h (x iα − x)} 2 ,(7)
with respect to a 0,α and a 1,α . The spline-backfitted local linear (SBLL) estimator of the α-th component m α ism * α =â 0,α in (7). The final sample design-based SBLL estimator of m (x) is defined aŝ
m * (x) = 1 Nt y + d α=1m * α (x α ) .(8)
Substitutingm * i ≡m * (x i ) into the existing generalized difference estimator (see page 221 of [21]), the SBLL estimator for t y is defined bŷ
t y,SBLL = i∈U Nm * i + i∈s y i −m * i π i = i∈s y i π i + i∈U N 1 − I i π i m * i ,(9)
where I i = 1 if i ∈ s and I i = 0 otherwise.


## Remark 1.

In the first step spline smoothing, the number of knots J N can be determined by n N and a tuning constant c:
J N = min [cn 1/4 N log(n N )] + 1, [(n N /2 − 1) /d − 1] .(10)
As discussed in [29], the choice of c makes little difference. In the second step local polynomial smoothing, one can use the quartic kernel and the rule-of-thumb bandwidth.


## Properties of the Estimator


### Weighting and Calibration

In the last decade, calibration estimation has developed into an important field of research in survey sampling. As discussed in [7] and [15], calibration is a highly desirable property for survey weights, which allows the survey practitioner to simply adjust the original design weights to incorporate the information of the auxiliary variables. Several national statistical agencies have developed software to compute calibrated weights based on auxiliary information available in population registers and other sources. The proposed SBLL estimator in this paper also shares this property in certain sense.

Let y s be the column vector of the response values y i for i ∈ s and define the diagonal matrix of inverse inclusion probabilities
Π s = diag {1/π i } i∈s . For Γ(x) in (3), denote Γ s = Γ (x i ) T
i∈s the sample truncated power spline matrix. Let B s be the collection of the estimated spline coefficient in (4), then B s =
(Γ T s Π s Γ s ) −1 Γ T s Π s y s .
Thus the pilot spline estimator of m α (x α ) in (5) can be written asm
α (x α ) = Γ (x) T D α B s − N −1 1 T n Π s Γ s D α B s y s ,(11)
where 1 n is a vector of length n N with all "1"s, and
D α = diag{0, ..., 0, 1, ..., 1 , from (J+1)(α−1)+2 to (J+1)α+1 0, ..., 0}(12)
is a G d × G d diagonal matrix. Denoting the spline smoothing matrix and its centered version by
Ψ sα = Γ s D α (Γ T s Π s Γ s ) −1 Γ T s Π s , Ψ * sα = I − N −1 1 n 1 T n Π s Ψ sα , we havem α ≡ {m α (x iα )} i∈s = Ψ * sα y s , for α = 1, ..., d. Further forŷ iα in (6), let y α ≡ {ŷ iα } i∈s = y s − 1
Nt y 1 n − β =αm β , and define the matrices
X siα = 1 x kα − x iα k∈s , W siα = diag 1 π k K h (x kα − x iα ) k∈s .
Then the SBLL estimator of m α at x iα can be written aŝ
m * iα ≡m * α (x iα ) = e T 1 (X T siα W siα X siα ) −1 X T siα W siαŷα ,(13)
where e 1 = (1, 0) T . Therefore, the SBLL estimator in (8) 
of m (x) at x i iŝ m * i = 1 Nt y + d α=1 e T 1 (X T siα W siα X siα ) −1 X T siα W siα y s −t y N 1 n − β =α Ψ * sβ y s ≡ ρ T si y s , where ρ T si = e T 1 d α=1 (X T siα W siα X siα ) −1 X T siα W siα I + 1−d dN 1 n 1 T n Π s − β =α Ψ * sβ .
Similar to [20], we define the "g-weight"
g is = 1 + π i j∈U N 1 − I j π j ρ T sj a i ,(14)
where a i is a n N -dimensional vector with a "1" in the ith position and "0" elsewhere. Thus the proposed estimatort y,SBLL in (8) can be written aŝ
t y,SBLL = i∈s y i π i + j∈U N 1 − I j π j ρ T sj y s ≡ i∈s g is y i / π i ,
which is a linear combination of the sample y i 's with a sampling weight, π −1 i , and the "g-weight". Because the weights are independent of y i , they can be applied to any study variable of interest.

As we show below, the weight system gives our estimator of the known total i∈U N x iα to be itself. Theorem 1. For any α = 1, ..., d and the "g-weight" defined in (14),
t xα,SBLL ≡ i∈s g is x iα /π i = i∈U N x iα . Proof. Let x α = {x iα } i∈s . We havê t xα,SBLL = i∈s π −1 i x iα + j∈U N 1 − I j π −1 j e T 1 × d γ=1 X T sjγ W sjγ X sjγ −1 X T sjγ W sjγ I + 1 − d dN 1 n 1 T n Π s − β =γ Ψ * sβ x α .
Observe that
(Γ T s Π s Γ s ) −1 Γ T s Π s x α = τ α , Ψ sβ x α = Γ s D β τ α = x α , for β = α 0, for β = α ,
where τ α is the vector of dimension G d with a "1" in the {2 + (J + 1) (α − 1)}th position and "0" elsewhere. Then we have,
I + 1 − d dN 1 n 1 T n Π s − β =γ Ψ * sβ x α = I + 1−d dN 1 n 1 T n Π s x α , for γ = α 1 dN 1 n 1 T n Π s x α , for γ = α .
Note that for any i ∈ U N ,
e T 1 (X T siα W siα X siα ) −1 X T siα W siα x α = x iα , e T 1 (X T siα W siα X siα ) −1 X T siα W siα 1 n = 1, thus e T 1 d γ=1 X T sjγ W sjγ X sjγ −1 X T sjγ W sjγ I + 1 − d dN 1 n 1 T n Π s − β =γ Ψ * sβ x α = e T 1 (X T siα W siα X siα ) −1 X T siα W siα I + (dN) −1 (1 − d)1 n 1 T n Π s x α +d −1 e T 1 γ =α X T sjγ W sjγ X sjγ −1 X T sjγ W sjγ 1 n 1 T n Π s x α = x jα .
Hence the proposed SBLL estimator defined in (9) preserves the calibration property.


### Assumptions

For the asymptotic properties of the estimators, we adopt the traditional asymptotic framework in [1] where both the population and sample sizes increase as N → ∞. There are two sources of "variation" to be considered here. The first is introduced by the random sample design and the corresponding measure is denoted by p. The "O p ", "o p " and "E p (·)" notation below is with respect to this measure. The second is associated with the superpopulation from which the finite population is viewed as a sample. The corresponding measure and notation are "ξ". For simplicity, let π ij − π i π j = ∆ ij .

(A1) The density f (x) of X is continuous and bounded away from 0 and ∞.

The marginal densities f α (x α ) of x α have continuous derivatives and are bounded away from 0 and ∞.
(A2) The second order derivative of m α (x α ) is continuous, ∀ α = 1, ..., d. (A3) There exists a positive constant M such that E ξ |ε| 2+δ |X < M for some δ > 1/2; σ (x) is continuous on [0, 1] d and bounded away from 0 and ∞. (A4) As N → ∞, n N → ∞ and n N N −1 → π < 1. (A5) The number of knots J N ∼ n 1/4 N log(n N ). (A6) The kernel function K is Lipschitz continuous, bounded, nonnegative, symmetric, and supported on [−1, 1]. The bandwidth h N ∼ n −1/5 N , i.e., c h n −1/5 N ≤ h N ≤ C h n −1/5 N for some positive constants c h , C h . (A7) For all N, min i∈U N π i ≥ λ > 0, min i,j∈U N π ij ≥ λ * > 0 and lim sup N →∞ n N max i,j∈U N ,i =j |∆ ij | < ∞. (A8) Let D k,N be the set of all distinct k-tuples (i 1 , i 2 , ..., i k ) from U N . Then lim sup N →∞ n 2 N max (i 1 ,i 2 ,i 3 ,i 4 )∈D 4,N |E p [(I i 1 − π i 1 ) (I i 2 − π i 2 ) (I i 3 − π i 3 ) (I i 4 − π i 4 )]| < ∞, lim sup N →∞ n 2 N max (i 1 ,i 2 ,i 3 ,i 4 )∈D 4,N |E p [(I i 1 I i 2 − π i 1 i 2 ) (I i 3 I i 4 − π i 3 i 4 )]| < ∞, lim sup N →∞ n 2 N max (i 1 ,i 2 ,i 3 )∈D 3,N E p (I i 1 − π i 1 ) 2 (I i 2 − π i 2 ) (I i 3 − π i 3 ) < ∞.
Remark 2. Assumptions (A1)-(A3) are typical in the smoothing literature; see, for instance, [10,12,29]. Assumption (A5) is about how to choose the number of interior knots J N for the spline estimation in the first stage. In practice, J N can be determined by (10). Assumption (A6) is how to select the kernel function and the corresponding bandwidth. Such assumptions were used in [29] in the additive autoregressive model fitting. Assumptions (A7) and (A8) involve the inclusion probabilities of the design, which were also assumed in [1].


### Asymptotic properties of the estimator

Like the local polynomial estimators in [1], the following theorem shows that the estimatort y,SBLL in (9) is asymptotically design unbiased and design consistent. (9) is asymptotically design unbiased in the sense that
Theorem 2. Under Assumptions (A1)-(A7), the estimatort y,SBLL inlim N →∞ E p t y,SBLL − t y N = 0 with ξ-probability 1,
and is design consistent in the sense that for all η > 0,
lim N →∞ E p I {|ty,SBLL−ty|>Nη} = 0 with ξ -probability 1.
Lett y,SBLL be the population-based generalized difference estimator of t y when the entire realization were known; see (A.4) in Appendix A.1 for the formal definition. Like the estimators in the local polynomial estimators in [1], the penalized spline estimators in [2], and the backfitting estimators in [3], the following theorem shows that the proposed estimatort y,SBLL also inherits the limiting distribution of the "oracle" estimatort y,SBLL .


## Theorem 3. Under Assumptions (A1)-(A8),
N −1 t y,SBLL − t y Var 1/2 p N −1t y,SBLL d −→ N (0, 1) as N → ∞ implies N −1 t y,SBLL − t y V 1/2 N −1t y,SBLL d −→ N (0, 1) , where V N −1t y,SBLL = 1 N 2 i,j∈s ∆ ij π ij y i −m * i π i y j −m * j π j .(15)
The next theorem proves thatt y,SBLL is robust as in [1] and it also asymptotically attains the Godambe-Joshi lower bound to the anticipated variance
Var N −1 t y − t y = E N −1 t y − t y 2 − E 2 N −1 t y − t y ,
where the expectation is taken over both design, p N , and population ξ in (1).


## Theorem 4.

Under Assumptions (A1)-(A8),t y,SBLL asymptotically attains the Godambe-Joshi lower bound, in the sense that
n N E t y,SBLL − t y N 2 = n N N 2 i∈U N σ 2 (x i ) 1 − π i π i + o (1) .
The proofs of Theorems 2-4 are given in the Appendix.


## Auxiliary Variable Selection

In this section, we propose a BIC-based method to select the auxiliary variables for use in the superpopulation model (2).

The BIC was first proposed in [23] for the selection of parametric models. Recently, [14] proposed a fast and consistent model selection method based on spline estimation with the BIC to select significant lags in non-linear additive autoregression. Analogous to the approach in [14], if the entire realization were known by "oracle", one can select significant auxiliary variables based on the BIC. For an index set of variables r ∈ {1, ..., d}, the BIC is defined as
BIC (r) = log AMSE (r) N −1t y,SBLL + J r n N log(n N ),(16)
where J r = 1 + α∈r (J N + 1), and AMSE N −1t y,SBLL is the asymptotic mean squared error (AMSE) of N −1t y,SBLL in (A.11), i.e. the asymptotic expectation of N −1 t y,SBLL − t y 2 .

Next let f = n N /N be the fixed sampling fraction. Under simple random
sampling (SRS) design, if σ 2 (x) = c T x, AMSE N −1t y,SBLL = 1 − f n N (N − 1) i∈U N (y i −m * i ) 2 .
Thus, using similar arguments in Section 5 of [14], we can show that the above BIC in (16) is consistent under SRS.

By Theorem A.2, AMSE N −1t y,SBLL can be estimated consistently by
V g ≡ V g N −1t y,SBLL = 1 N 2 i,j∈s ∆ ij π ij g is (y i −m * i ) π i g js (y j −m * j ) π j ,(17)
a modified version of (15) proposed by [20] with the "g-weight" in (14). So the sample-based BIC is defined as
BIC (r) = log V (r) g + J r n N log(n N ),(18)
and we select the subsectr ⊂ {1, ..., d} that gives the smallest BIC value.


## Remark 3.

Under SRS design, the variance estimator given in (17) can be simplified as
V g = 1 − f n N (n N − 1) i∈s g 2 is (y i −m * i ) 2 .
In practice, we first decide on a set of candidate variables to be selected. Since a full search through all possible subsets of variables is in general computationally too costly in actual implementation of the BIC method, we consider a forward selection procedure and a backward selection procedure. Let d denote the total number of candidate variables to be selected from. In the forward selection procedure, we pre-specify the maximal number of variables d max = min d,
n N 2(J N +1)
that are allowed in the model, in which [a] denotes the integer part of a. We start from the empty set of auxiliary variables, add one variable at a time to the current model, choosing between the various candidate variables that have not yet been selected by minimizing BIC in (18). The process stops when the number of variables selected reaches d max . In the backward selection procedure, we start with a set of variables of the maximal size d max , delete one variable at a time by minimizing the BIC and stop when no variable remains in the model. If d max < d, we first apply the forward selection procedure, then we start with the maximal set of variables selected in the last step of the forward stage.


## Simulation Study

In this section, simulations are carried out to investigate the finite-sample performance oft y,SBLL . For comparison we also obtained the results of four other estimators: the HT estimator which does not make use of the auxiliary population, the linear regression (LREG) estimator in [21], the one-step linear spline (LS) estimator defined bŷ
t y,LS = i∈s (y i −m i )/π i + i∈U Nm i ,m i = N −1t y + d α=1m iα
withm iα ≡m α (x iα ) given in (5), and the single-index model-assisted (SIM) estimator in [27]. The number of knots J N for the LS and SBLL is determined by (10). For the superpopulation model (1), the following four additive models (no interactions) were considered:
2-dim linear: Y = −1 + 2X 3 + 4X 6 + σ 0 ε, 2-dim quadratic: Y = 5.5 − 6X 2 + 8(X 2 − .5) 2 − 3X 10 + 32(X 10 − .5) 3 + σ 0 ε, 3-dim mixed: Y = 8(X 2 − .5) 2 + exp (2X 5 − 1) + sin {2π(X 8 − .5)} + σ 0 ε, 5-dim sinusoid: Y = 2 + d α=1 sin {2π(X α − .5)} + σ 0 2 ( d α=1 X α ) 1/2 ε, d = 5.
The auxiliary variable vectors x i , i ∈ U N , were generated from i.i.d. uniform (0, 1) random vectors. The errors ε were generated from i.i.d. N (0, 1) with noise level σ 0 = 0.1, 0.4. The population size was N = 1000. SRS Samples were generated of size n N = 50, 100 and 200. For each combination of noise level and sample size, 1000 replicated SRS samples were selected from the same population, the estimators were calculated, and the design bias and the design mean squared errors were computed empirically. Table 1 shows the ratios of the mean squared error (MSE) for the various estimators to the proposed SBLL estimators. From the table, one sees that the model-assisted estimators, LREG, LS, SIM and SBLL, perform much better than the simple HT regardless the type of mean function, standard error and sample size. For Model 1, LREG is expected to be the preferred estimator, since the assumed model is correctly specified. However, not much efficiency is lost by using SBLL instead of LREG and the MSE ratios of LREG to SBLL are at least 0.89 for all cases. For all other scenarios, SBLL performs consistently better than LREG. The SBLL estimators also improve upon the LS estimators across almost every combination of noise level and sample size, which implies that our second local linear smoothing step is not redundant.

To see how fast the computation is, Table 1 also provides the average time of generating one sample of size n N and obtaining the SBLL estimator on an ordinary PC with Intel Pentium IV 1.86 GHz processor and 1.0 GB RAM. It shows that the proposed SBLL estimation is extremely fast. For instance, for Model 4, the SBLL estimation of a 5-dimensional of size 200 takes on average merely 0.2 second. We also carried out simulations for high dimensional data with sample size n N = 1000 generated from the population of size 10000. Remarkably, it takes on average less than 60 seconds to get the SBLL estimator even when the dimension reaches 50.

In Table 2 we give the Monte Carlo bias and standard error of the SBLL estimator based on its sampling distribution over 1000 replications. Table 2 also show the square root of the average estimated variance of the population total (15). We see that the biases of the SBLL estimator are very small and the variance estimator appears to perform well for medium sample size.

Next we conducted simulations to evaluate the performance of the variable selection method. We generated 100 replications for each of the above models. The variables were searched from {1, 2, ..., 10} for all methods and we set the maximum number of variables allowed in the model to be 10. Table 3 shows the number of correct fit (C), underfit (U) and overfit (O) based on the BIC in (18) over 100 simulation runs. Here underfitting means that the method misses at least one of the significant variables. From Table 3, we can see that both the forward and the backward selection procedures perform very well for moderately large sample size. We also obtained the ratio of MSE of the SBLL estimates calculated by using the selected model to the MSE of the oracle SBLL estimates computed by using the true model. In all the cases, the ratios are very close to 1 or exactly 1 for moderately large sample size.


## Discussion

Nonparametric additive methods enhance the flexibility of the models that survey practitioners use. However, due to the limitations in either interpretability, computational complexity or theoretical reliability, these models have not been widely used as general tools in survey data analysis. In this paper, we have advanced additive models as flexible, computationally efficient and theoretically attractive tools for studying survey data. We also developed a consistent procedure to select the significant auxiliary variables under simple random sampling design.

The proposed method in this paper is appropriate only for survey data that follow simple additive model. The limitation of the basic additive model is that the interactions between the input features are not considered. There are other models, for instance, single-index model [27], additive model with second-order interaction terms [24], which reduce dimensionality but also incorporate interactions. Additive partially linear model [11] is another parsimonious candidate when one believes that the relationship between the study variable and some of the auxiliary variables has a parametric form, while the relationship between the study variable and the remaining auxiliary covariates may not be linear. These alternative models are supposed to be more efficient in certain cases, but obtaining the asymptotics is likely to be very complicated, thus we leave it as future research work.

Finally, in our methodology development, we have assumed that the auxiliary variables are available for all population elements. It would be interesting to consider the limited auxiliary information case [5] where only some summary quantities such as means are available at the population level. This is also a challenging problem for future research.


## Appendix

To show the asymptotic properties of the proposed estimatort y,SBLL , we first introduce an "oracle" SBLL estimator of t y if the entire realization were known.


## A.1. The Population-based Estimator

If the entire realization were known, let
Γ U = Γ (x i ) T i∈U N
be the populationbased truncated power spline matrix, where Γ (x) is given in (3). Let y be the vector of the response values y i for i ∈ U N . Further let B U = (Γ T U Γ U ) −1 Γ T U y. The centered pilot estimators of m α (x α ) at the first stage is
m α (x α ) = Γ (x) T D α B U − N −1 1 T N Γ U D α B U , (A.1)
where vector 1 T N = {1, 1, ..., 1} of length N. The pilot estimators for all elements in the population is denoted bỹ
m α ≡ {m α (x iα )} i∈U N = I − N −1 1 N 1 T N Γ U D α B U , α = 1, ..., d.
For the second stage kernel smoothing, define the matrices
X U iα = 1 x kα − x iα k∈U N , W U iα = diag {K h (x kα − x iα )} k∈U N .
Then the SBLL estimator of each component at x i is given bỹ
m * iα ≡ e T 1 (X T U iα W U iα X U iα ) −1 X T U iα W U iαỹα , (A.2)
whereỹ α = y − 1 N t y 1 N − β =αm β is collection of the pseudo-responses. The SBLL estimator of m (x i ) based on the entire population is given bỹ
m * i = 1 N t y + d α=1m * iα , i ∈ U N . (A.3)
Clearly,m * i is the prediction at x i based on the entire finite population. If thesẽ m * i were known, a design-unbiased estimator of t y would bẽ
t y,SBLL = i∈U Nm * i + i∈s y i −m * i π i . (A.4)
The proof of the asymptotic properties oft y,SBLL uses reasoning similar to that in [1], in which a key step is the Taylor linearization. Recall that our proposed estimator involves two smoothing stages: spline smoothing in the first stage and kernel smoothing in the second stage. In the following, we establish the Taylor linearization for these two smoothing stages one by one.


## A.2. Taylor Linearization at the First Stage

Lemma A.1. Under Assumptions (A1)-(A7), for any α = 1, ..., d,
lim N →∞ sup xα∈[0,1] |m α (x α ) −m α (x α )| = O p J N (N −1 log N ) 1/2 , wherem α (x α ) andm α (x α )
are the pilot estimators given in (11) and (A.1).
Proof. Let S = N −1 Γ T U Γ U and V = N −1 Γ T U y be matrices with components s jj ′ = N −1
k∈U N Γ U,kj Γ U,kj ′ and v j = N −1 k∈U N Γ U,kj y k , respectively. Denote S π = N −1 Γ T s Π s Γ s and V π = N −1 Γ T s Π s y s the sample versions of the matrices S and V with components s π,jj ′ = N −1 k∈s Γ s,kj Γ s,kj ′ / π k and v π,j = N −1 k∈s Γ s,kj y k / π k . For each α = 1, ..., d and the spline basis Γ (x) in (3), let
ζ (S π , V π ; x α ) = Γ (x) T D α S −1 π V π − S −1 V (A.5)
be a nonlinear function of {s π,jj ′ } 1≤j,j ′ ≤G d and {v π,j } G d j=1 with respect to x α . The differencem α (x α ) −m α (x α ) = ζ (S π , V π ; x α ) + O p (N −1/2 ). Simple calculation shows that the first order derivatives of ζ in (A.5) of s π,jj ′ and v π,j are
∂ζ ∂s π,jj ′ = Γ (x) T D α −S −1 π Λ jj ′ S −1 π V π , 1 ≤ j, j ′ ≤ G d , ∂ζ ∂v π,j = Γ (x) T D α S −1 π λ j , 1 ≤ j ≤ G d ,
where λ j is a G d -vector with "1" in the jth component and "0" elsewhere; and Λ jj ′ is a G d × G d matrix with "1" in positions (j, j ′ ) and (j ′ , j) and "0" everywhere else.

Using the Taylor linearization, one can approximate ζ in (A.5) by a linear one so that the difference betweenm α (x α ) andm α (x α ) can be decomposed as
G d j=1 ϕ αj (x α ) (v π,j − v j )−ĉ α +c α − 1≤j,j ′ ≤G d ψ αjj ′ (x α ) (s π,jj ′ − s jj ′ )+Q αN (x α ), where for any 1 ≤ j, j ′ ≤ G d , ϕ αj (x α ) = ∂ζ ∂v π,j v π,j =v j = Γ (x) T D α S −1 λ j , ψ αjj ′ (x α ) = ∂ζ ∂s π,jj ′ s π,jj ′ =s jj ′ = Γ (x) T D α S −1 Λ jj ′ S −1 V, and Q αN (x α ) is the remainder. Note that G d j=1 ϕ αj (x α ) (v π,j − v j ) = N −1 k∈U N G d j=1 ϕ αj (x α ) Γ U,kj y k 1 − I k π k −N −1 k∈U N G d j=1 ϕ αj (x α ) (Γ U,kj − Γ s,kj ) 1 − I k π k y k +N −1 k∈U N G d j=1 ϕ αj (x α ) y k (Γ U,kj − Γ s,kj ) .
By the discretization method given in Lemma A.4 of [29], the Borel-Cantelli Lemma entails that each single term in the right hand side of the above is of the order O p J N (N −1 log N) 1/2 . Therefore, we have
sup xα∈[0,1] G d j=1 ϕ αj (x α ) (v π,j − v j ) = O p J N (N −1 log N ) 1/2 .

## Similar arguments lead to sup
xα∈[0,1] 1≤j,j ′ ≤G d ψ αjj ′ (x α ) (s π,jj ′ − s jj ′ ) is of the order O p J N (N −1 log N ) 1/2 , and sup xα∈[0,1] |Q αN (x α )| = o p J N (N −1 log N) 1/2 . Thus sup xα∈[0,1] ζ (S π , V π ; x α ) = O p J N (N −1 log N) 1/2 . The desired result is established.

## A.3. Taylor Linearization at the Second Stage

Let
t iαq = k∈U N K h (x kα − x iα ) (x kα − x iα ) q−1 , t iαq = k∈s 1 π k K h (x kα − x iα ) (x kα − x iα ) q−1 ,
for q = 1, 2, 3 and
t iαq = k∈U N K h (x kα − x iα ) (x kα − x iα ) q−4ỹ kα , t iαq = k∈s 1 π k K h (x kα − x iα ) (x kα − x iα ) q−4ŷ kα ,
for q = 4, 5. We rewritem * iα in (A.2) andm * iα in (13) bỹ
m * iα = t iα3 t iα4 − t iα2 t iα5 t iα1 t iα3 − t 2 iα2 ,m * iα =t iα3tiα4 −t iα2tiα5 t iα1tiα3 −t 2 iα2 . Let z iαk = 5 q=1 ∂m * iα ∂(N −1t iαq ) t iα =t iα z iαkq , where t iα = {t iαq } 5 q=1 and z iαkq = K h (x kα − x iα ) (x kα − x iα ) q−1 , for q = 1, 2, 3, K h (x kα − x iα ) (x kα − x iα ) q−4 y kα , for q = 4, 5.
Then one can approximatem * iα −m * iα by a linear sum, i.e.,
m * iα −m * iα = 1 N k∈U N z iαk I k π k − 1 − L iαN + R iαN , (A.6) where L iαN = 4 q=1 L iαN q with L iαN 1 = 1 N 2 t y − t y ∂m * iα ∂ N −1t iα4 t iα =t iα k∈U N z iαk1 I k π k − 1 , L iαN 2 = 1 N ∂m * iα ∂ N −1t iα4 t iα =t iα k∈U N z iαk1 I k π k − 1 β =α {m β (x kβ ) −m β (x kβ )} , L iαN 3 = 1 N 2 t y − t y ∂m * iα ∂ N −1t iα5 t iα =t iα k∈U N z iαk2 I k π k − 1 , L iαN 4 = 1 N ∂m * iα ∂ N −1t iα5 t iα =t iα k∈U N z iαk2 I k π k − 1 β =α {m β (x kβ ) −m β (x kβ )} ,
and R iαN is the remainder. Similar to the proof of Lemma 3 in [1],
n N N i∈U N E p R 2 iαN = O n −1 N h −2 N . (A.7) Lemma A.2. Under Assumptions (A1)-(A8), N −1 i∈U N E p (L 2 iαN ) → 0. Proof.
By the Cauchy-Schwartz inequality, it suffices to show that for q = 1, ..., 4, N −1 i∈U N E p L 2 iαN q → 0. Without loss of generality, we only show the cases for q = 1 and 2. Similarly to the proof of Lemma 2 (v) in [1], the first order derivatives ofm * iα with respect to N −1t iαq evaluated att i = t i are uniformly bounded in i. So by Assumption (A7)  (8),
1 N i∈U N E p L 2 iαN 1 = 1 N 5 E p   t y − t y ∂m * iα ∂ N −1t iα4 t i =t i k∈U N z iαk1 I k π k − 1 2   ≤ C N 5 j,k,l,p∈U N z iαj1 z iαl1 y k y p π jl − π j π l π j π l π kp − π k π p π k π p ≤ C N 3 k,p∈U N |y k y p | → 0. Next 1 N i∈U N E p (L iαN 2 ) 2 = 1 N 3 E p ∂m * iα ∂ N −1t iα4 t iα =t iα k∈U N z iαk1 I k π k − 1 β =α {m β (x kβ ) −m β (x kβ )} 2 ≤ CN −3 k∈U N l∈U N E p I k π k − 1 I l π l − 1 × β =α γ =α {m β (x kβ ) −m β (x kβ )} {m γ (x lγ ) −m γ (x lγ )} . By Lemma A.1, N −1 i∈U N E p (L 2 iαN 2 ) → 0,lim N →∞ 1 N E p i∈U N (m * i −m * i ) 2 = 0.
Proof. According to (A.6), one has
1 N E p i∈U N (m * iα −m * iα ) 2 = 1 N 3 i∈U N k,l∈U N ∆ kl z iαk π k z iαl π l − 2 N 2 i,k∈U N z iαk E p I k π k − 1 (L iαN − R iαN ) + 1 N i∈U N E p (L iαN − R iαN ) 2 .
Following from Lemma 4 in [1] and Assumption (A7), the first term converges to zero as N → ∞. The third term also converges to zero by (A.7) and Lemma A.2.

By the Cauchy-Schwartz inequality, lim N →∞
1 N E p i∈U N (m * iα −m * iα ) 2 = 0, α = 1, ..., d. Note that i∈U N (m * i −m * i ) 2 = i∈U N 1 N t y − t y + d α=1 (m * iα −m * iα ) 2 = 1 N t y − t y 2 + 2 N t y − t y i∈U N d α=1 (m * iα −m * iα ) + i∈U N d α=1 (m * iα −m * iα ) 2 .
By Assumption (A.7),
1 N 2 E p t y − t y 2 ≤ 1 λ + n N max i,j∈U N ,i =j |∆ ij | λ 2 1 N 2 i∈U N y 2 i → 0.
Thus the desired result is obtained from the Cauchy-Schwartz inequality.

Proof of Theorem 2. Note that E p [I i ] = π i and t y,
SBLL − t y N = 1 N i∈U N (y i −m * i ) (I i /π i − 1)+ 1 N i∈U N (m * i −m * i ) (1 − I i /π i ) . (A.8) Then E p t y,SBLL − t y N ≤ 1 N E p i∈U N (y i −m * i ) (I i /π i − 1) (A.9) + 1 N 2 E p i∈U N (m * i −m * i ) 2 E p i∈U N (1 − I i /π i ) 2 1/2 .
According to Assumptions (A1)-(A6), lim sup N →∞ 1 N i∈U N (y i −m * i ) 2 < ∞. Following the same arguments of Theorem 1 in [1], the first term on the right of (A.9) converges to zero as N → ∞. For the second term, (A7) implies that
E p 1 N i∈U N (1 − I i /π i ) 2 = i∈U N π i (1 − π i ) Nπ 2 i ≤ 1 λ . According to Lemma A.3, lim N →∞ 1 N i∈U N E p (m * i −m * i ) 2
→ 0 and the result follows from the Markov's inequality.

The next theorem is to derive the asymptotic mean squared error of the proposed spline estimator in (9).
Theorem A.1. Under Assumptions (A1)-(A8), n N E p t y,SBLL − t y N 2 = n N N 2 i,j∈U N ∆ ij y i −m * i π i y j −m * j π j + o (1) . (A.10) Denote AMSE N −1t y,SBLL = 1 N 2 i,j∈U N ∆ ij y i −m * i π i y j −m * j π j (A.11)
the asymptotic mean squared error in (A.10). The next result shows that it can be estimated consistently by V N −1t y,SBLL in (15).
Theorem A.2. Under (A1)-(A8), lim N →∞ n N E p V N −1t y,SBLL − AMSE N −1t y,SBLL = 0.
The proofs of Theorems A.1 and A.2 are somewhat trivial and we refer the readers to [28].

Proof of Theorem 3. According to (A.8),
t y,SBLL − t y N =t y,SBLL − t y N + i∈U Nm * i −m * i N I i π i − 1 .

## From the proof of Theorem
A.1, i∈U Nm * i −m * i N I i π i − 1 = o p n −1/2 N . Theorem
A.2 implies that V N −1t y,SBLL /AMSE N −1t y,SBLL → 1 in probability. The desired result follows.

Proof of Theorem 4. Let
T 1 = n 1/2 N N i∈U N {m * i − m (x i )} 1 − I i π i , T 2 = n 1/2 N N i∈U N (m * i −m * i ) 1 − I i π i ,T 3 = n 1/2 N N i∈U N σ (x i ) ε i I i π i − 1 .
Then n 1/2 N N −1 t y,SBLL − t y can be represented as the sum of T 1 , T 2 and T 3 . For the first term,
ET 2 1 = n N N 2 i,j∈U N E (m (x i ) −m * i ) m (x j ) −m * j ∆ ij π i π j ≤ n N N 1 λ + N max i,j∈U N ,i =j |∆ ij | λ 2 1 N i∈U N E {m (x i ) −m * i } 2 .
By Theorem 2.1 in [29], |m (x i ) −m * i | = o p n −2/5 log n , for any i ∈ U N , which implies that ET 2 1 → 0. Now for T 2
ET 2 2 ≤ n N N 1 λ + N max i,j∈U N ,i =j |∆ ij | λ 2 1 N i∈U N E (m * i −m * i ) 2 .
By Lemma A.1, ET 2 2 → 0. Finally,
ET 2 3 = n N N 2 i∈U N σ 2 (x i ) 1 − π i π i ≤ n N Nλ 1 N i∈U N σ 2 (x i ) , lim sup N →∞ ET 2 3 ≤ 1 λ lim sup N →∞ 1 N i∈U N σ 2 (x i ) < ∞.
By the Cauchy-Schwartz inequality the cross product terms go to zero as N → ∞. The desired result follows.   

## .
Under Assumptions (A1)-(A8), for the population and sample based SBLL estimators of m (x iα ) given in (A.3) and

## Table 1 :
1Ratio of MSE of the HT, LREG, LS and SIM estimators to that of the SBLL estimator and the average computing time of the SBLL estimator based on 1000 replications of SRS samples from four fixed populations of size N = 1000.Model 
Error Sample size 
MSE Ratio 
SBLL 
σ 
n N 
HT LREG 
LS 
SIM (seconds) 

1 

0.1 

50 140.36 
0.89 1.12 
1.60 
0.07 
100 148.03 
0.91 1.07 
1.33 
0.07 
200 147.03 
0.92 1.10 
1.02 
0.09 

0.4 

50 
9.78 
0.92 1.16 
1.24 
0.07 
100 
10.50 
0.95 1.10 
1.02 
0.07 
200 
10.47 
0.98 1.05 
1.04 
0.09 

2 

0.1 

50 134.05 
28.38 2.11 19.77 
0.07 
100 282.47 
58.10 1.03 36.58 
0.07 
200 313.93 
66.63 0.98 41.15 
0.09 

0.4 

50 
18.45 
4.25 2.36 
3.44 
0.07 
100 
23.67 
5.34 1.04 
3.69 
0.07 
200 
23.36 
5.63 1.02 
3.92 
0.09 

3 

0.1 

50 
63.14 
30.83 1.10 37.12 
0.07 
100 103.33 
49.62 1.01 50.76 
0.07 
200 115.13 
56.57 1.02 57.04 
0.09 

0.4 

50 
6.80 
3.46 1.11 
3.93 
0.07 
100 
8.18 
4.20 1.14 
4.40 
0.07 
200 
18.39 
4.52 1.09 
4.57 
0.09 

4 

0.1 

50 
55.81 
25.26 1.01 27.61 
0.07 
100 151.59 
62.63 1.03 65.78 
0.07 
200 230.44 
97.91 0.97 99.45 
0.09 

0.4 

50 
9.97 
4.75 1.03 
5.22 
0.07 
100 
16.35 
7.10 1.01 
7.44 
0.07 
200 
19.95 
8.60 1.05 
8.74 
0.09 


## Table 2 :
2Monte Carlo bias, standard error and the square root of the average estimated variances (15) of the population total based on 1000 simulations.Model 
σ 
n 
Bias 
SE 
Est. SE 

1 

0.1 

50 
−0.10 
14.69 
13.18 
100 
−0.36 
9.85 
9.32 
200 
−0.13 
6.55 
6.29 

0.4 

50 
−1.62 
57.73 
51.81 
100 
−1.55 
38.51 
36.77 
200 
−0.42 
25.71 
24.86 

2 

0.1 

50 
1.27 
24.49 
14.06 
100 
0.62 
11.52 
9.13 
200 
0.37 
7.06 
6.10 

0.4 

50 
2.41 
67.66 
52.45 
100 
−0.47 
40.94 
36.15 
200 
−0.13 
26.54 
24.33 

3 

0.1 

50 
2.29 
20.40 
13.38 
100 
0.90 
10.91 
8.74 
200 
0.48 
6.82 
5.88 

0.4 

50 
2.17 
64.89 
50.86 
100 
−0.04 
40.17 
35.44 
200 
0.32 
26.30 
23.99 

4 

0.1 

50 
−1.98 
29.04 
18.04 
100 
−0.51 
12.28 
8.22 
200 
−0.10 
6.38 
4.82 

0.4 

50 
−4.38 
69.69 
43.31 
100 
−1.18 
37.92 
27.72 
200 
−0.37 
22.58 
18.56 


## Table 3 :
3Simulation results for auxiliary variable selection based on 100 replications of SRS samples from four fixed populations of size N = 1000. (Here the MSE Ratio is the ratio of MSE of the SBLL estimator calculated by using the selected model to the MSE of the oracle SBLL estimates computed by using the true model.)Model 
σ 0 
n 
Forward 
Backward 

C 
U 
O 
MSE 
C 
U 
O 
MSE 
Ratio 
Ratio 

1 

0.1 

50 
72 
0 
28 
1.150 
73 
0 
27 
1.124 
100 
97 
0 
3 
1.001 
97 
0 
3 
1.001 
200 
99 
0 
1 
0.999 
99 
0 
1 
0.999 

0.4 

50 
76 
0 
24 
1.147 
77 
0 
23 
1.145 
100 
98 
0 
2 
1.002 
98 
0 
2 
1.002 
200 
100 
0 
0 
1.000 
100 
0 
0 
1.000 

2 

0.1 

50 
87 
0 
13 
1.255 
87 
0 
13 
1.255 
100 
96 
0 
4 
1.012 
96 
0 
4 
1.012 
200 
100 
0 
0 
1.000 
100 
0 
0 
1.000 

0.4 

50 
79 
0 
21 
1.019 
80 
0 
20 
1.022 
100 
98 
0 
2 
1.000 
98 
0 
2 
1.000 
200 
100 
0 
0 
1.000 
100 
0 
0 
1.000 

3 

0.1 

50 
87 
0 
13 
1.082 
86 
0 
14 
1.082 
100 
91 
0 
9 
1.000 
91 
0 
9 
1.001 
200 
100 
0 
0 
1.000 
100 
0 
0 
1.000 

0.4 

50 
83 
0 
17 
1.020 
83 
0 
17 
1.020 
100 
99 
0 
1 
1.000 
99 
0 
1 
1.000 
200 
100 
0 
0 
1.000 
100 
0 
0 
1.000 

4 

0.1 

50 
68 
0 
32 
1.277 
69 
0 
31 
1.277 
100 
88 
0 
12 
1.029 
88 
0 
12 
1.029 
200 
100 
0 
0 
1.000 
100 
0 
0 
1.000 

0.4 

50 
69 
0 
31 
1.063 
69 
0 
31 
1.063 
100 
97 
0 
3 
1.000 
97 
0 
3 
1.031 
200 
100 
0 
0 
1.000 
100 
0 
0 
1.000 

AcknowledgmentThe research work of the first author was supported by NSF grant DMS-0905730. The authors thank Professor Lijian Yang for helpful discussions. The authors would also like to thank two anonymous referees for their insightful comments.
Local polynomial regression estimators in survey sampling. F J Breidt, J D Opsomer, Ann. Statist. 28F.J. Breidt, J.D. Opsomer, Local polynomial regression estimators in survey sampling, Ann. Statist. 28 (2000) 1026-1053.

Model-assisted estimation for complex surveys using penalised splines. F J Breidt, G Claeskens, J D Opsomer, Biometrika. 92F.J. Breidt, G. Claeskens, J.D. Opsomer, Model-assisted estimation for complex surveys using penalised splines, Biometrika 92 (2005) 831-846.

Semiparametric model-assisted estimation for natural resource surveys. F J Breidt, J D Opsomer, A A Johnson, M G Ranalli, Survey Methodology. 33F.J. Breidt, J.D. Opsomer, A.A. Johnson, M.G. Ranalli, Semiparametric model-assisted estimation for natural resource surveys, Survey Methodology 33 (2007) 35-44.

Robust case-weighting for multipurpose establishment surveys. R L Chambers, J Official Statist. 12R.L. Chambers, Robust case-weighting for multipurpose establishment sur- veys, J Official Statist. 12 (1996) 3-32.

Limited information likelihood analysis of survey data. R L Chambers, A H Dorfman, S Wang, J. Roy. Statist. Soc. Ser. B. 60R.L. Chambers, A.H. Dorfman, S. Wang, Limited information likelihood analysis of survey data, J. Roy. Statist. Soc. Ser. B 60 (1998) 397-411.

Bias robust estimation in finite populations using nonparametric calibration. R L Chambers, A H Dorfman, T E Wehrly, J. Amer. Statist. Assoc. 88R.L. Chambers, A.H. Dorfman, T.E. Wehrly, Bias robust estimation in finite populations using nonparametric calibration, J. Amer. Statist. Assoc. 88 (1993) 268-277.

Calibration estimators in survey sampling. J C Deville, C E Särndal, J. Amer. Statist. Assoc. 87J.C. Deville, C.E. Särndal, Calibration estimators in survey sampling, J. Amer. Statist. Assoc. 87 (1992) 376-382.

Nonparametric regression for estimating totals in finite populations. A H Dorfman, Proceedings of the Section on Survey Research Methods. the Section on Survey Research MethodsAlexandria, VAA.H. Dorfman, Nonparametric regression for estimating totals in finite pop- ulations, Proceedings of the Section on Survey Research Methods (1992) 622-625. Amer. Statist. Assoc., Alexandria, VA.

Estimators of the finite population distribution function using nonparametric regression. A H Dorfman, P Hall, Ann. Statist. 21A.H. Dorfman, P. Hall, Estimators of the finite population distribution function using nonparametric regression, Ann. Statist. 21 (1993) 1452-1475.

J Fan, I Gijbels, Local Polynomial Modelling, Its Applications. Chapman, Hall, LondonJ. Fan, I. Gijbels, Local Polynomial Modelling, Its Applications, Chapman, Hall, London, 1996.

A kernel-based method for estimating additive partially linear models. Y Fan, Q Li, Statist. Sinica. 13Y. Fan, Q. Li, A kernel-based method for estimating additive partially linear models, Statist. Sinica 13 (2003) 739-762.

Applied Nonparametric Regression. W Härdle, Cambridge University PressCambridgeW. Härdle, Applied Nonparametric Regression, Cambridge University Press, Cambridge, 1990.

T J Hastie, R J Tibshirani, Generalized Additive Models. Chapman, Hall, LondonT.J. Hastie, R.J. Tibshirani, Generalized Additive Models, Chapman, Hall, London, 1990.

Identification of nonlinear additive autoregression models. J Z Huang, L Yang, J. Roy. Statist. Soc. Ser. B. 66J.Z. Huang, L. Yang, Identification of nonlinear additive autoregression models, J. Roy. Statist. Soc. Ser. B 66 (2004) 463-477.

Calibration estimation using empirical likelihood in survey sampling. J K Kim, Statist. Sinica. 19J.K. Kim, Calibration estimation using empirical likelihood in survey sam- pling, Statist. Sinica 19 (2009) 145-158.

A kernel method of estimating structured nonparametric regression based on marginal integration. O B Linton, J P Nielsen, Biometrika. 82O.B. Linton, J.P. Nielsen, A kernel method of estimating structured non- parametric regression based on marginal integration, Biometrika 82 (1995) 93-101.

The existence, asymptotic properties of a backfitting projection algorithm under weak conditions. E Mammen, O Linton, J Nielsen, Ann. Statist. 27E. Mammen, O. Linton, J. Nielsen, The existence, asymptotic properties of a backfitting projection algorithm under weak conditions, Ann. Statist. 27 (1999) 1443-1490.

Finite sample performance of kernel-based regression methods for non-parametric additive models under common bandwidth selection criterion. C Martins-Filho, K Yang, J. Nonparametr. Stat. 19C. Martins-Filho, K. Yang, Finite sample performance of kernel-based re- gression methods for non-parametric additive models under common band- width selection criterion, J. Nonparametr. Stat. 19 (2007) 23-62.

Model-assisted estimation of forest resources with generalized additive models (with discussion). J D Opsomer, F J Breidt, G G Moisen, G Kauermann, J. Amer. Statist. Assoc. 102J.D. Opsomer, F.J. Breidt, G.G. Moisen, G. Kauermann, Model-assisted estimation of forest resources with generalized additive models (with dis- cussion), J. Amer. Statist. Assoc. 102 (2007) 400-416.

The weighted residual technique for estimating the variance of the general regression estimator of the finite population total. C E Särndal, B Swensson, J Wretman, Biometrika. 76C.E. Särndal, B. Swensson, J. Wretman, The weighted residual technique for estimating the variance of the general regression estimator of the finite population total, Biometrika 76 (1989) 527-537.

C E Särndal, B Swensson, J Wretman, Model Assisted Survey Sampling. New YorkSpringer-VerlagC.E. Särndal, B. Swensson, J. Wretman, Model Assisted Survey Sampling, Springer-Verlag, New York, 1992.

C E Särndal, S Lundström, Estimation in Surveys with Nonresponse. New YorkWileyC.E. Särndal, S. Lundström, Estimation in Surveys with Nonresponse, Wi- ley, New York, 2005.

Estimating the dimension of a model. G E Schwarz, Ann. Statist. 6G.E. Schwarz, Estimating the dimension of a model, Ann. Statist. 6 (1978) 461-464.

Nonparametric estimation and testing of interaction in additive models. S Sperlich, D Tjøstheim, L Yang, Econom. Theory. 18S. Sperlich, D. Tjøstheim, L. Yang, Nonparametric estimation and testing of interaction in additive models, Econom. Theory 18 (2002) 197-251.

Additive regression, other nonparametric models. C J Stone, Ann. Statist. 13C.J. Stone, Additive regression, other nonparametric models, Ann. Statist. 13 (1985) 689-705.

The use of polynomial splines, their tensor products in multivariate function estimation. C J Stone, Ann. Statist. 22C.J. Stone, The use of polynomial splines, their tensor products in multi- variate function estimation, Ann. Statist. 22 (1994) 118-184.

Single-index model-assisted estimation in survey sampling. L Wang, J. Nonpar. Statist. 21L. Wang, Single-index model-assisted estimation in survey sampling, J. Nonpar. Statist. 21 (2009) 487-504.

Nonparametric additive model-assisted estimation for survey data. L Wang, S Wang, L. Wang, S. Wang, Nonparametric additive model-assisted estimation for survey data, http://arxiv.org.

Spline-backfitted kernel smoothing of nonlinear additive autoregression model. L Wang, L Yang, Ann. Statist. 35L. Wang, L. Yang, Spline-backfitted kernel smoothing of nonlinear additive autoregression model, Ann. Statist. 35 (2007) 2474-2503.

A new estimator for the finite population distribution function. S Wang, A H Dorfman, Biometrika. 83S. Wang, A.H. Dorfman, A new estimator for the finite population distri- bution function, Biometrika 83 (1997) 639-652.