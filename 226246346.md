# Video Generative Adversarial Networks: A Review

CorpusID: 226246346
 
tags: #Computer_Science, #Engineering

URL: [https://www.semanticscholar.org/paper/5c1b1ab734ebc6ba56ea5d8af9d01a9fe8e0ba8b](https://www.semanticscholar.org/paper/5c1b1ab734ebc6ba56ea5d8af9d01a9fe8e0ba8b)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

Video Generative Adversarial Networks: A Review


Nuha Aldausari 
University of New South Wales
Australia

Arcot Sowmya 
Princess Nourah bint Abdulrahman University
University of New South Wales
KSAAustralia

Nadine Marcus 
University of New South Wales
Australia

Gelareh Mohammadi 
University of New South Wales
Australia

Video Generative Adversarial Networks: A Review
and Phrases: Generative Adversarial NetworksVideo synthesisMultimodal dataconditional generationVideo generationsurveyreview
With the increasing interest in the content creation field in multiple sectors such as media, education, and entertainment, there is an increasing trend in the papers that uses AI algorithms to generate content such as images, videos, audio, and text. Generative Adversarial Networks (GANs) in one of the promising models that synthesizes data samples that are similar to real data samples. While the variations of GANs models in general have been covered to some extent in several survey papers, to the best of our knowledge, this is among the first survey papers that reviews the state-of-the-art video GANs models. This paper first categorized GANs review papers into general GANs review papers, image GANs review papers and special field GANs review papers such as anomaly detection, medical imaging, or cybersecurity. The paper then summarizes the main improvements in GANs frameworks that are not initially developed for the video domain but have been adopted in multiple video GANs variations. Then, a comprehensive review of video GANs models is provided under two main divisions according to the presence or non-presence of a condition. The conditional models then further grouped according to the type of condition into audio, text, video, and image. The paper is concluded by highlighting the main challenges and limitations of the current video GANs models. A comprehensive list of datasets, applied loss functions, and evaluation metrics is provided in the supplementary material.Additional

# INTRODUCTION

The field of Computer Vision mainly deals with two types of data, namely images and videos, and these data can be used in many real-life applications for data generation, editing and classification. Data generation has gained significant attention since Ian Goodfellow released a model called Generative Adversarial Networks (GANs) in 2014 [1]. According to Google Scholar, there is an upward trend since the mid 2010's in publications when specifying "generative adversarial networks" as a search keyword, as demonstrated in Figure 1.

discriminator, contest against each other, while the word "Networks" illustrates that the model consists of (two) networks. The networks could be fully connected neural networks, convolutional neural networks, recurrent neural networks, long short term memory neural networks, autoencoders or any combination thereof.

GANs consist of two networks competing in a minimax game, as illustrated in Figure 2a. One network, called the generator, takes a random noise vector as input, and produces new instances by learning to follow real data distribution. On the other hand, the discriminator accepts training data and the data that is synthesized by the generator. The discriminator is a classification network that is supposed to classify real training data samples as "1"s and the generated data points as "0"s. The objective of the generator is to generate samples that cannot be distinguished from the real data samples by the discriminator. On the contrary, the discriminator targets the discrimination of real data samples from fake ones via classification. The two networks compete in a minimax game to improve each other's performance. The ultimate goal for both networks is to reach the Nash equilibrium, which is a state where neither of the networks can improve by changing their parameters. In practice, however, it is difficult to find the Nash equilibrium [12].

The objective function for the generator (G) and the discriminator (D), as previously stated, is defined as a loss function (L) [1]:

!"# $ !&' ( )(+, -) = 0 1∼3 4 (1) [678 +(')] + 0 ;∼3 < (;) [678(1 − +(-(?)))]

(1)

In equation (1), 0 @ is the expectation with respect to a distribution p, p B refers to the distribution of the real data, and p C is the distribution of the input noise vector ?. The discriminator is trained to recognize the real samples ', and produce high values close to one. Therefore 0 1∼3 4 (1) [678 +(')] should be maximized.

Meanwhile, the discriminator is also trained to recognize the fake samples -(?) and produces low values close to zero, which means maximizing 0 ;∼3 < (;) [678(1 − +(-(?)))]. In contrast, the generator needs to generate samples -(?) that are similar to the real samples in order to fool the discriminator 0 ;∼3 < (;) [678(1 − +(-(?))).


## Conditional GAN

In vanilla GANs [1], the model is unable to control the type of the generated samples. The generated data points could be from any category of the training data distribution. When sampling occurs, the generated samples might not represent all possible variations of the training data. In contrast, conditional GAN (CGAN) [13] adds a condition to both the generator and the discriminator, as shown in Figure 2b. The condition might be a class, text or any other type of data, and the generated data is expected to match the condition. The loss function for CGAN is a modified version of the GANs loss function in equation (1):

!"# $ !&' ( )(+, -) = 0 1∼3 4 (1) [678 +('|R)] + 0 ;∼3 < (;) [678(1 − +(-(?|R))] [13] (2) where c is the condition added to the model. Information Maximizing GAN, InfoGAN [14] for short, uses a slightly different approach to control the generation process. As illustrated in Figure 2c, the input in InfoGAN [14] to the generator is a noise vector along with another variable. Unlike the condition in CGAN, the variable vector in InfoGAN is unknown. The purpose of using the variable is to control specific properties in the generated samples by maximizing the mutual information between this variable and the generated samples. Through training InfoGAN, the generator learns how to disentangle certain properties in the generated samples in an unsupervised manner through another model called the auxiliary model. The auxiliary model shares the same parameters as the discriminator. The aim of the auxiliary model is to predict the properties that are disentangled while the discriminator's purpose is to distinguish real samples from fake ones. After training, the generated samples can be controlled by specifying some features that are learned such as colour, shape, rotation in image samples or class [14]. The loss function for InfoGAN is defined as follows: [14] (
!"# $ !&' ( )(+, -) − ST (R; -(?, R))
The loss function is the same as CGAN, but with an additional term ST (R; -(?, R)) to represent the mutual information loss between the variable c and the generated samples [14].

Auxiliary Classifier GAN (AC-GAN) [15] is another framework under conditional GANs-based architectures.

AC-GAN frameworks share characteristic with InfoGAN and another with CGAN. In terms of the similarity with CGAN, a condition is fed into the generator, which can be a text, class or any other type of data. However, the condition is not an input for the discriminator. The common factor between AC-GANs and InfoGAN is that there is an auxiliary classifier that outputs the class of the input sample. The differences in the overall architecture between CGAN, InfoGAN, and AC-GAN are demonstrated in Figure 2b, 2c, and 2d respectively. The loss function of AC-GAN is divided into two terms: one for evaluating the predicted class and another one to discriminate fake from real samples. The reviewed papers under conditional video generation (see section 4.2) use one of the conditional GANsbased architectures. The reason behind choosing the conditional architecture is that conditions can enhance the network stability and provide higher quality samples [13].


## Convolutional GANs

Vanilla GANs [1], considered as the simplest type of GANs, uses the Multi-Layer Perceptron (MLP) in both the generator and the discriminator. However, one of the main disadvantages of vanilla GANs is that the training process is not stable [12]. One of the possible solutions to this problem is to use a convolutional neural network (CNN) instead. In convolutional GANs [11], the generator uses a deconvolution structure, while the discriminator applies convolutional layers to classify generated images from the real images. While the type of networks in the generator and discriminator are different in vanilla GANs and convolutional GANs, the overall architecture in vanilla GANs and convolutional GANs are the identical (see Figure 2a). A significant number of recent GANs frameworks adopt CNN in their generators, discriminators, or in both. This is because the CNN in Convolutional GANs outperforms MLP in vanilla GANs in terms of the performance, quality of the resulted samples (usually images) and stability of the training process.


## BigGAN

The GANs frameworks discussed so far when applied in image domain produce low resolution images as output, i.e 64×64 or 128×128 images. In contrast, BigGAN [16] scales up the resolution by increasing the number of parameters and scaling up the batch size. Therefore, training BigGAN on ImageNet results in 256×256 and 512×512 images. The architecture of BigGAN is based on the self-attention GAN (SAGAN) [17]. The main advantage of SAGAN is that it focuses on different parts of the image by introducing an attention map that is applied to the feature maps in the deep convolutional model architecture.


## Wasserstein GAN

Another variation of GANs that has been used in video generation is called Wasserstein GAN (WGAN). In vanilla GANs [1], the discriminator attempts to classify real data points from fake ones. However, WGAN's discriminator [18] is a "critic", whose responsibility is to assign a score that represents the distance between the distribution of the observed real data and the distribution of observed fake samples. WGAN uses Wasserstein distance instead of the Jensen-Shannon (JS) divergence and Kullback-Leibler (KL) divergence that are used in other generative models, and there is substantial improvement in the quality of the generated images and in the training stability.


# RELATED WORK

As the study of GANs is accelerating rapidly, there are constantly new GANs frameworks that were not covered in the existing review papers. The timeline of survey papers found by searching Google Scholar for the keywords "overview of generative adversarial networks", "survey of generative adversarial networks", and "review of generative adversarial networks", as well as the papers cited in the retrieved papers, is illustrated in Figure 3. The timeline also includes the publication dates of the review papers, which will be discussed in this section, besides the publication dates of notable GANs frameworks discussed in section 2. While some of the review papers provide overviews of the state-of-the-art GANs, others focused on GANs for a specific domain (e.g. image generation). Reviews of GANs for general visual image datasets exceed other specialized GANs reviews, including those in cybersecurity, anomaly detection and medical imaging (see below). As shown in Figure 3, the red diamonds that represent general reviews of GANs dominate other categories. 


## General GANs

The majority of the survey papers discuss GANs in general [7][8][9][10][19][20][21][22][23]. One of the first attempts to review general GANs was in late 2017 [22]. Another general GANs survey was published in early 2018 that draws an analogy between GANs concepts and signal processing concepts to facilitate the understanding of GANs from a signal processing perspective [20]. Around the same time, Hitawala et al. [23] 


# Dec

Saxena et al.


# May

Jabbar al.


# Jun


## InfoGAN


# Nov

Wu et al.


# Dec

Sorin et al.


# Feb

Pan et al.

14 Mar


# Feb

Zamorski et al.


# May


## Convolutional GANs


# Nov


## BigGAN


# Sep


## CGAN


# Nov


## AC-GAN


# Aug

Hong et al.


## Wang & Gou et al.


# Sep

Creswell et al.


# Jan

Hitawala et al.


# Jan

Huang et al.


# Mar

Y.-J. Cao et al.


# Dec

Torres-Reyes et al.


# Jan

Wang & she et al.


# Jun

Agnese et al.


# Oct

Di Mattia et al.


# Jun

Yinka et al.


# May

Yi et al.


# Dec

Alqahtani et al.


# Dec


## GAN Variants

General Review


## Image Review

Other GANs Reviews improvements in the GANs frameworks. In 2019 and 2020, there was an increase in the number of papers that review GANs in general [7-10, 19, 21, 24-26]. It is worth mentioning that a more comprehensive and lengthy general review is available [19], and Cheng et al. [25] conducted comparative qualitative experiments on the mainstream GANs applied to the MNIST dataset [27], in which AC-GANs obtained the top classification accuracy. Saxena et al. [26] concentrate on models that address GANs disadvantages such as training instability and model collapse by modifying the architecture, the loss function or the optimization method.


## Image GANs

Since the original GANs frameworks were initially built upon images, there is no doubt that the number of GANs applications in the image domain surpasses other areas such as text, voice and video. Multiple reviews [28][29][30][31][32] focus on image synthesis, even though the major proportion of the general GANs reviews mentioned above, are also in the image domain. Huang et al. [29] categorize the image synthesis GANs frameworks into three types based on the overall architecture. These include direct architectures based on vanilla GANs, hierarchal models and iterative models consisting of multiple generators and discriminators. While each generator in an hierarchal architecture is tasked to deal with a different aspect of the disentangled representations of training images, the goal of a generator in the iterative models is to refine the quality of the generated images. Wu et al. [30] place image GANs models in two main categories, namely conditional image synthesis models and unconditional image synthesis models. In the unconditional models, different network modules handle texture, image super resolution, image inpainting, face synthesis and human synthesis. A comparative study [31] of image GANs frameworks was conducted using two datasets: MNIST [27], and Fashion-MNIST [33]. The paper reviews different applications of GANs such as style transfer, image inpainting, super-resolution and text to image. Similar applications are also reviewed elsewhere [28], with additional applications such as face ageing and 3D image synthesis. Moreover, Agnese et al. [32] direct attention to GANs models that are conditioned on text and produce images. Some of these models fall under semantic enhancement GANs, where the main goal is to ensure that text is semantically coherent with the generated image. Another category focusses on producing high resolution images conditioned on text. An additional category is for ensuring the diversity of synthesized images based on input text. The last type is text to video models that consider the temporal dimension of the training samples.


## Other GANs

GANs have been applied successfully in other areas including the medical field, with medical GANs frameworks reviewed [34,35]. Yi et al. [34] provide illustrations of juxtaposed GANs variation architectures. In addition, medical imaging GANs models are categorized based on the aim, such as quality improvement of the synthesized images, data augmentation, segmentation, classification, registration and object detection. Another review [35] specializes in radiology imaging based on 33 papers. GANs models facilitate synthesis of new radiology images, improving the quality of existing ones, converting radiology images from one type to another and localizing a specific object.

Anomaly detection, which deals with finding data samples that deviate from the normal, has also taken advantage of GANs models. A survey paper [36] reviews existing GANs models that contribute to identifying anomalies, and re-implements the reviewed models to further verify the effectiveness of such models.

The field of synthesizing and enhancing audio using GANs architectures has also been reviewed [37]. In the audio generation models, audio can be generated form a noise vector or text, while in audio enhancement GANs models, the model input is a noisy audio signal while the output is a refined signal. Audio GANs models are usually trained on audio spectrogram representations or raw audio waveforms.

Yinka-Banjo et al. [38] review studies that utilize GANs frameworks in cybersecurity systems. Such models are used for protection or attack purposes. In the case of protection, GANs models synthesize new poisoning samples, and the cybersecurity system learns how to protect itself against such samples that might cause an attack. GANs systems can also be used to spoof other GANs cybersecurity systems by creating adversarial samples that are similar to the real authorized training data points.

Besides general GANs reviews, there are also domain-specific reviews that focus on image, audio and medical imaging as listed above. In addition, several review papers are focussed on specific tasks such as anomaly detection and cybersecurity. However, no systematic review of video GANs has been performed, which have very specific characteristics due to their temporal dimension and the necessity to maintain temporal cohesiveness. The next section contributes to filling this gap and pays close attention to GANs models that generate videos and capture their temporal behavior.


# VIDEO GANS

Video GANs models vary depending on the condition settings. While at one end, there are video GANs frameworks that are not supplied with conditional signals, discussed in section 4.1, other models are conditioned on audios, texts, semantic maps, images and videos, as discussed in sections 4.2.1, 4.2.2, 4.2.3, and 4.2.4 respectively. In all these categories, the overall architecture can be divided roughly into three subcategories. First are the GANs frameworks that use RNN architectures to address the time-series nature of video data [39,40]. The second are progressive video GANs models [41,42] in which initial frames are first generated, and the generated data is fed into another generator to produce an enhanced result. The third are the two-stream architecture video GANs [6,43], where each stream considers a different aspect of the video. The following sections review the video GANs models based on the presence or absence of input condition and the variations within each category.


## Unconditional video generation

This section is a review of unsupervised GANs frameworks in the video domain, and a summary of these frameworks, the datasets and evaluation metrics used can be found in Table S4. So far, the output videos produced by these frameworks are short and have low-quality frames due to the lack of any information provided as a condition with the videos during the training phase. Although such models produce low-quality videos, the unconditional models have become the foundation for conditional frameworks. For instance, MoCoGAN [40] architecture, which is an unconditional model, is used in Text-Filter conditioning Generative Adversarial Network (TFGAN) [44] and storyGAN [39], both of which are conditional models.

Video-GAN (VGAN) [6] was the first attempt to generate videos using GANs. The generator consists of two convolutional networks: the first is a 3D spatio-temporal convolutional network that captures moving objects in the foreground, while the second is a 2D spatial convolutional model for the static background. The generated frames from the two-stream generator are combined, and then fed to the discriminator to distinguish real videos from the fake ones.

In VGAN [6], the foreground stream captures the foreground objects and their motions. However, the foreground layer in the generated result usually contains some flaws in temporal or spatial aspects. Flow and Texture Generative Adversarial Networks (FTGAN) [45] adds optical flow for representing the object motion more effectively. FTGAN follows a progressive architecture that starts with a GANs framework to captures the optical flow, followed by another GANs model to generate the texture that is conditioned on the result of the previous optical flow GANs, and produces the desired frames. Both texture and flow generators in FTGAN adopt VGAN structure by separating the foreground from the background, and setting the background to zero for the flow generator.

VGAN is based on disentangling the foreground from the background. Similarly, Motion Content GAN (MoCoGAN) [40], which is another type of unconditional video generator, separates the content from the movements to provide more control over these components. While VGAN [6] and FTGAN [45] map a video to a point in the latent vector, the MoCoGAN framework traverses N latent points, one per frame, where each vector can be decomposed into the motion vector and content vector. Therefore, it consists of an N-to-N RNN that accepts N random variables and produces N latent motion vectors. The motion vectors are combined with a fixed content vector for all N motion variables and fed to the generators to synthesize N images, each of which is a frame in the generated video. The generated images and videos are evaluated using two discriminators: one for the images, and the other for the generated video.

Similar to MoCoGAN [40], Temporal Generative Adversarial Nets (TGAN) [46] use N latent vectors for N frames. However, the main difference between MoCoGAN and TGAN is that a frame is generated for every latent vector in TGAN, which is different from the disentangled content and dynamic latent vectors in MoCoGAN [40]. Another difference is that MoCoGAN utilizes RNN structure for the generators while in TGAN, there are N 2D image deconvolutional generators to produce N frames. The resultant frames along with the videos in the training set are fed into the 3D convolutional discriminator. TGAN employs WGAN and fulfills K-Lipschitz constraint by proposing a parameter clipping method called singular value clipping using WGAN to provide stable training. Temporal GAN v2 (TGANv2) [47] focusses on a training technique based on understanding the relationship between the resolution of the generated images and the computational cost. The main cause of the increase in computational cost is the end part of the generator. This is because the spatial resolution / feature map is increasing when moving forward in the generator net. Since TGANv2 is an unsupervised model, it is essential to supply the model with a larger batch size in order to generalize properly. A subsampling layer is introduced to reduce the batch size by stochastically sampling videos within a mini-batch, and then sampling a frame within each chosen video. Applying the subsampling technique multiple times in the generator network facilitates reduction in the size of the mini-batch.

Dual video discriminator GAN (DVD-GAN) [48] expands BigGAN [16] capabilities in the video domain to produce 48 high quality images up to 256*256 based on complex datasets such as Kinetics human action dataset. DVD-GAN is trained on the entire dataset, Kinetics, and this is not the case in prior works [42,44] that use only a subset and pre-processed samples. Similar to MoCoGAN [40], there are two discriminators to deal with the temporal and spatial aspects of a video.


## 4.2


## Conditional video generation

There are several works that employ a conditional signal in GANs to direct the process and control modes of the generated data;The condition may be audio signal, text, semantic map, image or video. The following subsections review conditional GANs based on the condition type.


### Speech to video synthesis

This subsection discusses the GANs frameworks that are used to synchronize speech audio with facial movements, and Table S5 summarizes speech-to-video synthesis models.

Lips movement generation frameworks were the initial attempt at synchronizing a moving head with audio. Chen et al. [49] proposed a model that encodes the starting image and audio file. Then, the encoded features are combined and used as input to a decoder to generate videos. The synthesis videos are evaluated using a three-stream discriminator.

While Chen et al. [49] consider only lips, other works [50][51][52][53] study the synchronization between audio and the entire face. Jalalifar et al. [53] proposed a progressive framework that combined LSTM with CGAN. The purpose of LSTM is to extract the landmarks of the mouth region. Given the landmarks as a conditional setting, CGAN synthesizes a synchronized talking face for the audio signal. Vougioukas et al. [50] converts an audio waveform file to a synchronized spoken person, without an intermediate step for extracting the landmarks as in Jalalifar et al. [53]. In this model, given the initial frame, a temporal GANs model with two discriminators, one for the frame level and the other for the sequence level, are trained to produce synchronized videos for the audios.

The approach of disentangled representations has been considered [51,52]. The intuition behind the Disentangled Audio-Visual System (DAVS) [51] is to decouple the talking head information into person-related information and speech-related information in order to overcome the blurry and incoherent videos generated from a plain speech file. DAVS is able to generate videos of talking faces based on audio files or other video files, while other studies [50] [52] are conditioned only on the audio file. DAVS is able to retarget face movements from one video to another.

Mittal et al. [52] decoupled the audio into three aspects: content, emotion and noise, contrary to other works [51], [50], [49] that utilize the audio file without any per-possessing step. Decomposing audio representations using Variational AutoEncoder (VAE) at the first stage facilitates discarding of the background noise that appears in real world recorded datasets. Moreover, eliminating emotion from content reduces the effect of emotion on the generated videos. In the second stage, the content component of the disentangled audio with an image from the video is fed into the generator to produce a frame. There are two discriminators in this framework: a frame level discriminator and a video level discriminator.


### Text to video synthesis

This subsection considers GANs-based frameworks that aim to produce videos according to a conditional text. These frameworks have two main purposes. The first is to maintain semantic consistency between the condition and the generated video. The second purpose is to generate realistic quality videos that preserve the coherence and consistency within the frames. The main text to video GANs frameworks are summarized in Table S6.

Temporal GANs conditioning on captions (TGANs-C) framework [54] first encodes the text using an LSTM based encoder. The output of the sentence encoder is concatenated with a noise vector and then given to the generator, which is a 3D deconvolution network. The model has three discriminators for the video level, frame level and the motion level, to ensure that adjacent frames have coherent motion.

While videos in TGANs-C [54] are generated using a single generator, the GANs framework proposed elsewhere [42] generates videos progressively using multiple generators in several stages. Firstly, the conditional variational autoencoder that is conditioned on encoded text produces the initial image. This initial image provides an overall representation, which may be the background image, the colours of the image and its structure. The initial image with the encoded text is an input to a CGAN to generate higher quality images.

The original method used in CGAN [13] to incorporate a condition is to concatenate the condition and the noise vector; this method is also applied in text to video GANs frameworks [42,54]. However, TFGAN [44] introduces a multi-scale text-conditioning method, where the text features are extracted from the encoded text to generate convolution filters. Then, the convolution filters are input to discriminator network to facilitate strengthening of the associations between the texts and the videos.

In video generation models based on text reviewed so far [42,44,54], a model synthesizes a video according to one conditional sentence per video. In contrast, storyGAN [39] is a story visualization model that is conditioned on multiple sentences. StoryGAN [39] contains a context encoder and a story encoder. The story encoder encodes the entire story as a low dimensional vector that serves as an initial input to the context encoder. At each time step in the RNN context encoder, one sentence with concatenated noise is introduced along with encoded story vector to produce a Gist vector, which is combined information about a specific sentence and the story. The generator then accepts a Gist vector and produces an image. There is a discriminator for the image and another discriminator for the story. Different to other video generation frameworks, storyGAN pays less attention to the continuity of motion and instead focusses on the global consistency of the story.


### Semantic map to video synthesis

This section represents video GANs frameworks that are conditioned on semantic maps. This section could potentially fall under section 4.2.5, video to video synthesis, since the frameworks are conditioned on videos. However, the conditioning videos are pre-processed into semantic maps first, and Table S7 provides a summary of the cited semantic map to video frameworks.

Video to video (vid2vid) [55] is a conditional GANs framework that converts semantic videos into frames.

Semantic videos consist of semantic maps, where each map is a collection of segmented objects that are labeled with different colours. The semantic videos could be in the form of segmentation masks or boundaries. Few-shot-vid2vid [56] is an extension to vid2vid. Both vid2vid and few-shot-vid2vid share an overall architecture for the generator network that is conditioned on the previous frame, a previous semantic image, and the source semantic videos. The generator consists of three modules, namely W to extract the optical flow, M to predict the occlusion map, and H to generate the intermediate frames. The main difference between vid2vid and fewshot-vid2vid is that the module H in vid2vid has fixed weights, whereas the weights are dynamic in few-shot-vid2vid. An adaptive network with dynamic weights [56] facilitates generation of videos of unseen objects in the training dataset by providing multiple images of the object at test time. There are two discriminators in both architectures: a video discriminator and an image discriminator. The modules are trained in a progressive manner, which means that the number of frames and the quality of generated images increase gradually. While the conditional signal in vid2vid and few-shot-vide2vid [55,56] is a sequence of semantic maps, Pan et al. [57] only use one semantic label map. They claim that providing a single semantic map helps loosen the restrictions during the synthesizing process. To generate a video conditioned on a single semantic map, there are two phases. The first is an image-to-image phase that is conditioned on the semantic map to generate the initial frame with fine details. The second phase produces a video given the starting frame using conditional VAE.


### Image to video synthesis

The main purpose of image-to-video GANs frameworks is to predict future frames based on a given frame, and Table S8 lists the reviewed image to video frameworks and some of their properties.

Early versions of image to video architectures [58][59][60][61] do not disentangle the representations of the training videos, resulting in blurriness in the synthesized videos. Mathieu et al. [58] made the first attempt to employ adversarial training in the video prediction domain. The generator is a multiscale network that is focussed on synthesizing coherent frames conditioned on front frames. The adversarial network solves the issue related to blurry frames that results from standard mean squared error loss function. Lee et al. [59] incorporate VAEs with GANs for a video prediction system. Combining VAEs with GANs was first performed in the image domain [4,62]. The reason for using both GANs and VAEs networks is that a GANs model helps produce more realistic images, while VAE facilitates diversity in the generated images. Multi-Discriminator GAN (MD-GAN) [60] employs two consecutive GANs. While the first generates the content of the frames, the other GANs model refines the output of the first stage. Similar to MD-GAN [60], the model by Cai et al. [61] adopts a two-stage framework based on pose estimation. The first network generates human pose sequences of the conditioned type of pose, whereas the second stage network maps from the pose space to pixel space, given a reference image.

Several studies [43,[63][64][65][66][67] on frame prediction have shown that decomposing the information in videos enhances the quality of the synthesized videos. Motion Content Network (MCnet) [63] encodes motion and content using separate encoders in an unsupervised manner. The outputs of the previous stages are combined and decoded to produce the next frame. Walker et al. [64] also tackle content and motion in a progressive manner by utilizing two architectures, namely VAEs and GANs, as do Lee et al. [59]. The VAEs framework predicts future human poses and motions, while the GANs framework is conditioned on the motions to predict future frames as a pixel-level representation. Similar to Villegas et al. [63] and Walker et al. [64], both Sun et al. [43] and Liang et al. [65] attempt to separate the dynamics from the content by employing dual GANs. One of the GANs is dedicated to generating the content of a frame, while the other GANs model predicts the dynamics. The difference between the two architectures is that Liang et al. [65] use one encoder to encode the front frames, while TwoStreamVAN [43] encodes an initial frame with the content encoder and the motion map is encoded using the motion encoder. Using same approach as TwoStreamVAN, DRNET [66] has two encoder networks: one for the dynamic content and the other for time-independent content. The encoded representations are concatenated and fed into the decoder to predict the next frames. When decomposing motion from content to forecast a future frame, the motion component is detected from the input frames [43,[63][64][65][66]. However, Hu et al. [67] utilize a motion stroke, which is a continuous line that represents object motion, instead of extracting motion from the input frames. The model first encodes the initial image and the motion strokes. This is followed by an additional encoder to encode the encoded initial image, encoded motion strokes and features of the previous frame. Next, the generator is used to generate the sequence frames with two discriminators, one each for image level and sequence level.


### Video to video synthesis

One of the major applications of video to video synthesis is object animation, where motion is retargetted from one object to another. Some GANs frameworks in this domain are limited to a specific dataset, while others can be applied in different domains, and Table S9 lists video animation frameworks.

Many works [68][69][70] share the main objective of converting a person's dance movements to those of another immature dancer. Zhou et al. [69] build their architecture in a progressive manner, where the first phase synthesizes frames for the immature dancer based on the pose of the mature dancer and body parts of the immature dancer. The second network provides more realism to the final output by fusing the target performer with a background and adding necessary shadows to combine the foreground with the background. Chan et al. [68] start with a pose detector network, whose result is inserted as an input to a GANs framework. Then, the GANs face framework is used to enhance realism. Yang [69] , as the latter's dataset is collected from YouTube and it is not feasible to collect different poses of the same person.

Studies by Siarohin et al. [72] and Liu et al. [73] aim to generate a video that has similar action as an input video and containing an object similar to ones in an input image; in these models, the action is not restricted to a dance movement. When comparing the methods for pose extraction, it is important to mention that models by Zhou et al. Chan et al. [68,69] are based on pre-trained networks for extracting subject movements. In contrast, MOviNg KEYpoints (Monkey-net) [72] is based on a self-supervised framework while Liu et al. [73] used 3D reconstruction software to rebuild the desired poses. In addition, other generation processes [56,69,72,73] are conditioned on a specific human target, whereas Chan et al. [68] use a random human subject from the dataset in generated videos. Monkey-net is composed of three stages: the first stage extracts key points of two random images of the input video; the second stage is a motion prediction network that computes the optical flow of the result of the previous stage; and the last stage performs image synthesis using a Variational AutoEncoder. Liu et al. [73] start with extracting a 3D object from the static images and motion of the given video, and end with a CGAN framework to produce realistic frames.

vid2Game [74] follows a different approach to control the human subject in generated frames. The synthesized images are conditioned on a low dimensional signal such as joystick movements. There are two networks: the first is Pose2pose, which generates a new pose given the current one and the control signal using an autoencoder. The control signal is input to the residual blocks in the middle of the autoencoder to facilitate smooth motion. The second network is the Pose2frame network, which generates a mask and an image. The image is combined with the mask to produce a frame. In other work [55,56,68,69], the generator network is conditioned on the pose extracted from real images of the training dataset, while in vid2Game [74], the network is conditioned on synthesized poses. Clearly, working with poses extracted from synthesized frames requires more effort compared to working with poses from the training images, as artefacts in the generated poses need to be dealt with.

While some studies [64,65,68,95] retarget the motion of an individual body to another individual, others focus only on the head [71,75]. Deep video portraits [71] outputs transferred motion video that can be modified by changing adjustable parameters such as head pose and facial expression. Deep video portraits translate both source and target videos to low dimensional parameter vectors that are adjusted to input to a rendering video. ReenactGAN [75] extracts the face boundary as an initial step, then the source boundary is aligned with the target boundary to produce target video animation. Wu et al. [75] claim that ReenactGAN is better at representing minor changes in the face between frames than other frameworks [71].

Recycle-GAN [76] is another video retargetting application that takes a different approach. While other frameworks transfer motion from one object to another [64,65,68,95] [71,75] , Recycle-GAN converts a sequence of frames from one domain to another while maintaining the style of the second domain. A general object retargetting model is trained on only the source domain, and the target domain video is provided at testing time. In contrast, Recycle-GAN contains two GANs frameworks trained on two datasets from the source and target domains. Unlike previous video retargetting models [68,69,72,74], Recycle-GAN is conditioned on videos from one domain, and not on semantic maps or detected poses. Recycle-GAN is an extension of Cycle-GAN [77], which is an unsupervised method used in the image domain to translate unpaired images form one domain to another by applying the cycle consistency loss. Additionally, Recycle-GAN incorporates spatial and temporal constraints within a GANs framework.


# CONCLUSION

Generative models such as GANs provide promising results in multiple domains including images, videos, audios and texts. Video synthesis is still in the early stages compared to other domains such as images. The current state of the art for video GANs suffers from low quality frames or low number of frames or both. One reason could be the higher requirement for computational power as videos are high dimensional data, and so necessitate networks with a large number of parameters. To handle such data, there is a need for a complex architecture that takes into consideration spatial as well as temporal data. For example, DVD-GAN uses one TPU and TGANv2 utilizes 8 GPUs. In addition, videos are usually multimodal and may include audio stream as well, which makes the processing even more complex. Collecting domain-specific videos is also more timeconsuming and expensive comparing to other domains such as images, as automatic video retrieval algorithms are not yet very accurate and video data collection involves a lot of manual work to select, clean and preprocess the data. Nevertheless, the trend is upward and every year more studies are being done in this area. The applications of video GANs are broad and include speech animation, video prediction, video retargetting, generating stories from caption and video completion. Although the progress on GANs in areas other than videos is well documented through several review papers, video GANs models have received less attention so far, and if at all included, they were only a section in other review papers despite their broad range. Considering the increasing number of studies on video GANs during the past few years, it is the right time to survey the field, categorise different models according to their applications and compare their differences. This paper is among the initial attempts to review GANs models that produce videos and highlight their main differences.

While 3D CNN GANs as proposed by Vondrick et al. [6] appear to be an intuitive choice to synthesise videos and represent frames along with time dimension, 3D convolutions may cause overfitting [78]. An alternative to 3D CNN is to utilize RNN with 2D convolution, as in MoCoGAN [40]. Using 2D convolution and 1D convolution to disentangle content from the motion dimension is another way to represent videos [46].

Videos can be generated using GANs either without conditional settings or by introducing a conditional signal such as an audio, image, video, text, label or semantic map. This survey paper initially groups the video GANs frameworks according to their conditional setting: unconditional video generation vs conditional video generation, and discusses the most important models proposed so far in each category and outlines the differences. Moreover, it goes deeper into the conditional frameworks and categorizes the methods according to their condition and presents and compare the different models in each of those categories. In addition, a list of all datasets used in the reviewed frameworks, their characteristics, evaluation metrics and the loss function applied in each work are presented in supplementary material. The hope is that this paper will serve as a good review of the domain so far and provide the reader with a better understanding of different frameworks and their potential applications.


## A SUPPLEMENTARY MATERIAL

This supplementary material provides summary tables for the reviewed frameworks in the paper. To better organize the information in different tables and include as much information as possible for each framework, codes linked to Tables S1, S2 and S3 are used. In Table S1, all datasets used in the reviewed papers are listed and coded with prefix "D" and a number (e.g. D1 refers to Clever-sv dataset and so on). It also presents the number of videos in the dataset, duration (if available), resolution and purpose. In Table S2, all metrics used in the reviewed frameworks are summarised and codes with prefix "E" and a number (e.g. E1 refers to human evaluation). Likewise, in Table S3 lists the loss functions used and codes them with prefix L and a number (e.g. L4 refers to GDL loss). In Table S4, the unconditional GANs frameworks reviewed in section 4.1 are listed, and includes the dataset(s), the loss function(s) and the evaluation metric(s) used in each work. In Table S5, speech to video GANs frameworks discussed in section 4.2.1 are summarised. In Table S6, text to video frameworks of section 4.2.2 are listed, and Table S7 is the list of semantic maps to video frameworks in section 4.2.3. In Table S8, all image to video frameworks in section 4.2.4 are listed. Finally, Table S9 is a summary of video to video GANs models of section 4.2.5. All tables contain information such as dataset, type of condition, loss functions and evaluation metrics.     Chen et al. [49] audio, initial image L3, L6, L7, L10 D5, D23, D27 E7-9


## A.1 Unconditional GANs


## A.2 Conditional GANs

Jalalifar et al. [53] audio, set of landmarks L3 D8 -    

## Figure 3 :
3Timeline of the review papers in section 3, along with recent GANs advances in section 2 (light blue)


et al.'s architecture [70] is similar to Kim et al.'s [71], where the model starts with disentangling the video representations of the source and target videos into distinct parameters that can be combined to produce retargeted video. Often ,the datasets [68] [70] are limited, with a small number of participants performing a wide range of movements. Thus, Chan et al's [68] computational module translates the poses more easily than Zhou et al.'s

## Table S1 :
S1Datasets used in video GANs frameworks reviewed in this paper. They are coded with prefix D and number used to link to the other tables. NA means the information for a particular cell is not availableAbbreviation 
Dataset 
Purpose 
Number of 
videos 

Duration 
Scaled 
Resolution 
D1 
Clevr-sv[39] 
layouts of 
objects 

13000 
NA 
NA 

D2 
UCF-101[79] 
human actions 
13320 
NA 
64×64 in [40][44] 
192×192 in [47] 
D3 
Penn action dataset [80] 
human actions 
2326 
32 frames 
64×64 
D4 
Kinetics human action 
[81] 

human actions 
500000 
NA 
NA 

D5 
GRID[82] 
speakers 
uttering short 
phrases 

48614 
NA 
NA 

D6 
TCD TIMIT[83] 
speakers 
uttering short 
phrases 

9881 
NA 
NA 

D7 
Pororo dataset[84] 
cartoon 
16000 
1 second 
NA 
D8 
President Obama's 
weekly address videos 

Obama uttering 
speeches 

-
NA 
NA 

D9 
BAIR action free[85] 
robotic arm 
pushing a variety 
of objects 

41216 
NA 
64×64 

D10 
FaceForensics[86] 
human faces 
854 
NA 
256×256 
D11 
Shape motion[40] 
two shapes 
moving 

4000 
16 frames 
64×64 

D12 
Mug facial 
expression[87] 

facial 
expressions 

3528 
50-160 frames 
96×96 

D13 
Dataset in literature [73] 
moving people 
-
12k frames 
NA 
D14 
Tai chi[40] 
human actions 
4500 
32-100 frames 
64×64 
D15 
Unlabeled video 
dataset[6] 

golf course, 
babies in 
hospital, 
beaches, and 
train stations 

2000000 
32 frames 
64×64 

Abbreviation 

Dataset 
Purpose 
Number of 
videos 

Duration 
Scaled 
Resolution 
D16 
Humman3.6M[88] 
3D humans in 
varies poses 

NA 
NA 
NA 

D17 
Moving MNIST[89] 
moving numbers 
10000 
16-20 frames 
64×64 
D18 
Golf scene [6] 
golf scene 
20268 
NA 
64×64 
D19 
Sport1M[90] 
human actions 
NA 
NA 
32×32 
D20 
THUMOS-15[91] 
human actions 
NA 
NA 
NA 
D21 
NORB: objects[92] 
3D objects 
NA 
NA 
NA 
D22 
SUNCG: chairs [93] 
3D chair models 
NA 
NA 
NA 
D23 
Linguistic Data 
Consortium (LDC) [94] 

speakers 
uttering short 
phrases 

NA 
NA 
NA 

D24 
Syn-action dataset [43] 
synthetic action 
6000 
NA 
NA 
D25 
Weizmann human 
action[95] 

human actions 
90 
NA 
NA 

D26 
SURREAL[96] 
Computer 
Graphics 
(CG)human 
actions 

67582 
32 frames 
64×64 

D27 
LRW [97] 
word-level lip 
reading 

850 
1 second 
256×256 

D28 
CRowd-sourced 
Emotional Multimodal 
Actors Dataset (CREMA-
D) [98] 

speakers 
uttering short 
phrases with 
motion 

7442 
NA 
NA 

D29 
Lip Reading Sentence 3 
(LRS3) Dataset[99] 

speakers from 
TED talks 

NA 
NA 
NA 

D30 
Dataset in literature [42] 
human actions 
4000 
NA 
NA 
D31 
KITTI [100] 
street scenes 
NA 
NA 
NA 
D32 
Caltech Pedestrian [101] 
street scenes 
NA 
NA 
NA 
D33 
Dataset(training) in 
literature [68] 

dance 
4 
8-17 minutes 
1920×1080 
1280×720 
D34 
Dataset in literature [74] 
tennis-walking -
fencing 

NA 
NA 
NA 

D35 
UvA-Nemo[102] 
facial dynamics 
analysis 

1240 
32 frames 
64×64 

D36 
Viper dataset [103] 
realistic 
computer games 
scenes 

77 
NA 
NA 

D37 
Dataset(training) in 
literature [69] 

dance 
8 
4-12 minutes 
NA 

D38 
YouTube dancing videos 
[55] 

dance 
1500 
NA 
NA 

D39 
Street-scene videos [56] 
street scenes 
NA 
NA 
NA 
D40 
Cityscapes [104] 
street scenes 
5000 
30 frames 
NA 
D41 
Apolloscape [105] 
street scenes 
73 
100-1000 
frames 

NA 

D42 
SBMG[106] 
one digit 
bouncing 
handwritten 

12000 
16 frames 
64×64 

D43 
TBMG[106] 
two digit 
bouncing 
handwritten 

NA 
NA 
NA 

D44 
MSVD [107] 
YouTube with 
manual 
annotated 
sentences 

1970 
NA 
NA 

D45 
Shapes-v1 [44] 
moving shapes 
NA 
NA 
NA 
D46 
Shapes-v2 [44] 
moving shapes 
NA 
NA 
NA 

Abbreviation 

Dataset 
Purpose 
Number of 
videos 

Duration 
Scaled 
Resolution 
D47 
Sky scene[60] 
time-lapse sky 
videos 

38207 
32 frames 
128×128 

D48 
Datasets in literature[71] 
Varies datasets 
for public figures 
giving speeches 
as Elizabeth II, 
Putin, and 
Obama 

NA 
NA 
NA 

D49 
CelebV [75] 
public figures 
giving speeches 

200000 
30 minutes 
NA 

D50 
Mixamo 
dance 
NA 
NA 
NA 
D51 
Solo-Dancer[70] 
dance 
NA 
NA 
NA 



## Table S2 :
S2All the evaluation measures used in video GANs frameworks reviewed in this paper. Each evaluation metric is coded with prefix "E" and a number used to link to the other tables.Abbreviation 
Measures 

E1 
Human evaluation 

E2 
Average Content Distance (ACD) 

E3 
Inception score 

E4 
Motion control score 

E5 
Fre ćhet Inception Distance (FID) 

E6 
Generative Adversarial Metric (GAM) 

E7 
Peak Signal-to-Noise Ratio (PSNR) 

E8 
Structural Similarity (SSIM) index 

E9 
Cumulative Probability Blur Detection (CPBD) measure 

E10 
Frequency domain Blurriness Measure (FDBM) 

E11 
Word Error Rate (WER) 

E12 
Segmentation accuracy 

E13 
Pose error 

E14 
Landmark Distance (LMD) 

E15 
Classification accuracy 

E16 
Sharpness measure 

E17 
Inter-Entropy 

E18 
Intra-Entropy 

E19 
Learned Perceptual Image Patch Similarity (LPIPS) 

E20 
Average VGG cosine similarity 

E21 
Mean Square Error (MSE) 

E22 
Maximum Mean Discrepancy (MMD) 

E23 
Average key point distance 

Abbreviation 

Measures 

E24 
Missing key point rate 

E25 
Intersection over Union (IoU) 

E26 
Mean Pixel accuracy (MP) 

E27 
Facial action consistency 

E28 
Mean Absolute Error (MAE) 



## Table S3 :
S3Loss functions used in video GANs frameworks in the reviewed studies. Each loss function is coded with prefix L and a number used to link to the other tables.Abbreviation Losses 

L1 
L1 

L2 
L2 

L3 
Adversarial loss 

L4 
GDL loss 

L5 
KL loss 

L6 
Perceptual loss 

L7 
Pixel-like loss 

L8 
VAE loss 

L9 
Negative log-likelihood function 

L10 
Cosine similarity loss 

L11 
Feature-matching loss presented in pix2pixHD 

L12 
Mask loss 

L13 
Recycle-loss 

L14 
Recurrent loss 

L15 
Semantic layout and pose feature losses 

L16 
Margin ranking losses 

L17 
Flow loss 

L18 
Forward-backward consistency loss 

L19 
Reconstruction loss 

L20 
Cycle loss 

L21 
Triplet Loss 

L22 
Ranking Loss 



## Table S4 :
S4The reviewed unconditional video GANs frameworks in section 4.1. The second column provides the type of condition. The third, fourth and fifth columns give information on loss functions, training datasets and evaluation measures respectively. More infoPublication 
Conditional information 
Loss 
Dataset 
Measures 

VGAN[6] 
no condition 
L3 
D15 
E1, E15 

FTGAN[45] 
no condition 
L3 
D3, D26 
E1, E15 

MoCoGAN[40] 
no condition 
L3 
D2, D11, D12, 
D14, D25 

E1, E2, E3, E4 

TGAN [46] 
no condition 
L3 (WGAN) 
D2, D17, D18 
E1, E3, E6 

TGANv2 [47] 
no condition 
L3 
D2, D10 
E3, E5 

DVD-GAN [48] 
no condition 
L3 
D2, D4 
E3, E5 



## Table S5 :
S5The reviewed speech to video GANs frameworks in section 4.2.1. The second column provides the type of condition. The third, fourth and fifth columns give information on loss functions, training datasets and evaluation measures respectively. More inform L1, L2, L3, L5, L6, L9, L16 D5, D28, D29 E7, E8, E14Publication 
Conditional information 
Loss 
Dataset 
Measure 

Vougioukas et al.[50] 
audio, initial image 
L1, L3 
D5, D6 
E1, E2, E7 -11 
DAVS [51] 
(audio or video), initial image 
L1, L3 
D27 
E1, E7, E8 

Mittal et al.[52] 
audio-based 
content 
representations, initial image 



## Table S6 :
S6The reviewed text to video GANs frameworks in section 4.2.2. The second column provides the type of condition. The third, fourth and fifth columns give information on loss functions, training datasets and evaluation measures respectively. More information on the last three columns can be found in Tables S1-S3.Publication 
Conditional information 
Loss 
Dataset 
Measure 

TGANs-C[54] 
one sentence 
L2, L3 
D42, D43, D44 
E1, E6 

Li et al.[42] 
one sentence 
L1, L3, L8 
D30 
E15 

TFGAN [44] 
one sentence 
L3 
D4, D45, D46 
E5, E15 

StoryGAN [39] 
Multiple sentences 
L3, L5 
D1, D7 
E1, E8, E15 



## Table S7 :
S7The reviewed semantic map to video GANs frameworks in section 4.2.3. The second column provides the type of condition. The third, fourth and fifth columns give information on loss functions, training datasets and evaluation measures respectively. More information on the last three columns can be found in Tables S1-S3.Publication 
Conditional information 
Loss 
Dataset 
Measure 

Vid2vid [55] 
semantic video 
L3, L17 
D10, D38, D40, D41 
E1, E5 

Few-shot-vid2vid [56] 
Semantic video, initial 
image 

L3, L17 
D10, D38, D39 
E1, E5, 
E12, E13 
Pan et al.[57] 
semantic video, initial 
image 

L1, L5, L6, L18, 
L19 

D2, D4, D31, D40 
E1, E5 



## Table S8 :
S8The reviewed image to video GANs frameworks in section 4.2.4. The second column provides the type of condition. The third, fourth and fifth columns give information on loss functions, training datasets and evaluation measures respectively. More information on the last three columns can be found in Tables S1-S3.Publication 
Conditional information 
Loss 
Dataset 
Measures 

Mathieu et al.[58] 
input frames 
L2, L3, L4 
D2, D19 
E7, E8, E16 

Lee et al.[59] 
input frame 
L3, L8 
D4, D9 
E1, E7, E8, 
E20 
MCnet [63] 
input frames 
L2, L3, L4 
D2, D4, D25 
E7, E8 

Walker et al.[64] 
input frames 
L3, L8 
D2 
E3, E22 

TwoStreamVAN[43] 
input frame, category, 
motion map 

L3, L8 
D12, D24, D25 
E3, E15, E17, 
E18 
Liang et al.[65] 
input frames 
L3, L8 
D2, D20, D31, D32 
E7, E8, E21 

DRNET [66] 
input frames 
L2, L3 
D4, D17, D21, D22 
E15, E3, 

Hu et al. [67] 
input frame, stroke 
L2, L3, L6 
D4, D16, D17 
E5, E19 

MD-GAN [60] 
Input frame 
L1, L3, L22 
D47 
E7, E8, E21 

Cai et al.[61] 
Input image, label 
L1, L2, L3 
D2, D16 
E3 



## Table S9 :
S9The reviewed video to video GANs frameworks in section 4.2.5. The second column provides the type of condition. The third, fourth and fifth columns give information about loss functions, training datasets and evaluation measures respectively. More information on the last three columns can be found in Tables S1-S3.Publication 
Conditional information 
Loss 
Dataset 
Measure 

Zhou et al.[69] 
video, image 
L3, L6, L11, L15 
D37 
E1, E7, E8, E21 

Chan et al. [68] 
video 
L3, L6, L11 
D33 
E1, E8, E19 

Vid2Game[74] 
video, control signal 
L3, L6, L11, l12 
D34 
E8, E19 

Monkey-net [72] 
video, image 
L3, L11 
D9, D14, D35, 
E1, E5, E21, E23, 
E24 
Recycle-GAN [76] 
video 
L3, L13, L14 
D36 
E1, E15, E25, E26 

Liu et al. [73] 
video, image 
L1, L3 
D13 
E1, E8 

Deep Video Portraits 
[71] 

video 
L1, L3 
D48 
E1, E28 

ReenactGAN [75] 
video 
L1, L2, L3, L20 
D49 
E1, E27 

TransMoMo[70] 
video 
L1, L3, L19, L21 
D50, D51 
E1, E3, E21, E28 


Generative adversarial nets. I Goodfellow, Advances in neural informationzprocessing systems. I. Goodfellow et al., "Generative adversarial nets," in Advances in neural informationzprocessing systems, 2014, pp. 2672-2680.

Auto-encoding variational bayes. P K Diederik, M Welling, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)P. K. Diederik and M. Welling, "Auto-encoding variational bayes," in Proceedings of the International Conference on Learning Representations (ICLR), 2014.

Deep Generative Models for Image Generation: A Practical Comparison Between Variational Autoencoders and Generative Adversarial Networks. M El-Kaddoury, A Mahmoudi, M M Himmi, International Conference on Mobile, Secure, and Programmable Networking. SpringerM. El-Kaddoury, A. Mahmoudi, and M. M. Himmi, "Deep Generative Models for Image Generation: A Practical Comparison Between Variational Autoencoders and Generative Adversarial Networks," in International Conference on Mobile, Secure, and Programmable Networking, 2019: Springer, pp. 1-8.

Autoencoding beyond pixels using a learned similarity metric. A B L Larsen, S K Sønderby, H Larochelle, O Winther, arXiv:1512.09300arXiv preprintA. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther, "Autoencoding beyond pixels using a learned similarity metric," arXiv preprint arXiv:1512.09300, 2015.

Progressive growing of gans for improved quality, stability, and variation. T Karras, T Aila, S Laine, J Lehtinen, arXiv:1710.10196arXiv preprintT. Karras, T. Aila, S. Laine, and J. Lehtinen, "Progressive growing of gans for improved quality, stability, and variation," arXiv preprint arXiv:1710.10196, 2017.

Generating videos with scene dynamics. C Vondrick, H Pirsiavash, A Torralba, Advances In Neural Information Processing Systems. C. Vondrick, H. Pirsiavash, and A. Torralba, "Generating videos with scene dynamics," in Advances In Neural Information Processing Systems, 2016, pp. 613-621.

How Generative Adversarial Networks and Their Variants Work: An Overview. Y Hong, U Hwang, J Yoo, S Yoon, ACM Computing Surveys (CSUR). 52110Y. Hong, U. Hwang, J. Yoo, and S. Yoon, "How Generative Adversarial Networks and Their Variants Work: An Overview," ACM Computing Surveys (CSUR), vol. 52, no. 1, p. 10, 2019.

A Jabbar, X Li, B Omar, arXiv:2006.05132A Survey on Generative Adversarial Networks: Variants, Applications, and Training. arXiv preprintA. Jabbar, X. Li, and B. Omar, "A Survey on Generative Adversarial Networks: Variants, Applications, and Training," arXiv preprint arXiv:2006.05132, 2020.

Applications of Generative Adversarial Networks (GANs): An Updated Review. H Alqahtani, M Kavakli-Thorne, G Kumar, Archives of Computational Methods in Engineering. H. Alqahtani, M. Kavakli-Thorne, and G. Kumar, "Applications of Generative Adversarial Networks (GANs): An Updated Review," Archives of Computational Methods in Engineering, pp. 1-28.

J Gui, Z Sun, Y Wen, D Tao, J Ye, arXiv:2001.06937A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications. arXiv preprintJ. Gui, Z. Sun, Y. Wen, D. Tao, and J. Ye, "A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications," arXiv preprint arXiv:2001.06937, 2020.

Unsupervised representation learning with deep convolutional generative adversarial networks. A Radford, L Metz, S Chintala, arXiv:1511.06434arXiv preprintA. Radford, L. Metz, and S. Chintala, "Unsupervised representation learning with deep convolutional generative adversarial networks," arXiv preprint arXiv:1511.06434, 2015.

I Goodfellow, arXiv:1701.00160NIPS 2016 tutorial: Generative adversarial networks. arXiv preprintI. Goodfellow, "NIPS 2016 tutorial: Generative adversarial networks," arXiv preprint arXiv:1701.00160, 2016.

Conditional generative adversarial nets. M Mirza, S Osindero, arXiv:1411.1784arXiv preprintM. Mirza and S. Osindero, "Conditional generative adversarial nets," arXiv preprint arXiv:1411.1784, 2014.

Infogan: Interpretable representation learning by information maximizing generative adversarial nets. X Chen, Y Duan, R Houthooft, J Schulman, I Sutskever, P Abbeel, Advances in neural information processing systems. X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel, "Infogan: Interpretable representation learning by information maximizing generative adversarial nets," in Advances in neural information processing systems, 2016, pp. 2172-2180.

Conditional image synthesis with auxiliary classifier gans. A Odena, C Olah, J Shlens, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningJMLR. org70A. Odena, C. Olah, and J. Shlens, "Conditional image synthesis with auxiliary classifier gans," in Proceedings of the 34th International Conference on Machine Learning-Volume 70, 2017: JMLR. org, pp. 2642-2651.

Large scale gan training for high fidelity natural image synthesis. A Brock, J Donahue, K Simonyan, arXiv:1809.11096arXiv preprintA. Brock, J. Donahue, and K. Simonyan, "Large scale gan training for high fidelity natural image synthesis," arXiv preprint arXiv:1809.11096, 2018.

Self-attention generative adversarial networks. H Zhang, I Goodfellow, D Metaxas, A Odena, International Conference on Machine Learning. H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, "Self-attention generative adversarial networks," in International Conference on Machine Learning, 2019, pp. 7354-7363.

Wasserstein gan. M Arjovsky, S Chintala, L Bottou, arXiv:1701.07875arXiv preprintM. Arjovsky, S. Chintala, and L. Bottou, "Wasserstein gan," arXiv preprint arXiv:1701.07875, 2017.

Recent progress on generative adversarial networks (GANs): A survey. Z Pan, W Yu, X Yi, A Khan, F Yuan, Y Zheng, IEEE Access. 7Z. Pan, W. Yu, X. Yi, A. Khan, F. Yuan, and Y. Zheng, "Recent progress on generative adversarial networks (GANs): A survey," IEEE Access, vol. 7, pp. 36322-36333, 2019.

Generative adversarial networks: An overview. A Creswell, T White, V Dumoulin, K Arulkumaran, B Sengupta, A A Bharath, IEEE Signal Processing Magazine. 351A. Creswell, T. White, V. Dumoulin, K. Arulkumaran, B. Sengupta, and A. A. Bharath, "Generative adversarial networks: An overview," IEEE Signal Processing Magazine, vol. 35, no. 1, pp. 53-65, 2018.

Generative Adversarial Networks: recent developments. M Zamorski, A Zdobylak, M Zięba, J Świątek, International Conference on Artificial Intelligence and Soft Computing. SpringerM. Zamorski, A. Zdobylak, M. Zięba, and J. Świątek, "Generative Adversarial Networks: recent developments," in International Conference on Artificial Intelligence and Soft Computing, 2019: Springer, pp. 248-258.

Generative adversarial networks: introduction and outlook. K Wang, C Gou, Y Duan, Y Lin, X Zheng, F.-Y. Wang, IEEE/CAA Journal of Automatica Sinica. 44K. Wang, C. Gou, Y. Duan, Y. Lin, X. Zheng, and F.-Y. Wang, "Generative adversarial networks: introduction and outlook," IEEE/CAA Journal of Automatica Sinica, vol. 4, no. 4, pp. 588-598, 2017.

Comparative study on generative adversarial networks. S Hitawala, arXiv:1801.04271arXiv preprintS. Hitawala, "Comparative study on generative adversarial networks," arXiv preprint arXiv:1801.04271, 2018.

Z Wang, Q She, T E Ward, arXiv:1906.01529Generative Adversarial Networks: A Survey and Taxonomy. arXiv preprintZ. Wang, Q. She, and T. E. Ward, "Generative Adversarial Networks: A Survey and Taxonomy," arXiv preprint arXiv:1906.01529, 2019.

An analysis of generative adversarial networks and variants for image synthesis on MNIST dataset. K Cheng, R Tahir, L K Eric, M Li, Multimedia Tools and Applications. K. Cheng, R. Tahir, L. K. Eric, and M. Li, "An analysis of generative adversarial networks and variants for image synthesis on MNIST dataset," Multimedia Tools and Applications, pp. 1-28.

D Saxena, J Cao, arXiv:2005.00065Generative Adversarial Networks (GANs): Challenges, Solutions, and Future Directions. arXiv preprintD. Saxena and J. Cao, "Generative Adversarial Networks (GANs): Challenges, Solutions, and Future Directions," arXiv preprint arXiv:2005.00065, 2020.

MNIST handwritten digit database. Y Lecun, C Cortes, C Burges, Y. LeCun, C. Cortes, and C. Burges, "MNIST handwritten digit database," 2010.

ASurvey OF STATE-OF-THE-ART GAN-BASED APPROACHES TO IMAGE SYNTHESIS. S N Esfahani, S Latifi, S. N. Esfahani and S. Latifi, "ASurvey OF STATE-OF-THE-ART GAN-BASED APPROACHES TO IMAGE SYNTHESIS."

An introduction to image synthesis with generative adversarial nets. H Huang, P S Yu, C Wang, arXiv:1803.04469arXiv preprintH. Huang, P. S. Yu, and C. Wang, "An introduction to image synthesis with generative adversarial nets," arXiv preprint arXiv:1803.04469, 2018.

A survey of image synthesis and editing with generative adversarial networks. X Wu, K Xu, P Hall, Tsinghua Science and Technology. 226X. Wu, K. Xu, and P. Hall, "A survey of image synthesis and editing with generative adversarial networks," Tsinghua Science and Technology, vol. 22, no. 6, pp. 660-674, 2017.

Recent Advances of Generative Adversarial Networks in Computer Vision. Y.-J Cao, IEEE Access. 7Y.-J. Cao et al., "Recent Advances of Generative Adversarial Networks in Computer Vision," IEEE Access, vol. 7, pp. 14985-15006, 2018.

A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis. J Agnese, J Herrera, H Tao, X Zhu, arXiv:1910.09399arXiv preprintJ. Agnese, J. Herrera, H. Tao, and X. Zhu, "A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis," arXiv preprint arXiv:1910.09399, 2019.

Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. H Xiao, K Rasul, R Vollgraf, arXiv:1708.07747arXiv preprintH. Xiao, K. Rasul, and R. Vollgraf, "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms," arXiv preprint arXiv:1708.07747, 2017.

Generative adversarial network in medical imaging: A review. X Yi, E Walia, P Babyn, 101552Medical image analysisX. Yi, E. Walia, and P. Babyn, "Generative adversarial network in medical imaging: A review," Medical image analysis, p. 101552, 2019.

Creating Artificial Images for Radiology Applications Using Generative Adversarial Networks (GANs)-A Systematic Review. V Sorin, Y Barash, E Konen, E Klang, Academic Radiology. V. Sorin, Y. Barash, E. Konen, and E. Klang, "Creating Artificial Images for Radiology Applications Using Generative Adversarial Networks (GANs)-A Systematic Review," Academic Radiology, 2020.

A Survey on GANs for Anomaly Detection. F Di Mattia, P Galeone, M De Simoni, E Ghelfi, arXiv:1906.11632arXiv preprintF. Di Mattia, P. Galeone, M. De Simoni, and E. Ghelfi, "A Survey on GANs for Anomaly Detection," arXiv preprint arXiv:1906.11632, 2019.

Audio Enhancement and Synthesis using Generative Adversarial Networks: A Survey. N Torres-Reyes, S Latifi, International Journal of Computer Applications. 9758887N. Torres-Reyes and S. Latifi, "Audio Enhancement and Synthesis using Generative Adversarial Networks: A Survey," International Journal of Computer Applications, vol. 975, p. 8887.

A review of generative adversarial networks and its application in cybersecurity. C Yinka-Banjo, O.-A Ugot, Artificial Intelligence Review. C. Yinka-Banjo and O.-A. Ugot, "A review of generative adversarial networks and its application in cybersecurity," Artificial Intelligence Review, pp. 1-16, 2019.

StoryGAN: A Sequential Conditional GAN for Story Visualization. Y Li, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionY. Li et al., "StoryGAN: A Sequential Conditional GAN for Story Visualization," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 6329-6338.

Mocogan: Decomposing motion and content for video generation. S Tulyakov, M.-Y Liu, X Yang, J Kautz, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionS. Tulyakov, M.-Y. Liu, X. Yang, and J. Kautz, "Mocogan: Decomposing motion and content for video generation," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 1526-1535.

Cascade Attention Guided Residue Learning GAN for Cross-Modal Translation. B Duan, W Wang, H Tang, H Latapie, Y Yan, arXiv:1907.01826arXiv preprintB. Duan, W. Wang, H. Tang, H. Latapie, and Y. Yan, "Cascade Attention Guided Residue Learning GAN for Cross-Modal Translation," arXiv preprint arXiv:1907.01826, 2019.

Video generation from text. Y Li, M R Min, D Shen, D Carlson, L Carin, Thirty-Second AAAI Conference on Artificial Intelligence. Y. Li, M. R. Min, D. Shen, D. Carlson, and L. Carin, "Video generation from text," in Thirty-Second AAAI Conference on Artificial Intelligence, 2018.

A Two-Stream Variational Adversarial Network for Video Generation. X Sun, H Xu, K Saenko, arXiv:1812.01037arXiv preprintX. Sun, H. Xu, and K. Saenko, "A Two-Stream Variational Adversarial Network for Video Generation," arXiv preprint arXiv:1812.01037, 2018.

Conditional GAN with discriminative filter generation for text-to-video synthesis. Y Balaji, M R Min, B Bai, R Chellappa, H P Graf, Proceedings of the 28th International Joint Conference on Artificial Intelligence. the 28th International Joint Conference on Artificial IntelligenceAAAI PressY. Balaji, M. R. Min, B. Bai, R. Chellappa, and H. P. Graf, "Conditional GAN with discriminative filter generation for text-to-video synthesis," in Proceedings of the 28th International Joint Conference on Artificial Intelligence, 2019: AAAI Press, pp. 1995-2001.

Hierarchical video generation from orthogonal information: Optical flow and texture. K Ohnishi, S Yamamoto, Y Ushiku, T Harada, Thirty-Second AAAI Conference on Artificial Intelligence. K. Ohnishi, S. Yamamoto, Y. Ushiku, and T. Harada, "Hierarchical video generation from orthogonal information: Optical flow and texture," in Thirty-Second AAAI Conference on Artificial Intelligence, 2018.

Temporal generative adversarial nets with singular value clipping. M Saito, E Matsumoto, S Saito, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionM. Saito, E. Matsumoto, and S. Saito, "Temporal generative adversarial nets with singular value clipping," in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 2830-2839.

TGANv2: Efficient Training of Large Models for Video Generation with Multiple Subsampling Layers. M Saito, S Saito, arXiv:1811.09245arXiv preprintM. Saito and S. Saito, "TGANv2: Efficient Training of Large Models for Video Generation with Multiple Subsampling Layers," arXiv preprint arXiv:1811.09245, 2018.

Efficient video generation on complex datasets. A Clark, J Donahue, K Simonyan, arXiv:1907.06571arXiv preprintA. Clark, J. Donahue, and K. Simonyan, "Efficient video generation on complex datasets," arXiv preprint arXiv:1907.06571, 2019.

Lip movements generation at a glance. L Chen, Z Li, R Maddox, Z Duan, C Xu, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)L. Chen, Z. Li, R. K Maddox, Z. Duan, and C. Xu, "Lip movements generation at a glance," in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 520-535.

End-to-end speech-driven facial animation with temporal gans. K Vougioukas, S Petridis, M Pantic, arXiv:1805.09313arXiv preprintK. Vougioukas, S. Petridis, and M. Pantic, "End-to-end speech-driven facial animation with temporal gans," arXiv preprint arXiv:1805.09313, 2018.

Talking face generation by adversarially disentangled audio-visual representation. H Zhou, Y Liu, Z Liu, P Luo, X Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33H. Zhou, Y. Liu, Z. Liu, P. Luo, and X. Wang, "Talking face generation by adversarially disentangled audio-visual representation," in Proceedings of the AAAI Conference on Artificial Intelligence, 2019, vol. 33, pp. 9299-9306.

Animating Face using Disentangled Audio Representations. G Mittal, B Wang, The IEEE Winter Conference on Applications of Computer Vision. G. Mittal and B. Wang, "Animating Face using Disentangled Audio Representations," in The IEEE Winter Conference on Applications of Computer Vision, 2020, pp. 3290-3298.

Speech-driven facial reenactment using conditional generative adversarial networks. S A Jalalifar, H Hasani, H Aghajan, arXiv:1803.07461arXiv preprintS. A. Jalalifar, H. Hasani, and H. Aghajan, "Speech-driven facial reenactment using conditional generative adversarial networks," arXiv preprint arXiv:1803.07461, 2018.

To create what you tell: Generating videos from captions. Y Pan, Z Qiu, T Yao, H Li, T Mei, Proceedings of the 25th ACM international conference on Multimedia. the 25th ACM international conference on MultimediaY. Pan, Z. Qiu, T. Yao, H. Li, and T. Mei, "To create what you tell: Generating videos from captions," in Proceedings of the 25th ACM international conference on Multimedia, 2017, pp. 1789-1798.

Video-to-video synthesis. T.-C Wang, arXiv:1808.06601arXiv preprintT.-C. Wang et al., "Video-to-video synthesis," arXiv preprint arXiv:1808.06601, 2018.

Few-shot video-to-video synthesis. T.-C Wang, M.-Y Liu, A Tao, G Liu, J Kautz, B Catanzaro, arXiv:1910.12713arXiv preprintT.-C. Wang, M.-Y. Liu, A. Tao, G. Liu, J. Kautz, and B. Catanzaro, "Few-shot video-to-video synthesis," arXiv preprint arXiv:1910.12713, 2019.

Video generation from single semantic label map. J Pan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJ. Pan et al., "Video generation from single semantic label map," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 3733-3742.

Deep multi-scale video prediction beyond mean square error. M Mathieu, C Couprie, Y Lecun, arXiv:1511.05440arXiv preprintM. Mathieu, C. Couprie, and Y. LeCun, "Deep multi-scale video prediction beyond mean square error," arXiv preprint arXiv:1511.05440, 2015.

Stochastic adversarial video prediction. A X Lee, R Zhang, F Ebert, P Abbeel, C Finn, S Levine, arXiv:1804.01523arXiv preprintA. X. Lee, R. Zhang, F. Ebert, P. Abbeel, C. Finn, and S. Levine, "Stochastic adversarial video prediction," arXiv preprint arXiv:1804.01523, 2018.

Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks. W Xiong, W Luo, L Ma, W Liu, J Luo, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionW. Xiong, W. Luo, L. Ma, W. Liu, and J. Luo, "Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 2364-2373.

Deep video generation, prediction and completion of human action sequences. H Cai, C Bai, Y.-W Tai, C.-K Tang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)H. Cai, C. Bai, Y.-W. Tai, and C.-K. Tang, "Deep video generation, prediction and completion of human action sequences," in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 366-382.

Toward multimodal image-to-image translation. J.-Y Zhu, Advances in neural information processing systems. J.-Y. Zhu et al., "Toward multimodal image-to-image translation," in Advances in neural information processing systems, 2017, pp. 465- 476.

Decomposing motion and content for natural video sequence prediction. R Villegas, J Yang, S Hong, X Lin, H Lee, arXiv:1706.08033arXiv preprintR. Villegas, J. Yang, S. Hong, X. Lin, and H. Lee, "Decomposing motion and content for natural video sequence prediction," arXiv preprint arXiv:1706.08033, 2017.

The pose knows: Video forecasting by generating pose futures. J Walker, K Marino, A Gupta, M Hebert, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionJ. Walker, K. Marino, A. Gupta, and M. Hebert, "The pose knows: Video forecasting by generating pose futures," in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 3332-3341.

Dual motion GAN for future-flow embedded video prediction. X Liang, L Lee, W Dai, E P Xing, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionX. Liang, L. Lee, W. Dai, and E. P. Xing, "Dual motion GAN for future-flow embedded video prediction," in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 1744-1752.

Unsupervised learning of disentangled representations from video. E L Denton, Advances in neural information processing systems. E. L. Denton, "Unsupervised learning of disentangled representations from video," in Advances in neural information processing systems, 2017, pp. 4414-4423.

Video synthesis from a single image and motion stroke. Q Hu, A Waelchli, T Portenier, M Zwicker, P Favaro, arXiv:1812.01874arXiv preprintQ. Hu, A. Waelchli, T. Portenier, M. Zwicker, and P. Favaro, "Video synthesis from a single image and motion stroke," arXiv preprint arXiv:1812.01874, 2018.

Everybody dance now. C Chan, S Ginosar, T Zhou, A A Efros, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionC. Chan, S. Ginosar, T. Zhou, and A. A. Efros, "Everybody dance now," in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 5933-5942.

Dance Dance Generation: Motion Transfer for Internet Videos. Y Zhou, Z Wang, C Fang, T Bui, T L Berg, arXiv:1904.00129arXiv preprintY. Zhou, Z. Wang, C. Fang, T. Bui, and T. L. Berg, "Dance Dance Generation: Motion Transfer for Internet Videos," arXiv preprint arXiv:1904.00129, 2019.

TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting. Z Yang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionZ. Yang et al., "TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 5306-5315.

Deep video portraits. H Kim, ACM Transactions on Graphics (TOG). 374H. Kim et al., "Deep video portraits," ACM Transactions on Graphics (TOG), vol. 37, no. 4, pp. 1-14, 2018.

Animating arbitrary objects via deep motion transfer. A Siarohin, S Lathuilière, S Tulyakov, E Ricci, N Sebe, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Siarohin, S. Lathuilière, S. Tulyakov, E. Ricci, and N. Sebe, "Animating arbitrary objects via deep motion transfer," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 2377-2386.

Neural rendering and reenactment of human actor videos. L Liu, ACM Transactions on Graphics (TOG). 385L. Liu et al., "Neural rendering and reenactment of human actor videos," ACM Transactions on Graphics (TOG), vol. 38, no. 5, pp. 1-14, 2019.

Vid2game: Controllable characters extracted from real-world videos. O Gafni, L Wolf, Y Taigman, arXiv:1904.08379arXiv preprintO. Gafni, L. Wolf, and Y. Taigman, "Vid2game: Controllable characters extracted from real-world videos," arXiv preprint arXiv:1904.08379, 2019.

Reenactgan: Learning to reenact faces via boundary transfer. W Wu, Y Zhang, C Li, C Qian, C. Change Loy, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)W. Wu, Y. Zhang, C. Li, C. Qian, and C. Change Loy, "Reenactgan: Learning to reenact faces via boundary transfer," in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 603-619.

Recycle-gan: Unsupervised video retargeting. A Bansal, S Ma, D Ramanan, Y Sheikh, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)A. Bansal, S. Ma, D. Ramanan, and Y. Sheikh, "Recycle-gan: Unsupervised video retargeting," in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 119-135.

Unpaired image-to-image translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionJ.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, "Unpaired image-to-image translation using cycle-consistent adversarial networks," in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2223-2232.

On the difficulty of training recurrent neural networks. R Pascanu, T Mikolov, Y Bengio, International conference on machine learning. R. Pascanu, T. Mikolov, and Y. Bengio, "On the difficulty of training recurrent neural networks," in International conference on machine learning, 2013, pp. 1310-1318.

UCF101: A dataset of 101 human actions classes from videos in the wild. K Soomro, A R Zamir, M Shah, arXiv:1212.0402arXiv preprintK. Soomro, A. R. Zamir, and M. Shah, "UCF101: A dataset of 101 human actions classes from videos in the wild," arXiv preprint arXiv:1212.0402, 2012.

From actemes to action: A strongly-supervised representation for detailed action understanding. W Zhang, M Zhu, K G Derpanis, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionW. Zhang, M. Zhu, and K. G. Derpanis, "From actemes to action: A strongly-supervised representation for detailed action understanding," in Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 2248-2255.

Recognizing human actions: a local SVM approach. C Schuldt, I Laptev, B Caputo, Proceedings of the 17th International Conference on Pattern Recognition. the 17th International Conference on Pattern RecognitionIEEE3C. Schuldt, I. Laptev, and B. Caputo, "Recognizing human actions: a local SVM approach," in Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004., 2004, vol. 3: IEEE, pp. 32-36.

An audio-visual corpus for speech perception and automatic speech recognition. M Cooke, J Barker, S Cunningham, X Shao, The Journal of the Acoustical Society of America. 1205M. Cooke, J. Barker, S. Cunningham, and X. Shao, "An audio-visual corpus for speech perception and automatic speech recognition," The Journal of the Acoustical Society of America, vol. 120, no. 5, pp. 2421-2424, 2006.

TCD-TIMIT: An audio-visual corpus of continuous speech. N Harte, E Gillen, IEEE Transactions on Multimedia. 175N. Harte and E. Gillen, "TCD-TIMIT: An audio-visual corpus of continuous speech," IEEE Transactions on Multimedia, vol. 17, no. 5, pp. 603-615, 2015.

Deepstory: Video story qa by deep embedded memory networks. K.-M Kim, M.-O Heo, S.-H Choi, B.-T Zhang, arXiv:1707.00836arXiv preprintK.-M. Kim, M.-O. Heo, S.-H. Choi, and B.-T. Zhang, "Deepstory: Video story qa by deep embedded memory networks," arXiv preprint arXiv:1707.00836, 2017.

Self-supervised visual planning with temporal skip connections. F Ebert, C Finn, A X Lee, S Levine, arXiv:1710.05268arXiv preprintF. Ebert, C. Finn, A. X. Lee, and S. Levine, "Self-supervised visual planning with temporal skip connections," arXiv preprint arXiv:1710.05268, 2017.

Faceforensics: A large-scale video dataset for forgery detection in human faces. A Rössler, D Cozzolino, L Verdoliva, C Riess, J Thies, M Nießner, arXiv:1803.09179arXiv preprintA. Rössler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Nießner, "Faceforensics: A large-scale video dataset for forgery detection in human faces," arXiv preprint arXiv:1803.09179, 2018.

The MUG facial expression database. N Aifanti, C Papachristou, A Delopoulos, 11th International Workshop on Image Analysis for Multimedia Interactive Services WIAMIS 10. IEEEN. Aifanti, C. Papachristou, and A. Delopoulos, "The MUG facial expression database," in 11th International Workshop on Image Analysis for Multimedia Interactive Services WIAMIS 10, 2010: IEEE, pp. 1-4.

Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. C Ionescu, D Papava, V Olaru, C Sminchisescu, IEEE transactions on pattern analysis and machine intelligence. 36C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, "Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments," IEEE transactions on pattern analysis and machine intelligence, vol. 36, no. 7, pp. 1325-1339, 2013.

Gradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. 8611Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, "Gradient-based learning applied to document recognition," Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, 1998.

Large-scale video classification with convolutional neural networks. A Karpathy, G Toderici, S Shetty, T Leung, R Sukthankar, L Fei-Fei, Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. the IEEE conference on Computer Vision and Pattern RecognitionA. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei, "Large-scale video classification with convolutional neural networks," in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2014, pp. 1725-1732.

A Gorban, THUMOS challenge: Action recognition with a large number of classes. A. Gorban et al., "THUMOS challenge: Action recognition with a large number of classes," ed, 2015.

Learning methods for generic object recognition with invariance to pose and lighting. Y Lecun, F J Huang, L Bottou, Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. the 2004 IEEE Computer Society Conference on Computer Vision and Pattern RecognitionIEEE2104Y. LeCun, F. J. Huang, and L. Bottou, "Learning methods for generic object recognition with invariance to pose and lighting," in Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004., 2004, vol. 2: IEEE, pp. II-104.

Semantic scene completion from a single depth image. S Song, F Yu, A Zeng, A X Chang, M Savva, T Funkhouser, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionS. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser, "Semantic scene completion from a single depth image," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1746-1754.

Audiovisual database of spoken American English. Linguistic Data Consortium. C Richie, S Warburton, M Carter, C. Richie, S. Warburton, and M. Carter, Audiovisual database of spoken American English. Linguistic Data Consortium, 2009.

Actions as space-time shapes. M Blank, L Gorelick, E Shechtman, M Irani, R Basri, Tenth IEEE International Conference on Computer Vision (ICCV'05. IEEE1M. Blank, L. Gorelick, E. Shechtman, M. Irani, and R. Basri, "Actions as space-time shapes," in Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1, 2005, vol. 2: IEEE, pp. 1395-1402.

Learning from synthetic humans. G Varol, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionG. Varol et al., "Learning from synthetic humans," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 109-117.

Lip reading in the wild. J S Chung, A Zisserman, Asian Conference on Computer Vision. SpringerJ. S. Chung and A. Zisserman, "Lip reading in the wild," in Asian Conference on Computer Vision, 2016: Springer, pp. 87-103.

CREMA-D: Crowd-sourced emotional multimodal actors dataset. H Cao, D G Cooper, M K Keutmann, R C Gur, A Nenkova, R Verma, IEEE transactions on affective computing. 54H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova, and R. Verma, "CREMA-D: Crowd-sourced emotional multimodal actors dataset," IEEE transactions on affective computing, vol. 5, no. 4, pp. 377-390, 2014.

LRS3-TED: a large-scale dataset for visual speech recognition. T Afouras, J S Chung, A Zisserman, arXiv:1809.00496arXiv preprintT. Afouras, J. S. Chung, and A. Zisserman, "LRS3-TED: a large-scale dataset for visual speech recognition," arXiv preprint arXiv:1809.00496, 2018.

Vision meets robotics: The kitti dataset. A Geiger, P Lenz, C Stiller, R Urtasun, The International Journal of Robotics Research. 3211A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, "Vision meets robotics: The kitti dataset," The International Journal of Robotics Research, vol. 32, no. 11, pp. 1231-1237, 2013.

Pedestrian detection: A benchmark. B Schiele, P Dollár, C Wojek, P Perona, Computer Vision and Pattern Recognition (CVPR). B. Schiele, P. Dollár, C. Wojek, and P. Perona, "Pedestrian detection: A benchmark," in Computer Vision and Pattern Recognition (CVPR), 2009.

Are you really smiling at me? spontaneous versus posed enjoyment smiles. H Dibeklioğlu, A A Salah, T Gevers, European Conference on Computer Vision. SpringerH. Dibeklioğlu, A. A. Salah, and T. Gevers, "Are you really smiling at me? spontaneous versus posed enjoyment smiles," in European Conference on Computer Vision, 2012: Springer, pp. 525-538.

Playing for benchmarks. S R Richter, Z Hayder, V Koltun, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionS. R. Richter, Z. Hayder, and V. Koltun, "Playing for benchmarks," in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 2213-2222.

The cityscapes dataset for semantic urban scene understanding. M Cordts, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionM. Cordts et al., "The cityscapes dataset for semantic urban scene understanding," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 3213-3223.

The apolloscape dataset for autonomous driving. X Huang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsX. Huang et al., "The apolloscape dataset for autonomous driving," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2018, pp. 954-960.

Sync-draw: automatic video generation using deep recurrent attentive architectures. G Mittal, T Marwah, V N Balasubramanian, Proceedings of the 25th ACM international conference on Multimedia. the 25th ACM international conference on MultimediaG. Mittal, T. Marwah, and V. N. Balasubramanian, "Sync-draw: automatic video generation using deep recurrent attentive architectures," in Proceedings of the 25th ACM international conference on Multimedia, 2017, pp. 1096-1104.

Collecting highly parallel data for paraphrase evaluation. D Chen, W B Dolan, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesD. Chen and W. B. Dolan, "Collecting highly parallel data for paraphrase evaluation," in Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 2011, pp. 190-200.