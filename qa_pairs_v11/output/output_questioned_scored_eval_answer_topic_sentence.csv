corpusid,title,url,beginning_sentence,score_of_answer_topic_sentence
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,We have comprehensively covered the research works related to T-BPLMs.,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,Knowing the association of a neuron with a concept enables explanation of model's output.,1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations.,1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"LLMs can also be used for quality assessment on some NLG tasks, such as summarization and translation.",1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"Although LLMs have achieved remarkable success in various natural language processing tasks, their performance in regression tasks has been less impressive.",1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"Gaining a deeper understanding of emergent abilities, inverse scaling phenomenon and U-shape phenomenon in LLMs is essential for advancing research in this field.",1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community.",1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"Scaling of LLMs (e.g. parameters, training computation, etc.) can greatly empower pretrained language models.",1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training.,1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness.",1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,Closed-book question-answering tasks require the model to answer a given question about factual knowledge without any external information.,1.0
249642175,Multimodal Learning with Transformers: A Survey,https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,"Discussion All these aforementioned self-attention variants for multimodal interactions are modality-generic, and can be applied in flexible strategies and for multi-granular tasks.",1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"In information retrieval (IR) tasks, LLMs are not widely exploited yet.",1.0
249642175,Multimodal Learning with Transformers: A Survey,https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,"In spite of the recent advances, multimodal pretraining Transformer methods still have some obvious bottlenecks.",1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"When deploying LLMs for downstream tasks, we often face challenges stemming from distributional differences between the test/user data and that of the training data.",1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant.",1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful.",1.0
249642175,Multimodal Learning with Transformers: A Survey,https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,"In general, MML Transformers fuse information across multiple modalities primarily at three levels: input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion).",1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"Although language models are typically task-agnostic in architecture, these methods require fine-tuning on datasets of the specific downstream task.",1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages.,1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections.,1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,The models that have high accuracy on the scenario also have good robustness.,1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks.",1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships.,1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task.",1.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,"In this section, the performance of models on HotpotQA [11] will be investigated.",1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks.",1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision.,1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized.,1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,Task scheduling determines the order of tasks in which an MTL model is trained.,1.0
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,A number of studies have looked at the types of knowledge encoded in BERT's weights.,1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"Fairness and Bias. LLMs have been shown to exhibit disparate treatment and impact, perpetuating societal biases and potentially leading to discrimination [10,17].",1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"For text generation tasks, MTL is brought in to improve the quality of the generated text.",1.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,"The sequence models have been first used for single-hop MRC tasks, and most of them are based on Recurrent Neural Networks (RNNs), some studies focus on using them in multi-hop MRC.",1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks.",1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously.",1.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,"Complicated question is a basic challenge of multi-hop MRC, unlike the single-hop questions, they cannot be answered easily and require complicated reasoning.",1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"Harmful content. Due to the high coherence, quality, and plausibility of texts generated by LLMs, harmful contents from LLMs can cause significant harm, including hate speech, discrimination, incitement to violence, false narratives, and even social engineering attack.",1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"Spurious Biases. The shortcut learning problem has been observed in various natural language understanding tasks under the pretraining and fine-tuning paradigm, where models heavily rely on spurious correlations between input and labels in the fine-tuning data for prediction [31,35,98].",1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"As natural language data is readily available and unsupervised training paradigms have been proposed to better utilize extremely large datasets, this motivates the unsupervised learning of natural language.",1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,Pre-training data plays a pivotal role in the development of large language models.,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,"Once we have identified neurons that capture a certain concept learned in a model, these can be utilized for controlling the model's behavior w.r.t to that concept.",1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,It is difficult to obtain a large amount of in-domain text in some cases.,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,"IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets.",1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,SentencePiece [79] -A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space.,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,The second class of corpusbased methods aim to discover neurons for a given concept.,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,Below is a summary of the key findings that emerged from the work we covered in this survey.,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,Main embeddings represent the given input sequence in low dimensional space.,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,"In the last few decades, the amount of biomedical literature is growing at a rapid scale.",1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics.,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs.,1.0
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,Pre-training + fine-tuning workflow is a crucial part of BERT.,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,"In the case of small models, BioBERT outperformed others.",1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,Data Augmentation -Data augmentation helps us to create new training instances from existing instances.,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,"In the beginning, the standard approach to develop BPLMs is to start with general PLMs and then further pretrain them on large volumes of biomedical text [16].",1.0
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,"To date, more studies were devoted to BERT's knowledge of syntactic rather than semantic phenomena.",1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,Pretraining provides BPLMs with necessary background knowledge which can be transferred to downstream tasks.,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,"During pretraining, the language models learn language representations based on the supervision provided by one or more pretraining tasks.",1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"In hierarchical feature pipeline, the output of one task is used as extra features for another task.",1.0
249642175,Multimodal Learning with Transformers: A Survey,https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,Identifying the strengths of Transformers for multimodal machine learning is a big open problem.,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it.,1.0
249642175,Multimodal Learning with Transformers: A Survey,https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,"In practice, the distribution gap between training data and practical data is noticeable.",1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade.,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,Neurons capture syntactic concepts and complex semantic concepts.,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,"Human languages are hierarchical in structure where morphology and phonology sit at the bottom followed by lexemes, followed by syntactic structures.",1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,Neurons exhibit monosemous and polysemous behavior.,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion.,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,"Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function.",1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,"While it is exciting to see that networks somewhat preserve linguistic hierarchy, many authors found that information is not discretely preserved at any individual layer, but is distributed and is redundantly present in the network.",1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,The distribution of neurons across the network has led researchers to draw interesting crossarchitectural comparisons.,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,Most of the biomedical language models (except ELECTRA-based models) are pre-trained using MLM.,0.5
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"In some cases, multiple tasks are learned sequentially.",0.5
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1.,0.5
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,"Models like BioBERT, PubMedBERT have achieved good results in many of the tasks.",0.5
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients.",0.5
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,Fine-tuning must be done iteratively to reduce the noisy labeled instances.,0.5
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"Interactive Architecture. Different from most machine learning models that give predictions in a single pass, hierarchical interactive MTL explicitly models the interactions between tasks via a multi-turn prediction mechanism which allows a model to refine its predictions over multiple steps with the help of the previous outputs from other tasks in a way similar to recurrent neural networks.",0.5
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment.",0.5
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,"Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123].",0.5
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus.,0.5
249642175,Multimodal Learning with Transformers: A Survey,https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,Transformers is an open problem.,0.5
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,"As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text.",0.5
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks.",0.5
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset.",0.5
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,Multi-label datasets can be created by giving extra manual annotations to existing data.,0.5
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,Code embeddings -Code embeddings map the given sequence of codes into a sequence of vectors.,0.5
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,"Subword Embeddings-In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words.",0.5
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,"Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only.",0.5
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,Unigram [78] -Unigram is similar to BPE and Word-Piece in fixing the vocabulary size at the beginning itself.,0.5
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,"In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.",0.5
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task.",0.5
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"Scaling of models also endows the model with some unprecedented, fantastic abilities that go beyond the power-law rule.",0.5
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"Additionally, LLMs are highly skilled in open-ended generations.",0.5
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"In machine translation (MT), LLMs can perform competent translation, although the average performance is slightly worse than some commercial translation tools [45] considering some automatic metrics like BLEU [78].",0.5
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,"Initialization can have a dramatic effect on the training process (Petrov, 2010).",0.5
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,"Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have.",0.5
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary).,0.5
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,Transforming input from tasks like IR and sentence labeling into a few/zero-short instruction form is non-trivial.,0.5
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,"At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019).",0.5
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"However, different parts of the shared features are not equally important to each task.",0.5
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,Benchmarks that require verbal reasoning.,0.5
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,Visualization has been used as a qualitative measure to evaluate the selected neurons.,0.5
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations.,0.5
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,"Concept A concept represents a coherent fragment of knowledge, such as ""a class containing certain objects as elements, where the objects have certain properties"" (Stock, 2010).",0.5
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,"Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018).",0.5
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,"A simple way to discover the role of a neuron is by visualizing its activations and manually identifying the underlying concept over a set of sentences (Karpathy et al., 2015;Fyshe et al., 2015;Li et al., 2016a).",0.5
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value.,0.5
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,"Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018).",0.5
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,"The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed.",0.5
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,Concept Search This set of methods take a neuron as an input and search for a concept that the neuron has learned.,0.5
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"Another scenario is that the knowledge within LLMs about real world is useless to the task, or even the required knowledge is counterfactual to the real world.",0.5
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,"Probing-based methods train diagnostic classifiers (Hupkes et al., 2018) over activations to identify neurons with respect to pre-defined concepts.",0.5
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,"MLM component of BERT is easy to adapt for knowledge induction by filling in the blanks (e.g. ""Cats like to chase [ ]"").",0.5
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"Additionally, some mechanics such as instruction tuning [91,112] and human alignment tuning [77] further boost the capabilities of LLMs to better comprehend and follow user instructions.",0.5
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,"Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE .",0.5
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"Parameter-Efficient Tuning. In practice, we may tune the model on some specific datasets.",0.5
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,Privacy. LLMs can face serious security issues.,0.5
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,The final layers of BERT are the most taskspecific.,0.5
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,"The main part of this model is Relation Extractor with two different structures: 1) classification-type (CRERC), where the evidence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; 2) span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor can automatically extract multiple corresponding spans from the question text as question relations.",0.5
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,They first find all possible path from passages.,0.5
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5).,0.5
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,"Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task.",0.5
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,"There are two phases in this model: 1) the Selecting phase to select the most relevant sentence to the network memory state as the initial sentence of the current hop, and 2) the Establishing phase to prepare to go to the next hop by updating the network memory state.",0.5
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,Graph-based techniques because of their natural language relationship representation ability [39] has attracted attention in multihop MRC.,0.5
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,"Song et al. [40] focused on inferring global context as an important key in multi-hop reading comprehension, while previous studies approximate global evidence with local coreference information with DAG-styled GRU.",0.5
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,"For inferring the global context, the related information of the constructed graph has been merged.",0.5
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,GF: Shao et al. [53] attempted to answer this question: How much does graph structure contribute to answer a multi-hop question.,0.5
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,"To date, the most systematic study of BERT architecture was performed by .",0.0
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,"Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,.",0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,"For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation.",0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,"Recent works exploited general models for clinical STS [56], [57], [65], [159].",0.0
211532403,A Primer in BERTology: What we know about how BERT works,https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,"To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge.",0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"Examples of Autoregressive Language Models include GPT-3 [16], OPT [126], PaLM [22], and BLOOM [92].",0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,Masked Language Modeling (MLM).,0.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,Limitation The attribution-based methods highlight salient neurons with respect to a prediction.,0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,Then they proposed Answer Multi-hop questions by Single-hop QA (AMS) models and used a single-hop QA models based on the attention mechanism with the HGN's document filter.,0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,"Then they proposed the latent query reformulation method (IP-LQR), which incorporates phrases in the latent query reformulation to improve the cognitive ability of the system.",0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,"However, [35] have shown that if the number of inferences increases, the complexity of models will rise sharply due to the iteration of cumbersome message passing algorithm, resulting in low efficiency.",0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,Tu et al. [46] proposed a more complex graph named Heterogeneous Document-Entity (HDE) graph with different types of nodes and edges.,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,"Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification.",0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,"Self-assembling MNM: Jiang and Bansal [18] focused on identifying the sub-questions in the correct reasoning order and presented an interpretable and controller-based self-assembling neural modular network for the multi-hop reasoning process which includes two main parts, Modular Network with a Controller (top) and the Dynamically-assembled Modular Network (bottom) that can be seen in Figure 10.",0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"Flops, while a T5 11B model only requires 3.30 × 10 22 , which is 10 times less.",0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,"For example. GPT-3 [16] shows the emergent ability for word sorting, and word unscrambling tasks.",0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,The massive multitask language understanding (MMLU) [40] is also highly knowledge-intensive.,0.0
249642175,Multimodal Learning with Transformers: A Survey,https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,"Tokenization Vanilla Transformer was originally proposed for machine translation as a sequence-to-sequence model, thus it is straightforward to take the vocabulary sequences as input.",0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,One of the representative tasks is miscellaneous text classification [59].,0.0
249642175,Multimodal Learning with Transformers: A Survey,https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,"Thus, all the multimodal token positions can be attended as a whole sequence, such that the positions of each modality can be encoded well by conditioning the context of other modalities.",0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,CivilComments [13] even the best one is only better than random guessing [59].,0.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,"Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end.",0.0
249642175,Multimodal Learning with Transformers: A Survey,https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,Multimodal Transformers suffer from two major efficiency issues: (1),0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,"As Figure 16 shows, after embedding sentences into a matrix and constructing a graph, the policy is run to select the next sentence.",0.0
