corpusid,title,domain,url,section,paragraph,answer,filtered_refids,filtered_refids_qualified,num_reference,segmented_answer,beginning_sentence,question,QA pair,score_of_answer_topic_sentence,score_of_answer_standalone,score_of_answer_synthesis,score_of_answer_coherence,score_of_answer_legibility,score_of_answer_coherence_seperated,score_of_answer_coherence_diff,score_of_answer_standalone_hardrule,score_of_answer_standalone_diff,score_of_answer_depthness
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s1,p1.1,"In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","['b65', 'b21']","[True, True]",2,"1. In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.
2. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
3. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.
4. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.
5. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.
6. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.",What do studies reveal about the characteristics of BERT's embeddings?,"Question: What do studies reveal about the characteristics of BERT's embeddings?

1. In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.
2. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
3. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.
4. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.
5. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.
6. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).",0.5,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s2,p2.0,"A number of studies have looked at the types of knowledge encoded in BERT's weights. The popular approaches include fill-in-the-gap probes of BERT's MLM, analysis of self-attention weights, and probing classifiers using different BERT representations as inputs.  showed that BERT representations are hierarchical rather than linear, i.e. there is something akin to syntactic tree structure in addition to the word order information. Tenney et al. (2019b) and  also showed that BERT embeddings encode information about parts of speech, syntactic chunks and roles. However, BERT's knowledge of syntax is partial, since probing classifiers could not recover the labels of distant parent nodes in the syntactic tree .",['b51'],[True],1,"1. A number of studies have looked at the types of knowledge encoded in BERT's weights.
2. The popular approaches include fill-in-the-gap probes of BERT's MLM, analysis of self-attention weights, and probing classifiers using different BERT representations as inputs.
3. showed that BERT representations are hierarchical rather than linear, i.e. there is something akin to syntactic tree structure in addition to the word order information.
4. Tenney et al. (2019b) and  also showed that BERT embeddings encode information about parts of speech, syntactic chunks and roles.
5. However, BERT's knowledge of syntax is partial, since probing classifiers could not recover the labels of distant parent nodes in the syntactic tree .",A number of studies have looked at the types of knowledge encoded in BERT's weights.,What methods reveal the types of knowledge encoded in BERT's weights?,"Question: What methods reveal the types of knowledge encoded in BERT's weights?

1. A number of studies have looked at the types of knowledge encoded in BERT's weights.
2. The popular approaches include fill-in-the-gap probes of BERT's MLM, analysis of self-attention weights, and probing classifiers using different BERT representations as inputs.
3. showed that BERT representations are hierarchical rather than linear, i.e. there is something akin to syntactic tree structure in addition to the word order information.
4. Tenney et al. (2019b) and  also showed that BERT embeddings encode information about parts of speech, syntactic chunks and roles.
5. However, BERT's knowledge of syntax is partial, since probing classifiers could not recover the labels of distant parent nodes in the syntactic tree .",1.0,1.0,0.5,1.0,1.0,1.0,0.0,0.6,0.4,0.0
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s3,p3.2,"Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","['b64', None, 'b59']","[True, True, True]",3,"1. Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task.
2. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences.
3. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations.
4. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input.
5. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019).
6. This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task.",How does BERT's MLM demonstrate syntactic competence and limitations according to recent studies?,"Question: How does BERT's MLM demonstrate syntactic competence and limitations according to recent studies?

1. Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task.
2. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences.
3. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations.
4. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input.
5. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019).
6. This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.",0.5,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s4,p4.0,"To date, more studies were devoted to BERT's knowledge of syntactic rather than semantic phenomena. However, we do have evidence from an MLM probing study that BERT has some knowledge for semantic roles (Ettinger, 2019). BERT is even able to prefer the incorrect fillers for semantic roles that are semantically related to the correct ones, to those that are unrelated (e.g. ""to tip a chef"" should be better than ""to tip a robin"", but worse than ""to tip a waiter""). Tenney et al. (2019b) showed that BERT encodes information about entity types, relations, semantic roles, and proto-roles, since this information can be detected with probing classifiers.",['b51'],[True],1,"1. To date, more studies were devoted to BERT's knowledge of syntactic rather than semantic phenomena.
2. However, we do have evidence from an MLM probing study that BERT has some knowledge for semantic roles (Ettinger, 2019).
3. BERT is even able to prefer the incorrect fillers for semantic roles that are semantically related to the correct ones, to those that are unrelated (e.g. ""to tip a chef"" should be better than ""to tip a robin"", but worse than ""to tip a waiter"").
4. Tenney et al. (2019b) showed that BERT encodes information about entity types, relations, semantic roles, and proto-roles, since this information can be detected with probing classifiers.","To date, more studies were devoted to BERT's knowledge of syntactic rather than semantic phenomena.",What does research reveal about BERT's understanding of semantic roles?,"Question: What does research reveal about BERT's understanding of semantic roles?

1. To date, more studies were devoted to BERT's knowledge of syntactic rather than semantic phenomena.
2. However, we do have evidence from an MLM probing study that BERT has some knowledge for semantic roles (Ettinger, 2019).
3. BERT is even able to prefer the incorrect fillers for semantic roles that are semantically related to the correct ones, to those that are unrelated (e.g. ""to tip a chef"" should be better than ""to tip a robin"", but worse than ""to tip a waiter"").
4. Tenney et al. (2019b) showed that BERT encodes information about entity types, relations, semantic roles, and proto-roles, since this information can be detected with probing classifiers.",1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.75,0.25,1.0
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s5,p5.0,"MLM component of BERT is easy to adapt for knowledge induction by filling in the blanks (e.g. ""Cats like to chase [ ]""). There is at least one probing study of world knowledge in BERT (Ettinger, 2019), but the bulk of evidence comes from  (Petroni et al., 2019) numerous practitioners using BERT to extract such knowledge. Petroni et al. (2019) showed that, for some relation types, vanilla BERT is competitive with methods relying on knowledge bases (Figure 3). Davison et al. (2019) suggest that it generalizes better to unseen data. However, to retrieve BERT's knowledge we need good template sentences, and there is work on their automatic extraction and augmentation (Bouraoui et al., 2019; However, BERT cannot reason based on its world knowledge. Forbes et al. (2019) show that BERT can ""guess"" the affordances and properties of many objects, but does not have the information about their interactions (e.g. it ""knows"" that people can walk into houses, and that houses are big, but it cannot infer that houses are bigger than people.)  and  also show that the performance drops with the number of necessary inference steps. At the same time, Poerner et al. (2019) show that some of BERT's success in factoid knowledge retrieval comes from learning stereotypical character combinations, e.g. it would predict that a person with an Italian-sounding name is Italian, even when it is factually incorrect.","['b27', 'b31', None]","[True, True, True]",3,"1. MLM component of BERT is easy to adapt for knowledge induction by filling in the blanks (e.g. ""Cats like to chase [ ]"").
2. There is at least one probing study of world knowledge in BERT (Ettinger, 2019), but the bulk of evidence comes from  (Petroni et al., 2019) numerous practitioners using BERT to extract such knowledge.
3. Petroni et al. (2019) showed that, for some relation types, vanilla BERT is competitive with methods relying on knowledge bases (Figure 3).
4. Davison et al. (2019) suggest that it generalizes better to unseen data.
5. However, to retrieve BERT's knowledge we need good template sentences, and there is work on their automatic extraction and augmentation (Bouraoui et al., 2019; However, BERT cannot reason based on its world knowledge.
6. Forbes et al. (2019) show that BERT can ""guess"" the affordances and properties of many objects, but does not have the information about their interactions (e.g. it ""knows"" that people can walk into houses, and that houses are big, but it cannot infer that houses are bigger than people.)  and  also show that the performance drops with the number of necessary inference steps.
7. At the same time, Poerner et al. (2019) show that some of BERT's success in factoid knowledge retrieval comes from learning stereotypical character combinations, e.g. it would predict that a person with an Italian-sounding name is Italian, even when it is factually incorrect.","MLM component of BERT is easy to adapt for knowledge induction by filling in the blanks (e.g. ""Cats like to chase [ ]"").",How does BERT adapt for knowledge induction and what are its limitations in reasoning?,"Question: How does BERT adapt for knowledge induction and what are its limitations in reasoning?

1. MLM component of BERT is easy to adapt for knowledge induction by filling in the blanks (e.g. ""Cats like to chase [ ]"").
2. There is at least one probing study of world knowledge in BERT (Ettinger, 2019), but the bulk of evidence comes from  (Petroni et al., 2019) numerous practitioners using BERT to extract such knowledge.
3. Petroni et al. (2019) showed that, for some relation types, vanilla BERT is competitive with methods relying on knowledge bases (Figure 3).
4. Davison et al. (2019) suggest that it generalizes better to unseen data.
5. However, to retrieve BERT's knowledge we need good template sentences, and there is work on their automatic extraction and augmentation (Bouraoui et al., 2019; However, BERT cannot reason based on its world knowledge.
6. Forbes et al. (2019) show that BERT can ""guess"" the affordances and properties of many objects, but does not have the information about their interactions (e.g. it ""knows"" that people can walk into houses, and that houses are big, but it cannot infer that houses are bigger than people.)  and  also show that the performance drops with the number of necessary inference steps.
7. At the same time, Poerner et al. (2019) show that some of BERT's success in factoid knowledge retrieval comes from learning stereotypical character combinations, e.g. it would predict that a person with an Italian-sounding name is Italian, even when it is factually incorrect.",0.5,1.0,1.0,1.0,1.0,1.0,0.0,0.8571428571428571,0.1428571428571429,1.0
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s7,p7.9,"Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","['b9', None]","[True, True]",2,"1. Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,.
2. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks.
3. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998).
4. Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,.",How do specific BERT heads impact coreference resolution and semantic relation tasks?,"Question: How do specific BERT heads impact coreference resolution and semantic relation tasks?

1. Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,.
2. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks.
3. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998).
4. Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.",0.0,1.0,0.5,1.0,1.0,0.5,-0.5,1.0,0.0,0.0
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s8,p8.2,"The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.","['b5', 'b57', 'b50']","[True, True, True]",3,"1. The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5).
2. There is conflicting evidence about syntactic chunks.
3. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling.
4. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing.
5. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.",The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5).,How do BERT's middle layers impact syntactic information processing according to recent studies?,"Question: How do BERT's middle layers impact syntactic information processing according to recent studies?

1. The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5).
2. There is conflicting evidence about syntactic chunks.
3. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling.
4. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing.
5. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.",0.5,0.0,1.0,1.0,0.0,1.0,0.0,0.8,-0.8,0.0
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s8,p8.3,"The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.","['b9', None, 'b50']","[True, True, True]",3,"1. The final layers of BERT are the most taskspecific.
2. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable .
3. In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019).
4. At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance.
5. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers.
6. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182).
7. But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial.
8. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.",The final layers of BERT are the most taskspecific.,How do the final layers of BERT impact its task specificity and model performance?,"Question: How do the final layers of BERT impact its task specificity and model performance?

1. The final layers of BERT are the most taskspecific.
2. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable .
3. In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019).
4. At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance.
5. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers.
6. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182).
7. But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial.
8. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.",0.5,1.0,1.0,1.0,1.0,1.0,0.0,0.875,0.125,1.0
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s10,p10.9,"Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).","['b9', 'b31']","[True, True]",2,"1. Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE .
2. Alternatively, SemBERT  integrates semantic role information with BERT representations.
3. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides.
4. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6).
5. However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).","Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE .",How can external knowledge be integrated into BERT models for improved performance?,"Question: How can external knowledge be integrated into BERT models for improved performance?

1. Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE .
2. Alternatively, SemBERT  integrates semantic role information with BERT representations.
3. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides.
4. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6).
5. However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).",0.5,0.5,0.5,0.5,1.0,0.5,0.0,0.6,-0.0999999999999999,0.0
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s11,p11.0,"To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.","['b58', 'b75', 'b17', 'b20']","[True, True, True, True]",4,"1. To date, the most systematic study of BERT architecture was performed by .
2. They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others.
3. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable.
4. Larger hidden representation size was consistently better, but the gains varied by setting.
5. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance.
6. They also publish their recommendations for other model parameters.
7. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.
8. observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.","To date, the most systematic study of BERT architecture was performed by .",What are the key findings from systematic studies on optimizing BERT architecture?,"Question: What are the key findings from systematic studies on optimizing BERT architecture?

1. To date, the most systematic study of BERT architecture was performed by .
2. They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others.
3. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable.
4. Larger hidden representation size was consistently better, but the gains varied by setting.
5. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance.
6. They also publish their recommendations for other model parameters.
7. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.
8. observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.",0.0,0.0,0.0,1.0,0.0,0.5,-0.5,0.625,-0.625,1.0
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s12,p12.0,"Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).","['b9', None]","[True, True]",2,"1. Pre-training + fine-tuning workflow is a crucial part of BERT.
2. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand.
3. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns.
4. It is understandable why fine-tuning would increase the attention to  Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).",Pre-training + fine-tuning workflow is a crucial part of BERT.,How does fine-tuning affect BERT's attention to linguistic patterns?,"Question: How does fine-tuning affect BERT's attention to linguistic patterns?

1. Pre-training + fine-tuning workflow is a crucial part of BERT.
2. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand.
3. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns.
4. It is understandable why fine-tuning would increase the attention to  Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).",1.0,0.0,0.5,0.5,1.0,1.0,0.5,1.0,-1.0,0.0
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s12,p12.5,"Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.",['b28'],[True],1,"1. Initialization can have a dramatic effect on the training process (Petrov, 2010).
2. However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018).
3. Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation.
4. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.","Initialization can have a dramatic effect on the training process (Petrov, 2010).",How does initialization affect NLP model training and reported performance improvements?,"Question: How does initialization affect NLP model training and reported performance improvements?

1. Initialization can have a dramatic effect on the training process (Petrov, 2010).
2. However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018).
3. Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation.
4. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.",0.5,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,0.0
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s13,p13.1,"Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).","['b37', 'b46', 'b20', None, 'b58', 'b9', 'b50']","[True, True, True, True, True, True, True]",7,"1. Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have.
2. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance.
3. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head.
4. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance.
5. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019).
6. Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).","Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have.",How do pruning Transformer heads and layers affect BERT model performance?,"Question: How do pruning Transformer heads and layers affect BERT model performance?

1. Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have.
2. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance.
3. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head.
4. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance.
5. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019).
6. Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).",0.5,1.0,1.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s15,p15.0,"Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).","['b64', 'b68', 'b36', 'b13', 'b30']","[True, True, True, True, True]",5,"1. Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary).
2. Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing.
3. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019).
4. The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019).
5. Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).",Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary).,What is Multilingual BERT and how does it perform across different tasks and languages?,"Question: What is Multilingual BERT and how does it perform across different tasks and languages?

1. Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary).
2. Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing.
3. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019).
4. The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019).
5. Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).",0.5,1.0,1.0,1.0,1.0,1.0,0.0,0.8,0.1999999999999999,0.0
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s15,p15.3,"At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.","['b68', 'b30', 'b36', 'b13']","[True, True, True, True]",4,"1. At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019).
2. Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages.
3. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary.
4. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.","At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019).",How does mBERT's syntactic awareness and crosslingual transfer capabilities function across languages?,"Question: How does mBERT's syntactic awareness and crosslingual transfer capabilities function across languages?

1. At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019).
2. Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages.
3. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary.
4. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.",0.5,0.0,1.0,0.0,0.0,1.0,1.0,0.75,-0.75,0.0
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s15,p15.4,"To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.",['b50'],[True],1,"1. To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge.
2. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"".
3. There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis .
4. A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.","To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge.",What are the proposed improvements for mBERT based on its knowledge and probing studies?,"Question: What are the proposed improvements for mBERT based on its knowledge and probing studies?

1. To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge.
2. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"".
3. There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis .
4. A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.",0.0,1.0,0.5,1.0,1.0,1.0,0.0,0.0,1.0,0.0
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s16,p16.1,"Benchmarks that require verbal reasoning. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020). As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.","['b77', 'b41', 'b45', 'b35']","[True, True, True, True]",4,"1. Benchmarks that require verbal reasoning.
2. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems.
3. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020).
4. As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it.
5. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.",Benchmarks that require verbal reasoning.,What are the limitations of BERT's verbal reasoning abilities according to recent studies?,"Question: What are the limitations of BERT's verbal reasoning abilities according to recent studies?

1. Benchmarks that require verbal reasoning.
2. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems.
3. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020).
4. As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it.
5. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.",0.5,1.0,0.5,1.0,1.0,1.0,0.0,0.8,0.1999999999999999,0.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s1,p1.2,"Concept A concept represents a coherent fragment of knowledge, such as ""a class containing certain objects as elements, where the objects have certain properties"" (Stock, 2010). For example, a concept could be lexical: e.g. words ending with suffix ""ed"", morphological: e.g. gerund verbs, or semantic: e.g. names of cities. We loosely define a concept C as a group of words that are coherent w.r.t to a linguistic property. Table 1 shows an example sentence with different concept annotations.",['b44'],[True],1,"1. Concept A concept represents a coherent fragment of knowledge, such as ""a class containing certain objects as elements, where the objects have certain properties"" (Stock, 2010).
2. For example, a concept could be lexical: e.g. words ending with suffix ""ed"", morphological: e.g. gerund verbs, or semantic: e.g. names of cities.
3. We loosely define a concept C as a group of words that are coherent w.r.t to a linguistic property.
4. Table 1 shows an example sentence with different concept annotations.","Concept A concept represents a coherent fragment of knowledge, such as ""a class containing certain objects as elements, where the objects have certain properties"" (Stock, 2010).",What is a concept in linguistic terms?,"Question: What is a concept in linguistic terms?

1. Concept A concept represents a coherent fragment of knowledge, such as ""a class containing certain objects as elements, where the objects have certain properties"" (Stock, 2010).
2. For example, a concept could be lexical: e.g. words ending with suffix ""ed"", morphological: e.g. gerund verbs, or semantic: e.g. names of cities.
3. We loosely define a concept C as a group of words that are coherent w.r.t to a linguistic property.
4. Table 1 shows an example sentence with different concept annotations.",0.5,0.0,0.5,1.0,1.0,1.0,0.0,0.5,-0.5,0.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s3,p3.0,"A simple way to discover the role of a neuron is by visualizing its activations and manually identifying the underlying concept over a set of sentences (Karpathy et al., 2015;Fyshe et al., 2015;Li et al., 2016a). Given that deep NLP models are † Table 3 in Appendix gives a more comprehensive list. trained using billions of neurons, it is impossible to visualize all the neurons. A number of clues have been used to shortlist the neurons for visualization, for example, selecting saturated neurons, high/low variance neurons, or ignoring dead neurons (Karpathy et al., 2015) when using ReLU activation function. ‡ Limitation While visualization is a simple approach to find an explanation for a neuron, it has some major limitations: i) it is qualitative and subjective, ii) it cannot be scaled to the entire network due to an extensive human-in-the-loop effort, iii) it is difficult to interpret polysemous neurons that acquire multiple roles in different contexts, iv) it is ineffective in identifying group neurons, and lastly and v) not all neurons are visually interpretable. Visualization nevertheless remains a useful tool when applied in combination to other interpretation methods that are discussed below.","['b22', 'b11', 'b20']","[True, True, True]",3,"1. A simple way to discover the role of a neuron is by visualizing its activations and manually identifying the underlying concept over a set of sentences (Karpathy et al., 2015;Fyshe et al., 2015;Li et al., 2016a).
2. Given that deep NLP models are † Table 3 in Appendix gives a more comprehensive list.
3. trained using billions of neurons, it is impossible to visualize all the neurons.
4. A number of clues have been used to shortlist the neurons for visualization, for example, selecting saturated neurons, high/low variance neurons, or ignoring dead neurons (Karpathy et al., 2015) when using ReLU activation function.
5. ‡ Limitation While visualization is a simple approach to find an explanation for a neuron, it has some major limitations: i)
6. it is qualitative and subjective, ii) it cannot be scaled to the entire network due to an extensive human-in-the-loop effort, iii) it is difficult to interpret polysemous neurons that acquire multiple roles in different contexts, iv) it is ineffective in identifying group neurons, and lastly and v) not all neurons are visually interpretable.
7. Visualization nevertheless remains a useful tool when applied in combination to other interpretation methods that are discussed below.","A simple way to discover the role of a neuron is by visualizing its activations and manually identifying the underlying concept over a set of sentences (Karpathy et al., 2015;Fyshe et al., 2015;Li et al., 2016a).",What are the methods and limitations of discovering a neuron's role through visualization?,"Question: What are the methods and limitations of discovering a neuron's role through visualization?

1. A simple way to discover the role of a neuron is by visualizing its activations and manually identifying the underlying concept over a set of sentences (Karpathy et al., 2015;Fyshe et al., 2015;Li et al., 2016a).
2. Given that deep NLP models are † Table 3 in Appendix gives a more comprehensive list.
3. trained using billions of neurons, it is impossible to visualize all the neurons.
4. A number of clues have been used to shortlist the neurons for visualization, for example, selecting saturated neurons, high/low variance neurons, or ignoring dead neurons (Karpathy et al., 2015) when using ReLU activation function.
5. ‡ Limitation While visualization is a simple approach to find an explanation for a neuron, it has some major limitations: i)
6. it is qualitative and subjective, ii) it cannot be scaled to the entire network due to an extensive human-in-the-loop effort, iii) it is difficult to interpret polysemous neurons that acquire multiple roles in different contexts, iv) it is ineffective in identifying group neurons, and lastly and v) not all neurons are visually interpretable.
7. Visualization nevertheless remains a useful tool when applied in combination to other interpretation methods that are discussed below.",0.5,0.0,0.5,1.0,0.0,1.0,0.0,0.4285714285714285,-0.4285714285714285,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s4,p4.1,"Concept Search This set of methods take a neuron as an input and search for a concept that the neuron has learned. They sort the input instances based on the activation values of the given neuron. The top activating instances represent a concept the neuron represents. Kádár et al. (2017) discovered neurons that learn various linguistic con-cepts using this approach. They extracted top-20, 5-gram contexts for each neuron based on the magnitude of activations and manually identified the underlying concepts. This manual effort of identifying concepts is cumbersome and requires a human-in-the-loop. Na et al. (2019) addressed this by using lexical concepts of various granularities. Instead of 5-gram contexts, they extracted top-k activating sentences for each neuron. They parsed the sentences to create concepts (words and phrases) using the nodes of the parse trees. They then created synthetic sentences that highlight a concept e.g. a particular word occurring in all synthetic sentences. The neurons that activates largely on these sentences are considered to have learned the concept. This methodology is useful in analyzing neurons that are responsible for multi-word concepts such as phrases and idiomatic collocations. However, the synthetic sentences are often ungrammatical and lead towards a risk of identifying neurons that exhibit arbitrary behavior (like repetition) instead of concept specific behavior.","['b19', 'b34']","[True, True]",2,"1. Concept Search This set of methods take a neuron as an input and search for a concept that the neuron has learned.
2. They sort the input instances based on the activation values of the given neuron.
3. The top activating instances represent a concept the neuron represents.
4. Kádár et al. (2017) discovered neurons that learn various linguistic con-cepts using this approach.
5. They extracted top-20, 5-gram contexts for each neuron based on the magnitude of activations and manually identified the underlying concepts.
6. This manual effort of identifying concepts is cumbersome and requires a human-in-the-loop.
7. Na et al. (2019) addressed this by using lexical concepts of various granularities.
8. Instead of 5-gram contexts, they extracted top-k activating sentences for each neuron.
9. They parsed the sentences to create concepts (words and phrases) using the nodes of the parse trees.
10. They then created synthetic sentences that highlight a concept e.g. a particular word occurring in all synthetic sentences.
11. The neurons that activates largely on these sentences are considered to have learned the concept.
12. This methodology is useful in analyzing neurons that are responsible for multi-word concepts such as phrases and idiomatic collocations.
13. However, the synthetic sentences are often ungrammatical and lead towards a risk of identifying neurons that exhibit arbitrary behavior (like repetition) instead of concept specific behavior.",Concept Search This set of methods take a neuron as an input and search for a concept that the neuron has learned.,How do concept search methods identify what a neuron has learned?,"Question: How do concept search methods identify what a neuron has learned?

1. Concept Search This set of methods take a neuron as an input and search for a concept that the neuron has learned.
2. They sort the input instances based on the activation values of the given neuron.
3. The top activating instances represent a concept the neuron represents.
4. Kádár et al. (2017) discovered neurons that learn various linguistic con-cepts using this approach.
5. They extracted top-20, 5-gram contexts for each neuron based on the magnitude of activations and manually identified the underlying concepts.
6. This manual effort of identifying concepts is cumbersome and requires a human-in-the-loop.
7. Na et al. (2019) addressed this by using lexical concepts of various granularities.
8. Instead of 5-gram contexts, they extracted top-k activating sentences for each neuron.
9. They parsed the sentences to create concepts (words and phrases) using the nodes of the parse trees.
10. They then created synthetic sentences that highlight a concept e.g. a particular word occurring in all synthetic sentences.
11. The neurons that activates largely on these sentences are considered to have learned the concept.
12. This methodology is useful in analyzing neurons that are responsible for multi-word concepts such as phrases and idiomatic collocations.
13. However, the synthetic sentences are often ungrammatical and lead towards a risk of identifying neurons that exhibit arbitrary behavior (like repetition) instead of concept specific behavior.",0.5,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s5,p5.0,"The second class of corpusbased methods aim to discover neurons for a given concept. The underlying idea is the same i.e. to establish a link between the concept and neurons based on co-occurrences stats, but in the opposite direction. The activation values play a role in weighing these links to obtained a ranked list of neurons against the concept. Mu and Andreas (2020) achieved this by creating a binary mask of a neuron based on a threshold on its activation values for every sentence in the corpus. Similarly, they created a binary mask for every concept based on its presence or absence in a sentence. They then computed the overlap between a given neuron mask vector and a concept mask vector using intersection-over-union (IoU), and use these to generate compositional explanations. Differently from them, Suau et al. (2020) used the values of neuron activations as prediction scores and computed the average precision per neuron and per concept. Finally, Antverg and Belinkov (2022) considered the mean activation values of a neuron with respect to instances that posses the concept of interest.",[None],[True],1,"1. The second class of corpusbased methods aim to discover neurons for a given concept.
2. The underlying idea is the same i.e. to establish a link between the concept and neurons based on co-occurrences stats, but in the opposite direction.
3. The activation values play a role in weighing these links to obtained a ranked list of neurons against the concept.
4. Mu and Andreas (2020) achieved this by creating a binary mask of a neuron based on a threshold on its activation values for every sentence in the corpus.
5. Similarly, they created a binary mask for every concept based on its presence or absence in a sentence.
6. They then computed the overlap between a given neuron mask vector and a concept mask vector using intersection-over-union (IoU), and use these to generate compositional explanations.
7. Differently from them, Suau et al. (2020) used the values of neuron activations as prediction scores and computed the average precision per neuron and per concept.
8. Finally, Antverg and Belinkov (2022) considered the mean activation values of a neuron with respect to instances that posses the concept of interest.",The second class of corpusbased methods aim to discover neurons for a given concept.,How do corpus-based methods identify neurons associated with specific concepts?,"Question: How do corpus-based methods identify neurons associated with specific concepts?

1. The second class of corpusbased methods aim to discover neurons for a given concept.
2. The underlying idea is the same i.e. to establish a link between the concept and neurons based on co-occurrences stats, but in the opposite direction.
3. The activation values play a role in weighing these links to obtained a ranked list of neurons against the concept.
4. Mu and Andreas (2020) achieved this by creating a binary mask of a neuron based on a threshold on its activation values for every sentence in the corpus.
5. Similarly, they created a binary mask for every concept based on its presence or absence in a sentence.
6. They then computed the overlap between a given neuron mask vector and a concept mask vector using intersection-over-union (IoU), and use these to generate compositional explanations.
7. Differently from them, Suau et al. (2020) used the values of neuron activations as prediction scores and computed the average precision per neuron and per concept.
8. Finally, Antverg and Belinkov (2022) considered the mean activation values of a neuron with respect to instances that posses the concept of interest.",1.0,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s6,p6.0,"Probing-based methods train diagnostic classifiers (Hupkes et al., 2018) over activations to identify neurons with respect to pre-defined concepts. They are a global interpretation methods that discover a set of neurons with respect to each concept using supervised data annotations. They are highly scalable, and can be easily applied on a large set of neurons and over a large set of concepts. In the following, we cover two types of classifiers used for probing.",['b17'],[True],1,"1. Probing-based methods train diagnostic classifiers (Hupkes et al., 2018) over activations to identify neurons with respect to pre-defined concepts.
2. They are a global interpretation methods that discover a set of neurons with respect to each concept using supervised data annotations.
3. They are highly scalable, and can be easily applied on a large set of neurons and over a large set of concepts.
4. In the following, we cover two types of classifiers used for probing.","Probing-based methods train diagnostic classifiers (Hupkes et al., 2018) over activations to identify neurons with respect to pre-defined concepts.",What are probing-based methods and how do they function in neural network interpretation?,"Question: What are probing-based methods and how do they function in neural network interpretation?

1. Probing-based methods train diagnostic classifiers (Hupkes et al., 2018) over activations to identify neurons with respect to pre-defined concepts.
2. They are a global interpretation methods that discover a set of neurons with respect to each concept using supervised data annotations.
3. They are highly scalable, and can be easily applied on a large set of neurons and over a large set of concepts.
4. In the following, we cover two types of classifiers used for probing.",0.5,1.0,0.0,1.0,1.0,1.0,0.0,0.75,0.25,0.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s7,p7.0,"The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.","['b21', 'b38']","[True, True]",2,"1. The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed.
2. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept.
3. The regularization of the classifier directly effects the weights and therefore the ranking of neurons.
4. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero.
5. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features.
6. This translates to discovering group neurons that are jointly responsible for a concept.
7. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.","The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed.",How does regularization affect neuron importance in concept learning models?,"Question: How does regularization affect neuron importance in concept learning models?

1. The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed.
2. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept.
3. The regularization of the classifier directly effects the weights and therefore the ranking of neurons.
4. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero.
5. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features.
6. This translates to discovering group neurons that are jointly responsible for a concept.
7. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.",0.5,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s7,p7.1,"Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.","['b15', 'b55']","[True, True]",2,"1. Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018).
2. Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization.
3. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.
4. suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.","Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018).",What are the limitations and solutions in probing classifiers for concept reflection?,"Question: What are the limitations and solutions in probing classifiers for concept reflection?

1. Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018).
2. Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization.
3. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.
4. suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.",0.5,0.0,0.5,1.0,1.0,1.0,0.0,0.75,-0.75,0.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s8,p8.1,"Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.","['b22', 'b21']","[True, True]",2,"1. Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value.
2. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance.
3. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised).
4. The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a).
5. The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019).
6. Here, the output class serves as the concept against which we want to find the salient neurons.",Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value.,What is ablation and how does it identify salient neurons in neural networks?,"Question: What is ablation and how does it identify salient neurons in neural networks?

1. Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value.
2. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance.
3. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised).
4. The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a).
5. The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019).
6. Here, the output class serves as the concept against which we want to find the salient neurons.",0.5,1.0,1.0,1.0,1.0,1.0,0.0,0.8333333333333334,0.1666666666666666,0.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s8,p8.3,"Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.","['b26', 'b46', None, 'b1', 'b49']","[True, True, True, True, True]",5,"1. Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018).
2. Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact.
3. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact.
4. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.","Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018).",What are attribution-based methods in neural network research?,"Question: What are attribution-based methods in neural network research?

1. Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018).
2. Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact.
3. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact.
4. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.",0.5,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,0.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s8,p8.4,Limitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.,[None],[True],1,"1. Limitation The attribution-based methods highlight salient neurons with respect to a prediction.
2. What concepts these salient neurons have learned is unknown.
3. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept.
4. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.",Limitation The attribution-based methods highlight salient neurons with respect to a prediction.,What are the limitations of attribution-based methods in understanding learned concepts?,"Question: What are the limitations of attribution-based methods in understanding learned concepts?

1. Limitation The attribution-based methods highlight salient neurons with respect to a prediction.
2. What concepts these salient neurons have learned is unknown.
3. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept.
4. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.",0.0,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,0.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s9,p9.1,"Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.","['b37', 'b19', 'b6']","[True, True, True]",3,"1. Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations.
2. It is possible that a neuron represents a diverse concept which is not featured in the corpus.
3. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations.
4. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role.
5. Corpus generation has been widely explored in Computer Vision.
6. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron.
7. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs.
8. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.",Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations.,How does Corpus Generation enhance neuron analysis in NLP?,"Question: How does Corpus Generation enhance neuron analysis in NLP?

1. Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations.
2. It is possible that a neuron represents a diverse concept which is not featured in the corpus.
3. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations.
4. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role.
5. Corpus generation has been widely explored in Computer Vision.
6. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron.
7. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs.
8. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.",0.5,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s9,p9.3,"Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.",['b35'],[True],1,"1. Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function.
2. Given a model, the activations of an input sentence form a matrix.
3. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept.
4. MF is a local interpretation method.
5. It is commonly used in analyzing vision models (Olah et al., 2018).
6. We could not find any research using MF on the NLP models.
7. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.","Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function.",What is Matrix Factorization and its application in analyzing vision and NLP models?,"Question: What is Matrix Factorization and its application in analyzing vision and NLP models?

1. Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function.
2. Given a model, the activations of an input sentence form a matrix.
3. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept.
4. MF is a local interpretation method.
5. It is commonly used in analyzing vision models (Olah et al., 2018).
6. We could not find any research using MF on the NLP models.
7. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.",1.0,1.0,0.0,0.5,1.0,1.0,0.5,0.7142857142857143,0.2857142857142857,0.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s9,p9.5,"Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.","['b28', 'b29']","[True, True]",2,"1. Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion.
2. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster.
3. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.
4. aimed at identifying redundant neurons in the network.
5. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them.
6. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.",Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion.,How do clustering methods analyze neuron groups in unsupervised learning?,"Question: How do clustering methods analyze neuron groups in unsupervised learning?

1. Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion.
2. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster.
3. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.
4. aimed at identifying redundant neurons in the network.
5. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them.
6. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.",1.0,1.0,0.5,1.0,1.0,1.0,0.0,0.8333333333333334,0.1666666666666666,0.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s9,p9.7,"Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it. The search involves identifying neurons that behave similarly across the models. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.",['b39'],[True],1,"1. Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it.
2. The search involves identifying neurons that behave similarly across the models.
3. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models.
4. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model.
5. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.",Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it.,What is multi-model search and how is it implemented?,"Question: What is multi-model search and how is it implemented?

1. Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it.
2. The search involves identifying neurons that behave similarly across the models.
3. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models.
4. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model.
5. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.",1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s15,p15.0,"Visualization has been used as a qualitative measure to evaluate the selected neurons. For example, Dalvi et al. (2019) visualized the top neurons and showed that they focus on very specific linguistic properties. They also visualized top-k activating words for the top neurons per concept to demonstrate the efficacy of their method. Visualization can be a very effective tool to evaluate the interpretations when it works in tandem with other methods e.g. using Concept Search or Probingbased methods to reduce the search space towards only highly activating concepts or the most salient neurons for these concepts respectively.",[None],[True],1,"1. Visualization has been used as a qualitative measure to evaluate the selected neurons.
2. For example, Dalvi et al. (2019) visualized the top neurons and showed that they focus on very specific linguistic properties.
3. They also visualized top-k activating words for the top neurons per concept to demonstrate the efficacy of their method.
4. Visualization can be a very effective tool to evaluate the interpretations when it works in tandem with other methods e.g. using Concept Search or Probingbased methods to reduce the search space towards only highly activating concepts or the most salient neurons for these concepts respectively.",Visualization has been used as a qualitative measure to evaluate the selected neurons.,How is visualization used to evaluate neurons in linguistic studies?,"Question: How is visualization used to evaluate neurons in linguistic studies?

1. Visualization has been used as a qualitative measure to evaluate the selected neurons.
2. For example, Dalvi et al. (2019) visualized the top neurons and showed that they focus on very specific linguistic properties.
3. They also visualized top-k activating words for the top neurons per concept to demonstrate the efficacy of their method.
4. Visualization can be a very effective tool to evaluate the interpretations when it works in tandem with other methods e.g. using Concept Search or Probingbased methods to reduce the search space towards only highly activating concepts or the most salient neurons for these concepts respectively.",0.5,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,0.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s18,p18.1,"Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.","['b37', 'b34', 'b20', 'b22', 'b19']","[True, True, True, True, True]",5,"1. Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end.
2. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment.
3. For example ""I like this movie a lot"" or ""the movie is incredibly good"".
4. Similarly they discovered neurons that captured ""negation"".
5. Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained.
6. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task.
7. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"".
8. Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc.
9. They also found neurons that learn phrasal concepts.
10. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation.
11. They provided finer interpretation of the neurons by generating synthetic instances.
12. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.","Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end.",How do neurons in neural networks learn and represent different linguistic and conceptual information?,"Question: How do neurons in neural networks learn and represent different linguistic and conceptual information?

1. Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end.
2. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment.
3. For example ""I like this movie a lot"" or ""the movie is incredibly good"".
4. Similarly they discovered neurons that captured ""negation"".
5. Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained.
6. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task.
7. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"".
8. Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc.
9. They also found neurons that learn phrasal concepts.
10. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation.
11. They provided finer interpretation of the neurons by generating synthetic instances.
12. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.",0.0,1.0,0.0,0.5,1.0,1.0,0.5,1.0,0.0,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s20,p20.2,"Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.","[None, 'b54']","[True, True]",2,"1. Neurons exhibit monosemous and polysemous behavior.
2. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis.
3. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc.
4. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"".
5. concepts. Suau et al. (2020) discovered neurons that capture different senses of a word.
6. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.",Neurons exhibit monosemous and polysemous behavior.,How do neurons exhibit monosemous and polysemous behavior in language processing?,"Question: How do neurons exhibit monosemous and polysemous behavior in language processing?

1. Neurons exhibit monosemous and polysemous behavior.
2. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis.
3. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc.
4. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"".
5. concepts. Suau et al. (2020) discovered neurons that capture different senses of a word.
6. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.",1.0,1.0,0.0,0.5,1.0,1.0,0.5,0.6666666666666666,0.3333333333333333,0.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s20,p20.3,Neurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.,"['b34', 'b21', 'b42', 'b20']","[True, True, True, True]",4,"1. Neurons capture syntactic concepts and complex semantic concepts.
2. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates.
3. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency.
4. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases.
5. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.",Neurons capture syntactic concepts and complex semantic concepts.,How do neurons capture syntactic and complex semantic concepts in language processing?,"Question: How do neurons capture syntactic and complex semantic concepts in language processing?

1. Neurons capture syntactic concepts and complex semantic concepts.
2. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates.
3. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency.
4. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases.
5. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.",1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s23,p23.0,"Human languages are hierarchical in structure where morphology and phonology sit at the bottom followed by lexemes, followed by syntactic structures. Concepts such as semantics and pragmatics are placed on the top of the hierarchy.  analyzed linguistic hierarchy by studying the spread of neurons across layers in various pre-trained language models. They extracted salient neurons with respect to different linguistic concepts (e.g. morphology and syntax) and found that neurons that capture word morphology were predominantly found in the lower and middle layers and those learning about syntax were found at the higher layers. The observation was found to be true in both LSTMand the transformer-based architectures, and are inline with the findings of representation analysis (Liu et al., 2019;Tenney et al., 2019;Belinkov et al., 2020b). Similarly Suau et al. (2020) analyzed sub-modules within GPT and RoBERTa transformer blocks and showed that lower layers within a transformer block accumulate more salient neurons than higher layers on the tasks of word sense disambiguation or homograph detection. They also found that the neurons that learn homographs are distributed across the network as opposed to sense neurons that were more predominantly found at the lower layers.","[None, 'b48', 'b25']","[True, True, True]",3,"1. Human languages are hierarchical in structure where morphology and phonology sit at the bottom followed by lexemes, followed by syntactic structures.
2. Concepts such as semantics and pragmatics are placed on the top of the hierarchy.
3. analyzed linguistic hierarchy by studying the spread of neurons across layers in various pre-trained language models.
4. They extracted salient neurons with respect to different linguistic concepts (e.g. morphology and syntax) and found that neurons that capture word morphology were predominantly found in the lower and middle layers and those learning about syntax were found at the higher layers.
5. The observation was found to be true in both LSTMand the transformer-based architectures, and are inline with the findings of representation analysis (Liu et al., 2019;Tenney et al., 2019;Belinkov et al., 2020b).
6. Similarly Suau et al. (2020) analyzed sub-modules within GPT and RoBERTa transformer blocks and showed that lower layers within a transformer block accumulate more salient neurons than higher layers on the tasks of word sense disambiguation or homograph detection.
7. They also found that the neurons that learn homographs are distributed across the network as opposed to sense neurons that were more predominantly found at the lower layers.","Human languages are hierarchical in structure where morphology and phonology sit at the bottom followed by lexemes, followed by syntactic structures.",How do human language hierarchies relate to neuron distribution in language models?,"Question: How do human language hierarchies relate to neuron distribution in language models?

1. Human languages are hierarchical in structure where morphology and phonology sit at the bottom followed by lexemes, followed by syntactic structures.
2. Concepts such as semantics and pragmatics are placed on the top of the hierarchy.
3. analyzed linguistic hierarchy by studying the spread of neurons across layers in various pre-trained language models.
4. They extracted salient neurons with respect to different linguistic concepts (e.g. morphology and syntax) and found that neurons that capture word morphology were predominantly found in the lower and middle layers and those learning about syntax were found at the higher layers.
5. The observation was found to be true in both LSTMand the transformer-based architectures, and are inline with the findings of representation analysis (Liu et al., 2019;Tenney et al., 2019;Belinkov et al., 2020b).
6. Similarly Suau et al. (2020) analyzed sub-modules within GPT and RoBERTa transformer blocks and showed that lower layers within a transformer block accumulate more salient neurons than higher layers on the tasks of word sense disambiguation or homograph detection.
7. They also found that the neurons that learn homographs are distributed across the network as opposed to sense neurons that were more predominantly found at the lower layers.",1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.8571428571428571,0.1428571428571429,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s24,p24.0,"While it is exciting to see that networks somewhat preserve linguistic hierarchy, many authors found that information is not discretely preserved at any individual layer, but is distributed and is redundantly present in the network. This is an artifact of various training choices such as dropout that encourages the model to distribute knowledge across the network. For example, Li et al. (2016b) found specialized frequency neurons in a GloVe model trained without dropout, as opposed to the variant trained with dropout where the information was more redundantly available.  showed that a significant amount of redundancy existed within pre-trained models. They showed that 85% of the neurons across the network are redundant and at least 92% of the neurons can be removed when optimizing towards a downstream task in feature-based transfer learning.",['b23'],[True],1,"1. While it is exciting to see that networks somewhat preserve linguistic hierarchy, many authors found that information is not discretely preserved at any individual layer, but is distributed and is redundantly present in the network.
2. This is an artifact of various training choices such as dropout that encourages the model to distribute knowledge across the network.
3. For example, Li et al. (2016b) found specialized frequency neurons in a GloVe model trained without dropout, as opposed to the variant trained with dropout where the information was more redundantly available.
4. showed that a significant amount of redundancy existed within pre-trained models.
5. They showed that 85% of the neurons across the network are redundant and at least 92% of the neurons can be removed when optimizing towards a downstream task in feature-based transfer learning.","While it is exciting to see that networks somewhat preserve linguistic hierarchy, many authors found that information is not discretely preserved at any individual layer, but is distributed and is redundantly present in the network.",How does dropout influence information distribution in neural networks?,"Question: How does dropout influence information distribution in neural networks?

1. While it is exciting to see that networks somewhat preserve linguistic hierarchy, many authors found that information is not discretely preserved at any individual layer, but is distributed and is redundantly present in the network.
2. This is an artifact of various training choices such as dropout that encourages the model to distribute knowledge across the network.
3. For example, Li et al. (2016b) found specialized frequency neurons in a GloVe model trained without dropout, as opposed to the variant trained with dropout where the information was more redundantly available.
4. showed that a significant amount of redundancy existed within pre-trained models.
5. They showed that 85% of the neurons across the network are redundant and at least 92% of the neurons can be removed when optimizing towards a downstream task in feature-based transfer learning.",1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.8,0.1999999999999999,0.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s25,p25.0,"The distribution of neurons across the network has led researchers to draw interesting crossarchitectural comparisons. Wu et al. (2020) performed correlation clustering of neurons across architectures and found that different architectures may have similar representations, but their individual neurons behave differently. Hennigen et al. (2020) compared neurons in contextualized (BERT) embedding with neurons in the static embedding (fastText) and found that fastText required two neurons to capture any morphosyntactic phenomenon as opposed to BERT which required up to 35 neurons to obtain the same performance.  showed that the linguistic knowledge in BERT (auto-encoder) is highly distributed across the network as opposed to XLNet (auto-regressive) where neurons from a few layers are mainly responsible for a concept (see Figure 2). Similarly Suau et al. (2020) compared RoBERTa and GPT (auto-encoder vs. generative) models and found differences in the distribution of expert neurons.  extended the cross-architectural comparison towards fine-tuned models. They showed that after finetuning on GLUE tasks, the neurons capturing linguistic knowledge are regressed to lower layers in RoBERTa and XLNet as opposed to BERT where it is still retained at the higher layers.","['b53', None]","[True, True]",2,"1. The distribution of neurons across the network has led researchers to draw interesting crossarchitectural comparisons.
2. Wu et al. (2020) performed correlation clustering of neurons across architectures and found that different architectures may have similar representations, but their individual neurons behave differently.
3. Hennigen et al. (2020) compared neurons in contextualized (BERT) embedding with neurons in the static embedding (fastText) and found that fastText required two neurons to capture any morphosyntactic phenomenon as opposed to BERT which required up to 35 neurons to obtain the same performance.
4. showed that the linguistic knowledge in BERT (auto-encoder) is highly distributed across the network as opposed to XLNet (auto-regressive) where neurons from a few layers are mainly responsible for a concept (see Figure 2).
5. Similarly Suau et al. (2020) compared RoBERTa and GPT (auto-encoder vs. generative) models and found differences in the distribution of expert neurons.
6. extended the cross-architectural comparison towards fine-tuned models.
7. They showed that after finetuning on GLUE tasks, the neurons capturing linguistic knowledge are regressed to lower layers in RoBERTa and XLNet as opposed to BERT where it is still retained at the higher layers.",The distribution of neurons across the network has led researchers to draw interesting crossarchitectural comparisons.,How do neuron distributions vary across different neural network architectures?,"Question: How do neuron distributions vary across different neural network architectures?

1. The distribution of neurons across the network has led researchers to draw interesting crossarchitectural comparisons.
2. Wu et al. (2020) performed correlation clustering of neurons across architectures and found that different architectures may have similar representations, but their individual neurons behave differently.
3. Hennigen et al. (2020) compared neurons in contextualized (BERT) embedding with neurons in the static embedding (fastText) and found that fastText required two neurons to capture any morphosyntactic phenomenon as opposed to BERT which required up to 35 neurons to obtain the same performance.
4. showed that the linguistic knowledge in BERT (auto-encoder) is highly distributed across the network as opposed to XLNet (auto-regressive) where neurons from a few layers are mainly responsible for a concept (see Figure 2).
5. Similarly Suau et al. (2020) compared RoBERTa and GPT (auto-encoder vs. generative) models and found differences in the distribution of expert neurons.
6. extended the cross-architectural comparison towards fine-tuned models.
7. They showed that after finetuning on GLUE tasks, the neurons capturing linguistic knowledge are regressed to lower layers in RoBERTa and XLNet as opposed to BERT where it is still retained at the higher layers.",1.0,0.0,1.0,1.0,1.0,1.0,0.0,0.7142857142857143,-0.7142857142857143,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s26,p26.0,"Below is a summary of the key findings that emerged from the work we covered in this survey. Neurons learned within Deep NLP models capture non-trivial linguistic knowledge ranging from lexical phenomenon such as morphemes, words and multi-word expressions to highly complex global phenomenon such as semantic roles and syntactic dependencies. Neuron analysis resonates with the findings of representation analysis (Belinkov et al., 2017a,b;Tenney et al., 2019;Liu et al., 2019) in demonstrating that the networks follow linguistic hierarchy. Linguistic neurons are distributed across the network based on their complexity, with lower layers focused on the lexical concepts and middle and higher layers learning global phenomenon based on long-range contextual dependencies. While the networks preserve linguistic hierarchy, many authors showed that information is not discretely preserved, but is rather distributed and redundantly present in the network. It was also shown that a small optimal subset of neurons w.r.t any concept can be extracted from a network. On another dimension, a few works showed that some concepts are localized to fewer neurons while others are distributed to a large group. Finally, some interesting cross architectural analyses were drawn based on how the neurons are distributed within their layers.","[None, 'b48', 'b25']","[True, True, True]",3,"1. Below is a summary of the key findings that emerged from the work we covered in this survey.
2. Neurons learned within Deep NLP models capture non-trivial linguistic knowledge ranging from lexical phenomenon such as morphemes, words and multi-word expressions to highly complex global phenomenon such as semantic roles and syntactic dependencies.
3. Neuron analysis resonates with the findings of representation analysis (Belinkov et al., 2017a,b;Tenney et al., 2019;Liu et al., 2019) in demonstrating that the networks follow linguistic hierarchy.
4. Linguistic neurons are distributed across the network based on their complexity, with lower layers focused on the lexical concepts and middle and higher layers learning global phenomenon based on long-range contextual dependencies.
5. While the networks preserve linguistic hierarchy, many authors showed that information is not discretely preserved, but is rather distributed and redundantly present in the network.
6. It was also shown that a small optimal subset of neurons w.r.t any concept can be extracted from a network.
7. On another dimension, a few works showed that some concepts are localized to fewer neurons while others are distributed to a large group.
8. Finally, some interesting cross architectural analyses were drawn based on how the neurons are distributed within their layers.",Below is a summary of the key findings that emerged from the work we covered in this survey.,What do neurons in Deep NLP models learn about linguistic knowledge and hierarchy?,"Question: What do neurons in Deep NLP models learn about linguistic knowledge and hierarchy?

1. Below is a summary of the key findings that emerged from the work we covered in this survey.
2. Neurons learned within Deep NLP models capture non-trivial linguistic knowledge ranging from lexical phenomenon such as morphemes, words and multi-word expressions to highly complex global phenomenon such as semantic roles and syntactic dependencies.
3. Neuron analysis resonates with the findings of representation analysis (Belinkov et al., 2017a,b;Tenney et al., 2019;Liu et al., 2019) in demonstrating that the networks follow linguistic hierarchy.
4. Linguistic neurons are distributed across the network based on their complexity, with lower layers focused on the lexical concepts and middle and higher layers learning global phenomenon based on long-range contextual dependencies.
5. While the networks preserve linguistic hierarchy, many authors showed that information is not discretely preserved, but is rather distributed and redundantly present in the network.
6. It was also shown that a small optimal subset of neurons w.r.t any concept can be extracted from a network.
7. On another dimension, a few works showed that some concepts are localized to fewer neurons while others are distributed to a large group.
8. Finally, some interesting cross architectural analyses were drawn based on how the neurons are distributed within their layers.",1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.875,0.125,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s28,p28.0,"Once we have identified neurons that capture a certain concept learned in a model, these can be utilized for controlling the model's behavior w.r.t to that concept. Bau et al. (2019) identified Switch Neurons in NMT models that activate positively for the present-tense verbs and negatively for the past-tense verbs. By manipulating the values of these neurons, they were able to successfully change output translations from present to past tense during inference. The authors additionally found neurons that capture gender and number agreement concepts and manipulated them to control the system's output. Another effort along this line was carried by Suau et al. (2020) Controlling model's behavior using neurons en-ables on-the-fly manipulation of output, for example it can be used to debias the output of the model against sensitive attributes like race and gender.",[None],[True],1,"1. Once we have identified neurons that capture a certain concept learned in a model, these can be utilized for controlling the model's behavior w.r.t to that concept.
2. Bau et al. (2019) identified Switch Neurons in NMT models that activate positively for the present-tense verbs and negatively for the past-tense verbs.
3. By manipulating the values of these neurons, they were able to successfully change output translations from present to past tense during inference.
4. The authors additionally found neurons that capture gender and number agreement concepts and manipulated them to control the system's output.
5. Another effort along this line was carried by Suau et al. (2020) Controlling model's behavior using neurons en-ables on-the-fly manipulation of output, for example it can be used to debias the output of the model against sensitive attributes like race and gender.","Once we have identified neurons that capture a certain concept learned in a model, these can be utilized for controlling the model's behavior w.r.t to that concept.",How can identified neurons control a model's behavior regarding specific learned concepts?,"Question: How can identified neurons control a model's behavior regarding specific learned concepts?

1. Once we have identified neurons that capture a certain concept learned in a model, these can be utilized for controlling the model's behavior w.r.t to that concept.
2. Bau et al. (2019) identified Switch Neurons in NMT models that activate positively for the present-tense verbs and negatively for the past-tense verbs.
3. By manipulating the values of these neurons, they were able to successfully change output translations from present to past tense during inference.
4. The authors additionally found neurons that capture gender and number agreement concepts and manipulated them to control the system's output.
5. Another effort along this line was carried by Suau et al. (2020) Controlling model's behavior using neurons en-ables on-the-fly manipulation of output, for example it can be used to debias the output of the model against sensitive attributes like race and gender.",1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.8,0.1999999999999999,1.0
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s31,p31.0,"Knowing the association of a neuron with a concept enables explanation of model's output. Mu and Andreas (2020) identified neurons that learn certain concepts in vision and NLP models. Using a composition of logical operators, they provided an explanation of model's prediction. Figure 3 presents an explanation using a gender-sensitive neuron. The neuron activates for contradiction when the premise contains the word man. Such explanations provide a way to generate adversarial examples that change model's predictions.",['b32'],[True],1,"1. Knowing the association of a neuron with a concept enables explanation of model's output.
2. Mu and Andreas (2020) identified neurons that learn certain concepts in vision and NLP models.
3. Using a composition of logical operators, they provided an explanation of model's prediction.
4. Figure 3 presents an explanation using a gender-sensitive neuron.
5. The neuron activates for contradiction when the premise contains the word man.
6. Such explanations provide a way to generate adversarial examples that change model's predictions.",Knowing the association of a neuron with a concept enables explanation of model's output.,How do neurons linked to concepts help explain AI model outputs?,"Question: How do neurons linked to concepts help explain AI model outputs?

1. Knowing the association of a neuron with a concept enables explanation of model's output.
2. Mu and Andreas (2020) identified neurons that learn certain concepts in vision and NLP models.
3. Using a composition of logical operators, they provided an explanation of model's prediction.
4. Figure 3 presents an explanation using a gender-sensitive neuron.
5. The neuron activates for contradiction when the premise contains the word man.
6. Such explanations provide a way to generate adversarial examples that change model's predictions.",1.0,0.0,0.5,1.0,1.0,1.0,0.0,0.8333333333333334,-0.8333333333333334,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s3,p3.0,"As natural language data is readily available and unsupervised training paradigms have been proposed to better utilize extremely large datasets, this motivates the unsupervised learning of natural language. One common approach is to predict masked words in a sentence while considering the surrounding context. This training paradigm is known as the Masked Language Model. This type of training allows the model to develop a deeper understanding of the relationships between words and the context in which they are used. These models are trained on a large corpus of texts using techniques such as the Transformer architecture and have achieved state-of-the-art results in many NLP tasks, such as sentiment analysis and named entity recognition. Notable examples of Masked Language Models include BERT [28], RoBERTa [65], and T5 [84]. MLMs have become an important tool in the field of natural language processing due to their success in a wide range of tasks.","['b62', 'b25', 'b81']","[True, True, True]",3,"1. As natural language data is readily available and unsupervised training paradigms have been proposed to better utilize extremely large datasets, this motivates the unsupervised learning of natural language.
2. One common approach is to predict masked words in a sentence while considering the surrounding context.
3. This training paradigm is known as the Masked Language Model.
4. This type of training allows the model to develop a deeper understanding of the relationships between words and the context in which they are used.
5. These models are trained on a large corpus of texts using techniques such as the Transformer architecture and have achieved state-of-the-art results in many NLP tasks, such as sentiment analysis and named entity recognition.
6. Notable examples of Masked Language Models include BERT [28], RoBERTa [65], and T5 [84].
7. MLMs have become an important tool in the field of natural language processing due to their success in a wide range of tasks.","As natural language data is readily available and unsupervised training paradigms have been proposed to better utilize extremely large datasets, this motivates the unsupervised learning of natural language.",What motivates the unsupervised learning of natural language using Masked Language Models?,"Question: What motivates the unsupervised learning of natural language using Masked Language Models?

1. As natural language data is readily available and unsupervised training paradigms have been proposed to better utilize extremely large datasets, this motivates the unsupervised learning of natural language.
2. One common approach is to predict masked words in a sentence while considering the surrounding context.
3. This training paradigm is known as the Masked Language Model.
4. This type of training allows the model to develop a deeper understanding of the relationships between words and the context in which they are used.
5. These models are trained on a large corpus of texts using techniques such as the Transformer architecture and have achieved state-of-the-art results in many NLP tasks, such as sentiment analysis and named entity recognition.
6. Notable examples of Masked Language Models include BERT [28], RoBERTa [65], and T5 [84].
7. MLMs have become an important tool in the field of natural language processing due to their success in a wide range of tasks.",1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s4,p4.0,"Although language models are typically task-agnostic in architecture, these methods require fine-tuning on datasets of the specific downstream task. Researchers found that scaling up language models significantly improves the few-shot, even zero-shot performance [16]. The most successful models for better few-shot and zero-show performance are Autoregressive Language Models, which are trained by generating the next word in a sequence given the preceding words. These models have been widely used for downstream tasks such as text generation and question answering.",['b13'],[True],1,"1. Although language models are typically task-agnostic in architecture, these methods require fine-tuning on datasets of the specific downstream task.
2. Researchers found that scaling up language models significantly improves the few-shot, even zero-shot performance [16].
3. The most successful models for better few-shot and zero-show performance are Autoregressive Language Models, which are trained by generating the next word in a sequence given the preceding words.
4. These models have been widely used for downstream tasks such as text generation and question answering.","Although language models are typically task-agnostic in architecture, these methods require fine-tuning on datasets of the specific downstream task.",How do Autoregressive Language Models improve few-shot and zero-shot performance?,"Question: How do Autoregressive Language Models improve few-shot and zero-shot performance?

1. Although language models are typically task-agnostic in architecture, these methods require fine-tuning on datasets of the specific downstream task.
2. Researchers found that scaling up language models significantly improves the few-shot, even zero-shot performance [16].
3. The most successful models for better few-shot and zero-show performance are Autoregressive Language Models, which are trained by generating the next word in a sequence given the preceding words.
4. These models have been widely used for downstream tasks such as text generation and question answering.",1.0,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s4,p4.1,"Examples of Autoregressive Language Models include GPT-3 [16], OPT [126], PaLM [22], and BLOOM [92]. The game changer, GPT-3, for the first time, demonstrated reasonable few-/zero-shot performance via prompting and in-context learning, thus showing the superiority of autoregressive language models. There are also models such as CodeX [2] that are optimized for specific tasks such as code generation, BloombergGPT [117] for the financial domain. The recent breakthrough is ChatGPT, which refines GPT-3 specifically for conversational tasks, resulting in more interactive, coherent, and context-aware conversational for various real-world applications.","['b123', 'b89', None, 'b13', 'b19', 'b114']","[True, True, True, True, True, True]",6,"1. Examples of Autoregressive Language Models include GPT-3 [16], OPT [126], PaLM [22], and BLOOM [92].
2. The game changer, GPT-3, for the first time, demonstrated reasonable few-/zero-shot performance via prompting and in-context learning, thus showing the superiority of autoregressive language models.
3. There are also models such as CodeX [2] that are optimized for specific tasks such as code generation, BloombergGPT [117] for the financial domain.
4. The recent breakthrough is ChatGPT, which refines GPT-3 specifically for conversational tasks, resulting in more interactive, coherent, and context-aware conversational for various real-world applications.","Examples of Autoregressive Language Models include GPT-3 [16], OPT [126], PaLM [22], and BLOOM [92].",What are examples and advancements of autoregressive language models?,"Question: What are examples and advancements of autoregressive language models?

1. Examples of Autoregressive Language Models include GPT-3 [16], OPT [126], PaLM [22], and BLOOM [92].
2. The game changer, GPT-3, for the first time, demonstrated reasonable few-/zero-shot performance via prompting and in-context learning, thus showing the superiority of autoregressive language models.
3. There are also models such as CodeX [2] that are optimized for specific tasks such as code generation, BloombergGPT [117] for the financial domain.
4. The recent breakthrough is ChatGPT, which refines GPT-3 specifically for conversational tasks, resulting in more interactive, coherent, and context-aware conversational for various real-world applications.",0.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s7,p7.0,"Pre-training data plays a pivotal role in the development of large language models. As the foundation of remarkable capabilities [5,47] of LLMs, the quality, quantitative, and diversity of pre-training data influence the performance of LLMs significantly [124]. The commonly used pretraining data consists of a myriad of text sources, including books, articles, and websites. The data is carefully curated to ensure a comprehensive representation of human knowledge, linguistic nuances, and cultural perspectives. The importance of pretraining data lies in its capacity to inform the language model with a rich understanding of word knowledge, grammar, syntax, and semantics, as well as the ability to recognize context and generate coherent responses. The diversity of pretraining data also plays a crucial role in shaping the model's performance, and the selection of LLMs highly depends on the components of the pretraining data. For example, PaLM [22] and BLOOM [92] excel in multilingual tasks and machine translation with an abundance of multilingual pretraining data. Moreover, PaLM's performance in Question Answering tasks is enhanced by incorporating a considerable amount of social media conversations and Books corpus [22]. Likewise, code execution and code completion capabilities of GPT-3.5 (code-davinci-002) are amplified by the integration of code data in its pretraining dataset. In brief, when selecting LLMs for downstream tasks, it is advisable to choose the model pre-trained on a similar field of data.","['b2', 'b121', 'b44', 'b89', 'b19']","[True, True, True, True, True]",5,"1. Pre-training data plays a pivotal role in the development of large language models.
2. As the foundation of remarkable capabilities [5,47] of LLMs, the quality, quantitative, and diversity of pre-training data influence the performance of LLMs significantly [124].
3. The commonly used pretraining data consists of a myriad of text sources, including books, articles, and websites.
4. The data is carefully curated to ensure a comprehensive representation of human knowledge, linguistic nuances, and cultural perspectives.
5. The importance of pretraining data lies in its capacity to inform the language model with a rich understanding of word knowledge, grammar, syntax, and semantics, as well as the ability to recognize context and generate coherent responses.
6. The diversity of pretraining data also plays a crucial role in shaping the model's performance, and the selection of LLMs highly depends on the components of the pretraining data.
7. For example, PaLM [22] and BLOOM [92] excel in multilingual tasks and machine translation with an abundance of multilingual pretraining data.
8. Moreover, PaLM's performance in Question Answering tasks is enhanced by incorporating a considerable amount of social media conversations and Books corpus [22].
9. Likewise, code execution and code completion capabilities of GPT-3.5 (code-davinci-002) are amplified by the integration of code data in its pretraining dataset.
10. In brief, when selecting LLMs for downstream tasks, it is advisable to choose the model pre-trained on a similar field of data.",Pre-training data plays a pivotal role in the development of large language models.,How does pre-training data influence large language model performance?,"Question: How does pre-training data influence large language model performance?

1. Pre-training data plays a pivotal role in the development of large language models.
2. As the foundation of remarkable capabilities [5,47] of LLMs, the quality, quantitative, and diversity of pre-training data influence the performance of LLMs significantly [124].
3. The commonly used pretraining data consists of a myriad of text sources, including books, articles, and websites.
4. The data is carefully curated to ensure a comprehensive representation of human knowledge, linguistic nuances, and cultural perspectives.
5. The importance of pretraining data lies in its capacity to inform the language model with a rich understanding of word knowledge, grammar, syntax, and semantics, as well as the ability to recognize context and generate coherent responses.
6. The diversity of pretraining data also plays a crucial role in shaping the model's performance, and the selection of LLMs highly depends on the components of the pretraining data.
7. For example, PaLM [22] and BLOOM [92] excel in multilingual tasks and machine translation with an abundance of multilingual pretraining data.
8. Moreover, PaLM's performance in Question Answering tasks is enhanced by incorporating a considerable amount of social media conversations and Books corpus [22].
9. Likewise, code execution and code completion capabilities of GPT-3.5 (code-davinci-002) are amplified by the integration of code data in its pretraining dataset.
10. In brief, when selecting LLMs for downstream tasks, it is advisable to choose the model pre-trained on a similar field of data.",1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s8,p8.0,"When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant. In this section, we provide a succinct overview of the appropriate models to employ for each scenario. Zero annotated data: In scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach. LLMs have been shown to outperform previous zero-shot methods [120]. Additionally, the absence of a parameter update process ensures that catastrophic forgetting [49] is avoided since the language model parameters remain unaltered.","['b46', 'b117']","[True, True]",2,"1. When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant.
2. In this section, we provide a succinct overview of the appropriate models to employ for each scenario.
3. Zero annotated data: In scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach.
4. LLMs have been shown to outperform previous zero-shot methods [120].
5. Additionally, the absence of a parameter update process ensures that catastrophic forgetting [49] is avoided since the language model parameters remain unaltered.","When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant.",What are the best model deployment strategies based on annotated data availability?,"Question: What are the best model deployment strategies based on annotated data availability?

1. When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant.
2. In this section, we provide a succinct overview of the appropriate models to employ for each scenario.
3. Zero annotated data: In scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach.
4. LLMs have been shown to outperform previous zero-shot methods [120].
5. Additionally, the absence of a parameter update process ensures that catastrophic forgetting [49] is avoided since the language model parameters remain unaltered.",1.0,1.0,0.5,1.0,1.0,1.0,0.0,0.8,0.1999999999999999,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s9,p9.0,"When deploying LLMs for downstream tasks, we often face challenges stemming from distributional differences between the test/user data and that of the training data. These disparities may encompass domain shifts [132], out-of-distribution variations [31], or even adversarial examples [82]. Such challenges significantly hinder fine-tuned modes' effectiveness in real-world applications. They fit into a specific distribution and have a poor ability to generalize to OOD data.","['b28', 'b79', 'b129']","[True, True, True]",3,"1. When deploying LLMs for downstream tasks, we often face challenges stemming from distributional differences between the test/user data and that of the training data.
2. These disparities may encompass domain shifts [132], out-of-distribution variations [31], or even adversarial examples [82].
3. Such challenges significantly hinder fine-tuned modes' effectiveness in real-world applications.
4. They fit into a specific distribution and have a poor ability to generalize to OOD data.","When deploying LLMs for downstream tasks, we often face challenges stemming from distributional differences between the test/user data and that of the training data.",What challenges do LLMs face when applied to downstream tasks due to distributional differences?,"Question: What challenges do LLMs face when applied to downstream tasks due to distributional differences?

1. When deploying LLMs for downstream tasks, we often face challenges stemming from distributional differences between the test/user data and that of the training data.
2. These disparities may encompass domain shifts [132], out-of-distribution variations [31], or even adversarial examples [82].
3. Such challenges significantly hinder fine-tuned modes' effectiveness in real-world applications.
4. They fit into a specific distribution and have a poor ability to generalize to OOD data.",1.0,1.0,0.5,1.0,1.0,1.0,0.0,0.75,0.25,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s13,p13.2,"CivilComments [13] even the best one is only better than random guessing [59]. On the other hand, most popular fine-tuned models can obtain much better performance [33]. and the Perspective API 3 is still one of the best for detecting toxicity. This API is powered by a multilingual BERT-based model, which is tuned on publicly available toxicity data and several smaller single-language CNNs distilled from this model. This might be due to the fact that toxicity is defined by subtle nuances in linguistic expressions, and large language models are unable to accurately comprehend this task solely based on the provided input.","['b56', 'b10', 'b30']","[True, True, True]",3,"1. CivilComments [13] even the best one is only better than random guessing [59].
2. On the other hand, most popular fine-tuned models can obtain much better performance [33].
3. and the Perspective API 3 is still one of the best for detecting toxicity.
4. This API is powered by a multilingual BERT-based model, which is tuned on publicly available toxicity data and several smaller single-language CNNs distilled from this model.
5. This might be due to the fact that toxicity is defined by subtle nuances in linguistic expressions, and large language models are unable to accurately comprehend this task solely based on the provided input.",CivilComments [13] even the best one is only better than random guessing [59].,What makes the Perspective API 3 effective in detecting toxicity?,"Question: What makes the Perspective API 3 effective in detecting toxicity?

1. CivilComments [13] even the best one is only better than random guessing [59].
2. On the other hand, most popular fine-tuned models can obtain much better performance [33].
3. and the Perspective API 3 is still one of the best for detecting toxicity.
4. This API is powered by a multilingual BERT-based model, which is tuned on publicly available toxicity data and several smaller single-language CNNs distilled from this model.
5. This might be due to the fact that toxicity is defined by subtle nuances in linguistic expressions, and large language models are unable to accurately comprehend this task solely based on the provided input.",0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.8,0.1999999999999999,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s13,p13.4,"In information retrieval (IR) tasks, LLMs are not widely exploited yet. One major reason is that IR tasks are fundamentally different from others. There's no natural way to transform the thousands of candidate texts into a few/zero-shot form which is required by LLMs. The existing evaluation results on MS MARCO(regular/TREC) [73] show that methods based on fine-tuned models have better performance [59]. In this evaluation, the LLMs rank passages in an unorthodox way, which requires the LLMs to produce probabilities for passages one by one.","['b56', 'b70']","[True, True]",2,"1. In information retrieval (IR) tasks, LLMs are not widely exploited yet.
2. One major reason is that IR tasks are fundamentally different from others.
3. There's no natural way to transform the thousands of candidate texts into a few/zero-shot form which is required by LLMs.
4. The existing evaluation results on MS MARCO(regular/TREC) [73] show that methods based on fine-tuned models have better performance [59].
5. In this evaluation, the LLMs rank passages in an unorthodox way, which requires the LLMs to produce probabilities for passages one by one.","In information retrieval (IR) tasks, LLMs are not widely exploited yet.",Why are LLMs not widely used in information retrieval tasks?,"Question: Why are LLMs not widely used in information retrieval tasks?

1. In information retrieval (IR) tasks, LLMs are not widely exploited yet.
2. One major reason is that IR tasks are fundamentally different from others.
3. There's no natural way to transform the thousands of candidate texts into a few/zero-shot form which is required by LLMs.
4. The existing evaluation results on MS MARCO(regular/TREC) [73] show that methods based on fine-tuned models have better performance [59].
5. In this evaluation, the LLMs rank passages in an unorthodox way, which requires the LLMs to produce probabilities for passages one by one.",1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s13,p13.8,"Transforming input from tasks like IR and sentence labeling into a few/zero-short instruction form is non-trivial. There may be better ways to adapt language models to traditional NLP tasks in the future. On the other hand, the upper limit of capabilities of fine-tuned models is not reached, and some methods like FLAN-tuning [67] can further boost the performance on NLU tasks. Another interesting finding is that on NLU tasks, after fine-tuning, masked language models, like T5 [85], are better than most auto-regressive language models at the same scale, while some recent results imply that this gap can be bridged by scaling [22].","['b64', 'b19', 'b82']","[True, True, True]",3,"1. Transforming input from tasks like IR and sentence labeling into a few/zero-short instruction form is non-trivial.
2. There may be better ways to adapt language models to traditional NLP tasks in the future.
3. On the other hand, the upper limit of capabilities of fine-tuned models is not reached, and some methods like FLAN-tuning [67] can further boost the performance on NLU tasks.
4. Another interesting finding is that on NLU tasks, after fine-tuning, masked language models, like T5 [85], are better than most auto-regressive language models at the same scale, while some recent results imply that this gap can be bridged by scaling [22].",Transforming input from tasks like IR and sentence labeling into a few/zero-short instruction form is non-trivial.,What are the challenges and future prospects in adapting language models for NLP tasks?,"Question: What are the challenges and future prospects in adapting language models for NLP tasks?

1. Transforming input from tasks like IR and sentence labeling into a few/zero-short instruction form is non-trivial.
2. There may be better ways to adapt language models to traditional NLP tasks in the future.
3. On the other hand, the upper limit of capabilities of fine-tuned models is not reached, and some methods like FLAN-tuning [67] can further boost the performance on NLU tasks.
4. Another interesting finding is that on NLU tasks, after fine-tuning, masked language models, like T5 [85], are better than most auto-regressive language models at the same scale, while some recent results imply that this gap can be bridged by scaling [22].",0.5,1.0,0.5,0.5,1.0,0.5,0.0,1.0,0.0,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s13,p13.10,"One of the representative tasks is miscellaneous text classification [59]. In contrast to classic domain-specific text classification tasks such as sentiment analysis, miscellaneous text classification deals with a diverse range of topics and categories that may not have a clear or strong relationship with one another. It's closer to real-world cases and hard to be formatted for using fine-tuned models. Another is the Adversarial NLI (ANLI) [74]. It is a difficult dataset composed of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). LLMs have shown superior performance on ANLI, especially on the R3 and R2. Both examples demonstrate the exceptional ability of LLMs to generalize well on out-of-distribution and sparsely annotated data in traditional NLP tasks, surpassing that of fine-tuned models. We've discussed this in the section above 3.3.","['b56', 'b71']","[True, True]",2,"1. One of the representative tasks is miscellaneous text classification [59].
2. In contrast to classic domain-specific text classification tasks such as sentiment analysis, miscellaneous text classification deals with a diverse range of topics and categories that may not have a clear or strong relationship with one another.
3. It's closer to real-world cases and hard to be formatted for using fine-tuned models.
4. Another is the Adversarial NLI (ANLI)[74].
5. It is a difficult dataset composed of adversarially mined natural language inference questions in three rounds (R1, R2, and R3).
6. LLMs have shown superior performance on ANLI, especially on the R3 and R2.
7. Both examples demonstrate the exceptional ability of LLMs to generalize well on out-of-distribution and sparsely annotated data in traditional NLP tasks, surpassing that of fine-tuned models.
8. We've discussed this in the section above 3.3.",One of the representative tasks is miscellaneous text classification [59].,What are examples of tasks showcasing LLMs' generalization ability in NLP?,"Question: What are examples of tasks showcasing LLMs' generalization ability in NLP?

1. One of the representative tasks is miscellaneous text classification [59].
2. In contrast to classic domain-specific text classification tasks such as sentiment analysis, miscellaneous text classification deals with a diverse range of topics and categories that may not have a clear or strong relationship with one another.
3. It's closer to real-world cases and hard to be formatted for using fine-tuned models.
4. Another is the Adversarial NLI (ANLI)[74].
5. It is a difficult dataset composed of adversarially mined natural language inference questions in three rounds (R1, R2, and R3).
6. LLMs have shown superior performance on ANLI, especially on the R3 and R2.
7. Both examples demonstrate the exceptional ability of LLMs to generalize well on out-of-distribution and sparsely annotated data in traditional NLP tasks, surpassing that of fine-tuned models.
8. We've discussed this in the section above 3.3.",0.0,0.0,0.0,0.5,1.0,1.0,0.5,0.875,-0.875,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s15,p15.4,"In machine translation (MT), LLMs can perform competent translation, although the average performance is slightly worse than some commercial translation tools [45] considering some automatic metrics like BLEU [78]. LLMs are particularly good at translating some low-resource language texts to English texts, such as in the Romanian-English translation of WMT'16 [11], zero-shot or few-shot LLMs can perform better than SOTA fine-tuned model [22]. This is mainly due to the fact that English resources compose the main part of the pre-training data. BLOOM [92] is pre-trained on more multi-lingual data, leading to better translation quality in both rich-resource and low-resource translation.","['b89', 'b19', 'b42', 'b8', 'b75']","[True, True, True, True, True]",5,"1. In machine translation (MT), LLMs can perform competent translation, although the average performance is slightly worse than some commercial translation tools [45] considering some automatic metrics like BLEU [78].
2. LLMs are particularly good at translating some low-resource language texts to English texts, such as in the Romanian-English translation of WMT'16 [11], zero-shot or few-shot LLMs can perform better than SOTA fine-tuned model [22].
3. This is mainly due to the fact that English resources compose the main part of the pre-training data.
4. BLOOM [92] is pre-trained on more multi-lingual data, leading to better translation quality in both rich-resource and low-resource translation.","In machine translation (MT), LLMs can perform competent translation, although the average performance is slightly worse than some commercial translation tools [45] considering some automatic metrics like BLEU [78].",How do LLMs compare to commercial tools in machine translation performance?,"Question: How do LLMs compare to commercial tools in machine translation performance?

1. In machine translation (MT), LLMs can perform competent translation, although the average performance is slightly worse than some commercial translation tools [45] considering some automatic metrics like BLEU [78].
2. LLMs are particularly good at translating some low-resource language texts to English texts, such as in the Romanian-English translation of WMT'16 [11], zero-shot or few-shot LLMs can perform better than SOTA fine-tuned model [22].
3. This is mainly due to the fact that English resources compose the main part of the pre-training data.
4. BLOOM [92] is pre-trained on more multi-lingual data, leading to better translation quality in both rich-resource and low-resource translation.",0.5,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s15,p15.6,"Additionally, LLMs are highly skilled in open-ended generations. One example is that the news articles generated by LLMs are almost indistinguishable from real news articles by humans [16]. LLMs are remarkably adept at code synthesis as well. Either for text-code generation, such as HumanEval [18] and MBPP [7], or for code repairing, such as DeepFix [39], LLMs can perform pretty well. GPT-4 can even pass 25% problems in Leetcode, which are not trivial for most human coders [76]. With training on more code data, the coding capability of LLMs can be improved further [22].","['b4', 'b36', 'b13', 'b15', 'b19', 'b73']","[True, True, True, True, True, True]",6,"1. Additionally, LLMs are highly skilled in open-ended generations.
2. One example is that the news articles generated by LLMs are almost indistinguishable from real news articles by humans [16].
3. LLMs are remarkably adept at code synthesis as well.
4. Either for text-code generation, such as HumanEval [18] and MBPP [7], or for code repairing, such as DeepFix [39], LLMs can perform pretty well.
5. GPT-4 can even pass 25% problems in Leetcode, which are not trivial for most human coders [76].
6. With training on more code data, the coding capability of LLMs can be improved further [22].","Additionally, LLMs are highly skilled in open-ended generations.",What are the capabilities of LLMs in open-ended generation and code-related tasks?,"Question: What are the capabilities of LLMs in open-ended generation and code-related tasks?

1. Additionally, LLMs are highly skilled in open-ended generations.
2. One example is that the news articles generated by LLMs are almost indistinguishable from real news articles by humans [16].
3. LLMs are remarkably adept at code synthesis as well.
4. Either for text-code generation, such as HumanEval [18] and MBPP [7], or for code repairing, such as DeepFix [39], LLMs can perform pretty well.
5. GPT-4 can even pass 25% problems in Leetcode, which are not trivial for most human coders [76].
6. With training on more code data, the coding capability of LLMs can be improved further [22].",0.5,1.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s18,p18.3,"Closed-book question-answering tasks require the model to answer a given question about factual knowledge without any external information. It does require the memorization of real-world knowledge in the model. LLMs perform better on nearly all datasets, such as on NaturalQuestions [52], WebQuestions [9], and TriviaQA [46]. On TriviaQA, even zero-shot LLMs is still much better [22].","['b43', 'b19', 'b6', 'b49']","[True, True, True, True]",4,"1. Closed-book question-answering tasks require the model to answer a given question about factual knowledge without any external information.
2. It does require the memorization of real-world knowledge in the model.
3. LLMs perform better on nearly all datasets, such as on NaturalQuestions [52], WebQuestions [9], and TriviaQA[46].
4. On TriviaQA, even zero-shot LLMs is still much better [22].",Closed-book question-answering tasks require the model to answer a given question about factual knowledge without any external information.,What are the advantages of LLMs in closed-book question-answering tasks?,"Question: What are the advantages of LLMs in closed-book question-answering tasks?

1. Closed-book question-answering tasks require the model to answer a given question about factual knowledge without any external information.
2. It does require the memorization of real-world knowledge in the model.
3. LLMs perform better on nearly all datasets, such as on NaturalQuestions [52], WebQuestions [9], and TriviaQA[46].
4. On TriviaQA, even zero-shot LLMs is still much better [22].",1.0,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s18,p18.4,"The massive multitask language understanding (MMLU) [40] is also highly knowledge-intensive. Some tasks only require the model to capture the self-contained knowledge in the contexts. The knowledge in the contexts from the input is enough for the model to make predictions. For these tasks, small fine-tuned models can work pretty well. One such task is machine reading comprehension (MRC). An MRC task provides several paragraphs and requires the model to predict the answer to questions based on these paragraphs. We've discussed MRC in the previous section because it's also a traditional NLU task.",['b37'],[True],1,"1. The massive multitask language understanding (MMLU) [40] is also highly knowledge-intensive.
2. Some tasks only require the model to capture the self-contained knowledge in the contexts.
3. The knowledge in the contexts from the input is enough for the model to make predictions.
4. For these tasks, small fine-tuned models can work pretty well.
5. One such task is machine reading comprehension (MRC).
6. An MRC task provides several paragraphs and requires the model to predict the answer to questions based on these paragraphs.
7. We've discussed MRC in the previous section because it's also a traditional NLU task.",The massive multitask language understanding (MMLU) [40] is also highly knowledge-intensive.,What makes machine reading comprehension a suitable task for small fine-tuned models?,"Question: What makes machine reading comprehension a suitable task for small fine-tuned models?

1. The massive multitask language understanding (MMLU) [40] is also highly knowledge-intensive.
2. Some tasks only require the model to capture the self-contained knowledge in the contexts.
3. The knowledge in the contexts from the input is enough for the model to make predictions.
4. For these tasks, small fine-tuned models can work pretty well.
5. One such task is machine reading comprehension (MRC).
6. An MRC task provides several paragraphs and requires the model to predict the answer to questions based on these paragraphs.
7. We've discussed MRC in the previous section because it's also a traditional NLU task.",0.0,0.5,0.0,1.0,1.0,1.0,0.0,0.8571428571428571,-0.3571428571428571,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s18,p18.5,"Another scenario is that the knowledge within LLMs about real world is useless to the task, or even the required knowledge is counterfactual to the real world. As a result, the LLMs cannot work well on such tasks. In some cases, inconsistent knowledge may even make the LLMs worse than random guessing. For example, in Big-Bench, the Mnist ascii task requires the model to tell the digit represented by an ASCII art. The capability required by this task is nothing about real-world knowledge. Also, in the Inverse Scaling Phenomenon competition [70], the task redefine math redefines a common symbol and requires the model to choose between the original meaning and the meaning derived from the redefinition. What it requires contrasts to the LLMs' knowledge, thus LLMs even perform worse than random guessing.",['b67'],[True],1,"1. Another scenario is that the knowledge within LLMs about real world is useless to the task, or even the required knowledge is counterfactual to the real world.
2. As a result, the LLMs cannot work well on such tasks.
3. In some cases, inconsistent knowledge may even make the LLMs worse than random guessing.
4. For example, in Big-Bench, the Mnist ascii task requires the model to tell the digit represented by an ASCII art.
5. The capability required by this task is nothing about real-world knowledge.
6. Also, in the Inverse Scaling Phenomenon competition [70], the task redefine math redefines a common symbol and requires the model to choose between the original meaning and the meaning derived from the redefinition.
7. What it requires contrasts to the LLMs' knowledge, thus LLMs even perform worse than random guessing.","Another scenario is that the knowledge within LLMs about real world is useless to the task, or even the required knowledge is counterfactual to the real world.",Why might LLMs perform poorly on tasks unrelated to real-world knowledge?,"Question: Why might LLMs perform poorly on tasks unrelated to real-world knowledge?

1. Another scenario is that the knowledge within LLMs about real world is useless to the task, or even the required knowledge is counterfactual to the real world.
2. As a result, the LLMs cannot work well on such tasks.
3. In some cases, inconsistent knowledge may even make the LLMs worse than random guessing.
4. For example, in Big-Bench, the Mnist ascii task requires the model to tell the digit represented by an ASCII art.
5. The capability required by this task is nothing about real-world knowledge.
6. Also, in the Inverse Scaling Phenomenon competition [70], the task redefine math redefines a common symbol and requires the model to choose between the original meaning and the meaning derived from the redefinition.
7. What it requires contrasts to the LLMs' knowledge, thus LLMs even perform worse than random guessing.",0.5,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s19,p19.0,"Scaling of LLMs (e.g. parameters, training computation, etc.) can greatly empower pretrained language models. With the model scaling up, a model generally becomes more capable in a range of tasks. Reflected in some metrics, the performance shows a power-law relationship with the model scale. For example, the cross-entropy loss which is used to measure the performance for language modeling decreases linearly with the exponential increase in the model scale, which is also called 'scaling-law' [41,47]. For some crucial abilities, such as reasoning, scaling the model has gradually transformed these abilities from a very low state to a usable state, and even approaching human capabilities. In this section, we provide an overview of the usage of LLMs in terms of the abilities and behaviors of LLMs along with scaling.","['b44', 'b38']","[True, True]",2,"1. Scaling of LLMs (e.g. parameters, training computation, etc.) can greatly empower pretrained language models.
2. With the model scaling up, a model generally becomes more capable in a range of tasks.
3. Reflected in some metrics, the performance shows a power-law relationship with the model scale.
4. For example, the cross-entropy loss which is used to measure the performance for language modeling decreases linearly with the exponential increase in the model scale, which is also called 'scaling-law' [41,47].
5. For some crucial abilities, such as reasoning, scaling the model has gradually transformed these abilities from a very low state to a usable state, and even approaching human capabilities.
6. In this section, we provide an overview of the usage of LLMs in terms of the abilities and behaviors of LLMs along with scaling.","Scaling of LLMs (e.g. parameters, training computation, etc.) can greatly empower pretrained language models.",How does scaling affect the performance and abilities of large language models (LLMs)?,"Question: How does scaling affect the performance and abilities of large language models (LLMs)?

1. Scaling of LLMs (e.g. parameters, training computation, etc.) can greatly empower pretrained language models.
2. With the model scaling up, a model generally becomes more capable in a range of tasks.
3. Reflected in some metrics, the performance shows a power-law relationship with the model scale.
4. For example, the cross-entropy loss which is used to measure the performance for language modeling decreases linearly with the exponential increase in the model scale, which is also called 'scaling-law' [41,47].
5. For some crucial abilities, such as reasoning, scaling the model has gradually transformed these abilities from a very low state to a usable state, and even approaching human capabilities.
6. In this section, we provide an overview of the usage of LLMs in terms of the abilities and behaviors of LLMs along with scaling.",1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.8333333333333334,0.1666666666666666,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s22,p22.0,"Scaling of models also endows the model with some unprecedented, fantastic abilities that go beyond the power-law rule. These abilities are called ""emergent ability"". As defined in [113], emergent abilities of LLMs are abilities that are not present in smaller-scale models but are present in large-scale models. This means such abilities cannot be predicted by extrapolating the performance improvements on smaller-scale models and the model suddenly gains good performance on some tasks once the scale exceeds a certain range. The emergent ability is typically unpredictable and surprising, leading to tasks that emerge randomly or unexpectedly. We examine concrete examples of the emergent abilities of LLMs and provide them as an important reference for deciding whether to leverage LLMs' emergent abilities.",['b110'],[True],1,"1. Scaling of models also endows the model with some unprecedented, fantastic abilities that go beyond the power-law rule.
2. These abilities are called ""emergent ability"".
3. As defined in [113], emergent abilities of LLMs are abilities that are not present in smaller-scale models but are present in large-scale models.
4. This means such abilities cannot be predicted by extrapolating the performance improvements on smaller-scale models and the model suddenly gains good performance on some tasks once the scale exceeds a certain range.
5. The emergent ability is typically unpredictable and surprising, leading to tasks that emerge randomly or unexpectedly.
6. We examine concrete examples of the emergent abilities of LLMs and provide them as an important reference for deciding whether to leverage LLMs' emergent abilities.","Scaling of models also endows the model with some unprecedented, fantastic abilities that go beyond the power-law rule.",What are emergent abilities in large language models (LLMs)?,"Question: What are emergent abilities in large language models (LLMs)?

1. Scaling of models also endows the model with some unprecedented, fantastic abilities that go beyond the power-law rule.
2. These abilities are called ""emergent ability"".
3. As defined in [113], emergent abilities of LLMs are abilities that are not present in smaller-scale models but are present in large-scale models.
4. This means such abilities cannot be predicted by extrapolating the performance improvements on smaller-scale models and the model suddenly gains good performance on some tasks once the scale exceeds a certain range.
5. The emergent ability is typically unpredictable and surprising, leading to tasks that emerge randomly or unexpectedly.
6. We examine concrete examples of the emergent abilities of LLMs and provide them as an important reference for deciding whether to leverage LLMs' emergent abilities.",0.5,1.0,0.5,1.0,1.0,1.0,0.0,0.8333333333333334,0.1666666666666666,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s22,p22.2,"For example. GPT-3 [16] shows the emergent ability for word sorting, and word unscrambling tasks. PaLM [22] exhibits the emergent ability on ASCII word recognition 4 and hyperbaton 5 task. The logical abilities of language models tend to emerge as the model scales up, such as logical deduction, logical sequence, and logic grid puzzles. Additionally, other tasks, such as advanced coding (e.g., auto debugging, code line description), and concept understanding (e.g., novel concepts, simple Turing concepts), are also use cases with the emergent abilities of large language models.","['b19', 'b13']","[True, True]",2,"1. For example. GPT-3 [16] shows the emergent ability for word sorting, and word unscrambling tasks.
2. PaLM [22] exhibits the emergent ability on ASCII word recognition 4 and hyperbaton 5 task.
3. The logical abilities of language models tend to emerge as the model scales up, such as logical deduction, logical sequence, and logic grid puzzles.
4. Additionally, other tasks, such as advanced coding (e.g., auto debugging, code line description), and concept understanding (e.g., novel concepts, simple Turing concepts), are also use cases with the emergent abilities of large language models.","For example. GPT-3 [16] shows the emergent ability for word sorting, and word unscrambling tasks.",What emergent abilities do large language models exhibit as they scale up?,"Question: What emergent abilities do large language models exhibit as they scale up?

1. For example. GPT-3 [16] shows the emergent ability for word sorting, and word unscrambling tasks.
2. PaLM [22] exhibits the emergent ability on ASCII word recognition 4 and hyperbaton 5 task.
3. The logical abilities of language models tend to emerge as the model scales up, such as logical deduction, logical sequence, and logic grid puzzles.
4. Additionally, other tasks, such as advanced coding (e.g., auto debugging, code line description), and concept understanding (e.g., novel concepts, simple Turing concepts), are also use cases with the emergent abilities of large language models.",0.0,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s23,p23.3,"Gaining a deeper understanding of emergent abilities, inverse scaling phenomenon and U-shape phenomenon in LLMs is essential for advancing research in this field. In a certain sense, the U-shape phenomenon suggests that small-scale models and huge-scale models make predictions with different internal mechanisms. From this perspective, the U-shape phenomenon can be seen as a transformation of the inverse-scaling phenomenon due to some emergent abilities from sufficiently large models [114]. GPT-4 [76] exhibits a reversal of the inverse scaling phenomenon in some cases, such as on a task called Hindsight Neglect. The explanation for these behaviors of LLMs during scaling is still an open problem. Several hypotheses have been proposed. For emergent abilities, one explanation is that there may be multiple key steps for a task and the LLM cannot handle this task until it's large enough to handle every step, and another explanation is focused on the granularity of evaluation metrics [113]. For inverse-scaling phenomenon and u-shape phenomenon, the explanations mainly focus on the model's over-reliance on information from its prior rather than the input prompts, valid but misleading few-shot examples, and distracting easier tasks within a hard task [114].","['b110', 'b111', 'b73']","[True, True, True]",3,"1. Gaining a deeper understanding of emergent abilities, inverse scaling phenomenon and U-shape phenomenon in LLMs is essential for advancing research in this field.
2. In a certain sense, the U-shape phenomenon suggests that small-scale models and huge-scale models make predictions with different internal mechanisms.
3. From this perspective, the U-shape phenomenon can be seen as a transformation of the inverse-scaling phenomenon due to some emergent abilities from sufficiently large models [114].
4. GPT-4 [76] exhibits a reversal of the inverse scaling phenomenon in some cases, such as on a task called Hindsight Neglect.
5. The explanation for these behaviors of LLMs during scaling is still an open problem.
6. Several hypotheses have been proposed.
7. For emergent abilities, one explanation is that there may be multiple key steps for a task and the LLM cannot handle this task until it's large enough to handle every step, and another explanation is focused on the granularity of evaluation metrics [113].
8. For inverse-scaling phenomenon and u-shape phenomenon, the explanations mainly focus on the model's over-reliance on information from its prior rather than the input prompts, valid but misleading few-shot examples, and distracting easier tasks within a hard task [114].","Gaining a deeper understanding of emergent abilities, inverse scaling phenomenon and U-shape phenomenon in LLMs is essential for advancing research in this field.",What are the key phenomena and their explanations in scaling large language models?,"Question: What are the key phenomena and their explanations in scaling large language models?

1. Gaining a deeper understanding of emergent abilities, inverse scaling phenomenon and U-shape phenomenon in LLMs is essential for advancing research in this field.
2. In a certain sense, the U-shape phenomenon suggests that small-scale models and huge-scale models make predictions with different internal mechanisms.
3. From this perspective, the U-shape phenomenon can be seen as a transformation of the inverse-scaling phenomenon due to some emergent abilities from sufficiently large models [114].
4. GPT-4 [76] exhibits a reversal of the inverse scaling phenomenon in some cases, such as on a task called Hindsight Neglect.
5. The explanation for these behaviors of LLMs during scaling is still an open problem.
6. Several hypotheses have been proposed.
7. For emergent abilities, one explanation is that there may be multiple key steps for a task and the LLM cannot handle this task until it's large enough to handle every step, and another explanation is focused on the granularity of evaluation metrics [113].
8. For inverse-scaling phenomenon and u-shape phenomenon, the explanations mainly focus on the model's over-reliance on information from its prior rather than the input prompts, valid but misleading few-shot examples, and distracting easier tasks within a hard task [114].",1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s26,p26.1,"Although LLMs have achieved remarkable success in various natural language processing tasks, their performance in regression tasks has been less impressive. For example, ChatGPT's performance on the GLUE STS-B dataset, which is a regression task evaluating sentence similarity, is inferior to a fine-tuned RoBERTa performance [130]. The Regression tasks typically involve predicting a continuous value rather than a discrete label, posing unique challenges for LLMs. One primary reason for their subpar performance is the inherent difference between the language modeling objective and the regression task objective. LLMs are designed to predict the next word in a sequence or generate coherent text, with their pre-training focused on capturing linguistic patterns and relationships. Consequently, their internal representations may not be well-suited for modeling continuous numerical outputs. Besides, LLMs have predominantly been trained on text data, focusing on capturing the intricacies of natural language processing. As a result, their performance on multimodal data, which involves handling multiple data types such as text, images, audio, video, actions, and robotics, remains largely unexplored. And fine-tuned multimodal models, like BEiT [110] and PaLI [19], still dominate many tasks such as visual question answering (VQA) and image captioning. Nonetheless, the recently introduced GPT-4 [76] has taken the step in multimodal fusion, but there is still a lack of detailed evaluation of its capabilities.","['b127', 'b107', 'b16', 'b73']","[True, True, True, True]",4,"1. Although LLMs have achieved remarkable success in various natural language processing tasks, their performance in regression tasks has been less impressive.
2. For example, ChatGPT's performance on the GLUE STS-B dataset, which is a regression task evaluating sentence similarity, is inferior to a fine-tuned RoBERTa performance [130].
3. The Regression tasks typically involve predicting a continuous value rather than a discrete label, posing unique challenges for LLMs.
4. One primary reason for their subpar performance is the inherent difference between the language modeling objective and the regression task objective.
5. LLMs are designed to predict the next word in a sequence or generate coherent text, with their pre-training focused on capturing linguistic patterns and relationships.
6. Consequently, their internal representations may not be well-suited for modeling continuous numerical outputs.
7. Besides, LLMs have predominantly been trained on text data, focusing on capturing the intricacies of natural language processing.
8. As a result, their performance on multimodal data, which involves handling multiple data types such as text, images, audio, video, actions, and robotics, remains largely unexplored.
9. And fine-tuned multimodal models, like BEiT [110] and PaLI [19], still dominate many tasks such as visual question answering (VQA) and image captioning.
10. Nonetheless, the recently introduced GPT-4 [76] has taken the step in multimodal fusion, but there is still a lack of detailed evaluation of its capabilities.","Although LLMs have achieved remarkable success in various natural language processing tasks, their performance in regression tasks has been less impressive.",Why do LLMs underperform in regression tasks compared to discrete label prediction tasks?,"Question: Why do LLMs underperform in regression tasks compared to discrete label prediction tasks?

1. Although LLMs have achieved remarkable success in various natural language processing tasks, their performance in regression tasks has been less impressive.
2. For example, ChatGPT's performance on the GLUE STS-B dataset, which is a regression task evaluating sentence similarity, is inferior to a fine-tuned RoBERTa performance [130].
3. The Regression tasks typically involve predicting a continuous value rather than a discrete label, posing unique challenges for LLMs.
4. One primary reason for their subpar performance is the inherent difference between the language modeling objective and the regression task objective.
5. LLMs are designed to predict the next word in a sequence or generate coherent text, with their pre-training focused on capturing linguistic patterns and relationships.
6. Consequently, their internal representations may not be well-suited for modeling continuous numerical outputs.
7. Besides, LLMs have predominantly been trained on text data, focusing on capturing the intricacies of natural language processing.
8. As a result, their performance on multimodal data, which involves handling multiple data types such as text, images, audio, video, actions, and robotics, remains largely unexplored.
9. And fine-tuned multimodal models, like BEiT [110] and PaLI [19], still dominate many tasks such as visual question answering (VQA) and image captioning.
10. Nonetheless, the recently introduced GPT-4 [76] has taken the step in multimodal fusion, but there is still a lack of detailed evaluation of its capabilities.",1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s27,p27.3,"LLMs can also be used for quality assessment on some NLG tasks, such as summarization and translation. On summarization tasks, GPT-4 as an evaluator achieves a higher correlation with humans than other methods with a large margin [64]. Some other evaluators based on LLMs [34,50,64,108] also show good human alignment in more NLG tasks, especially compared with traditional automatic metrics. But the LLM evaluator may have a bias towards the LLM-generated texts [64].","['b61', 'b31', 'b47', 'b105']","[True, True, True, True]",4,"1. LLMs can also be used for quality assessment on some NLG tasks, such as summarization and translation.
2. On summarization tasks, GPT-4 as an evaluator achieves a higher correlation with humans than other methods with a large margin [64].
3. Some other evaluators based on LLMs [34,50,64,108] also show good human alignment in more NLG tasks, especially compared with traditional automatic metrics.
4. But the LLM evaluator may have a bias towards the LLM-generated texts [64].","LLMs can also be used for quality assessment on some NLG tasks, such as summarization and translation.","How do LLMs contribute to NLG task quality assessment, particularly in summarization and translation?","Question: How do LLMs contribute to NLG task quality assessment, particularly in summarization and translation?

1. LLMs can also be used for quality assessment on some NLG tasks, such as summarization and translation.
2. On summarization tasks, GPT-4 as an evaluator achieves a higher correlation with humans than other methods with a large margin [64].
3. Some other evaluators based on LLMs [34,50,64,108] also show good human alignment in more NLG tasks, especially compared with traditional automatic metrics.
4. But the LLM evaluator may have a bias towards the LLM-generated texts [64].",1.0,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s29,p29.2,"Additionally, some mechanics such as instruction tuning [91,112] and human alignment tuning [77] further boost the capabilities of LLMs to better comprehend and follow user instructions. These methods improve the model's ability to generate helpful, harmless, and honest responses while maintaining coherence and consistency [77,91,112]. While both methods can make LLMs better generalize to unseen tasks and instructions, it has been noticed that while human labelers prefer models tuned for human alignment [77] to models tuned with instructions from public NLP tasks, such as FLAN [112] and T0 [91]. The reason may be similar to reasons for fine-tuned models' inferiority: public NLP tasks/datasets are designed for easy and automatic evaluation, and they can only cover a small part of real-world usage.","['b88', 'b74', 'b109']","[True, True, True]",3,"1. Additionally, some mechanics such as instruction tuning [91,112] and human alignment tuning [77] further boost the capabilities of LLMs to better comprehend and follow user instructions.
2. These methods improve the model's ability to generate helpful, harmless, and honest responses while maintaining coherence and consistency [77,91,112].
3. While both methods can make LLMs better generalize to unseen tasks and instructions, it has been noticed that while human labelers prefer models tuned for human alignment [77] to models tuned with instructions from public NLP tasks, such as FLAN [112] and T0 [91].
4. The reason may be similar to reasons for fine-tuned models' inferiority: public NLP tasks/datasets are designed for easy and automatic evaluation, and they can only cover a small part of real-world usage.","Additionally, some mechanics such as instruction tuning [91,112] and human alignment tuning [77] further boost the capabilities of LLMs to better comprehend and follow user instructions.",How do instruction and human alignment tuning enhance LLMs' comprehension and response generation?,"Question: How do instruction and human alignment tuning enhance LLMs' comprehension and response generation?

1. Additionally, some mechanics such as instruction tuning [91,112] and human alignment tuning [77] further boost the capabilities of LLMs to better comprehend and follow user instructions.
2. These methods improve the model's ability to generate helpful, harmless, and honest responses while maintaining coherence and consistency [77,91,112].
3. While both methods can make LLMs better generalize to unseen tasks and instructions, it has been noticed that while human labelers prefer models tuned for human alignment [77] to models tuned with instructions from public NLP tasks, such as FLAN [112] and T0 [91].
4. The reason may be similar to reasons for fine-tuned models' inferiority: public NLP tasks/datasets are designed for easy and automatic evaluation, and they can only cover a small part of real-world usage.",0.5,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s32,p32.2,"Flops, while a T5 11B model only requires 3.30 × 10 22 , which is 10 times less. In addition to these costs, hardware requirements are also substantial. OpenAI has collaborated with Microsoft on a supercomputer hosted in the Microsoft Azure cloud, consisting of 285k CPU cores and 10k high-end GPUs to support the training of large models. For users of the OpenAI API, pricing varies based on the model and usage, with options such as GPT-3.5-turbo charging $0.002 per 1k tokens for chat service. However, for users who require custom models, training costs $0.03 per 1k tokens, while usage costs $0.12 per 1k tokens [4]. Therefore, for users who cannot afford such a large cost, such as small startups, individual users, etc., a small, fine-tuned model is a better and more reasonable choice.",[None],[True],1,"1. Flops, while a T5 11B model only requires 3.30 × 10 22 , which is 10 times less.
2. In addition to these costs, hardware requirements are also substantial.
3. OpenAI has collaborated with Microsoft on a supercomputer hosted in the Microsoft Azure cloud, consisting of 285k CPU cores and 10k high-end GPUs to support the training of large models.
4. For users of the OpenAI API, pricing varies based on the model and usage, with options such as GPT-3.5-turbo charging $0.002 per 1k tokens for chat service.
5. However, for users who require custom models, training costs $0.03 per 1k tokens, while usage costs $0.12 per 1k tokens [4].
6. Therefore, for users who cannot afford such a large cost, such as small startups, individual users, etc., a small, fine-tuned model is a better and more reasonable choice.","Flops, while a T5 11B model only requires 3.30 × 10 22 , which is 10 times less.",What are the computational and financial costs of training large AI models like T5 11B?,"Question: What are the computational and financial costs of training large AI models like T5 11B?

1. Flops, while a T5 11B model only requires 3.30 × 10 22 , which is 10 times less.
2. In addition to these costs, hardware requirements are also substantial.
3. OpenAI has collaborated with Microsoft on a supercomputer hosted in the Microsoft Azure cloud, consisting of 285k CPU cores and 10k high-end GPUs to support the training of large models.
4. For users of the OpenAI API, pricing varies based on the model and usage, with options such as GPT-3.5-turbo charging $0.002 per 1k tokens for chat service.
5. However, for users who require custom models, training costs $0.03 per 1k tokens, while usage costs $0.12 per 1k tokens [4].
6. Therefore, for users who cannot afford such a large cost, such as small startups, individual users, etc., a small, fine-tuned model is a better and more reasonable choice.",0.0,1.0,0.5,1.0,1.0,0.0,-1.0,0.8333333333333334,0.1666666666666666,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s32,p32.4,"Parameter-Efficient Tuning. In practice, we may tune the model on some specific datasets. Parameter-Efficient Tuning (PET) is an efficient technique to tune a small portation of model parameters (or extra parameters) while freezing most parameters of the pre-trained LLMs. The main goal of PEFT is to greatly decrease the computational and storage costs while keeping the performance of the original models. The common techniques for PET are LoRA [42], Prefix Tuning [58], P-Tuning [62,63]. As an illustration, the LoRA method maintains the weights of the pre-trained model and incorporates low-rank matrices into every layer of the Transformer architecture. This approach considerably minimizes the number of parameters that require training for subsequent tasks, thereby increasing overall efficiency.","['b59', 'b55', 'b39', 'b60']","[True, True, True, True]",4,"1. Parameter-Efficient Tuning. In practice, we may tune the model on some specific datasets.
2. Parameter-Efficient Tuning (PET) is an efficient technique to tune a small portation of model parameters (or extra parameters) while freezing most parameters of the pre-trained LLMs.
3. The main goal of PEFT is to greatly decrease the computational and storage costs while keeping the performance of the original models.
4. The common techniques for PET are LoRA [42], Prefix Tuning [58], P-Tuning [62,63].
5. As an illustration, the LoRA method maintains the weights of the pre-trained model and incorporates low-rank matrices into every layer of the Transformer architecture.
6. This approach considerably minimizes the number of parameters that require training for subsequent tasks, thereby increasing overall efficiency.","Parameter-Efficient Tuning. In practice, we may tune the model on some specific datasets.",What is Parameter-Efficient Tuning and its common techniques in model optimization?,"Question: What is Parameter-Efficient Tuning and its common techniques in model optimization?

1. Parameter-Efficient Tuning. In practice, we may tune the model on some specific datasets.
2. Parameter-Efficient Tuning (PET) is an efficient technique to tune a small portation of model parameters (or extra parameters) while freezing most parameters of the pre-trained LLMs.
3. The main goal of PEFT is to greatly decrease the computational and storage costs while keeping the performance of the original models.
4. The common techniques for PET are LoRA [42], Prefix Tuning [58], P-Tuning [62,63].
5. As an illustration, the LoRA method maintains the weights of the pre-trained model and incorporates low-rank matrices into every layer of the Transformer architecture.
6. This approach considerably minimizes the number of parameters that require training for subsequent tasks, thereby increasing overall efficiency.",0.5,1.0,1.0,1.0,1.0,1.0,0.0,0.8333333333333334,0.1666666666666666,1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s33,p33.2,"The models that have high accuracy on the scenario also have good robustness. However, the robustness of the zero-shot becomes worse after being tuned on extra application-specific tasks data [116]. This may due to overfitting, which leads to poor generalizability due to the extremely high complexity of the model and the limited training samples from downstream tasks [43]. In a similar vein, it has been observed that fine-tuning a model can result in significant miscalibrations, owing to over-parameterization [51]. Therefore, fine-tuned models may not be an optimal choice when robustness and calibration are critical considerations. However, human alignment has been found as a potential solution for enhancing model robustness. InstructGPT davinci v2 (175B*) has been shown to outperform other models in terms of robustness. On the other hand, achieving optimal calibration of the model depends on the scenario and adaptation procedure employed.","['b40', 'b113', 'b48']","[True, True, True]",3,"1. The models that have high accuracy on the scenario also have good robustness.
2. However, the robustness of the zero-shot becomes worse after being tuned on extra application-specific tasks data [116].
3. This may due to overfitting, which leads to poor generalizability due to the extremely high complexity of the model and the limited training samples from downstream tasks [43].
4. In a similar vein, it has been observed that fine-tuning a model can result in significant miscalibrations, owing to over-parameterization [51].
5. Therefore, fine-tuned models may not be an optimal choice when robustness and calibration are critical considerations.
6. However, human alignment has been found as a potential solution for enhancing model robustness.
7. InstructGPT davinci v2 (175B*) has been shown to outperform other models in terms of robustness.
8. On the other hand, achieving optimal calibration of the model depends on the scenario and adaptation procedure employed.",The models that have high accuracy on the scenario also have good robustness.,How does fine-tuning affect model robustness and calibration in machine learning?,"Question: How does fine-tuning affect model robustness and calibration in machine learning?

1. The models that have high accuracy on the scenario also have good robustness.
2. However, the robustness of the zero-shot becomes worse after being tuned on extra application-specific tasks data [116].
3. This may due to overfitting, which leads to poor generalizability due to the extremely high complexity of the model and the limited training samples from downstream tasks [43].
4. In a similar vein, it has been observed that fine-tuning a model can result in significant miscalibrations, owing to over-parameterization [51].
5. Therefore, fine-tuned models may not be an optimal choice when robustness and calibration are critical considerations.
6. However, human alignment has been found as a potential solution for enhancing model robustness.
7. InstructGPT davinci v2 (175B*) has been shown to outperform other models in terms of robustness.
8. On the other hand, achieving optimal calibration of the model depends on the scenario and adaptation procedure employed.",1.0,1.0,0.5,0.5,1.0,0.5,0.0,1.0,0.0,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s33,p33.3,"Fairness and Bias. LLMs have been shown to exhibit disparate treatment and impact, perpetuating societal biases and potentially leading to discrimination [10,17]. To ensure fairness and equity for all users, it is crucial to address these issues in the development and deployment of NLP models. Disparities in performance between demographic groups can serve as an indicator of fairness problems. LLMs are particularly susceptible to fairness issues, as significant performance disparities have been observed across demographic categories such as dialect, religion, gender, and race [59]. However, research has shown that aligning models with human instructions can improve LLM performance regardless of their size, with the InstructGPTmodel (davinci v2) exhibiting smaller performance disparities than other LLMs [23].","['b56', 'b14', 'b20', 'b7']","[True, True, True, True]",4,"1. Fairness and Bias. LLMs have been shown to exhibit disparate treatment and impact, perpetuating societal biases and potentially leading to discrimination [10,17].
2. To ensure fairness and equity for all users, it is crucial to address these issues in the development and deployment of NLP models.
3. Disparities in performance between demographic groups can serve as an indicator of fairness problems.
4. LLMs are particularly susceptible to fairness issues, as significant performance disparities have been observed across demographic categories such as dialect, religion, gender, and race [59].
5. However, research has shown that aligning models with human instructions can improve LLM performance regardless of their size, with the InstructGPTmodel (davinci v2) exhibiting smaller performance disparities than other LLMs [23].","Fairness and Bias. LLMs have been shown to exhibit disparate treatment and impact, perpetuating societal biases and potentially leading to discrimination [10,17].",What are the fairness and bias issues in LLMs and how can they be addressed?,"Question: What are the fairness and bias issues in LLMs and how can they be addressed?

1. Fairness and Bias. LLMs have been shown to exhibit disparate treatment and impact, perpetuating societal biases and potentially leading to discrimination [10,17].
2. To ensure fairness and equity for all users, it is crucial to address these issues in the development and deployment of NLP models.
3. Disparities in performance between demographic groups can serve as an indicator of fairness problems.
4. LLMs are particularly susceptible to fairness issues, as significant performance disparities have been observed across demographic categories such as dialect, religion, gender, and race [59].
5. However, research has shown that aligning models with human instructions can improve LLM performance regardless of their size, with the InstructGPTmodel (davinci v2) exhibiting smaller performance disparities than other LLMs [23].",1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s33,p33.4,"Spurious Biases. The shortcut learning problem has been observed in various natural language understanding tasks under the pretraining and fine-tuning paradigm, where models heavily rely on spurious correlations between input and labels in the fine-tuning data for prediction [31,35,98]. For example, in reading comprehension tasks, fine-tuned models tend to focus on the lexical matching of words between the question and the original passage, neglecting the intended reading comprehension task itself [53]. In contrast, large language models are not directly trained on fine-tuned datasets, which makes it less likely for them to learn shortcut features present in the fine-tuned dataset, thereby enhancing the model's generalization capabilities. However, LLMs are not infallible and may exhibit some shortcut learning during in-context learning. For example, recent preliminary studies have begun investigating the robustness of prompt-based methods in large-scale language models [111,129]. One such study evaluates the few-shot learning performance of GPT-3 on text classification and information extraction tasks [129]. and reveal that the examined LLMs are susceptible to majority label bias and position bias, where they tend to predict answers based on the frequency or position of the answers in the training data. Moreover, these LLMs exhibit common token bias, favoring answers that are prevalent in their pre-training corpus. Recent studies show that this positional bias can be mitigated by selecting proper prompts [68]. In summary, while LLMs significantly reduce the shortcut learning problem prevalent in fine-tuned models, they still exhibit some shortcut learning issues and should be approached with caution when deploying them in downstream applications.","['b28', 'b108', 'b65', 'b32', 'b95', 'b126', 'b50']","[True, True, True, True, True, True, True]",7,"1. Spurious Biases. The shortcut learning problem has been observed in various natural language understanding tasks under the pretraining and fine-tuning paradigm, where models heavily rely on spurious correlations between input and labels in the fine-tuning data for prediction [31,35,98].
2. For example, in reading comprehension tasks, fine-tuned models tend to focus on the lexical matching of words between the question and the original passage, neglecting the intended reading comprehension task itself [53].
3. In contrast, large language models are not directly trained on fine-tuned datasets, which makes it less likely for them to learn shortcut features present in the fine-tuned dataset, thereby enhancing the model's generalization capabilities.
4. However, LLMs are not infallible and may exhibit some shortcut learning during in-context learning.
5. For example, recent preliminary studies have begun investigating the robustness of prompt-based methods in large-scale language models [111,129].
6. One such study evaluates the few-shot learning performance of GPT-3 on text classification and information extraction tasks [129].
7. and reveal that the examined LLMs are susceptible to majority label bias and position bias, where they tend to predict answers based on the frequency or position of the answers in the training data.
8. Moreover, these LLMs exhibit common token bias, favoring answers that are prevalent in their pre-training corpus.
9. Recent studies show that this positional bias can be mitigated by selecting proper prompts [68].
10. In summary, while LLMs significantly reduce the shortcut learning problem prevalent in fine-tuned models, they still exhibit some shortcut learning issues and should be approached with caution when deploying them in downstream applications.","Spurious Biases. The shortcut learning problem has been observed in various natural language understanding tasks under the pretraining and fine-tuning paradigm, where models heavily rely on spurious correlations between input and labels in the fine-tuning data for prediction [31,35,98].",What are the challenges of shortcut learning in natural language understanding tasks?,"Question: What are the challenges of shortcut learning in natural language understanding tasks?

1. Spurious Biases. The shortcut learning problem has been observed in various natural language understanding tasks under the pretraining and fine-tuning paradigm, where models heavily rely on spurious correlations between input and labels in the fine-tuning data for prediction [31,35,98].
2. For example, in reading comprehension tasks, fine-tuned models tend to focus on the lexical matching of words between the question and the original passage, neglecting the intended reading comprehension task itself [53].
3. In contrast, large language models are not directly trained on fine-tuned datasets, which makes it less likely for them to learn shortcut features present in the fine-tuned dataset, thereby enhancing the model's generalization capabilities.
4. However, LLMs are not infallible and may exhibit some shortcut learning during in-context learning.
5. For example, recent preliminary studies have begun investigating the robustness of prompt-based methods in large-scale language models [111,129].
6. One such study evaluates the few-shot learning performance of GPT-3 on text classification and information extraction tasks [129].
7. and reveal that the examined LLMs are susceptible to majority label bias and position bias, where they tend to predict answers based on the frequency or position of the answers in the training data.
8. Moreover, these LLMs exhibit common token bias, favoring answers that are prevalent in their pre-training corpus.
9. Recent studies show that this positional bias can be mitigated by selecting proper prompts [68].
10. In summary, while LLMs significantly reduce the shortcut learning problem prevalent in fine-tuned models, they still exhibit some shortcut learning issues and should be approached with caution when deploying them in downstream applications.",1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.9,0.0999999999999999,1.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s34,p34.3,"Harmful content. Due to the high coherence, quality, and plausibility of texts generated by LLMs, harmful contents from LLMs can cause significant harm, including hate speech, discrimination, incitement to violence, false narratives, and even social engineering attack. The implementation of safeguards to detect and correct those contents can be mitigation [97]. These LLMs can also have dual-use potential by providing required illicit information, leading to risks such as the proliferation of weapons [75] and even terrorism attack planning. It is crucial to ensure using these LLMs responsibly, with safeguards in place to prevent harm. Also, in existing work, feedback from humans plays an important role in getting rid of harmful outputs.","['b94', 'b72']","[True, True]",2,"1. Harmful content. Due to the high coherence, quality, and plausibility of texts generated by LLMs, harmful contents from LLMs can cause significant harm, including hate speech, discrimination, incitement to violence, false narratives, and even social engineering attack.
2. The implementation of safeguards to detect and correct those contents can be mitigation [97].
3. These LLMs can also have dual-use potential by providing required illicit information, leading to risks such as the proliferation of weapons [75] and even terrorism attack planning.
4. It is crucial to ensure using these LLMs responsibly, with safeguards in place to prevent harm.
5. Also, in existing work, feedback from humans plays an important role in getting rid of harmful outputs.","Harmful content. Due to the high coherence, quality, and plausibility of texts generated by LLMs, harmful contents from LLMs can cause significant harm, including hate speech, discrimination, incitement to violence, false narratives, and even social engineering attack.",What are the risks and necessary safeguards associated with using large language models (LLMs)?,"Question: What are the risks and necessary safeguards associated with using large language models (LLMs)?

1. Harmful content. Due to the high coherence, quality, and plausibility of texts generated by LLMs, harmful contents from LLMs can cause significant harm, including hate speech, discrimination, incitement to violence, false narratives, and even social engineering attack.
2. The implementation of safeguards to detect and correct those contents can be mitigation [97].
3. These LLMs can also have dual-use potential by providing required illicit information, leading to risks such as the proliferation of weapons [75] and even terrorism attack planning.
4. It is crucial to ensure using these LLMs responsibly, with safeguards in place to prevent harm.
5. Also, in existing work, feedback from humans plays an important role in getting rid of harmful outputs.",1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s34,p34.4,"Privacy. LLMs can face serious security issues. An example is the issue of user privacy. It is reported that Samsung employees were using ChatGPT to process their work when they inadvertently leaked top-secret data, including the source code proper of the new program, internal meeting minutes related to the hardware, etc. The Italian data protection agency declared that OpenAI, the developer of ChatGPT, illicitly gathered personal user data, leading Italy to become the first government to prohibit ChatGPT over privacy concerns [1].",['b0'],[True],1,"1. Privacy. LLMs can face serious security issues.
2. An example is the issue of user privacy.
3. It is reported that Samsung employees were using ChatGPT to process their work when they inadvertently leaked top-secret data, including the source code proper of the new program, internal meeting minutes related to the hardware, etc.
4. The Italian data protection agency declared that OpenAI, the developer of ChatGPT, illicitly gathered personal user data, leading Italy to become the first government to prohibit ChatGPT over privacy concerns [1].",Privacy. LLMs can face serious security issues.,"How do LLMs pose privacy risks, as illustrated by incidents involving Samsung and OpenAI?","Question: How do LLMs pose privacy risks, as illustrated by incidents involving Samsung and OpenAI?

1. Privacy. LLMs can face serious security issues.
2. An example is the issue of user privacy.
3. It is reported that Samsung employees were using ChatGPT to process their work when they inadvertently leaked top-secret data, including the source code proper of the new program, internal meeting minutes related to the hardware, etc.
4. The Italian data protection agency declared that OpenAI, the developer of ChatGPT, illicitly gathered personal user data, leading Italy to become the first government to prohibit ChatGPT over privacy concerns [1].",0.5,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s7,p7.0,"Complicated question is a basic challenge of multi-hop MRC, unlike the single-hop questions, they cannot be answered easily and require complicated reasoning. Since the human reasoning about complex questions is done by decomposition, answering subquestions, summarizing, and comparing [26], then this technique has focused on simplifying the problem by decomposition of a complex question into multiple simple sub-questions. It means it reduces multi-hop MRC to multiple single-hop MRC. This technique mostly uses the single-hop MRC models to find the answers to sub-questions and then combine the answers to obtain the final answer. In the following, the models which use this technique for multi-hop MRC will be reviewed in detail.",['b25'],[True],1,"1. Complicated question is a basic challenge of multi-hop MRC, unlike the single-hop questions, they cannot be answered easily and require complicated reasoning.
2. Since the human reasoning about complex questions is done by decomposition, answering subquestions, summarizing, and comparing [26], then this technique has focused on simplifying the problem by decomposition of a complex question into multiple simple sub-questions.
3. It means it reduces multi-hop MRC to multiple single-hop MRC.
4. This technique mostly uses the single-hop MRC models to find the answers to sub-questions and then combine the answers to obtain the final answer.
5. In the following, the models which use this technique for multi-hop MRC will be reviewed in detail.","Complicated question is a basic challenge of multi-hop MRC, unlike the single-hop questions, they cannot be answered easily and require complicated reasoning.",How does decomposition simplify multi-hop MRC problem-solving?,"Question: How does decomposition simplify multi-hop MRC problem-solving?

1. Complicated question is a basic challenge of multi-hop MRC, unlike the single-hop questions, they cannot be answered easily and require complicated reasoning.
2. Since the human reasoning about complex questions is done by decomposition, answering subquestions, summarizing, and comparing [26], then this technique has focused on simplifying the problem by decomposition of a complex question into multiple simple sub-questions.
3. It means it reduces multi-hop MRC to multiple single-hop MRC.
4. This technique mostly uses the single-hop MRC models to find the answers to sub-questions and then combine the answers to obtain the final answer.
5. In the following, the models which use this technique for multi-hop MRC will be reviewed in detail.",1.0,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,1.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s7,p7.1,"Self-assembling MNM: Jiang and Bansal [18] focused on identifying the sub-questions in the correct reasoning order and presented an interpretable and controller-based self-assembling neural modular network for the multi-hop reasoning process which includes two main parts, Modular Network with a Controller (top) and the Dynamically-assembled Modular Network (bottom) that can be seen in Figure 10. The main idea of the model to handle multi-hop questions is done with Controller that computes an attention distribution over all question words at every reasoning step, which finds the sub-question that should be answered at the current step. In summary, Controller reads the question and predicts a series of modules that could be executed in order to answer the given question. Each module deals with a single-hop sub-question, then they will be chained together according to the predicated order by controller to get the final answer. The mentioned modules are described as follows: All modules take the question representation , context representation ℎ, and sub-question vector as input.",['b17'],[True],1,"1. Self-assembling MNM: Jiang and Bansal [18] focused on identifying the sub-questions in the correct reasoning order and presented an interpretable and controller-based self-assembling neural modular network for the multi-hop reasoning process which includes two main parts, Modular Network with a Controller (top) and the Dynamically-assembled Modular Network (bottom) that can be seen in Figure 10.
2. The main idea of the model to handle multi-hop questions is done with Controller that computes an attention distribution over all question words at every reasoning step, which finds the sub-question that should be answered at the current step.
3. In summary, Controller reads the question and predicts a series of modules that could be executed in order to answer the given question.
4. Each module deals with a single-hop sub-question, then they will be chained together according to the predicated order by controller to get the final answer.
5. The mentioned modules are described as follows: All modules take the question representation , context representation ℎ, and sub-question vector as input.","Self-assembling MNM: Jiang and Bansal [18] focused on identifying the sub-questions in the correct reasoning order and presented an interpretable and controller-based self-assembling neural modular network for the multi-hop reasoning process which includes two main parts, Modular Network with a Controller (top) and the Dynamically-assembled Modular Network (bottom) that can be seen in Figure 10.",How does the self-assembling neural modular network facilitate multi-hop reasoning?,"Question: How does the self-assembling neural modular network facilitate multi-hop reasoning?

1. Self-assembling MNM: Jiang and Bansal [18] focused on identifying the sub-questions in the correct reasoning order and presented an interpretable and controller-based self-assembling neural modular network for the multi-hop reasoning process which includes two main parts, Modular Network with a Controller (top) and the Dynamically-assembled Modular Network (bottom) that can be seen in Figure 10.
2. The main idea of the model to handle multi-hop questions is done with Controller that computes an attention distribution over all question words at every reasoning step, which finds the sub-question that should be answered at the current step.
3. In summary, Controller reads the question and predicts a series of modules that could be executed in order to answer the given question.
4. Each module deals with a single-hop sub-question, then they will be chained together according to the predicated order by controller to get the final answer.
5. The mentioned modules are described as follows: All modules take the question representation , context representation ℎ, and sub-question vector as input.",0.0,0.0,0.5,1.0,1.0,1.0,0.0,0.6,-0.6,0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s10,p10.1,"The main part of this model is Relation Extractor with two different structures: 1) classification-type (CRERC), where the evidence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; 2) span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor can automatically extract multiple corresponding spans from the question text as question relations. Figure 9: Architecture of the RERC models [26] The Decomposition technique was one of the first ideas for multi-hop MRC and as you can see in recent years (2021) still has attracted attention. The main disadvantage of this technique is that, instead of focusing on multi-hop reasoning as an important key of multi-hop MRC, it focuses on reducing the problem to a single-hop MRC. Thus, they actually do not go far beyond single-hop models.",['b25'],[True],1,"1. The main part of this model is Relation Extractor with two different structures: 1) classification-type (CRERC), where the evidence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; 2) span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor can automatically extract multiple corresponding spans from the question text as question relations.
2. Figure 9: Architecture of the RERC models [26]
3. The Decomposition technique was one of the first ideas for multi-hop MRC and as you can see in recent years (2021) still has attracted attention.
4. The main disadvantage of this technique is that, instead of focusing on multi-hop reasoning as an important key of multi-hop MRC, it focuses on reducing the problem to a single-hop MRC.
5. Thus, they actually do not go far beyond single-hop models.","The main part of this model is Relation Extractor with two different structures: 1) classification-type (CRERC), where the evidence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; 2) span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor can automatically extract multiple corresponding spans from the question text as question relations.",What are the structures and limitations of Relation Extractor in multi-hop MRC models?,"Question: What are the structures and limitations of Relation Extractor in multi-hop MRC models?

1. The main part of this model is Relation Extractor with two different structures: 1) classification-type (CRERC), where the evidence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; 2) span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor can automatically extract multiple corresponding spans from the question text as question relations.
2. Figure 9: Architecture of the RERC models [26]
3. The Decomposition technique was one of the first ideas for multi-hop MRC and as you can see in recent years (2021) still has attracted attention.
4. The main disadvantage of this technique is that, instead of focusing on multi-hop reasoning as an important key of multi-hop MRC, it focuses on reducing the problem to a single-hop MRC.
5. Thus, they actually do not go far beyond single-hop models.",0.5,0.0,0.0,0.5,1.0,0.0,-0.5,0.8,-0.8,0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s11,p11.0,"The sequence models have been first used for single-hop MRC tasks, and most of them are based on Recurrent Neural Networks (RNNs), some studies focus on using them in multi-hop MRC. It can be called state-based reasoning models and they are closer to a standard attention-based RC model with an additional ""state"" representation that is iteratively updated. The changing state representation results in the model focusing on different parts of the passage during each iteration, allowing it to combine information from different parts of the passage [28]. Most models, presented in this section, have used advanced neural network concepts, such as attention mechanism and network memory for multi-hop reasoning. In the following the models which use this technique will be reviewed in detail including the architecture alongside the superiority and the motivation of them.",['b27'],[True],1,"1. The sequence models have been first used for single-hop MRC tasks, and most of them are based on Recurrent Neural Networks (RNNs), some studies focus on using them in multi-hop MRC.
2. It can be called state-based reasoning models and they are closer to a standard attention-based RC model with an additional ""state"" representation that is iteratively updated.
3. The changing state representation results in the model focusing on different parts of the passage during each iteration, allowing it to combine information from different parts of the passage [28].
4. Most models, presented in this section, have used advanced neural network concepts, such as attention mechanism and network memory for multi-hop reasoning.
5. In the following the models which use this technique will be reviewed in detail including the architecture alongside the superiority and the motivation of them.","The sequence models have been first used for single-hop MRC tasks, and most of them are based on Recurrent Neural Networks (RNNs), some studies focus on using them in multi-hop MRC.",How do sequence models with state-based reasoning enhance multi-hop MRC tasks?,"Question: How do sequence models with state-based reasoning enhance multi-hop MRC tasks?

1. The sequence models have been first used for single-hop MRC tasks, and most of them are based on Recurrent Neural Networks (RNNs), some studies focus on using them in multi-hop MRC.
2. It can be called state-based reasoning models and they are closer to a standard attention-based RC model with an additional ""state"" representation that is iteratively updated.
3. The changing state representation results in the model focusing on different parts of the passage during each iteration, allowing it to combine information from different parts of the passage [28].
4. Most models, presented in this section, have used advanced neural network concepts, such as attention mechanism and network memory for multi-hop reasoning.
5. In the following the models which use this technique will be reviewed in detail including the architecture alongside the superiority and the motivation of them.",1.0,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s15,p15.3,"They first find all possible path from passages. It starts with selecting a passage that contains a head entity from the question, and then finds all entities and noun phrases from the same sentence. Afterward, it selects the next passage that contains the potential intermediate entity identified above. Finally, it is checked whether the next passage contains any of the candidate answer choices or not. The resulting will be a set of entity sequences. After obtaining all potential paths, it is time to score each path using the PathNet model based on two perspectives: 1) Context-based Path Scoring, which is based on the interaction with the question encoding, and 2) Passage-based Path Scoring, which is based on the interaction between the passage-based path encoding vector and the candidate encoding. There is an example of the process in Figure 15 which In the Rank-1 path, the model composes the implicit located in relations between (Zoo lake, Johannesburg) and (Johannesburg, Gauteng). However, this method extracts many invalid paths, then causes wasting the computing resources [35].",['b34'],[True],1,"1. They first find all possible path from passages.
2. It starts with selecting a passage that contains a head entity from the question, and then finds all entities and noun phrases from the same sentence.
3. Afterward, it selects the next passage that contains the potential intermediate entity identified above.
4. Finally, it is checked whether the next passage contains any of the candidate answer choices or not.
5. The resulting will be a set of entity sequences.
6. After obtaining all potential paths, it is time to score each path using the PathNet model based on two perspectives: 1) Context-based Path Scoring, which is based on the interaction with the question encoding, and 2) Passage-based Path Scoring, which is based on the interaction between the passage-based path encoding vector and the candidate encoding.
7. There is an example of the process in Figure 15 which In the Rank-1 path, the model composes the implicit located in relations between (Zoo lake, Johannesburg) and (Johannesburg, Gauteng).
8. However, this method extracts many invalid paths, then causes wasting the computing resources [35].",They first find all possible path from passages.,How does the PathNet model identify and score potential paths for answering questions?,"Question: How does the PathNet model identify and score potential paths for answering questions?

1. They first find all possible path from passages.
2. It starts with selecting a passage that contains a head entity from the question, and then finds all entities and noun phrases from the same sentence.
3. Afterward, it selects the next passage that contains the potential intermediate entity identified above.
4. Finally, it is checked whether the next passage contains any of the candidate answer choices or not.
5. The resulting will be a set of entity sequences.
6. After obtaining all potential paths, it is time to score each path using the PathNet model based on two perspectives: 1) Context-based Path Scoring, which is based on the interaction with the question encoding, and 2) Passage-based Path Scoring, which is based on the interaction between the passage-based path encoding vector and the candidate encoding.
7. There is an example of the process in Figure 15 which In the Rank-1 path, the model composes the implicit located in relations between (Zoo lake, Johannesburg) and (Johannesburg, Gauteng).
8. However, this method extracts many invalid paths, then causes wasting the computing resources [35].",0.5,0.0,0.5,0.5,1.0,1.0,0.5,0.875,-0.875,0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s16,p16.1,"As Figure 16 shows, after embedding sentences into a matrix and constructing a graph, the policy is run to select the next sentence. In this regard, the convolution-based policy network is used to learn a traversal policy from current options, previously accepted sentences, and the current question. For example, in this figure, the policy has learned to select the correct next sentence (ct) from the previous sentence (ct-1). After this action, the answer sentence (o1) can be selected during the next step. SMR: Huo, Ge and Zhao [37] proposed a Sentence-level Multi-hop Reasoning approach named SMR while most existing approaches only use document-level or entity-level inferences. In this regard, an initial sentence is first found based on the main entity in the question, and that sentence is then used to find more relevant sentences to create multiple sentence-based reasoning chains as a memory network. Besides, some sentences are concatenated to prevent the mistakes of Co-Reference resolution methods and to reduce the number of hops. An example of such a concatenation is shown in Figure 17.",['b36'],[True],1,"1. As Figure 16 shows, after embedding sentences into a matrix and constructing a graph, the policy is run to select the next sentence.
2. In this regard, the convolution-based policy network is used to learn a traversal policy from current options, previously accepted sentences, and the current question.
3. For example, in this figure, the policy has learned to select the correct next sentence (ct) from the previous sentence (ct-1).
4. After this action, the answer sentence (o1) can be selected during the next step.
5. SMR: Huo, Ge and Zhao [37] proposed a Sentence-level Multi-hop Reasoning approach named SMR while most existing approaches only use document-level or entity-level inferences.
6. In this regard, an initial sentence is first found based on the main entity in the question, and that sentence is then used to find more relevant sentences to create multiple sentence-based reasoning chains as a memory network.
7. Besides, some sentences are concatenated to prevent the mistakes of Co-Reference resolution methods and to reduce the number of hops.
8. An example of such a concatenation is shown in Figure 17.","As Figure 16 shows, after embedding sentences into a matrix and constructing a graph, the policy is run to select the next sentence.",How does the SMR approach utilize a convolution-based policy network for sentence selection?,"Question: How does the SMR approach utilize a convolution-based policy network for sentence selection?

1. As Figure 16 shows, after embedding sentences into a matrix and constructing a graph, the policy is run to select the next sentence.
2. In this regard, the convolution-based policy network is used to learn a traversal policy from current options, previously accepted sentences, and the current question.
3. For example, in this figure, the policy has learned to select the correct next sentence (ct) from the previous sentence (ct-1).
4. After this action, the answer sentence (o1) can be selected during the next step.
5. SMR: Huo, Ge and Zhao [37] proposed a Sentence-level Multi-hop Reasoning approach named SMR while most existing approaches only use document-level or entity-level inferences.
6. In this regard, an initial sentence is first found based on the main entity in the question, and that sentence is then used to find more relevant sentences to create multiple sentence-based reasoning chains as a memory network.
7. Besides, some sentences are concatenated to prevent the mistakes of Co-Reference resolution methods and to reduce the number of hops.
8. An example of such a concatenation is shown in Figure 17.",0.0,0.0,0.5,0.5,1.0,1.0,0.5,0.75,-0.75,0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s16,p16.2,"There are two phases in this model: 1) the Selecting phase to select the most relevant sentence to the network memory state as the initial sentence of the current hop, and 2) the Establishing phase to prepare to go to the next hop by updating the network memory state. Finally, the information of the reasoning chains is used to predict the final answer. Figure 17: Representation enrichment via concatenation of adjacent sentences [37] ChainEx: Chen, Lin and Durrett [38] proposed a sentence-based model that does not rely on gold annotated chains or supporting facts at the training and test phases, instead, pseudo-gold reasoning chains are derived using some heuristics based on named entity recognition and coreference resolution during the training time, and it learns to extract chains from raw texts at the test time. They first extract a reasoning chain over the text for a multi-hop reasoning task, and then apply a BERT-based QA system to find the answer by learning from these chains.","['b37', 'b36']","[True, True]",2,"1. There are two phases in this model: 1) the Selecting phase to select the most relevant sentence to the network memory state as the initial sentence of the current hop, and 2) the Establishing phase to prepare to go to the next hop by updating the network memory state.
2. Finally, the information of the reasoning chains is used to predict the final answer.
3. Figure 17: Representation enrichment via concatenation of adjacent sentences [37] ChainEx: Chen, Lin and Durrett [38] proposed a sentence-based model that does not rely on gold annotated chains or supporting facts at the training and test phases, instead, pseudo-gold reasoning chains are derived using some heuristics based on named entity recognition and coreference resolution during the training time, and it learns to extract chains from raw texts at the test time.
4. They first extract a reasoning chain over the text for a multi-hop reasoning task, and then apply a BERT-based QA system to find the answer by learning from these chains.","There are two phases in this model: 1) the Selecting phase to select the most relevant sentence to the network memory state as the initial sentence of the current hop, and 2) the Establishing phase to prepare to go to the next hop by updating the network memory state.",What are the phases and methodology of ChainEx's multi-hop reasoning model?,"Question: What are the phases and methodology of ChainEx's multi-hop reasoning model?

1. There are two phases in this model: 1) the Selecting phase to select the most relevant sentence to the network memory state as the initial sentence of the current hop, and 2) the Establishing phase to prepare to go to the next hop by updating the network memory state.
2. Finally, the information of the reasoning chains is used to predict the final answer.
3. Figure 17: Representation enrichment via concatenation of adjacent sentences [37] ChainEx: Chen, Lin and Durrett [38] proposed a sentence-based model that does not rely on gold annotated chains or supporting facts at the training and test phases, instead, pseudo-gold reasoning chains are derived using some heuristics based on named entity recognition and coreference resolution during the training time, and it learns to extract chains from raw texts at the test time.
4. They first extract a reasoning chain over the text for a multi-hop reasoning task, and then apply a BERT-based QA system to find the answer by learning from these chains.",0.5,0.0,0.5,0.0,1.0,0.0,0.0,0.75,-0.75,0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s17,p17.0,"Graph-based techniques because of their natural language relationship representation ability [39] has attracted attention in multihop MRC. It is natural to model natural language context into graph structure and the process of multi-hop reasoning as moving among nodes [14]. The main idea of the Graph-based technique is to construct a graph based on the context and question, and then the reasoning is performed by message passing over this structure using graph neural networks. The process of constructing the graph from large textual data and reasoning over it are challenging tasks. There are a lot of studies that focus on these challenges which are explained in this subsection in detail.","['b38', 'b13']","[True, True]",2,"1. Graph-based techniques because of their natural language relationship representation ability [39] has attracted attention in multihop MRC.
2. It is natural to model natural language context into graph structure and the process of multi-hop reasoning as moving among nodes [14].
3. The main idea of the Graph-based technique is to construct a graph based on the context and question, and then the reasoning is performed by message passing over this structure using graph neural networks.
4. The process of constructing the graph from large textual data and reasoning over it are challenging tasks.
5. There are a lot of studies that focus on these challenges which are explained in this subsection in detail.",Graph-based techniques because of their natural language relationship representation ability [39] has attracted attention in multihop MRC.,Why are graph-based techniques prominent in multihop machine reading comprehension (MRC)?,"Question: Why are graph-based techniques prominent in multihop machine reading comprehension (MRC)?

1. Graph-based techniques because of their natural language relationship representation ability [39] has attracted attention in multihop MRC.
2. It is natural to model natural language context into graph structure and the process of multi-hop reasoning as moving among nodes [14].
3. The main idea of the Graph-based technique is to construct a graph based on the context and question, and then the reasoning is performed by message passing over this structure using graph neural networks.
4. The process of constructing the graph from large textual data and reasoning over it are challenging tasks.
5. There are a lot of studies that focus on these challenges which are explained in this subsection in detail.",0.5,0.0,0.5,1.0,1.0,1.0,0.0,1.0,-1.0,0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s19,p19.0,"Song et al. [40] focused on inferring global context as an important key in multi-hop reading comprehension, while previous studies approximate global evidence with local coreference information with DAG-styled GRU. They proposed a model for better connecting global evidence, with a more complex graph compared to DAGs. They construct an entity graph with three types of edges: the edge between the same entity within a passage, the edges between two mentions of different entities within a context window, and coreference-typed edges. The graph might also have cycles which makes it difficult to apply a DAG network to it. (A graph with three types of edges and a DAG graph are shown in Figure 21).",['b39'],[True],1,"1. Song et al. [40] focused on inferring global context as an important key in multi-hop reading comprehension, while previous studies approximate global evidence with local coreference information with DAG-styled GRU.
2. They proposed a model for better connecting global evidence, with a more complex graph compared to DAGs.
3. They construct an entity graph with three types of edges: the edge between the same entity within a passage, the edges between two mentions of different entities within a context window, and coreference-typed edges.
4. The graph might also have cycles which makes it difficult to apply a DAG network to it.
5. (A graph with three types of edges and a DAG graph are shown in Figure 21).","Song et al. [40] focused on inferring global context as an important key in multi-hop reading comprehension, while previous studies approximate global evidence with local coreference information with DAG-styled GRU.",How does Song et al.'s model improve multi-hop reading comprehension?,"Question: How does Song et al.'s model improve multi-hop reading comprehension?

1. Song et al. [40] focused on inferring global context as an important key in multi-hop reading comprehension, while previous studies approximate global evidence with local coreference information with DAG-styled GRU.
2. They proposed a model for better connecting global evidence, with a more complex graph compared to DAGs.
3. They construct an entity graph with three types of edges: the edge between the same entity within a passage, the edges between two mentions of different entities within a context window, and coreference-typed edges.
4. The graph might also have cycles which makes it difficult to apply a DAG network to it.
5. (A graph with three types of edges and a DAG graph are shown in Figure 21).",0.5,0.0,0.0,1.0,1.0,1.0,0.0,0.8,-0.8,0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s19,p19.1,"For inferring the global context, the related information of the constructed graph has been merged. In this study, two recent graph neural networks have been applied to this graph: graph convolutional network (GCN) and graph recurrent network (GRN) for evidence aggregation. Afterward, an attention mechanism is applied in order to match the hidden states at each graph encoding step with the question representation. Finally, a probability distribution is calculated from the matching results. The architecture of this model is shown in Figure 22. However, this model still only implicitly combines knowledge from all passages, and are therefore unable to provide explicit reasoning paths [28]. DFGN: Xiao et al. [41] proposed a model to improve the interaction between the information of documents and the entity graph.","['b27', 'b40']","[True, True]",2,"1. For inferring the global context, the related information of the constructed graph has been merged.
2. In this study, two recent graph neural networks have been applied to this graph: graph convolutional network (GCN) and graph recurrent network (GRN) for evidence aggregation.
3. Afterward, an attention mechanism is applied in order to match the hidden states at each graph encoding step with the question representation.
4. Finally, a probability distribution is calculated from the matching results.
5. The architecture of this model is shown in Figure 22.
6. However, this model still only implicitly combines knowledge from all passages, and are therefore unable to provide explicit reasoning paths [28].
7. DFGN: Xiao et al. [41] proposed a model to improve the interaction between the information of documents and the entity graph.","For inferring the global context, the related information of the constructed graph has been merged.",How do GCN and GRN contribute to evidence aggregation in graph-based models?,"Question: How do GCN and GRN contribute to evidence aggregation in graph-based models?

1. For inferring the global context, the related information of the constructed graph has been merged.
2. In this study, two recent graph neural networks have been applied to this graph: graph convolutional network (GCN) and graph recurrent network (GRN) for evidence aggregation.
3. Afterward, an attention mechanism is applied in order to match the hidden states at each graph encoding step with the question representation.
4. Finally, a probability distribution is calculated from the matching results.
5. The architecture of this model is shown in Figure 22.
6. However, this model still only implicitly combines knowledge from all passages, and are therefore unable to provide explicit reasoning paths [28].
7. DFGN: Xiao et al. [41] proposed a model to improve the interaction between the information of documents and the entity graph.",0.5,0.0,0.5,0.5,1.0,1.0,0.5,0.8571428571428571,-0.8571428571428571,0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s22,p22.0,"Tu et al. [46] proposed a more complex graph named Heterogeneous Document-Entity (HDE) graph with different types of nodes and edges. This graph can cover different granularity levels of information in context and also enables rich information interaction among different types of nodes for accurate reasoning. The nodes in the HDE graph are candidates, documents, and entities. Besides, it has seven types of edges: 1) between a document node and a candidate node that appears in the same document.",['b45'],[True],1,"1. Tu et al. [46] proposed a more complex graph named Heterogeneous Document-Entity (HDE) graph with different types of nodes and edges.
2. This graph can cover different granularity levels of information in context and also enables rich information interaction among different types of nodes for accurate reasoning.
3. The nodes in the HDE graph are candidates, documents, and entities.
4. Besides, it has seven types of edges: 1) between a document node and a candidate node that appears in the same document.",Tu et al. [46] proposed a more complex graph named Heterogeneous Document-Entity (HDE) graph with different types of nodes and edges.,What is the Heterogeneous Document-Entity graph and its significance in information processing?,"Question: What is the Heterogeneous Document-Entity graph and its significance in information processing?

1. Tu et al. [46] proposed a more complex graph named Heterogeneous Document-Entity (HDE) graph with different types of nodes and edges.
2. This graph can cover different granularity levels of information in context and also enables rich information interaction among different types of nodes for accurate reasoning.
3. The nodes in the HDE graph are candidates, documents, and entities.
4. Besides, it has seven types of edges: 1) between a document node and a candidate node that appears in the same document.",0.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,0.0,0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s22,p22.2,"However, [35] have shown that if the number of inferences increases, the complexity of models will rise sharply due to the iteration of cumbersome message passing algorithm, resulting in low efficiency.  [46] SAE: Tu et al. [47] proposed an interpretable system named Select, Answer, and Explain (SAE) with multi-hop reasoning graphs based on GNN with contextual sentence embeddings as nodes. This kind of sentence graph makes lead to an interpretable model because it can directly output supporting sentences with the answer prediction. The edges capture the global information presented within each document and also the cross-document reasoning path. Also, the contextual sentence embedding used in GNN is summarized over token representations based on a novel mixed attentive pooling mechanism. The attention weight is calculated from both answer span logits and self-attention output on token representations. This attention-based interaction enables the exploitation of complementary information between ""answer"" and ""explain"" tasks. The SAE system first filters unrelated documents, and selects gold documents using a document classifier trained with a novel pairwise learning-to-rank loss function.","['b46', 'b45', 'b34']","[True, True, True]",3,"1. However, [35] have shown that if the number of inferences increases, the complexity of models will rise sharply due to the iteration of cumbersome message passing algorithm, resulting in low efficiency.
2. [46] SAE: Tu et al. [47] proposed an interpretable system named Select, Answer, and Explain (SAE) with multi-hop reasoning graphs based on GNN with contextual sentence embeddings as nodes.
3. This kind of sentence graph makes lead to an interpretable model because it can directly output supporting sentences with the answer prediction.
4. The edges capture the global information presented within each document and also the cross-document reasoning path.
5. Also, the contextual sentence embedding used in GNN is summarized over token representations based on a novel mixed attentive pooling mechanism.
6. The attention weight is calculated from both answer span logits and self-attention output on token representations.
7. This attention-based interaction enables the exploitation of complementary information between ""answer"" and ""explain"" tasks.
8. The SAE system first filters unrelated documents, and selects gold documents using a document classifier trained with a novel pairwise learning-to-rank
9. loss function.","However, [35] have shown that if the number of inferences increases, the complexity of models will rise sharply due to the iteration of cumbersome message passing algorithm, resulting in low efficiency.",How does the SAE system enhance interpretability and efficiency in multi-hop reasoning models?,"Question: How does the SAE system enhance interpretability and efficiency in multi-hop reasoning models?

1. However, [35] have shown that if the number of inferences increases, the complexity of models will rise sharply due to the iteration of cumbersome message passing algorithm, resulting in low efficiency.
2. [46] SAE: Tu et al. [47] proposed an interpretable system named Select, Answer, and Explain (SAE) with multi-hop reasoning graphs based on GNN with contextual sentence embeddings as nodes.
3. This kind of sentence graph makes lead to an interpretable model because it can directly output supporting sentences with the answer prediction.
4. The edges capture the global information presented within each document and also the cross-document reasoning path.
5. Also, the contextual sentence embedding used in GNN is summarized over token representations based on a novel mixed attentive pooling mechanism.
6. The attention weight is calculated from both answer span logits and self-attention output on token representations.
7. This attention-based interaction enables the exploitation of complementary information between ""answer"" and ""explain"" tasks.
8. The SAE system first filters unrelated documents, and selects gold documents using a document classifier trained with a novel pairwise learning-to-rank
9. loss function.",0.0,0.0,0.0,0.5,1.0,0.0,-0.5,0.7777777777777778,-0.7777777777777778,0.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s23,p23.1,"Then they proposed the latent query reformulation method (IP-LQR), which incorporates phrases in the latent query reformulation to improve the cognitive ability of the system. They also design a semantic-augmented fusion method based on the phrase graph, which is then used to propagate the information (Figure 38). After encoding the context and question into the high vector space and acquiring the representations of phrases, sentences, and paragraphs via the mean-pooling layer. Then, a similarity evaluation strategy is designed to calculate the weights of edges in the graph, also the fusion layer is used as an information aggregation to latently update the original question's representation. Finally, a re-attention mechanism is used to help locate the gold answer based on the new representation.  Recently, graph-based models have become very popular and recognized as the main solution for multi-hop MRC because of the nature of modeling such a process into graph structure and the good results. But there is some drawback to this technique, the first one is the expensive computational process of the graph-based methods [52], and the second problem is that the graph-based models often can't cover all the inherent structure of documents and loss valuable structural information by modeling documents into graphs [12].","['b51', 'b11']","[True, True]",2,"1. Then they proposed the latent query reformulation method (IP-LQR), which incorporates phrases in the latent query reformulation to improve the cognitive ability of the system.
2. They also design a semantic-augmented fusion method based on the phrase graph, which is then used to propagate the information (Figure 38).
3. After encoding the context and question into the high vector space and acquiring the representations of phrases, sentences, and paragraphs via the mean-pooling layer.
4. Then, a similarity evaluation strategy is designed to calculate the weights of edges in the graph, also the fusion layer is used as an information aggregation to latently update the original question's representation.
5. Finally, a re-attention mechanism is used to help locate the gold answer based on the new representation.
6. Recently, graph-based models have become very popular and recognized as the main solution for multi-hop MRC because of the nature of modeling such a process into graph structure and the good results.
7. But there is some drawback to this technique, the first one is the expensive computational process of the graph-based methods [52], and the second problem is that the graph-based models often can't cover all the inherent structure of documents and loss valuable structural information by modeling documents into graphs [12].","Then they proposed the latent query reformulation method (IP-LQR), which incorporates phrases in the latent query reformulation to improve the cognitive ability of the system.",How does the IP-LQR method enhance multi-hop MRC systems?,"Question: How does the IP-LQR method enhance multi-hop MRC systems?

1. Then they proposed the latent query reformulation method (IP-LQR), which incorporates phrases in the latent query reformulation to improve the cognitive ability of the system.
2. They also design a semantic-augmented fusion method based on the phrase graph, which is then used to propagate the information (Figure 38).
3. After encoding the context and question into the high vector space and acquiring the representations of phrases, sentences, and paragraphs via the mean-pooling layer.
4. Then, a similarity evaluation strategy is designed to calculate the weights of edges in the graph, also the fusion layer is used as an information aggregation to latently update the original question's representation.
5. Finally, a re-attention mechanism is used to help locate the gold answer based on the new representation.
6. Recently, graph-based models have become very popular and recognized as the main solution for multi-hop MRC because of the nature of modeling such a process into graph structure and the good results.
7. But there is some drawback to this technique, the first one is the expensive computational process of the graph-based methods [52], and the second problem is that the graph-based models often can't cover all the inherent structure of documents and loss valuable structural information by modeling documents into graphs [12].",0.0,0.0,0.5,1.0,1.0,1.0,0.0,1.0,-1.0,1.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s24,p24.1,"GF: Shao et al. [53] attempted to answer this question: How much does graph structure contribute to answer a multi-hop question. They reimplement a graph-based model-Dynamically Fused Graph Network [54]-and claimed that the graph structure can play an important role only if the pre-trained models are used in a feature-based manner, while if the pre-trained models are used in the fine-tuning approach, the graph structure may not be helpful. Then the adjacency matrix based on manually defined rules and the graph structure can be regarded as prior knowledge, which could be learned by self-attention or Transformers. They proved that when texts have been modeled as an entity graph, both graph-attention and self-attention can achieve comparable results, but when texts have been modeled as a sequence structure, only a 2-layer Transformer could achieve similar results as DFGN. As you can see in Figure 40 when the entity graph is fully connected, a graph-attention layer will degenerate into a selfattention layer. However, this study as the first attempt of a graph-free model suffers from huge performance gap compared to the state-art-of the graph based-models [14]. Figure 40: An example of the relation between the graph-attention network and the self-attention network [53] AMS: Yuntao et al. [52] proposed another non-graph model with a focus on the document filters step to denoise irrelevant documents, and proves that if this step has been done properly, even a single-hop model can be used for multi-hop. They investigate that promising performance of the filter from Hierarchical Graph Network (HGN) (Fang et al., 2020) and showed that for 2paragraph selection, both precision and recall can achieve around 95%, and for 4-paragraph selection, recall will be nearly 99%.","['b52', 'b51', 'b53', None, 'b13', 'b47']","[True, True, True, True, True, True]",6,"1. GF: Shao et al. [53] attempted to answer this question: How much does graph structure contribute to answer a multi-hop question.
2. They reimplement a graph-based model-Dynamically Fused Graph Network [54]-and claimed that the graph structure can play an important role only if the pre-trained models are used in a feature-based manner, while if the pre-trained models are used in the fine-tuning approach, the graph structure may not be helpful.
3. Then the adjacency matrix based on manually defined rules and the graph structure can be regarded as prior knowledge, which could be learned by self-attention or Transformers.
4. They proved that when texts have been modeled as an entity graph, both graph-attention and self-attention can achieve comparable results, but when texts have been modeled as a sequence structure, only a 2-layer Transformer could achieve similar results as DFGN.
5. As you can see in Figure 40 when the entity graph is fully connected, a graph-attention layer will degenerate into a selfattention layer.
6. However, this study as the first attempt of a graph-free model suffers from huge performance gap compared to the state-art-of the graph based-models [14].
7. Figure 40: An example of the relation between the graph-attention network and the self-attention network [53] AMS: Yuntao et al. [52] proposed another non-graph model with a focus on the document filters step to denoise irrelevant documents, and proves that if this step has been done properly, even a single-hop model can be used for multi-hop.
8. They investigate that promising performance of the filter from Hierarchical Graph Network (HGN) (Fang et al., 2020) and showed that for 2paragraph selection, both precision and recall can achieve around 95%, and for 4-paragraph selection, recall will be nearly 99%.",GF: Shao et al. [53] attempted to answer this question: How much does graph structure contribute to answer a multi-hop question.,How does graph structure impact multi-hop question answering effectiveness?,"Question: How does graph structure impact multi-hop question answering effectiveness?

1. GF: Shao et al. [53] attempted to answer this question: How much does graph structure contribute to answer a multi-hop question.
2. They reimplement a graph-based model-Dynamically Fused Graph Network [54]-and claimed that the graph structure can play an important role only if the pre-trained models are used in a feature-based manner, while if the pre-trained models are used in the fine-tuning approach, the graph structure may not be helpful.
3. Then the adjacency matrix based on manually defined rules and the graph structure can be regarded as prior knowledge, which could be learned by self-attention or Transformers.
4. They proved that when texts have been modeled as an entity graph, both graph-attention and self-attention can achieve comparable results, but when texts have been modeled as a sequence structure, only a 2-layer Transformer could achieve similar results as DFGN.
5. As you can see in Figure 40 when the entity graph is fully connected, a graph-attention layer will degenerate into a selfattention layer.
6. However, this study as the first attempt of a graph-free model suffers from huge performance gap compared to the state-art-of the graph based-models [14].
7. Figure 40: An example of the relation between the graph-attention network and the self-attention network [53] AMS: Yuntao et al. [52] proposed another non-graph model with a focus on the document filters step to denoise irrelevant documents, and proves that if this step has been done properly, even a single-hop model can be used for multi-hop.
8. They investigate that promising performance of the filter from Hierarchical Graph Network (HGN) (Fang et al., 2020) and showed that for 2paragraph selection, both precision and recall can achieve around 95%, and for 4-paragraph selection, recall will be nearly 99%.",0.5,0.0,0.5,0.0,1.0,1.0,1.0,0.75,-0.75,1.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s24,p24.2,"Then they proposed Answer Multi-hop questions by Single-hop QA (AMS) models and used a single-hop QA models based on the attention mechanism with the HGN's document filter. As you can see in Figure 41 after the Document denoise layer, they used an Attention-based single-hop layer. AMS could achieve a comparable result with stat-of-the-art graph-bade models but still couldn't improve them. Figure 41: Overall architecture of AMS [52] S2G: Wu, Zhang, and Zhao [14] investigated whether graph modeling is necessary for multi-hop. In this regard, they first proved that the retrieval stage is the most important module, while the existing studies focus on the reader module by graph modeling. This study presents a graph-free alternative named select-to-guide (S2G) to retrieve evidence paragraphs in a coarse-tofine manner, incorporated with two attention mechanisms, which shows conforming to the nature of multi-hop reasoning. For the paragraph retrieval module, this study introduced a cascaded paragraph retrieval module that retrieves the evidence paragraphs in an explicit coarse-to-fine manner, and in multi-task module there are one shared encoder module alongside with two interdependent modules with an attention mechanism ( Figure 42). However Concrete error analysis on S2G shows that there is still room for improvement on the multi-hop retriever module design. Figure 42: The multi-task module of S2G [14] As the last part of this section, Figure 43 has been prepared to summarize the techniques and models.","['b51', 'b13']","[True, True]",2,"1. Then they proposed Answer Multi-hop questions by Single-hop QA (AMS) models and used a single-hop QA models based on the attention mechanism with the HGN's document filter.
2. As you can see in Figure 41 after the Document denoise layer, they used an Attention-based single-hop layer.
3. AMS could achieve a comparable result with stat-of-the-art graph-bade models but still couldn't improve them.
4. Figure 41: Overall architecture of AMS [52] S2G: Wu, Zhang, and Zhao [14] investigated whether graph modeling is necessary for multi-hop.
5. In this regard, they first proved that the retrieval stage is the most important module, while the existing studies focus on the reader module by graph modeling.
6. This study presents a graph-free alternative named select-to-guide (S2G) to retrieve evidence paragraphs in a coarse-tofine manner, incorporated with two attention mechanisms, which shows conforming to the nature of multi-hop reasoning.
7. For the paragraph retrieval module, this study introduced a cascaded paragraph retrieval module that retrieves the evidence paragraphs in an explicit coarse-to-fine manner, and in multi-task module there are one shared encoder module alongside with two interdependent modules with an attention mechanism ( Figure 42).
8. However Concrete error analysis on S2G shows that there is still room for improvement on the multi-hop retriever module design.
9. Figure 42: The multi-task module of S2G [14]
10. As the last part of this section, Figure 43 has been prepared to summarize the techniques and models.",Then they proposed Answer Multi-hop questions by Single-hop QA (AMS) models and used a single-hop QA models based on the attention mechanism with the HGN's document filter.,What are the AMS and S2G models' approaches to multi-hop question answering?,"Question: What are the AMS and S2G models' approaches to multi-hop question answering?

1. Then they proposed Answer Multi-hop questions by Single-hop QA (AMS) models and used a single-hop QA models based on the attention mechanism with the HGN's document filter.
2. As you can see in Figure 41 after the Document denoise layer, they used an Attention-based single-hop layer.
3. AMS could achieve a comparable result with stat-of-the-art graph-bade models but still couldn't improve them.
4. Figure 41: Overall architecture of AMS [52] S2G: Wu, Zhang, and Zhao [14] investigated whether graph modeling is necessary for multi-hop.
5. In this regard, they first proved that the retrieval stage is the most important module, while the existing studies focus on the reader module by graph modeling.
6. This study presents a graph-free alternative named select-to-guide (S2G) to retrieve evidence paragraphs in a coarse-tofine manner, incorporated with two attention mechanisms, which shows conforming to the nature of multi-hop reasoning.
7. For the paragraph retrieval module, this study introduced a cascaded paragraph retrieval module that retrieves the evidence paragraphs in an explicit coarse-to-fine manner, and in multi-task module there are one shared encoder module alongside with two interdependent modules with an attention mechanism ( Figure 42).
8. However Concrete error analysis on S2G shows that there is still room for improvement on the multi-hop retriever module design.
9. Figure 42: The multi-task module of S2G [14]
10. As the last part of this section, Figure 43 has been prepared to summarize the techniques and models.",0.0,0.0,0.0,0.5,0.0,1.0,0.5,0.5,-0.5,1.0
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s29,p29.0,"In this section, the performance of models on HotpotQA [11] will be investigated. Exact-match (EM) and F1 metrics have been used in this study to evaluate the model performance. EM is used to show whether the predicated answer and the ground-truth answer are exactly the same. F1 measures the average overlap between the predicted answer and the ground-truth answer. In addition, there are three sets of metrics: 1) Answer EM/F1 to evaluate the answer span prediction, 2) Support EM/F1 to evaluate the supporting facts prediction, and 3) Joint EM/F1 to combine the two previous ones.",['b10'],[True],1,"1. In this section, the performance of models on HotpotQA [11] will be investigated.
2. Exact-match (EM) and F1 metrics have been used in this study to evaluate the model performance.
3. EM is used to show whether the predicated answer and the ground-truth answer are exactly the same.
4. F1 measures the average overlap between the predicted answer and the ground-truth answer.
5. In addition, there are three sets of metrics: 1) Answer EM/F1 to evaluate the answer span prediction, 2) Support EM/F1 to evaluate the supporting facts prediction, and 3) Joint EM/F1 to combine the two previous ones.","In this section, the performance of models on HotpotQA [11] will be investigated.",How are models evaluated on HotpotQA using EM and F1 metrics?,"Question: How are models evaluated on HotpotQA using EM and F1 metrics?

1. In this section, the performance of models on HotpotQA [11] will be investigated.
2. Exact-match (EM) and F1 metrics have been used in this study to evaluate the model performance.
3. EM is used to show whether the predicated answer and the ground-truth answer are exactly the same.
4. F1 measures the average overlap between the predicted answer and the ground-truth answer.
5. In addition, there are three sets of metrics: 1) Answer EM/F1 to evaluate the answer span prediction, 2) Support EM/F1 to evaluate the supporting facts prediction, and 3) Joint EM/F1 to combine the two previous ones.",1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s1,p1.2,"The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task. A solution is to equip the shared trunk with task-specific encoders [47,58,137]. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages. Another way is to make different groups of tasks share different parts of the trunk [37,79,89]. Moreover, this idea can be applied to the decoder. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.","['b78', 'b136', 'b46', 'b64', 'b88', 'b36', 'b127', 'b57']","[True, True, True, True, True, True, True, True]",8,"1. The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task.
2. A solution is to equip the shared trunk with task-specific encoders [47,58,137].
3. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages.
4. Another way is to make different groups of tasks share different parts of the trunk [37,79,89].
5. Moreover, this idea can be applied to the decoder.
6. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.","The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task.",How can the expressive power of a tree-like architecture model be enhanced for multiple tasks?,"Question: How can the expressive power of a tree-like architecture model be enhanced for multiple tasks?

1. The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task.
2. A solution is to equip the shared trunk with task-specific encoders [47,58,137].
3. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages.
4. Another way is to make different groups of tasks share different parts of the trunk [37,79,89].
5. Moreover, this idea can be applied to the decoder.
6. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.",1.0,0.5,0.5,1.0,1.0,1.0,0.0,1.0,-0.5,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s1,p1.5,"However, different parts of the shared features are not equally important to each task. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | · | compute the absolute value in an element-wise manner. After that, S is concatenated with task-specific representations to form the input to the output layer.","['b147', 'b68', 'b81', 'b38', 'b132']","[True, True, True, True, True]",5,"1. However, different parts of the shared features are not equally important to each task.
2. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks.
3. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate.
4. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks.
5. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism.
6. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | ·
7. | compute the absolute value in an element-wise manner.
8. After that, S is concatenated with task-specific representations to form the input to the output layer.","However, different parts of the shared features are not equally important to each task.",How do models control information flow in multi-task learning to reduce inter-task interference?,"Question: How do models control information flow in multi-task learning to reduce inter-task interference?

1. However, different parts of the shared features are not equally important to each task.
2. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks.
3. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate.
4. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks.
5. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism.
6. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | ·
7. | compute the absolute value in an element-wise manner.
8. After that, S is concatenated with task-specific representations to form the input to the output layer.",0.5,0.5,1.0,1.0,1.0,1.0,0.0,0.75,-0.25,1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s1,p1.8,"Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task. Given tasks, the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .","['b135', 'b100', 'b143']","[True, True, True]",3,"1. Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task.
2. Given tasks , the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight.
3. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task.
4. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .","Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task.",How does task routing contribute to feature fusion in neural networks?,"Question: How does task routing contribute to feature fusion in neural networks?

1. Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task.
2. Given tasks , the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight.
3. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task.
4. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .",0.5,0.5,0.0,1.0,1.0,1.0,0.0,0.5,0.0,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s2,p2.1,"In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.","['b96', 'b67', 'b84', 'b153', 'b15', 'b61']","[True, True, True, True, True, True]",6,"1. In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful.
2. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154].
3. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task.
4. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks.
5. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.","In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful.",How can auxiliary tasks enhance MTL performance in various applications?,"Question: How can auxiliary tasks enhance MTL performance in various applications?

1. In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful.
2. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154].
3. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task.
4. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks.
5. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.",1.0,0.5,0.5,1.0,1.0,1.0,0.0,1.0,-0.5,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s4,p4.1,"In hierarchical feature pipeline, the output of one task is used as extra features for another task. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks. For example, [14] feeds the output of a question-review pair recognition model to the question answering model. [44] feeds the output of aspect term extraction to aspect-term sentiment classification. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.","['b138', 'b13', 'b43', 'b109', 'b151']","[True, True, True, True, True]",5,"1. In hierarchical feature pipeline, the output of one task is used as extra features for another task.
2. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks.
3. For example, [14] feeds the output of a question-review pair recognition model to the question answering model.
4. [44] feeds the output of aspect term extraction to aspect-term sentiment classification.
5. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations.
6. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections.
7. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.","In hierarchical feature pipeline, the output of one task is used as extra features for another task.",How does hierarchical feature pipeline enhance multi-task models in machine learning?,"Question: How does hierarchical feature pipeline enhance multi-task models in machine learning?

1. In hierarchical feature pipeline, the output of one task is used as extra features for another task.
2. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks.
3. For example, [14] feeds the output of a question-review pair recognition model to the question answering model.
4. [44] feeds the output of aspect term extraction to aspect-term sentiment classification.
5. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations.
6. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections.
7. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.",1.0,0.5,0.5,1.0,1.0,1.0,0.0,1.0,-0.5,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s4,p4.2,"Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.","['b42', 'b29', 'b32', 'b106', 'b111']","[True, True, True, True, True]",5,"1. Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships.
2. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction.
3. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks.
4. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task.
5. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task.
6. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.",Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships.,How is hierarchical feature pipeline utilized in various natural language processing tasks?,"Question: How is hierarchical feature pipeline utilized in various natural language processing tasks?

1. Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships.
2. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction.
3. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks.
4. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task.
5. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task.
6. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.",1.0,0.0,0.5,1.0,1.0,1.0,0.0,1.0,-1.0,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s4,p4.4,"In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50]. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction. The outputs of a task can also be used for post-processing. For instance, [145] uses the output of NER to help extract multi-token entities.","['b144', 'b49', 'b73', 'b105']","[True, True, True, True]",4,"1. In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks.
2. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50].
3. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features.
4. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction.
5. The outputs of a task can also be used for post-processing.
6. For instance, [145] uses the output of NER to help extract multi-token entities.","In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks.",How does hierarchical signal pipeline enhance task performance in machine learning?,"Question: How does hierarchical signal pipeline enhance task performance in machine learning?

1. In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks.
2. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50].
3. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features.
4. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction.
5. The outputs of a task can also be used for post-processing.
6. For instance, [145] uses the output of NER to help extract multi-token entities.",1.0,0.5,0.0,1.0,1.0,1.0,0.0,1.0,-0.5,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s5,p5.0,"Interactive Architecture. Different from most machine learning models that give predictions in a single pass, hierarchical interactive MTL explicitly models the interactions between tasks via a multi-turn prediction mechanism which allows a model to refine its predictions over multiple steps with the help of the previous outputs from other tasks in a way similar to recurrent neural networks. [44] maintains a shared latent representation which is updated by iterations. Multi-step attention network [51] refines its prediction by attending to input representations in previous steps. In cyclic MTL [146], the output of one task is used as an extra input to its successive lower-level task and the output of the last task is fed to the first one, forming a loop. Most hierarchical interactive MTL models as introduced above report that performance converges quickly at = 2 steps, showing the benefit and efficiency of doing multi-step prediction.","['b43', 'b145', 'b50']","[True, True, True]",3,"1. Interactive Architecture. Different from most machine learning models that give predictions in a single pass, hierarchical interactive MTL explicitly models the interactions between tasks via a multi-turn prediction mechanism which allows a model to refine its predictions over multiple steps with the help of the previous outputs from other tasks in a way similar to recurrent neural networks.
2. [44] maintains a shared latent representation which is updated by iterations.
3. Multi-step attention network [51] refines its prediction by attending to input representations in previous steps.
4. In cyclic MTL [146], the output of one task is used as an extra input to its successive lower-level task and the output of the last task is fed to the first one, forming a loop.
5. Most hierarchical interactive MTL models as introduced above report that performance converges quickly at = 2 steps, showing the benefit and efficiency of doing multi-step prediction.","Interactive Architecture. Different from most machine learning models that give predictions in a single pass, hierarchical interactive MTL explicitly models the interactions between tasks via a multi-turn prediction mechanism which allows a model to refine its predictions over multiple steps with the help of the previous outputs from other tasks in a way similar to recurrent neural networks.",What is hierarchical interactive multi-task learning and how does it work?,"Question: What is hierarchical interactive multi-task learning and how does it work?

1. Interactive Architecture. Different from most machine learning models that give predictions in a single pass, hierarchical interactive MTL explicitly models the interactions between tasks via a multi-turn prediction mechanism which allows a model to refine its predictions over multiple steps with the help of the previous outputs from other tasks in a way similar to recurrent neural networks.
2. [44] maintains a shared latent representation which is updated by iterations.
3. Multi-step attention network [51] refines its prediction by attending to input representations in previous steps.
4. In cyclic MTL [146], the output of one task is used as an extra input to its successive lower-level task and the output of the last task is fed to the first one, forming a loop.
5. Most hierarchical interactive MTL models as introduced above report that performance converges quickly at = 2 steps, showing the benefit and efficiency of doing multi-step prediction.",0.5,0.5,0.5,1.0,1.0,1.0,0.0,1.0,-0.5,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s6,p6.1,"The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did. [1] shares word and character embedding matrices and combines them differently for different tasks. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT). Modular designs are also widely used in multi-lingual tasks. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks. Shared embeddings can be used alongside task-specific embeddings [59,138] as well. In addition to word embeddings, [147] shares label embeddings between tasks. Researchers have also developed modular architectures at a finer granularity. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer. [92] creates encoder modules on different levels, including task level, task group level, and universal level.","['b137', 'b102', 'b118', 'b0', 'b146', 'b91', 'b23', 'b58', 'b155', 'b57', 'b48']","[True, True, True, True, True, True, True, True, True, True, True]",11,"1. The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1.
2. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did.
3. [1] shares word and character embedding matrices and combines them differently for different tasks.
4. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT).
5. Modular designs are also widely used in multi-lingual tasks.
6. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks.
7. Shared embeddings can be used alongside task-specific embeddings [59,138] as well.
8. In addition to word embeddings, [147] shares label embeddings between tasks.
9. Researchers have also developed modular architectures at a finer granularity.
10. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation.
11. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer.
12. [92] creates encoder modules on different levels, including task level, task group level, and universal level.",The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1.,What are common practices in designing modular architectures for multi-task learning?,"Question: What are common practices in designing modular architectures for multi-task learning?

1. The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1.
2. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did.
3. [1] shares word and character embedding matrices and combines them differently for different tasks.
4. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT).
5. Modular designs are also widely used in multi-lingual tasks.
6. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks.
7. Shared embeddings can be used alongside task-specific embeddings [59,138] as well.
8. In addition to word embeddings, [147] shares label embeddings between tasks.
9. Researchers have also developed modular architectures at a finer granularity.
10. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation.
11. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer.
12. [92] creates encoder modules on different levels, including task level, task group level, and universal level.",0.5,0.0,0.0,1.0,1.0,1.0,0.0,0.8333333333333334,-0.8333333333333334,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s7,p7.0,"Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.","['b137', 'b69', 'b118', 'b77', 'b126']","[True, True, True, True, True]",5,"1. Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision.
2. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones.
3. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images.
4. A similar idea can be used in MTL for NLP tasks.
5. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model.
6. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.",Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision.,How do Generative Adversarial Networks (GANs) enhance generative tasks in computer vision and NLP?,"Question: How do Generative Adversarial Networks (GANs) enhance generative tasks in computer vision and NLP?

1. Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision.
2. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones.
3. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images.
4. A similar idea can be used in MTL for NLP tasks.
5. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model.
6. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.",1.0,1.0,0.5,0.5,1.0,1.0,0.5,0.8333333333333334,0.1666666666666666,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s7,p7.1,"An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.","['b123', 'b97']","[True, True]",2,"1. An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized.
2. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents.
3. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach.
4. First, a discriminator that rates the quality of candidate answers is trained on labeled samples.
5. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.",An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized.,How do generative adversarial architectures utilize unlabeled data to enhance model performance?,"Question: How do generative adversarial architectures utilize unlabeled data to enhance model performance?

1. An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized.
2. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents.
3. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach.
4. First, a discriminator that rates the quality of candidate answers is trained on labeled samples.
5. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.",1.0,0.5,0.5,1.0,1.0,1.0,0.0,1.0,-0.5,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s9,p9.10,"Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g . Notice that PCGrad is a special case of GradVac when = 0. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.","['b128', 'b0']","[True, True]",2,"1. Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients.
2. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g .
3. Notice that PCGrad is a special case of GradVac when = 0.
4. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.","Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients.",How does GradVac optimize multi-lingual model performance through gradient manipulation?,"Question: How does GradVac optimize multi-lingual model performance through gradient manipulation?

1. Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients.
2. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g .
3. Notice that PCGrad is a special case of GradVac when = 0.
4. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.",0.5,1.0,0.5,1.0,1.0,1.0,0.0,0.75,0.25,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s11,p11.0,"Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.","['b2', 'b147', 'b133', 'b110', 'b142']","[True, True, True, True, True]",5,"1. Task scheduling determines the order of tasks in which an MTL model is trained.
2. A naive way is to train all tasks together.
3. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions.
4. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together.
5. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step.
6. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.",Task scheduling determines the order of tasks in which an MTL model is trained.,What is task scheduling in MTL model training and its naive approach?,"Question: What is task scheduling in MTL model training and its naive approach?

1. Task scheduling determines the order of tasks in which an MTL model is trained.
2. A naive way is to train all tasks together.
3. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions.
4. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together.
5. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step.
6. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.",1.0,0.5,0.0,1.0,0.0,1.0,0.0,0.8333333333333334,-0.3333333333333333,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s11,p11.5,"In some cases, multiple tasks are learned sequentially. Such tasks usually form a clear dependency relationship or are of different difficulty levels. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1). Unicoder [49] trains its five pre-training objectives sequentially in each step. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.","['b89', 'b84', 'b42', 'b93', 'b29', 'b17', 'b48', 'b49']","[True, True, True, True, True, True, True, True]",8,"1. In some cases, multiple tasks are learned sequentially.
2. Such tasks usually form a clear dependency relationship or are of different difficulty levels.
3. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties.
4. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches.
5. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1).
6. Unicoder [49] trains its five pre-training objectives sequentially in each step.
7. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain.
8. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters.
9. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.","In some cases, multiple tasks are learned sequentially.",What are the strategies for sequential multi-task learning in machine learning models?,"Question: What are the strategies for sequential multi-task learning in machine learning models?

1. In some cases, multiple tasks are learned sequentially.
2. Such tasks usually form a clear dependency relationship or are of different difficulty levels.
3. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties.
4. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches.
5. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1).
6. Unicoder [49] trains its five pre-training objectives sequentially in each step.
7. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain.
8. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters.
9. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.",0.5,0.0,0.0,1.0,1.0,1.0,0.0,0.8888888888888888,-0.8888888888888888,1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s11,p11.6,"For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data. [125] first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.","['b43', 'b124', 'b54', 'b13']","[True, True, True, True]",4,"1. For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks.
2. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task.
3. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data.
4. [125] first pre -trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data.
5. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.","For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks.",What is the pre-train then fine-tune methodology in auxiliary MTL?,"Question: What is the pre-train then fine-tune methodology in auxiliary MTL?

1. For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks.
2. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task.
3. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data.
4. [125] first pre -trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data.
5. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.",1.0,0.5,0.0,1.0,1.0,1.0,0.0,1.0,-0.5,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s14,p14.0,"Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.","['b27', 'b52', 'b55', 'b32', 'b106', 'b54', 'b132', 'b13', 'b58', 'b70', 'b111', 'b73']","[True, True, True, True, True, True, True, True, True, True, True, True]",12,"1. Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification.
2. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks.
3. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction.
4. [59] enhances a rumor detection model with user credibility features.
5. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance.
6. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token.
7. In [107], text classification is boosted by learning the predominant sense of words.
8. [133] assists the fake news detection task by stance classification.
9. [14] jointly learns the answer identification task with an auxiliary question answering task.
10. To improve slot filling performance for online shopping assistants, [56]0 adds NER and segment tagging tasks as auxiliary tasks.
11. In [56]1, the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.","Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification.",How is auxiliary multi-task learning applied in various classification and detection tasks?,"Question: How is auxiliary multi-task learning applied in various classification and detection tasks?

1. Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification.
2. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks.
3. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction.
4. [59] enhances a rumor detection model with user credibility features.
5. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance.
6. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token.
7. In [107], text classification is boosted by learning the predominant sense of words.
8. [133] assists the fake news detection task by stance classification.
9. [14] jointly learns the answer identification task with an auxiliary question answering task.
10. To improve slot filling performance for online shopping assistants, [56]0 adds NER and segment tagging tasks as auxiliary tasks.
11. In [56]1, the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.",0.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,-1.0,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s14,p14.2,"For text generation tasks, MTL is brought in to improve the quality of the generated text. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155]. [105] implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing. [9] enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks. [99] views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.","['b26', 'b37', 'b104', 'b102', 'b143', 'b154', 'b153', 'b36', 'b8', 'b127', 'b72', 'b98']","[True, True, True, True, True, True, True, True, True, True, True, True]",12,"1. For text generation tasks, MTL is brought in to improve the quality of the generated text.
2. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain.
3. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks.
4. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks.
5. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data.
6. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder.
7. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks.
8. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks.
9. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155].
10. (NMT)0 implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing.
11. (NMT)1 enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks.
12. (NMT)2 views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.","For text generation tasks, MTL is brought in to improve the quality of the generated text.",How does Multi-Task Learning (MTL) enhance neural machine translation and text generation quality?,"Question: How does Multi-Task Learning (MTL) enhance neural machine translation and text generation quality?

1. For text generation tasks, MTL is brought in to improve the quality of the generated text.
2. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain.
3. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks.
4. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks.
5. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data.
6. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder.
7. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks.
8. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks.
9. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155].
10. (NMT)0 implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing.
11. (NMT)1 enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks.
12. (NMT)2 views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.",1.0,0.5,0.0,0.5,1.0,1.0,0.5,0.6666666666666666,-0.1666666666666666,1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s14,p14.3,"Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.","['b125', 'b155', 'b113']","[True, True, True]",3,"1. Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks.
2. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks.
3. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation.
4. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.","Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks.",What are the approaches to building general-purpose text representations for future tasks?,"Question: What are the approaches to building general-purpose text representations for future tasks?

1. Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks.
2. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks.
3. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation.
4. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.",1.0,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s15,p15.0,"Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.","['b147', 'b71', 'b42', 'b144', 'b140', 'b39', 'b7', 'b92', 'b43', 'b150', 'b101', 'b109']","[True, True, True, True, True, True, True, True, True, True, True, True]",12,"1. Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously.
2. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other.
3. Table 2 gives an overview of task combinations used in joint MTL models.
4. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other.
5. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively.
6. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model.
7. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks.
8. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification.
9. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews.
10. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures.
11. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks.
12. [40,141,145] learn entity extraction alongside relation extraction.
13. For sentiment analysis tasks, [110]0 jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture.
14. [110]1 learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis.
15. [110]2 builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.","Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously.",How do joint MTL models differ from auxiliary MTL in optimizing task performance?,"Question: How do joint MTL models differ from auxiliary MTL in optimizing task performance?

1. Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously.
2. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other.
3. Table 2 gives an overview of task combinations used in joint MTL models.
4. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other.
5. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively.
6. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model.
7. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks.
8. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification.
9. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews.
10. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures.
11. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks.
12. [40,141,145] learn entity extraction alongside relation extraction.
13. For sentiment analysis tasks, [110]0 jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture.
14. [110]1 learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis.
15. [110]2 builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.",1.0,0.0,0.0,1.0,1.0,1.0,0.0,0.8666666666666667,-0.8666666666666667,1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s15,p15.2,"Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118]. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [32], Predicate-Argument Structures (PAS) [76], and Prague Semantic Dependencies (PSD) [42]) jointly. In [47], a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation (AMR) [4], Semantic Dependency Parsing (SDP) [88], and Universal Dependencies (UD) [87], and it shows that joint training improves performance on the testing UCCA dataset. [71] jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT. [29] shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks. In [143], an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.","['b117', 'b90', 'b46', 'b3', 'b59', 'b4', 'b31', 'b70', 'b40', 'b41', 'b87', 'b86', 'b82', 'b142', 'b75', 'b28', 'b130', 'b53', 'b5', 'b20', 'b115']","[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]",21,"1. Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks.
2. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions.
3. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118].
4. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces.
5. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [83]0, Predicate-Argument Structures [83]1 [83]2, and Prague Semantic Dependencies [83]3 [83]4) jointly.
6. In [83]5, a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation [83]6 [83]7, Semantic Dependency Parsing [83]8 [83]9, and Universal Dependencies [21]0 [21]1, and it shows that joint training improves performance on the testing UCCA dataset.
7. [21]2 jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT.
8. [21]3 shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks.
9. In [21]4, an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.","Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks.",What are the applications and benefits of joint MTL in multi-domain and multi-formalism NLP tasks?,"Question: What are the applications and benefits of joint MTL in multi-domain and multi-formalism NLP tasks?

1. Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks.
2. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions.
3. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118].
4. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces.
5. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [83]0, Predicate-Argument Structures [83]1 [83]2, and Prague Semantic Dependencies [83]3 [83]4) jointly.
6. In [83]5, a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation [83]6 [83]7, Semantic Dependency Parsing [83]8 [83]9, and Universal Dependencies [21]0 [21]1, and it shows that joint training improves performance on the testing UCCA dataset.
7. [21]2 jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT.
8. [21]3 shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks.
9. In [21]4, an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.",0.5,0.5,0.0,1.0,0.0,1.0,0.0,1.0,-0.5,1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s16,p16.0,"Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.","['b78', 'b126', 'b77']","[True, True, True]",3,"1. Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections.
2. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models.
3. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English.
4. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively.
5. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.",Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections.,How does multi-task learning benefit multi-lingual machine learning models in NLP?,"Question: How does multi-task learning benefit multi-lingual machine learning models in NLP?

1. Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections.
2. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models.
3. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English.
4. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively.
5. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.",1.0,0.5,1.0,1.0,1.0,1.0,0.0,1.0,-0.5,1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s16,p16.1,"Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.","['b118', 'b85']","[True, True]",2,"1. Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages.
2. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese.
3. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process.
4. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts.
5. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English.
6. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.",Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages.,How does multi-lingual MTL enhance knowledge transfer and improve language processing tasks?,"Question: How does multi-lingual MTL enhance knowledge transfer and improve language processing tasks?

1. Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages.
2. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese.
3. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process.
4. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts.
5. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English.
6. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.",1.0,0.5,0.5,1.0,1.0,1.0,0.0,1.0,-0.5,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s16,p16.2,"Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.","['b64', 'b107', 'b48']","[True, True, True]",3,"1. Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations.
2. [108] learns multi-lingual representations from two tasks.
3. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples.
4. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification.
5. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training.
6. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels.
7. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way.
8. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.",Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations.,How can cross-lingual knowledge transfer be achieved through learning language representations?,"Question: How can cross-lingual knowledge transfer be achieved through learning language representations?

1. Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations.
2. [108] learns multi-lingual representations from two tasks.
3. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples.
4. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification.
5. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training.
6. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels.
7. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way.
8. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.",1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s17,p17.0,"As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.",['b15'],[True],1,"1. As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community.
2. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks.
3. MTL is a natural choice for implicitly injecting multimodal features into a single model.
4. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language.
5. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.","As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community.",What is multimodal learning in NLP and how is it applied in speech translation?,"Question: What is multimodal learning in NLP and how is it applied in speech translation?

1. As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community.
2. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks.
3. MTL is a natural choice for implicitly injecting multimodal features into a single model.
4. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language.
5. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.",1.0,0.5,0.5,1.0,1.0,1.0,0.0,1.0,-0.5,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s17,p17.3,"In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.",['b114'],[True],1,"1. In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment.
2. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels.
3. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning.
4. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.","In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment.",How does a dual-attention mechanism enhance visual and textual alignment in AI models?,"Question: How does a dual-attention mechanism enhance visual and textual alignment in AI models?

1. In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment.
2. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels.
3. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning.
4. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.",0.5,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s18,p18.0,"A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.","['b76', 'b70', 'b52', 'b103']","[True, True, True, True]",4,"1. A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training.
2. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks.
3. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy.
4. This finding also holds for rumor verification [53].
5. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other.
6. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels.
7. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets.
8. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.",A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training.,What determines task suitability for multi-task learning in NLP?,"Question: What determines task suitability for multi-task learning in NLP?

1. A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training.
2. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks.
3. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy.
4. This finding also holds for rumor verification [53].
5. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other.
6. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels.
7. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets.
8. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.",1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s18,p18.1,"As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.","['b90', 'b53', 'b21', 'b9', 'b48']","[True, True, True, True, True]",5,"1. As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness.
2. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks.
3. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted.
4. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar.
5. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks.
6. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.","As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness.",How does multi-task learning (MTL) effectiveness relate to task relatedness and selection?,"Question: How does multi-task learning (MTL) effectiveness relate to task relatedness and selection?

1. As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness.
2. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks.
3. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted.
4. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar.
5. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks.
6. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.",1.0,0.0,0.5,1.0,1.0,1.0,0.0,1.0,-1.0,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s21,p21.1,"Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.","['b53', 'b28', 'b90', 'b120']","[True, True, True, True]",4,"1. Multi-label datasets can be created by giving extra manual annotations to existing data.
2. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input.
3. [121] labels Twitter posts with 4 demographic labels.
4. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.",Multi-label datasets can be created by giving extra manual annotations to existing data.,How can multi-label datasets be created from existing data?,"Question: How can multi-label datasets be created from existing data?

1. Multi-label datasets can be created by giving extra manual annotations to existing data.
2. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input.
3. [121] labels Twitter posts with 4 demographic labels.
4. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.",0.5,1.0,0.0,0.0,1.0,1.0,1.0,1.0,0.0,0.0
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s21,p21.2,"The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa.","['b63', 'b96', 'b30', 'b85', 'b125', 'b32', 'b106', 'b129', 'b124', 'b115', 'b23', 'b55', 'b61', 'b48']","[True, True, True, True, True, True, True, True, True, True, True, True, True, True]",14,"1. The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset.
2. Extra labels can be obtained using pre-defined rules [62,97].
3. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels.
4. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation.
5. [116] uses hashtags to represent genres of tweet posts.
6. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database.
7. Unicoder [49] uses translated text from the source language to fine-tune on the target language.
8. [125] creates disfluent sentences by randomly repeating or inserting -grams.
9. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models.
10. [107] obtains dominant word sense labels from WordNet [31].
11. [56]0 applies entity linking for QA data over databases through an entity linker.
12. [56]1 assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method.
13. [56]2 uses the output of a meta-network as labels for unsupervised training data.
14. As a special case of multi-label dataset, mask orchestration [56]3 provides different parts of an instance to different tasks by applying different masks.
15. That is, labels for one task may become the input for another task and vice versa.","The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset.",How can extra annotations be automatically generated for self-supervised multi-label datasets?,"Question: How can extra annotations be automatically generated for self-supervised multi-label datasets?

1. The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset.
2. Extra labels can be obtained using pre-defined rules [62,97].
3. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels.
4. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation.
5. [116] uses hashtags to represent genres of tweet posts.
6. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database.
7. Unicoder [49] uses translated text from the source language to fine-tune on the target language.
8. [125] creates disfluent sentences by randomly repeating or inserting -grams.
9. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models.
10. [107] obtains dominant word sense labels from WordNet [31].
11. [56]0 applies entity linking for QA data over databases through an entity linker.
12. [56]1 assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method.
13. [56]2 uses the output of a meta-network as labels for unsupervised training data.
14. As a special case of multi-label dataset, mask orchestration [56]3 provides different parts of an instance to different tasks by applying different masks.
15. That is, labels for one task may become the input for another task and vice versa.",0.5,0.0,0.0,0.5,1.0,1.0,0.5,1.0,-1.0,1.0
249642175,Multimodal Learning with Transformers: A Survey,"Computer Science, Medicine",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s8,p8.0,"Tokenization Vanilla Transformer was originally proposed for machine translation as a sequence-to-sequence model, thus it is straightforward to take the vocabulary sequences as input. As mentioned previously, the original selfattention can model an arbitrary input as a fully-connected graph, independently of modalities. Specifically, both Vanilla and variant Transformers take in the tokenized sequences, where each token can be regarded as a node of the graph. 1. In this survey, ""multimodal Transformer"" means ""Transformer in multimodal learning context"".",['b0'],[True],1,"1. Tokenization Vanilla Transformer was originally proposed for machine translation as a sequence-to-sequence model, thus it is straightforward to take the vocabulary sequences as input.
2. As mentioned previously, the original selfattention can model an arbitrary input as a fully-connected graph, independently of modalities.
3. Specifically, both Vanilla and variant Transformers take in the tokenized sequences, where each token can be regarded as a node of the graph.
4. 1. In this survey, ""multimodal Transformer"" means ""Transformer in multimodal learning context"".","Tokenization Vanilla Transformer was originally proposed for machine translation as a sequence-to-sequence model, thus it is straightforward to take the vocabulary sequences as input.",What is the role of tokenization in the original Transformer model for machine translation?,"Question: What is the role of tokenization in the original Transformer model for machine translation?

1. Tokenization Vanilla Transformer was originally proposed for machine translation as a sequence-to-sequence model, thus it is straightforward to take the vocabulary sequences as input.
2. As mentioned previously, the original selfattention can model an arbitrary input as a fully-connected graph, independently of modalities.
3. Specifically, both Vanilla and variant Transformers take in the tokenized sequences, where each token can be regarded as a node of the graph.
4. 1. In this survey, ""multimodal Transformer"" means ""Transformer in multimodal learning context"".",0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.75,0.25,0.0
249642175,Multimodal Learning with Transformers: A Survey,"Computer Science, Medicine",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s10,p10.0,"Transformers is an open problem. It can be understood as a kind of implicit coordinate basis of feature space, to provide temporal or spatial information to the Transformer. For cloud point [164] and sketch drawing stroke [163], their token element is already a coordinate, meaning that position embedding is optional, not necessary. Furthermore, position embedding can be regarded as a kind of general additional information. In other words, from a mathematical point of view, any additional information can be added, such as detail of the manner of position embedding, e.g., the pen state of sketch drawing stroke [163], cameras and viewpoints in surveillance [165]. There is a comprehensive survey [166] discussing the position information in Transformers. For both sentence structures (sequential) and general graph structures (sparse, arbitrary, and irregular), position embeddings help Transformers to learn or encode the underlying structures. Considered from the mathematical perspective of self-attention, i.e., scaled dot-product attention, attentions are invariant to the positions of words (in text) or nodes (in graphs), if position embedding information is missing. Thus, in most cases, position embedding is necessary for Transformers.","['b165', 'b163', 'b162', 'b164']","[True, True, True, True]",4,"1. Transformers is an open problem.
2. It can be understood as a kind of implicit coordinate basis of feature space, to provide temporal or spatial information to the Transformer.
3. For cloud point [164] and sketch drawing stroke [163], their token element is already a coordinate, meaning that position embedding is optional, not necessary.
4. Furthermore, position embedding can be regarded as a kind of general additional information.
5. In other words, from a mathematical point of view, any additional information can be added, such as detail of the manner of position embedding, e.g., the pen state of sketch drawing stroke [163], cameras and viewpoints in surveillance [165].
6. There is a comprehensive survey [166] discussing the position information in Transformers.
7. For both sentence structures (sequential) and general graph structures (sparse, arbitrary, and irregular), position embeddings help Transformers to learn or encode the underlying structures.
8. Considered from the mathematical perspective of self-attention, i.e., scaled dot-product attention, attentions are invariant to the positions of words (in text) or nodes (in graphs), if position embedding information is missing.
9. Thus, in most cases, position embedding is necessary for Transformers.",Transformers is an open problem.,How do position embeddings enhance Transformers in processing various data structures?,"Question: How do position embeddings enhance Transformers in processing various data structures?

1. Transformers is an open problem.
2. It can be understood as a kind of implicit coordinate basis of feature space, to provide temporal or spatial information to the Transformer.
3. For cloud point [164] and sketch drawing stroke [163], their token element is already a coordinate, meaning that position embedding is optional, not necessary.
4. Furthermore, position embedding can be regarded as a kind of general additional information.
5. In other words, from a mathematical point of view, any additional information can be added, such as detail of the manner of position embedding, e.g., the pen state of sketch drawing stroke [163], cameras and viewpoints in surveillance [165].
6. There is a comprehensive survey [166] discussing the position information in Transformers.
7. For both sentence structures (sequential) and general graph structures (sparse, arbitrary, and irregular), position embeddings help Transformers to learn or encode the underlying structures.
8. Considered from the mathematical perspective of self-attention, i.e., scaled dot-product attention, attentions are invariant to the positions of words (in text) or nodes (in graphs), if position embedding information is missing.
9. Thus, in most cases, position embedding is necessary for Transformers.",0.5,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,1.0
249642175,Multimodal Learning with Transformers: A Survey,"Computer Science, Medicine",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s21,p21.12,"Thus, all the multimodal token positions can be attended as a whole sequence, such that the positions of each modality can be encoded well by conditioning the context of other modalities. VideoBERT [7] is the one of the first multimodal Transformer works, where video and text are fused via early concatenation that can encode the global multimodal context well [188]. However, the longer sequence after concatenation will increase computational complexity. Early concatenation is also termed ""all-attention"" or ""Co-Transformer"" [137].","['b6', 'b136', 'b187']","[True, True, True]",3,"1. Thus, all the multimodal token positions can be attended as a whole sequence, such that the positions of each modality can be encoded well by conditioning the context of other modalities.
2. VideoBERT [7] is the one of the first multimodal Transformer works, where video and text are fused via early concatenation that can encode the global multimodal context well [188].
3. However, the longer sequence after concatenation will increase computational complexity.
4. Early concatenation is also termed ""all-attention"" or ""Co-Transformer"" [137].","Thus, all the multimodal token positions can be attended as a whole sequence, such that the positions of each modality can be encoded well by conditioning the context of other modalities.",How does VideoBERT encode multimodal contexts and what are its computational implications?,"Question: How does VideoBERT encode multimodal contexts and what are its computational implications?

1. Thus, all the multimodal token positions can be attended as a whole sequence, such that the positions of each modality can be encoded well by conditioning the context of other modalities.
2. VideoBERT [7] is the one of the first multimodal Transformer works, where video and text are fused via early concatenation that can encode the global multimodal context well [188].
3. However, the longer sequence after concatenation will increase computational complexity.
4. Early concatenation is also termed ""all-attention"" or ""Co-Transformer"" [137].",0.0,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,0.0
249642175,Multimodal Learning with Transformers: A Survey,"Computer Science, Medicine",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s23,p23.1,"Discussion All these aforementioned self-attention variants for multimodal interactions are modality-generic, and can be applied in flexible strategies and for multi-granular tasks. Specifically, these interactions can be flexibly combined and nested. For instance, multiple cross-attention streams are used in hierarchical attention (one-stream to multi-stream) that in a two-stream decoupled model [191] T f 2 and T f 3 of Eq. 11 are implemented by cross-attention defined in Eq. 12. Moreover, they can be extended to multiple (≥ 3) modalities. TriBERT [183] is a tri-modal cross-attention (coattention) for vision, pose, and audio, where given a Query embedding, its Key and Value embeddings are the concatenation from the other modalities. Cross-attention to concatenation is applied to three modalities (i.e., language, video, and audio) in [189].","['b182', 'b190', 'b188']","[True, True, True]",3,"1. Discussion All these aforementioned self-attention variants for multimodal interactions are modality-generic, and can be applied in flexible strategies and for multi-granular tasks.
2. Specifically, these interactions can be flexibly combined and nested.
3. For instance, multiple cross-attention streams are used in hierarchical attention (one-stream to multi-stream) that in a two-stream decoupled model [191] T f 2 and T f 3 of Eq. 11 are implemented by cross-attention defined in Eq. 12.
4. Moreover, they can be extended to multiple (≥ 3) modalities.
5. TriBERT [183] is a tri-modal cross-attention (coattention) for vision, pose, and audio, where given a Query embedding, its Key and Value embeddings are the concatenation from the other modalities.
6. Cross-attention to concatenation is applied to three modalities (i.e., language, video, and audio) in [189].","Discussion All these aforementioned self-attention variants for multimodal interactions are modality-generic, and can be applied in flexible strategies and for multi-granular tasks.",How do self-attention variants facilitate multimodal interactions in AI models?,"Question: How do self-attention variants facilitate multimodal interactions in AI models?

1. Discussion All these aforementioned self-attention variants for multimodal interactions are modality-generic, and can be applied in flexible strategies and for multi-granular tasks.
2. Specifically, these interactions can be flexibly combined and nested.
3. For instance, multiple cross-attention streams are used in hierarchical attention (one-stream to multi-stream) that in a two-stream decoupled model [191] T f 2 and T f 3 of Eq. 11 are implemented by cross-attention defined in Eq. 12.
4. Moreover, they can be extended to multiple (≥ 3) modalities.
5. TriBERT [183] is a tri-modal cross-attention (coattention) for vision, pose, and audio, where given a Query embedding, its Key and Value embeddings are the concatenation from the other modalities.
6. Cross-attention to concatenation is applied to three modalities (i.e., language, video, and audio) in [189].",1.0,0.0,0.0,0.5,0.0,1.0,0.5,1.0,-1.0,0.0
249642175,Multimodal Learning with Transformers: A Survey,"Computer Science, Medicine",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s30,p30.0,"In spite of the recent advances, multimodal pretraining Transformer methods still have some obvious bottlenecks. For instance, as discussed by [208] in VLP field, while the BERT-style cross-modal pretraining models produce excellent results on various down-stream visionlanguage tasks, they fail to be applied to generative tasks directly. As discussed in [208], both VideoBERT [7] and CBT [107] have to train a separate video-to-text decoder for video captioning. This is a significant gap between the pretraining models designed for discriminative and generative tasks, as the main reason is discriminative task oriented pretraining models do not involve the decoders of Transformer. Therefore, how to design more unified pipelines that can work for both discriminative and generative down-stream tasks is also an open problem to be solved. Again for instance, common multimodal pretraining models often underperform for fine-grained/instance-level tasks as discussed by [137].","['b106', 'b207', 'b6', 'b136']","[True, True, True, True]",4,"1. In spite of the recent advances, multimodal pretraining Transformer methods still have some obvious bottlenecks.
2. For instance, as discussed by [208] in VLP field, while the BERT-style cross-modal pretraining models produce excellent results on various down-stream visionlanguage tasks, they fail to be applied to generative tasks directly.
3. As discussed in [208], both VideoBERT [7] and CBT [107] have to train a separate video-to-text decoder for video captioning.
4. This is a significant gap between the pretraining models designed for discriminative and generative tasks, as the main reason is discriminative task oriented pretraining models do not involve the decoders of Transformer.
5. Therefore, how to design more unified pipelines that can work for both discriminative and generative down-stream tasks is also an open problem to be solved.
6. Again for instance, common multimodal pretraining models often underperform for fine-grained/instance-level tasks as discussed by [137].","In spite of the recent advances, multimodal pretraining Transformer methods still have some obvious bottlenecks.",What are the limitations of current multimodal pretraining Transformer methods?,"Question: What are the limitations of current multimodal pretraining Transformer methods?

1. In spite of the recent advances, multimodal pretraining Transformer methods still have some obvious bottlenecks.
2. For instance, as discussed by [208] in VLP field, while the BERT-style cross-modal pretraining models produce excellent results on various down-stream visionlanguage tasks, they fail to be applied to generative tasks directly.
3. As discussed in [208], both VideoBERT [7] and CBT [107] have to train a separate video-to-text decoder for video captioning.
4. This is a significant gap between the pretraining models designed for discriminative and generative tasks, as the main reason is discriminative task oriented pretraining models do not involve the decoders of Transformer.
5. Therefore, how to design more unified pipelines that can work for both discriminative and generative down-stream tasks is also an open problem to be solved.
6. Again for instance, common multimodal pretraining models often underperform for fine-grained/instance-level tasks as discussed by [137].",1.0,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,0.0
249642175,Multimodal Learning with Transformers: A Survey,"Computer Science, Medicine",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s35,p35.0,"In general, MML Transformers fuse information across multiple modalities primarily at three levels: input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion). Common early fusion based MML Transformer models [7], [104], [108] are also known as one-stream architecture, allowing the adoption of the merits of BERT due to minimal architectural modification. The main difference between these one-stream models is the usage of problem-specific modalities with variant masking techniques. With attention operation, a noticeable fusion scheme is introduced based on a notion of bottleneck tokens [175]. It applies for both early and middle fusion by simply choosing to-be-fused layers. We note that the simple prediction-based late fusion [247], [248] is less adopted in MML Transformers. This makes sense considering the motivations of learning stronger multimodal contextual representations and great advance of computing power. For enhancing and interpreting the fusion of MML, probing the interaction and measuring the fusion between modalities [249] would be an interesting direction to explore.","['b246', 'b6', 'b247', 'b174', 'b248', 'b107', 'b103']","[True, True, True, True, True, True, True]",7,"1. In general, MML Transformers fuse information across multiple modalities primarily at three levels: input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion).
2. Common early fusion based MML Transformer models [7], [104], [108] are also known as one-stream architecture, allowing the adoption of the merits of BERT due to minimal architectural modification.
3. The main difference between these one-stream models is the usage of problem-specific modalities with variant masking techniques.
4. With attention operation, a noticeable fusion scheme is introduced based on a notion of bottleneck tokens [175].
5. It applies for both early and middle fusion by simply choosing to-be-fused layers.
6. We note that the simple prediction-based late fusion [247], [248] is less adopted in MML Transformers.
7. This makes sense considering the motivations of learning stronger multimodal contextual representations and great advance of computing power.
8. For enhancing and interpreting the fusion of MML, probing the interaction and measuring the fusion between modalities [249] would be an interesting direction to explore.","In general, MML Transformers fuse information across multiple modalities primarily at three levels: input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion).",How do MML Transformers integrate information across different modalities?,"Question: How do MML Transformers integrate information across different modalities?

1. In general, MML Transformers fuse information across multiple modalities primarily at three levels: input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion).
2. Common early fusion based MML Transformer models [7], [104], [108] are also known as one-stream architecture, allowing the adoption of the merits of BERT due to minimal architectural modification.
3. The main difference between these one-stream models is the usage of problem-specific modalities with variant masking techniques.
4. With attention operation, a noticeable fusion scheme is introduced based on a notion of bottleneck tokens [175].
5. It applies for both early and middle fusion by simply choosing to-be-fused layers.
6. We note that the simple prediction-based late fusion [247], [248] is less adopted in MML Transformers.
7. This makes sense considering the motivations of learning stronger multimodal contextual representations and great advance of computing power.
8. For enhancing and interpreting the fusion of MML, probing the interaction and measuring the fusion between modalities [249] would be an interesting direction to explore.",1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.875,0.125,1.0
249642175,Multimodal Learning with Transformers: A Survey,"Computer Science, Medicine",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s37,p37.2,"In practice, the distribution gap between training data and practical data is noticeable. For instance, supervised data samples (well-labelled, well-aligned) are costly in practical applications, thus how to transfer the supervised multimodal Transformers pretrained on well-aligned cross-modal pairs/tuples to the weakly aligned test bed is challenging [137]. CLIP [9] is an inspiring solution that transfers knowledge across modalities by learning a shared multimodal embedding space, enabling zero-shot transfer of the model to down-stream tasks. The main inspiration that CLIP presents the community is that the pretrained multimodal (image and text) knowledge can be transferred to down-stream zero-shot image prediction by using a prompt template ""A photo of a {label}."" to bridge the distribution gap between training and test datasets.","['b8', 'b136']","[True, True]",2,"1. In practice, the distribution gap between training data and practical data is noticeable.
2. For instance, supervised data samples (well-labelled, well-aligned) are costly in practical applications, thus how to transfer the supervised multimodal Transformers pretrained on well-aligned cross-modal pairs/tuples to the weakly aligned test bed is challenging [137].
3. CLIP [9] is an inspiring solution that transfers knowledge across modalities by learning a shared multimodal embedding space, enabling zero-shot transfer of the model to down-stream tasks.
4. The main inspiration that CLIP presents the community is that the pretrained multimodal (image and text) knowledge can be transferred to down-stream zero-shot image prediction by using a prompt template ""A photo of a {label}.""
5. to bridge the distribution gap between training and test datasets.","In practice, the distribution gap between training data and practical data is noticeable.",How does CLIP address the distribution gap between training and practical data?,"Question: How does CLIP address the distribution gap between training and practical data?

1. In practice, the distribution gap between training data and practical data is noticeable.
2. For instance, supervised data samples (well-labelled, well-aligned) are costly in practical applications, thus how to transfer the supervised multimodal Transformers pretrained on well-aligned cross-modal pairs/tuples to the weakly aligned test bed is challenging [137].
3. CLIP [9] is an inspiring solution that transfers knowledge across modalities by learning a shared multimodal embedding space, enabling zero-shot transfer of the model to down-stream tasks.
4. The main inspiration that CLIP presents the community is that the pretrained multimodal (image and text) knowledge can be transferred to down-stream zero-shot image prediction by using a prompt template ""A photo of a {label}.""
5. to bridge the distribution gap between training and test datasets.",1.0,1.0,0.5,1.0,1.0,1.0,0.0,0.6,0.4,0.0
249642175,Multimodal Learning with Transformers: A Survey,"Computer Science, Medicine",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s38,p38.0,"Multimodal Transformers suffer from two major efficiency issues: (1) Due to the large model parameter capacity, they are data hungry and thus dependent on huge scale training datasets. (2) They are limited by the time and memory complexities that grow quadratically with the input sequence length, which are caused by the self-attention. In multimodal contexts, calculation explosion will become worse due to jointly high dimension representations. These two bottlenecks are interdependent and should be considered together.",['b0'],[True],1,"1. Multimodal Transformers suffer from two major efficiency issues: (1)
2. Due to the large model parameter capacity, they are data hungry and thus dependent on huge scale training datasets.
3. (2) They are limited by the time and memory complexities that grow quadratically with the input sequence length, which are caused by the self-attention.
4. In multimodal contexts, calculation explosion will become worse due to jointly high dimension representations.
5. These two bottlenecks are interdependent and should be considered together.",Multimodal Transformers suffer from two major efficiency issues: (1),What are the two major efficiency issues faced by Multimodal Transformers?,"Question: What are the two major efficiency issues faced by Multimodal Transformers?

1. Multimodal Transformers suffer from two major efficiency issues: (1)
2. Due to the large model parameter capacity, they are data hungry and thus dependent on huge scale training datasets.
3. (2) They are limited by the time and memory complexities that grow quadratically with the input sequence length, which are caused by the self-attention.
4. In multimodal contexts, calculation explosion will become worse due to jointly high dimension representations.
5. These two bottlenecks are interdependent and should be considered together.",0.0,1.0,1.0,1.0,1.0,1.0,0.0,0.6,0.4,0.0
249642175,Multimodal Learning with Transformers: A Survey,"Computer Science, Medicine",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s42,p42.3,"Identifying the strengths of Transformers for multimodal machine learning is a big open problem. The following main points can be summarized from the literature: (1) Transformers can encode implicit knowledge [32]. (2) The multi-head brings multiple modelling sub-spaces that can further enhance the expressive ability of the model. Ideally, multiple heads after training are good and different. This is essentially a good practice of ensemble learning.",['b31'],[True],1,"1. Identifying the strengths of Transformers for multimodal machine learning is a big open problem.
2. The following main points can be summarized from the literature: (1) Transformers can encode implicit knowledge [32].
3. (2) The multi-head brings multiple modelling sub-spaces that can further enhance the expressive ability of the model.
4. Ideally, multiple heads after training are good and different.
5. This is essentially a good practice of ensemble learning.",Identifying the strengths of Transformers for multimodal machine learning is a big open problem.,What are the strengths of Transformers in multimodal machine learning?,"Question: What are the strengths of Transformers in multimodal machine learning?

1. Identifying the strengths of Transformers for multimodal machine learning is a big open problem.
2. The following main points can be summarized from the literature: (1) Transformers can encode implicit knowledge [32].
3. (2) The multi-head brings multiple modelling sub-spaces that can further enhance the expressive ability of the model.
4. Ideally, multiple heads after training are good and different.
5. This is essentially a good practice of ensemble learning.",1.0,1.0,0.0,0.5,1.0,1.0,0.5,0.8,0.1999999999999999,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s10,p10.0,"Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade. This is because deep learning models can learn features automatically which eliminates the requirement of expensive feature engineering and process the inputs in an end-to-end manner i.e., take raw inputs and give the decisions. The success of the deep learning algorithms comes from the knowledge gained during training from human-labeled instances. However, supervised learning has a lot of limitations a) with less data, the model may get overfitted and prone to bias b) some of the domains like biomedical are supervision starved i.e., difficult to get labeled data. In general, we expect models to be close to human intelligence i.e., more general and make decisions with just a few samples. This desire of developing models with more generalization ability and learning from fewer samples has made the researchers focus on other learning paradigms like Self-Supervised Learning [34].",['b33'],[True],1,"1. Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade.
2. This is because deep learning models can learn features automatically which eliminates the requirement of expensive feature engineering and process the inputs in an end-to-end manner i.e., take raw inputs and give the decisions.
3. The success of the deep learning algorithms comes from the knowledge gained during training from human-labeled instances.
4. However, supervised learning has a lot of limitations a) with less data, the model may get overfitted and prone to bias b) some of the domains like biomedical are supervision starved i.e., difficult to get labeled data.
5. In general, we expect models to be close to human intelligence i.e., more general and make decisions with just a few samples.
6. This desire of developing models with more generalization ability and learning from fewer samples has made the researchers focus on other learning paradigms like Self-Supervised Learning [34].",Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade.,Why did deep learning algorithms outperform rule-based and machine learning algorithms in the last decade?,"Question: Why did deep learning algorithms outperform rule-based and machine learning algorithms in the last decade?

1. Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade.
2. This is because deep learning models can learn features automatically which eliminates the requirement of expensive feature engineering and process the inputs in an end-to-end manner i.e., take raw inputs and give the decisions.
3. The success of the deep learning algorithms comes from the knowledge gained during training from human-labeled instances.
4. However, supervised learning has a lot of limitations a) with less data, the model may get overfitted and prone to bias b) some of the domains like biomedical are supervision starved i.e., difficult to get labeled data.
5. In general, we expect models to be close to human intelligence i.e., more general and make decisions with just a few samples.
6. This desire of developing models with more generalization ability and learning from fewer samples has made the researchers focus on other learning paradigms like Self-Supervised Learning [34].",1.0,0.5,1.0,1.0,1.0,1.0,0.0,0.8333333333333334,-0.3333333333333333,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s13,p13.1,"Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs. It is also referred to as further pretraining. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7). For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles [16]. In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].","['b15', 'b17']","[True, True]",2,"1. Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs.
2. It is also referred to as further pretraining.
3. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7).
4. For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles [16].
5. In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].",Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs.,What is Continual Pretraining in biomedical NLP research?,"Question: What is Continual Pretraining in biomedical NLP research?

1. Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs.
2. It is also referred to as further pretraining.
3. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7).
4. For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles [16].
5. In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].",1.0,0.0,0.0,1.0,1.0,1.0,0.0,0.8,-0.8,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s16,p16.0,"During pretraining, the language models learn language representations based on the supervision provided by one or more pretraining tasks. A pretraining task is a pseudo-supervised task whose labels are generated automatically. A pretraining task can be main or auxiliary. The main pretraining tasks allow the model to learn language representations while auxiliary pretraining tasks allow the model to gain knowledge from human-curated sources like Ontology [34], [45]- [47]. The classification of pretraining tasks is given in Figure 12 and a brief summary of various pretraining tasks is presented in Table 1.","['b46', 'b44', 'b33']","[True, True, True]",3,"1. During pretraining, the language models learn language representations based on the supervision provided by one or more pretraining tasks.
2. A pretraining task is a pseudo-supervised task whose labels are generated automatically.
3. A pretraining task can be main or auxiliary.
4. The main pretraining tasks allow the model to learn language representations while auxiliary pretraining tasks allow the model to gain knowledge from human-curated sources like Ontology [34], [45]- [47].
5. The classification of pretraining tasks is given in Figure 12 and a brief summary of various pretraining tasks is presented in Table 1.","During pretraining, the language models learn language representations based on the supervision provided by one or more pretraining tasks.",What do language models learn during pretraining and how are pretraining tasks classified?,"Question: What do language models learn during pretraining and how are pretraining tasks classified?

1. During pretraining, the language models learn language representations based on the supervision provided by one or more pretraining tasks.
2. A pretraining task is a pseudo-supervised task whose labels are generated automatically.
3. A pretraining task can be main or auxiliary.
4. The main pretraining tasks allow the model to learn language representations while auxiliary pretraining tasks allow the model to gain knowledge from human-curated sources like Ontology [34], [45]- [47].
5. The classification of pretraining tasks is given in Figure 12 and a brief summary of various pretraining tasks is presented in Table 1.",1.0,0.0,0.0,0.5,0.0,1.0,0.5,0.8,-0.8,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s17,p17.1,"Masked Language Modeling (MLM). It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens [2]. The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens. However, the meaning of a word depends on both the left and right contexts. Devlin et al. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model. Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI. Less challenging as it involves topic prediction which is a relatively easy task.",['b1'],[True],1,"1. Masked Language Modeling (MLM).
2. It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens [2].
3. The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens.
4. However, the meaning of a word depends on both the left and right contexts.
5. Devlin et al. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model.
6. Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI.
7. Less challenging as it involves topic prediction which is a relatively easy task.",Masked Language Modeling (MLM).,What is Masked Language Modeling and how does it improve upon Unidirectional LM?,"Question: What is Masked Language Modeling and how does it improve upon Unidirectional LM?

1. Masked Language Modeling (MLM).
2. It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens [2].
3. The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens.
4. However, the meaning of a word depends on both the left and right contexts.
5. Devlin et al. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model.
6. Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI.
7. Less challenging as it involves topic prediction which is a relatively easy task.",0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s21,p21.0,"IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets. IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]- [58]. For example, Cengiz et al. [55] fine-tuned indomain model on general NLI datasets like SNLI [59] and MNLI [60] before fine-tuning on MedNLI [61].","['b60', 'b59', 'b54', 'b58', 'b57']","[True, True, True, True, True]",5,"1. IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets.
2. IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains.
3. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]- [58].
4. For example, Cengiz et al. [55] fine-tuned indomain model on general NLI datasets like SNLI [59] and MNLI [60] before fine-tuning on MedNLI [61].","IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets.",How does IFT improve model performance on small target datasets?,"Question: How does IFT improve model performance on small target datasets?

1. IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets.
2. IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains.
3. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]- [58].
4. For example, Cengiz et al. [55] fine-tuned indomain model on general NLI datasets like SNLI [59] and MNLI [60] before fine-tuning on MedNLI [61].",1.0,0.5,0.0,1.0,1.0,1.0,0.0,1.0,-0.5,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s24,p24.1,"Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only. Each character is represented using an embedding. These embeddings are initialized randomly and learned during model pretraining. ELMo embedding model uses CharCNN to generate word representations from character embeddings [74]. Inspired by ELMo, BioCharBERT also uses CharCNN on the top of character embeddings to generate word representations [28]. AlphaBERT [75] also uses character embeddings. Unlike CharacterBERT, AlphaBERT directly combines character embeddings with position embeddings and then applies a stack of transformer encoders. The main advantage with character embeddings is the small size of vocabulary as it includes only characters. The disadvantage is longer pretraining times [28]. As the sequence length increases with character level embeddings, models are slow to pre-train.","['b27', 'b75', 'b74']","[True, True, True]",3,"1. Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only.
2. Each character is represented using an embedding.
3. These embeddings are initialized randomly and learned during model pretraining.
4. ELMo embedding model uses CharCNN to generate word representations from character embeddings [74].
5. Inspired by ELMo, BioCharBERT also uses CharCNN on the top of character embeddings to generate word representations [28]. AlphaBERT [75] also uses character embeddings.
6. Unlike CharacterBERT, AlphaBERT directly combines character embeddings with position embeddings and then applies a stack of transformer encoders.
7. The main advantage with character embeddings is the small size of vocabulary as it includes only characters.
8. The disadvantage is longer pretraining times [28].
9. As the sequence length increases with character level embeddings, models are slow to pre-train.","Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only.",What are character embeddings and their advantages and disadvantages in model pretraining?,"Question: What are character embeddings and their advantages and disadvantages in model pretraining?

1. Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only.
2. Each character is represented using an embedding.
3. These embeddings are initialized randomly and learned during model pretraining.
4. ELMo embedding model uses CharCNN to generate word representations from character embeddings [74].
5. Inspired by ELMo, BioCharBERT also uses CharCNN on the top of character embeddings to generate word representations [28]. AlphaBERT [75] also uses character embeddings.
6. Unlike CharacterBERT, AlphaBERT directly combines character embeddings with position embeddings and then applies a stack of transformer encoders.
7. The main advantage with character embeddings is the small size of vocabulary as it includes only characters.
8. The disadvantage is longer pretraining times [28].
9. As the sequence length increases with character level embeddings, models are slow to pre-train.",0.5,1.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s24,p24.2,"Subword Embeddings-In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words. The main principle driving the subword embedding vocabulary construction is that frequent words should be represented as a single word and rare words should be represented in terms of meaningful subwords. Subword embedding vocabularies are always moderate in size as they use sub-words to represent rare and misspelled words. Some of the popular algorithms to generate vocabulary for sub-word embeddings are Byte-Pair Encoding (BPE) [76], Byte-Level BPE [77], Word-Piece [30], Unigram [78], and Sentencepiece [79].","['b78', 'b76', 'b79', 'b29', 'b77']","[True, True, True, True, True]",5,"1. Subword Embeddings-In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words.
2. The main principle driving the subword embedding vocabulary construction is that frequent words should be represented as a single word and rare words should be represented in terms of meaningful subwords.
3. Subword embedding vocabularies are always moderate in size as they use sub-words to represent rare and misspelled words.
4. Some of the popular algorithms to generate vocabulary for sub-word embeddings are Byte-Pair Encoding (BPE)
5. [76], Byte-Level BPE [77], Word-Piece [30], Unigram [78], and Sentencepiece [79].","Subword Embeddings-In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words.",What are the principles and popular algorithms of subword embeddings?,"Question: What are the principles and popular algorithms of subword embeddings?

1. Subword Embeddings-In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words.
2. The main principle driving the subword embedding vocabulary construction is that frequent words should be represented as a single word and rare words should be represented in terms of meaningful subwords.
3. Subword embedding vocabularies are always moderate in size as they use sub-words to represent rare and misspelled words.
4. Some of the popular algorithms to generate vocabulary for sub-word embeddings are Byte-Pair Encoding (BPE)
5. [76], Byte-Level BPE [77], Word-Piece [30], Unigram [78], and Sentencepiece [79].",0.5,1.0,0.0,0.5,1.0,1.0,0.5,0.8,0.1999999999999999,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s24,p24.5,"SentencePiece [79] -A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space. However, this assumption is not applicable in all languages. To overcome this, SentencePiece treats space as a character and includes it in the base vocabulary. The final vocabulary is generated iteratively using BPE or Unigram. XLNet [81], ALBERT [15], and T5 [4] models use SentencePiece embeddings.","['b79', 'b14', 'b81']","[True, True, True]",3,"1. SentencePiece [79] -A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space.
2. However, this assumption is not applicable in all languages.
3. To overcome this, SentencePiece treats space as a character and includes it in the base vocabulary.
4. The final vocabulary is generated iteratively using BPE or Unigram.
5. XLNet [81], ALBERT [15], and T5 [4] models use SentencePiece embeddings.",SentencePiece [79] -A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space.,How does SentencePiece address space assumption issues in tokenization for various languages?,"Question: How does SentencePiece address space assumption issues in tokenization for various languages?

1. SentencePiece [79] -A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space.
2. However, this assumption is not applicable in all languages.
3. To overcome this, SentencePiece treats space as a character and includes it in the base vocabulary.
4. The final vocabulary is generated iteratively using BPE or Unigram.
5. XLNet [81], ALBERT [15], and T5 [4] models use SentencePiece embeddings.",1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s24,p24.6,"Unigram [78] -Unigram is similar to BPE and Word-Piece in fixing the vocabulary size at the beginning itself. BPE and Word-piece start with a small base vocabulary and then augments the base vocabulary for a certain number of iterations. Unlike BPE and Word-Piece, Unigram starts with a large base vocabulary and then iteratively trims the symbols to arrive at a small final vocabulary. It is not used directly in any of the models. SentencePiece uses the Unigram algorithm to generate the final vocabulary.",['b78'],[True],1,"1. Unigram [78] -Unigram is similar to BPE and Word-Piece in fixing the vocabulary size at the beginning itself.
2. BPE and Word-piece start with a small base vocabulary and then augments the base vocabulary for a certain number of iterations.
3. Unlike BPE and Word-Piece, Unigram starts with a large base vocabulary and then iteratively trims the symbols to arrive at a small final vocabulary.
4. It is not used directly in any of the models.
5. SentencePiece uses the Unigram algorithm to generate the final vocabulary.",Unigram [78] -Unigram is similar to BPE and Word-Piece in fixing the vocabulary size at the beginning itself.,How does Unigram's approach to vocabulary size differ from BPE and Word-Piece?,"Question: How does Unigram's approach to vocabulary size differ from BPE and Word-Piece?

1. Unigram [78] -Unigram is similar to BPE and Word-Piece in fixing the vocabulary size at the beginning itself.
2. BPE and Word-piece start with a small base vocabulary and then augments the base vocabulary for a certain number of iterations.
3. Unlike BPE and Word-Piece, Unigram starts with a large base vocabulary and then iteratively trims the symbols to arrive at a small final vocabulary.
4. It is not used directly in any of the models.
5. SentencePiece uses the Unigram algorithm to generate the final vocabulary.",0.5,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s24,p24.7,"Code embeddings -Code embeddings map the given sequence of codes into a sequence of vectors. For example, in the case of models like BERT-EHR [32], MedBERT [31], and BEHRT [27], the input is not a sequence of words. Instead, input is patient visits. Each patient visit is represented as a sequence of codes. The number of code embeddings varies from model to model. For example, MedBERT and BEHRT include embeddings only for disease codes while BERT-EHR includes embeddings for disease, medication, procedure, and clinical notes.","['b26', 'b30', 'b31']","[True, True, True]",3,"1. Code embeddings -Code embeddings map the given sequence of codes into a sequence of vectors.
2. For example, in the case of models like BERT-EHR [32], MedBERT [31], and BEHRT [27], the input is not a sequence of words.
3. Instead, input is patient visits.
4. Each patient visit is represented as a sequence of codes.
5. The number of code embeddings varies from model to model.
6. For example, MedBERT and BEHRT include embeddings only for disease codes while BERT-EHR includes embeddings for disease, medication, procedure, and clinical notes.",Code embeddings -Code embeddings map the given sequence of codes into a sequence of vectors.,"What are code embeddings and how do they vary among models like BERT-EHR, MedBERT, and BEHRT?","Question: What are code embeddings and how do they vary among models like BERT-EHR, MedBERT, and BEHRT?

1. Code embeddings -Code embeddings map the given sequence of codes into a sequence of vectors.
2. For example, in the case of models like BERT-EHR [32], MedBERT [31], and BEHRT [27], the input is not a sequence of words.
3. Instead, input is patient visits.
4. Each patient visit is represented as a sequence of codes.
5. The number of code embeddings varies from model to model.
6. For example, MedBERT and BEHRT include embeddings only for disease codes while BERT-EHR includes embeddings for disease, medication, procedure, and clinical notes.",0.5,1.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s25,p25.0,"Main embeddings represent the given input sequence in low dimensional space. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings. The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings. Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].","['b26', 'b1', 'b31']","[True, True, True]",3,"1. Main embeddings represent the given input sequence in low dimensional space.
2. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better.
3. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings.
4. The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence.
5. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings.
6. Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].",Main embeddings represent the given input sequence in low dimensional space.,What is the purpose and types of auxiliary embeddings in model training?,"Question: What is the purpose and types of auxiliary embeddings in model training?

1. Main embeddings represent the given input sequence in low dimensional space.
2. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better.
3. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings.
4. The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence.
5. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings.
6. Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].",1.0,0.5,0.5,1.0,1.0,1.0,0.0,0.8333333333333334,-0.3333333333333333,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s31,p31.0,"In the last few decades, the amount of biomedical literature is growing at a rapid scale. As knowledge discovery from biomedical literature is useful in many applications, biomedical text mining is gaining popularity in the research community [16]. However, biomedical text significantly differs from the general text with a lot of domain-specific words. As a result, the performance of general T-PLMs is limited in many of the tasks. So, biomedical researchers focused on developing in-domain T-PLMs to handle biomedical text. PubMed and PMC are the two popular sources of biomedical text. PubMed con-  tains only biomedical literature citations and abstracts only while PMC contains full-text biomedical articles. As of March 2020, PubMed includes 30M citations and abstracts while PMC contains 7.5M full-text articles. Due to the large collection and broad coverage, these two are the first choice to pretrain T-BPLMs [16], [43], [96].","['b15', 'b42', 'b96']","[True, True, True]",3,"1. In the last few decades, the amount of biomedical literature is growing at a rapid scale.
2. As knowledge discovery from biomedical literature is useful in many applications, biomedical text mining is gaining popularity in the research community [16].
3. However, biomedical text significantly differs from the general text with a lot of domain-specific words.
4. As a result, the performance of general T-PLMs is limited in many of the tasks.
5. So, biomedical researchers focused on developing in-domain T-PLMs to handle biomedical text.
6. PubMed and PMC are the two popular sources of biomedical text.
7. PubMed con-  tains only biomedical literature citations and abstracts only while PMC contains full-text biomedical articles.
8. As of March 2020, PubMed includes 30M citations and abstracts while PMC contains 7.5M full-text articles.
9. Due to the large collection and broad coverage, these two are the first choice to pretrain T-BPLMs [16], [43], [96].","In the last few decades, the amount of biomedical literature is growing at a rapid scale.",Why is biomedical text mining becoming increasingly popular in research?,"Question: Why is biomedical text mining becoming increasingly popular in research?

1. In the last few decades, the amount of biomedical literature is growing at a rapid scale.
2. As knowledge discovery from biomedical literature is useful in many applications, biomedical text mining is gaining popularity in the research community [16].
3. However, biomedical text significantly differs from the general text with a lot of domain-specific words.
4. As a result, the performance of general T-PLMs is limited in many of the tasks.
5. So, biomedical researchers focused on developing in-domain T-PLMs to handle biomedical text.
6. PubMed and PMC are the two popular sources of biomedical text.
7. PubMed con-  tains only biomedical literature citations and abstracts only while PMC contains full-text biomedical articles.
8. As of March 2020, PubMed includes 30M citations and abstracts while PMC contains 7.5M full-text articles.
9. Due to the large collection and broad coverage, these two are the first choice to pretrain T-BPLMs [16], [43], [96].",1.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s31,p31.1,"As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text. BioBERT [16] is the first biomedical pre-trained language model which is obtained by further pretraining general BERT on biomedical literature. BioBERTpt-bio [97] is obtained by further pretraining BERT Multilingual (base) on Brazilian biomedical corpus -scientific papers from PubMed (0.8M-only literature titles) + Scielo (health -12.4M + biological-3.2M: both titles and abstracts). BioMedBERT [100] is obtained by further pretraining BERT-large on BREATHE 1.0 corpus. BioMedBERT outperformed BioBERT on biomedical question answering.","['b15', 'b100', 'b97']","[True, True, True]",3,"1. As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text.
2. BioBERT [16] is the first biomedical pre-trained language model which is obtained by further pretraining general BERT on biomedical literature.
3. BioBERTpt-bio [97] is obtained by further pretraining BERT Multilingual (base) on Brazilian biomedical corpus -scientific papers from PubMed (0.8M-only literature titles) + Scielo (health -12.4M + biological-3.2M: both titles and abstracts).
4. BioMedBERT [100] is obtained by further pretraining BERT-large on BREATHE 1.0 corpus.
5. BioMedBERT outperformed BioBERT on biomedical question answering.","As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text.",How are biomedical pre-trained language models like BioBERT and BioMedBERT developed?,"Question: How are biomedical pre-trained language models like BioBERT and BioMedBERT developed?

1. As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text.
2. BioBERT [16] is the first biomedical pre-trained language model which is obtained by further pretraining general BERT on biomedical literature.
3. BioBERTpt-bio [97] is obtained by further pretraining BERT Multilingual (base) on Brazilian biomedical corpus -scientific papers from PubMed (0.8M-only literature titles) + Scielo (health -12.4M + biological-3.2M: both titles and abstracts).
4. BioMedBERT [100] is obtained by further pretraining BERT-large on BREATHE 1.0 corpus.
5. BioMedBERT outperformed BioBERT on biomedical question answering.",0.5,1.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s32,p32.0,"It is difficult to obtain a large amount of in-domain text in some cases. For example, MIMIC [87], [88] is the largest publicly available dataset of medical records.  The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC). However, to pretrain a transformer-based PLM from scratch, we require large volumes of text. To overcome this, some of the models are pretrained on general + in-domain text [41], [104] or, in-domain + related domain text [18], [28], [43], [97]. For example, BERT (jpCR+jpW) [ Table 6 contains hybrid corpora-based T-BPLMs.","['b104', 'b87', 'b27', 'b42', 'b88', None, 'b17', 'b97', 'b40']","[True, True, True, True, True, True, True, True, True]",9,"1. It is difficult to obtain a large amount of in-domain text in some cases.
2. For example, MIMIC [87], [88] is the largest publicly available dataset of medical records.
3. The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC).
4. However, to pretrain a transformer-based PLM from scratch, we require large volumes of text.
5. To overcome this, some of the models are pretrained on general + in-domain text [41], [104] or, in-domain + related domain text [18], [28], [43], [97].
6. For example, BERT (jpCR+jpW) [ Table 6 contains hybrid corpora-based T-BPLMs.",It is difficult to obtain a large amount of in-domain text in some cases.,What challenges exist in pretraining transformer-based PLMs with in-domain text?,"Question: What challenges exist in pretraining transformer-based PLMs with in-domain text?

1. It is difficult to obtain a large amount of in-domain text in some cases.
2. For example, MIMIC [87], [88] is the largest publicly available dataset of medical records.
3. The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC).
4. However, to pretrain a transformer-based PLM from scratch, we require large volumes of text.
5. To overcome this, some of the models are pretrained on general + in-domain text [41], [104] or, in-domain + related domain text [18], [28], [43], [97].
6. For example, BERT (jpCR+jpW) [ Table 6 contains hybrid corpora-based T-BPLMs.",1.0,0.5,0.5,1.0,0.0,1.0,0.0,0.6666666666666666,-0.1666666666666666,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s36,p36.0,"CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords. This kind of representation increases the overall length of the input as well as hinders the model learning. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text. However, both these approaches involve learning the model parameters from scratch which is highly expensive. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].","['b123', 'b122']","[True, True]",2,"1. CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus.
2. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords.
3. This kind of representation increases the overall length of the input as well as hinders the model learning.
4. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text.
5. However, both these approaches involve learning the model parameters from scratch which is highly expensive.
6. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].",CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus.,How do CPT and DSPT/SPT adapt T-PLMs to in-domain tasks and their limitations?,"Question: How do CPT and DSPT/SPT adapt T-PLMs to in-domain tasks and their limitations?

1. CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus.
2. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords.
3. This kind of representation increases the overall length of the input as well as hinders the model learning.
4. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text.
5. However, both these approaches involve learning the model parameters from scratch which is highly expensive.
6. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].",0.5,0.5,1.0,1.0,1.0,1.0,0.0,1.0,-0.5,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s36,p36.1,"Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123]. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction. This approach is completely inexpensive as it requires only CPU. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive. Table 9 contains summary of Green T-BPLMs.","['b123', 'b122']","[True, True]",2,"1. Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123].
2. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach.
3. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors.
4. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors.
5. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge.
6. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction.
7. This approach is completely inexpensive as it requires only CPU.
8. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module.
9. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text.
10. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive.
11. Table 9 contains summary of Green T-BPLMs.","Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123].",What are Green Models and how do they adapt T-PLMs for the biomedical domain?,"Question: What are Green Models and how do they adapt T-PLMs for the biomedical domain?

1. Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123].
2. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach.
3. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors.
4. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors.
5. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge.
6. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction.
7. This approach is completely inexpensive as it requires only CPU.
8. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module.
9. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text.
10. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive.
11. Table 9 contains summary of Green T-BPLMs.",0.5,0.0,0.0,1.0,0.0,1.0,0.0,0.8181818181818182,-0.8181818181818182,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s40,p40.0,"Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics. It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence. Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval [136] in the general domain and medical concept normalization [137], [138], semantic relatedness [139], question answering [66] in the biomedical domain. NLI is framed as a three-way sentence pair classification problem. Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair. MedNLI [61] is the in-domain NLI dataset with around 14k instances generated from MIMIC-III [88] clinical notes. As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al. [140] further pretrained BioBERT [16] on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45%. Cengiz et al. [55] applied an ensemble of two BioBERT models and achieved an accuracy of 84.7%. The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI [59] and MultiNLI [60] and then finetuned them on MedNLI.","['b137', 'b139', 'b138', 'b58', 'b136', 'b60', 'b59', 'b88', 'b140', 'b54', 'b66', 'b15']","[True, True, True, True, True, True, True, True, True, True, True, True]",12,"1. Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics.
2. It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence.
3. Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval [136] in the general domain and medical concept normalization [137], [138], semantic relatedness [139], question answering [66] in the biomedical domain.
4. NLI is framed as a three-way sentence pair classification problem.
5. Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair.
6. MedNLI [61] is the in-domain NLI dataset with around 14k instances generated from MIMIC-III [88] clinical notes.
7. As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al. [140] further pretrained BioBERT [16] on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45%.
8. Cengiz et al. [136]0 applied an ensemble of two BioBERT models and achieved an accuracy of 84.7%.
9. The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI [136]1 and MultiNLI [136]2 and then finetuned them on MedNLI.",Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics.,What is Natural Language Inference and its significance in NLP?,"Question: What is Natural Language Inference and its significance in NLP?

1. Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics.
2. It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence.
3. Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval [136] in the general domain and medical concept normalization [137], [138], semantic relatedness [139], question answering [66] in the biomedical domain.
4. NLI is framed as a three-way sentence pair classification problem.
5. Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair.
6. MedNLI [61] is the in-domain NLI dataset with around 14k instances generated from MIMIC-III [88] clinical notes.
7. As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al. [140] further pretrained BioBERT [16] on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45%.
8. Cengiz et al. [136]0 applied an ensemble of two BioBERT models and achieved an accuracy of 84.7%.
9. The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI [136]1 and MultiNLI [136]2 and then finetuned them on MedNLI.",1.0,0.5,0.5,1.0,1.0,1.0,0.0,1.0,-0.5,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s42,p42.1,"Recent works exploited general models for clinical STS [56], [57], [65], [159]. For example, Yang et al. [56] achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model. As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets [56], [57], [65], [71]. Xiong et al. [160] enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE [161] based entity representations. Mutinda et al. [155] achieved a Pearson correlation score of 0.8320 by finetuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets. Mahajan et al. [71] proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset. Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning. The main drawback in the above existing works is giving '[CLS]' vector as sentence pair representation to the sigmoid layer. This is because the '[CLS]' vector contains only partial information. Unlike existing works, Wang et al. [159] applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results.","['b56', 'b160', 'b159', 'b65', 'b71', 'b161', 'b155', 'b55']","[True, True, True, True, True, True, True, True]",8,"1. Recent works exploited general models for clinical STS [56], [57], [65], [159].
2. For example, Yang et al. [56] achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model.
3. As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets [56], [57], [65], [71].
4. Xiong et al. [160] enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE [161] based entity representations.
5. Mutinda et al. [155] achieved a Pearson correlation score of 0.8320 by finetuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets.
6. Mahajan et al. [71] proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset.
7. Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning.
8. The main drawback in the above existing works is giving '[CLS]' vector as sentence pair representation to the sigmoid layer.
9. This is because the '[CLS]' vector contains only partial information.
10. Unlike existing works, Wang et al. [159] applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results.","Recent works exploited general models for clinical STS [56], [57], [65], [159].",How have recent studies improved clinical STS model performance?,"Question: How have recent studies improved clinical STS model performance?

1. Recent works exploited general models for clinical STS [56], [57], [65], [159].
2. For example, Yang et al. [56] achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model.
3. As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets [56], [57], [65], [71].
4. Xiong et al. [160] enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE [161] based entity representations.
5. Mutinda et al. [155] achieved a Pearson correlation score of 0.8320 by finetuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets.
6. Mahajan et al. [71] proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset.
7. Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning.
8. The main drawback in the above existing works is giving '[CLS]' vector as sentence pair representation to the sigmoid layer.
9. This is because the '[CLS]' vector contains only partial information.
10. Unlike existing works, Wang et al. [159] applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results.",0.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,-1.0,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s46,p46.2,"In the case of small models, BioBERT outperformed others. Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text. They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences. Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text. BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence. The sentences with the highest score are considered as the summary. Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT.","['b75', 'b190', 'b191']","[True, True, True]",3,"1. In the case of small models, BioBERT outperformed others.
2. Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text.
3. They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences.
4. Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text.
5. BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence.
6. The sentences with the highest score are considered as the summary.
7. Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT.","In the case of small models, BioBERT outperformed others.",What are the novel approaches for summarizing biomedical text?,"Question: What are the novel approaches for summarizing biomedical text?

1. In the case of small models, BioBERT outperformed others.
2. Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text.
3. They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences.
4. Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text.
5. BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence.
6. The sentences with the highest score are considered as the summary.
7. Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT.",1.0,1.0,0.0,0.5,1.0,0.0,-0.5,1.0,0.0,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s52,p52.2,"Fine-tuning must be done iteratively to reduce the noisy labeled instances. BERT models to the biomedical domain. Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44]. TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191]. b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123]. The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123].","['b43', 'b123', 'b192', 'b122']","[True, True, True, True]",4,"1. Fine-tuning must be done iteratively to reduce the noisy labeled instances.
2. BERT models to the biomedical domain.
3. Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44].
4. TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances.
5. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191].
6. b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123].
7. The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123].",Fine-tuning must be done iteratively to reduce the noisy labeled instances.,What are low-cost domain adaptation methods for BERT models in the biomedical field?,"Question: What are low-cost domain adaptation methods for BERT models in the biomedical field?

1. Fine-tuning must be done iteratively to reduce the noisy labeled instances.
2. BERT models to the biomedical domain.
3. Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44].
4. TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances.
5. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191].
6. b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123].
7. The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123].",0.5,1.0,0.0,0.5,1.0,0.0,-0.5,0.8571428571428571,0.1428571428571429,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s53,p53.0,"Models like BioBERT, PubMedBERT have achieved good results in many of the tasks. However, these models lack knowledge from human-curated knowledge sources. These models can be further enhanced by ontology knowledge injection. Ontology knowledge injection can be done in many ways namely a) continual pretraining the models on UMLS synonyms [34], [46] or relations [45] or both [47] b) continual pretraining the models on UMLS concept definitions [192] and c) feature vector constructed using ontology is added to the sequence vector learned by the models [174].","['b44', 'b175', 'b46', 'b193', 'b45', 'b33']","[True, True, True, True, True, True]",6,"1. Models like BioBERT, PubMedBERT have achieved good results in many of the tasks.
2. However, these models lack knowledge from human-curated knowledge sources.
3. These models can be further enhanced by ontology knowledge injection.
4. Ontology knowledge injection can be done in many ways namely a) continual pretraining the models on UMLS synonyms [34], [46] or relations [45] or both [47] b) continual pretraining the models on UMLS concept definitions [192] and c) feature vector constructed using ontology is added to the sequence vector learned by the models [174].","Models like BioBERT, PubMedBERT have achieved good results in many of the tasks.",How can ontology knowledge injection enhance BioBERT and PubMedBERT models?,"Question: How can ontology knowledge injection enhance BioBERT and PubMedBERT models?

1. Models like BioBERT, PubMedBERT have achieved good results in many of the tasks.
2. However, these models lack knowledge from human-curated knowledge sources.
3. These models can be further enhanced by ontology knowledge injection.
4. Ontology knowledge injection can be done in many ways namely a) continual pretraining the models on UMLS synonyms [34], [46] or relations [45] or both [47] b) continual pretraining the models on UMLS concept definitions [192] and c) feature vector constructed using ontology is added to the sequence vector learned by the models [174].",0.5,0.5,0.5,1.0,1.0,1.0,0.0,1.0,-0.5,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s54,p54.2,"Data Augmentation -Data augmentation helps us to create new training instances from existing instances. These newly creating training instances are close to original training data and helpful in low resource scenarios. Back translation and EDA [195] are the top popular techniques for data augmentation. For example, Wang et al. [159] used back translation to augment the training instances to train the clinical text similarity model. The domain-specific ontologies like UMLS can also be used to augment the training instances [153].","['b159', 'b196', 'b153']","[True, True, True]",3,"1. Data Augmentation -Data augmentation helps us to create new training instances from existing instances.
2. These newly creating training instances are close to original training data and helpful in low resource scenarios.
3. Back translation and EDA [195] are the top popular techniques for data augmentation.
4. For example, Wang et al. [159] used back translation to augment the training instances to train the clinical text similarity model.
5. The domain-specific ontologies like UMLS can also be used to augment the training instances [153].",Data Augmentation -Data augmentation helps us to create new training instances from existing instances.,What are popular techniques for data augmentation in low resource scenarios?,"Question: What are popular techniques for data augmentation in low resource scenarios?

1. Data Augmentation -Data augmentation helps us to create new training instances from existing instances.
2. These newly creating training instances are close to original training data and helpful in low resource scenarios.
3. Back translation and EDA [195] are the top popular techniques for data augmentation.
4. For example, Wang et al. [159] used back translation to augment the training instances to train the clinical text similarity model.
5. The domain-specific ontologies like UMLS can also be used to augment the training instances [153].",1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s58,p58.0,"For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation. According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information. The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space. Finally, a softmax is applied to convert it into a vector of probabilities. However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector.","['b56', 'b136', 'b176', 'b159', 'b1', 'b171', 'b177']","[True, True, True, True, True, True, True]",7,"1. For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation.
2. According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information.
3. The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space.
4. Finally, a softmax is applied to convert it into a vector of probabilities.
5. However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector.","For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation.",How is the final input sequence representation obtained in text classification according to Devlin et al.?,"Question: How is the final input sequence representation obtained in text classification according to Devlin et al.?

1. For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation.
2. According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information.
3. The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space.
4. Finally, a softmax is applied to convert it into a vector of probabilities.
5. However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector.",0.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s62,p62.0,"In the beginning, the standard approach to develop BPLMs is to start with general PLMs and then further pretrain them on large volumes of biomedical text [16]. The main drawback of this approach is the lack of indomain vocabulary. Without domain-specific vocabulary, many of the in-domain are split into a number of subwords which hinders model learning during pretraining or fine-tuning. Moreover, continual pretraining is quite expensive as it involves pretraining on large volumes of unlabeled text. To overcome these drawbacks, there are low-cost domain adaptation approaches that extend the general domain vocabulary with in-domain vocabulary [122], [123]. The extra in-domain vocabulary is generated using Word2vec and then aligned [122] or generated directly using WordPiece [123] over biomedical text. The main drawback in these low-cost domain adaptation approaches is an increase in the size of the model with the addition of in-domain vocabulary. Further research on this topic can result in more novel methods for lowcost domain adaptation.","['b15', 'b123', 'b122']","[True, True, True]",3,"1. In the beginning, the standard approach to develop BPLMs is to start with general PLMs and then further pretrain them on large volumes of biomedical text [16].
2. The main drawback of this approach is the lack of indomain vocabulary.
3. Without domain-specific vocabulary, many of the in-domain are split into a number of subwords which hinders model learning during pretraining or fine-tuning.
4. Moreover, continual pretraining is quite expensive as it involves pretraining on large volumes of unlabeled text.
5. To overcome these drawbacks, there are low-cost domain adaptation approaches that extend the general domain vocabulary with in-domain vocabulary [122], [123].
6. The extra in-domain vocabulary is generated using Word2vec and then aligned [122] or generated directly using WordPiece [123] over biomedical text.
7. The main drawback in these low-cost domain adaptation approaches is an increase in the size of the model with the addition of in-domain vocabulary.
8. Further research on this topic can result in more novel methods for lowcost domain adaptation.","In the beginning, the standard approach to develop BPLMs is to start with general PLMs and then further pretrain them on large volumes of biomedical text [16].",What are the challenges and solutions in developing biomedical pre-trained language models?,"Question: What are the challenges and solutions in developing biomedical pre-trained language models?

1. In the beginning, the standard approach to develop BPLMs is to start with general PLMs and then further pretrain them on large volumes of biomedical text [16].
2. The main drawback of this approach is the lack of indomain vocabulary.
3. Without domain-specific vocabulary, many of the in-domain are split into a number of subwords which hinders model learning during pretraining or fine-tuning.
4. Moreover, continual pretraining is quite expensive as it involves pretraining on large volumes of unlabeled text.
5. To overcome these drawbacks, there are low-cost domain adaptation approaches that extend the general domain vocabulary with in-domain vocabulary [122], [123].
6. The extra in-domain vocabulary is generated using Word2vec and then aligned [122] or generated directly using WordPiece [123] over biomedical text.
7. The main drawback in these low-cost domain adaptation approaches is an increase in the size of the model with the addition of in-domain vocabulary.
8. Further research on this topic can result in more novel methods for lowcost domain adaptation.",1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s63,p63.0,"Most of the biomedical language models (except ELECTRA-based models) are pre-trained using MLM. In MLM, only 15% of tokens are randomly masked and the model learns by predicting that 15% of masked tokens only. Here the main drawbacks are a) as tokens are randomly chosen for masking, the model may not learn much by predicting random tokens b) as only 15% of tokens are predicted, the training signal per example is less. So, the model has to see more examples to learn enough language information which results in the requirement of large pretraining corpora and more computational resources. There is a need for novel pretraining tasks like Replaced Token Detection (RTD) which can provide more training signal per example. Moreover, when the model is pretrained using multiple pretraining tasks, the model receives more training signals per example and hence can learn enough language information using less pretraining corpora and computational resources [206].",['b207'],[True],1,"1. Most of the biomedical language models (except ELECTRA-based models) are pre-trained using MLM.
2. In MLM, only 15% of tokens are randomly masked and the model learns by predicting that 15% of masked tokens only.
3. Here the main drawbacks are a) as tokens are randomly chosen for masking, the model may not learn much by predicting random tokens b) as only 15% of tokens are predicted, the training signal per example is less.
4. So, the model has to see more examples to learn enough language information which results in the requirement of large pretraining corpora and more computational resources.
5. There is a need for novel pretraining tasks like Replaced Token Detection (RTD) which can provide more training signal per example.
6. Moreover, when the model is pretrained using multiple pretraining tasks, the model receives more training signals per example and hence can learn enough language information using less pretraining corpora and computational resources [206].",Most of the biomedical language models (except ELECTRA-based models) are pre-trained using MLM.,What are the limitations of using MLM for pre-training most biomedical language models?,"Question: What are the limitations of using MLM for pre-training most biomedical language models?

1. Most of the biomedical language models (except ELECTRA-based models) are pre-trained using MLM.
2. In MLM, only 15% of tokens are randomly masked and the model learns by predicting that 15% of masked tokens only.
3. Here the main drawbacks are a) as tokens are randomly chosen for masking, the model may not learn much by predicting random tokens b) as only 15% of tokens are predicted, the training signal per example is less.
4. So, the model has to see more examples to learn enough language information which results in the requirement of large pretraining corpora and more computational resources.
5. There is a need for novel pretraining tasks like Replaced Token Detection (RTD) which can provide more training signal per example.
6. Moreover, when the model is pretrained using multiple pretraining tasks, the model receives more training signals per example and hence can learn enough language information using less pretraining corpora and computational resources [206].",0.5,1.0,0.5,1.0,1.0,1.0,0.0,1.0,0.0,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s66,p66.0,"Pretraining provides BPLMs with necessary background knowledge which can be transferred to downstream tasks. However, pretraining is computationally very expensive and also requires large volumes of pretraining data. So, there is need of novel model architecture which reduces the pretraining time as well as the amount of pretraining corpus. In general NLP, recently efficient models like ConvBERT [215] and DeBERTa [216] are proposed which reduces the pretraining time and amount of pretraining corpus required respectively. DeBERTa with two novel improvements (Disentangled attention mechanism and enhanced masked decoder) achieves better compared to RoBERTa. DeBERTa is pretrained on just 78GB of data while RoBERTa is pretrained on 160GB of data. ConvBERT with mixed attention block outperforms ELECTRA while using just 1/4 th of its pretraining cost. Biomedical NLP research community must focus on developing pretrained models based on these novel model architectures.","['b217', 'b216']","[True, True]",2,"1. Pretraining provides BPLMs with necessary background knowledge which can be transferred to downstream tasks.
2. However, pretraining is computationally very expensive and also requires large volumes of pretraining data.
3. So, there is need of novel model architecture which reduces the pretraining time as well as the amount of pretraining corpus.
4. In general NLP, recently efficient models like ConvBERT [215] and DeBERTa [216] are proposed which reduces the pretraining time and amount of pretraining corpus required respectively.
5. DeBERTa with two novel improvements (Disentangled attention mechanism and enhanced masked decoder) achieves better compared to RoBERTa. DeBERTa is pretrained on just 78GB of data while RoBERTa is pretrained on 160GB of data.
6. ConvBERT with mixed attention block outperforms ELECTRA while using just 1/4 th of its pretraining cost.
7. Biomedical NLP research community must focus on developing pretrained models based on these novel model architectures.",Pretraining provides BPLMs with necessary background knowledge which can be transferred to downstream tasks.,What are the challenges and solutions in reducing pretraining resources for NLP models?,"Question: What are the challenges and solutions in reducing pretraining resources for NLP models?

1. Pretraining provides BPLMs with necessary background knowledge which can be transferred to downstream tasks.
2. However, pretraining is computationally very expensive and also requires large volumes of pretraining data.
3. So, there is need of novel model architecture which reduces the pretraining time as well as the amount of pretraining corpus.
4. In general NLP, recently efficient models like ConvBERT [215] and DeBERTa [216] are proposed which reduces the pretraining time and amount of pretraining corpus required respectively.
5. DeBERTa with two novel improvements (Disentangled attention mechanism and enhanced masked decoder) achieves better compared to RoBERTa. DeBERTa is pretrained on just 78GB of data while RoBERTa is pretrained on 160GB of data.
6. ConvBERT with mixed attention block outperforms ELECTRA while using just 1/4 th of its pretraining cost.
7. Biomedical NLP research community must focus on developing pretrained models based on these novel model architectures.",1.0,0.5,1.0,1.0,1.0,1.0,0.0,1.0,-0.5,1.0
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s67,p67.0,"We have comprehensively covered the research works related to T-BPLMs. As our focus is on T-BPLMs, we have not included any papers related to context insensitive biomedical embeddings. For detailed information regarding context insensitive biomedical embeddings, please refer the survey paper written by Kalyan and Sangeetha [22]. As it is a survey focused on T-BPLMs, we have covered the foundation concepts like transformers and self-supervised learning in a very brief way only.",['b21'],[True],1,"1. We have comprehensively covered the research works related to T-BPLMs.
2. As our focus is on T-BPLMs, we have not included any papers related to context insensitive biomedical embeddings.
3. For detailed information regarding context insensitive biomedical embeddings, please refer the survey paper written by Kalyan and Sangeetha [22].
4. As it is a survey focused on T-BPLMs, we have covered the foundation concepts like transformers and self-supervised learning in a very brief way only.",We have comprehensively covered the research works related to T-BPLMs.,What does the survey on T-BPLMs focus on and exclude?,"Question: What does the survey on T-BPLMs focus on and exclude?

1. We have comprehensively covered the research works related to T-BPLMs.
2. As our focus is on T-BPLMs, we have not included any papers related to context insensitive biomedical embeddings.
3. For detailed information regarding context insensitive biomedical embeddings, please refer the survey paper written by Kalyan and Sangeetha [22].
4. As it is a survey focused on T-BPLMs, we have covered the foundation concepts like transformers and self-supervised learning in a very brief way only.",1.0,0.0,0.0,1.0,1.0,1.0,0.0,0.25,-0.25,0.0
