# A Survey on Graph Processing Accelerators: Challenges and Opportunities

CorpusID: 67856002 - [https://www.semanticscholar.org/paper/32604282b8e53584a906d31ab1238f7b41d0dd39](https://www.semanticscholar.org/paper/32604282b8e53584a906d31ab1238f7b41d0dd39)

Fields: Engineering, Computer Science

## (s3) Graph Algorithms
(p3.0) We review several common graph algorithms with different requirements in computation, communication and memory access. These graph algorithms are also widely studied for the exprimental evaluation in the previous studies [12,13,17] .

(p3.1) Breadth-First Search (BFS) is a basic graph traversal algorithm, which is used as the kernel of Graph500 benchmarks. The neighboring vertices are iteratively accessed from the root vertex until all vertices of the graph are visited.

(p3.2) Single Source Shortest Path (SSSP) is another graph traversal algorithm that computes the shortest paths from a source vertex to other vertices. Different from BFS, it has less number of redundant computations in checking edges. Each vertex may be activated more than once. Therefore, it needs more memory space than BFS.

(p3.3) Betweenness Centrality (BC) is widely used to measure the importance of a vertex in a graph. The betweenness centrality value of a vertex is calculated by the ratio of shortest paths between any other two vertices. BC algorithm requires to compute the shortest paths between all pairs of vertices.
## (s6) Graph Preprocessing
(p6.0) The data size of real-world graphs can easily exceed the on-chip/board memory capacity of graph processing accelerators which is a significant challenge for accelerators. This issue can cause large amounts of I/O and communication cost. In order to make data access efficient, preprocessing of graph data is often required to adapt the data structure onto the target graph accelerators. In this section, we will review the following major graph preprocessing methods used in the designs of graph processing accelerators.

(p6.1) • Graph Layout Reorganization. Graph layout is an important factor to affect the graph processing efficiency. Most previous studies have attempted to reorganize the layout to improve data accessing efficiency from many distinct aspects, e.g., data locality, memory storage, and memory access patterns.

(p6.2) • Graph Ordering. Graph ordering aims to change the order of the vertices or the edges, such that data locality with less data conflicts can be obtained while the structure of the graph remains the same [27,69] .
## (s13) Discussions.
(p13.0) Edge-centric paradigm is usually used by existing graph accelerators for improving the utilization of their limited memory bandwidth [27,28,75] . However, the point is that edge-centric paradigm is lack of flexible scheduling potential in contrast to the vertex-centric one. Almost all of edges have to be processed multiple times to complete the whole process. In addition, this paradigm may also lead to a large number of random accesses to vertices. Thus, additional optimizations are often cooperatively needed, e.g., fined grained partitioning and tailored vertex update strategies [28,70] .

(p13.1) For graph processing accelerators, the selection and design of iterative paradigm for graph processing accelerator must also ensure that: 1) programming is easy to use and understand for graph algorithm representation; 2) parallelism is easy to expose and exploit for high throughput and fast hardware development. It is also important to dedicate the accelerators according to the features of applications. Note that advantages can be combined by incorporating hybrid approaches into a  
## (s15) ASIC-based Designs
(p15.0) As for the memory hierarchy, these graph accelerators commonly adopt the scratchpad memory to replace traditional cache. The scratchpad memory acts as a content addressable cache and can be controlled manually. Graphicionado [16] uses the eDRAM as the scratchpad memory to store graph data that needs frequent random accesses, e.g., the destination vertices.
## (s16) PIM-enabled Designs
(p16.0) ReRAM-assisted Graph Processing. ReRAM is a kind of non-volatile RAM with the enabled computing ability by changing the resistance across a dielectric solid-state material [35] . A ReRAM cell is with high density, low read latency and high energy efficiency [99] .

(p16.1) The ReRAM cells can be connected as a dense crossbar architecture to provide high parallelism and memory capacity. Generally, the graph can be represented as a matrix which can be naturally mapped to ReRAM cells. Each cell stores an edge or a vertex. When input voltages are applied to certain rows of the cells array, the stored values of each row will multiply the relevant input value. The stored values of each columns will be then added together. These features make it possible to realize efficient graph processing on ReRAM.
## (s17) Large-Scale Graph Processing Acceleration
(p17.0) Real-world graph data size can easily exceed the onchip/board memory capacity of graph processing accelerators. Most of existing accelerators only consider the case that the whole graph fits into the no-chip/board memory. However, how to deal with larger graphs on accelerators is a vital issue for practical applications.

(p17.1) There is an amount of work that has taken this issue into account, and a series of solutions are further proposed [25,26,28,32,80,94] . These solutions can be typically divided into three categories: the out-of-core solution, the multi-accelerators solution, and the heterogeneous solution.
## (s19) 3) Heterogeneous Acceleration.
(p19.0) As the rapid development of memory integration technologies (e.g., 3D stacking), the host memory becomes large or even huge with trillions of capacity [3,50] .  [80] . For supporting efficient cooperation, the dedicated memory subsystem is needed to alleviate the transmission overhead between the host and the graph accelerator. As a result, the data organization of graphs is the key to reduce the communication overhead. In order to avoid conflicts of computing devices, runtime scheduling schemes are also important for efficient task scheduling.
## (s21) Parallelism Extension
(p21.0) The processing units in either ASIC-or FPGA- Pipeline Duplication. An intuitive method to increase the throughput is to duplicate multiple pipelines for the parallel processing on more vertices and edges.

(p21.1) This simple method has been widely used in a wide spectrum of previous work [16,27,29,30,85,92,107] . Nevertheless, there still remain some potential problems that might prevent the scalable efficiency of multipipeline from expectation, which is significantly understudied. For instance, considerable communication between pipelines may lead to the additional overhead via crossbars and controllers [16,29] . In addition, there also exists workload balance issue that needs specialized data partitioning [16,28] .
## (s24) Multiple I/O Ports.
(p24.0) Another method is to design multiple I/O ports for a memory block [27,88,92] . By increasing the I/O ports, multiple memory requests can run concurrently. Usually the number of ports can be manually organized on the scratchpad memory. High MLP can be attained when the number of ports are equal to the number of processing units [16] . BRAMs on FPGAs can also be manually controlled to achieve this goal [27] . These BRAMs are usually combined together to form a memory block with multiple I/O ports.

(p24.1) 2) Improving Bandwidth Utilization. This method is widely adopted in graph accelerators [27,71,88,92,93] . For example, if the memory requests are adjacent in a vertex or edge list, these requests can be coalesced as one request for a block.
## (s26) Execution-aware Prefetching.
(p26.0) This method prefetches the graph data according to the execution requirements. It avoids the inefficiency of fixed traditional cache prefetching mechanism. For example, in vertex-centric model, the source vertex list and its corresponding edge list can be prefetched sequentially [32] .
## (s30) Communication Model
(p30.0) The communication model is a well-known pattern that exists commonly to propagate the information between different processing units. We next survey several patterns that have been used in off-the-shelf graph accelerators.

(p30.1) Message-based Pattern. Message-based communication model is widely used in distributed environments. In message-based communication model, communication is realized by sending messages among different processing units. These massages can carry the updated data or computation commands that will be execute locally. This model is widely used in HMCassisted graph processing accelerators [32,81] . As mentioned previously, the vaults in HMCs communicate with each other via messages.

(p30.2) Tesseract [32] designs the remote function call mecha- Partitioning methods and coalescing methods are usually needed to reduce the number of messages [81] . Besides, extra memory copying operations and buffers are also needed. FPGP [26]  However, there may exist many data races on the same memory location if some vertices are updated by many neighboring vertices.
## (s31) Execution Model
(p31.0) The execution model typically has two major concerns: 1) scheduling timing, and 2) scheduling order. processors [113] . In graph accelerators, the graph is usually partitioned into subgraphs that are processed by different processing units. When a processing unit finishes its work, it has to wait for other processing units finished. Then the values of different subgraphs are synchronized [25] . During each iteration, only the local values of graph data can be accessed and updated [26] .

(p31.1) The synchronous execution is easy to realize on graph accelerators and suits for memory-bound graph algorithms. It can utilize the memory bandwidth better because the data is updated in a bulk synchronous way.
## (s33) Remarks.
(p33.0) A single graph processing accelerator may have limited hardware resources and memory capacity. For mobilizing the potentials of these resources, in addition to the effective resource layout, an efficient runtime scheduling scheme is the key, which decides when and where a specified data is supposed to be pro-cessed. Considering the complexity of the hardware circuit layouts, unlike the pure software implementations, the runtime scheduling on a graph accelerator has to be co-designed with the necessary hardware supports in many cases for better efficiency.

(p33.1) For instance, software-assisted runtime scheduling for ensuring the sequential consistency can use locking mechanisms that are easy to implement. However, these mechanisms can be also error-prone and even suffer from significant performance degradation in hardware implementation. The specialized hardware supports with CAM structure [109] or more advanced designs [15] make the scheduling for sequential consistency easy. Runtime scheduler can therefore focus more on the parallelism exploitation [114] . In addition, this also greatly mitigates the atomicity overhead. Combined with irregular accesses and large sizes of graphs, more extra efforts still have to be done for runtime scheduling.
## (s37) 4) Graph Processing
(p37.0) Framework. According to the "Generality" column in Table 4, most of the graph processing accelerators target a set of typical graph processing algorithms, while the other accelerators may focus on optimizing a specific graph processing algorithm.

(p37.1) It is essentially a trade-off between generality and performance. It is not fair to compare these accelerators when the "Generality" is different.  Table 6.  Power consumption is an important metric to measure the energy efficiency [29] . The power consumption in Fig.4(a) presents an increasing trend as the graph size increases. This is because that it generally demands more computing and storage resources to handle large graphs. Besides, different kinds of hardware designs can contribute to various energy behaviours.

(p37.2) The accelerator with the lowest power consumption adopts the emerging ReRAM which has intuitive high energy efficiency [70] . In order to process larger graphs, the hosts may be involved and result in higher power consumption [94] . In Fig.4(a), accelerators with IDs by 1 [16] and 2 [29] can handle large graphs with good energy efficiency, which are both ASIC-based accelerators. This is because of the dedicated circuit designs and memory subsystems.

(p37.3) As for performance analysis, in spite that the re-  Fig.4(b). Most of the results with high performance are based on small graph size that the graph can fit into the on-chip/board memories. However, with graph size increasing, performance based on single accelerator decreases because external storages are often required [26,94] . Some designs based on multiple accelerators can maintain high performance when deal with large graphs [28,86] because the graphs can still be held in on-chip/board memory.

(p37.4) Remarks. It gets clear that comparing different graph accelerators is extremely challenging due to the distinct evaluation parameters. To resolve this problem, the common practice in prior work is to compare the accelerator with some known systems as shown in  Graph Reordering Graph Partition
## (s41) Opportunities
(p41.0) Widespread Adoption. To the best of our knowledge, graph processing has been used in many fields, e.g., social network, literature network, traffic network and knowledge atlas. The earlier work focuses on addressing typical problems regarding graph searching, random walking and graph clustering. Although there emerge a few latest advances that are attempting to solve the large, complex problems by leveraging graph processing [134] , the application of graph processing still needs to expand. It is a series of open questions regarding how to leverage graph processing and further renovate its hardware acceleration to solve wider practical problems.

(p41.1) Emerging Technologies. As discussed before, a few recent studies have used emerging memory technologies (e.g., HMC and ReRAM) to accelerate graph processing, and made the good results in both performance and energy. Nevertheless, the potentials of these emerging technologies are still being under-utilized. For instance, GraphR [70] uses just one layer ReRAM only, but the fact is that the future ReRAM is often stacked. It is an interesting question on how to use the stacked ReRAM for graph processing acceleration in a more significant way in practice. To this point, more effective and efficient techniques for better supporting emerging technologies have to be settled.
