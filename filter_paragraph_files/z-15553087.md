# A Short Survey on Data Clustering Algorithms

CorpusID: 15553087 - [https://www.semanticscholar.org/paper/5615769ad70a42ab8ee1c05fda2021c05a84e375](https://www.semanticscholar.org/paper/5615769ad70a42ab8ee1c05fda2021c05a84e375)

Fields: Mathematics, Computer Science

## (s1) II. CLUSTERING PARADIGMS
(p1.0) Since most data clustering problems have been shown to be NP-hard [5], different methods have been proposed in the past. In general, those methods can be categorized into different paradigms: Partitional Clustering, Hierarchical Clustering, Density-based Clustering, Grid-based Clustering, Correlation Clustering, Spectral Clustering, Gravitational Clustering, Herd Clustering, and Others.
## (s3) B. Hierarchical Clustering
(p3.0) Clusters are formed by following either a bottom-up approach or a top-down approach. For example, single-linkage clustering [10] is a classic bottom-up approach in which data points are gradually agglomerated together to form clusters. In each step, all pair-wise distances are computed to identify the minimum. The parties involved in the minimal pair-wise distance are linked together. Such a step is repeated until all data points are linked together. A hierarchical tree is constructed to connect all data points at the end. A tree depth level can be chosen to cut the tree, forming clusters. To model data dynamically, a special hierarchical clustering method called Chameleon has been proposed [11]. It makes use of the inter-connectivity and closeness concept to merge and divide clusters. If the inter-connectivity and closeness between two clusters are higher than those within the clusters, then the two clusters are merged.
## (s4) C. Density-based Clustering
(p4.0) Apart from the well-known clustering methods, there are different clustering paradigms. In density-based clustering, data is clustered based on some connectivity and density functions. For example, DBscan [12] uses density-based notions to define clusters. Two connectivity functions density-reachable and density-connected have been proposed to define each data point as either a core point or a border point. DBscan visits points arbitrarily until all points have been visited. If the point is a core point, it tries to expand and form a cluster around itself. Based on the experimental results, the authors have demonstrated its robustness toward discovering arbitrarily shaped clusters.
## (s5) D. Grid-based Clustering
(p5.0) In grid-based clustering, the data space is divided into multiple portions (grids) at different granularity levels to be clustered individually. For example, CLIQUE [13] can automatically find subspaces with high density clusters. No data distribution assumption has been made. The empirical results demonstrated that it could scale well with the number of dimensions. Thus it is especially efficient in clustering highdimensional data.
## (s6) E. Correlation Clustering
(p6.0) Correlation clustering [14] was motivated from a document clustering problem in which one has a pair-wise similarity function f learned from past data. The goal is to partition the current set of documents in a way that correlates with f as much as possible. In other words, we have a complete graph of N vertices, where each edge is labeled either + or −. Our goal is to produce a partition of vertices (a clustering) that agrees with the edge labels. The authors have proved that this problem is a NP-complete problem. Hence they proposed two approximation algorithms to achieve the partitioning.

(p6.1) The first method called Cautious is to minimize the disagreements (number of − edges inside clusters plus the number of + edges between clusters), whereas the second method called PTAS is to maximize the agreements (number of + edges inside clusters plus the number of − edges between clusters). Basically, the ideas of the above two methods are the same (to aggregate the vertices which agree with their edge labels). The first method is discussed in detail in this work.

(p6.2) First, we arbitrarily choose a vertex v. Then we pick up all the positive neighbors (the neighbor vertices with + edge) of the vertex and put them into a set A. Having picked up all the positive neighbors of the vertex, we perform pruning. That is the 'Vertex Removal Step'. In this step, we move on to check 3δ-bad for all the positive neighbors of the vertex, where δ = 1/44. If there are, we remove it from the set A. After the removal step, the next step is 'Vertex Addition

(p6.3) Step' in which we try to add back some vertices which are 7δ-good with the chosen vertex v to the set A. The vertices in the set A are then chosen as one cluster. The above steps are repeated until no vertices are left or the set A becomes empty.
## (s7) F. Spectral Clustering
(p7.0) Some of the existing clustering approaches may find local minima and require an iterative algorithm to find good clusters using different initial cluster starting points. In contrast, spectral clustering [15], [16], [17] is a relatively promising approach for clustering based on the leading eigenvectors of the matrix derived from a distance matrix. The main idea is to make use of the spectrum of the similarity matrix of the data to perform dimensionality reduction for k-means clustering in fewer dimensions. The seminal work [15] is discussed in this work.

(p7.1) At the beginning, we form an affinity matrix A, which is a NxN matrix and N is the total number of data points. Each entry A ij corresponds to the similarity measure between the data points s i and s j . The scaling parameter σ 2 controls how rapidly A ij falls off with the distance between s i and s j . After we have formed the affinity matrix A, we construct the Laplacian matrix L from the normalized affinity matrix of A. Then we find the k leading eigenvectors (i.e. with k leading eigenvalues) of L and form the matrix X by stacking the eigenvectors in column. After we have stacked the eigenvectors to form the matrix X, we normalize each row. Then we treat each row in X as a data vector and use k-means clustering algorithm to cluster them. The clustering results are projected back onto the original data (i.e. it assigns the original point s i to cluster j if and only if row i of the matrix X is assigned to cluster j).
## (s8) G. Gravitational Clustering
(p8.0) Distinct from the works we have mentioned, gravitational clustering is considered as a rather unique method. It was first proposed by Wright [18]. In the method, each data instance is considered as a particle within the feature space. A physical model is applied to simulate the movements of the particles. As described in [19], Jonatan et al. proposed a new gravitational clustering method using Newton laws of motion. A simplified version of gravitational clustering was proposed by Long et al. [20]. Wang et al. proposed a local shrinking method to move data toward the medians of their k nearest neighbors [21]. Blekas and Lagaris [22] proposed a similar method called Newtonian Clustering in which Newton's equations of motion are applied to shrink and separate data, followed by Gaussian mixture model building. Molecular dynamics-like mechanism was also applied for clustering by Junlin et al [23].
## (s9) H. Herd Clustering
(p9.0) To tackle the clustering problem, a novel clustering method, Herd Clustering (HC), has been proposed by Wong et al. [24]. It novelties lie in two aspects: (1) HC is inspired from the nature, herd behavior, which is a commonly seen phenomenon in the real world including human mobility patterns [25]. Thus it is very intuitive and easy to be understood for its good performance. (2) HC also demonstrates that cluster analysis can be done in a non-traditional way by making data alive.
## (s12) A. Clustering on Data Stream
(p12.0) The previous clustering methods assume data are static during clustering. Nonetheless, modern data are not static necessarily. In fact, data can be transmitted in streaming form; for instance, real-time financial stock market data, video surveillance data, and social media data. Modern data keeps itself changing and evolving during the course of clustering. For analysis of such data, the ability to process the data in a timely manner with little memory is crucial. In light of that, different data stream clustering methods are proposed. Fo instance, Guha et al. have proposed one of the firstknown method, STREAM, to solve the k-median problem on streaming data with constant-factor approximation [45]. An incremental clustering method (COBWEB) has also been proposed to maintain a hierarchical clustering tree on streaming data by Fisher [46]. Zhang et al. have proposed an efficient data clustering method for large datasets [47]. Thanks to its linear complexity and single-pass nature, it can also be applied to cluster data streams with a tree data structure, CF Tree [47]. On the other hand, an incremental clustering method (C2ICM) has been proposed to data stream clustering problems. In particular, a lower bound for its clustering performance has also been provided [48].
## (s19) E. Statistical Tests
(p19.0) Since some of the existing clustering methods are stochastic, multiple replicate runs need to be executed for comprehensive benchmarking [24]. The means and standard deviations of performance metrics are usually reported for fair comparison. To justify the results, statistical tests are adopted to assess the statistical significances; For instance, t-tests, Mann-Whitney U-tests (MWU), and Kolmogorov-Smirnov test (KS).
