# Robust Recommender System: A Survey and Future Directions

CorpusID: 261556958 - [https://www.semanticscholar.org/paper/73d4a4e39eafc5884247f6cacf42228476ae3b54](https://www.semanticscholar.org/paper/73d4a4e39eafc5884247f6cacf42228476ae3b54)

Fields: Computer Science

## (s3) Taxonomy of Robustness in Recommender System
(p3.0) Recommender systems function as highly interactive platforms, making them vulnerable to various forms of abnormal data. These anomalies can originate from malicious activities such as injecting fake users and tampering with item information, or from natural noise which typically arises due to human errors or ambiguities in user behavior. For malicious attacks, attackers often aim to promote/nuke specific items or damage the performance of recommender systems. Generally, adversarial scenarios often restrict the attackers' abilities to manipulate a user's historical behavior. There are typically two types of attacks in recommendation scenarios, as shown in Figure 2(b): (1) Item side information modification: Attackers manipulate item side information to artificially augment the popularity of specific items [33], as depicted in the top of Figure 2(b). (2) Fake user injection (shilling attack): Introducing fake users to inflate or deflate the exposure of certain items or degrade the overall performance of the recommender systems [30,120], as shown in the bottom of Figure 2(b). It is essential to note that,  even with limitations on potential attack, several defense mechanisms against interaction-level attacks have been proposed in the literature [24,83,156,158]. For a comprehensive understanding, these defense methodologies will be elaborated upon in the subsequent sections, particularly in Section 3.2.4.

(p3.1) Natural noise mainly stems from user-generated factors such as human errors, uncertainty, and ambiguity in user behavior [151]. For instance, it may manifest in the historical interactions between users and items, such as misclicks or gifts reflecting others' tastes, as illustrated in Figure 2(c). Note that, noise can also occur in user or item side information, such as incorrect personal information or item tags. However, due to the paucity of research addressing these types of noise in the context of recommender systems [82], Figure 2(c) does not show this category.

(p3.2) To enhance the robustness of recommender systems, it is essential to differentiate between the various types of abnormal data. Accordingly, we categorize the robustness of recommender systems into two primary types based on the nature of these anomalies:
## (s5) Fraudster Detection
(p5.0) The detection of fraudsters is paramount for ensuring the robustness of recommender systems, especially against malicious attacks. Such attacks often involve fraudsters providing misleading feedback to manipulate the system [30,120]. Thus, fraudster detection aims to identify and eliminate these users to maintain reliable recommendations. Fraudster detection approaches can be categorized into three types, corresponding to when detection occurs: pre-processing, in-processing, and post-processing detection ( Figure 5(a)). Pre-processing detection [148,177] aims to find fraudsters in the original data before training, eliminating their impact from the source. In-processing detection [165] further leverages model feedback during training to detect and weaken fraudster impact. Post-processing detection [20] identifies and corrects bad recommendations caused by fraudsters. Each stage provides tailored detection strategies. Pre-processing is the mainstream approach, while in-processing and post-processing are more recent.

(p5.1) 3.1.1 Pre-processing Detection. Pre-processing detection refers to the process of identifying and mitigating fraudsters in recommender systems before the training of the model begins, ensuring an accurate and reliable training process. Pre-processing detection can be divided into two stages, of which the first stage is feature extraction, taking some statistics as the characteristics of each user; the second stage is detection, through the features extracted in the first stage to detect fraudsters. 
## (s8) In-processing Detection.
(p8.0) In-processing detection refers to identifying and mitigating fraudulent activities in recommender systems during model training. This approach allows for more comprehensive information to be gathered than pre-processing detection, as it takes into account both the initial data and the information from ongoing training process, which could reveal more complex and latent relationships among users and items.

(p8.1) A representative work, GraphRfi [165], formulates the in-processing detection as two tasks: Fraudster Detection: GraphRfi leverages the output of recommender systems to assist the detector. This assistance is based on the principle of cognitive psychology, which assumes that the behavior of genuine users is coherent and predictable. In other words, if a significant discrepancy exists between a user's actual behavior and their predicted behavior, i.e., the predictionˆ, is much different from the ground-truth rating , , the user is likely to be a fraudulent user. The discrepancy of user ' behavior can be calculated by:
## (s10) Post-processing Detection.
(p10.0) Post-processing refers to the detection process after the recommender systems have been trained. The primary objective of post-processing detection is to filter out poor-quality recommendations generated as a result of fraudsters, ensuring that the recommender systems provide accurate and reliable suggestions to its genuine users. A presentative method for post-processing detection is proposed by Cao et al. [20]. In the Reinforcement Learning (RL)-based recommendation scenario, they propose a two-part detection model. The first part is a Gated Recurrent Unit (GRU) encoder, which encodes the action methods, i.e., the list recommended by the RL agent, into a low-dimensional feature vector. The second part is an attention-based decoder with a classifier to distinguish between high-quality and low-quality recommendations. The method identifies poor-quality recommendations stemming from fraudsters. The RL-based framework allows the model to continuously learn and improve its decision-making process as it filters recommendations, thus enhancing the overall robustness of the recommender systems' output.
## (s11) Discussion of Fraudster Detection.
(p11.0) In the context of fraudster detection within recommender systems, there are three principal detection strategies: pre-processing, in-processing, and postprocessing detection. Pre-processing detection functions outside of model training, offering the advantage of computational efficiency during the training phase [4,74]. In-processing detection integrates model insights throughout training to achieve better accuracy [165]. Meanwhile, postprocessing detection aims to filter out bad recommendations for next items [20]. However, its performance can be compromised when the filtered objects are recommendation lists.

(p11.1) In the realm of fraudster detection, the close relationship between precision and recall is critical. When the threshold for classifying a user as fraudulent is lowered, the model might detect more fraudsters. However, this approach comes with the drawback of potentially misidentifying genuine users as fraudulent. Such misclassifications can negatively impact user experience and undermine the financial gains of the recommender system. Therefore, future detection methods should focus on retaining high precision while simultaneously amplifying recall. Exploring advanced deep learning methods also seems promising. For instance, graph neural networks [141] have shown potential in identifying complex patterns and relationships between users and items, which could enhance detection capabilities.
## (s16) Adversarial Perturbation on Interaction.
(p16.0) In the realm of recommender systems, there's a growing interest in methods that employ adversarial training on interaction graphs. Such methods focus on introducing adversarial perturbations within these graphs. The core motivation behind this is to bolster the robustness of recommender systems against potential threats targeting the interaction data between users and items. Recent studies, such as Conv-GCF [156] and AdvGraph [24], delve into embedding adversarial perturbations directly into the interaction matrix. This integration can be modeled as the optimization problem described by the following objective function:
## (s17) Certifiable Robust Training
(p17.0) Certifiable robust training is a crucial component in safeguarding the integrity and effectiveness of recommender systems when confronted with malicious attacks. This strategy is centered around the design and implementation of algorithms that assure the system's robustness to adversarial perturbations, therefore ensuring that recommendations retain their accuracy even amidst the activities of malicious users.

(p17.1) The work of Liu et al. [83] is an example of this approach, where they aim to define the robust boundary of the Factorization Machine (FM) model. In this setting, given a trained FM, i.e., , and an input instance with -dimensional features that reside in the binary space {0, 1} 1× , the recommendation task of the FM model, with second-order weight can be expressed as:

(p17.2) where 0 is the global bias, is the weight for the -th feature, ∈ R 1× is the embedding of the -th feature, the inner product < , > models the interactions between the -th and the -th features, and denotes the -th dimension of instance .

(p17.3) With a perturbation budget , which demarcates the maximum allowable number of feature flips in , Liu et al. [83] estimate the upper limit of prediction alteration, represented as ( ). If ( ) ≤ 0, then ( ) = max ′ ( ′ ) − ( ). Otherwise, ( ) = min ′ ( ′ ) − ( ). Here, ′ represents the instance derived from with alterations confined to the stipulated budget, meaning | ⊕ ′ | ≤ , where ⊕ indicates the XOR operation. This shift represents the difference in the model's output when the maximum allowable perturbation is applied to the instance . By examining whether the prediction changes significantly under this maximum prediction shift, the robustness of the model's output can be evaluated. Further exploration on this robustness evaluation is discussed in Section5.1. Building on this foundation, Liu et al. [83] propose a robust training algorithm, expressed as:

(p17.4) where ( ) + ( ) represents the bound of the prediction under the maximum prediction shift. This certifiable robust training provides a valuable tool for assessing and improving the robustness of recommender systems models, allowing developers to better protect their systems from malicious attacks and maintain the accuracy and reliability of recommendations.
## (s18) ROBUSTNESS AGAINST NATURAL NOISE
(p18.0) Recommender systems also face challenges stemming from natural noise, which refers to inconsistencies, inaccuracies, or missing information in the input data that may arise from various sources, such as human error, uncertainty, and vagueness [151]. Robustness against natural noise is crucial to ensure that the recommender systems can still provide accurate and reliable recommendations despite the presence of such noise in the user interactions. To address this issue, researchers have developed several techniques, including regularization, purification, and self-supervised learning. This section aims to discuss these methods in detail, showcasing how they contribute to enhancing the robustness of recommender systems against natural noise.
## (s20) Adjustment of Training Process
(p20.0) Orthogonal Mapping where , is the ground truth rating andˆ, is predicted rating computing by Θ. The 1 -norm offers enhanced robustness against outliers or anomalous data in comparison to the Euclidean distance (i.e., 2 -norm). By integrating the 1 -norm regularization, the model prioritizes essential features over noise, leading to more refined recommendations. In recent times, researchers continue to innovate regularization strategies to improve the robustness of recommender systems in the presence of natural noise. As an illustration, Chen et al. [23] meld Jacobian regularization [68] with the transformer block in sequential recommender systems. This regularization enables a significant reduction in the model's susceptibility to noisy sequences, consequently delivering more consistent and trustworthy recommendations amidst noise.

(p20.1) While regularization offers a versatile means to enhance noise robustness across diverse recommender systems, its broad scope might dilute its efficacy against specific noise types. While instrumental in countering overfitting and amplifying generalization, regularization does not directly target the genesis of noise. Consequently, it's necessary to couple regularization with other noise-mitigation approaches for superior outcomes.
## (s29) Certifiable robustness.
(p29.0) Certifiable robustness focuses on finding the robustness boundary for a given instance in the recommender systems model. Traditional methods for certifiable robustness [32,75] can be categorized into two approaches: randomized smoothing and directly finding the worst perturbation. Randomized smoothing is a technique that smoothes the input by applying random noise, aiming to find an adversarial boundary that causes the model to produce incorrect outputs. However, in the recommender systems scenario, it is difficult to smooth the input of the model due to the semantics and discreteness of the features.

(p29.1) Directly finding the worst perturbation involves searching for the worst perturbation that can lead to an incorrect prediction for a given input. Liu et al. [83] provide both non-robust certification and robust certification by approximately calculating the worst perturbation for the FM model. For a given FM model , input sample , which includes historical interaction and other features, and the perturbation budget , let ′ denote the perturbed instance corresponding to . Recall the formulation of the FM model ( ) in Eq. 14, Liu et al. [83] formulate the prediction shift as:
## (s33) From Perspective of Applications
(p33.0) E-commerce Recommender Systems: E-commerce recommender systems, a crucial feature on platforms like Amazon and eBay, are designed to simplify product discovery for customers [116]. They function based on the historical browsing or purchasing patterns of customers, leaning heavily on user-item interaction data and item-side information. Despite their effectiveness, these systems confront several robustness concerns. One significant issue is the natural noise in user data through unintentional clicks or purchases, which may not necessarily represent the user's genuine preferences [151]. Furthermore, the system's integrity can be compromised by attackers manipulating item side information to push certain products or fabricate user profiles to manipulate a product's exposure [33]. In an e-commerce context, considerations of recommender systems' robustness are comprehensive. Beyond the strategies delineated in Sections 3 and 4, real-world applications necessitate these systems to adapt to the dynamic nature of product trends, cope with extensive product catalogs, and manage the persistent relevance of products over time. Collectively, these requisites pose new challenges in upholding robustness.

(p33.1) Media Recommender Systems: Media recommender systems, used prominently on streaming platforms like Netflix and TikTok, curate media suggestions-ranging from movies to songs-based on a user's historical consumption patterns and explicit preferences [49]. Similar to e-commerce systems, short-media recommendations also grapple with robustness issues, including unintentional likes, misleading video tags, and orchestrated efforts to inflate likes or views (Section 4 and Sections 3). These issues are amplified by the rapid shift in trends and the shorter lifespan of the content. On the other hand, long-media recommendations, such as for movies and music, provide a better reflection of user preferences through indicators like viewing or listening duration, thereby reducing the impact of natural noise. However, these systems also face potential challenges from fake user interactions [120] and the manipulation of item side information, such as fake video tags, for promotion is more likely due to the nature of the content [33] (Section 3).

(p33.2) News Recommender Systems: News recommender systems, predominantly found on news websites and applications, tailor news article suggestions based on a user's reading habits and explicitly stated interests [66]. These systems are uniquely challenged by the extremely short lifespan of news articles and their primary function to disseminate information, making them a target for attackers looking to spread misinformation by creating or modifying news content. In this scenario, whether the news was tampered with by the attacker and introduced fake information will be more noteworthy than whether there is noise or not (like methods in Section 3.2.3).

(p33.3) Social Recommender Systems: Social recommender systems, prevalent on platforms such as Facebook and Twitter, suggest potential connections, pages, or groups to users based on their interactions and existing connections [146]. A unique aspect of these systems is that every participant serves a dual role, being both a user and an item. This duality accentuates the significance of side information, including user-generated content, and interactions among users, such as following activities [36]. Given the prevalence of biased content and artificial followers on social platforms, ensuring robustness becomes paramount. Thus, a key challenge is to detect and counteract the influence of harmful content, fake users, and social bots to uphold the platform's authenticity and stability as methods in Section 3.1.
## (s34) RELATIONSHIP WITH OTHER TRUSTWORTHY PROPERTIES
(p34.0) Robustness is a key property in trustworthy recommender systems. In this section, we will explore the interplay between robustness and other key properties of trustworthy recommender systems. The four critical aspects are accuracy, explainability, privacy, and fairness [78]. We will also discuss the potential trade-offs and synergies that may arise when pursuing robustness in conjunction with these other performance goals.
## (s36) Robustness v.s. Interpretability
(p36.0) Interpretability, within the domains of machine learning and deep learning, refers to the model's decision mechanism is locally or globally transparent [167]. A model with high interpretability not only reveals the basis for its decisions but also elucidates the factors instrumental in its decision-making process. Recent studies underscore a symbiotic relationship between robustness and interpretability [79,126]. Gaining insights into the model's internal operations on adversarial samples can pave the way for heightened model robustness.
## (s38) Robustness v.s. Fairness
(p38.0) Fairness in machine learning typically signifies that an algorithm or model provides impartial and unbiased predictions across different groups or individuals [97,134]. Many methods interpret fairness as a model's invariant prediction to alterations in sensitive attributes (e.g., gender) within the input data [69,97]. This objective shares a resemblance with adversarial training where small perturbations in input data shouldn't alter predictions [157]. Nonetheless, a distinction persists: fairness concentrates explicitly on modifications to select attributes without imposing constraints on the extent of these changes. In contrast, robustness isn't tied to specific attributes but stipulates that the magnitude of changes should remain moderate to avoid distorting the ground truth label. Recent studies suggest that enhancing a model's robustness can indirectly improve its fairness [99].

(p38.1) Within the domain of recommender systems, fairness on the user side aligns closely with traditional fairness, emphasizing that changes to a user's sensitive attributes shouldn't influence the recommendations [134]. However, the robustness emphasis in recommender systems predominantly surrounds potential noise in user actions or the intrusion of malicious users, which diverges from fairness objectives. In practical recommendation scenarios, the scope of noise might extend beyond just user behaviors. Given this broader perspective, the goals of robustness and fairness in recommender systems could be more unified.
## (s40) Mitigating Gap between Defense Assumption and Attack Goal
(p40.0) A significant challenge that arises in the realm of recommender systems is the gap between defense assumptions and the actual objectives of attacks. Attacks, especially shilling attacks, often aim to promote or diminish product exposure, rather than merely undermining recommendation performance. However, many prevailing defense strategies are predicated on the notion that attackers principally aim to degrade the functionality of recommender systems. This is particularly evident in adversarial training scenarios. For instance, He et al. [56] incorporate adversarial perturbations to model parameters during its training phase. Yet, these perturbations often don't align with real-world attack patterns, meaning certain adversarial examples optimized during training might be ineffective for defense. In another approach, Wu et al. [137] positively introduce empirical risk minimizing users to counterbalance the impacts of malicious users, who are often perceived as threats to recommendation quality.

(p40.1) These methods often fall short in addressing attacks stemming from different motivations, resulting in a fragmented approach toward improving the robustness of recommender systems. It is necessary to develop integrated strategies that seamlessly merge the understanding of attacks and defense mechanisms. In future research endeavors, it's essential to ensure that defense assumptions are more closely with real attack targets. This could involve (1) imposing more rigorous constraints on adversarial samples during training-potentially by adopting knowledge-enhanced adversarial perturbations or similar techniques that infuse prior knowledge into perturbation generation. Additionally, (2) considering new adversarial training paradigms could be valuable. For instance, shifting from the established "min-max" framework to a "min-min" perspective [137] might pave the way for a more specific defense against attacks like shilling.
## (s41) Improving Generalization of Defense Methods
(p41.0) The generalization of defense methods in recommender systems presents a considerable challenge. A significant portion of current defense strategies is on specific constraints. For example, the approach proposed by Liu et al. [83] is tailored exclusively for models based on the Factorization Machine, while the methodology introduced by Cao et al. [20] is designed for models grounded in Reinforcement Learning. Moreover, numerous pre-processing detection techniques hinge on predefined features for detecting malicious users [11,74]. Some even resort to specific attack methods for generating supervised data [149,162]. Such specialization limits the applicability of defense methods across diverse scenarios, posing challenges to effectively countering adversarial threats in large-scale, real-world recommender systems.

(p41.1) Given these complexities, there's an emerging need to craft defense methods that can scale effectively and be applicable across diverse models and contexts. Looking ahead, defense methods should be designed more from the perspective of the commonality of poisoned data distribution and model training process or loss function, rather than paying too much attention to specific models, specific data, or specific attack methods. Among the promising avenues for exploration are: (1) The development of feature-free detection methods, ensuring they neither depend on pre-selected features nor limit dataset adaptability. (2) In-process detection methodologies that can discern patterns exhibited by malicious users during the model's training phase.
