# Knowledge Representation in Learning Classifier Systems: A Survey

CorpusID: 2356998 - [https://www.semanticscholar.org/paper/bd68a38678140ff8f544d026786422ea616366b5](https://www.semanticscholar.org/paper/bd68a38678140ff8f544d026786422ea616366b5)

Fields: Computer Science

## (s0) Introduction
(p0.0) The first framework of Learning Classifier System (LCS) labeled "cognitive system" was introduced more than 30 years ago by John H. Holland (Holand, 1976). LCSs were originally inspired by the general principles of Darwinian evolution and cognitive learning. The LCS framework was reformed to use reinforcement learning techniques such as Q-learning (Sutton and Barto, 1998) in order to ensure appropriate reward estimation and propagation. LCS is also known as rule-based evolutionary online learning system. It is a heuristic method in which a population of production systems are consisted and adapted by using genetic algorithm and reinforcement learning techniques. Each production system can cover small region of environment and represent some portions of the overall solution. Therefore, a LCS system is able to solve a problem by using the best evolved production systems in its population.

LLM judge: YES

## (s5) Ellipsoidal Based Representation
(p5.0) In (Butz, 2005), Butz suggested a new structure of classifier condition, based on defining hyper sphere and hyper ellipsoid shapes. There, three structures were proposed; first, the condition part of the classifiers represents a hyper sphere in the problem space. As hyper sphere has common radius in all dimensions, the condition part consists of a center and a deviation, that is, C = { − → m, σ} = {m 1 , m 2 , ..., m n , σ}. Second, the condition part represents an axis-parallel hyper ellipsoid which has different deviation in each dimension, that is,

(p5.1) .., m n , σ 1 , σ 2 , ..., σ n }. Third, the condition part is redefined to present a general hyper ellipsoidal structure which is axis-independent unlike the previous one. So, it is defined by a center point in addition to elements of matrix Σ named transformation matrix which indicates fully Mahalanobis distance metric of a hyper ellipsoid. This matrix shows stretch and rotation of the represented hyper ellipsoid. The condition part of a classifier is defined as follows: C = { − → m, Σ} = {m 1 , m 2 , ..., m n , σ 1,1 , σ 1,2 , ..., σ (n,n−1) , σ (n,n) } where − → m shows the center of represented hyper ellipsoid and Σ shows the transformation matrix of the condition. This structure is a general form of the first and second ones; hyper sphere is a hyper ellipsoid where the diagonal entries of Σ are initialized with common value and all other entries are set to zero, and axisparallel hyper ellipsoid has a diagonal matrix as Σ. In the general form, angular orientation and stretch of the represented hyper ellipsoid are implicitly encoded in Σ. Due to the redundancy of this encoding; the mutation and crossover operators may not act beneficially . This would decrease the reproductive opportunities of the successful classifiers and lead the evolutionary progress to slow down. To solve this problem, Butz et al. (Butz et al., 2006;Butz et al., 2008) investigated another condition representation which is able to explicitly codify the rotation of the desired hyper ellipsoid in its structure. So, the condition part of each classifier is expressed by three vectors, C = { − → m, − → σ , − → γ }; a vector − → m that indicates the center point, a vector − → σ = (σ 1 , σ 2 , ..., σ n ) T which represent the stretch, and a vector − → γ with size of ( n 2 ) to point out the orientation angles of the corresponding hyper ellipsoid. Figure   3 highlights how a classifier with mentioned representations can cover a partition of the input space. In all these structures, activation of each classifier in forming [M ] is determined by a Gaussian kernel function, applied to the distance between the current input and the center point. So, to find whether a classifier can match the current input x or not, at first, the activation of the classifier cl.ac is computed using Formula 1, 2 or 3 according to its structure. Then, the current input will be matched with a classifier if the activation of such classifier is greater than a threshold θ m . ).

LLM judge: YES

## (s7) Fuzzy Logic Based Representation
(p7.0) In the rule based classifier systems, the comprehensibility and interpretability are two must considerable features (Hayes-Roth, 1985). Besides, fuzzy logic is one of the best known mechanisms providing such properties. There are number of approaches to use fuzzy logic and fuzzy set theory (Zadeh, 1965;Zadeh, 1973 2008a). The main goal behind such efforts is combining the generalization capabilities of LCS with the fine interpretability of fuzzy rules to achieve an online learning system with more accurate, general and well understandable rule set. In the following, first, we briefly describe important works in this area. Second, a comprehensive description of notable approaches which try to embed fuzzy logic in knowledge representation component of LCS is provided. He addressed the classic "Competition versus Cooperation" problem in genetic fuzzy systems, (Bonarini, 1996;Bonarini and Trianni, 2001). He proposed a Michigan style LCS named ELF where the rule set is divided into subpopulations. To produce the correct action, the classifiers of these subpopulations cooperated whilst the classifiers in each subpopulation competed with each other. The behavior of ELF in overcoming some of issues of strength-based LCSs was verified by applying it to several reinforcement learning problems such as the coordination of autonomous agents. In (Bonarini, 2000;, a general framework of learning classifier systems were introduced and later this framework was extended particularly for XCS called FIXCS in . In (Bonarini, 2000), the different components of this framework have been analyzed to be consistent with fuzzy models. In addition, some features are introduced for the sake of classifying LFCS proposals presented in the literature. , named Fuzzy-UCS, extended this approach to be applied in supervised learning classifier system, i.e. UCS which is a derivation of XCS introduced in 2003 (Bernadó-Mansilla and Garrell, 2003) for classification task in data mining. In following, a comprehensive description of well known fuzzy representation which is successfully used in LCS realms to produce accuracy based fuzzy rule based system such as Fuzzy-XCS and Fuzzy-UCS.

(p7.1) The main idea of using fuzzy logic in XCS as the knowledge representation tool is to represent the labels associated to fuzzy sets in the rule's structures and offer LCS a mechanism to evolve them. This mechanism must be consistent with fuzzy rules and it must also be able to learn a rule set in order to implement an input to output mapping where both input and output can be either real valued or nominal. In common manner, the rule set consists of fuzzy rules which are defined in disjunctive normal form (DNF) with the following structure:

LLM judge: YES

## (s10) First Order Logic Based Representation
(p10.0) Mellor proposal in (Mellor, 2005;Mellor, 2006) extended XCS to a new model named FOXCS where rules are represented based on the first-order logic. First-order logic is useful to improve the expressive power of XCS due to its ability to present the complex relationships among attributes of a task domain. The modified classifiers are in form of Horn clauses which consist of three parts; an action part, a condition part and a background part, that is action ← condition,Background. Condition part consists of a number of variables. Here, variables have a generalization role in XCS similar to the # symbol in the ternary representation. However, variables can also be placed in action part. But, as a classifier should be defined in Horn clause form, its action part must consist of just one variable, i.e. atom. The background part can be empty.

(p10.1) The main advantages of using the first order logic are; 1) it is facilitated to represent relational concepts which relate variables of action part to those of condition, like (A ← ABB) where A and B are variables of given problem. Obviously, the variable A which takes place in action part can also appear in condition part. To illustrate this property, there is an example for a Blocks World tasks in Figure 6. 2) Another advantage is that rules can contain background knowledge which is a feature of many inductive logic programming (ILP) and rational reinforcement learning (RRL) systems, and can be helpful to solve these tasks effectively.

LLM judge: YES

## (s14) Tile Coding Based Representation
(p14.0) Lanzi et al. had extended XCSF with tile coding prediction . Tile coding is one of the most common and successful methods to tackle complex environment in reinforcement learning realm (Sutton and Barto, 1998). In tile coding, the problem space is mapped into a set of overlapping tilings; each tiling partitions the input space into a set of nonoverlapping hyper-rectangles named tiles. Classifiers in XCSF with tile coding prediction have two additional parameters; the number of tilings and their resolution. The extended system can adapt these parameters through GA. Also the usual linear prediction function in XCSF is replaced with a tile coding approximator. So, the weight vector of each classifier contains the parameters related to each tile. Figure 11 shows how a classifier with two tilings partitions the input space and which tiles (states) could match the specified input. The new extended XCSF was tested on three multistep problems taken from the reinforcement learning literature: the 2D Gridworld (Boyan and Moore, 1995), the puddle world (Boyan and Moore, 1995), and the mountain car (Sutton, 1996). Indeed, such XCSF evolves an ensemble of tile coding approximators, each one on the problem subspace, instead of the typical monolithic approximator. The reported results showed that XCSF with tile coding can always reach an optimal solution and converge faster than XCSF with linear approximation.

LLM judge: YES

