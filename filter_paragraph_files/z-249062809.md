# Learning Mean Field Games: A Survey

CorpusID: 249062809 - [https://www.semanticscholar.org/paper/cfd71424c57a32a5a0f721982ec3526f1d034884](https://www.semanticscholar.org/paper/cfd71424c57a32a5a0f721982ec3526f1d034884)

Fields: Mathematics, Computer Science

## (s10) Background on MDPs
(p10.0) We recall a few important concepts pertaining to optimal control in discrete time for a single agent. We will only review the main ideas and we refer to e.g. Bertsekas and Shreve (1996); Puterman (2014) for more details. The notion of Markov decision processes will play a key role in the description of dynamic MFGs.

(p10.1) We recall a few important concepts pertaining to optimal control in discrete time for a single agent. We will only review the main ideas and we refer to e.g. Bertsekas and Shreve (1996); Puterman (2014) for more details. The notion of Markov decision processes will play a key role in the description of dynamic MFGs.
## (s39) Some remarks about the distribution
(p39.0) Observing the mean field. In the above presentation, we assume that the agent does not observe the distribution, or at least does not exploit this information to learn the equilibrium policy. Although this is the most common approach in the RL and MFGs literature, the question of learning population-dependent policies arises quite naturally since one could expect that agents learn how to react to the current distribution they observe. This is usual in MARL, see e.g. Yang et al. (2018b) who consider Q-functions depending on the actions of all the other players. In MFGs, we can expect that by learning a population-dependent policy, the agent will be able to generalize, i.e., to behave (approximately) optimally even for population configurations that have not been encountered during training.
## (s61) Background on MDPs
(p61.0) We recall a few important concepts pertaining to optimal control in discrete time for a single agent. We will only review the main ideas and we refer to e.g. Bertsekas and Shreve (1996); Puterman (2014) for more details. The notion of Markov decision processes will play a key role in the description of dynamic MFGs.

(p61.1) We recall a few important concepts pertaining to optimal control in discrete time for a single agent. We will only review the main ideas and we refer to e.g. Bertsekas and Shreve (1996); Puterman (2014) for more details. The notion of Markov decision processes will play a key role in the description of dynamic MFGs.
## (s90) Some remarks about the distribution
(p90.0) Observing the mean field. In the above presentation, we assume that the agent does not observe the distribution, or at least does not exploit this information to learn the equilibrium policy. Although this is the most common approach in the RL and MFGs literature, the question of learning population-dependent policies arises quite naturally since one could expect that agents learn how to react to the current distribution they observe. This is usual in MARL, see e.g. Yang et al. (2018b) who consider Q-functions depending on the actions of all the other players. In MFGs, we can expect that by learning a population-dependent policy, the agent will be able to generalize, i.e., to behave (approximately) optimally even for population configurations that have not been encountered during training.
