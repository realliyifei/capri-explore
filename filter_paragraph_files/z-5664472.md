# A Review on Video-Based Human Activity Recognition

CorpusID: 5664472 - [https://www.semanticscholar.org/paper/f2985c9c03e0113e27a473d5f2b064de10804110](https://www.semanticscholar.org/paper/f2985c9c03e0113e27a473d5f2b064de10804110)

Fields: Computer Science

## (s3) Background Subtraction
(p3.0) The most common method for static camera segmentation is background subtraction due to its simplicity and efficiency [98].The background model contains only the stationary background scene without any foreground object, and any image change is assumed to be caused only by moving objects.Hence the foreground object can be obtained by subtracting the current image of the background image, followed by a magnitude thresholding to obtain the segmentation mask.The segmentation mask often contains rough and fractional foreground object(s) and usually requires some post-processing, such as closing and opening morphological operations.The background subtraction has been extensively applied in all kinds of scenarios with various improved modifications.For example, for real-time human body tracking [39], the color distribution of each pixel in the background is first modeled with a Gaussian with a full covariance matrix.This background scene texture map is considered to be class zero.The foreground textures in different classes are grouped by the mean of a point and the covariance associated with that point.Another improvement is to discriminate moving objects, ghosts and shadow [40], based on statistical assumptions, with object-level knowledge, of moving objects, apparent objects (ghosts) and shadows.Besides, in order to overcome the limitation of the background subtraction on stationary background, Seki et al. [41] proposes a method to handle the dynamic (waving) background.The method learns the chronological changes in the observed scene's background in terms of distribution of image vectors.Generally speaking, the background subtraction is simple and efficient, but the simplicity of the background model sometimes causes the inaccurate classification of the pixels.Continuous and effective updating of the background in response to gradual changes of background also poses some challenges.
## (s5) Statistical Models
(p5.0) Besides GMM, the methods to build more complicated background models are also proposed.One of the typical methods is to build the background model as a statistical model.In [44], each pixel is modeled by four parameters, individually brightness distortion, chromaticity distortion, the variation of the brightness distortion, and the variation of the chromaticity distortion.As shown in Figure 5 [44], i E is the expected color value (say, as recorded in the background image) and i I is the color value of the ith pixel.The brightness distortion ( ) is defined as the shortest distance between the ith pixel i I and the line O E i .The chromaticity distortion is defined as the coefficient i  which minimizes the brightness distortion distance, .Based on the thresholding on brightness distortion and chromaticity distortion, every pixel in the current image can be segmented (classified) into one of the following group: the original background, shadow, highlighted background, and the moving foreground.More specifically, the background pixels are classified if brightness and chromaticity distortion are small.The shadow pixels are classified if the chromaticity distortion is small but with lower brightness.And the foreground pixels are classified if the chromaticity distortion is high.Compared to GMM, the statistical models are generally more efficient in building the background model, and can be used to segment out not only the foreground objects but also the shadows.
## (s6) Segmentation by Tracking
(p6.0) The above point-based methods, including background subtraction, GMM and statistical models, describe the low-level segmentation, which can capture the pixel-level details of the objects but is lacking the global information of objects.Hence, the region-based methods for segmentation by tracking are proposed.Brendel et al. [45] propose a segmentation method by tracking regions across frames with a new circular dynamic-time warping (CDTW) algorithm, which generalizes the conventional DTW algorithm to match closed boundaries of two regions, for region matching to identify the longest, best matching boundary portion of two regions.Moreover, Yu et al. [46] propose another segmentation method by tracking the spatial-color Gaussian mixture models (SCGMM).The SCGMMs are built in five dimensions, X, Y, R, G, B, with X, Y for spatial information and R, G, B for color information.Yu et al. also propose a tracking algorithm to iteratively update SCGMMs by fixing the color Gaussian models while updating spatial Gaussian models, and vice versa, with a constrained expectation maximization (EM) algorithm.Both [45] and [46] are region-based segmentation by tracking the objects' regions, whose performance highly depends on the performance of the tracking algorithms.It can avoid the pepper-and-salt noise frequently encountered in the pixel-based segmentation methods by tracking the target regions.However, the methods of segmentation by tracking need to have a robust scheme to segment out the target objects at the very first frame.Several methods have been proposed for the very first frame object segmentation, e.g., [45] applies mean-shift segmentation, on the other hand [46] adopts a boosting face detector [99] and a graph-cut segmentation.
## (s11) Space-Time Volumes (STV)
(p11.0) The space-time volume (STV) is formed by temporally stacking frames over a video sequence as a 3D cuboid of spatial-temporal shape.Blank et al. [2] propose a method, by stacking segmented silhouette frame-by-frame, to form a 3D spatial-temporal shape, from which the space-time features such as local space-time saliency, action dynamics, shape structure and orientation can be extracted [2].Ke et al. [3] further uses the spatial-temporal shapes for shaped-based matching, including spatial-temporal region extraction and region matching.For region matching, an unsupervised clustering technique is applied to group the video into classes of 3D volumes of consistent appearance.In order to overcome the limitation of shape-based approaches, such as changes in camera view and variability in the speed of actions, Ke et al. [3] also incorporate Shechtman and Irani's flow-based features [51] into the classifier to improve the performance.Moreover, Dollar et al. [11] applies a spatio-temporal interest point detector to find local region of interest in the cuboids of space and time for activity recognition.First, cuboids of spatio-temporally windowed data surrounding a feature point extracted from sample behaviors are clustered to form a dictionary of cuboid prototypes.The histogram of the cuboid types is then used as an activity descriptor for object recognition.Generally, the STV features provide a proper way to combine spatial and temporal information; however, STV features normally require good segmented silhouette and are sensitive to viewpoint and occlusion.
## (s12) Discrete Fourier Transform (DFT)
(p12.0) Besides spatial and temporal information, the frequency domain information, that is, the intensity variation of an image can also be taken advantage of.Kumari and Mitra [22] use discrete Fourier transforms (DFTs) of small image blocks as the selected features for activity recognition.It is generally assumed that the gray-level intensity of foreground object is different from that of background object; therefore, DFT of an image can be used to obtain information of the geometric structure (shape) in the spatial domain.The K-nearest neighbor (K-NN) algorithm can then be applied to the DFT features for human activity recognition.Generally, the DFT features can capture the shape energy of image frames, but are also sensitive to noise and occlusion.
## (s14) SIFT Features
(p14.0) Lowe [57,58] first proposes a class of local image features, which are invariant to image scaling, translation, and rotation, and partially invariant to illumination change and affine or 3D projection, known as scale invariant feature transform (SIFT).The scale-invariant features are enabled by using a staged filtering approach.As proposed in [58], the four major steps applied on an image frame to generate SIFT features are summarized as follows.The first step is the scale-space extrema detection, which applies a difference-of-Gaussian (DoG) function to search over all scales and locations of the image, and to identify interest points that are invariant to scale and orientation.The second step is the keypoint localization.Based on the identified interest points in the first step, a detailed model is used to determine the scale and location, and then the more stable points are selected as keypoints.The third step is the orientation assignment based on local image gradient directions.The last step is the keypoint descriptor.At each keypoint, the gradients of the image are measured at the selected scale, and then the measured image gradients are transformed into a representation to allow for local shape distortion and illumination changes.One example of the selected keypoints for SIFT features is shown in Figure 6 [58].Scovanner et al. [12] further introduce a 3D SIFT descriptor, which can reliably capture the spatio-temporal nature of the video data as well as 3D imagery such as MRI data.The SIFT descriptor is a popular descriptor due to its invariance to image rotation and scale and robust to affine distortion, noise corruption and illumination change.However, the high dimensionality is a drawback of SIFT in the feature matching step, because the distinctiveness of SIFT descriptor is achieved by assembling a high-dimensional vector representing the image gradients within a local region of the image.Another significant drawback is the SIFT features are not sufficiently discriminative.For example, the SIFT features are equally likely to be found on human objects and on the background.Moreover, the SIFT uses only grayscale information and misses important appearance information, such as color.
## (s15) HOG Features
(p15.0) Dalal and Triggs [59] were the first to propose histogram of oriented gradient (HOG) descriptors for human detection.The HOG is derived based on evaluating normalized local histograms of image gradient orientations in a dense grid by considering fine-scale gradient, fine orientation binning, relatively coarse spatial binning and high-quality local contrast normalization in overlapping descriptor blocks.An example of HOG descriptors is shown in Figure 7. Lu and Little [5] further propose a template-based algorithm to track and recognize athlete's actions by exploitation of the PCA-HOG descriptor.First, an HOG descriptor is applied to images in a video, and then is projected to a linear subspace by the principal component analysis (PCA).The proposed PCA-HOG descriptor in [5] is invariant to the variation of illuminations, poses, and viewpoints.At each time instance t, three procedures are performed, which are individually sequentially tracking, action recognition and template updating [5].The major drawback of HOG features is that the local descriptors are extracted at a fixed scale; therefore, the size of the human in the image can have great influence on the performance.
## (s16) NWFE Features
(p16.0) Taking into account the distance information and the width feature of a silhouette, Lin et al. [13] propose a new feature, called nonparametric weighted feature extraction (NWFE), to build histogram vectors for human activity recognition by using the nearest neighbor classifiers.NWFE features are extracted from the pose contour by combining the distance [102] and width features [103], which are then projected from the original high-dimensionality feature space to a low-dimensional subspace by PCA transformation and K-means clustering.The NWFE features reduce the computational complexity and still achieve high-recognition rate.However, the main drawbacks of NWFE features are that it relies on accurate human body silhouette and contour, and ignore the color appearance information of the image.
## (s17) LKT (Lucas-Kanade-Tomasi) Features
(p17.0) Lucas-Kanade [100] and Tomasi [101] propose a point tracking method based on the sum of squared intensity differences, named LKT (Lucas-Kanade-Tomasi) feature tracker.An example of fist tracking using LKT feature trackers is shown in Figure 8. Lu et al. [8] use an LKT feature tracker to track human body joints in key frames and actual frames, where key frames contain the prototype information to represent defined posture (one step of action).And a proposed factorized sampling algorithm is used to replace one tracker with a set of LKT trackers to enhance the joints tracking accuracy and recognize non-rigid human actions.Since the LKT feature tracker assumes that neighboring pixels in a small window have the same flow vector, resulting in the main limitation of being difficult to deal with large motion between frames.
## (s19) Appearance-Based Features
(p19.0) Compared with shaped-based features, appearance-based features can provide more discriminative information, such as color, and can be more robust in dealing with occlusion.Several appearance descriptors have been proposed in the literatures [52,53].Sedai et al. [52] propose a novel noiseresilient appearance descriptor, called histogram of local appearance context (HLAC) for 3D human pose estimation.The local appearance context (LAC) descriptors are first computed on the locations of human objects in an image based on HOG, and PCA is further applied for dimensionality reduction, followed by the histogramming of LAC to obtain HLAC.The experimental results showed that HLAC features can have superior performance for human activity recognition due to its incorporation of local appearance and the minimum clutter effect since no background subtraction is needed.

(p19.1) Moreover, Ramanan et al. [53] also use appearance-based features for human tracking.A discriminative model of appearance (color) is built either by a bottom-up approach [106] that looks for candidate body parts in each frame, or by a top-down approach [107] that looks for the entire human body.In [53], an instance-specific model is built first to capture a person's appearance (color).The person and body parts are then tracked by detecting the model in each frame.However, the main limitation of the appearance model is that it is sensitive to clothing and illumination changes.
## (s21) Model-Free
(p21.0) There is no a priori model, only simple blobs to represent the human poses.In [39,60], algorithms are developed to detect the foreground, detect the blobs and track the blobs, which are used to represent head/hands.Nakazawa et al. [61] use an ellipse to represent the human body and perform ellipse tracking by four steps, i.e., extraction of the human region from the image, generation of simulated image, matching and updating of human position.Furthermore, Iwasawa et al. [62] use stick-figures, resembling the human skeleton, to represent human structure information so as to enable the human body tracking.Generally, this model-free class relies on extensive training using ground truth data obtained by commercial motion capture systems.
## (s22) Indirect Model
(p22.0) In an indirect model scheme, a priori model is only used indirectly to guide the interpretation of measured data.Leung and Yang [63] use U-shaped edges to describe the outline of a moving human body from a video sequence.A moving edge detection technique is proposed to generate a more complete outline of the moving object, based on the difference picture and the coincidence edges which are edges of both the difference picture and the original intensity picture.Moreover, Huo et al. [15] use a head-shoulder-upper-body model to detect and track humans by particle filtering [109].The 2D and 3D coordinates of the torso and both hands are then transformed into normalized feature space for pose estimation.In this indirect model class, the estimated poses are generally not very detailed, and it is not easy to handle occlusion or to impose kinematics constraints for human body configuration.
## (s31) Relevance Vector Machine (RVM)
(p31.0) Tipping [81,82] proposes an RVM, which is an SVM-like probabilistic model with fewer kernel functions needed.Compared with an SVM, the RVM can relax the limitation of the SVM such as point prediction by providing a full predictive distribution and still retain the sparseness property of the SVM [82].Agarwal and Triggs [54] further apply RVMs to shape context descriptor features for 3D pose estimation.The main advantage of RVMs is that they allow sparse sets of highly relevant features or training examples to be selected for the training of the separation functions.But the main disadvantage is still the high computation cost of learning an RVM.
## (s32) Artificial Neural Network (ANN)
(p32.0) An ANN [84] is a mathematical model to describe the problems in a network of directed graphs, whose nodes are represented as artificial neurons and the weighted directed edges in the graphs are connections between artificial neurons.As mentioned in [84], an ANN enables to learn complex nonlinear input-output relationships with a universal approximation (a black box) model.With a systematically sequential training procedure, an ANN can train the model to adapt to the input data.ANNs use the concept of biological neuron networks to supply nonlinear capability via more discriminative feature transformation through hidden layers, resulting in more effective classification through multilayer perceptrons (MLPs), which apply the back propagation (BP) supervised learning algorithms to compute suitable connection weights and biases of the network.Fiaz and Ijaz [83] propose a method to design an ANN-based intelligent human activity monitoring system to detect and track suspicious activity in a surveillance environment.A three-layer perceptron is used for the classification of the human activities from the information of the distance vectors and the motion vectors for each frame in a video sequence.Moreover, Foroughi et al. [29] apply four-layer MLPs to learn eigen-motions, which are the movement patterns extracted by an eigenspace technique, for motion classification and falling detection.Although ANNs have the ability to implicitly describe complex nonlinear relationships between dependent and independent variables, they have higher computation burden during the learning, and are prone to over-fitting of the data.
## (s35) Binary Tree
(p35.0) A binary tree is a tree structure with a maximum of two children for each internal node.The binary tree structure can be applied in classification problems, called classification trees.Stauffer and Grimson [87] build a hierarchical classification binary tree by the accumulated joint co-occurrence statistics of the representations in a video sequence.The classification trees take the whole set of prototypes and the co-occurrence matrix to determine two distributions, or called probability mass functions (pmfs), through the prototypes of codebook that best represent the co-occurrence matrix.Moreover, Ribeiro and Santos-Victor [20] investigate the use of hierarchical binary tree classifiers on human activity recognition.For each node of the binary tree classifier, a Bayesian classifier is used and the likelihood functions are modeled and systematically learned as Gaussian mixtures.Binary tree classification is simple and fast, but the separation rules in each node are difficult to be general for other cases, making it difficult for complex scenarios.
## (s36) Multidimensional Indexing
(p36.0) Ben-Arie et al. [21] develop a multi-dimensional indexing method for view-based recognition of human activity from video sequences.More specifically, the human poses and velocity of hands, legs and torso are used to represent an activity and stored in a set of multidimensional hash tables.The activity is then recognized by indexing and sequencing a few pose vectors in the multidimensional harsh table.To be invariant to the speed of the activity, a sequenced-based voting approach is designed for the activity recognition.The computation of the indexing approach is comparatively low.The experimental results in [21] show that the multidimensional indexing method is invariant to viewpoint variations to the extent of 30  degrees in azimuth.
## (s37) K-Nearest Neighbor (K-NN)
(p37.0) The K-nearest neighbor (K-NN) [22] algorithm is a classification method based on the K, a predefined constant, closest training data in the feature space.A point/vector is classified to one label, which is the most frequent label among K nearest training points/vectors.Because K-NN classification decision is based on K neighborhood points/vectors, therefore K-NN can be easily used in multi-modal classification tasks.As described in [22], there are some advantages for K-NN.First, K-NN is a simple model with few parameters.Secondly, the computation time for testing phase is independent of the number of classes.Thirdly, K-NN is robust in the search space even for nonlinearly separable data.Kumari and Mitra [22] use DFT of the small image blocks as feature selection, and apply K-NN as the classifier for human activity recognition.The main drawback for K-NN is that the classification performance is sensitive to the selection of K. Different K values can be evaluated and validated during the training phase to decide the best K before performing classification.
## (s40) Trajectory
(p40.0) A trajectory is the path that a person moves as a function of time.The trajectory of a tracked person in a scene is often used to analyze the activity or behavior of the tracked person.Lu and Little [5] use a PCA-HOG descriptor to track and recognize sports videos, such as hockey and soccer.When the player runs with a ball, the trajectory is used to analyze the player's run-left, run-right, runs in, or run-out.Moreover, the loitering behavior can be easily inferred by analyzing trajectories.Bodor et al. [10] use Kalman filters to analyze pedestrian location and velocity, which are used to classify either a walking pedestrian, a running pedestrian, a loitering pedestrian or a falling-down pedestrian.Bird et al. [112] use an appearance-based method to detect loitering pedestrians by adopting a linear discriminative method based on the clothing color.The time stamps for each pedestrian are used to judge the present duration for a pedestrian, and a strip-shifting algorithm is designed to detect the loitering cases.Furthermore, the stalking behavior is another popular topic by tracking the trajectories.Niu et al. [113] propose a framework to detect the stalking behavior for surveillance environment based on simple motion detection algorithms such as frame differencing and feature correlation.SVM with the Gaussian kernel function is then used to recognize three behaviors including following, following-and-gaining and stalking behavior.The following behavior is characterized by an almost constant relative position and a nearly zero relative velocity; the followingand-gaining behavior is characterized by a linearly-shrinking change in the relative position and a nearly constant, but non-zero relative velocity; the stalking behavior is similar to the following-andgaining behavior, but with a much larger variance in both relative position and velocity.
## (s41) Falling Detection
(p41.0) Another popular topic of single person activity recognition is falling detection.Falling detection is significantly critical for security and safety environments, especially for the elderly who live alone.Töreyin et al. [114] model human motions with HMMs, and fuse audio channel data with the results of HMMs to detect a falling event.The audio information is essential to distinguish a falling person from a person simply sitting down or sitting on the floor.Moreover, Shieh and Huang [115] propose a human-shape-based algorithm to extract the one-pixel-wide edge features, and then a string matching algorithm [116,117] is applied for falling detection.But the building of the templates for falling posture might be costly, due to the various postures of falling.Furthermore, Sengto and Leauhatong [118] propose a falling detection algorithm by using a back-propagation neural network (BPNN) and a triaxial accelerometer.Four daily activities including walking, jumping, getting into bed and rising from the bed, and four falling actions including front fall, back fall, left fall and right fall, are recognized by the BPNN.However, the limitation of the accelerometer is needing to wear it, as it is cumbersome and easy to forget to put on.Foroughi et al. [26][27][28][29] conduct some methods for falling detection based on human shape information and multi-class SVMs, integrated with time motion images and BPNNs.Without the additional tri-axial accelerometer, falling detection can be made more natural and smooth to incorporate into daily life.
## (s43) Multiple People Interaction and Crowd Behavior
(p43.0) Multiple people interaction and the crowd behavior have drawn much attention recently due to the needs of environment security.Jacques Junior et al. [119] reviewed crowd analysis literature and tackled three important issues including people counting, people tracking, and crowd behavior understanding.

(p43.1) With regard to people counting, Subburaman et al. [120] investigate the count estimate of people in a crowded scene by detecting the head region, based on the state-of-art cascade of boosted integral features.The PETS 2012 and Turin metro station dataset are used to evaluate the performance of the head counting system.Merad et al. [121] propose a fast people counting method by using skeleton graph.The skeleton silhouette is decomposed into the head, torso and limbs, and the 3D head pose is estimated by finding the rigid transformation, minimizing the sum of square errors with an Orthogonal Iteration (OI) algorithm [122].Head counting is achieved by detecting the heads of people in a scene in order to count the number of pedestrians passing an indoor area.

(p43.2) Moreover, in the aspect of people tracking, McKenna et al. [123] propose a method to track groups of people by an adaptive background subtraction method combining color and gradient information to remove shadows and unreliable color cues.The case of a group splitting up into several groups, and the case of several groups merging into one group are dealt with by color histogram information.Some interactions with objects can also be detected.If a person removes or deposits an object, a new region will be split from the person.However, only groups of people are tracked, instead of individual persons.Chu et al. [124] propose a method for human tracking by adaptive Kalman filtering and multiple kernels tracking with projected gradients.The multiple kernels tracking is applied to deal with the occlusion cases.A person is modeled in two or four kernels, e.g., the upper body-part kernel and the lower body-part kernel for two-kernel cases.The CAVIAR, PETS 2010, i-LIDS, and Crowd Analysis datasets are used to evaluate the performance of the people tracker.

(p43.3) Furthermore, with regard to crowd behavior understanding, Saxena et al. [125] propose to model crowd events for specific end-user scenarios by the use of an extended Scenario Recognition Engine (SRE) [126] based on a Kanade-Lucas-Thomasi (KLT) tracker for multiple-frame feature point detection and tracking.The speed, direction and location of a crowd can be estimated in a scene and used to build event models.In the fighting event, the normal behaviors and abnormal behaviors are detected, including crowd moving forward (normal), opposite movement in a crowd (abnormal), crowd stopped (normal), and strong lateral crossing to right/left (robbery by crowd, abnormal).Szczodrak et al. [127] use optical flow combined with artificial neural network (ANN) to evaluate the types of crowd behavior.The crowd exit path observation is used to detect the people flow interruption caused by slowdown or stopping, based on the measurement of speed of the crowd flow.The normal behavior (walking), diverging, and abnormal behavior are classified by ANN.Besides, Cho and Kang et al. [128] propose a method to detect abnormal crowd behavior using integrated multiple behavior models and optical flow.The method can reflect not only social properties [129] but also personal properties such as speed and direction; therefore, it can effectively detect abnormal behavior in a crowd scene.
## (s47) Entertainment Environments
(p47.0) Human activity recognition can also be used to recognize entertainment activities, such as sport [3][4][5]7], dance [2,51] and gaming [10,15,39], in order to enrich lifestyles.Various video-based entertainment activity recognition systems have been proposed.

(p47.1) For the purpose of recognizing sportive activities, Yamato et al. [4] might well be pioneers in applying HMMs to recognize the time-sequential images of tennis scenes, including six tennis strokes such as forehand stroke, backhand stroke, forehand volley, backhand volley, smash and serving.In [7], Luo et al. developed an object-based method for video analysis and interpretation of sports video sequences.The sport behaviors are effectively recognized by using DBNs, which can generate a hierarchical description for video events, including bowling, downhill skiing, golf swing, pitching, and ski jumps recorded from real scenarios, with cluttered background and moving cameras.Ke et al. [3] exploit the use of volumetric features for the recognition of actions such as serve, run right and return serve actions in the tennis sequences when combined with flow-based correlation techniques.By using an extended behavior-based similarity measure, Shechtman and Irani [51] were successful in detecting dives into a pool during a swimming relay match.Despite the numerous simultaneous activities and despite the severe noise, this method is able to separate most dives from other activities.

(p47.2) The 3D shapes induced by the 2D silhouettes in the space-time volume are used by Blank et al. [2] for action detection in a ballet movie.The method utilizes properties of the solution to the Poisson equation to extract space-time features, such as local space-time salience, action dynamics, shape structure and orientation.The effectiveness and robustness of the method is demonstrated via an example of finding all the locations in the movie where an action called "cabriole" (beating feet together at an angle in the air) is performed by either a male or a female dancer.Besides the ability of detecting diving in a pool, the method developed by Shechtman and Irani [51] is also successful in detecting the single turn of a dancer in a very fast moving ballet video sequence.

(p47.3) One of the most popular leisure activities is playing video games.A number of methods are developed for this purpose [10,15,39].Richard et al. [39] developed Pfinder as a real-time system for tracking people and interpreting their actions by using a multi-class statistical model of color and shape to obtain the head, hands and feet positions in different viewing conditions.Pfinder has been successfully used in several different human interface applications, e.g., to navigate a 3D virtual game environment or to place the player at a particular place in a virtual room, which is populated by virtual occupants from real-time 3D computer graphics based on live video.In [15], Huo et al. present a method for human motion capture and pose recognition.The human torso and the hands are segmented from the whole body and tracked over time.A 2D model is used for the torso detection and tracking, while a skin color model is utilized for the hands tracking.Moreover, 3D location of these body parts are calculated and further used for pose recognition.The implementation of the proposed approach is simple, easy to realize, and suitable for real gaming applications.Ke et al. [69] also perform a similar 3D human pose estimation task, but they only use one camera rather than multiple cameras as in [15].Moreover, their system can estimate 3D poses not only the upper body part as in [15] but also the lower body part including knees and feet.A simple video game is implemented, i.e., a 3D avatar, generated by real-time, based on the derived 3D poses of the proposed system, to hit balls so as to get scores or to avoid attacks from flying balls.
## (s48) Healthcare Systems
(p48.0) The applications for activity recognition in healthcare systems analyze and understand patients' activities, so as to facilitate health workers to diagnose, treat and care for patients, resulting in improving the reliability of diagnosis, decreasing the working load for the medical personnel, shortening the hospital stay for patients, and improving patients' quality of life, etc. [1,[23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38].
## (s49) Daily Life Activity Monitoring
(p49.0) Daily life activity monitoring mainly focuses on learning and recognizing the daily life activities of seniors at home.The proposed systems are to provide seniors an opportunity to live safely, independently and comfortably.In order to accomplish this, most proposed systems continuously capture the movements of individual senior or multiple seniors at home, automatically recognizing their activities, and detecting gradual changes in baseline activities such as mobility functional disabilities, mental problems, as well as the urgent warning signs of abnormal activities such as falling down or having a stroke.Some of these scenarios can be summarized as follows.

(p49.1) Respiration behavior can be critical in diagnosing a patient's illness or recognizing distress during sleep.Many diseases, such as obstructive sleep-apnea syndrome, cardiovascular disease, and stroke, can induce abnormal respiration.Automated respiration monitoring is performed by Kuo et al. [23], where near-IR images are captured to measure the sleeper's respiration based on the periodic rising and falling motions of their chest or abdomen.

(p49.2) Gao et al. [24] measure feeding difficulties of nursing home residents with severe dementia, by automatically measuring the number of hand movements to the mouth using motion feature vectors and an HMM to identify the start and end of individual dining events.

(p49.3) Huynh et al. [25] present a video monitoring method for detecting and tracking face, mouth, hands and medication bottles in the context of medication intake.This aims to monitor medicine intake behavior of elderly at home to avoid the inappropriate use of medicine.

(p49.4) Falling is a major health risk of elderly as it is known to be the leading cause of injury and deaths among seniors.Foroughi et al. [26][27][28][29] conduct some methods to detect the fall, e.g., based on human shape variation.Extracted features, including combination of best-fit approximated ellipse around the human body, projection histograms of the segmented silhouette and temporal changes of head pose, are fed to a multi-class SVM [26] or an MLP ANN [27] for reliable classification of motions and determination of a fall event.Other features, also widely used for fall detection, are based on the combination of integrated time motion images (ITMI) and Eigen space technique [28,29].

(p49.5) In order to recognize the activities at a higher semantic level, the activity duration, the position of human, the interaction between people and person-object are the essential elements to be analyzed.

(p49.6) For the activity duration, Luhr et al. [30] use the explicit state duration HMM (ESD-HMM), in which a duration variable is introduced in a standard HMM.Duong et al. [1] introduce the switching hidden semi-Markov model (S-HSMM), which implicitly exploits the benefit of both the inherent hierarchical organization of the activities and their typical duration.In [31], similar to [1], Duong et al. further explicitly add the time duration information to a standard HMM, called hidden semi-Markov model (HSMM), to model the state duration by using the generic exponential family.

(p49.7) For the human position, in [31], the door, the stove, the fridge, the sink, the cupboard, and the table areas are used to identify the activities in a kitchen room.For example, meal preparation and consumption consists of twelve steps: take-food-from-fridge → bring-food-to-stove → wash-vegetable → come-back-to-stove-for-cooking → take-plates/cup-from-cupboard → return-to-stove-for-food → bring-food-to-table → take-drink-from-fridge → have-meal-at-table → clean-stove → wash-dishes-atsink → leave-the-kitchen.In [1], the kitchen is quantized into 28 square cells of 1 m 2 each and the position of the human is captured by four cameras mounted at the ceiling corners, and the tracking system returns the list of cells visited by the person as the moving trajectory path.

(p49.8) For the interaction, Liu et al. [32] propose an interaction-embedded hidden Markov model (IE-HMM) framework, for detecting and classifying individual human activities and group interactions in a nursing home environment.
## (s50) Rehabilitation Applications
(p50.0) Traditional rehabilitation systems often require patients to undergo several clinical visits for the physical therapy exercises and the scheduled evaluation until his/her full recovery of mobility function for daily activities.Such clinical visits can be avoided by using innovative rehabilitation systems, which are home-centered and self-health care with the help of video-based activity recognition techniques.Moreover, by continuously monitoring the daily activities and gaits, the early symptoms of some diseases can be timely detected so that the diagnosis and interventions are more appropriate.Some of these scenarios can be summarized as follows.

(p50.1) Stroke is a major cause of disability and health care expenditure around the world.Ghali et al. [33] design a system to provide real-time feedback to stroke patients performing daily activities necessary for independent living.More specifically, they envisage a situation in which a stroke patient stands in a standard kitchen and makes a cup of coffee in the usual way.The position and movement of the patient's hands and the objects he/she manipulates are captured by overhead cameras and monitored using histogram-based recognition methods.The key events (e.g., picking up a cup) are recognized and interpreted in the context of a model of the coffee-making task.

(p50.2) In order to objectively evaluate the improvement of motor functions of the elders at home, as well as to reduce burden on fitness instructors, Ryuichi et al. [34] propose a "multimedia fitness exercise progress notes" system, where the video capturing exercise movements of the elders are sent to an analysis center.Snapshots of the captured videos are used to semi-automatically measure many kinds of exercise parameters, such as lap time, distances and angles.

(p50.3) Goffredo et al. [35] propose the Gauss-Laguerre transform-based (GLT-based) motion estimation method in order to analyze the sit-to-stand (STS) motion from monocular videos.STS movement mainly involves hip and knee flexion-extension, and ankle plantar flexion-dorsiflexion is analyzed by utilizing a 2D human body model that considers the projections of body segments on the sagittal plane.

(p50.4) Besides sit-to-stand, walking gait is another human activity of great interest to many researchers due to the fact that the loss of ability to walk correctly can be caused by a serious health problem, such as pain, injury, paralysis, muscle damage, or even mental problems.Liao et al. [36] present a video-based system for analyzing four posture features of human walking, including body line, neck line, center of gravity (COG) and gait width based on the extracted silhouettes from front view and side view.Similarly, Leu et al. [37] also use two cameras and the feedback control structure at the segmentation level to extract the gait features, such as torso angle, left and right thigh angles, and left and right shank angles.With the aim of reducing the inconvenience of using many methods that require the captured images of human walking from either front or side view, Li et al. [38] propose a system with much less restrictions on walking direction.The system successfully extracts gait features, such as COG and pace length, from images obtained from two cameras with orthogonal views.
## (s51) Conclusions and Future Direction
(p51.0) Although progress in recent video-based human activity recognition has been encouraging, there are still some apparent performance issues that make it challenging for real-world deployment.More specifically:  The viewpoint issue remains the main challenge for human activity recognition.In real world activity recognition systems, the video sequences are usually observed from arbitrary camera viewpoints; therefore, the performance of systems needs to be invariant from different camera viewpoints.However, most recent algorithms are based on constrained viewpoints, such as the person needs to be in front-view (i.e., face a camera) or side-view.Some effective ways to solve this problem have been proposed, such as using multiple cameras to capture different view sequences then combining them as training data or a self-adaptive calibration and viewpoint determination algorithm can be used in advance.Sophisticated viewpoint invariant algorithms for monocular videos should be the ultimate objective to overcome these issues. Since most moving human segmentation algorithms are still based on background subtraction, which requires a reliable background model, a background model is needed that can be adaptively updated and can handle some moving background or dynamic cluttered background, as well as inconsistent lighting conditions.Learning how to effectively deal with the dynamic cluttered background as well as how to systematically understand the context (when, what, where, etc.), should enable better and more reliable segmentation of human objects.Another important challenge requiring research is how to handle occlusion, in terms of body-body part, human-human, human-objects, etc.  Natural human appearance can change due to many factors such as walking surface conditions (e.g., hard/soft, level/stairs, etc.), clothing (e.g., long dress, short skirt, coat, hat, etc.), footgear (e.g., stockings, sandals, slippers, etc.), object carrying (e.g., handbag, backpack, briefcase, etc.) [143].The change of human action appearance leads researchers to a new research direction, i.e., how to describe the activities that are less sensitive to appearance but still capture the most useful and unique characteristics of each action. Unlike speech recognition systems, where the features are more or less unified to be the mel-frequency cepstral coefficients (MFCCs) for HMM classifiers, there are still no clear winners on the features for human activity recognitions, nor the corresponding classifier designs.It can be expected that 3D viewpoint invariant modeling of human poses would be a good starting point for a unified effort.

(p51.1) Finally, human activity recognition tasks constitute the foundation of human behavior understanding, which requires additional contextual information such as W5+ (who, where, what, when, why, and how) [144].The same activity may have different behavior interpretations depending on the context in which it is performed.More specifically, the "where" (place) context can provide the location information to be used to detect abnormal behaviors.For example, lying down on the bed or a sofa is interpreted as taking a rest or sleeping, but in inappropriate places such as the floor of the bathroom or kitchen, it can be interpreted as a fall or a sign of a stroke.Moreover, the "when" (time) context also plays another important contextual role for behavior understanding.For example, a person usually watching TV after midnight can be regarded as an insomniac.Another example is that a person will be detected as picking something up if he/she squats and stands up quickly.However, if he/she squats for a longer period, there might be a motion difficulty due to osteoarthritis or senility.Furthermore, the number of repetitions of an action can also be informative.For example, eating too many times or too little a day can be an early symptom of depression.The interaction between people or between person and objects is also a good indicator to identify the meaning of the activity.For example, if a person is punching a punch-bag, he might be doing exercise.But if he is punching the wall, it can indicate anger or a mental disorder.

(p51.2) In conclusion, this review provides an extensive survey of existing research efforts on video-based human activity recognition systems, covering all critical modules of these systems such as object segmentation, feature extraction and representation, and activity detection and classification.Moreover, three application domains of video-based human activity recognition are reviewed, including surveillance, entertainment and healthcare.In spite of the great progress made on the subject, many challenges are raised herein together with the related technical issues that need to be resolved for real-world practical deployment.Furthermore, generating descriptive sentences from images [145] or videos is a further challenge, wherein objects, actions, activities, environment (scene) and context information are considered and integrated to generate descriptive sentences conveying key
