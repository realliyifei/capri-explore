# A Comparative Survey of Recent Natural Language Interfaces for Databases

CorpusID: 195316636 - [https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d](https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d)

Fields: Linguistics, Computer Science

## (s7) J, 2xS
(p7.0) for the evaluation of SODA (Blunschi et al (2012)) and the 10 sample questions of this paper. Therefore, we designed nine questions based on the operators of SQL and SPARQL: Joins, Filters (string, range, date or negation), Aggregations, Ordering, Union and Subqueries.

(p7.1) Furthermore, we added a question which is based on a concept (e.g., 'great movie'). Concepts are not part of SQL and SPARQL, but a common addition of NLIs. Table 1 shows those ten full sentence questions in English, which can be applied on the sample world (ordered roughly by difficulty).

(p7.2) The queries were designed in such a way that they cover a wide range of SQL-functionality (technical challenge) as well as linguistic variations (semantic challenge). We will now analyze each of these ten queries in more detail and describe the major challenges for the underlying system to solve them.

(p7.3) The first question (Q1) is a join over different tables (Person, Director, Directing and Movie) with an ISA-relationship between the tables Person and Director. Moreover, the query has a filter on the attribute Movie.Title, which has to be equal to 'Inglourious Basterds'. Therefore, the system faces three different challenges: (a) identify the bridge table Directing to link the tables Director and Movie, (b) identify the hierarchical structure (ISA-relationship) between Director and Person and (c) identify 'Inglourious Basterds' as a filter phrase for Movie.Title.

(p7.4) The second question (Q2) is based on a single table (Movie) with a range filter. The challenge for the NLIs is to translate 'higher than' into a comparison operator 'greater than'.

(p7.5) The third question (Q3) is a join over four tables (Person, Actor, Starring and Movie) and includes two filters: (a) a filter on the attribute Person.FirstName and Person.LastName and (b) a two-sided date range filter on the attribute Movie.ReleaseDate. The challenge in this query (compared to the previous ones) is the date range filter. The system needs to detect that 'from 2000 until 2010 ' refers to a range filter and that the numbers need to be translated into the dates 2000-01-01 and 2010-12-31.

(p7.6) The fourth question (Q4) is a join over two tables (Movie and Gross). In addition, an aggregation on the attribute Gross.Gross and grouping on the attribute Movie.id or ordering the result based on Gross.Gross is needed. For both approaches an aggregation to a single result (indicated by the keyword 'most') is requested.
## (s8) Question Analysis
(p8.0) In this section, we perform an analysis comparing our ten sample input questions with the two well-known question-answering corpora Yahoo! QA Corpus L6 2 (more than 4 million questions within a user community) and GeoData250 (Tang and Mooney, 2001) (250 questions against a database). We also summarize the findings of Bonifati et al (2017) and compare them to our input questions. The goal of the analysis is to better understand what types of questions users pose and how representative our sample input questions are (i.e., establish some external validity of their representativeness).

(p8.1) For the Yahoo! Corpus, we decided to only look into the labeled subset of movie questions since our sample world is based on movies. Out of these questions related to movies we extracted a random sample set of 100 questions. We used all the GeoData250 Questions. We labeled each of those 350 questions from both sources with Q1 to Q10 based on what poses the challenge in answering them.

(p8.2) For example, the GeoData250 question 'give me the cities in virginia? ' is labeled with Q1, because the challenge of this question is to identify the right filter ('virginia'). The question 'what is a good movie to go see this weekend? ' includes a time range and is therefore labeled as Q6. For the Yahoo! Corpus we also made some assumptions, for example, the question 'what is your favorite tom hanks movie? ' is interpreted as 'give me the best ranked tom hanks movie' and labeled with Q4. Furthermore, if a question could have multiple labels, the label of the more difficult (higher number) question is chosen. For example, the sample question 'can anyone tell a good action movie to watch? ' is labeled with Q6 because it requires handling of a concept ('good movie') and not Q1 because it uses a filter ('action movie'). If the question cannot be labeled with one of the input questions, we label it with x 3 . For example, Fig. 3 Mapping of 100 sample questions of Yahoo! QA Corpus L6 (movie) to our ten sample world questions. the question 'i want to make a girl mine but she is more beautiful than me. what can i do now? ' has nothing to do with movies.

(p8.3) As we can see in Figure 3, more than 40% of the Yahoo! questions are based on filtering only, i.e. corresponding to question Q1. For example, the question 'what movie had "wonderful world" by sam cooke at the beginning? ' has filters for the song 'wonderful world ' and a join on movie. About 30% of the questions are labeled with x which are off-topic questions. There are no questions labeled with Q2, this means that there are no questions with a numerical range. This can be explained by the composition of the corpus itself, which is a collection of questions from users to users. If users ask about the ranking of a movie, they ask something like 'what is your favorite movie? ' and not something similar to Q2.

(p8.4) In Figure 4, we can see the distribution of question types for the GeoData250 corpus. About 88% of the questions are labeled with Q1 or Q4. There are three concepts ('population density', 'major city' and 'major river ') used in the corpus that occur in roughly 8% of the questions, 7% of which are labeled with Q6. There are no numerical range questions (Q2) and no date questions (Q3). The latter can be explained by the dataset not including any dates. There are also no unions (Q5) and no questions with multiple subqueries (Q10). Bonifati et al (2017) investigated a large corpus of query logs from different SPARQL endpoints. The query log files are from seven different data sources from various domains. One part of the analysis was done by counting the SPARQL keywords used in the queries. Over all domains, 88% of the queries are Select-queries and 40% have Filter. Furthermore, they found huge differences between different domains. For example, the use of Filter ranges from 61% (LinkedGeoData) to 3% (OpenBioMed) or less. This implies that the distribution of the usage for the question types are domaindependent. Nevertheless, our ten sample question are fully covered in the query log analyzed by Bonifati et al (2017).

(p8.5) Our analysis indicates that our ten sample questions represent a large range of questions typically queried in question answering systems. Note that open-ended questions -the ones we labeled as x in the Yahoo! Corpus -are not covered by our analysis.

(p8.6) In this section, we perform an analysis comparing our ten sample input questions with the two well-known question-answering corpora Yahoo! QA Corpus L6 2 (more than 4 million questions within a user community) and GeoData250 (Tang and Mooney, 2001) (250 questions against a database). We also summarize the findings of Bonifati et al (2017) and compare them to our input questions. The goal of the analysis is to better understand what types of questions users pose and how representative our sample input questions are (i.e., establish some external validity of their representativeness).

(p8.7) For the Yahoo! Corpus, we decided to only look into the labeled subset of movie questions since our sample world is based on movies. Out of these questions related to movies we extracted a random sample set of 100 questions. We used all the GeoData250 Questions. We labeled each of those 350 questions from both sources with Q1 to Q10 based on what poses the challenge in answering them.

(p8.8) For example, the GeoData250 question 'give me the cities in virginia? ' is labeled with Q1, because the challenge of this question is to identify the right filter ('virginia'). The question 'what is a good movie to go see this weekend? ' includes a time range and is therefore labeled as Q6. For the Yahoo! Corpus we also made some assumptions, for example, the question 'what is your favorite tom hanks movie? ' is interpreted as 'give me the best ranked tom hanks movie' and labeled with Q4. Furthermore, if a question could have multiple labels, the label of the more difficult (higher number) question is chosen. For example, the sample question 'can anyone tell a good action movie to watch? ' is labeled with Q6 because it requires handling of a concept ('good movie') and not Q1 because it uses a filter ('action movie'). If the question cannot be labeled with one of the input questions, we label it with x 3 . For example, Fig. 3 Mapping of 100 sample questions of Yahoo! QA Corpus L6 (movie) to our ten sample world questions. the question 'i want to make a girl mine but she is more beautiful than me. what can i do now? ' has nothing to do with movies.

(p8.9) As we can see in Figure 3, more than 40% of the Yahoo! questions are based on filtering only, i.e. corresponding to question Q1. For example, the question 'what movie had "wonderful world" by sam cooke at the beginning? ' has filters for the song 'wonderful world ' and a join on movie. About 30% of the questions are labeled with x which are off-topic questions. There are no questions labeled with Q2, this means that there are no questions with a numerical range. This can be explained by the composition of the corpus itself, which is a collection of questions from users to users. If users ask about the ranking of a movie, they ask something like 'what is your favorite movie? ' and not something similar to Q2.

(p8.10) In Figure 4, we can see the distribution of question types for the GeoData250 corpus. About 88% of the questions are labeled with Q1 or Q4. There are three concepts ('population density', 'major city' and 'major river ') used in the corpus that occur in roughly 8% of the questions, 7% of which are labeled with Q6. There are no numerical range questions (Q2) and no date questions (Q3). The latter can be explained by the dataset not including any dates. There are also no unions (Q5) and no questions with multiple subqueries (Q10). Bonifati et al (2017) investigated a large corpus of query logs from different SPARQL endpoints. The query log files are from seven different data sources from various domains. One part of the analysis was done by counting the SPARQL keywords used in the queries. Over all domains, 88% of the queries are Select-queries and 40% have Filter. Furthermore, they found huge differences between different domains. For example, the use of Filter ranges from 61% (LinkedGeoData) to 3% (OpenBioMed) or less. This implies that the distribution of the usage for the question types are domaindependent. Nevertheless, our ten sample question are fully covered in the query log analyzed by Bonifati et al (2017).

(p8.11) Our analysis indicates that our ten sample questions represent a large range of questions typically queried in question answering systems. Note that open-ended questions -the ones we labeled as x in the Yahoo! Corpus -are not covered by our analysis.
## (s11) Synonymy
(p11.0) The difficulty of synonymy is that a simple lookup or matching is not enough. For example, the question 'All movies starring Brad Pitt from 2000 until 2010.' (Q3) could also be phrased as 'All movies playing Brad Pitt from 2000 until 2010.'. The answer should be the same, but because in the sample world no element is named 'playing', a lookup would not find an answer. Therefore, it is necessary that the system takes synonyms into account. A possible solution is the use of a translation dictionary. Usually such a dictionary is based on DBpedia (Lehmann et al, 2012) and/or WordNet (Miller, 1995).

(p11.1) The difficulty of synonymy is that a simple lookup or matching is not enough. For example, the question 'All movies starring Brad Pitt from 2000 until 2010.' (Q3) could also be phrased as 'All movies playing Brad Pitt from 2000 until 2010.'. The answer should be the same, but because in the sample world no element is named 'playing', a lookup would not find an answer. Therefore, it is necessary that the system takes synonyms into account. A possible solution is the use of a translation dictionary. Usually such a dictionary is based on DBpedia (Lehmann et al, 2012) and/or WordNet (Miller, 1995).
## (s16) Limitations
(p16.0) This survey's evaluation focuses on the ten sample questions introduced in Section 2.2. Those sample questions are based on the operators of the formal query languages SQL and SPARQL. This leads to the following limitations of the evaluation.

(p16.1) Limitation 1 -Theoretical: Our evaluation is theoretical and only based on the papers. A few systems have online demos (e.g., SPARKLIS (Ferré, 2017)), but others -especially older ones -are not available any more. Therefore, to handle all systems equally, we based our evaluation fully on the papers of the systems.
## (s25) Précis
(p25.0) Précis (Simitsis et al, 2008) is a keyword-based NLI for relational databases, which supports multiple terms combined through the operators AND, OR and NOT. For example, input question 'Show me all drama and comedy movies.' (Q5) would be formulated as '"drama" OR "comedy"'. The answer is an entire multi-relation database, which is a logical subset of the original database.

(p25.1) First, Précis transforms the input question into disjunct normal form (DNF). Afterwards, each term in the DNF is looked up in the inverted index of the base data. This is different to SODA, where the inverted index includes the meta data. If a term cannot be found, the next steps are not executed. The third step creates the schema of the logical database subset, which represents the answer of the input question. This includes the identification of the necessary join paths.

(p25.2) The strength of Précis is the ability to use brackets, AND, OR and NOT to define the input question. However, the weaknesses are that this again composes a logical query language, although a simpler one. Furthermore, it can only solve boolean questions, and the input question can only consist of terms which are located in the base data and not in the meta data. For example, the input question 'Who is the director of "Inglourious Basterds"? ' (Q1) cannot directly be solved because 'director ' is the name of a table and therefore part of the meta data. There is a mechanism included that adds more information to the answer (e.g., the actors, directors etc. to a movie), but then the user would have to search for the director in the answer.

(p25.3) Précis (Simitsis et al, 2008) is a keyword-based NLI for relational databases, which supports multiple terms combined through the operators AND, OR and NOT. For example, input question 'Show me all drama and comedy movies.' (Q5) would be formulated as '"drama" OR "comedy"'. The answer is an entire multi-relation database, which is a logical subset of the original database.

(p25.4) First, Précis transforms the input question into disjunct normal form (DNF). Afterwards, each term in the DNF is looked up in the inverted index of the base data. This is different to SODA, where the inverted index includes the meta data. If a term cannot be found, the next steps are not executed. The third step creates the schema of the logical database subset, which represents the answer of the input question. This includes the identification of the necessary join paths.

(p25.5) The strength of Précis is the ability to use brackets, AND, OR and NOT to define the input question. However, the weaknesses are that this again composes a logical query language, although a simpler one. Furthermore, it can only solve boolean questions, and the input question can only consist of terms which are located in the base data and not in the meta data. For example, the input question 'Who is the director of "Inglourious Basterds"? ' (Q1) cannot directly be solved because 'director ' is the name of a table and therefore part of the meta data. There is a mechanism included that adds more information to the answer (e.g., the actors, directors etc. to a movie), but then the user would have to search for the director in the answer.
## (s26) QUICK (QUery Intent Constructor for Keywords)
(p26.0) QUICK (Zenz et al, 2009) is an NLI that adds the expressiveness of semantic queries to the convenience of keyword-based search. To achieve this, the users start with a keyword question and then are guided through the process of incremental refinement steps to select the question's intention. The system provides the user with an interface that shows the semantic queries as graphs as well as textual form.

(p26.1) In a first step, QUICK takes the keywords of the input question and compares them against the KB. Each possible interpretation corresponds to a semantic query. For example, the input question 'Brad Pitt' can either mean 'movies where Brad Pitt played in', 'movies directed by Brad Pitt' or 'movies written by Brad Pitt' (see Figure 1). In the next step, the system provides the users with the information in such a way that they can select the semantic query which will answer their question. To do so, QUICK provides the users with possible interpretations of each keyword to select from. This is done with a graph as well as a textual form of the semantic query. The textual form is a translation of the SQL query into natural language based on templates. Furthermore, the system orders the keywords in such a way that the user interactions are as few as possible. When the users select the desired semantic query, QUICK executes it and displays the results in the user interface.

(p26.2) The strength of QUICK is the user interaction interface with the optimization for minimal user interaction during the semantic query selection. The weakness of QUICK is that it is limited to acyclic conjunctions of triple patterns.

(p26.3) QUICK (Zenz et al, 2009) is an NLI that adds the expressiveness of semantic queries to the convenience of keyword-based search. To achieve this, the users start with a keyword question and then are guided through the process of incremental refinement steps to select the question's intention. The system provides the user with an interface that shows the semantic queries as graphs as well as textual form.

(p26.4) In a first step, QUICK takes the keywords of the input question and compares them against the KB. Each possible interpretation corresponds to a semantic query. For example, the input question 'Brad Pitt' can either mean 'movies where Brad Pitt played in', 'movies directed by Brad Pitt' or 'movies written by Brad Pitt' (see Figure 1). In the next step, the system provides the users with the information in such a way that they can select the semantic query which will answer their question. To do so, QUICK provides the users with possible interpretations of each keyword to select from. This is done with a graph as well as a textual form of the semantic query. The textual form is a translation of the SQL query into natural language based on templates. Furthermore, the system orders the keywords in such a way that the user interactions are as few as possible. When the users select the desired semantic query, QUICK executes it and displays the results in the user interface.

(p26.5) The strength of QUICK is the user interaction interface with the optimization for minimal user interaction during the semantic query selection. The weakness of QUICK is that it is limited to acyclic conjunctions of triple patterns.
## (s28) SINA
(p28.0) SINA (Shekarpour et al, 2015) is a keyword-based NLI that transforms natural language input questions into conjunctive SPARQL queries. It uses a Hidden Markov Model to determine the most suitable resource for a given input question from different datasets.

(p28.1) In the first step, SINA reduces the input question to keywords (similar to NLP-Reduce), by using tokenization, lemmatization and stop word removal. In the next step, the keywords are grouped into segments, with respect to the available resources. For example, the keywords 'Inglourious' and 'Basterds' would be grouped into one segment based on the match for 'Inglorious Basterds'. In the third step, the relevant resources are retrieved based on string matching between the segments and the RDF label of the resource. In the following step, the best subset of resources for the given input question is determined (ranking). The fifth step, SINA constructs the SPARQL query using the graphstructure of the database. Finally, the results, retrieved by evaluating the generated SPARQL query, are shown to the users.
## (s29) Aqqu
(p29.0) Aqqu (Bast and Haussmann, 2015) is an NLI which uses templates to identify possible relations between keywords. At the end of the translation process, ML is used to rank the possible solutions.

(p29.1) To translate the input question into SPARQL, first the entities from the KB that match (possibly overlapping) parts of the input question are identified. The possible parts are identified by using PoS tags. For example, single token parts must be a noun (NN) and proper nouns (NNP) are not allowed to be split (e.g., 'Brad Pitt'). In the next step, Aqqu uses three different templates which define the general relationship between the keywords. Afterwards, Aqqu tries to identify the corresponding relationship. This can either be done with the help of the input question (verbs and adjectives), or with the help of ML which for example can identify abstract relationship like 'born → birth date'. The last step is the ranking which is solved with ML. The best result is achieved by using a binary random forest classifier.
## (s30) Pattern-based systems
(p30.0) Pattern-based NLIs are an extension of keyword-based systems with natural language patterns to answer more complex questions like concepts (Q6) or aggregations (Q7). For example, the question 'What was the best movie of each genre? ' (Q7) cannot be formulated with keywords only. It needs at least some linking phrase between 'best movie' and 'genre', which indicates the aggregation. This could be done with the non-keyword token (trigger word) 'by' for the aggregation, which will indicate that the right side includes the keywords for the group by-clause and the left side the keywords for the select-clause. The difficulty with trigger words is to find every possible synonym allowed by natural language. For example, an aggregation could be implied with the word 'by' but also 'of each' (compare Q7).

(p30.1) In the following, we summarize two pattern-based NLIs. We decided to describe NLQ/A (Zheng et al, 2017) in depth, because it is based on the idea that the errors made by NLP technologies are not worth the gain of information. Instead, the system is highly dependent on the users' input to solve ambiguity problems, and therefore it focuses on the optimization of the user interaction.

(p30.2) Pattern-based NLIs are an extension of keyword-based systems with natural language patterns to answer more complex questions like concepts (Q6) or aggregations (Q7). For example, the question 'What was the best movie of each genre? ' (Q7) cannot be formulated with keywords only. It needs at least some linking phrase between 'best movie' and 'genre', which indicates the aggregation. This could be done with the non-keyword token (trigger word) 'by' for the aggregation, which will indicate that the right side includes the keywords for the group by-clause and the left side the keywords for the select-clause. The difficulty with trigger words is to find every possible synonym allowed by natural language. For example, an aggregation could be implied with the word 'by' but also 'of each' (compare Q7).

(p30.3) In the following, we summarize two pattern-based NLIs. We decided to describe NLQ/A (Zheng et al, 2017) in depth, because it is based on the idea that the errors made by NLP technologies are not worth the gain of information. Instead, the system is highly dependent on the users' input to solve ambiguity problems, and therefore it focuses on the optimization of the user interaction.
## (s31) NLQ/A
(p31.0) NLQ/A (Zheng et al, 2017) is an NLI to query a knowledge graph. The system is based on a new approach without NLP technologies like parsers or PoS taggers. The idea being that the errors made by these technologies are not worth the gain of information. For example, a parse tree helps for certain questions like subqueries (e.g., Q9), but if the parse tree is wrong, the system will fail to translate even simpler questions. Instead, NLQ/A lets the users resolve all ambiguity problems, also those which could be solved with PoS tagging or parse trees. To avoid needing too many interaction steps, NLQ/A provides an efficient greedy approach for the interaction process.
## (s32) QuestIO (QUESTion-based Interface to Ontologies)
(p32.0) QuestIO (Damljanovic et al, 2008) is an NLI to query ontologies using unconstrained natural language. It automatically extracts human-understandable lexicalization from the ontology. Therefore, the quality of the semantic information in the ontology has to be very high to contain enough human-understandable labels and/or descriptions. For example, the attribute Movie.Release-Date would be extracted as 'Release Date', which is a human-understandable label. In contrast, the attribute Movie.OriginalLang would result in 'Original Lang', where the token 'Lang' is a shortened version for 'Language' and is not human-understandable.

(p32.1) QuestIO translates the input question with three steps: In the first step, the key concept identification tool identifies all tokens which refer to mentions of ontology resources such as instances, classes, properties or property values. This is similar to the dependent phrases of NLQ/A. In the next step, the context collector identifies patterns (e.g., key phrases like 'how many') in the remaining tokens that help the system to understand the query (similar to independent phrases of NLQ/A). The last step identifies relationships between the ontology resources collected during the previous steps and formulates the corresponding formal query. After executing the query, it will be sent to the result formatter to display the result in an user-friendly manner.

(p32.2) The automatic extraction of semantic information out of the ontology is both a strength and a weakness of QuestIO. It is highly dependent on the development of the human-understandable labels and descriptions, without them QuestIO will not be able to match the input questions to the automatic extracted information.

(p32.3) QuestIO (Damljanovic et al, 2008) is an NLI to query ontologies using unconstrained natural language. It automatically extracts human-understandable lexicalization from the ontology. Therefore, the quality of the semantic information in the ontology has to be very high to contain enough human-understandable labels and/or descriptions. For example, the attribute Movie.Release-Date would be extracted as 'Release Date', which is a human-understandable label. In contrast, the attribute Movie.OriginalLang would result in 'Original Lang', where the token 'Lang' is a shortened version for 'Language' and is not human-understandable.

(p32.4) QuestIO translates the input question with three steps: In the first step, the key concept identification tool identifies all tokens which refer to mentions of ontology resources such as instances, classes, properties or property values. This is similar to the dependent phrases of NLQ/A. In the next step, the context collector identifies patterns (e.g., key phrases like 'how many') in the remaining tokens that help the system to understand the query (similar to independent phrases of NLQ/A). The last step identifies relationships between the ontology resources collected during the previous steps and formulates the corresponding formal query. After executing the query, it will be sent to the result formatter to display the result in an user-friendly manner.

(p32.5) The automatic extraction of semantic information out of the ontology is both a strength and a weakness of QuestIO. It is highly dependent on the development of the human-understandable labels and descriptions, without them QuestIO will not be able to match the input questions to the automatic extracted information.
## (s33) Parsing-based systems
(p33.0) Parsing-based NLIs are going a step further than previously discussed systems: they parse the input question and use the information generated about the structure of the question to understand the grammatical structure. For example, the grammatical structure can be used to identify the dependencies given by the trigger word 'by' in a question. This is needed for long range dependencies which cannot be caught with simple natural language patterns. Furthermore, the dependency parser can help to handle the difficulty of verboseness. For example, the nominal modifier (nmod) could be used to identify aggregations.

(p33.1) In the following, we summarize eight parsing-based NLIs. We decided to describe ATHENA (Saha et al, 2016) in depth, because it can answer the most of the sample input questions. Furthermore, ATHENA uses the most NLP technologies and the authors describe all the steps in depth. Afterwards, the other systems are summarized and we highlight the delta to ATHENA and previous systems.

(p33.2) Parsing-based NLIs are going a step further than previously discussed systems: they parse the input question and use the information generated about the structure of the question to understand the grammatical structure. For example, the grammatical structure can be used to identify the dependencies given by the trigger word 'by' in a question. This is needed for long range dependencies which cannot be caught with simple natural language patterns. Furthermore, the dependency parser can help to handle the difficulty of verboseness. For example, the nominal modifier (nmod) could be used to identify aggregations.

(p33.3) In the following, we summarize eight parsing-based NLIs. We decided to describe ATHENA (Saha et al, 2016) in depth, because it can answer the most of the sample input questions. Furthermore, ATHENA uses the most NLP technologies and the authors describe all the steps in depth. Afterwards, the other systems are summarized and we highlight the delta to ATHENA and previous systems.
## (s34) ATHENA
(p34.0) ATHENA (Saha et al, 2016) is an ontology-driven NLI for relational databases, which handles full sentences in English as the input question. For ATHENA, ontologydriven means that it is based on the information of a given ontology and needs mapping between an ontology and a relational database. A set of synonyms can be associated with each ontology element. During the translation of an input question into a SQL query, ATHENA uses an intermediate query language before subsequently translating it into SQL.
## (s37) BELA
(p37.0) BELA (Walter et al, 2012) is an NLI with a layered approach. This means, that at each layer the best hypothesis is determined. If the confidence for a particular interpretation is 1 and the SPARQL query generated by it produces an answer with at least one result, the translation process is stopped and the answer is returned to the user. Only for ASK-questions (which have yes/no answers), the process continues until the confidence of the interpretations start to differ, then a threshold of 0.9 is applied and an empty result (which equals a no-answer) is also accepted.

(p37.1) Similar to Querix, BELA parses the input question and produces a set of query templates, which mirror the semantic structure. The next step is a lookup in the index, including Wikipedia redirects (this corresponds to the translation index of ATHENA). The first lookup is without fuzzy matching, if no result can be found, a threshold of 0.95 is applied for a matching with normalized Levenshtein distance. The third lookup enables the usage of synonyms, hypernyms and hyponyms from WordNet. The last lookup (and step) uses Explicit Semantic Analysis, which can be used to relate expressions like 'playing' to concepts like 'actor '.
## (s39) NaLIX (Natural Language Interface to XML)
(p39.0) NaLIX (Li et al, 2005) is an interactive NLI for querying an XML database with XQuery. The interaction is based on guiding the users to pose questions that the system can handle by providing meaningful feedback and helpful rephrasing suggestions. Furthermore, NaLIX provides templates and question history to the users. It preserves the users prior search efforts and provides the users with a quick starting point when they create new questions.

(p39.1) Similar to Querix, NaLIX mostly uses the parse tree for the translation of an input question into XQuery. Af-ter MiniPar 6 is used to parse the input question, NaLIX identifies the phrases in the parse tree of the input question that can be mapped to XQuery components and classifies them. In the next step, NaLIX validates the classified parse tree from the previous step: it checks if it knows how to translate the classified parse tree into XQuery and if all attribute names and values can be found in the database. The last step translates the classified parse tree into an appropriate XQuery expression if possible. During both the classification and the validation of the parse tree, information about errors (e.g., unknown phrases and invalid parse trees) is collected to report to the users for clarification.

(p39.2) The strength of NaLIX is the ability to solve difficult questions, which can include subqueries, by using and adjusting the parse tree. On the other hand, the reliance on a parse tree is also a weakness, because the system can only answer questions that are parseable.

(p39.3) NaLIX (Li et al, 2005) is an interactive NLI for querying an XML database with XQuery. The interaction is based on guiding the users to pose questions that the system can handle by providing meaningful feedback and helpful rephrasing suggestions. Furthermore, NaLIX provides templates and question history to the users. It preserves the users prior search efforts and provides the users with a quick starting point when they create new questions.

(p39.4) Similar to Querix, NaLIX mostly uses the parse tree for the translation of an input question into XQuery. Af-ter MiniPar 6 is used to parse the input question, NaLIX identifies the phrases in the parse tree of the input question that can be mapped to XQuery components and classifies them. In the next step, NaLIX validates the classified parse tree from the previous step: it checks if it knows how to translate the classified parse tree into XQuery and if all attribute names and values can be found in the database. The last step translates the classified parse tree into an appropriate XQuery expression if possible. During both the classification and the validation of the parse tree, information about errors (e.g., unknown phrases and invalid parse trees) is collected to report to the users for clarification.

(p39.5) The strength of NaLIX is the ability to solve difficult questions, which can include subqueries, by using and adjusting the parse tree. On the other hand, the reliance on a parse tree is also a weakness, because the system can only answer questions that are parseable.
## (s44) Ginseng (Guided Input Natural language Search ENGine)
(p44.0) Ginseng (Bernstein et al, 2005) is a guided input NLI for ontologies. The system is based on a grammar that describes both the parse rules of the input questions and the query composition elements for the RDF Data Query Language (RDQL) queries. The grammar is used to guide the users in formulating questions in English.

(p44.1) In contrast to TR Discover, Ginseng does not use an intermediate representation and therefore the parsing process translates directly into RDQL. The grammar rules are divided in two categories: dynamic and static grammar rules. The dynamic grammar rules are generated from the OWL ontologies. They include rules for each class, instance, objects property, data type property and synonyms. The static grammar rules consist of about 120 mostly empirically constructed domainindependent rules, which provide the basic sentence structures and phrases for input questions. The naming conventions used by Ginseng differ slightly from these used by TR Discover. Ginseng's dynamic rules correspond to TR Discover's lexical rules and Ginseng's static rules consists of both grammar and lexical rules in TR Discover.

(p44.2) The strength of Ginseng are the dynamic rules which are generated from the ontology. This, together with the domain-independent static rules, lead to an easier adaptability compared to systems like TR Discover. The weakness lies in the grammar rules: they need to cover all possible types of questions the users want to ask.

(p44.3) Ginseng (Bernstein et al, 2005) is a guided input NLI for ontologies. The system is based on a grammar that describes both the parse rules of the input questions and the query composition elements for the RDF Data Query Language (RDQL) queries. The grammar is used to guide the users in formulating questions in English.

(p44.4) In contrast to TR Discover, Ginseng does not use an intermediate representation and therefore the parsing process translates directly into RDQL. The grammar rules are divided in two categories: dynamic and static grammar rules. The dynamic grammar rules are generated from the OWL ontologies. They include rules for each class, instance, objects property, data type property and synonyms. The static grammar rules consist of about 120 mostly empirically constructed domainindependent rules, which provide the basic sentence structures and phrases for input questions. The naming conventions used by Ginseng differ slightly from these used by TR Discover. Ginseng's dynamic rules correspond to TR Discover's lexical rules and Ginseng's static rules consists of both grammar and lexical rules in TR Discover.

(p44.5) The strength of Ginseng are the dynamic rules which are generated from the ontology. This, together with the domain-independent static rules, lead to an easier adaptability compared to systems like TR Discover. The weakness lies in the grammar rules: they need to cover all possible types of questions the users want to ask.
## (s46) MEANS (MEdical question ANSwering)
(p46.0) MEANS (Ben Abacha and Zweigenbaum, 2015) is an NLI that uses a hybrid approach of patterns and ML to identify semantic relations. It is highly domain dependent and focuses on factual questions expressed by wh-pronouns and boolean questions in a medical subfield targeting the seven medical categories: problem, treatment, test, sign/symptom, drug, food and patient.
## (s47) AskNow
(p47.0) AskNow (Dubey et al, 2016) uses a novel query characterization structure that is resilient to paraphrasing, called Normalized Query Structure (NQS), which is less sensitive to structural variation in the input question. The identification of the elements in the NQS is highly dependent on POS tags. For example, the input question Q1 'Who is the director of " To translate the input question into SPARQL, As-kNow first identifies the sub-structures using a POS tagger and named entity recognition. Then it fits the sub-structures into their corresponding cells within the generic NQS templates. Afterwards the query type (set, boolean, ranking, count or property value) is identified based on desire and wh-type. In the next step, the query desire, query input and their relations will be matched to the KB. As an example, Spotlight can be used for the matching to DBpedia. During the matching process, AskNow uses WordNet synonyms and a BOA pattern library (bootstrapping).

(p47.1) The strength of AskNow, compared to the previous grammar-based systems, is that the users are free to formulate their questions without restrictions. In addition, the NQS templates allow complex questions which, for example, can include subqueries. One weakness of As-kNow is that it highly depends on the right PoS tags and restricts the types of questions that can be asked.

(p47.2) AskNow (Dubey et al, 2016) uses a novel query characterization structure that is resilient to paraphrasing, called Normalized Query Structure (NQS), which is less sensitive to structural variation in the input question. The identification of the elements in the NQS is highly dependent on POS tags. For example, the input question Q1 'Who is the director of " To translate the input question into SPARQL, As-kNow first identifies the sub-structures using a POS tagger and named entity recognition. Then it fits the sub-structures into their corresponding cells within the generic NQS templates. Afterwards the query type (set, boolean, ranking, count or property value) is identified based on desire and wh-type. In the next step, the query desire, query input and their relations will be matched to the KB. As an example, Spotlight can be used for the matching to DBpedia. During the matching process, AskNow uses WordNet synonyms and a BOA pattern library (bootstrapping).

(p47.3) The strength of AskNow, compared to the previous grammar-based systems, is that the users are free to formulate their questions without restrictions. In addition, the NQS templates allow complex questions which, for example, can include subqueries. One weakness of As-kNow is that it highly depends on the right PoS tags and restricts the types of questions that can be asked.
## (s48) SPARKLIS
(p48.0) SPARKLIS (Ferré, 2017) is a guided query builder for SPARQL using natural language for better understanding. It guides the users during their query phrasing by giving the possibilities to search through concepts, entities and modifiers in natural language. It relies on the rules of SPARQL to ensure syntactically correct SPARQL queries all the time during the process. The interaction with the system makes the question formulation more constrained, slower and less spontaneous, but it provides guidance and safeness with intermediate answers and suggestions at each step. The translation process for SPARKLIS is reversed: it translates possible SPARQL queries into natural language such that the users can understand their choices.

(p48.1) The auto-completion is different from the previous systems (e.g., TR Discover and Ginseng): the interface displays three lists where the users can search for concepts, entities or modifiers. To ensure completeness relative to user input, SPARKLIS uses a cascade of three stages. The first stage is on client side, where a partial list of suggestion is filtered. The second stage is executed if the filtered list gets empty, then the suggestions is re-computed by sending the query, including the users filter, to the SPARQL endpoint. The last stage is triggered if the list is still empty, then, new queries are again sent to the SPARQL endpoint, using the full SPARQL query instead of partial results. Only a limited number of suggestions are computed and no ranking is applied, because of scalability issues.

(p48.2) The strength of SPARKLIS is also its weakness: the restricted guidance of the users during the query formulation process allows only syntactically correct questions, but at the same time, the users' freedom is reduced. Furthermore, the limited number of suggestions has the negative consequence that they may be incomplete and therefore making some queries unreachable.

(p48.3) SPARKLIS (Ferré, 2017) is a guided query builder for SPARQL using natural language for better understanding. It guides the users during their query phrasing by giving the possibilities to search through concepts, entities and modifiers in natural language. It relies on the rules of SPARQL to ensure syntactically correct SPARQL queries all the time during the process. The interaction with the system makes the question formulation more constrained, slower and less spontaneous, but it provides guidance and safeness with intermediate answers and suggestions at each step. The translation process for SPARKLIS is reversed: it translates possible SPARQL queries into natural language such that the users can understand their choices.

(p48.4) The auto-completion is different from the previous systems (e.g., TR Discover and Ginseng): the interface displays three lists where the users can search for concepts, entities or modifiers. To ensure completeness relative to user input, SPARKLIS uses a cascade of three stages. The first stage is on client side, where a partial list of suggestion is filtered. The second stage is executed if the filtered list gets empty, then the suggestions is re-computed by sending the query, including the users filter, to the SPARQL endpoint. The last stage is triggered if the list is still empty, then, new queries are again sent to the SPARQL endpoint, using the full SPARQL query instead of partial results. Only a limited number of suggestions are computed and no ranking is applied, because of scalability issues.

(p48.5) The strength of SPARKLIS is also its weakness: the restricted guidance of the users during the query formulation process allows only syntactically correct questions, but at the same time, the users' freedom is reduced. Furthermore, the limited number of suggestions has the negative consequence that they may be incomplete and therefore making some queries unreachable.
## (s49) GFMed
(p49.0) GFMed (Marginean, 2017) is an NLI for biomedical linked data. It applies grammars manually built with Grammatical Framework 7 (GF). GF grammars are divided into abstract and concrete grammars. The abstract grammar defines the semantic model of the input language and for GFMed this is based on the biomedical domain. The concrete grammars define the syntax of the input language, which is English and SPARQL. Furthermore, GF supports multilingual applications and because of that Romanian is included as a second natural language in GFMed.

(p49.1) To translate the controlled natural language input into SPARQL, GFMed relies in a first step on the libraries of GF for syntax, morphological paradigms and coordination. GFMed covers basic elements of SPARQL to support term constraints, aggregates and negations. There is no support for property paths of length different from 1, optional graph pattern or assignment. Furthermore, only equality and regular expression operators are included.

(p49.2) The strength of GFMed is that it covers the basic elements of SPARQL. Furthermore, it introduces a second natural language besides of English for the users to ask questions. The weakness are the GF grammars which are domain dependent and restrict the number of questions that can be asked by the users.

(p49.3) GFMed (Marginean, 2017) is an NLI for biomedical linked data. It applies grammars manually built with Grammatical Framework 7 (GF). GF grammars are divided into abstract and concrete grammars. The abstract grammar defines the semantic model of the input language and for GFMed this is based on the biomedical domain. The concrete grammars define the syntax of the input language, which is English and SPARQL. Furthermore, GF supports multilingual applications and because of that Romanian is included as a second natural language in GFMed.

(p49.4) To translate the controlled natural language input into SPARQL, GFMed relies in a first step on the libraries of GF for syntax, morphological paradigms and coordination. GFMed covers basic elements of SPARQL to support term constraints, aggregates and negations. There is no support for property paths of length different from 1, optional graph pattern or assignment. Furthermore, only equality and regular expression operators are included.

(p49.5) The strength of GFMed is that it covers the basic elements of SPARQL. Furthermore, it introduces a second natural language besides of English for the users to ask questions. The weakness are the GF grammars which are domain dependent and restrict the number of questions that can be asked by the users.
## (s51) Evaluation of 24 recently developed NLIs
(p51.0) In this section, we provide a systematic analysis of the major NLIs. We categorized and analyzed the translation process of the 24 recently developed systems highlighted in Section 5 based on ten sample world questions with increasing complexity. Each system category, based on its technical approach, has its own strengths and weaknesses. There are also different aspects on which a system can focus (e.g., number of user interaction, ambiguity, efficiency, etc.), which we do not take into account in this paper.

(p51.1) We evaluate the systems based on what is reported in the papers. If there is either an example of a similar question (e.g. 'Who is the president of the united states?' (Q1) or a clear statement written in the paper (e.g., 'we can identify aggregations' (Q7), we label those questions for the system with a checkmark () in Table  3. If the question needs to be asked in a strict syntactical way (e.g., SODA needs the symbol '¿' instead of 'higher than') or the answer is partially correct (e.g., Q4 returns a ordered list of movies instead of only one), it is labeled with a triangle (L). If there is a clear statement that something is not implemented (e.g. ATHENA does not support negations), we label it with . If we were not able to conclude, if a system can or cannot answer a question based on the paper, we labeled it with a question mark in Table 3. In general, as shown in Table 3, we can say that keyword-based NLIs are the least powerful and can only answer simple questions like string filters (Q1). This limitation is based on the approach of these keyword-based systems: they expect just keywords (which are mostly filters) and the systems identify relationships between them. Therefore, they do not expect any complexer questions like Q4 or Q7. Pattern-based NLIs are an extension of keyword-based systems in such a way that they have a dictionary with trigger words to answer more complex questions like aggregations (Q7). However, they cannot answer questions of higher difficulties, including subqueries (Q9/Q10). For example, the difficulty with questions including subqueries is to identify which part of the input question belongs to which subquery. Trigger words are not sufficient to identify the range of each subquery.

(p51.2) Parsing-based NLIs are able to answer these questions by using dependency or constituency trees (e.g., NaLIR). This helps to identify and group the input question in such a way that the different parts of the subqueries can be identified. Still, some of those systems struggle with the same problem as pattern-based NLIs: the systems are using trigger word dictionaries to identify certain types of questions like aggregations (e.g., ATHENA), which is not a general solution of the problem.

(p51.3) Grammar-based systems offer the possibility to guide the users during the formulation of their questions. Dynamically applying the rules during typing allows the systems to ensure that the input question is always translatable into formal language. The huge disadvantage of grammar-based systems is that they need handcrafted rules. There are NLIs which use a set of general rules and domain-specific rules (e.g., SQUALL). The general rules can be used for other domains and therefore increase the adaptability of the system. Other systems try to extract the rules directly from the ontology and thereby reduce the number of handcrafted rules (e.g., Ginseng).

(p51.4) We will now analyze how well the different systems can handle the ten sample questions. The first question is a basic filter question and can be solved by all NLIs as shown in Table 3. The last three questions are the most difficult ones and can only be answered by a few systems (completely by SQUALL, SPARKLIS and partially by others). Complex questions (e.g., aggregations) cannot be phrased with keywords only. Therefore, the more complicated the users questions are, the more they will phrase them in grammatically correct sentences. In contrast, simple questions (e.g., string filters like Q1) can be easily asked with keywords. Both, Waltinger et al (2013) (USI Answer) and Lawrence and Riezler (2016) (NLmaps) are describing this phenomenon and that the users prefer to ask questions with keywords if possible. They adapted their systems so that they can handle different forms of user input. Because of that, Waltinger et al (2013) (USI Answer) point out that parse trees should only be used with caution. This is similar to the approach of Zheng et al (2017) (NLQ/A), who remark that NLP technologies are not worth the risk, because wrong interpretations in the processing leads to errors. Walter et al (2012) (BELA) propose a new approach of applying certain processing steps only if the question cannot be answered by using simpler mechanisms. This approach can be used to answer questions formulated as keywords or as complete sentences. Nevertheless, parse trees are useful to identify subqueries, but only in grammatically correct sentences (e.g., NaLIR). The identification of possible subqueries is necessary to answer questions like Q9 and Q10.
## (s75) J, 2xS
(p75.0) for the evaluation of SODA (Blunschi et al (2012)) and the 10 sample questions of this paper. Therefore, we designed nine questions based on the operators of SQL and SPARQL: Joins, Filters (string, range, date or negation), Aggregations, Ordering, Union and Subqueries.

(p75.1) Furthermore, we added a question which is based on a concept (e.g., 'great movie'). Concepts are not part of SQL and SPARQL, but a common addition of NLIs. Table 1 shows those ten full sentence questions in English, which can be applied on the sample world (ordered roughly by difficulty).

(p75.2) The queries were designed in such a way that they cover a wide range of SQL-functionality (technical challenge) as well as linguistic variations (semantic challenge). We will now analyze each of these ten queries in more detail and describe the major challenges for the underlying system to solve them.

(p75.3) The first question (Q1) is a join over different tables (Person, Director, Directing and Movie) with an ISA-relationship between the tables Person and Director. Moreover, the query has a filter on the attribute Movie.Title, which has to be equal to 'Inglourious Basterds'. Therefore, the system faces three different challenges: (a) identify the bridge table Directing to link the tables Director and Movie, (b) identify the hierarchical structure (ISA-relationship) between Director and Person and (c) identify 'Inglourious Basterds' as a filter phrase for Movie.Title.

(p75.4) The second question (Q2) is based on a single table (Movie) with a range filter. The challenge for the NLIs is to translate 'higher than' into a comparison operator 'greater than'.

(p75.5) The third question (Q3) is a join over four tables (Person, Actor, Starring and Movie) and includes two filters: (a) a filter on the attribute Person.FirstName and Person.LastName and (b) a two-sided date range filter on the attribute Movie.ReleaseDate. The challenge in this query (compared to the previous ones) is the date range filter. The system needs to detect that 'from 2000 until 2010 ' refers to a range filter and that the numbers need to be translated into the dates 2000-01-01 and 2010-12-31.

(p75.6) The fourth question (Q4) is a join over two tables (Movie and Gross). In addition, an aggregation on the attribute Gross.Gross and grouping on the attribute Movie.id or ordering the result based on Gross.Gross is needed. For both approaches an aggregation to a single result (indicated by the keyword 'most') is requested.
## (s76) Question Analysis
(p76.0) In this section, we perform an analysis comparing our ten sample input questions with the two well-known question-answering corpora Yahoo! QA Corpus L6 2 (more than 4 million questions within a user community) and GeoData250 (Tang and Mooney, 2001) (250 questions against a database). We also summarize the findings of Bonifati et al (2017) and compare them to our input questions. The goal of the analysis is to better understand what types of questions users pose and how representative our sample input questions are (i.e., establish some external validity of their representativeness).

(p76.1) For the Yahoo! Corpus, we decided to only look into the labeled subset of movie questions since our sample world is based on movies. Out of these questions related to movies we extracted a random sample set of 100 questions. We used all the GeoData250 Questions. We labeled each of those 350 questions from both sources with Q1 to Q10 based on what poses the challenge in answering them.

(p76.2) For example, the GeoData250 question 'give me the cities in virginia? ' is labeled with Q1, because the challenge of this question is to identify the right filter ('virginia'). The question 'what is a good movie to go see this weekend? ' includes a time range and is therefore labeled as Q6. For the Yahoo! Corpus we also made some assumptions, for example, the question 'what is your favorite tom hanks movie? ' is interpreted as 'give me the best ranked tom hanks movie' and labeled with Q4. Furthermore, if a question could have multiple labels, the label of the more difficult (higher number) question is chosen. For example, the sample question 'can anyone tell a good action movie to watch? ' is labeled with Q6 because it requires handling of a concept ('good movie') and not Q1 because it uses a filter ('action movie'). If the question cannot be labeled with one of the input questions, we label it with x 3 . For example, Fig. 3 Mapping of 100 sample questions of Yahoo! QA Corpus L6 (movie) to our ten sample world questions. the question 'i want to make a girl mine but she is more beautiful than me. what can i do now? ' has nothing to do with movies.

(p76.3) As we can see in Figure 3, more than 40% of the Yahoo! questions are based on filtering only, i.e. corresponding to question Q1. For example, the question 'what movie had "wonderful world" by sam cooke at the beginning? ' has filters for the song 'wonderful world ' and a join on movie. About 30% of the questions are labeled with x which are off-topic questions. There are no questions labeled with Q2, this means that there are no questions with a numerical range. This can be explained by the composition of the corpus itself, which is a collection of questions from users to users. If users ask about the ranking of a movie, they ask something like 'what is your favorite movie? ' and not something similar to Q2.

(p76.4) In Figure 4, we can see the distribution of question types for the GeoData250 corpus. About 88% of the questions are labeled with Q1 or Q4. There are three concepts ('population density', 'major city' and 'major river ') used in the corpus that occur in roughly 8% of the questions, 7% of which are labeled with Q6. There are no numerical range questions (Q2) and no date questions (Q3). The latter can be explained by the dataset not including any dates. There are also no unions (Q5) and no questions with multiple subqueries (Q10). Bonifati et al (2017) investigated a large corpus of query logs from different SPARQL endpoints. The query log files are from seven different data sources from various domains. One part of the analysis was done by counting the SPARQL keywords used in the queries. Over all domains, 88% of the queries are Select-queries and 40% have Filter. Furthermore, they found huge differences between different domains. For example, the use of Filter ranges from 61% (LinkedGeoData) to 3% (OpenBioMed) or less. This implies that the distribution of the usage for the question types are domaindependent. Nevertheless, our ten sample question are fully covered in the query log analyzed by Bonifati et al (2017).

(p76.5) Our analysis indicates that our ten sample questions represent a large range of questions typically queried in question answering systems. Note that open-ended questions -the ones we labeled as x in the Yahoo! Corpus -are not covered by our analysis.

(p76.6) In this section, we perform an analysis comparing our ten sample input questions with the two well-known question-answering corpora Yahoo! QA Corpus L6 2 (more than 4 million questions within a user community) and GeoData250 (Tang and Mooney, 2001) (250 questions against a database). We also summarize the findings of Bonifati et al (2017) and compare them to our input questions. The goal of the analysis is to better understand what types of questions users pose and how representative our sample input questions are (i.e., establish some external validity of their representativeness).

(p76.7) For the Yahoo! Corpus, we decided to only look into the labeled subset of movie questions since our sample world is based on movies. Out of these questions related to movies we extracted a random sample set of 100 questions. We used all the GeoData250 Questions. We labeled each of those 350 questions from both sources with Q1 to Q10 based on what poses the challenge in answering them.

(p76.8) For example, the GeoData250 question 'give me the cities in virginia? ' is labeled with Q1, because the challenge of this question is to identify the right filter ('virginia'). The question 'what is a good movie to go see this weekend? ' includes a time range and is therefore labeled as Q6. For the Yahoo! Corpus we also made some assumptions, for example, the question 'what is your favorite tom hanks movie? ' is interpreted as 'give me the best ranked tom hanks movie' and labeled with Q4. Furthermore, if a question could have multiple labels, the label of the more difficult (higher number) question is chosen. For example, the sample question 'can anyone tell a good action movie to watch? ' is labeled with Q6 because it requires handling of a concept ('good movie') and not Q1 because it uses a filter ('action movie'). If the question cannot be labeled with one of the input questions, we label it with x 3 . For example, Fig. 3 Mapping of 100 sample questions of Yahoo! QA Corpus L6 (movie) to our ten sample world questions. the question 'i want to make a girl mine but she is more beautiful than me. what can i do now? ' has nothing to do with movies.

(p76.9) As we can see in Figure 3, more than 40% of the Yahoo! questions are based on filtering only, i.e. corresponding to question Q1. For example, the question 'what movie had "wonderful world" by sam cooke at the beginning? ' has filters for the song 'wonderful world ' and a join on movie. About 30% of the questions are labeled with x which are off-topic questions. There are no questions labeled with Q2, this means that there are no questions with a numerical range. This can be explained by the composition of the corpus itself, which is a collection of questions from users to users. If users ask about the ranking of a movie, they ask something like 'what is your favorite movie? ' and not something similar to Q2.

(p76.10) In Figure 4, we can see the distribution of question types for the GeoData250 corpus. About 88% of the questions are labeled with Q1 or Q4. There are three concepts ('population density', 'major city' and 'major river ') used in the corpus that occur in roughly 8% of the questions, 7% of which are labeled with Q6. There are no numerical range questions (Q2) and no date questions (Q3). The latter can be explained by the dataset not including any dates. There are also no unions (Q5) and no questions with multiple subqueries (Q10). Bonifati et al (2017) investigated a large corpus of query logs from different SPARQL endpoints. The query log files are from seven different data sources from various domains. One part of the analysis was done by counting the SPARQL keywords used in the queries. Over all domains, 88% of the queries are Select-queries and 40% have Filter. Furthermore, they found huge differences between different domains. For example, the use of Filter ranges from 61% (LinkedGeoData) to 3% (OpenBioMed) or less. This implies that the distribution of the usage for the question types are domaindependent. Nevertheless, our ten sample question are fully covered in the query log analyzed by Bonifati et al (2017).

(p76.11) Our analysis indicates that our ten sample questions represent a large range of questions typically queried in question answering systems. Note that open-ended questions -the ones we labeled as x in the Yahoo! Corpus -are not covered by our analysis.
## (s79) Synonymy
(p79.0) The difficulty of synonymy is that a simple lookup or matching is not enough. For example, the question 'All movies starring Brad Pitt from 2000 until 2010.' (Q3) could also be phrased as 'All movies playing Brad Pitt from 2000 until 2010.'. The answer should be the same, but because in the sample world no element is named 'playing', a lookup would not find an answer. Therefore, it is necessary that the system takes synonyms into account. A possible solution is the use of a translation dictionary. Usually such a dictionary is based on DBpedia (Lehmann et al, 2012) and/or WordNet (Miller, 1995).

(p79.1) The difficulty of synonymy is that a simple lookup or matching is not enough. For example, the question 'All movies starring Brad Pitt from 2000 until 2010.' (Q3) could also be phrased as 'All movies playing Brad Pitt from 2000 until 2010.'. The answer should be the same, but because in the sample world no element is named 'playing', a lookup would not find an answer. Therefore, it is necessary that the system takes synonyms into account. A possible solution is the use of a translation dictionary. Usually such a dictionary is based on DBpedia (Lehmann et al, 2012) and/or WordNet (Miller, 1995).
## (s84) Limitations
(p84.0) This survey's evaluation focuses on the ten sample questions introduced in Section 2.2. Those sample questions are based on the operators of the formal query languages SQL and SPARQL. This leads to the following limitations of the evaluation.

(p84.1) Limitation 1 -Theoretical: Our evaluation is theoretical and only based on the papers. A few systems have online demos (e.g., SPARKLIS (Ferré, 2017)), but others -especially older ones -are not available any more. Therefore, to handle all systems equally, we based our evaluation fully on the papers of the systems.
## (s93) Précis
(p93.0) Précis (Simitsis et al, 2008) is a keyword-based NLI for relational databases, which supports multiple terms combined through the operators AND, OR and NOT. For example, input question 'Show me all drama and comedy movies.' (Q5) would be formulated as '"drama" OR "comedy"'. The answer is an entire multi-relation database, which is a logical subset of the original database.

(p93.1) First, Précis transforms the input question into disjunct normal form (DNF). Afterwards, each term in the DNF is looked up in the inverted index of the base data. This is different to SODA, where the inverted index includes the meta data. If a term cannot be found, the next steps are not executed. The third step creates the schema of the logical database subset, which represents the answer of the input question. This includes the identification of the necessary join paths.

(p93.2) The strength of Précis is the ability to use brackets, AND, OR and NOT to define the input question. However, the weaknesses are that this again composes a logical query language, although a simpler one. Furthermore, it can only solve boolean questions, and the input question can only consist of terms which are located in the base data and not in the meta data. For example, the input question 'Who is the director of "Inglourious Basterds"? ' (Q1) cannot directly be solved because 'director ' is the name of a table and therefore part of the meta data. There is a mechanism included that adds more information to the answer (e.g., the actors, directors etc. to a movie), but then the user would have to search for the director in the answer.

(p93.3) Précis (Simitsis et al, 2008) is a keyword-based NLI for relational databases, which supports multiple terms combined through the operators AND, OR and NOT. For example, input question 'Show me all drama and comedy movies.' (Q5) would be formulated as '"drama" OR "comedy"'. The answer is an entire multi-relation database, which is a logical subset of the original database.

(p93.4) First, Précis transforms the input question into disjunct normal form (DNF). Afterwards, each term in the DNF is looked up in the inverted index of the base data. This is different to SODA, where the inverted index includes the meta data. If a term cannot be found, the next steps are not executed. The third step creates the schema of the logical database subset, which represents the answer of the input question. This includes the identification of the necessary join paths.

(p93.5) The strength of Précis is the ability to use brackets, AND, OR and NOT to define the input question. However, the weaknesses are that this again composes a logical query language, although a simpler one. Furthermore, it can only solve boolean questions, and the input question can only consist of terms which are located in the base data and not in the meta data. For example, the input question 'Who is the director of "Inglourious Basterds"? ' (Q1) cannot directly be solved because 'director ' is the name of a table and therefore part of the meta data. There is a mechanism included that adds more information to the answer (e.g., the actors, directors etc. to a movie), but then the user would have to search for the director in the answer.
## (s94) QUICK (QUery Intent Constructor for Keywords)
(p94.0) QUICK (Zenz et al, 2009) is an NLI that adds the expressiveness of semantic queries to the convenience of keyword-based search. To achieve this, the users start with a keyword question and then are guided through the process of incremental refinement steps to select the question's intention. The system provides the user with an interface that shows the semantic queries as graphs as well as textual form.

(p94.1) In a first step, QUICK takes the keywords of the input question and compares them against the KB. Each possible interpretation corresponds to a semantic query. For example, the input question 'Brad Pitt' can either mean 'movies where Brad Pitt played in', 'movies directed by Brad Pitt' or 'movies written by Brad Pitt' (see Figure 1). In the next step, the system provides the users with the information in such a way that they can select the semantic query which will answer their question. To do so, QUICK provides the users with possible interpretations of each keyword to select from. This is done with a graph as well as a textual form of the semantic query. The textual form is a translation of the SQL query into natural language based on templates. Furthermore, the system orders the keywords in such a way that the user interactions are as few as possible. When the users select the desired semantic query, QUICK executes it and displays the results in the user interface.

(p94.2) The strength of QUICK is the user interaction interface with the optimization for minimal user interaction during the semantic query selection. The weakness of QUICK is that it is limited to acyclic conjunctions of triple patterns.

(p94.3) QUICK (Zenz et al, 2009) is an NLI that adds the expressiveness of semantic queries to the convenience of keyword-based search. To achieve this, the users start with a keyword question and then are guided through the process of incremental refinement steps to select the question's intention. The system provides the user with an interface that shows the semantic queries as graphs as well as textual form.

(p94.4) In a first step, QUICK takes the keywords of the input question and compares them against the KB. Each possible interpretation corresponds to a semantic query. For example, the input question 'Brad Pitt' can either mean 'movies where Brad Pitt played in', 'movies directed by Brad Pitt' or 'movies written by Brad Pitt' (see Figure 1). In the next step, the system provides the users with the information in such a way that they can select the semantic query which will answer their question. To do so, QUICK provides the users with possible interpretations of each keyword to select from. This is done with a graph as well as a textual form of the semantic query. The textual form is a translation of the SQL query into natural language based on templates. Furthermore, the system orders the keywords in such a way that the user interactions are as few as possible. When the users select the desired semantic query, QUICK executes it and displays the results in the user interface.

(p94.5) The strength of QUICK is the user interaction interface with the optimization for minimal user interaction during the semantic query selection. The weakness of QUICK is that it is limited to acyclic conjunctions of triple patterns.
## (s96) SINA
(p96.0) SINA (Shekarpour et al, 2015) is a keyword-based NLI that transforms natural language input questions into conjunctive SPARQL queries. It uses a Hidden Markov Model to determine the most suitable resource for a given input question from different datasets.

(p96.1) In the first step, SINA reduces the input question to keywords (similar to NLP-Reduce), by using tokenization, lemmatization and stop word removal. In the next step, the keywords are grouped into segments, with respect to the available resources. For example, the keywords 'Inglourious' and 'Basterds' would be grouped into one segment based on the match for 'Inglorious Basterds'. In the third step, the relevant resources are retrieved based on string matching between the segments and the RDF label of the resource. In the following step, the best subset of resources for the given input question is determined (ranking). The fifth step, SINA constructs the SPARQL query using the graphstructure of the database. Finally, the results, retrieved by evaluating the generated SPARQL query, are shown to the users.
## (s97) Aqqu
(p97.0) Aqqu (Bast and Haussmann, 2015) is an NLI which uses templates to identify possible relations between keywords. At the end of the translation process, ML is used to rank the possible solutions.

(p97.1) To translate the input question into SPARQL, first the entities from the KB that match (possibly overlapping) parts of the input question are identified. The possible parts are identified by using PoS tags. For example, single token parts must be a noun (NN) and proper nouns (NNP) are not allowed to be split (e.g., 'Brad Pitt'). In the next step, Aqqu uses three different templates which define the general relationship between the keywords. Afterwards, Aqqu tries to identify the corresponding relationship. This can either be done with the help of the input question (verbs and adjectives), or with the help of ML which for example can identify abstract relationship like 'born → birth date'. The last step is the ranking which is solved with ML. The best result is achieved by using a binary random forest classifier.
## (s98) Pattern-based systems
(p98.0) Pattern-based NLIs are an extension of keyword-based systems with natural language patterns to answer more complex questions like concepts (Q6) or aggregations (Q7). For example, the question 'What was the best movie of each genre? ' (Q7) cannot be formulated with keywords only. It needs at least some linking phrase between 'best movie' and 'genre', which indicates the aggregation. This could be done with the non-keyword token (trigger word) 'by' for the aggregation, which will indicate that the right side includes the keywords for the group by-clause and the left side the keywords for the select-clause. The difficulty with trigger words is to find every possible synonym allowed by natural language. For example, an aggregation could be implied with the word 'by' but also 'of each' (compare Q7).

(p98.1) In the following, we summarize two pattern-based NLIs. We decided to describe NLQ/A (Zheng et al, 2017) in depth, because it is based on the idea that the errors made by NLP technologies are not worth the gain of information. Instead, the system is highly dependent on the users' input to solve ambiguity problems, and therefore it focuses on the optimization of the user interaction.

(p98.2) Pattern-based NLIs are an extension of keyword-based systems with natural language patterns to answer more complex questions like concepts (Q6) or aggregations (Q7). For example, the question 'What was the best movie of each genre? ' (Q7) cannot be formulated with keywords only. It needs at least some linking phrase between 'best movie' and 'genre', which indicates the aggregation. This could be done with the non-keyword token (trigger word) 'by' for the aggregation, which will indicate that the right side includes the keywords for the group by-clause and the left side the keywords for the select-clause. The difficulty with trigger words is to find every possible synonym allowed by natural language. For example, an aggregation could be implied with the word 'by' but also 'of each' (compare Q7).

(p98.3) In the following, we summarize two pattern-based NLIs. We decided to describe NLQ/A (Zheng et al, 2017) in depth, because it is based on the idea that the errors made by NLP technologies are not worth the gain of information. Instead, the system is highly dependent on the users' input to solve ambiguity problems, and therefore it focuses on the optimization of the user interaction.
## (s99) NLQ/A
(p99.0) NLQ/A (Zheng et al, 2017) is an NLI to query a knowledge graph. The system is based on a new approach without NLP technologies like parsers or PoS taggers. The idea being that the errors made by these technologies are not worth the gain of information. For example, a parse tree helps for certain questions like subqueries (e.g., Q9), but if the parse tree is wrong, the system will fail to translate even simpler questions. Instead, NLQ/A lets the users resolve all ambiguity problems, also those which could be solved with PoS tagging or parse trees. To avoid needing too many interaction steps, NLQ/A provides an efficient greedy approach for the interaction process.
## (s100) QuestIO (QUESTion-based Interface to Ontologies)
(p100.0) QuestIO (Damljanovic et al, 2008) is an NLI to query ontologies using unconstrained natural language. It automatically extracts human-understandable lexicalization from the ontology. Therefore, the quality of the semantic information in the ontology has to be very high to contain enough human-understandable labels and/or descriptions. For example, the attribute Movie.Release-Date would be extracted as 'Release Date', which is a human-understandable label. In contrast, the attribute Movie.OriginalLang would result in 'Original Lang', where the token 'Lang' is a shortened version for 'Language' and is not human-understandable.

(p100.1) QuestIO translates the input question with three steps: In the first step, the key concept identification tool identifies all tokens which refer to mentions of ontology resources such as instances, classes, properties or property values. This is similar to the dependent phrases of NLQ/A. In the next step, the context collector identifies patterns (e.g., key phrases like 'how many') in the remaining tokens that help the system to understand the query (similar to independent phrases of NLQ/A). The last step identifies relationships between the ontology resources collected during the previous steps and formulates the corresponding formal query. After executing the query, it will be sent to the result formatter to display the result in an user-friendly manner.

(p100.2) The automatic extraction of semantic information out of the ontology is both a strength and a weakness of QuestIO. It is highly dependent on the development of the human-understandable labels and descriptions, without them QuestIO will not be able to match the input questions to the automatic extracted information.

(p100.3) QuestIO (Damljanovic et al, 2008) is an NLI to query ontologies using unconstrained natural language. It automatically extracts human-understandable lexicalization from the ontology. Therefore, the quality of the semantic information in the ontology has to be very high to contain enough human-understandable labels and/or descriptions. For example, the attribute Movie.Release-Date would be extracted as 'Release Date', which is a human-understandable label. In contrast, the attribute Movie.OriginalLang would result in 'Original Lang', where the token 'Lang' is a shortened version for 'Language' and is not human-understandable.

(p100.4) QuestIO translates the input question with three steps: In the first step, the key concept identification tool identifies all tokens which refer to mentions of ontology resources such as instances, classes, properties or property values. This is similar to the dependent phrases of NLQ/A. In the next step, the context collector identifies patterns (e.g., key phrases like 'how many') in the remaining tokens that help the system to understand the query (similar to independent phrases of NLQ/A). The last step identifies relationships between the ontology resources collected during the previous steps and formulates the corresponding formal query. After executing the query, it will be sent to the result formatter to display the result in an user-friendly manner.

(p100.5) The automatic extraction of semantic information out of the ontology is both a strength and a weakness of QuestIO. It is highly dependent on the development of the human-understandable labels and descriptions, without them QuestIO will not be able to match the input questions to the automatic extracted information.
## (s101) Parsing-based systems
(p101.0) Parsing-based NLIs are going a step further than previously discussed systems: they parse the input question and use the information generated about the structure of the question to understand the grammatical structure. For example, the grammatical structure can be used to identify the dependencies given by the trigger word 'by' in a question. This is needed for long range dependencies which cannot be caught with simple natural language patterns. Furthermore, the dependency parser can help to handle the difficulty of verboseness. For example, the nominal modifier (nmod) could be used to identify aggregations.

(p101.1) In the following, we summarize eight parsing-based NLIs. We decided to describe ATHENA (Saha et al, 2016) in depth, because it can answer the most of the sample input questions. Furthermore, ATHENA uses the most NLP technologies and the authors describe all the steps in depth. Afterwards, the other systems are summarized and we highlight the delta to ATHENA and previous systems.

(p101.2) Parsing-based NLIs are going a step further than previously discussed systems: they parse the input question and use the information generated about the structure of the question to understand the grammatical structure. For example, the grammatical structure can be used to identify the dependencies given by the trigger word 'by' in a question. This is needed for long range dependencies which cannot be caught with simple natural language patterns. Furthermore, the dependency parser can help to handle the difficulty of verboseness. For example, the nominal modifier (nmod) could be used to identify aggregations.

(p101.3) In the following, we summarize eight parsing-based NLIs. We decided to describe ATHENA (Saha et al, 2016) in depth, because it can answer the most of the sample input questions. Furthermore, ATHENA uses the most NLP technologies and the authors describe all the steps in depth. Afterwards, the other systems are summarized and we highlight the delta to ATHENA and previous systems.
## (s102) ATHENA
(p102.0) ATHENA (Saha et al, 2016) is an ontology-driven NLI for relational databases, which handles full sentences in English as the input question. For ATHENA, ontologydriven means that it is based on the information of a given ontology and needs mapping between an ontology and a relational database. A set of synonyms can be associated with each ontology element. During the translation of an input question into a SQL query, ATHENA uses an intermediate query language before subsequently translating it into SQL.
## (s105) BELA
(p105.0) BELA (Walter et al, 2012) is an NLI with a layered approach. This means, that at each layer the best hypothesis is determined. If the confidence for a particular interpretation is 1 and the SPARQL query generated by it produces an answer with at least one result, the translation process is stopped and the answer is returned to the user. Only for ASK-questions (which have yes/no answers), the process continues until the confidence of the interpretations start to differ, then a threshold of 0.9 is applied and an empty result (which equals a no-answer) is also accepted.

(p105.1) Similar to Querix, BELA parses the input question and produces a set of query templates, which mirror the semantic structure. The next step is a lookup in the index, including Wikipedia redirects (this corresponds to the translation index of ATHENA). The first lookup is without fuzzy matching, if no result can be found, a threshold of 0.95 is applied for a matching with normalized Levenshtein distance. The third lookup enables the usage of synonyms, hypernyms and hyponyms from WordNet. The last lookup (and step) uses Explicit Semantic Analysis, which can be used to relate expressions like 'playing' to concepts like 'actor '.
## (s107) NaLIX (Natural Language Interface to XML)
(p107.0) NaLIX (Li et al, 2005) is an interactive NLI for querying an XML database with XQuery. The interaction is based on guiding the users to pose questions that the system can handle by providing meaningful feedback and helpful rephrasing suggestions. Furthermore, NaLIX provides templates and question history to the users. It preserves the users prior search efforts and provides the users with a quick starting point when they create new questions.

(p107.1) Similar to Querix, NaLIX mostly uses the parse tree for the translation of an input question into XQuery. Af-ter MiniPar 6 is used to parse the input question, NaLIX identifies the phrases in the parse tree of the input question that can be mapped to XQuery components and classifies them. In the next step, NaLIX validates the classified parse tree from the previous step: it checks if it knows how to translate the classified parse tree into XQuery and if all attribute names and values can be found in the database. The last step translates the classified parse tree into an appropriate XQuery expression if possible. During both the classification and the validation of the parse tree, information about errors (e.g., unknown phrases and invalid parse trees) is collected to report to the users for clarification.

(p107.2) The strength of NaLIX is the ability to solve difficult questions, which can include subqueries, by using and adjusting the parse tree. On the other hand, the reliance on a parse tree is also a weakness, because the system can only answer questions that are parseable.

(p107.3) NaLIX (Li et al, 2005) is an interactive NLI for querying an XML database with XQuery. The interaction is based on guiding the users to pose questions that the system can handle by providing meaningful feedback and helpful rephrasing suggestions. Furthermore, NaLIX provides templates and question history to the users. It preserves the users prior search efforts and provides the users with a quick starting point when they create new questions.

(p107.4) Similar to Querix, NaLIX mostly uses the parse tree for the translation of an input question into XQuery. Af-ter MiniPar 6 is used to parse the input question, NaLIX identifies the phrases in the parse tree of the input question that can be mapped to XQuery components and classifies them. In the next step, NaLIX validates the classified parse tree from the previous step: it checks if it knows how to translate the classified parse tree into XQuery and if all attribute names and values can be found in the database. The last step translates the classified parse tree into an appropriate XQuery expression if possible. During both the classification and the validation of the parse tree, information about errors (e.g., unknown phrases and invalid parse trees) is collected to report to the users for clarification.

(p107.5) The strength of NaLIX is the ability to solve difficult questions, which can include subqueries, by using and adjusting the parse tree. On the other hand, the reliance on a parse tree is also a weakness, because the system can only answer questions that are parseable.
## (s112) Ginseng (Guided Input Natural language Search ENGine)
(p112.0) Ginseng (Bernstein et al, 2005) is a guided input NLI for ontologies. The system is based on a grammar that describes both the parse rules of the input questions and the query composition elements for the RDF Data Query Language (RDQL) queries. The grammar is used to guide the users in formulating questions in English.

(p112.1) In contrast to TR Discover, Ginseng does not use an intermediate representation and therefore the parsing process translates directly into RDQL. The grammar rules are divided in two categories: dynamic and static grammar rules. The dynamic grammar rules are generated from the OWL ontologies. They include rules for each class, instance, objects property, data type property and synonyms. The static grammar rules consist of about 120 mostly empirically constructed domainindependent rules, which provide the basic sentence structures and phrases for input questions. The naming conventions used by Ginseng differ slightly from these used by TR Discover. Ginseng's dynamic rules correspond to TR Discover's lexical rules and Ginseng's static rules consists of both grammar and lexical rules in TR Discover.

(p112.2) The strength of Ginseng are the dynamic rules which are generated from the ontology. This, together with the domain-independent static rules, lead to an easier adaptability compared to systems like TR Discover. The weakness lies in the grammar rules: they need to cover all possible types of questions the users want to ask.

(p112.3) Ginseng (Bernstein et al, 2005) is a guided input NLI for ontologies. The system is based on a grammar that describes both the parse rules of the input questions and the query composition elements for the RDF Data Query Language (RDQL) queries. The grammar is used to guide the users in formulating questions in English.

(p112.4) In contrast to TR Discover, Ginseng does not use an intermediate representation and therefore the parsing process translates directly into RDQL. The grammar rules are divided in two categories: dynamic and static grammar rules. The dynamic grammar rules are generated from the OWL ontologies. They include rules for each class, instance, objects property, data type property and synonyms. The static grammar rules consist of about 120 mostly empirically constructed domainindependent rules, which provide the basic sentence structures and phrases for input questions. The naming conventions used by Ginseng differ slightly from these used by TR Discover. Ginseng's dynamic rules correspond to TR Discover's lexical rules and Ginseng's static rules consists of both grammar and lexical rules in TR Discover.

(p112.5) The strength of Ginseng are the dynamic rules which are generated from the ontology. This, together with the domain-independent static rules, lead to an easier adaptability compared to systems like TR Discover. The weakness lies in the grammar rules: they need to cover all possible types of questions the users want to ask.
## (s114) MEANS (MEdical question ANSwering)
(p114.0) MEANS (Ben Abacha and Zweigenbaum, 2015) is an NLI that uses a hybrid approach of patterns and ML to identify semantic relations. It is highly domain dependent and focuses on factual questions expressed by wh-pronouns and boolean questions in a medical subfield targeting the seven medical categories: problem, treatment, test, sign/symptom, drug, food and patient.
## (s115) AskNow
(p115.0) AskNow (Dubey et al, 2016) uses a novel query characterization structure that is resilient to paraphrasing, called Normalized Query Structure (NQS), which is less sensitive to structural variation in the input question. The identification of the elements in the NQS is highly dependent on POS tags. For example, the input question Q1 'Who is the director of " To translate the input question into SPARQL, As-kNow first identifies the sub-structures using a POS tagger and named entity recognition. Then it fits the sub-structures into their corresponding cells within the generic NQS templates. Afterwards the query type (set, boolean, ranking, count or property value) is identified based on desire and wh-type. In the next step, the query desire, query input and their relations will be matched to the KB. As an example, Spotlight can be used for the matching to DBpedia. During the matching process, AskNow uses WordNet synonyms and a BOA pattern library (bootstrapping).

(p115.1) The strength of AskNow, compared to the previous grammar-based systems, is that the users are free to formulate their questions without restrictions. In addition, the NQS templates allow complex questions which, for example, can include subqueries. One weakness of As-kNow is that it highly depends on the right PoS tags and restricts the types of questions that can be asked.

(p115.2) AskNow (Dubey et al, 2016) uses a novel query characterization structure that is resilient to paraphrasing, called Normalized Query Structure (NQS), which is less sensitive to structural variation in the input question. The identification of the elements in the NQS is highly dependent on POS tags. For example, the input question Q1 'Who is the director of " To translate the input question into SPARQL, As-kNow first identifies the sub-structures using a POS tagger and named entity recognition. Then it fits the sub-structures into their corresponding cells within the generic NQS templates. Afterwards the query type (set, boolean, ranking, count or property value) is identified based on desire and wh-type. In the next step, the query desire, query input and their relations will be matched to the KB. As an example, Spotlight can be used for the matching to DBpedia. During the matching process, AskNow uses WordNet synonyms and a BOA pattern library (bootstrapping).

(p115.3) The strength of AskNow, compared to the previous grammar-based systems, is that the users are free to formulate their questions without restrictions. In addition, the NQS templates allow complex questions which, for example, can include subqueries. One weakness of As-kNow is that it highly depends on the right PoS tags and restricts the types of questions that can be asked.
## (s116) SPARKLIS
(p116.0) SPARKLIS (Ferré, 2017) is a guided query builder for SPARQL using natural language for better understanding. It guides the users during their query phrasing by giving the possibilities to search through concepts, entities and modifiers in natural language. It relies on the rules of SPARQL to ensure syntactically correct SPARQL queries all the time during the process. The interaction with the system makes the question formulation more constrained, slower and less spontaneous, but it provides guidance and safeness with intermediate answers and suggestions at each step. The translation process for SPARKLIS is reversed: it translates possible SPARQL queries into natural language such that the users can understand their choices.

(p116.1) The auto-completion is different from the previous systems (e.g., TR Discover and Ginseng): the interface displays three lists where the users can search for concepts, entities or modifiers. To ensure completeness relative to user input, SPARKLIS uses a cascade of three stages. The first stage is on client side, where a partial list of suggestion is filtered. The second stage is executed if the filtered list gets empty, then the suggestions is re-computed by sending the query, including the users filter, to the SPARQL endpoint. The last stage is triggered if the list is still empty, then, new queries are again sent to the SPARQL endpoint, using the full SPARQL query instead of partial results. Only a limited number of suggestions are computed and no ranking is applied, because of scalability issues.

(p116.2) The strength of SPARKLIS is also its weakness: the restricted guidance of the users during the query formulation process allows only syntactically correct questions, but at the same time, the users' freedom is reduced. Furthermore, the limited number of suggestions has the negative consequence that they may be incomplete and therefore making some queries unreachable.

(p116.3) SPARKLIS (Ferré, 2017) is a guided query builder for SPARQL using natural language for better understanding. It guides the users during their query phrasing by giving the possibilities to search through concepts, entities and modifiers in natural language. It relies on the rules of SPARQL to ensure syntactically correct SPARQL queries all the time during the process. The interaction with the system makes the question formulation more constrained, slower and less spontaneous, but it provides guidance and safeness with intermediate answers and suggestions at each step. The translation process for SPARKLIS is reversed: it translates possible SPARQL queries into natural language such that the users can understand their choices.

(p116.4) The auto-completion is different from the previous systems (e.g., TR Discover and Ginseng): the interface displays three lists where the users can search for concepts, entities or modifiers. To ensure completeness relative to user input, SPARKLIS uses a cascade of three stages. The first stage is on client side, where a partial list of suggestion is filtered. The second stage is executed if the filtered list gets empty, then the suggestions is re-computed by sending the query, including the users filter, to the SPARQL endpoint. The last stage is triggered if the list is still empty, then, new queries are again sent to the SPARQL endpoint, using the full SPARQL query instead of partial results. Only a limited number of suggestions are computed and no ranking is applied, because of scalability issues.

(p116.5) The strength of SPARKLIS is also its weakness: the restricted guidance of the users during the query formulation process allows only syntactically correct questions, but at the same time, the users' freedom is reduced. Furthermore, the limited number of suggestions has the negative consequence that they may be incomplete and therefore making some queries unreachable.
## (s117) GFMed
(p117.0) GFMed (Marginean, 2017) is an NLI for biomedical linked data. It applies grammars manually built with Grammatical Framework 7 (GF). GF grammars are divided into abstract and concrete grammars. The abstract grammar defines the semantic model of the input language and for GFMed this is based on the biomedical domain. The concrete grammars define the syntax of the input language, which is English and SPARQL. Furthermore, GF supports multilingual applications and because of that Romanian is included as a second natural language in GFMed.

(p117.1) To translate the controlled natural language input into SPARQL, GFMed relies in a first step on the libraries of GF for syntax, morphological paradigms and coordination. GFMed covers basic elements of SPARQL to support term constraints, aggregates and negations. There is no support for property paths of length different from 1, optional graph pattern or assignment. Furthermore, only equality and regular expression operators are included.

(p117.2) The strength of GFMed is that it covers the basic elements of SPARQL. Furthermore, it introduces a second natural language besides of English for the users to ask questions. The weakness are the GF grammars which are domain dependent and restrict the number of questions that can be asked by the users.

(p117.3) GFMed (Marginean, 2017) is an NLI for biomedical linked data. It applies grammars manually built with Grammatical Framework 7 (GF). GF grammars are divided into abstract and concrete grammars. The abstract grammar defines the semantic model of the input language and for GFMed this is based on the biomedical domain. The concrete grammars define the syntax of the input language, which is English and SPARQL. Furthermore, GF supports multilingual applications and because of that Romanian is included as a second natural language in GFMed.

(p117.4) To translate the controlled natural language input into SPARQL, GFMed relies in a first step on the libraries of GF for syntax, morphological paradigms and coordination. GFMed covers basic elements of SPARQL to support term constraints, aggregates and negations. There is no support for property paths of length different from 1, optional graph pattern or assignment. Furthermore, only equality and regular expression operators are included.

(p117.5) The strength of GFMed is that it covers the basic elements of SPARQL. Furthermore, it introduces a second natural language besides of English for the users to ask questions. The weakness are the GF grammars which are domain dependent and restrict the number of questions that can be asked by the users.
## (s119) Evaluation of 24 recently developed NLIs
(p119.0) In this section, we provide a systematic analysis of the major NLIs. We categorized and analyzed the translation process of the 24 recently developed systems highlighted in Section 5 based on ten sample world questions with increasing complexity. Each system category, based on its technical approach, has its own strengths and weaknesses. There are also different aspects on which a system can focus (e.g., number of user interaction, ambiguity, efficiency, etc.), which we do not take into account in this paper.

(p119.1) We evaluate the systems based on what is reported in the papers. If there is either an example of a similar question (e.g. 'Who is the president of the united states?' (Q1) or a clear statement written in the paper (e.g., 'we can identify aggregations' (Q7), we label those questions for the system with a checkmark () in Table  3. If the question needs to be asked in a strict syntactical way (e.g., SODA needs the symbol '¿' instead of 'higher than') or the answer is partially correct (e.g., Q4 returns a ordered list of movies instead of only one), it is labeled with a triangle (L). If there is a clear statement that something is not implemented (e.g. ATHENA does not support negations), we label it with . If we were not able to conclude, if a system can or cannot answer a question based on the paper, we labeled it with a question mark in Table 3. In general, as shown in Table 3, we can say that keyword-based NLIs are the least powerful and can only answer simple questions like string filters (Q1). This limitation is based on the approach of these keyword-based systems: they expect just keywords (which are mostly filters) and the systems identify relationships between them. Therefore, they do not expect any complexer questions like Q4 or Q7. Pattern-based NLIs are an extension of keyword-based systems in such a way that they have a dictionary with trigger words to answer more complex questions like aggregations (Q7). However, they cannot answer questions of higher difficulties, including subqueries (Q9/Q10). For example, the difficulty with questions including subqueries is to identify which part of the input question belongs to which subquery. Trigger words are not sufficient to identify the range of each subquery.

(p119.2) Parsing-based NLIs are able to answer these questions by using dependency or constituency trees (e.g., NaLIR). This helps to identify and group the input question in such a way that the different parts of the subqueries can be identified. Still, some of those systems struggle with the same problem as pattern-based NLIs: the systems are using trigger word dictionaries to identify certain types of questions like aggregations (e.g., ATHENA), which is not a general solution of the problem.

(p119.3) Grammar-based systems offer the possibility to guide the users during the formulation of their questions. Dynamically applying the rules during typing allows the systems to ensure that the input question is always translatable into formal language. The huge disadvantage of grammar-based systems is that they need handcrafted rules. There are NLIs which use a set of general rules and domain-specific rules (e.g., SQUALL). The general rules can be used for other domains and therefore increase the adaptability of the system. Other systems try to extract the rules directly from the ontology and thereby reduce the number of handcrafted rules (e.g., Ginseng).

(p119.4) We will now analyze how well the different systems can handle the ten sample questions. The first question is a basic filter question and can be solved by all NLIs as shown in Table 3. The last three questions are the most difficult ones and can only be answered by a few systems (completely by SQUALL, SPARKLIS and partially by others). Complex questions (e.g., aggregations) cannot be phrased with keywords only. Therefore, the more complicated the users questions are, the more they will phrase them in grammatically correct sentences. In contrast, simple questions (e.g., string filters like Q1) can be easily asked with keywords. Both, Waltinger et al (2013) (USI Answer) and Lawrence and Riezler (2016) (NLmaps) are describing this phenomenon and that the users prefer to ask questions with keywords if possible. They adapted their systems so that they can handle different forms of user input. Because of that, Waltinger et al (2013) (USI Answer) point out that parse trees should only be used with caution. This is similar to the approach of Zheng et al (2017) (NLQ/A), who remark that NLP technologies are not worth the risk, because wrong interpretations in the processing leads to errors. Walter et al (2012) (BELA) propose a new approach of applying certain processing steps only if the question cannot be answered by using simpler mechanisms. This approach can be used to answer questions formulated as keywords or as complete sentences. Nevertheless, parse trees are useful to identify subqueries, but only in grammatically correct sentences (e.g., NaLIR). The identification of possible subqueries is necessary to answer questions like Q9 and Q10.
