# Deep Learning for Visual Speech Analysis: A Survey

CorpusID: 248987614 - [https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785](https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785)

Fields: Computer Science

## (s2) Differences with Related Surveys
(p2.0) Several surveys on VSA [42,43,44,45,46] have been published. However, they have only partially reviewed specific VSA tasks. For example, [43,44,46] conducted reviews on VSR, and [42,45] focused on VSG. We give a brief conclusion related surveys and then emphasize our new contributions.
## (s7) Generation-Related Challenges
(p7.0) Different from visual speech recognition, visual speech generation requires not only speech-related information but also identity-related information. As shown in Fig. 4(a), generationrelated challenges mainly come from (1) information coupling, (2) diversity targets, and (3) evaluation validity.

(p7.1) A talking face video contains many types of coupled information, such as various motion-related information and  identity-related information. For motion coupling, motions that occur on a talking face video can be categorized into two types: intrinsic motions (head pose, facial expression, lip motion, etc.) and extrinsic motions (camera motion, background motion, etc.). All of these various motions are highly coupled. The motion coupling challenge stems not only from disentangling lip motion from all of these speech-irrelevant motions but also from integrating the synthesized lip sequence into a given identity image. The other coupling issue of visual speech generation is identity coupling. As illustrated in Fig. 4(c1), people may feel eerie and uncomfortable while observing these images due to the identity of generated face subtlely changed. This phenomenon, also known as the "uncanny valley effect" [100], occurs when people observe a synthetic face that's almost human-like but not quite perfect. Generally, the driving source contains rich information about the source identity. Therefore, the critical challenge is how to remove the identity information from the driving source to avoid corruption in the process of the target identity synthesis. Besides, most existing methods are only adaptive to specific target identity since different speakers have significant differences in appearance, speaking habits, etc. So, the lack of identity generalization is also an important challenge.

(p7.2) Semantic consistency and visual quality are the most desired properties of an excellent VSG method. Semantic consistency represents that the synthesized lip sequence should be synchronized and speech-consistent with the driving source. As shown in Fig. 4(c2), semantic consistency mainly involves two demands: temporal alignment and speech matching. However, the synchronous speech mapping between the driving source and the generated talking video is difficult to realize due to intrinsic differences in temporal resolution and speech characteristics of different data modalities. As for visual quality, There are two difficulties: (1) There is a lack of explicit training objectives since the fidelity and visual quality of the generated lip motion sequences are difficult to define quantitatively. (2) Because humans are sensitive to subtle artifacts, integrating the generated lip sequence into the whole face without peopleoriented perceptual errors is a complicated issue. Besides the aforementioned difficulties, efficiently evaluating visual speech generation methods is another challenge. Existing evaluations on this problem, including qualitative and quantitative metrics, have many limitations. For example, qualitative metrics like user study are unreproducible and unstable. As for quantitative metrics, although there are a dozen of evaluation metrics, some of them are not appropriate and even mutually contradictory.
## (s11) Datasets under controlled environments
(p11.0) As we can see from Table 1, before 2015, visual speech research mainly focused on controlled environments. Controllable factors include recording conditions, equipment, data types, scripts, etc. These datasets provide an excellent foundation for visual speech research. Next, we review some representative audio-visual datasets collected under controlled environments.

(p11.1) AVICAR [75] is the most representative public audio-visual dataset recorded in a car-driving environment. As mentioned above, visual speech can contribute to audio-based speech recognition, especially in noisy environments. Motivated by this, AVICAR is collected for modeling bimodal speech in a driving car, as car-driving is a typical acoustic noisy environment.

(p11.2) GRID [77], consisting of high-quality audio and video recordings of 1,000 syntactically identical phrases spoken by 34 talkers, is built for comprehensive audio-visual perceptual analysis and microscopic modeling. Besides speech recognition, it can also support audio-visual speech separation tasks.

(p11.3) MODALITY [79] contains 31 hours of recordings was created to test the robustness of audio-visual speech recognition (AVSR) systems. As for the difference from other datasets, its corpus includes high-resolution, high-framerate stereoscopic video streams from RGB-D cameras.

(p11.4) OuluVS2 [81] is a multi-view audio-visual dataset built for non-rigid mouth motion analysis. It includes 53 speakers uttering three types of utterances. Moreover, it is recorded from five different views spanned between the frontal and profile views. Multiple views of talking mouths simulate a real-world  Table. 1 for a summary of these datasets.

(p11.5) situation, as users may not face the video camera all the time while talking. IBM AV-ASR [83] is a large corpus containing 40 hours of audio-visual recordings from 262 speakers in clean, studio conditions. Compared to previous datasets under controlled environments, it has significant advantages in vocabulary and speaker number. However, this dataset is not publicly available.

(p11.6) VOCASET [69] is a 4D face dataset with about 29 minutes of 4D face scans with synchronized audio from 12 speakers (6 females and 6 males), and the 4D face scans are recorded at 60fps. As a representative high-quality 4D face audio-visual dataset, VOCASET greatly promoted the research on 3D VSG.
## (s12) Datasets under uncontrolled environments
(p12.0) Recently, researchers are gradually shifting their focus to inthe-wild visual speech learning. As a result, many large-scale in-the-wild audio-visual datasets are constructed to promote the research. We introduce some of the audio-visual datasets collected under in-the-wild environments in the following.

(p12.1) LRW [27] is a word-level audio-visual dataset constructed by a multi-stage data automatic collection pipeline. It revolutionary enlarged the dataset scale and speaker number based on the rich data volume of BBC television programs. It contains over 1,000k word instances spoken by over a thousand people. The main objective of LRW is to test speaker-independent word-level lip reading methods.

(p12.2) LRS2-BBC [6] is a sentence-level audio-visual dataset with a similar data collection pipeline and data source as that LRW dataset. It is built for sentence-level lip reading, a more challenging VSR problem than word-level lip reading. All videos in LRS2-BBC are collected from the BBC program, and it contains over 144.5k utterances with a vocabulary size of about 62.8k.
## (s14) Evaluation Metrics on VSR
(p14.0) The word-level VSR task is essentially a multi-class classification problem. Therefore, classification accuracy is the most common evaluation metric for classification models because of its simplicity and efficiency. Besides, T op − k accuracy, namely the standard accuracy of the actual class being equal to any of the k most probable classes predicted by the classification model, is also widely used in VSR.

(p14.1) As for the sentence-level task, Character Error Rate (CER) and Word Error Rate (WER) [109], also known as average characterlevel and word-level edit distances, are the most commonly used evaluation metrics. CER is defined as CER = (S + D + I)/N , where S, D and I are the numbers of substitutions, deletions, and insertions respectively to get from the reference to the hypothesis, and N is the number of characters in the reference. This metric imposes smaller penalties where the predicted string is similar to the ground truth. For example, if the ground truth is "about" and the model prediction is "above", then CER = 0.4. WER and CER are calculated in the same way. The difference lies in whether the formula is applied to character-level or wordlevel. Besides, BLEU [110], a modified form of n-gram precision to compare a candidate sentence to one or more reference sentences, is sometimes adopted.
## (s16) Visual Quality.
(p16.0) To evaluate the quality of the synthesized video frames, reconstruction error measurement (e.g., Mean Squared Error) is a natural evaluation way. However, reconstruction error only focuses on pixel-wise alignments and ignores global visual quality. Therefore, existing works usually adopt Peak Signal-to-Noise Ratio (PSNR) and Structure Similarity Index Measure (SSIM) to evaluate the global visual quality of generated frames. More recently, Prajwal et al. [38] introduced Fréchet Inception Distance (FID) to measure the distance between synthetic and real data distributions, as FID is more consistent with human perception evaluation. Besides, Cumulative Probability Blur Detection (CPBD) [115], a nonreference measure, is also widely used to evaluate the loss of sharpness during video generation.

(p16.1) Audio-visual Semantic Consistency. Semantic consistency of the generated video and the driving source mainly contains audio-visual synchronization and speech consistency. For, audio-visual synchronization, Landmark Distance (LMD) [116] computes the Euclidean distance of the lip region landmarks between the synthesized video frames and ground truth frames. The other synchronization evaluation metric is to use a pre-trained audio-to-video synchronisation network [48] to predict the offset of generated frames and the ground truth. For the speech consistency, Chen et al. [42] proposed a lipsynchronization evaluation metric, i.e., Lip-Reading Similarity Distance (LRSD), which measures the Euclidean distance of semantic-level speech embeddings obtained by lip reading networks. For better evaluation of speech consistency, lip reading results (accuracy, CER, or WER) comparisons of the generated frames and ground truth are also used as consistency evaluation metrics.
## (s18) The Overall Framework
(p18.0) Visual Speech Recognition (VSR), also known as lip reading, aims to decode speech from speakers' mouth movements. An essential preprocessing of VSR is mouth-centered region of interest (ROI) cropping. A talking face video contains a large amount of redundant information (such as pose, illumination, gender, skin color, etc.) unrelated to the VSR task. To reduce redundant information, it is necessary to crop mouth-centered videos from the raw input video. However, defining the size of mouth-centered ROI is still an open problem. Koumparoulis et al. [117] proved that the selection of ROI will significantly affect the final recognition performance, but it is still unable to determine the optimal ROI.

(p18.1) As shown in Fig. 6, a VSR system usually contains three sub-modules. The first sub-module is visual feature extraction, intending to extract compact and effective visual feature vectors from mouth-cropped videos. Then, the second submodule is temporal context aggregation, aiming to aggregate temporal context information for better text script decoding and recognition. The above two sub-modules are also the cores of deep learning based VSR methods. This paper will summarize and discuss existing deep networks for visual feature extraction and temporal context aggregation in Section. 4.2. The last sub-module is text decoding, i.e., converting the feature representations to text.

(p18.2) The rest of this section is organized as follows. Section. 4.2 presents our taxonomy of deep representation learning networks  for VSR. And then, we review and discuss various visual speech representation learning paradigms for VSR (supervised learning and unsupervised learning) in Section. 4.3. Section. 4.4 provides a comprehensive summary for readers to know the progress and limitations of existing VSR methods.
## (s19) Backbone Architectures
(p19.0) Before the era of deep learning, representation learning for VSR had already been explored for a long time. From the feature engineering perspective, traditional feature extraction methods can be categorized into three types: appearance-based, shapebased, and motion-based [8]. Although simple and explainable, traditional representation learning methods usually do not work well, especially in uncontrolled environments. This paper mainly focuses on summarizing and discussing representation learning methods driven by deep learning technologies. Considering the significant difference between deep representation learning and traditional feature extraction, we introduce a novel taxonomy strategy based on two independent parts: visual frontend network and temporal backend network.
## (s20) Visual frontend network
(p20.0) As shown in Fig. 6, there are mainly three types of input data: mouth-centered videos, dense optical flow, and landmark points. Among them, mouth-centered videos and dense optical flow are regular grid data, so CNNs are the most suitable and commonly used backbone architectures for them. On the other hand, as landmark points are irregular data, some existing works [53,54,118] adopted Graph Convolution Networks (GCNs) to extract visual features from landmark points. Next, we review these backbone architectures.

(p20.1) CNN-based Architectures. CNNs have been becoming one of the most common architectures in the field of deep learning. Since AlexNet [119] was proposed in 2012, researchers have invented a variety of deeper, wider, and lighter CNN models [120]. Representative CNN architectures, such as VGG [121], ResNet [122], MobileNet [123], DenseNet [124], ShuffleNet [125] etc, have been widely used in learning visual representation for VSR.
## (s23) Supervised learning for VSR
(p23.0) There are two mainstream VSR tasks: word-level and sentencelevel. With a limited number of word categories, the former is to recognize isolated words from the input videos (i.e., talking face video classification), usually trained with multi-classification cross-entropy loss. The latter is to make unconstrained sentencelevel sequence prediction. However, due to the unconstrained word categories and video frame length, it is much more complicated than the word-level VSR task.

(p23.1) Supervised learning of end-to-end sentence-level VSR tasks (sentence prediction) can be divided into two types. Given the input sequence, the first type uses a neural network as an emission model, which outputs the likelihood of each output symbol (e.g., phonemes, characters, words). These methods generally employ a second phase of decoding using HMM. A popular version of this variant is the Contortionist Temporal Classification (CTC) [141], where the model predicts framewise labels and then looks for the optimal alignment between the frame-wise predictions and the output sequence. The main weakness of CTC is that the output labels are not conditioned on each other (it assumes each unit is conditional independent), and hence a language model is needed as a post-processing step. Different from the basic CTC, Xu et al. [62] proposed LCANet that feeds the encoded spatio-temporal features into a cascaded attention CTC decoder. The introduction of an attention mechanism improves the defect of the conditional independence assumption CTC in hidden neural layers. Another assumption of this approach is that it assumes a monotonic ordering between input and output sequences, which is suitable for VSR but not for machine translation.
## (s28) VISUAL SPEECH GENERATION
(p28.0) Visual Speech Generation (VSG), also known as lip sequence generation, aims to synthesize a lip sequence corresponding to the driving source (a clip of audio or a piece of text). Traditional VSG methods suffer from severe practical challenges [45], such as complex generation pipelines, constrained applicable environments, over-reliance on finegrained viseme (phoneme) annotations, etc. To realize mapping driving sources to lip dynamics, representative traditional VSG methods mainly adopted cross-modal retrieval approaches [16,103,155,156] and HMM-based approaches [157,158]. For example, Thies et al. [103] introduced a typical image-based mouth synthesis approach that generates a realistic mouth interior by retrieving and warping best-matching mouth shapes from offline samples. However, retrieval-based methods are static text-phoneme-viseme mappings and do not really consider the contextual information of the speech. Meanwhile, retrieval-based methods are pretty sensitive to head pose changes. HMM-based methods also suffer from some drawbacks, such as the limitation of the prior assumptions (e.g., Gaussian Mixture Model (GMM) and its diagonal covariance). As deep learning technologies have extensively promoted the developments of VSG, we focus on reviewing deep learning based VSG methods in this section.
## (s30) Two-Stage VSG Framework
(p30.0) The two-stage VSG frameworks mainly consist of two steps: a) mapping the driving source to facial parameters using DNNs and b) transforming the learned facial parameters to output videos based on GPU rendering, video editing, or Generative Adversarial Networks (GANs) [37]. According to the data type of facial parameters, existing two-stage VSG approaches can be divided into Landmarks based, Coefficients based, Vertex based and others.
## (s33) 2D Coefficient based.
(p33.0) Active Appearance Model (AAM) is one of the most commonly used facial coefficient models, representing both the shape and texture variations and their correlation. Fan et al. [26] utilized a two-layer BiLSTM network to estimate AAM coefficients of the mouth area based on the overlapped triphone input, which then is transferred to a face image to produce a photo-realistic talking head. The experiments show that the BiLSTM network has superior performance to previous HMM-based approaches. Similarly, as shown in Fig. 8(e), Taylor et al. [66] introduced a simple and effective DNN as a sliding window predictor to automatically learn AAM coefficients based on the fixed-length phoneme sequence. Furthermore, the model can be retargeted to drive other face models with the help of an effective retargeting approach. The main practical limitation of AAM coefficients is that the reference face AAM parameterization may cause potential errors when retargeting to a new subject.
## (s36) GAN based Methods
(p36.0) To overcome the above limitations of Speech2Vid, many researchers try to improve VSG performance by utilizing generative adversarial training [37] strategies. As shown in Fig. 8(j), GAN based VSG methods usually consist of three subarchitectures, i.e., encoders, generators, and discriminators.
## (s37) Other Methods
(p37.0) In addition, some other one-stage VSG schemes have also been proposed. Inspired by the success of the neural radiance field (NeRF) [200], Guo et al. [73] proposed the audio-driven neural radiance fields (AD-NeRF) model for VSG. As shown in Fig. 8(k), AD-NeRF takes DeepSpeech audio features as conditional input, learning an implicit neural scene representation function to map audio features to dynamic neural radiance fields for talking face rendering. Furthermore, AD-NeRF models not only the head region but also the upper body via learning two individual neural radiance fields. However, AD-NeRF does not generalize well on mismatched driving audios and speakers. As shown in Fig. 8(l), unlike the previous concatenation-based feature fusion strategy, Ye et al. [74] presented a full convolutional neural network with dynamic convolution kernels (DCKs) for crossmodal feature fusion, which extracts features from audio and reshapes features as DCKs of the fully convolutional network. Due to the simple yet effective network architecture, the realtime performance of VSG is significantly improved.
