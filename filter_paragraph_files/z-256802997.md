# A Literature Review: ECG-Based Models for Arrhythmia Diagnosis Using Artificial Intelligence Techniques

CorpusID: 256802997 - [https://www.semanticscholar.org/paper/edcee80a688a18e34e3ba34ba459c4c0e78ab2ef](https://www.semanticscholar.org/paper/edcee80a688a18e34e3ba34ba459c4c0e78ab2ef)

Fields: Computer Science, Medicine

## (s4) Results of Studies' Exploration Selection
(p4.0) To search for the adequate papers, we rely on the process shown in Figure 1. First, we realized a quick search for the topic by keywords which results in 730 records. Second, we removed duplicates given that we used 5 research databases, then a paper inventory was held by sorting publications by abstract.

(p4.1) When applying the inclusion criteria in Table 1, we focused on selecting papers that do not deal with other cardiovascular diseases. Although the study 9 includes the treatment of myocardial ischemia, it was selected because there was no interference with arrhythmia diagnosis; the 2 diseases were independently addressed. Irfan et al 10 used a dataset with 13 types of heartbeats, including arrhythmias and myocardial infarctions, which added more variety to the dataset without affecting the performance of the model for arrhythmia diagnosis. 
## (s11) Pre-processing methods
(p11.0) According to Table 4, only 1 study was not subject to data preprocessing. The most used techniques are ECG heartbeat segmentation (17 studies), noise removal (13 studies), data normalization (8 studies), and QRS detection (6 studies). However, 4 studies relied on R-peak detection and this detection reached an accuracy of 99.3% in Oh et al. 31 Furthermore, the most used algorithms in the pre-processing phase are the Pan-Tompkins algorithm to detect accurately R peaks and QRS complexes, and the WT to reduce the cost of continuous wavelet computation. Table 6 below summarizes the pre-processing methods, their application, and the objective from their usage.

(p11.1) The ECG signal segmentation is applied with different sample-long segments that vary between 100-and 500-sample long. The samples are centered either around the detected R peaks or the detected QRS complexes. The segmentation can also rely on the extraction of T-to-T segments as in Zubair and Yoon, 44 or can simply rely on the database annotation files. Noise removal method is applied to remove different types of noise that can result from patient motion or respiration, power line interference, muscle artifacts, baseline drift, electrode motion artifact or data-collecting device noise. To the fact that each noise source resides in a characteristic frequency band, different filters and techniques are used depending on the type of noise.

(p11.2) Data normalization can be considered one of the most interesting methods due to its important influence on the classification process. Namely, signal rescaling improves significantly the backpropagation process by speeding up the convergence rate.

(p11.3) Most of the studies where pre-processing was applied to data showed a better performance on the classification as in Iftene et al 51 where CNN model reached an accuracy of 95% without pre-processing vs 98% when applying data augmentation and normalization.
## (s12) Prediction methods
(p12.0) As shown in Table 4, CNN and LSTM network are the most used techniques, followed by the SVM in 22% of the studies. Indeed, more than half of the studies used DL techniques to improve the accuracy. CNNs are used with different variants in the convolutional blocks as in literature. [44][45][46] Some studies reported the computation time in the learning phase; Yang et al 22 recorded the lowest training time with a value equal to 68.8 seconds using leads II and V1. The use of CCANet in the feature extraction phase has definitely reduced the computation cost and improved the accuracy and specificity which reached, respectively, 99.4% and 99.6%.

(p12.1) Shahin et al 40 reported a very interesting DL technique; the architecture of the adversarial multi-task model consists of 3 networks: the generator network, the heartbeat-type discriminator which discriminates between 5 types of heartbeats, and the subject discriminator which discriminates between 39 different subjects. This design has increased the performance allowing double discrimination and forcing the system to take into account only the heartbeat variations. Yet, it can be improved by changing the method of synthetic data generation; generating new data with GAN network or SMOTE technique instead of upsampling which generates duplicated data. Sabut et al 42 used a fusion of 2 CNN branches with different scales and an Attention module to mine the discriminative features. In fact, the attention mechanism boosts the classification performance as shown in Hu et al 45 where the attention helped to capture the inter-beat dependencies.

(p12.2) The combination of residual convolutional blocks and bidirectional LSTM model with Attention method in Ma et al 41 seems to be effective since it allows a local and global feature extraction, and high accuracy that reached 99.4%. Zubair and Yoon 44 mitigated the problem of imbalanced data in CNNs by designing a novel cost-sensitive loss function in the network. This learning strategy is based on training efficiently the model without changing the distribution of the data. Besides, the aforementioned study highlighted the use of 2 different paradigms: the intra-patient and inter-patient classifications to show how the latter achieves better generalization capability.

(p12.3) Luo et al 50 used a hybrid model combining CNN layers, LSTM, and GRU networks. Indeed, the authors took advantage of every network's strength: the high ability of temporal and spatial information extraction of CNN, acquiring sequential information by LSTM, retaining only relevant information by the GRU, and avoiding the gradient disappearance issue.

(p12.4) As for the development tools, Python was used with its different ML libraries, such as TensorFlow, PyTorch, Scikit-learn, and Keras. MATLAB is also employed in some studies. Iftene et al 51 developed the prediction technique in the Amazon Web Services platform using an integrated DL model.
## (s13) Evaluation and performances
(p13.0) First, we compare the studies that used the same datasets. We sort 7 different datasets available on the PhysioNet repository: 23 Figure 4, the model combining MIT-BIH and SV databases achieved a high accuracy of 99.8%, whereas the model relying only on SV database achieved an accuracy of 97.6% (Figure 7). Nevertheless, both the results were promising. Studies 19,22,37 enrolled the learning phase using INCART database for 4-class, 7-class, and 5-class prediction, respectively. The highest accuracy of 99.8% was achieved by the first study where they used 12-lead ECG recording format vs an accuracy of 98.76% with 2-lead format and 97.57% for the 2 other studies. For the rest of the studies where they used MIT-BIH arrhythmia database, all of them reached an accuracy above 90%.

(p13.1) Second, we carry on a comparison between studies using the same prediction methods. Regarding the studies applying SVM, they used different kernel functions and some of them were combined with other ML algorithms, but most of them yielded an accuracy greater than 94%. This can be explained by the powerful methods used for feature extraction and data pre-processing, including the use of DL techniques, 20,22,38 in these studies. When comparing the studies that applied CNN, all of them attained high accuracy rates above 94%. The lowest metrics (accuracy, specificity, and sensitivity) were obtained by Qin et al 34 that performed SVM on record-based training scheme where the classifier was trained and tested on separate records from different individuals.

(p13.2) Regarding the studies with smaller signal durations (between 7 and 30 seconds), they achieved good F1-score values but the highest scores were obtained by 30-minute duration studies. And yet, the increase in ECG signal length does not guarantee the highest accuracy rates. Indeed, in this review, the studies with the lowest signal duration 15,16,30 could perform better, especially when they applied deep CNNs; however, they either did not proceed data pre-processing 16 or they used imbalanced data 30 for the classification.

(p13.3) In Irfan et al, 10 the DL model achieved better results on the second dataset, this is due to the highly imbalanced data in the first dataset. Only the accuracy of the best model was reported in Table 4 (an overall accuracy of 99.35% for balanced data vs 93.33% for imbalanced data).

(p13.4) For Shahin et al, 40 the adversarial multi-task model achieved an overall accuracy of 86% on the validation set and 87% on the test set, which are lower comparing to other techniques, due to the imbalanced data.

(p13.5) Zubair and Yoon 44 achieved a high accuracy of 99.81% in the intra-patient paradigm with CNN with different size kernels and cost-sensitive function. Hu et al 45 reached an accuracy of 99.49% for 4 class-categorization with transformer encoder-decoder network with CNN layers and attention mechanism. The use of CNN with different kernel sizes (to capture different segment and interval lengths) allowed to obtain an accuracy of 98% for 41 arrhythmias classification.

(p13.6) Wang 49 used a novel method of premature ventricular contraction (PVC) detection where they modified a GRU network to avoid the redundancy of information in the forward and backward connections. This improved version of GRU yielded an accuracy of 97.9% on MIT-BIH data and 98.3% on CPDB.

(p13.7) Most of methods relying on DL, ML, statistical AI techniques, or a combination of them had performed high accuracies because all of the selected studies in this review realized rigorously the feature extraction phase and the pre-processing phase.

(p13.8) Among the studies selected, there are many that have used variety of approaches/databases/methods. Depending on each criterion, we linked the use of pre-processing and prediction methods to the accuracies which they are shown in Table 4.
