# Trait of Gait: A Survey on Gait Biometrics

CorpusID: 85517646 - [https://www.semanticscholar.org/paper/64e703c01f0b8846d1601bca71804a7b194fc983](https://www.semanticscholar.org/paper/64e703c01f0b8846d1601bca71804a7b194fc983)

Fields: Computer Science, Medicine

## (s1) Model-free techniques
(p1.0) The vast majority of model-free techniques tend to have a strong reliance on spatiotemporal analysis of silhouettes of the individual during gait. A spatiotemporal analysis takes into account the variation in the spatial domain with respect to that in the time domain. So when this is applied in gait recognition, the analyses involves the observation of the spatial locations of body parts and their movement in different stages in time.

(p1.1) Nearly all model-free methods have background subtraction and silhouette extraction as the first step. Background subtraction is a simple method in which the change in pixel values between one frame and the successive frame is observed to bring out only the objects that are seen in motion. From these objects, the moving human silhouette can be extracted. The result of background subtraction is usually binarized in which the moving object seems to be white and the background is black, or vice versa in some cases like the USF HumanID silhouette database.

(p1.2) The novelty mainly lies in how the features are extracted. The recognition process involves the use of an established machine learning algorithm such as the nearest neighbour classifier (used in [21][22][23][24][25][26][27][28]), and HMM (used in [29][30][31][32][33]). Some apply different techniques such as canonical analysis [34,35], spatiotemporal correlation [36], and DTW [25,37,38]. A few others devise newer techniques for closeness representation.
## (s2) Earlier methods
(p2.0) The earliest known spatiotemporal gait recognition techniques were published during the 90s. Niyogi et al. [22] proposed to recognise gait at a sagittal angle with the subject walking frontoparallel. It is a template-based method which modelled the human gait in the form of a set of spatiotemporal snakes [39] from the slices of the moving parts of the human contour along the time domain. They expressed the spatiotemporal dimensions as XYT signifying the 2D (x, y) spatial coordinates varying with respect to time t. The recognition they have obtained with 26 image sequences across five human subjects reaches up to 83%.

(p2.1) J. Little and J. Boyd [40] introduced the concept of optical flow in gait recognition. In principle, the points in the image sequence that vary with time tend to oscillate periodically during the subject's gait. By observing the optical flow of these points, the m time varying scalars can be produced. From these scalars, the phases of the oscillations φ 1 , φ 2 , ..., φ m can be extracted. The phases are normalised with respect to a reference phase φ m to form the feature vector F = (F 1 , F 2 , ..., F m−1 ) where F i = φ i − φ m . Hence F is used to represent the gait instance. They reached a higher recognition rate of up to 95% by testing their technique with seven instances for each of six human subjects.

(p2.2) An earlier version of template matching method was proposed by P. S. Huang et al. [34]. They used the Eigenspace transformation (EST), as adopted by Murase and Sakai [36], to convert the gait taken as a sequence of images to a template called the 'eigengait'. On top of this, canonical space transformation is applied with generalised Fisher linear discriminant function to separate the classes boundaries required for prediction. However, they also seemed to use the small dataset used by Little and Boyd for their application to show a questionable accuracy of 100%.

(p2.3) Static features are those that do not change over the temporal domain. These include features such as the height of the person, the length of the limbs. Johnson and Bobick [41] used static features to obtain a recognition rate of 94% with an expected confusion of 6.37%. The experiment involved placing cameras at each of two different locations; near and far. 18 human subjects were made to walk three times each and were observed at the two oblique angles. The features determined were the height of the bounding box, the Euclidean distance between the head and pelvis, maximum length between the pelvis and either of the feet, and the distance between the left and right foot.

(p2.4) Though the above methods show attractive recognition rates, all the methods proposed at those times before suffered one major drawback: their accuracies are biased to their samples which are too small when considering the application as a biometric.
## (s3) Template-based methods
(p3.0) These are a class of methods that involves transforming the sequence of silhouette images taken from a gait video to a single image that holds the composition of the motion-related features of the sequence.

(p3.1) Hayfron-Acquah et al. [21] assessed the symmetry of the extracted silhouette using a generalised symmetry operator. The contours of the silhouette are extracted by applying Sobel edge detection function. From the sequence of contours obtained, a symmetry map is produced. Euclidean distance between Fourier descriptors is used as a similarity measure for gait recognition. They used the SOTON database for their experimentation and had attained a correct classification rate (CCR) of 97.3 % for k = 1 and 96.4% k = 3 using the nearest neighbour classifier.

(p3.2) L. Wang et al. proposed a unique method to recognise gait by analysing the contours of the silhouettes [42]. The CASIA dataset A was used in this experiment. The shape of the contour of a given silhouette sequence is converted to a template with the use of Procrustes shape analysis. Different exemplars are created for each viewpoint. They tried three types of nearest neighbour algorithms, viz., NN, kNN, and ENN (NN with class exemplar), in which ENN provided the best results for gait recognition.

(p3.3) Experimental results of Cuntoor et al. [60] suggests that Dynamic Time Warping (DTW) and Hidden Markov Model (HMM) can be combined to produce a better gait recognition result. DTW was used to align the motion of the arms and legs to normalise the phase of gait while HMM is used to define the leg dynamics.

(p3.4) The efforts of the University of South Florida [43] has brought forth a new revolution to gait recognition. They compiled the USF Gait Challenge dataset which consists of 1,870 gait sequences obtained from 122 human subjects over 5 types of variation. The dataset was categorised to 12 challenge probe sets for experimentation and a gallery set for training. A simple baseline algorithm was developed to facilitate users of the dataset to compare the performance of gait algorithms effectively. The algorithm involves the use of the Tanimoto similarity measure (Eq 1) to guage the similarity between two silhouettes.

(p3.5) Here, p and q are two binarized images where each image is represented as an ordered set of pixel values. Their intersection corresponds to the number of pixels that are the same in their silhouettes. Their union provides the total number of pixel space taken when both silhouettes superimpose. The correlation between the silhouette similarities provides the measure of closeness used for the recognition step. Further details of the USF Gait Challenge dataset is provided in section 5.
## (s5) Non-template methods
(p5.0) Though found to be efficient in practice, not all silhouette-based methods in literature involves the production of a template image. We shall discuss some of them here. J.P. Foster et al. [35] have claimed to have attained recognition rates above 75% by running experiments on 114 subjects of the SOTON video gait dataset. Their method monitors the temporal changes in the areas of the clipped gait window segmented by masked sectors. Using these timevarying area metrics, they formulate a feature vector for recognition.

(p5.1) A framework for joint tracking and event detection using maximum a posteriori (MAP) hypothesis was suggested in [67]. Although not proposed as a gait recognition technique, this method would be able to produce an efficient gait recognition technique if utilised in such a way. Four events were modelled corresponding to the different directions of walking concerning the point of view: walking towards the camera, walking away from the camera, walking from left to right and from right to left. The corresponding HMM was trained and validated using the UMD Dataset 1.   In the work by N. V. Boulgouris et al. [70], a low dimensional feature matrix is represented by accounting the average distances from the centre of the silhouette. Each silhouette is represented as a feature vector which is composed of a sequence of angular transforms made on segmented angular slices ∆θ. The dimension of each vector is K = 360/∆θ. The feature matrix is produced by collating the vectors of each silhouette in the sequence of frames. Hence each column would represent a frame, which each row would depict the change in the angular transform for a given angular slice. In the USF gait challenge dataset, a period in the gallery (reference) is not equal to that of the probe sequence (test). Hence, a technique called linear time normalisation was utilised to make the feature matrix of each probe and gallery sequence comparable by compensating for the difference in the sequence length. The same method is used in their next paper [71] but with a different feature extraction technique. In this work, the body as depicted in the silhouette is segmented into components. The components are ranked with according to their proportional relevance during the comparison operation.

(p5.2) The work of Zongyi Liu and Sudeep Sarkar [29] promoted the fusion of face and gait biometrics. A subset of the USF dataset was used for this purpose. The exemplars, in this case, are not obtained from a single template but a specified stances analogous to the formally depicted gait cycle. These stance frames are modelled using a population EigenStance-HMM; a method that is extended from their previous technique, population HMM [31]. The entire population of the training set is fed in the learning phase. A given gait silhouette sequence can be segregated into these stance models by K-Means clustering with Tanimoto distance as the distance measure. A cyclic Bakis variant of HMM was modelled for the gait recognition. This gait recognition algorithm was used in combination of face recognition using Elastic Bunch Graph Matching (EBGM) based on Gabor features to attain a much higher performance over the Gait Challenge baseline algorithm when compared using the toughest sets of the USF Gait Challenge dataset.

(p5.3) A gait recognition that analyses both shape and motion is proposed in [38]. The gait period here is depicted as ten phases. Spatiotemporal shape features are obtained from these phases in the form of Fourier descriptors. The silhouette contour at each step is segmented by fitting ellipses. The similarity is calculated utilising Bhattacharyya distance between the ellipse parameters taken as features. Dynamic time warping (DTW) is applied to compare leading knee rotation with relevance to arm swing pattern over a gait cycle. DTW is used so as to counteract the effects of walking speed, clothing, shadows, and hair styles.
## (s6) Model-based techniques
(p6.0) While basic spatiotemporal methods give cues as to how the body position vary with accordance to time as a whole, it would be much more accurate to do a spatiotemporal analysis on all articulation points separately. The implementation of that sort will fall under the category of joint trajectory or model-based methods. That is, the trajectory of each joint is tracked live and analysed as individual components; efforts are made to model the human structure accurately.

(p6.1) Bulb markers can be attached to certain points on the body considered necessary for gait analysis such as ankles, knees, hands, elbows, shoulders and torso. When observed from a camera with a low exposure, only the bulb illumination can be perceived. This method facilitates an easier and a more accurate analysis of gait through computer vision. Rawesak T. & Bobick [37] implemented this by strapping 18 human subjects with 16 markers at appropriate locations and projected their gait at a sagittal angle. The sensors were able to reconstruct a mobile skeletal structure recovered from the joints. From this data, they were able to assess the articulation points over time. The variance in time was normalised by applying DTW, and the recognition was based on the nearest neighbour algorithm to produce a modest recognition rate of 73%.

(p6.2) A study by Z. J. Geradts et al. [74] was conducted to be able to extract gait-related parameters from all three planes -frontal, transverse and sagittal -from surveillance cameras. 11 human subjects participated in the experiment and involved the use of 11 bulb markers fitted to the necessary points to each subject. They were able to observe various parameters from step length, cycle time and cadence to joint angles and spatial positioning. After a simple analysis of variance (ANOVA) on the gait parameters extracted, it seemed like the foot angle exhibited the most variance and then the time average hip joint angles followed by the step length. Hence these are considered to be the best parameters to be used for recognition. However, their research concluded that the gait analysis cannot be used for identification at that time (the year 2002).

(p6.3) Rawesak T. & Bobick later produced a study on the recognition of gait in different speeds of walking [75], but this time, they resorted to a more comfortable reflective suit for the articulation point signal extraction. The experimental is described in [79]. A 12-camera VICON MoCap system is used for the 3D motion analysis. The result based on 15 human subjects concluded that a positive linear correlation could be observed between cadence and speed, and a negative exponential correlation could be observed between stride time and speed.

(p6.4) The methods described so far require the use of complicated hardware for better accuracy. They either require reflectors, bulb markers, or magneto sensors to be fitted on to the points of interest of the human subject to gather the point-light information during his/her gait. Due to their nature, these methods are not practically feasible for biometric application. It is to note that the concept of being an 'unobtrusive' means of biometrics is lost here.

(p6.5) Not all model-based techniques, however, are impractical for application. Zhang et al. [32] show that it is possible to extract a five-link biped human model from a two-dimensional video feed to produce a model-based gait recognition system. The five links extracted consist of both the lower and upper leg portions and the upper body. The Metropolis-Hastings algorithm [80] was used for the feature extraction. The arm motion is neglected in this study. The changes in angular displacement of these links in the sagittal elevation angle (SEA) are analysed to using the HMM for the gait recognition. The method was applied to both CMU MoBo dataset as well as USF Gait Challenge video dataset.

(p6.6) An innovative yet simple method for locating the articulation points of the lower limb joints was implemented by M. Goffredo et al. [76]. By making smart estimates on where to initialise the points, the point-light data for the hip, both knees and ankles were extracted. This method was able to extract these points over multiple views as provided by the CASIA-B video database. By recording the temporal changes of these points, a profile recorded could be recognised with the help of the k-nearest neighbour algorithm (kNN).

(p6.7) A standard video feed would provide a two-dimensional data for processing. When added with depth based information, more accurate conclusions can be drawn to aid recognition. Microsoft Kinect provides this functionality. The Kinect is used in references [77] and [78] by facilitating three-dimensional data flow for a more natural and efficient biometric gait recognition.  (2) 
## (s8) First Datasets on Gait
(p8.0) When gait analysis was increasing in interest as a biometric, the University of Southampton (SOTON) generated its gait dataset in the year 1996 [94]. It had only four subjects with a total of just 16 gait sequences. Self-occlusion occurs when the visual perception of one leg overlaps with that of the other leg. The first SOTON dataset attempted to reduce the self-occlusion problem by making the subjects wear white tracksuits with a single dark stripe vertically down either side walking frontoparallel with a plain cloth background. The visible line can help differentiate and track the leg closest to the camera throughout the gait cycle.
## (s14) University of Southampton (SOTON)
(p14.0) The HumanID programme caused SOTON to reformulate its dataset with more individuals and reduced clothing constraints. With 28 subjects and four sequences per subject, the size of the dataset came up to 112 sequences in the year 2001. This database was used in [21,35,43]. SOTON again revamped its database in the year 2002. The gait database was structured in two different datasets called the SOTON small and SOTON large.
## (s16) Small Dataset
(p16.0) Though the large database had the most members, the small database with 12 subjects had the most covariates compared to all other currently available public databases. Gait sequences were recorded in different conditions as follows: The background is staged with a green curtain and a dark green walkway. There is not yet a credible source to state exactly how many gait sequences are present in this database. It is also to note that not all 12 subjects perform all of the above variations; studies shows records of only 11 subjects [28,45].  Figure 3: A sample sub-sequence of the silhouettes provided in the USF Gait Challenge database. This illustration contains one frame for every three frames in the sequence. Notice that the shadow produced by the subject is also captured leaving the heel angle incalculable for most of the sequences.
## (s17) Temporal Dataset
(p17.0) The database consists of gait sequences from 25 subjects spanning over a period of a complete year taken at 5 (irregular) intervals. 12 cameras (v) capture the gait of each subject. The sessions were conducted as shown in Table5. 20 instances, i, were recorded for every subject for each session. Calculating with the above information, we obtain the total number of sequences (25s + 23s + 22s + 21s + 18s) × 20i × 12v = 26160 seq.

(p17.1) The observation was done in the Multi-Biometric Tunnel of the University of Southampton [96]. The background is tiled with red, green and blue squares. This facilitates feature mapping to consolidate the camera views to construct a 3-D image of anything that passes through the tunnel. There were also slight specific variations in clothing and footwear. Details of which are provided in [92]. This gait database is clearly the largest to cover the temporal aspect of gait to date.
## (s20) Dataset A
(p20.0) This dataset was previously known as the NLPR Gait Database, early literature such as [68] and [87] use this name. CASIA-A is a basic gait dataset with the gait of 20 subjects, s, recorded at 3 views, v. 4 instances, i, were recorded for each subject leading to sequence count of 20s × 4i × 3v = 240 seq.
## (s22) • 2 instances carrying a bag
(p22.0) Along with an additional video of bare background, the total number of videos actually becomes 15004 (approximately 17.4 GB). The background is mostly plain light green with two stage markings covering both the wall and the floor [86]. This is the most common multiview database in literature. Figure 4 shows a depiction of the dataset for a given time instant from each of the 11 angles.
## (s23) Dataset C
(p23.0) The gait sequences of 153 subjects, s, were collected. Each subject performed 4 gait instances in different conditions, c: slow walk, normal walk, fast walk, and walk while carrying a bag [87]. Unlike CASIA-A and CASIA-B, this dataset is only observed at a single sagittal angle. Hence the total number of gait sequences becomes 153s × 4c = 612 seq.
## (s25) Osaka University -Institute of Scientific and Industrial Research (OU-ISIR)
(p25.0) The OU-ISIR produced a whole range of biometric databases. The ones that directly correspond to gait analysis are provided in this section. The treadmill datasets were prepared over a long course of time (from 2007 to 2012). The information for all treadmill datasets can be obtained from [93].
## (s28) Treadmill dataset D -Gait fluctuations
(p28.0) The features of a gait cycle, in general, would not be the same in every period of an individual's gait. A certain amount of fluctuation can be observed. This dataset aims to measure this fluctuation with the aim to use this feature as a biometric attribute. 185 subjects, s, were recorded at the sagittal angle to measure fluctuations that could occur in multiple gait periods. Each member had two trials, i resulting in a number of sequences of 185s × 2i = 370 seq.

(p28.1) Gait fluctuations are measured in terms of Normalised AutoCorrelation (NAC) [93]. The subjects were grouped into that with higher NAC (least fluctuating), DB high and that of lower NAC (most fluctuating), DB low . There were 100 members in each group, and 4 trials were made for each, but 15 subjects were part of both groups.
## (s29) Large Population Dataset
(p29.0) This dataset consists of gait silhouettes of over 4000 subjects with ages of 1 to 94 years. The gait sequences were captured with two cameras at two different angles out of which only one is available so far as part of the publicly available dataset. This is the gait dataset that involves the largest number of human subjects till date. The total number of gait sequences in this dataset is reported to be 7842 [51,95]. A detailed description of this dataset can be found in [90].
## (s30) Speed Transition Dataset
(p30.0) While all other datasets concentrated on keeping the speed relatively constant for a given gait sequence, this dataset aims to observe the gait while allowing changes in speed within a single gait sequence. This is separated into of two datasets. In dataset 1, 179 subjects are made to walk towards a wall from a given point while exhibiting gradual decrease in walking speed. While dataset 1 was recorded indoors on flat ground, dataset 2 was treadmill-based. Dataset 2 consisted of gait sequences from 178 subjects. Each gait sequence included either of acceleration or deceleration of walking speed between 1 km/h to 5 km/h. Both datasets have a gallery set which includes the subjects walking at a constant speed of 4 km/h [89].

(p30.1) There aren't much reported research that concerns the use of this particular dataset which could be due to the level of challenge to be faced by algorithms to recognise gait in such constraints. The rare use of this database could also be due to the extent of artificial constraints applied to the subjects. The subjects have to control their gait effectively to be conscious about the speed transition. This anticipation could inhibit their natural gait.
## (s31) Inertial Sensor Dataset
(p31.0) Unlike the other datasets, this one includes the kinetic data from gait. The apparatus is in the form of a belt that has three IMUZ sensors and a smartphone attached. Each IMUZ sensor consists of a triaxial accelerometer and a gyroscope. The subject wears the belt around the waist as he/she walks through the specified track. It is claimed to be the largest ever inertial sensor-based gait dataset [91]. The dataset is split into two subsets. The first subset has 744 subjects with two trials of flat-ground gait. The second subset is of 495 subjects two trials for no inclination, one for up-slope walk and another for down-slope walk. Thus an estimate of the total number of gait sequences would come across (744s × 2i) + (495s × 4i) = 3468 seq. 
## (s33) TUM-IITKGP
(p33.0) The joint research efforts of the TUM and the Indian Institute of Technology Kharagpur (IIT-KGP) has brought forth a gait database which has become the first to address the problem of occlusion in gait recognition [61]. Each of the 35 subjects, s, followed 6 different walking conditions, c, two trials each for left to right and right to left directions. Hence 4 instances are recorded for each of the configuration per subject. The total number of gait sequences becomes 35s × 6c × 4i = 840 seq.

(p33.1) The configurations are regular, hands in pocket, wearing a backpack, wearing a gown, dynamic occlusion, and static occlusion. Dynamic occlusion refers to the scenario when the subject under observation is occluded by a moving object. In this case, dynamic object relates to another individual moving in the opposite direction and parallel to the subject. Static occlusion is the scenario where the subject is temporarily occluded by an object that doesn't move. The static object in here refers to another individual standing in front of the camera so as to block a portion of the gait sequence capture.
## (s34) TUM-GAID
(p34.0) This is a database that was created with an intention to recognise gait from audio, image and depth (GAID) information ( Figure 5). The gait data was compiled from 305 subjects in three walking conditions (normal, backpack, coating shoes). While the image and depth data have been studied before, the audio data would facilitate acoustic gait recognition [51]. The variation in the dataset is as given below. 176 subjects were recorded in the January session (at -15°C) and 161 subjects in the April session (at 15°C).

(p34.1) There were five trials in each subject per session. Each trial consists of walking once towards the right of the camera and another towards the left. Three trials were recorded for normal walk, one trial while wearing coating shoes over their own shoes, and one trial while wearing a backpack. The total number of sequences can be calculated as (176s + 161s) × 5c × 2i = 3370 seq.

(p34.2) The different clothing conditions are also associated with the weather of the sessions. Subjects wore heavier winter clothing in the first session while a more casual clothing in the second session. 32 of the subjects were common in both sessions. Hence the data from these members can be used for analysing the variations in gait with respect to the differences in clothing and time.
## (s36) Discussion
(p36.0) Ever since the time of its introduction, gait analysis was largely of medical relevance. It was only at the break of the 21st century did gait analysis also become of increasing interest outside the clinic and into forensic science. The human body is designed to walk in a way such that the energy that is expended is kept to a minimum. The dissipation of energy is dependent on various factors such as the biological structure, load exerted, and the experience of the individual. These differences make the natural gait of each person to be distinct. This distinction is what analysts try to capture as gait biometrics.

(p36.1) Biometric gait recognition is yet to reach its peak accuracy. The state of the art published literature shows groundbreaking results. Nevertheless, they have been produced with datasets that are imposed by serious constraints. Even with the complete description of the USF Gait Challenge, papers show conflicting results with the same set of algorithms. A thorough, unbiased, practical evaluation of the recent methods is much of need at this time. This assessment should also cover validation through more than a single dataset. It is also questionable whether the true natural gait of each subject is captured in the popular gait datasets. During the start of the walk, an individual would have adequate control over his/her gait. The currently available datasets do not always take this issue into account. Though datasets have attempted to account for the inhibiting factors of gait, there is still the problem of internal and external control that one can exhibit in his/her gait.

(p36.2) There was a study which took place at the end of the 20 th century which compares the performance of 3D camera systems available at that time [103]. However, the technology has evolved and many new techniques have arrived today leaving the results obsolete. Hence a similar study remains yet to be compared to evaluate the performance of the state of the art MoCap systems.
## (s38) Active areas
(p38.0) • Reliable recognition independent of clothing style. Several algorithms have been outlined to combat this problem (e.g., [53,86]) and a handful of datasets to help with this regard (CASIA-B, OU-ISIR-B, TUM-GAID, and so on). However, state of the art gait biometrics algorithms do suffer a significant depreciation when the clothing style changes.

(p38.1) • Gait recognition at different walking speed. With datasets like CASIA-C and OU-ISIR-A, gait recognition at different speeds is becoming more of interest [75,79,104]. There are, however, further improvements that can be made.

(p38.2) • View-independent gait recognition. Numerous studies that propose recognition models that could cope with multiple views. Many of which can be observed in Tables 1, 2 and 3. Steady progress is still being made in this area.

(p38.3) • A comparative study between model-based and model-free recognition methods. Though studies contrast on the merits and demerits of both approaches, there is still space for an indepth comparative analysis with newer technology such as the upgraded versions of Kinect.

(p38.4) • Unbiased comparison of state of the art gait recognition. Many of the studies discussed in this paper report conflicting results of correct classification rates when performances of established algorithms are compared, even with the same dataset. The existing algorithms are to be compared with multiple datasets and without bias.

(p38.5) • Gait reidentification performance. Gait identification with occlusion can be a challenge by itself wherein multiple cycles are provided for training per person. However, identifying whether two given gait signatures are from the same person can be even more difficult especially when considering multiple views.
