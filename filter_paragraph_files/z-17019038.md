# WAVELET BASED IMAGE CODING SCHEMES: A RECENT SURVEY

CorpusID: 17019038 - [https://www.semanticscholar.org/paper/dcf48ef38e425755c31c4cf1cecfeea22c1ead6e](https://www.semanticscholar.org/paper/dcf48ef38e425755c31c4cf1cecfeea22c1ead6e)

Fields: Computer Science

## (s10) The SPIHT (Set Partitioning in Hierarchical Trees) Coding
(p10.0) The SPIHT coding [11] is an improved version of the EZW algorithm that achieves higher compression and better performance than EZW. It was introduced by Said and Pearlman [25] in 1996. SPIHT is expanded as Set Partitioning in Hierarchical Trees. The term Hierarchical Trees refers to the quadtrees that is defined in the discussion of EZW. Set Partitioning refers to the way these quadtrees divide up and partition the wavelet transform values at a given threshold. By a careful analysis of this partitioning of transform values, Said and Pearlman were able to develop the EZW algorithm, considerably increasing its compressive power. SPIHT algorithm produces an embedded bit stream from which the best reconstructed images with minimal mean square error can be extracted at various bit rates. Some of the best results like highest PSNR values for given compression ratios for a wide range of images, have been obtained with SPIHT algorithm.
## (s11) The SPECK Coder (Set Partitioned Embedded Block Coder)
(p11.0) The SPECK [12] algorithm can be said to belong to the class of scalar quantized significance testing schemes. The roots of this algorithm primarily lie in the ideas developed in the SPIHT, and a few block coding algorithms. The SPECK coder is different from the aforementioned schemes in that it does not use trees which span and also exploits the similarity across different sub bands. As an alternative, it makes use of sets in the form of blocks. The main idea is to exploit the clustering of energy in frequency and space in the hierarchical structures of wavelet transformed images.
## (s13) The EBCOT (Embedded Block Coding with Optimized Truncation) Algorithm
(p13.0) The EBCOT algorithm [13] shows advanced compression performance while producing a bitstream with a rich set of features, like resolution and SNR scalability together with a "random access" property. All these features coexist within a single bit-stream without considerable reduction in compression efficiency. The EBCOT algorithm makes use of a wavelet transform to create the subband coefficients which are then quantized and coded. Although the usual dyadic wavelet decomposition is often used, other "packet" decompositions are also supported and occasionally preferred. The original image is characterized in terms of a collection of subbands, which may be organized into increasing resolution levels. The lowest resolution level comprises of the single LL subband. Each successive resolution level contains the additional subbands, that are required to reconstruct the image with twice the horizontal and vertical resolution.

(p13.1) The EBCOT algorithm is a scalable type image compression technique where the advantage is that the target bit-rate or reconstruction resolution need not be known at the time of compression. Another advantage of practical significance is that the image need not be compressed multiple times so as to achieve a target bit-rate, which is common in the existing JPEG compression technique. EBCOT algorithm divides each subband into comparatively small blocks of samples and creates a separate highly scalable bit-stream to represent each so called code-block.

(p13.2) The algorithm has modest complexity and is well suited to applications involving remote browsing of large compressed images. This algorithm uses code-blocks of size 64 x 64 with subblocks of size 16 x 16. The EBCOT bit-stream is composed of a collection of quality layers and SNR scalability is obtained by discarding unwanted layers.

(p13.3) The EBCOT images exhibit significantly less ringing around edges and superior interpretation of texture. Simulations have showed that some details preserved in the EBCOT images are totally lost by the SPIHT algorithm. However, the performance of EBCOT algorithm continues to be competitive with the state-of-the-art compression algorithms, considerably outdoing the SPIHT algorithm especially.
## (s14) WDR (Wavelet Difference Reduction)
(p14.0) A major disadvantage of SPIHT algorithm is that it only implicitly locates the position of significant coefficients. Performing operations which depend on the location of significant transform values, such as region selection (a portion of a compressed image that requires increased resolution) on compressed data is a challenging task. For instance, this can occur with a portion of a low resolution medical image that has been sent at a low bit rate in order to arrive quickly. Such kind of compressed data operations are possible with the Wavelet Difference Reduction algorithm [14]. The term, difference reduction indicates the way in which WDR encodes the locations of the significant wavelet transform values. The main difference between WDR and bit-plane encoding is the significant pass. In Wavelet Difference Reduction, the output from the significance pass holds the signs of significant values along with sequences of bits which briefly describe the exact location of the significant values. Although WDR will not produce higher PSNR values than SPIHT method at low bit rates, as observed from Table 4, it can produce perceptually superior images, especially at high compression rates.
## (s15) ASWDR (Adaptively Scanned Wavelet Difference Reduction)
(p15.0) An improvement in the WDR algorithm gave rise to the ASWDR algorithm [15]. This algorithm was introduced by Walker. The term adaptively scanned says that this algorithm modifies the scanning order used in WDR to achieve improved performance. ASWDR adapts the scanning order so as to forecast locations of new significant values. If a prediction is true, then the output specifying that location will just be the sign of the new significant value and the reduced binary expansion of the number of steps will be empty. So a good prediction scheme will considerably reduce the coding output of WDR. The forecast method used by ASWDR is as follows:

(p15.1) If w(m) is significant for threshold T, then the values of the children of m are expected to be significant for half threshold T/2. For several natural images, this prediction method is found rationally good. Table 4 shows the improved PSNR values for ASWDR compared to WDR. The WDR and ASWDR algorithms allow for ROI while SPIHT does not. In addition, multiresolution detection is facilitated by their superior performance in displaying edge details at low bit rates. Figure 7 shows magnifications of 128:1 and 64:1 compressions of the "Lena" image. The WDR compression does a better job in preserving the shape of Lena's nose and in retaining some of the striping in the band around her hat at 0.0625 bpp. Similar remarks apply to the 0.125 bpp compressions. However, the SPIHT algorithm preserves parts of Lena's eyes better. The ASWDR compressions yet better preserves the shape of Lena's nose and fine details of her hat, and show less distortion along the side of her left cheek and eyes (especially for the 0.125 bpp). 
## (s16) SFQ (Space -Frequency Quantization)
(p16.0) The standard separable 2-D wavelet transform has lately achieved great success in image processing because it provides a sparse representation of smooth images. However, it fails to efficiently capture 1-D discontinuities, like edges or contours. These features, being elongated and characterized by geometrical regularity along different directions, intersect and make many large magnitude wavelet coefficients. Since contours are very important elements in the visual perception of images, it is fundamental to preserve good reconstruction of these directional features to provide a good visual quality of compressed images. This is achieved by spacefrequency quantization (SFQ) compression algorithm using directionlets [16].

(p16.1) SFQ [17] is a wavelet based image coding scheme that fits itself into a new class of image coding algorithms which combines standard scalar quantization of frequency coefficients with the tree structured quantization. The performance of the algorithm is very good and appears to confirm the promised efficiencies of hierarchical representation.

(p16.2) The Space -Frequency Quantization technique exploits both spatial and frequency compaction property of the wavelet transforms through the use of two simple quantization modes. It defines a symbol that indicates a spatial region of high frequency coefficients having zero value. This is to exploit the spatial compaction property. Application of this symbol is told as zero-tree quantization, because it involves setting to zero a tree-structured set of wavelet coefficients. This is done in the first phase which is called Tree Pruning Algorithm. In the next phase, called Predicting the Tree, the relation between a spatial region in image and the tree -structured set of coefficients is exploited. A mechanism for pointing to the location where high frequency coefficients are clustered is called Zero tree quantization. Thus, this quantization mode directly exploits the spatial clustering of high frequency coefficients predicted. For coefficients that are not set to zero by the zero tree quantization, a common uniform scalar quantization, independent of coefficient frequency band, is applied. Nearly optimal coding efficiency is provided by uniform scalar quantization followed by entropy coding.
## (s18) The CREW (Compression with Reversible Embedded Wavelet) Coding
(p18.0) A new form of wavelet transform based still image compression technique developed at the RICOH California Research Centre in Menlo Park, California, is the CREW [19]. It is both lossy and lossless compression scheme, bi-level and continuous-tone, progressive by resolution and pixel depth. The CREW can preserve the source image at encoder and quantize for the target device at decoder or transmission. It is hierarchical and progressive by nature.

(p18.1) Three new technologies combine to make CREW possible: the reversible wavelet transform (nonlinear filters that have exact reconstruction implemented in minimal integer arithmetic), the embedded code stream (a method of implying quantization in the code stream) and a high-speed, high-compression binary entropy coder
## (s19) The Stack-Run (SR)
(p19.0) SR [20] image coding algorithm works by the technique of raster scanning [24] within subbands. Therefore it involves much lower addressing complexity than other compression algorithms such as zero tree coding which requires creation and maintenance of lists of dependencies across different decomposition levels. The SR algorithm is a new approach in which a 4-ary arithmetic coder is used to represent significant coefficient values and the length of zero runs between coefficients. Despite its simplicity and uncomplicatedness and also the fact that these dependencies are not explicitly used, this algorithm is competitive with the finest enhancements of embedded zero tree wavelet coding.
## (s20) The GW (Geometric Wavelet) Coding
(p20.0) Geometric wavelet is a recent advancement in the field of multivariate nonlinear piecewise polynomials approximation proposed by Deckel et.al [21]. It is a new and efficient method best suited for low bit-rate image coding. It combines the binary space partition tree approximation [23] with geometric wavelet (GW) coding so as to efficiently capture curve singularities and thus providing a sparse representation of the image. The GW method successfully competes with advanced wavelet methods such as the EZW, SPIHT, and EBCOT algorithms. A gain of about 0.4 dB over the SPIHT and EBCOT algorithms at the bit-rate 0.0625 bits-per-pixels (bpp) is achieved. The GW technique also outperforms other recent segmentation methods that are based on "sparse geometric representation". Table 5 shows the PSNR values for Lena test image using the GW approach. An improvement over the geometric wavelet (GW) image coding method by using the slope intercept representation of the straight line in the binary space partition scheme is proposed by Chopra and Pal [22]. The performance of this algorithm is compared with other classical wavelet transform-based compression methods such as EZW, the SPIHT and the EBCOT algorithms. The proposed image compression algorithm outperforms the EZW and SPIHT algorithm, as shown in Table 6. The PSNR results for Lena image using the improved GW method for compression ratios of 64, 128 and 256 are given in Table 6. These results are also compared with other algorithms. Table 6. PSNR values for Lena image of size 512x512 using improved GW, compared with other state-of-the art-methods
