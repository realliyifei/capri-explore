# A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress

CorpusID: 49312150 - [https://www.semanticscholar.org/paper/9d4d8509f6da094a7c31e063f307e0e8592db27f](https://www.semanticscholar.org/paper/9d4d8509f6da094a7c31e063f307e0e8592db27f)

Fields: Mathematics, Computer Science

## (s2) Demonstration Substitutes Manual Specification of Reward
(p2.0) Typically, if a designer wants a specific behavior in an agent, she manually formulates the problem as a forward learning or forward control task solvable using solution techniques in RL, optimal control, or predictive control. A key element of this formulation is a specification of the agent's preferences and goals via a reward function. In the traffic merge example, we may hand design a reward function for Car A. For example, +1 reward if taking an action in a state decreases the relative velocity of Car A w.r.t. Car B within a predefined distance from merging junction, thereby allowing for a safe merge. Analogously, a negative reward of -1 if taking an action in a state increases the relative velocity of Car A w.r.t. Car B. However, an accurate specification of the reward function often needs much trial-and-error for properly tuning the variables such as the thresholds for distance and velocity, and the emphasis of +1. As such, the specification of a reward function is often time-intensive and cumbersome.

(p2.1) The need to pre-specify the reward function limits the applicability of RL and optimal control to problems where a lucid reward function can be easily specified. IRL offers a way to broaden the applicability and reduce the manual specification of the model, given that the desired policy or demonstrations of desired behavior are available. While acquiring the complete desired policy is usually infeasible, we have easier access to demonstrations of behaviors, often in the form of recorded data. For example, all state to action mappings for all contingencies for Car A are not typically available, but datasets such as NGSIM contain trajectories of Car A in real-world driving. Thus, IRL forms a key method for learning from demonstration [5].
## (s10) Accurate Inference
(p10.0) Classical IRL takes an expert demonstration of a task consisting of a finite set of trajectories, knowledge of the environment and expert dynamics, and generates the expert's potential reward function; this is illustrated in Fig. 4 Figure 4: Pipeline for a classical IRL process. The learner receives an optimal policy or trajectories as input. The prior domain knowledge (shown here as a pentagon) include completely observable states and fully known transition probabilities.

(p10.1) A critical challenge, first noticed by Ng and Russell [2], is that many reward functions (including highly degenerate ones such as a function with all rewards values zero) explain the observations. This is because the input is usually a finite and small set of trajectories (or a policy) and many reward functions in the set of all reward functions can generate policies that realize the observed demonstration. Thus, IRL suffers from an ambiguity in solution.  In addition, the learner's sensing of expert's state and action could be imprecise due to noisy sensors, as illustrated in Fig. 5. For example, vehicle tracking data in NGSIM mixes up car IDs at multiple points in the data due to imprecise object recognition when cars are close to each other. Inaccurate input may negatively impact the process of learning reward function. This could be corrected to some extent if a model of the observation noise is available, and IRL solution method integrates this model.
## (s13) Growth in Solution Complexity with Problem Size
(p13.0) Methods for IRL are iterative as they involve a constrained search through the space of reward functions. As the number of iterations may vary based on whether the optimization is convex, it is linear, the gradient can be computed quickly, or none of these, we focus on analyzing the complexity of each iteration. Consequently, the computational complexity is the sum of the time complexity of each iteration and its space complexity.

(p13.1) Each iteration's time is dominated by the complexity of solving the ascribed MDP using the reward function learned. While the complexity of solving an MDP is polynomial in the size of its parameters, the parameters such as the state space are impacted by the curse of dimensionality -its size is exponential in the number of components of state vector (dimensions). Furthermore, the state space in domains such as robotics is often continuous and an effective discretization also leads to an exponential blowup in the number of discrete states. Therefore, increasing problem size adversely impacts the run time of each iteration of IRL methods.

(p13.2) Another type of complexity affecting IRL is sample complexity, which refers to the number of trajectories present in the input demonstration. As the problem size increases, the expert must demonstrate more trajectories in order to maintain the required level of coverage in training data. Abbeel and Ng [22] analyze the impact of sample complexity on the convergence of IRL method.
## (s14) Direct Learning of Reward or Policy Matching
(p14.0) Two distinct approaches to IRL present themselves, each with its own set of challenges. First seeks to approximate the reward functionR E by tuning it using input data. The second approach focuses on learning a policy by matching its actions (action values) with the behavior demonstrated, using reward function only as means.

(p14.1) Success of the first approach hinges on selecting an accurate and complete prior knowledge (e.g. set of feature functions) that compose the reward function. Though learning a reward function offers better generalization to the task at hand, it may lead to policies that do not fully reproduce the observed trajectories. The second approach is limited in its generalizability -learned policies may perform poorly in portions of the state space not present in the input data. More importantly, Neu et al. [24] point out that this optimization is convex only when the actions are deterministic and the demonstration spans the complete state space. Consequently, choosing between the two approaches involves prioritizing generalizability over a precise imitation or vice versa.
## (s18) Linear Programming.
(p18.0) One of the earliest methods for IRL is Ng and Russell's [2], which takes in the expert's policy as input. It formulates a linear program to retrieve the reward function that not only produces the given policy as optimal output from the complete MDP, but also maximizes the sum of differences between the value of the optimal action and that of the next-best action for every state. In addition to maximizing this margin, it also prefers reward functions with smaller values as a form of regularization. We may express the reward function as a linear and weighted sum of basis functions,

(p18.1) For spaces which are not discrete or small-sized, the constraints defined for each discrete state must be generalized to the continuous state space, or algorithm can sample a large subset of the state space and restrict the constraints to these sampled states only (as Ng and Russell suggested).

(p18.2) Apprenticeship learning. Two methods [22] under this general label extend the linear programming and perform max margin optimization. Noting that the learner does not typically have access to the expert's policy, max-margin and projection take a demonstration (defined in Def. 2) as input and learns a linear reward function. expected feature count is the discounted sum of feature values under expectation of a policy,

(p18.3) Authors successfully used these expected feature counts or feature expectations as a representation of the value function of Eq. 1: Figure 6: Iterations of the max-margin method (inspired from Figure 1 in [22]) computing the weight vector, w, and the feature expectations, µ φ .μ φ (π E ) is the estimation of the feature counts µ φ (π E ) of the expert. w j is the learned weight vector in j th iteration and π H E j is corresponding optimal policy for intermediate hypothesis.

(p18.4) The methods seeks a reward function that minimizes the margin between the feature expectations of policy computed by learner and the empirically computed feature expectations of expert. Both methods iteratively tune weights w by computing policy for intermediate hypothesis at each step and using it to obtain intermediate feature counts. These counts are compared with the feature countsμ φ (π E ) of expert, as shown in Fig. 6 and are updated each step. Abbeel and Ng [22] point out that the performance of these methods is contingent on matching the feature expectations, which may not yield an accurateR E because feature expectations are based on the policy. An advantage of these methods is that their sample complexity depends on the number of features and not on the complexity of expert's policy or the size of the state space.
## (s19) Entropy Optimization
(p19.0) IRL is essentially an ill-posed problem because multiple reward functions can explain the experts behavior. While the max-margin approach for solution is expected to be valid in some domains, it introduces a bias into the learned reward function in general. Thus, multiple methods take recourse to the maximum entropy principle [26] to obtain a distribution over potential reward functions while avoiding any bias. The distribution that maximizes entropy makes minimal commitments beyond the constraints and is least wrong.

(p19.1) Maximum entropy IRL. This popular method for IRL introduced the principle of maximum entropy as a way of addressing the challenge of accurate inference in IRL. Maximum entropy IRL [13] recovers a distribution over all trajectories, which has the maximum entropy among all such distributions under the constraint that feature expectations of learned policy match that of demonstrated behavior. Mathematically, this problem can be formulated as a convex, nonlinear optimization:
## (s22) Multi-label classification.
(p22.0) Klein et al. [40] viewed IRL as a multi-label classification problem (scirl) in which the demonstration is training, and the state-n-action pairs are data-n-label pairs. They consider the action values Q π E (s, a) (score function of classifier) in terms of feature expectations , Q π (s, a) = K k=1 w k µ φ k (π)(s, a) (see Eq. 3), which makes weights shared parameters between Q-function and reward function. A multi-label classification algorithm computes solution by inferring a weight vector that minimizes the classification error.

(p22.1) An extension of the above method [41] (called csi) estimates the transition probabilities if they are unknown. csi utilizes standard regression on a simulated demonstration data set to estimate the transition model and thereby learn reward function (not necessarily linear this time).
## (s26) Learning from Faulty Input
(p26.0) Perturbations. Sub optimal actions characterize a perturbed demonstration. Melo et al. [45] aim for a formal analysis and characterization of the space of solutions for the cases when some of actions in demonstration are not optimal (a perturbation in the distribution modeling the expert's policy) and when demonstration does not include samples in all states.
## (s28) Lowering Sensitivity to Features
(p28.0) Performance of methods such as projection, max-margin, mmp, mwal, learch, and mlirl are all highly sensitive to the selection of features, a challenge pointed out in Section 3.3. Few of the methods that use reward features attempt to mitigate this challenge. hybrid-IRL that uses policy matching and all maximum entropy based methods tune distributions over the policies or trajectories, which reduces the impact that feature selection has on the performance of IRL [24].

(p28.1) Apart from selecting appropriate features, the size of the feature space influences the error in learned feature expectations for the methods that rely onV π E , e.g. projection,mmp, mwal, maxentirl. If a reward function is linear with the magnitude of its k features bounded from the above, then the high-probability bound on the error scales linearly with k [13]. However, maximum entropy based methods show an improvement in this aspect as O(log k) dependence.
## (s29) Theoretically Guaranteed Accuracy
(p29.0) From a theoretical viewpoint, some methods have better analytically-proved performance guarantees than others. The maximum entropy probability distribution over space of policies (or trajectories) minimizes the worst-case expected loss [52]. Consequently, maxentirl learns a behavior which is neither much better nor much worse than expert's [53]. However, the worst-case analysis may not represent the performance on real applications because the performance of optimization-based learning methods can be improved by exploiting favorable properties of the application domain. Classification based approaches such as csi and scirl admit a theoretical guarantee for the quality ofR E , in terms of optimality of learned behaviorπ E , given that both classification and regression errors are small. Nevertheless, these methods may not reduce the loss as much as mwal as the latter is the only method, in our knowledge, which has no probabilistic lower bound on the value-loss incurred [25].
## (s30) Analysis and Improvement of Complexity
(p30.0) Active birl offers a benefit over traditional birl by exhibiting reduced sample complexity (see Section 3.4). This is because it seeks to ascertain the most informative states where a demonstration is needed, and queries for it. Consequently, less demonstrations are often needed and the method becomes more targeted. Of course, this efficiency exists at the computational expense of interacting with the expert. In a different direction, Francisco et al. [34] exploiting an equivalence among the states to reduce the sample complexity, as shown in Fig. 5.2. Likewise, reirl uses fewer samples (input trajectories) as compared to alternative methods including a model-free variant of mmp [28].

(p30.1) While emphasis on reducing time complexity and space complexity is generally lacking among IRL techniques, a small subset does seek to reduce the time complexity. An analysis of birl shows that computing the policy π E for the mean of posterior distribution over solutions is computationally more efficient than the direct minimization of expected value-loss over the posterior [32]. Specifically, the Markov chain approximating the Bayesian posterior, with a uniform prior, converges in polynomial time. Next, mwal requires O(ln k) (k is number of features) iterations for convergence, which is lower than O(k ln k) for the projection method. birl using bisimulation (Section 4.3) has low computational cost because it does not need to solve the MDP repeatedly and the computation of bisimulation metric over space S occurs once regardless of the number of applications. Although an iteration of firl is slower than mmp and projection due to the computationally expensive step of regression, former converges in a fewer number of iterations as compared to latter.
## (s38) POMDP-IRL.
(p38.0) An expert with noisy sensors that senses its state imperfectly can be modeled as a partially observable MDP (POMDP) [67]. The expert's uncertainty about its current physical state is modeled as a belief (distribution) over its state space. The expert's policy is then a mapping from its beliefs to optimal actions. Choi et al. [21] propose making either this policy available to the learner or the prior belief along with the sequence of expert's observations and actions (that can be used to reconstruct the expert's sequence of beliefs). The POMDP policy is represented as a finite-state machine whose nodes give the actions to perform on receiving observations that form the edge labels. The learner conducts a search through space of policies by gradually improving on the previous policy until it explains the observed behavior. Figure 12 illustrates this approach.  [21], consider a POMDP with two actions and two observations. π E (solid lines) is a fsm with nodes {n 1 , n 2 } associated to actions and edges as observations {z 1 , z 2 }. The one-step deviating policies (dashed lines)

(p38.1) are policies which are slightly modified from π E . Each π i visits n i instead of n i and then becomes same as π E . The comparison ofV π E with {V (π i )} 2 i=1 characterizes the set of potential solutions. Since such policies are suboptimal yet similar to expert's, to reduces computations, they are preferable for comparison instead of comparingV π E with all possible policies.
## (s45) Other Extensions
(p45.0) Ranchod et al. [80] presented a Bayesian method to segment a set of unstructured demonstration trajectories, identifying the reward functions (as reusable skills in a task) that maximize the likelihood of demonstration. The method is nonparamteric as the number of functions is unknown.

(p45.1) Munzer et al. [81] extend the classification-regression steps in csi to include relational learning in order to benefit from the strong generalization and transfer properties that are associated with relational-learning representations. The process shapes the reward function for the score function as computed by the classification (see Fig. 6.5).
## (s48) Formalization of Reliable Evaluation Metric
(p48.0) As mentioned in Section 3.1, the evaluation metrics like reward-loss are not reliable. In the pursuit of different challenges in IRL problem, some researchers formalized the norm of value-loss as a metric for error in IRL. Choi et al. [21] name the normed loss as Inverse Learning Error or ILE. Givenπ E and π E in the space of MDP policies, ILE is calculated as
