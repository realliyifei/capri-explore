# a section of the journal Frontiers in Microbiology Overview of Virus Metagenomic Classification Methods and Their Biological Applications

CorpusID: 5034390 - [https://www.semanticscholar.org/paper/38b6634cd639fcb94166d9492b84bc9fef380f7e](https://www.semanticscholar.org/paper/38b6634cd639fcb94166d9492b84bc9fef380f7e)

Fields: Computer Science, Medicine, Biology

## (s0) INTRODUCTION
(p0.0) Unbiased sequencing of nucleic acids from environmental samples has great potential for the discovery and identification of diverse microorganisms (Tang and Chiu, 2010;Chiu, 2013;Culligan et al., 2014;Pallen, 2014). We know this technique as metagenomics, or random, agnostic or shotgun high-throughput sequencing. In theory, metagenomics techniques enable the identification and genomic characterisation of all microorganisms present in a sample with a generic lab procedure (Wooley and Ye, 2009). The approach has gained popularity with the introduction of next-generation sequencing (NGS) methods that provide more data in less time at a lower cost than previous sequencing techniques. While initially mainly applied to the analysis of the bacterial diversity, modifications in sample preparation protocols allowed characterisation of viral genomes as well. The fields of virus discovery and biodiversity characterisation have seized the opportunity to expand their knowledge (Cardenas and Tiedje, 2008;Tang and Chiu, 2010;Chiu, 2013;Pallen, 2014).
## (s7) Database Searching
(p7.0) In the search step, sequences (either reads or contigs) are matched to a reference database. Twenty-six of the workflows we found search with the well-known BLAST algorithms BLASTn or BLASTx (Altschul et al., 1990; Table 2). Other oftenused programs are Bowtie (Langmead, 2010;Langmead and Salzberg, 2012), BWA (Li and Durbin, 2009), and Diamond (Buchfink et al., 2015). These programs rely on alignments to a reference database and report matched sequences with alignment scores. Bowtie and BWA, which are also popular programs for the filtering step, align nucleotide sequences exclusively.

(p7.1) Diamond aligns amino acid sequences and BLAST can do either nucleotides or amino acids. As analysis time can be quite long for large datasets, algorithms have been developed to reduce this time by using alternatives to classical alignment. One approach is to match k-mers with a reference, as used in FACS (Stranneheim et al., 2010), LMAT (Ames et al., 2013), Kraken (Wood and Salzberg, 2014), Taxonomer , and MetLab (Norling et al., 2016). Exact k-mer matching is generally faster than alignment, but requires a lot of computer memory. Another approach is to use probabilistic models of multiple sequence alignments, or profile hidden Markov models (HMMs). For HMM methods, protein domains are used, which allows the detection of more remote homology between query and reference. A popular HMM search program is HMMER (Mistry et al., 2013). ClassyFlu (Van der Auwera et al., 2014) and vFam (Skewes- Cox et al., 2014) rely exclusively on HMM searches, while VMGAP (Lorenzi et al., 2011), Metavir (Roux et al., 2011), VirSorter (Roux et al., 2015), and MetLab can also use HMMER. All of these search methods are examples of similarity search-homology or alignment-based methods. The other search method is composition search, in which oligonucleotide frequencies or k-mer counts are matched to references. Composition search requires the program to be "trained" on reference data and it is not used much in viral genomics. Only two workflows discussed here use composition search: NBC (Rosen et al., 2011) and Metavir 2 (Roux et al., 2014), while Metavir 2 only uses it complementary to similarity search (Data Sheet 1).

(p7.2) All search methods rely on reference databases, such as NCBI GenBank (https://www.ncbi.nlm.nih.gov/genbank/), RefSeq (https://www.ncbi.nlm.nih.gov/refseq/), or BLAST nucleotide (nt) and non-redundant protein (nr) databases (ftp://ftp.ncbi. nlm.nih.gov/blast/db/). Thirty-four workflows use GenBank for their references, most of which select only reference sequences from organisms of interest (Table 2). GenBank has the benefits of being a large, frequently updated database with many different organisms and annotation depends largely on the data providers.

(p7.3) Other tools make use of virus-specific databases such as GIB-V (Hirahata et al., 2007) or ViPR (Pickett et al., 2012), which have the advantage of better annotation and curation at the expense of the number of included sequences. Also, protein databases like Pfam (Sonnhammer et al., 1998) and UniProt (UniProt, 2015) are used, which provide a broad range of sequences. Search at the protein level may allow for the detection of more remote homology, which may improve detection of divergent viruses, but non-translated genomic regions are left unused. A last group of workflows requires the user to provide a reference database file. This enables customization of the workflow to the user's research question and requires more effort.
## (s8) Post-processing
(p8.0) Classifications of the sequencing reads can be made by setting the parameters of the search algorithm beforehand to return a single annotation per sequence (cut-offs). Another option is to return multiple hits and then determine the relationship between the query sequence and a cluster of similar reference sequences. This process of finding the most likely or best supported taxonomic assignment among a set of references is called post-processing. Post-processing uses phylogenetic or other computational methods such as the lowest common ancestor (LCA) algorithm, as introduced by MEGAN (Huson et al., 2007). Six workflows use phylogeny to place sequences in a phylogenetic tree with homologous reference sequences and thereby classify them. This is especially useful for outbreak tracing to elucidate relationships between samples. Twelve workflows use other computational methods such as the LCA taxonomy-based algorithm to make more confident but less specific classifications (Data Sheet 1). In total, 18 workflows include post-processing ( Figure 1B).
## (s13) DISCUSSION
(p13.0) Based on available literature, 49 available virus metagenomics classification workflows were evaluated for their analysis methods and performance and guidelines are provided to select the proper workflow for particular purposes (Figures 5, 6). Only workflows that have been tested with viral data were included, thus leaving out a number of metagenomics workflows that had been tested only on bacterial data, which may be applicable to virus classification as well. Also note that our inclusion criteria leave out most phylogenetic analysis tools, which start from contigs or classifications. The variety in methods is striking. Although each workflow is designed to provide taxonomic classification, the strategies employed to achieve this differ from simple one-step tools to analyses with five or more steps and creative combinations of algorithms. Clearly, the field has not yet adopted a standard method to facilitate comparison of classification results. Usability varied from a few remarkably user-friendly workflows with easy access online to many command-line programs, which are generally more difficult to use. Comparison of the results of the validation experiments is precarious. Every test is different and if the reader has different study goals than the writers, assessing classification performance is complex.

(p13.1) Due to the variable benchmark tests with different workflows, the data we looked at is inherently limited and heterogeneous. This has left confounding factors in the data, such as test data, references used, algorithms and computing platforms. These factors are the result of the intended use of the workflow, e.g., Clinical PathoScope was developed for clinical use and was not intended or validated for biodiversity studies. Also, benchmarks usually only take one type of data to simulate a particular use case. Therefore, not all benchmark scores are directly comparable and it is impossible to significantly determine correlations and draw firm conclusions.

(p13.2) We do highlight some general findings. For instance, when high sensitivity is required filtering steps should be minimized, as these might accidentally remove viral reads. Furthermore, the choice of search algorithms has an impact on sensitivity. High sensitivity may be required in characterization of environmental biodiversity (Tangherlini et al., 2016) and virus discovery. Additionally, for identification of novel variant viruses and virus discovery de novo assembly of genomes is beneficial. Discoveries typically are confirmed by secondary methods, thus reducing the impact in case of lower specificity. For example, RIEMS showed high sensitivity and applies de novo assembly. MetLab FIGURE 2 | Different benchmark scores of virus classification workflows. Twenty-seven different workflows (Left) have been subjected to benchmarks, by the developers (Top) or by independent groups (Bottom), measuring sensitivity (Left column), specificity (Middle column) and precision (Right column) in different numbers of tests. Numbers between brackets (n = a, b, c) indicate number of sensitivity, specificity, and precision tests, respectively.

(p13.3) combines de novo assembly with Kraken, which also displayed high sensitivity. When higher specificity is required, in medical settings for example, pre-processing and search methods with the appropriate references are recommended. RIEMS and MetLab are also examples of high-specificity workflows including preprocessing. Studies that require high precision benefit from pre-processing, filtering and assembly. High-precision methods are essential in variant calling analyses for the characterization of viral quasispecies diversity (Posada-Cespedes et al., 2016), and in medical settings for preventing wrong diagnoses. RINS performs pre-processing, filtering and assembly and scored high in precision tests, while Kraken also scored well in precision and with MetLab it can be combined with filtering and assembly as needed.

(p13.4) Clinicians and public health policymakers would be served by taxonomic output accompanied by reliability scores, as is possible with HMM-based search methods and phylogeny with bootstrapping, for example. Reliability scores could also be based on similarity to known pathogens and contig coverage. However, classification to a higher taxonomic rank (e.g., order) is more generally reliable, but less informative than a classification at a lower rank (e.g., species) (Randle-Boggis et al., 2016). Therefore, the use of reliability scores and the associated trade-offs need to be properly addressed per application.

(p13.5) Besides, medical applications may be better served by a functional rather than a taxonomic annotation. For example, a clinician would probably find more use in a report FIGURE 3 | Correlations between performance scores and analysis steps. Sensitivity, specificity and precision scores (in columns) for workflows that incorporated different analysis steps (in rows). Numbers at the bottom indicate number of benchmarks performed.

(p13.6) of known pathogenicity markers than a report of species composition. Bacterial metagenomics analyses often include this, but it is hardly applied to virus metagenomics. Although FIGURE 4 | Correlation between performance and search algorithm and runtime. Sensitivity, specificity and precision scores (in columns) for workflows that incorporated different search algorithms, using either nucleotide sequences, amino acid sequences or both, and workflows with different runtimes (rows). Numbers at the bottom indicate number of benchmarks performed.
