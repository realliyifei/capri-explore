# A Review of Environmental Context Detection for Navigation Based on Multiple Sensors

CorpusID: 221237778 - [https://www.semanticscholar.org/paper/715727543701132420e565b29891cc87b272e99e](https://www.semanticscholar.org/paper/715727543701132420e565b29891cc87b272e99e)

Fields: Environmental Science, Engineering, Computer Science, Medicine

## (s0) Introduction
(p0.0) In the past decade, much work has been done to make navigation more precise and more reliable: sensor fusion, improvement of signal quality, receiver hardware enhancement (antennas arrays for instance), mitigation of errors, robust filtering, and so on. Fusing data from different sensors is a good way to make a navigation system more accurate, but it could also degrade the navigation accuracy when some of those data are erroneous. For instance, fusing Global Navigation Satellite System (GNSS) data with vision-based data in an indoor situation or in a harsh environment would create positioning errors since GNSS data might be unreliable in such a context. That is why context information is interesting and useful for navigation purposes, as mentioned in [1,2]. There are two very different concepts of "context" for navigation: behavioural and environmental contexts. The behavioural context represents the activity of either a vehicle in the case of robotic applications such as Unmanned Ground/Aerial Vehicles (UGVs or UAVs) (for example, accelerating/slowing down) or a user in the case of a smartphone (climbing stairs, running, taking a lift, etc.). The environmental context defines the surroundings, like indoor/outdoor, close to a building, under a tree, and so on. Reference [1] is an interesting introduction to environmental contexts since it gives an exhausting list including even the space or submarine environment.
## (s6) C/N0
(p6.0) The Carrier to Noise ratio (C/N0) is probably the most well-known indicator of the GNSS signal. It is a signal-to-noise ratio that is not quantified in dB, but in dB.Hz. It gives information on the signal quality and on how badly (or not) the signal has been attenuated. According to [34], a standard C/N0 is around 45 dB.Hz in an open sky area. They also showed that different ranges of C/N0 values can be representative of a specific context (Table 2). When looking at this table, it seems very easy to distinguish indoor environments from outdoor ones: a simple threshold seems more than enough. However, the reality is much more complex. Firstly, C/N0 is not that stable and reliable. This was showcased in [1], where they acquired 100 s of C/N0 values (at a frequency of 1Hz) in different contexts such as indoors, outdoors in urban/residential areas, and open field. For each of those recordings, the authors computed histograms of C/N0. Those histograms showed that C/N0 values vary greatly during the period of acquisition (due to temporal losses of satellite visibility for a short time period). As a conclusion, we can fairly say that it is not possible to implement a simple threshold, as given in Table 2, to detect a context. However, the authors of [1] made two observations based on those data:
## (s8) Satellite Elevations
(p8.0) Satellite elevation is not an indicator that gives directly any information on the context. However, it can be used as supplementary information to help the context detection, like the pseudo-range. Indeed, the lower the satellite is, the higher the risk is to have its signal occluded by a building. Elevation can be used for detecting multipaths or for NLOS/LOS classification. To do so, it is mandatory to have a height map of the environment to compare buildings height with satellites elevations (azimuths are also used in this process). Formerly, such height maps needed to be created by various methods [50], but currently, 3D city models are widely available [51][52][53][54]. All those techniques do work, they require a 3D environment model and an accurate estimation of the receiver position, which are far from being available everywhere. It was also stated in [55] that those techniques require much computational resource, which cannot be supported by every navigation setup.
## (s10) Combination of Multiple Indicators
(p10.0) For now, except the ones in [19,20], all the GNSS-based context detection solutions presented so far were based on a single indicator. This part presents techniques that combine several indicators for context detection. Most of them are based on Machine Learning (ML) approaches.
## (s14) Scene Analysis/Classification
(p14.0) Now that we have seen the importance of what is above the vehicle (sky extraction and NLOS detection), we will take a look at what it is facing. Most of the autonomous vehicles have a frontal camera for navigation and/or mission purposes. The art of recognizing what contains a scene is called scene analysis. This is interesting for context detection, as it could help to distinguish indoors or outdoors for instance. Such decision making is referred to as scene classification. Filtering indoor scenes from outdoor ones seems like a simplification of scene classification, but in fact, it is not so simple. Indeed, the type of objects can vary greatly in both situations (plants can be found inside for instance), and external parameters such as illumination or weather can greatly influence the rendering of a picture. Different solutions exist, but there are two main families: the methods based on an analysis of the picture as a whole, which are called holistic methods, and approaches based on local descriptors, which are indicators computed on a sub-part of the image. Most of the time, the sub-parts are created by a segmentation algorithm. There are also a few methods that cannot be applied in our case. One such method is based on the labelling of the different objects of a scene to then classify it depending on the found objects (for example, if a desk, a bed, and a chair are detected in the picture, it can be classified as a bedroom). Such solutions are useful when it is necessary to precisely classify scenes to perform a mission task at a specific place. However, in our navigation application, where we only need to distinguish indoors from outdoors, the level is too high [82]. Lastly, methods based on the metadata of the picture also exist [83,84]. Indeed, with the exif (exchangeable image file) format, it is possible to have access to various parameters, which are referred as metadata. There is a wide range of possible parameters, like GPS position, time, camera manufacturer, or even information, in the picture. Those last parameters are the most interesting for image classification. Here is a non-exhaustive list of them: aperture, autofocus distance, exposure time, f-number, use of the flash, and so on. In our case, we do not have access to all the required data (for instance, it is not possible to use flash since there is no flash on a UGV setup), and therefore, we cannot use this technique.
## (s16) Aerial Photography
(p16.0) Little work has been recently done on aerial photography since it is an alternative to satellite imagery with a much higher resolution and pixel intensities. Aerial photography actually works in a very similar manner to the satellite imagery solutions except that satellite imagery is very often hyperspectral, which is rarely the case for aerial photography. This creates major challenges to achieve a high resolution segmentation for Aerial Photography (AP) since only RGB channels are available and also some objects may have a similar aspect, but are very different semantically. For instance, tree regions are very close to grass or bush regions from a textural and colour point of view, but semantically totally different. The opposite is also true, as for example cars, which have different shapes and colours, but are semantically the same.

(p16.1) In the first example of [15], they tried to detect urban and wild land contexts with two different approaches: object-based and pixel-based approaches. The pixel-based algorithm uses ISODATA, an unsupervised classifier that uses the minimum spectral distance. Then, a 3 × 3 majority filter is used to find a class. As for the object-based method, the first step is to segment the image. To do so, the authors used the software called Definiens eCognition, which has a bottom-up algorithm based on spectral, textural, and user parameters. The classification is performed using a multi-scale system. The large segmented areas are classified by applying a fuzzy algorithm, while the smaller objects are classified by a KNN (K-Nearest-Neighbours) algorithm. At the end, the pixel-based method had a 62% correct classification rate, while the object-based one had 80%. The authors concluded that an object-based algorithm is more accurate than a pixel-based one for remote sensing.
## (s19) Context Detection Based on Other Sensors
(p19.0) The two previous sections covered GNSS-and vision-based indicators that can be used for context detection. We are still missing indicators on the IMU, our last sensor available in the classical navigation setup. Actually, the IMU does not give interesting information on the context, as it functions in the same manner regardless of the environment. Using the IMU measurement enables detecting pedestrian steps [117], road vibrations (to check if the car is moving) [118], or how straight a vehicle is going thanks to its gyro [119]. Measurements from accelerometers and gyroscopes are also used in more complex systems to identify the behavioural context and the activity of smartphone users (for example, running, climbing stairs, inside a lift [120]), to create an activity map for example [121]. Such information will not directly help our environmental context detection. Nonetheless, using IMU data is still the best and simplest way to know if the vehicle is in motion or not. This information could be interesting for managing context detection because if the drone is idle, we have no reason to look for a new context (it is however still important to follow the evolution of the satellite constellation).

(p19.1) It has been shown in past years that a depth map could help the indoor/outdoor classification [30]. Generally, the depth map should be computed from a stereo vision setup, which is far from being implemented on every ground vehicle. In the previous article, the depth map has been estimated (thanks to random Markov fields) due to the lack of a stereoscopic dataset. The depth map can also be extracted thanks to LiDAR or RGB-D cameras. Papers based on those sensors are presented in the following paragraphs.

(p19.2) Considering LiDAR (LIght Detection And Ranging) or an RGB-D camera, it has been shown that such a sensor could be used to perform indoor/outdoor classification. The LiDAR sensor is often used on drones and other autonomous vehicles to create Digital Elevation Models (DEMs). It is usually used in addition to the standard visible camera since features cannot be found in non-textured environments. For example, in [122], they tried to detect windows using camera-LiDAR fusion. Such detection results could give us information on how far the vehicle is from windows or potentially from the exit/entrance of the buildings. Börcs et al. [123] trained a CNN model to classify buildings, vehicles, pedestrians, ground, and road clutter from a 3D point cloud, which could help us to detect outdoor scenes. Lim and Suter [18] combined features from LiDAR and a camera (heights, colours, spin image, estimated normals) to label super-voxels of a 3D point cloud as five classes (trees, trunk, building, pathway, grass) using a multi-scale conditional random field. This method could be useful for the context mapping task, but for online navigation use, it has a drawback compared to the AP method since it cannot be pre-loaded. However, it is important to note that LiDAR has not been used as a stand alone sensor for context detection so far.

(p19.3) RGB-D cameras, which provide colour and depth images, are also used for context detection, but only in indoor applications. Most of the time, they are used in scenes where there is a wide range of possible objects and where pixel data are not sufficient to label them. Adding another type of information with the depth map helps to label objects. Such a method is referred to as scene labelling (see [124]). Once objects have been labelled, a classifier is used to find the context depending on the objects in the scene. For instance, Gupta et al. [125] classified rooms thanks to objects detection by using an SVM classifier on the histogram of oriented gradients of the depth map. RGB-D cameras are not very useful in our case of vehicle navigation, since this technology is not reliable in outdoor environments. If any depth map data are needed for context detection, then it would be preferable to use LiDAR.
## (s20) Summary of the Different Solutions and Perspectives
(p20.0) This paper overviewed existing environmental context detection solutions mainly based on GNSS and vision sensors, from a navigation point of view. We firstly focused on the GNSS-based indicators and saw that C/N0 and the K-Rician factor are good indicators to detect indoor/outdoor contexts. The negative part is that a certain amount of integration time is needed to compute those values. It is also important to note that those indicators are highly sensitive and not robust to environmental variations (loss of satellites, sudden attenuation, etc.). We then took a look at the pseudo-range residual and its second derivative, which suffer from the same problems as C/N0: sensitive to noise and a need much integration time. Nonetheless, these values can be useful to distinguish LOS from NLOS satellites. Knowing the number of NLOS satellites would give information on the context: the more NLOS satellites there are, the higher the chance of being indoors (or in urban canyons) will be. Another indicator to confirm the LOS/NLOS classification (which is also important information for shadow matching) is the satellite elevation. Indeed, if the satellite is low in the sky, there is a higher chance for the signal to be blocked (NLOS). We then introduced the ACF indicators. This function is much more complicated to extract since it is only accessible inside the GNSS receiver and not as a simple output like the previous indicators. Different solutions exist, but most of them only detect multipaths without differentiating this from NLOS, which is not sufficient for context detection purposes. Finally, we presented different solutions based on multivariate GNSS data. Most of those solutions apply machine learning techniques and seem to suffer from the difficulty in creating a labelled database. Furthermore, often, the classification is just among indoors, outdoors, and intermediate, which is not fine enough for navigation adaptation purposes.

(p20.1) In the second part of this paper, we showed the different methods to extract context using vision information. The first method is to extract the sky segment (on an image of a wide angle camera) and to project the satellites' positions on it in order to classify LOS/NLOS satellites. This provides us with two major pieces of information: the number of NLOS satellites and the presence of sky. Such information can give an idea about the context (indoors, urban, open sky, etc.). The sky extraction enables the GNSS navigation to improve its positioning accuracy by detecting and excluding the NLOS satellites. However, this step must be done with caution since it can lead to too few satellites available, which makes the GNSS position estimation impossible. We have seen multiple indicators that can be used in addition to help the satellite filtering like C/N0 or pseudo-range. The second vision-based solution exposed is to use a frontal camera and to analyse the scene in order to classify it and detect context. Different methods exist from local descriptors to the holistic method. Both techniques will at the end need a classifier. The third approach is to use satellite imagery, or aerial photography. We quickly saw that Satellite Imagery (SI) is not precise enough in terms of resolution for our vehicle navigation purposes, and thus, AP should be used. Most of the segmentation algorithms for AP are similar to those for SI. Those AP-based techniques are interesting since they can help us detect basically every type of context, including trees and water, that the GNSS indicators cannot (the only way to detect trees or water with GNSS is to use reflectometry, which is not possible in our case, since the receiver is near the ground level and in motion). It is also important to mention that object-based image analysis is more powerful than pixel-based (which creates a salt and pepper effect since there is no region segmentation) in the case of remote sensing, as proven in [137].

(p20.2) A summary table of all the reviewed methods of context detection is given in Table 5. This table is only based on the literature and not on any tests, which can explain our doubts about certain indicators (denoted as "?"). 
