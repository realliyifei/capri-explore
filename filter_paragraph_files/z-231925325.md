# Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition

CorpusID: 231925325 - [https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23](https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23)

Fields: Engineering, Computer Science

## (s3) A. RNN-based encoder-decoder architecture
(p3.0) Sequence-to-sequence RNN-based ASR models are based on an encoder-decoder architecture. The encoder is an RNN which takes input sequence and converts it into hidden states. The decoder is also an RNN which takes the last encoder hidden state as input and process it to decoder hidden states which in turn used for output predictions. This traditional encoder-decoder structure has some limitations:

(p3.1) • The encoder hidden state, h T (last one) which is fed to the decoder has the entire input sequence information compressed into it. For longer input sequences, it may cause information loss as h T may not capture long-range dependencies effectively. • There is no alignment between the input sequence frames and the output. For predicting each output symbol, instead The above issues can be overcome by letting the decoder to access all the encoder hidden states (instead of the last one) and at each decoder time step, relevant input frames are given higher priorities than others. It is achieved by incorporating attention mechanism to the encoder-decoder model. As a part of sequence-to-sequence modelling, attention mechanism was introduced in [71] for machine translation. Inspired by the effectiveness in [71], the attention mechanism was introduced to ASR in [11]. An earlier version of this work has been presented in [10].

(p3.2) The model in [11] is named as attention-based recurrent sequence generator (ASRG). The graphical representation of this model is shown in Figure 1. The encoder of ASRG processes the input audio frames to encoder hidden states which are then used to predict output phonemes. By focusing on the relevant encoder hidden states, at i th decoder time step, prediction of phoneme y i is given by (1)

(p3.3) where c i is the context given by (2) generated by attention mechanism at the i th decoder time step. s i given by (3) is the decoder hidden state at i th time step. It is the output of a recurrent function like LSTM or GRU. Spell(., .) is a feedforward neural network with softmax output activation.

(p3.4) where h j is the encoder hidden state at the j th encoder time step. α i,j given by (4) is the attention probability belonging to the j th encoder hidden state for the output prediction at i th decoder time step. In other words, α i,j captures the importance of the j th input speech frame (or encoder hidden state) for decoding the i th output word (or phoneme or character). α i values are also considered as the alignment of encoder hidden states (h j∈[1,··· ,L] ) to predict an output at i th decoder time step. Therefore, c i is the sum of the products (SOP) of attention probabilities and the hidden states belonging to all encoder time steps at the i th decoder time step and it provides a context to the decoder to decode (or predict) the corresponding output.

(p3.5) where e i,j is the matching score between the i th decoder hidden state and the j th encoder hidden state. It is computed using a hybrid attention mechanism given by (5) in a general form and by (6) in a parametric form.

(p3.6) where w and b are vectors and W , V and U are matrices. These are all trainable parameters. f i = F * α i−1 is a set of vectors which are extracted for every encoder state h j of the previous alignment α i−1 which is convolved with a trainable matrix F . The tanh function produces a vector. However, e i,j is a single score. Therefore, a dot product of tanh outcome and w is performed. The mechanism in (5) is referred to as hybrid attention as it considers both location (α) and content (h) information. By dropping either α i−1 or h j , the Attend mechanism is called content-based or location-based attention.

(p3.7) B. Transformer-based encoder-decoder architeture RNN-based encoder-decoder architecture is sequential in nature. To capture the dependencies, hidden states are generated sequentially and at each time step, the generated hidden state is the output of a function of previous hidden state. This sequential process is time consuming. Also, during the training, error back propagates through time and this process is again time consuming.

(p3.8) To overcome the limitations of RNN, Transformer network is proposed completely based on attention mechanism. In Transformer network, no recurrent connection is used. Instead, the input farmes are processed parallelly at the same time, and during training, no back propagation through time is applicable.

(p3.9) Transormer network was introduced in [20] for machine translation and later it is successfully applied to ASR tasks. In this section, the idea of Transformer is given as described in [20]. The graphical representation of Transformer is shown in Figure 2.
## (s5) A. Global Attention with RNN
(p5.0) Global attention is computed over the entire encoder hidden states at every decoder time step. The mechanism illustrated in Section III-A as per [11] is an example of global attention. Since [11], a lot of progress has been made by many researchers.

(p5.1) The authors of [24] presented a global attention mechanism in their Listen, Attend and Spell (LAS) model. Here, Spell function takes inputs as current decoder state s i and the context c i . y i = Spell(s i , c i ). s i is computed using a recurrent function which takes inputs as previous decoder state (s i−1 ), previous output prediction (y i−1 ) and previous context (c i−1 ).
## (s6) B. Local attention with RNN
(p6.0) In global attention model, each encoder hidden states are attended at each decoder time step. This results in a quadratic computation complexity. In addition, the prediction of a particular decoder output mostly depends on a small number of encoder hidden states. Therefore, it is not necessary to attend the entire set of encoder hidden states at each decoder time step. The application of local attention fulfils the requirement of reducing the computation complexity by focusing on relevant encoder hidden states. Local attention mechanism is mostly popular in streaming speech recognition but, it has been applied to offline speech recognition as well. The core idea of local attention is to attend a set of encoder hidden states within a window or range at each decoder time step instead of attending the entire set of encoder hidden states. Local attention was introduced in [74] for machine translation and thereafter, it has been applied to ASR as well.
## (s8) D. RNN-free Transformer-based models
(p8.0) Self-attention is a mechanism to capture the dependencies within a sequence. It allows to compute the similarity between different frames in the same sequence. In other words, selfattention finds to what extent different positions of a sequence relate to each other. Transformer network [20] is entirely built using self-attention for seq2seq processing and has been successfully used in ASR as well.

(p8.1) Transformer was introduced to ASR domain in [26] by proposing Speech-transformer. Instead of capturing only temporal dependencies, the authors of [26] have also captured spectral dependencies by computing attention along time and frequency axis of input spectrogram features. Hence, this attention mechanism is named as "2D attention". The set of (q, k, v) for time-domain attention is computed using (8).

(p8.2) Here, the input embedding (X) is the convolutional features of spectrogram. For frequency-domain attention, the set of (q, k, v) are the transpose of same parameters in the timedomain. At each block of multi-head attention, the timedomain and frequency-domain attentions are computed parallelly and after that they are concatenated using (9). In this case attention heads belong to both time and frequency domains. Speech transformer was built to output word predictions and later on it is explored for different modelling units like phonemes, syllables, characters in [46], [47] and for largescale speech recognition in [48].

(p8.3) A very deep Transformer model for ASR is proposed in [49]. The authors have claimed that depth is an important factor for obtaining effective ASR performance using Transformer network. Therefore, instead of using the original version of six stacked layers for both encoder and decoder, more layers (deep configuration) are used in the structure. Specifically, the authors have shown 36 − 12 layers for the encoder-decoder is the most effective configuration. To facilitate the training of this deep network, around each sub-layer, a stochastic residual connection is employed before the layernormalisation. Another deep Transformer model is proposed in [50] where it has been shown that the ASR performance is continually increased with the increase of layers up to 42 and the attention heads up to 16. The effect on performance beyond 42 layers and 16 attention-heads is not provided, probably due to the increased computation complexity. The authors have also experimentally shown that sinusoidal positional encoding [20] is not required for deep Transformer model. To increase the model capacity efficiently, the deep Transformer proposed in [51] replaced the single-layer feed-forward network in each Transformer sub-layer by a deep neural network with residual connections.
