# Journal of Soft Computing and Data Mining 53 Facial Expression Recognition Based on Deep Learning Convolution Neural Network: A Review

CorpusID: 234833736 - [https://www.semanticscholar.org/paper/5af785e46b8a8c59f9fc5ea29a534a79086aab7a](https://www.semanticscholar.org/paper/5af785e46b8a8c59f9fc5ea29a534a79086aab7a)

Fields: Computer Science

## (s2) Facial Expression Recognition Survey
(p2.0) This literature explores the latest research on the recognition of facial expression. It provides insight into the face, identification methods, model architecture for extraction and classification features, and precision obtained by the following researchers in FER's field.

(p2.1) Kaviya et al. [24] used two separate datasets, namely FER-2013 and customized datasets. The RGB image is preprocessed by turning it into a gray image for emotional recognition. The faces are then identified in real-time photographs or static images of Haar. If the face is detected, the face characteristics can be resized and processed. CNN is used to train the facial traits acquired to identify them according to the five emotions. Community emotion is measured using a weighted average feeling. Finally, the expected group emotion is moved to the speech synthesizer to get audio output. The result showed that the proposed CNN would gain facial expression and the accuracy of testing of the model is 65% for FER-2013 and 60% for custom datasets.

(p2.2) Nie [51] learned a framework focused on deep learning of facial expression, which uses the OpenCV computer vision class library to complete face recognition; Opencv, based on the AdaBoost algorithm, is used for facial recognition. The neural network construction model's CNN architecture adopts Keras as the open-source artificial neural network library written by python. The model uses the Stochastic algorithm of Descent Gradient (SGD). He used the FER2013 database as CNN neural network training data.

(p2.3) Hussein et al. [36] recommended a CNN model to understand face emotions with three continuum emotions. This model uses residual blocks and depth-separable convolutions inspired by Xception to minimize the sum of parameters to 33k. They use a convolutional neural FER network for emotional stability identification. CNN uses convolution operations to learn extract features from the input images, which reduces the need to extract features from images manually. The proposed model offers 81 percent total precision for invisible results. It senses negative and positive emotions, respectively, with a precision of 87% and 85%. However, the accuracy of neutral emotion detection is just 51%.

(p2.4) Ravi et al. [58] made a fair contrast between two of the most widely used FER techniques and shed some light on their accuracy. LBP and CNN are the approaches being used here. The LBP is intended only to extract features to classify the extracted features from LBP using the SVM classifier. The results indicate that CNN is better than LBP for its integrated classifier (softmax).

(p2.5) Ganapathy et al. [22] attempted to identify emotional conditions with Electrodermal Activation (EDA) signals and the CNN's learned features. The EDA signals are received from the public DEAP database and are standardized and used to decompose into tonic and phasic sections by the cvxEDA process. In a short time, the phasic part will be subject to the change in the Fourier. 38-time, frequency, and time-frequency functions are extracted from the phasic signal. CNN uses these extracted properties to learn robust and powerful features. Five algorithms are being used to classify machine learning, including LDA, Multi-Layer Perceptron (MLP), SVM, DT, and Extreme Learning Machine (ELM). The findings demonstrate that the method presented can classify the emotional states with the dimensions of enthusiasm. The suggested technique used seems to help evaluate both the ordinary and clinical conditions' different emotional circumstances.

(p2.6) Fei et al. [37] proposed a new emotional intelligence system focused on a new deep CNN to promote mental state identification and diagnosis. The proposed method will process face images and analyze emotional, temporal creation via a new approach. Deep characteristics are excluded from the fully connected AlexNet 6 sheet, using a typical linear LDA to achieve the final classification results. The machine contains three parts: the input of videos of facial expressions, the pre-processing procedure of photographs, and the facial expression's predictive interpretation. Experiments presented show that the proposed method exceeds the other precise and efficient techniques, which indicate that it can serve as an intelligent, low-cost cognitive assist for identifying, tracking, and diagnosing the patient's mental health using an automated analysis of facial expression.

(p2.7) Ameur et al. [59] proposed an approach based on Monogenic Binary Pattern (MBP) and CNN to increase the face's detection rate. The proposed method firstly extracts salient local features by MBP, a robust local descriptor compared to the ubiquitous Gabor filters-based LBP models. Besides, they use DCNN, which is one of the best technologies for improving large-scale image recognition. MBP-CNN has strong robustness to variations of lighting, occlusion, facial expression, texture, and facial form since it incorporates MBP and neural network. MBP-CNN is even more reliable when combinations of global and local knowledge were used.

(p2.8) Bargshady et al. [29] suggested a model CNN standard Bidirectional Long Short-Term Memory (BiLSTM) hybrid method for the deep learning of facial images for four-stage pain recognition. Maximize the proposed algorithm's total calculation efficiency. The PCA (VGG) face's completely connected layers have been improved with a fully linked additional layer and the extracted features' dimensionality. The reduced extracted features, which were the most helpful patterns to determine pain severity, feed into the newly established Enhanced Joint Hybrid (EJH)-CNN-BiLSTM classification portion. Experimental findings found that the proposed EJH-CNN-BiLSTM system increases efficiency considerably without the use of the standard solution. The improved algorithm reached 98.4 percent Area under Curve (AUC) and 90 percent test precision in the UNBC-McMaster Shoulder Pain database. The artificial intelligence methods built in their study may have applicable repercussions for the diagnostic fields, especially in support of clinicians and other medical scientists' automated pain control practices.

(p2.9) Ozcan and Basturk [23] developed An optimized FER method. The fundamental aspects of this approach are data processing and hyperparameter tuning for CNN-supported transfer learning. The picture is transferred by the face alignment method as first for the data preparation portion. Then the sample is transformed from RGB to gray format, sound reduction, and picture sharpening steps. The data increase is the last step in the data planning section. The Particle Swarm Optimization (PSO) algorithm with high global search potential is then used to collect hyperparameters. The testing of this proposed optimized approach has been tested on the JAFFE data collection, and the best accuracy score available in the literature was reached. The latest Erciyes University FER (ERUFER) dataset has been introduced. This dataset consists of 9005 samples in 10 groups. The ERUFER groups are six simple phrases, plus neutral, disdain, concern, and enthusiastic. The dataset can be commonly used because of the large number of participants and many samples.

(p2.10) Li et al. [34] suggested a new approach for understanding facial expression using an attention mechanism. Not only raw images but also LBP features are applied to the network attention layers. LBP features provide texture details, representing delicate skin textural changes that may help to discern gestures with slight distinctions. They also compiled and branded a new dataset to identify facial expressions called Nanchang University Facial Expression (NCUFE). The dataset consists of 490 photographs obtained from 35 subjects with seven facial expressions (i.e., anger, disgust, fear, happiness, sadness, surprise, and neutral). They captured both RGB images and depth images for each subject. Substantial studies on five separate datasets are carried out. Datasets, such as CK+, JAFFE, Oulu-CASIA, and NCUFE, are obtained in the real world and those collected under laboratory conditions such as FER2013. They also equate model output with state-of-the-art algorithms for speech recognition. The model findings demonstrate that it is superior to many of the current data sets approaches. The process is, therefore, only suitable for 2D images.

(p2.11) Meryl et al. [49] tested the efficacy of CNN for expression recognition with (RBF). The patients' psychiatric problems can quickly be assessed, and remedial steps accelerated. They focus on studying how people feel, cope, and gain ill health by analyzing their facial expressions. (FER) has essential steps in the extraction and classification of features. The designation is one of the critical mechanisms by which terms such as pleasure, sadness, anger, hate, surprise, and fear are identified. There are three kinds of signals on the face: static, sluggish, and heavy. The experimental results indicate that the suggested mix of methods gives the (FER) 2013 data set comparatively improved precision.

(p2.12) Jiang et al. [60] introduced a new loss feature called the advanced softmax loss to eradicate imbalanced training expressions. The proposed losses guarantee that any class would have a level playing field and potential using fixed (unlearnable) weight parameters of the same size and equally allocated in angular space. The research shows that proposed (FER) methods are better than specific state-of-the-art FER methods. The proposed loss can be used as an isolated signal or used simultaneously with other loss functions. To sum up, detailed studies on FER2013 and the realworld practical face (RAF) databases have shown that ASL is considerably more precise and effective than many stateof-the-art approaches.

(p2.13) Agrawal et al. [61] introduced two novel CNN architectures based on a FER-2013 dataset. These are both main and unique for the collection of hyperparameters across network layers. The two Model 1 and Model 2 network architectures achieve human accuracy with a FER-2013 dataset. The Model2 edition is streamlined. Model 1 architecture is unique because it uses a set kernel size and specifies the number of filters across the network depth. The number of filters decreases with network depth in this design. Model2 is compacter to Model1. Both architectures use a particular kernel size of 8-compare the proposed models with the state-of-the-art signs of the most fitting architecture for data set FER-2013.

(p2.14) Chen et al. [14] suggested a two-stage social signal analysis method focused on DCNN to understand facial expression. In the context of the face language's non-state nature, the proposed system comprises two stages: the first stage is taken immediately out of the sequence of facial expression by the SoftMax score for a binary CNN. In the second stage, the chosen neutral expression framework and the fully-expressed frame are then fed to the corresponding DCNN. The suggested approach will essentially remove the individual discrepancy with the neutral language system's variations and the entire expression frame. The accuracy achieved was 96,28% in an e-learning context for the student's affective condition study.

(p2.15) Mohan et al. [25] created a new DCNN system to categorize facial expressions using holistic features of features. In this way, a biological feature extractor is used to retrieve the local low-level features before using the proposed Deep Belief network (DCNN) model. The GF feature extractor will generate two local intermediate features (M and D). At the end of the proposed DCNN model, a Softmax classifier is used to determine the likelihood values of 7 expressions. This process combines the outputs using measured data and discrete data based on the proposed model. Empirical findings suggest that both local and holistic elements will boost the FER mission together. Nevertheless, in general, the efficiency of FER in a laboratory-controlled environment is not as good. Besides, the proposed model should be investigated in specific real-life applications.

(p2.16) Cheng and Zhou [52] suggested an expression reconnaissance model for a DCNN enhanced VGG (CNN). The model optimizes network structure and network parameters based on the VGG-19. They used migration learning methods to address the lack of picture samples. In the CK+ Database, Shallow CNN, Alex-Net, and enhanced VGG-19 deep CNN train and evaluate the facial expression data and compare the findings collected for experiments. The test results showed that in the extraction of image features, the DCNN is superior to the shallow CNN, but a database should calculate the number of layers, and the number should be moderate. Otherwise, the over-fit phenomenon occurs. The combination of convolution layers with tiny filters will also display more efficient input data features and use fewer parameters than a convolution layer with large filters with the same receptivity. The enhanced VGG-19 network model will achieve the performance of other network models.

(p2.17) Zou et al. [62] suggested the inclusion of batch regularization and the ReLU activation function convolution neural network compared to the original CK+ neural network to overcome the gradient disappearing problem. Dropout technology is added to solve the issue of network fitting. To increase identification accuracy, they suggest the extracted features be used to classify the expressions. They compared their findings with the CK+ data collection AlexNet and Visual Geometry Group (VGG-19) algorithms. The results indicate that the enhanced neural network algorithm's average recognition score is 6.9% higher than that of the traditional AlexNet algorithm. The identification rate also increased by 4.07 percent compared to the VGG-19 algorithm.
## (s3) Discussion
(p3.0) In [37] and [52], the author's use of Alex-Net architecture is noted ,which is more recent than LeNet. In [37], the author used DCNN with the entire Alexnet architecture to create user-friendly, inexpensive, and reliable systems. The average accuracy was reached 88% using JAFEE and KDEF dataset. While in [52], the writer used Alex-Net, Shallow CNN, and enhanced VGG-19 DCNN for data processing to boost the machine learning algorithm's accuracy and performance, with the accuracy reached 96 %. After observing the results from these two previous research types, the accuracy was higher with DCNN [52]. The findings revealed that the DCNN was better than the shallow CNN at extracting image features that allowed more layers. Still, too many layers would result in overfitting of the training data, and output may decline.

(p3.1) Some researchers have used the VGG architecture in the FER, a more contemporary architecture than Alex-net and LeNet, as in [29], the author used VGG architecture and the PCA dimension reduction process to extend effective facial recognition to binary classes of gestures. The proposed method can significantly impact medical research areas with an accuracy of 90 % to detect pain.

(p3.2) Besides, some studies have combined two architectures at once, as indicated in [61], where the author used both VGG net architecture and Alexnet architecture to create a reduced model for state of the art, precision results proved that the size of the kernel and the number of layers influence network accuracy .

(p3.3) To avoid the negative impact of the transition of learning characteristics between various data sets, the author in [65] replaced the fully connected layer in CNN by CRBM. It is used to identify facial expressions for learning transmission where the accuracy differed according to the dataset used, which reached the highest level 99.2% with the JAFFE and the lowest 73.75% with the FER2013.

(p3.4) On the other hand, many feature extraction methods have been reviewed in this study. Among them, two researchers ( [34] and [58]) depended on LBP. In [34] and the attention mechanism, the writer used LBP to develop the attention model to achieve better outcomes. CNN and LBP were used in research [58]. Results show that CNN, with its built-in classifier (softmax), is better than LBP in terms of accuracy, there was 97.32% with the CK+ dataset, but with YALE FACE, it was low, about 31.8 %. MBP is used in [59] to improve FER system performance, a robust local descriptor for feature extraction compared to the widespread Gabor filters-based LBP models.

(p3.5) Moreover 6 researchers ( [25], [28], [61], [62], [64] and [65]) relied on DCNN with its built-in extraction and classification features.in [62] the author used maximum pooling method for feature extraction, and softmax as classification method.in [25], they proposed DCNN that has two branches. The first branch examines geometric features, such as edges, curves, and arcs, while the second branch extracts holistic features. The results showed that both local and holistic elements would effectively improve the FER mission.

(p3.6) In [24], the author used the Haar filter to extract facial features. As observed, the accuracy ratio was low compared to other methods, taking into account the used dataset, where the accuracy was 65% using the FER2013 dataset. In [29], the PCA was used for the dimension reduction process rather than extracting the features.

(p3.7) Also, some reviewed FER methods were based on the optimization approaches. These methods used several optimized algorithms such as PSO in [23], SGD in [14], and [51]. In [23], PSO is used in hyperparameter selection. In contrast, SGD in [14] and [51] is used for training a neural network for increasing the precision of recognition.
