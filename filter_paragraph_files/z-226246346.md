# Video Generative Adversarial Networks: A Review

CorpusID: 226246346 - [https://www.semanticscholar.org/paper/5c1b1ab734ebc6ba56ea5d8af9d01a9fe8e0ba8b](https://www.semanticscholar.org/paper/5c1b1ab734ebc6ba56ea5d8af9d01a9fe8e0ba8b)

Fields: Engineering, Computer Science

## (s0) INTRODUCTION
(p0.0) The field of Computer Vision mainly deals with two types of data, namely images and videos, and these data can be used in many real-life applications for data generation, editing and classification. Data generation has gained significant attention since Ian Goodfellow released a model called Generative Adversarial Networks (GANs) in 2014 [1]. According to Google Scholar, there is an upward trend since the mid 2010's in publications when specifying "generative adversarial networks" as a search keyword, as demonstrated in Figure 1.

(p0.1) discriminator, contest against each other, while the word "Networks" illustrates that the model consists of (two) networks. The networks could be fully connected neural networks, convolutional neural networks, recurrent neural networks, long short term memory neural networks, autoencoders or any combination thereof.
## (s42) Unconditional video generation
(p42.0) This section is a review of unsupervised GANs frameworks in the video domain, and a summary of these frameworks, the datasets and evaluation metrics used can be found in Table S4. So far, the output videos produced by these frameworks are short and have low-quality frames due to the lack of any information provided as a condition with the videos during the training phase. Although such models produce low-quality videos, the unconditional models have become the foundation for conditional frameworks. For instance, MoCoGAN [40] architecture, which is an unconditional model, is used in Text-Filter conditioning Generative Adversarial Network (TFGAN) [44] and storyGAN [39], both of which are conditional models.

(p42.1) Video-GAN (VGAN) [6] was the first attempt to generate videos using GANs. The generator consists of two convolutional networks: the first is a 3D spatio-temporal convolutional network that captures moving objects in the foreground, while the second is a 2D spatial convolutional model for the static background. The generated frames from the two-stream generator are combined, and then fed to the discriminator to distinguish real videos from the fake ones.

(p42.2) In VGAN [6], the foreground stream captures the foreground objects and their motions. However, the foreground layer in the generated result usually contains some flaws in temporal or spatial aspects. Flow and Texture Generative Adversarial Networks (FTGAN) [45] adds optical flow for representing the object motion more effectively. FTGAN follows a progressive architecture that starts with a GANs framework to captures the optical flow, followed by another GANs model to generate the texture that is conditioned on the result of the previous optical flow GANs, and produces the desired frames. Both texture and flow generators in FTGAN adopt VGAN structure by separating the foreground from the background, and setting the background to zero for the flow generator.

(p42.3) VGAN is based on disentangling the foreground from the background. Similarly, Motion Content GAN (MoCoGAN) [40], which is another type of unconditional video generator, separates the content from the movements to provide more control over these components. While VGAN [6] and FTGAN [45] map a video to a point in the latent vector, the MoCoGAN framework traverses N latent points, one per frame, where each vector can be decomposed into the motion vector and content vector. Therefore, it consists of an N-to-N RNN that accepts N random variables and produces N latent motion vectors. The motion vectors are combined with a fixed content vector for all N motion variables and fed to the generators to synthesize N images, each of which is a frame in the generated video. The generated images and videos are evaluated using two discriminators: one for the images, and the other for the generated video.
## (s46) Text to video synthesis
(p46.0) This subsection considers GANs-based frameworks that aim to produce videos according to a conditional text. These frameworks have two main purposes. The first is to maintain semantic consistency between the condition and the generated video. The second purpose is to generate realistic quality videos that preserve the coherence and consistency within the frames. The main text to video GANs frameworks are summarized in Table S6.

(p46.1) Temporal GANs conditioning on captions (TGANs-C) framework [54] first encodes the text using an LSTM based encoder. The output of the sentence encoder is concatenated with a noise vector and then given to the generator, which is a 3D deconvolution network. The model has three discriminators for the video level, frame level and the motion level, to ensure that adjacent frames have coherent motion.

(p46.2) While videos in TGANs-C [54] are generated using a single generator, the GANs framework proposed elsewhere [42] generates videos progressively using multiple generators in several stages. Firstly, the conditional variational autoencoder that is conditioned on encoded text produces the initial image. This initial image provides an overall representation, which may be the background image, the colours of the image and its structure. The initial image with the encoded text is an input to a CGAN to generate higher quality images.
## (s50) CONCLUSION
(p50.0) Generative models such as GANs provide promising results in multiple domains including images, videos, audios and texts. Video synthesis is still in the early stages compared to other domains such as images. The current state of the art for video GANs suffers from low quality frames or low number of frames or both. One reason could be the higher requirement for computational power as videos are high dimensional data, and so necessitate networks with a large number of parameters. To handle such data, there is a need for a complex architecture that takes into consideration spatial as well as temporal data. For example, DVD-GAN uses one TPU and TGANv2 utilizes 8 GPUs. In addition, videos are usually multimodal and may include audio stream as well, which makes the processing even more complex. Collecting domain-specific videos is also more timeconsuming and expensive comparing to other domains such as images, as automatic video retrieval algorithms are not yet very accurate and video data collection involves a lot of manual work to select, clean and preprocess the data. Nevertheless, the trend is upward and every year more studies are being done in this area. The applications of video GANs are broad and include speech animation, video prediction, video retargetting, generating stories from caption and video completion. Although the progress on GANs in areas other than videos is well documented through several review papers, video GANs models have received less attention so far, and if at all included, they were only a section in other review papers despite their broad range. Considering the increasing number of studies on video GANs during the past few years, it is the right time to survey the field, categorise different models according to their applications and compare their differences. This paper is among the initial attempts to review GANs models that produce videos and highlight their main differences.

(p50.1) While 3D CNN GANs as proposed by Vondrick et al. [6] appear to be an intuitive choice to synthesise videos and represent frames along with time dimension, 3D convolutions may cause overfitting [78]. An alternative to 3D CNN is to utilize RNN with 2D convolution, as in MoCoGAN [40]. Using 2D convolution and 1D convolution to disentangle content from the motion dimension is another way to represent videos [46].

(p50.2) Videos can be generated using GANs either without conditional settings or by introducing a conditional signal such as an audio, image, video, text, label or semantic map. This survey paper initially groups the video GANs frameworks according to their conditional setting: unconditional video generation vs conditional video generation, and discusses the most important models proposed so far in each category and outlines the differences. Moreover, it goes deeper into the conditional frameworks and categorizes the methods according to their condition and presents and compare the different models in each of those categories. In addition, a list of all datasets used in the reviewed frameworks, their characteristics, evaluation metrics and the loss function applied in each work are presented in supplementary material. The hope is that this paper will serve as a good review of the domain so far and provide the reader with a better understanding of different frameworks and their potential applications.
## (s51) A SUPPLEMENTARY MATERIAL
(p51.0) This supplementary material provides summary tables for the reviewed frameworks in the paper. To better organize the information in different tables and include as much information as possible for each framework, codes linked to Tables S1, S2 and S3 are used. In Table S1, all datasets used in the reviewed papers are listed and coded with prefix "D" and a number (e.g. D1 refers to Clever-sv dataset and so on). It also presents the number of videos in the dataset, duration (if available), resolution and purpose. In Table S2, all metrics used in the reviewed frameworks are summarised and codes with prefix "E" and a number (e.g. E1 refers to human evaluation). Likewise, in Table S3 lists the loss functions used and codes them with prefix L and a number (e.g. L4 refers to GDL loss). In Table S4, the unconditional GANs frameworks reviewed in section 4.1 are listed, and includes the dataset(s), the loss function(s) and the evaluation metric(s) used in each work. In Table S5, speech to video GANs frameworks discussed in section 4.2.1 are summarised. In Table S6, text to video frameworks of section 4.2.2 are listed, and Table S7 is the list of semantic maps to video frameworks in section 4.2.3. In Table S8, all image to video frameworks in section 4.2.4 are listed. Finally, Table S9 is a summary of video to video GANs models of section 4.2.5. All tables contain information such as dataset, type of condition, loss functions and evaluation metrics.     Chen et al. [49] audio, initial image L3, L6, L7, L10 D5, D23, D27 E7-9
