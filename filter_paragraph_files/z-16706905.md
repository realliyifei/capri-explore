# Next Generation Business Intelligence and Analytics: A Survey

CorpusID: 16706905 - [https://www.semanticscholar.org/paper/a9059799312f06f120ba3ae2c54b924a95fd7da4](https://www.semanticscholar.org/paper/a9059799312f06f120ba3ae2c54b924a95fd7da4)

Fields: Business, Computer Science

## (s0) I. INTRODUCTION
(p0.0) Business Intelligence can be expressed as the automated process to collect raw data from heterogeneous sources, and to organize them in a systematic manner. With the automated processes, models and insights can be derived from the data to improve business processes. The best practice in enterprise BI architectures is to split back-end architecture, associated with the data collection and data organization, with the the frontend, where data analyzed and displayed to the user. The transactions data are generated when transactions are processed and they are stored in the Online Transaction Processing server (OLTP), also called Operational Data Sources. From the OLTP servers, data is extracted, transformed, and stored into a data warehouse, which is a structured data repository. Different query optimization techniques can be applied on the data warehouse for speed-up of data analysis and analytics query can run on the data warehouse. Further speed-up can be achieved through creation of data marts, which are subsets of the data warehouse.

(p0.1) In addition to the traditional data sources, i.e. transaction data, the sources of BI data are evolving to include even the messages sent via company intranets and personal profiles of employees and customers from the web. The mobile devices and other sensor data also add to the data sources. However, many of these data sources are not structured. Texts from messages posted on online social networks and data from different sensors are two type of unstructured data. This makes it challenging to maintain as traditional relational database while still achieving query efficiency. In the perspective of the data analysis, more data means more opportunity for the analytics engines to discover more insight. However, there still remains the big data challenges even in the analytic perspective.

(p0.2) The increase in data opens up opportunities in expanding the scope of BI. That is going beyond being just a mechanism to analyze trends from historical data. Situational BI can combine real time data from sensors and other personal information in real time to infer insights that are not traditionally available [1]. Operational BI is concerned with providing real time insights to the business operations, such as a call center operative who may benefit by getting instant feedback on their work. Analytics is also evolving with the notion of self-service BI, where the user may compose the analytics rules based on meta-information about the data exposed to her. These new approaches to BI however, must be carefully orchestrated such that the enterprise governance and compliance models are not violated.

(p0.3) In this survey, we capture the shifting trends in BI architecture. For the backend, we show how different technology evolutions are transforming the architecture. For the frontend, where analytics engines play the pivotal role, we focus on different trends in Machine Learning that are enabling the evolution of BI from the traditional historical analysis tool. We also chart the how challenges in enforcing the enterprise governance models are being addressed in this landscape.

(p0.4) Business Intelligence is no longer just a tool to support enterprise environments. It can be used by public enterprises and by the government to understand social scale initiatives and predict requirements. We showcase healthcare use case to illustrate how the evolved BI architectures fit into the use cases. Two other technology trends, namely big data and cloud computing, are also closely tied to the changes happening in the BI architecture. We present the connections to big data and use of cloud computing as opportunity, and discuss research arXiv:1704.03402v1 [cs.AI] 11 Apr 2017  
## (s1) II. PRELIMINARIES A. Traditional BI
(p1.0) Traditional BI systems use reporting mechanisms to access transaction data stored in data warehouse. Analyzing transaction data can help us to detect patterns and predict business trends. A traditional BI system consists of three separated layers as shown in figure 2: presentation layer, application layer, and database layer With the three-tier architecture, it is challenging to fulfill service level objectives such as maximal response time and minimal throughput rates. This is due to the difficulties in predicting execution times where the application layer does not know about the data storage management at the low-lever layers.

(p1.1) Although, typical BI system can give us a forward view of the business, it is well-known that traditional BI systems are slow, rigid, time-consuming, and maintenance requires a expert knowledge. Many researches has been conducted towards improvement of the three-tier architecture as well as to add modern features of the next generation BI.

(p1.2) B. Modern Features of Next Generation BI 1) Operational (Real-Time) BI: The competitive pressure of today's businesses has increased the need for near realtime BI, also called operational BI. The goal of operational BI is to reduce the latency between the data acquisition time and data analysis time. Reduction on the latency enables the system to take appropriate actions when an event occurs. With the operational BI realized, businesses can detect the patterns or temporal trends over the streaming operational data.

(p1.3) 2) Situational BI: Situational BI enable situational awareness. Situation BI is important in companies were fast shift of situations, often external business trends, affect the business [2]. However, such external information, which mostly come from the corporate intranet, external vendor, or internet, are unstructured. Moreover, these unstructured data need to be integrated with structured information from local data warehouse to support decision marking in real-time. For example, a business might want to know whether its customers are posting positive or negative comments about its new product. With the analysis of the comment, businesses can provide immediate feedback to the development team to make the product more competitive. As another example, it is important for a company to know whether a natural disaster has affected its contracted suppliers. Being aware of the natural disasters, enable business operatives to take appropriate actions necessary in minimizing the loss [3].

(p1.4) 3) Self-service BI: Self-service BI (SSBI) enables end users to create analytical queries and reports without the IT department's involvement. The user interface in SSBI applications must be user-friendly, intuitive and easy to use, so that a technical knowledge of the data warehouse is not required. The user also should be allowed to access or extend not just IT-curated data sources, but also non-traditional ones. presentation, application, and database. With the three-tier architecture the execution time is hard to predict, due to the correlation between low-level data management operations and high-level processes. The workload management solutions are usually built on top of general-purpose database management systems, which require time delays when executing requests in parallel. This creates challenges for modern business applications to be able to work as operational or real-time BI. Therefore, technologies that enable performing analytical queries and business transaction queries at the same time on the same data is important.

(p1.5) Today's enterprises use an extraction, transformation, and load (ETL) model to extract data, perform transformations, and load the transformed data into the data warehouse. This model rely on two types of processes which are vital to business operations: online transaction processing (OLTP) and online analytical processing (OLAP). OLTP is used to manage business processes, such as order processing. OLAP is used to support strategic decision making, such as sales analytics.

(p1.6) Workloads from OLTP and OLAP are traditionally executed on the same database system. However OLAP workloads composes mostly of massive read-only operations on the data that is constantly being updated by the OLTP. Therefore, when both workloads executed in a single database, the transaction processing performance might be unpredictable due to resource contention. It is thus beneficial to separated out the workloads for OLTP and OLAP. Figure 3-a shows the traditional ETL-based BI where OLTP and OLAP are separate. In this architecture, each OLAP workload has to wait until the data in date warehouse are completely updated and visible causing delays.

(p1.7) To reduce the delay, today's operational BI systems perform OLTP and shorter-running analytical queries, called short OLAP workloads, together on the operational database management system (DBMS), as shown in Figure 3-b. However, many short OLTP transactions, which make changes to the database, may conflict with longer-running OLAP workloads. High synchronization overhead is required to handle the resource contention, which results in low overall resource utilization.
## (s3) A. Data
(p3.0) The business insight is obtained from the raw data which is heterogeneous in nature. The heterogeneity in the data may be as a result of difference in data sources, the content of data format, type of data or as a result of diversity in data extraction process. Depending on the content of study data source can be human generated, machine generated, internal data sources, web and social media, transaction data, biometric data etc. Further, the context data format may vary from being structured, unstructured, semi-structured, images, text, videos, audio etc [23], [24]. Considering the data type, there is heterogeneity as meta data, master data, historical and transactional data. In the business intelligence study the heterogeneity in data is also contributed by the data extraction method that may depend on the on-demand feeds, continuous feed, real time feeds or time series.
## (s5) C. Time series forecasting
(p5.0) In this survey, we will discuss these dynamic decision techniques for time series forecasting. The description of time series data includes high dimensionality, volume and continuously evolving. The analysis of time series data is a powerful analytics tool as it help to address questions as rate of change in user behavior with time, co-variance between the product, marketing promotion strategies, current trend in product sale, profit monitoring, determine anomalies etc. in nearly all enterprises as sales, manufacturing, mobile companies, hospitals, etc.

(p5.1) 1) Machine Learning in financial forecasting: The financial prediction on the stock price is of great interest for the investors as well as the analytics. However, the prediction about the current time to buy or sell any stock is not an easy task as the price is influenced by number many factors. We list some of the machine learning techniques from the literature commonly used to carry out such predictions. Support Vector Machine (SVM) is a popular supervised machine learning algorithm, which can be used for classification or regression problems. SVM algorithms are able to capture complex relationships between data samples without carrying out difficult transformations. Cao et al. [29] proposed an application of SVM for financial time series forecasting using single data source. The work proposed a SVM with adaptive parameter (ASVM) to handle the structural changes in the financial data. The experiment was carried out on five real futures contracts collated from the Chicago Mercantile Market.

(p5.2) The Fuzzy logic methods have been proven effective in dealing with complex systems containing uncertainties that are otherwise difficult to model. In [30] a granular computing is proposed based fuzzy model to improve the accuracy of financial forecast. The data source considered for experimentation work considered first-order fuzzy time series model were Taiwan Stock Exchange Capitalization Weighted Stock Index (TAIEX), Dow-Jones Industrial Average (DJIA), S&P 500 and IBOVESPA stock indexes.

(p5.3) Hybrid methods are also widely used. A hybrid neurogenetic approach that combined a recurrent neural network and the genetic algorithm was also used to predict economic growth [31]. The recurrent neural network had one hidden layer is used for the prediction model and the genetic algorithm was used to optimize weight. The data source used were 36 companies data from New York Stock Exchange (NYSE) and National Association of Securities Dealers Automated Quotations (NASDAQ). Another hybrid approach namely regularized least squares fuzzy support vector regression was proposed to address noise and non-stationarity existing in financial time series data [32]. Six financial data sets were gathered from Yahoo financial website for IBM, Microsoft, Google Inc., Redhat Software, Citigroups, and Standard & Poor 500 enterprise. The performance using multi-output support vector regression (MSVR) considering interval-valued over short and long horizons [33]. The global dataset used for testing were S&P 500 for the US, FTSE 100 for the UK, and Nikkei 225 for Japan. Today, the stock markets databases are flurried from a wide range of complex data from diverse sources as market data, reference data, exchange, security description, fundamental data as enterprise financial, analyst report, filing etc., and even data from social media that may include blogs, web feeds etc. Moreover, the financial data is highly dynamic and volatile which raise a need for some integrative approach that combine data from the other sources and contributes towards the accuracy of the forecast.

(p5.4) 2) Machine learning in sales forecasting: The prediction of future sale by an enterprise is termed as sales forecasting, which is a part of its critical management strategy. The machine learning techniques as Genetic algorithms (GA) are suitable candidates for this task since GAs are most useful in multiclass, high-dimensionality problems where heuristic knowledge is sparse or incomplete. Neural networks are also considered as efficient computing models for pattern classification, function approximation and regression problems. The sales forecasting problem for printed circuit board (PCB) sales addressed in [ [34]], where the model is built by integrating GAs and Neural Network. The study was carried out for PCB electronic industries in Taiwan, where the feature of data included monthly sales amount, total production square measures, etc. The PCB sale is studied [ [35]], using integrated genetic fuzzy systems (GFS) and data clustering. The approach was experimented on the PCB data source with parameters as pre-processed historical data, Consumer price index, Liquid crystal element demand and Total production value of PCB. Kuo et al. [36] proposed a hybrid machine learning system with fuzzy neural network on locally chain supermarket data. They method enabled incorporating expert knowledge in the forecasting. In one example of automobile sale forecast [37], adaptive network-based fuzzy inference system was considered, which included several economic variables as current automobile sales quantity, coincident indicator, leading indicator, wholesale price index and income for prediction. The data analyzed was whole automobile market in Taiwan that included sales data corresponding to sedan, small commercial vehicle, and large commercial vehicle. The sales forecasting is very complicated owing to influence by internal and external environments.

(p5.5) 3) Machine learning in heathcare: The time series data are studied by the healthcare enterprises using machine learning to understand existing patterns that help the administration to make strategic decisions. Some prediction based on the time series data includes outpatient visits, and customer behavior in choosing hospital Chang et al. [38] propose a fuzzy logic based approach based on weighted transitional matrix to forecast the outpatient (patient who receives medical treatment without being admitted to a hospital) visits. The forecasting is important as effective prediction helps the administration to manage operation, distribute resources and other aspects. The build model was tested on the data gathered from the department of internal medicine in a hospital. The data had two features to be monitored, month of the year and the number of outpatient. Another similar study was carried out by Hadavandi et al. [39] were a hybrid model was built that combine genetic algorithm with fuzzy rule based learning to forecast outpatient visits. The data were collected from the department of internal medicine in a hospital in Taiwan and four big hospitals in Iran. The data features being the month and the number of outpatient. In a different application, neural network techniques were used to make predictions about the consumers behavior in choosing hospital [40]. The results are useful as the hospital operating environment is getting more competitive. The data feature considered were cost of medical care, accessibility, parking, hospital reputation, doctor reputation, doctors medical skill, modern equipment, etc D. Improving BI via integrative data analysis

(p5.6) In todays enterprise, data are created from multiple source. The data from multiple sources can provide an insight to increase productivity, improve policy making, support performance measurement, and can help in strategic planning. These insights can help achieve benefits as improved customer satisfaction, quality improvement, increased accessibility and analysis of information, timeliness and better information utilization. The need to handle the heterogeneous data and automated analytics algorithm can be resolved by doing predictive analysis. It supports the study of the integrated data using machine learning algorithm that continuously evolve the accuracy of predictive models and enable it to adjust.

(p5.7) An integrative analysis task involves machine learning techniques for the integration of the available training data from different data sources in order to better analysis and generate a proactive response. A single data source may not contain all required information about the data object. Thus, when we combine multiple sources of information for a particular data object it helps to add on some different or missing information that may lead to a better prediction accuracy. Integrating data from multiple sources and making decisions from these combined sources is becoming common to enhance the prediction performance for different applications as in bio-informatics, image classification, stock market, etc. The two most common integrative analysis are Multiple Kernel Learning and the Bayesian Network (BN) [23]. Tensor decomposition based data mining algorithms is also promising. Tensor are high dimensional array which naturally allows multiple source of the same component can be integrative represented and analyzed in both time series data as well as static data [41]- [43]. However, we will mainly focus on applications of MKL.

(p5.8) 1) Multiple kernel learning: Multiple kernel learning (MKL) refers to a set of machine learning methods that use a predefined set of kernels, where kernel selection depend on the notions of similarity that may exist in the data source. MKL learn an optimal linear or non-linear combination of kernels as part of the algorithm that result in data integration. MKL algorithms have been developed for supervised, semisupervised, as well as unsupervised learning.

(p5.9) In stock price prediction it is observed that relying on the study of the time series historical data is not sufficient, rather by considering multiple sources of information such as news and trading volume one can significantly improve the stock market volatility predication [44]. The experiments carried out on HKEx 2001 stock market datasets shows that the use of multiple kernel learning approach on the multiple data sources results in higher accuracy and lower degree of false prediction as compared to single source data.

(p5.10) The work [45], shows that analyzing communication dynamics on the internet and using stock price movements may provide some new insights into relations between stock prices and communication patterns. They use MKL to combine information from time series data, stock price and stock volume with other data source: news and comments that included the frequency of News, frequency of the comments, average Length/ Standard Deviation of length of comments, number of Early/Late response etc. The experiment was tested for the stock of Amazon, Microsoft and Google and it was found that MKL prediction model outperforms other baseline methods Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE) and Root Mean Square Error(RMSE).

(p5.11) Integrative data approaches are applied to draw inferences from the biomedical data. In [46], the paper reports the advantage of the MKL integrative approach that thoroughly combining complementary information from the heterogeneous data sources over the sparse integration method for the biomedical data. The experiments were carried out for different applications as disease relevant gene prioritization by genomic data fusion, Prioritization of recently discovered prostate cancer genes by genomic data fusion, Clinical decision support by integrating microarray and proteomics data, Clinical decision support by integrating multiple kernels integration of genomewide data for clinical decision support in cancer diagnosis, and Computational complexity and numerical experiments on large scale problems. An application is also seen in [47], where the different biological measurements are integrated using the regularized unsupervised multiple kernel learning to find cancer subtypes. In [48], bayesian networks are considered for data integration of two different biological data (gene expression and transcription factor binding (ChIP-chip)) for discovering the transcriptional modules.

(p5.12) Integrative data analysis finds applicability in highdimensional hyperspectral classification. In satellite remote sensor application [49], MKL approach proves beneficial because of the high-dimensional feature induced by using many heterogeneous information sources. The approach is based on the automatic optimization of a linear combination of kernels dedicated to different meaningful sets of features as groups of bands, contextual or textural features, or bands acquired by different sensors. The results obtained showed a good performance of the method in image classification, when multispectral, hyperspectral, contextual or multi-sensor information were used. The urban classification using MKL was carried [50], to integrate heterogeneous features from two data sources, i.e., spectral images and LiDAR data. The features from spectral images are good at identifying ground truth such as trees, grass, and soil; whereas features from LiDAR data perform better for classes with flat surfaces such as buildings. The classification model build by combining the data source with complementary relationship significantly improve the classification accuracy.
## (s7) A. Use case: BI in Healthcare
(p7.0) The wealth of available data in healthcare will continue to increase. This together with raises the demand for improved quality of patient care, necessitates the improvement of Healthcare BI. This rising demand can be best addressed by considering the business intelligence technological solutions of data acquisition, storage, interpretation and evaluation.

(p7.1) 1) Heterogeneous data in healthcare: Healthcare deals with huge heterogeneity in the data coming from different sources. The major challenges involved in managing healthcare data are format, structure and complex nature. Health care data occurs in different formats as numeric, textual, digital, images, videos, multimedia etc. Electronic health record (EHR) holds hundreds of rows of textual and numerical data corresponding to patient demographics, progress notes, vital signs, medical histories, diagnoses, radiology images, medications, lab and test results. In healthcare, the data is structured and unstructured. Structured data refer to the lab or patient demographic data that is consistent and stored in a pre-defined format. The unstructured data are nonuniform and can be of great value in analyzing the patient data. These include clinical notes, audio voice dictations, email messages and attachments, text message, online video and typed transcriptions. The presence of structured and unstructured data makes the healthcare data complex to process and increase further as the number of variable increases.

(p7.2) 2) Use of sensor in healthcare: Business intelligence in healthcare provides a wide range of analytics to improve the decision making process related to both patient and performance that covers many functional areas, including resource planing, care delivery, patient accounting, financial and revenue cycle. The technological advancement has allowed a greater accessibility to data. The use of sensors in healthcare produces large volumes of data continuously over time. An application can be seen in the Intensive Care Unit (ICU) were sensors are used to monitor the current state of patients, it includes ECG, EEG, blood pressure monitors, respiratory monitors.

(p7.3) 3) ICU case study: Intensive care units has a data rich environment with multiple source of continuous data originate from medical devices which includes electroencephalogram, Bedside Monitors, Brain Tissue Oxygen Monitor, Central Venous Catheters (CVC), Clinical Information Systems and ventilators, resulting in several kilobits of data each second per patient. A patient in severe health state are often monitored by number of body sensors connected to monitoring devices producing large volumes of physiological data, along with comprehensive and detailed clinical data and minute-byminute trends for the patient. Another, ICU patient monitoring involves the system to generate some alerts or alarms when the physiological state of the patient shows the detection of relevant abnormalities or changes in a patient's condition. The physicians use certain thresholds, which when exceeds triggers the in alarm. However, such simple alerting schemes may result in large number of false alarms, example,Tsien and Fackler [51] found that 92% of alarms were false alarms in their observation in a pediatric intensive care unit, in [52], authors digitally recorded all the alarms for 38 patients on a 12-bed medical ICU and retrospectively assessed their relevance and validity: Only 17% of the alarms were relevant, with 44% being technically false.

(p7.4) In case of medical emergency, it is critical for the patient to receive the medical aid at the earliest. The recent advancement in the sensor technology helps to achieve this goal by monitoring patient crucial parameters such as heart beat, body temperature, blood monitoring, EEG etc. In the given scenario, business intelligence can play a key role by carrying out the predictive analysis by integrating the sensor data with the available traditional data. The processed data that give the vital statistics of the patient can be transmitted to the doctor to guide the paramedic traveling with the patient. In addition, this information can be used to take appropriate action as soon as the patient arrives at the hospital.

(p7.5) The existing complexity in the data makes the existing approaches may not be suitable to manage data in healthcare. In healthcare business rules and definitions are volatile and may change over a period of time. This calls for a solution that is based on agile approach and can handle data from multiple sources, the different format, the structured and unstructured data and manages the complexity within an ever-changing regulatory environment. Thus, an intelligent machine learning algorithms are required to find a coherent meaning from disparate data to process heterogeneous data captured through different sensors.
