# A Survey on Web Pre-Fetching and Web Caching Techniques for Latency Optimization in Mobile Environment

CorpusID: 53526524 - [https://www.semanticscholar.org/paper/f2379292204b014ddc8344d98372462feb61afe6](https://www.semanticscholar.org/paper/f2379292204b014ddc8344d98372462feb61afe6)

Fields: Computer Science

## (s0) Introduction
(p0.0) Web traffic is a major component of internet traffic, which corresponds to the unstable growth and causes bandwidth consumption that this medium is experiencing. Intention of many researches towards this area is to reduce users perceived latency when they are navigating through websites. The prefetching and caching techniques help a lot in reducing latency. An important advantage of the WWW is that many web servers keep a server access log of its users. These logs are used to prepare a prediction model for future document accesses. Based on these models, one can obtain frequent access patterns in web logs and mine association rules for path prediction. Web prefetching can be applied in web environment between clients and web server, proxy servers and web server and between clients and proxy servers [1]. Applying perfecting between web server and clients reduces latency but increases network traffic and between proxy server and web server decreases bandwidth usage. Web prefetching is also used to fetch web pages by proxy server/client before the page is requested by a client/proxy server. The proxy server intercepts the requests from clients, serves with the requested object if it is present in proxies (pre-fetch area), or retrieves new objects from the appropriate web server, then caches the new objects or updates the existing objects if it has been modified since the last reference. The key idea forecast takes improvement of the idle periods to fetch the files that will likely be requested in the future, so that the user's average waiting time can be decreased. The major factor is to select an efficient web prefetching algorithm; the selection is done by considering its ability to predict the web object to be pre-fetched in order to reduce the latency, without consuming more bandwidth.

(p0.1) Web pre-fetching techniques are broadly categorized into three. They are probability based, clustering based and using weight-functions [2].Probability based method uses history access and assumes that the request follows the same pattern. Clustering based prefetching technique assumes that the pages that are close to those previously fetched pages are more likely to be requested soon. In the weight functions, a weight is given to an accessed page and assumes that the one with the highest weight will be referred in the near future. Although web browsing is an important application for prefetching, generally prefetching scheme may be applied to any network application in which prefetching is possible.
## (s2) Interactive perfetching
(p2.0) In interactive perfetching technique [4] the retrieved pages are fetched along with hyper links and inline images. Since this method prefetches all hyperlinks in web pages, it requires a lot of space to store the pre-fetched web pages and it increases web traffic for perfetching the hyperlinks.
## (s3) Adaptive prefetching
(p3.0) Jiang and Leonard [5] proposed an Adaptive prefetch scheme, in which prefetching is based on user access history and network conditions. It has two modules, a prediction module and a threshold module. Files to be prefetched are the one whose access probabilities greater than or equal to the prefetch threshold.
## (s5) Prediction by Partial Match (PPM)
(p5.0) PPM makes pre-fetching decisions by reviewing the URLs that clients on a particular server have accessed over some period [23]. PPM algorithm has three parameters; order, depth and threshold. Order is the length of the history substring that the algorithm uses to find a match. Depth is the number of accesses into the future the algorithm attempts to predict. Thershold is the minimum probability and access must have in order to be considered a pre-fetch candidate. The standard PPM model uses multiple Markov models to store historical URL paths.
## (s7) Dynamic Prefetching
(p7.0) Dynamic Prefetching [9] technique stores the preference list of sites of a user in a data base of proxy server. The list of accessed URLs and its weight is stored in a hash table.
## (s8) Top -10 approach
(p8.0) In Top-10 approach the server periodically calculates the list of most popular documents [10]. These documents are periodically pushed from web servers to web proxies then to clients. It is simple and easy to implement in client server architecture.
## (s9) Domain top approach
(p9.0) The proxy"s active knowledge of their most popular domains and documents with client access profiles are combined in the domain-top approach (DT). Proxy side is responsible for periodically calculating popular domains and popular documents in each domain. Based on the proxy"s active knowledge of popular domains and documents, a rank list is prepared for anticipating the client"s future requests [11]. In this approach prefetching occurs only in proxy server.
## (s10) Link prefetching
(p10.0) It is a browser mechanism, utilizes browser idle time to download documents that the user might visit in the near future. Fisher et.al devised an approach for link prefetching [12]. The browser follows special directives from the web server or proxy server that instructs it to prefetch specific documents. It allows the servers to control the contents to be prefetched by the browser.
## (s11) Non-interfering prefetching
(p11.0) This scheme avoids interference by effectively utilizing spare resources on the servers and the network [13]. Prediction and resource management are the important tasks. Predictor predicts prioritized lists of high valued documents for prefetching. Resource manager limits the number of documents to prefetch and schedules the prefetch request to avoid interference.
## (s12) Semantic prefetching
(p12.0) In this scheme prediction of future request is based on preferences of past retrieved documents in semantics [14,15]. Semantic prefetching techniques tend to capture the client surfing interest from users past access patterns and predict future preferences from a list of objects when a new page is visited. Semantic knowledge of web documents are automatically extracted and adaptive semantic nets are constructed between web documents.
## (s16) Predictions from HTML content
(p16.0) Davidson [20] suggested a technique that predicts the user"s next request by analyzing the recently requested page content by the user. It prefetches the top 5 URLs predicted by this system.
## (s17) Proxy based ad prefetching
(p17.0) A mobile advertising system has five parties: mobile clients, advertisers, ad servers, ad exchanges, ad networks [21]. This approach uses a proxy between the ad server and the mobile client. A client with available ad slots contacts the proxy that prefetches a batch of ads from the ad exchange (through the ad server) and sends the batch to the client. After the client has displayed all ads of the batch, it contacts the proxy again and get the next batch of ads.
## (s22) Distributed caching
(p22.0) Distributed cache is a set of cooperative caches placed at the same level of the network, a missing of resources at one proxy causes a search in all cooperating cache servers for caches hit. In distributed caching no intermediate caches are set up and there are only institution caches at the edge of the network which cooperate to serve each other"s misses [27,28,29].
## (s23) Hierarchical caching
(p23.0) Proxies or cache servers are arranged in a tree like structure either logically or physically. In hierarchical caching architecture, caches are placed at multiple levels of the network [30]. Individual caches can be interconnected hierarchically to mirror an inter network"s topology. Each bottom level cache is associated with a set of clients. A client request is first sent to the bottom level cache and then iteratively forwarded up the hierarchy such as institutional cache, regional cache, national cache, until the request is satisfied. If the root cache does not have target object, the request is finally directed to the origin server [31].
## (s24) Hybrid caching
(p24.0) Hybrid caching architecture combines hierarchical caching with distributed caching at even level of a caching hierarchy [32]. In a hybrid scheme, cache cooperates with other caches at same level or at a higher level using distributed caching. The document is fetched from parent/neighbor cache that has the lowest round trip time.
## (s25) Transparent web caching
(p25.0) Transparent web caching uses network devices to redirect HTTP traffic to caching servers. This technique is called transparent because web browsers do not have to be explicitly configured to point to a cache server [33]. There are two ways to deploy transparent proxy caching: at the switch level and at the router level. Router based transparent caching uses policy-based routing to direct request to the appropriate caches. In switch based transparent caching, the switch acts as a dedicated load balancer [34]. 
## (s26) Cascaded caching
(p26.0) Cascaded caching is also known as Multi-layer caching; Multiple copies of same object may be available in many caches placed at different locations. There can be cache servers both at the client location and server location. This allows a cache server at client location to get a requested document from the cache server. The overall performance of cascaded caching depends on how the cache contents are managed, including object placement and replacement algorithms [35].
## (s27) Adaptive caching
(p27.0) An adaptive web caching has a scalable, robust, adaptive, and fully distributed protocol for sell-organizing cache servers into overlapping multicast groups [36]. Web caches maintain a URL routing table. The primary keys of URL routing table are URL prefixes, associated with one or more identifiers to the next hop caches or cache groups. The routing table is used for deciding whether to forward a request to another cache in the web caching infrastructure. Adaptive algorithms are used for exchanging of information among caches in a cache mesh. Cache group management protocol is used for making the entire cache topology group-wise connected and management protocol is used for minimizing the number of hops a URL request must travel upon cache fault.
## (s28) Server side caching
(p28.0) Even if client side caches could not satisfy the user requests, a cache at sever side is useful in reducing the delay in accessing the documents from the server"s hard disk. Temporal and geographical localities of reference are exploited on a much large scale at server side [37]. Web server accelerator contains a cache and load balancer. It resides in front of a web server for delivering cached responses and leaving the role of content generation to the web server. Cached objects are directly sent from the accelerator to the clients.
## (s29) Caching of Dynamic contents
(p29.0) Dynamic web pages are created on request by application programs stored in the back-end site infrastructure, caching of dynamic web pages are essential for improving the performance of web sites containing significant dynamic content and information personalized to individual users [38]. Dynamic content has three forms of locality: identical requests, equivalent requests and partially equivalent requests. Identical requests have identical URLs which result in the generation of the same content. The URLs of equivalent requests are syntactically different but result in generation of identical content. Partially equivalent requests result in generation of content which can be used as temporary place-holder for each other.
## (s30) Proxy caching
(p30.0) Proxy servers intercept HTTP requests from clients and if the requested objects are in its cache, it returns the object to the client. If not, it forwards the object to the server, gets the object, stores it in cache and returns the object to the user. The drawback of this design is that the cache represents single point failure in the network [39]. This can be avoided by sharing the caches of proxy servers.
## (s34) Universal Mobile Caching
(p34.0) Universal Mobile Caching (UMC), which is suitable for managing object caches in structurally varying environments and which is self-optimizing for changing workloads [41]. UMC is based on a simple set of basic criteria which reflects a spectrum of possible caching policies. UMC has demonstrated the ability to provide caching benefits in the on-demand retrieval of web documents for the mobile web, wherein multiple levels of intervening caches can create adverse workloads for other general caching schemes. UMC policy offers a selfoptimizing replacement algorithm that is usable for general object caching. Universal Mobile Caching uses a very simple set of object properties to select which objects (of varying size) will be removed from the cache.
