# An Overview of the Determinants of Financial Volatility: An Explanation of Measuring Techniques

CorpusID: 9212240 - [https://www.semanticscholar.org/paper/3dfb9bedb6e33ae8d4a87c84dc22a936d1d87c8d](https://www.semanticscholar.org/paper/3dfb9bedb6e33ae8d4a87c84dc22a936d1d87c8d)

Fields: Business, Mathematics, Economics

## (s7) Modeling Volatility
(p7.0) According to Engle (1993), financial market volatility is predictable.In principle this claim may only be justified when ARCH effects are present.The implication of this observation for risk averse investors is that they can adjust their portfolios by reducing their commitments to assets whose volatilities are predicted to increase, thereby reducing their exposure to risk.Predicting volatility is really just a prediction of variance, a prediction that the potential size of a price move is small or great.Volatility forecasting is an imprecise activity, just like predicting rain.You can be correct in predicting the probability of rain, but still have no rain.

(p7.1) In modeling volatility, time series statistics are used to find the best forecast of volatility.By using time series statistics it is possible to determine whether recent information is more important than old information and how fast information decays.We can determine whether volatility is equally sensitive to market up moves as it is to down moves, and whether the size of past returns is proportional to the magnitude of volatility experienced today.
## (s13) ARCH-M
(p13.0) Assets with high expected risk must offer a high rate of return to induce investors to hold them.In this case, increases in conditional variance should be associated with increases in the conditional mean.Merton (1980) derives an equation that relates the expected return on the market linearly to the conditional variance of the market:
## (s17) Comparing Alternative Volatility Models
(p17.0) In comparing alternative ARCH models, a number of approaches can be taken.Nelson and Foster's (1994) research on ARCH models concentrates on refining techniques which approximate the measurement accuracy of ARCH conditional variance estimates and on comparing the efficiency achieved by different ARCH models.By deriving the asymptotic distribution of the measurement error, Nelson and Foster (1994) are able to characterise the relative importance of different kinds of misspecification.In particular, they show that misspecifying conditional means adds only trivially to measurement error, while other factors (for example capturing the 'leverage effect', accommodating thick-tailed residuals, and correctly modelling the variability of the conditional variance process) are potentially much more important.
## (s21) Estimating Volatility in Practice
(p21.0) In reality, several problems exist in attempting to measure volatility.Volatility clearly changes with time.The value of using all available data is severely limited by the fact that prices and returns for many securities appear to have some serial correlation and other distortions at both short and long intervals.Positive autocorrelation in returns will reduce estimated volatility.To limit the effect of serial correlation at high frequencies, researchers can estimate using fewer data points, but this may increase sampling error.Another way in which actual security returns differ from equation (1.24) is the well-documented problem of 'fat-tails'.Equities and many other securities exhibit more large price changes than is consistent with the lognormal diffusion model.Allowing for the fact that securities prices do not come from a constant volatility lognormal diffusion process, computing historical volatility as shown in equations (1.25 -1.27) is no longer theoretically optimal.But many academic researchers and practitioners typically ignore them and calculate historical volatility estimates by the most basic method.The most common method of producing volatility forecasts from historical data is simply to select a sampling interval and the number of past prices to include in the calculation and then to apply equations (1.25 -1.27).

(p21.1) Above we examined several extensions to the basic ARCH model of Engle.The complexity of these models is in part due to the training of financial economists and practitioners in classical statistics.While the estimation procedures greatly influence the kinds of models and procedures used to tackle quantitative problems in their specialist fields, the complexities involved with measuring volatility warrant equally sophisticated models to estimate volatility.However, on this point Figlewski (1996), has argued that the classical statistics' view of the world does not accurately represent the nature of the underlying structure of a financial market.In particular those trained in classical statistics tend to build models that are too complex and expect too much from them.Figlewski (1996) illustrates this with an example.If it is assumed that estimation and forecasting are very similar to each other, goodness of fit statistics tell us how closely a model fits the data that was used in estimating it.However it is here that the classical statistics framework fundamentally misrepresents the nature of a financial market and leads those who adopt it to expect much better forecasting performance than can be achieved in practice.Consider the following estimation and prediction problem.Suppose we wanted to predict the movement of a whale, based on observing it over a period of time.The whale being a large animal, should move with a fairly predictable pattern over the short run simply from extrapolation.However, we do not think of a whale as following a fixed and immutable pattern, or one that we could ever hope to understand completely.The whale's behavior as a complex living organism must remain partially unpredictable no matter how much past data we may have.In this case, we are not looking at a fixed structure with constant but unknown parameters, but rather at a system that evolves over time, and perhaps alters its behavior rapidly on occasion.Because its evolution is partly stochastic, no amount of past data will allow us to know the exact structure of the system now or in the future.Prediction is possible only because the system evolves slowly and therefore our accumulated information from observing it decays only slowly.In this case, there may be an enormous difference between how well a model fits in-sample and how well it can forecast out-of-sample, and classical goodness of fit statistics may give little guidance about the latter.Also, having a large data sample for estimation does not guarantee that accurate parameter values can be computed.In any case, expanding the estimation data set by adding observations from the distant past can easily make the estimates of the current state of the system worse rather than better.In summary therefore, the more detailed and elaborate a model is, the better the fit one is generally able to obtain in-sample, but the faster the model tends to go off track when it is taken out-of-sample.Forecasting is a very different operation from in-sample estimation.In particular, financial markets behave very much like a whale.Nonetheless, it may be said that historical volatility computed over the recent periods provides the most accurate forecast for both long and short-run horizons.
