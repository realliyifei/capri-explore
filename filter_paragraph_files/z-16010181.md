# Facial Feature Point Detection: A Comprehensive Survey

CorpusID: 16010181 - [https://www.semanticscholar.org/paper/fcecaa3eed9574028bb3887a0eaa6a8b5a30bd9d](https://www.semanticscholar.org/paper/fcecaa3eed9574028bb3887a0eaa6a8b5a30bd9d)

Fields: Computer Science

## (s0) Introduction
(p0.0) Facial feature points, also known as facial landmarks or facial fiducial points, have semantic meaning. Facial feature points are mainly located around facial components such as eyes, mouth, nose and chin (see Fig.  1). Facial feature point detection (FFPD) refers to a supervised or semi-supervised process using abundant manually labeled images. FFPD usually starts from a rectangular bounding box returned by face detectors (Viola and Jones, 2004;Yang et al, 2002) which implies the location of a face. This bounding box can be employed to initialize the positions of facial feature points. Facial feature points are different from keypoints for image registration (Ozuysal et al, 2010) and keypoint detection is usually an unsupervised procedure.

(p0.1) Suggested by , facial feature points can be reduced to three types: points labeling parts of faces with application-dependent significance, such as the center of an eye or the sharp corners of a arXiv:1410.1037v1 [cs.CV] 4 Oct 2014 Fig. 1 Illustration of an example image with 68 manually labeled points from the Multi-PIE database (Gross et al, 2010). boundary; points labeling application-independent elements, such as the highest point on a face in a particular orientation, or curvature extrema (the highest point along the bridge of the nose); and points interpolated from points of the previous two types, such as points along the chin. According to various application scenarios, different numbers of facial feature points are labeled as, for example, a 17-point model, 29-point model or 68point model. Whatever the number of points is, these points should cover several frequently-used areas: eyes, nose, and mouth. These areas carry the most important information for both discriminative and generative purposes. Generally speaking, more points indicate richer information, although it is more time-consuming to detect all the points.

(p0.2) The points shown in Fig. 1 can be concatenated to represent a shape x = (x 1 , · · · , x N , y 1 , · · · , y N ) T where (x i , y i ) denotes the location of the i-th point and N is the number of points (N is 68 in this figure). Given a sufficiently large number of manually labeled points and corresponding images as the training data, the target of facial feature point detection is to localize the shape of an input testing image according to the facial appearance. Detecting the shape of a facial image is a challenging problem due to both the rigid (scale, rotation, and translation) and non-rigid (such as facial expression variation) face deformation. FFPD generally consists of two phases: in the training phase, a model is learned from the appearance variations to the shape variations; and in the testing phase, the learned model is applied to an input testing image to localize facial feature points (shape). Normally the shape search process starts from a coarse initialization, following which the initial shape is moved to a better position step by step until convergence. According to the method of modeling the shape variation and the appearance variation, existing FFPD methods can be grouped into four cate-gories: constrained local model (CLM)-based methods (here, the term CLM should be not confused with that in Cristinacce and Cootes (2006b) which is a special case of CLM in our nomenclature), active appearance model (AAM)-based methods, regression-based methods and other methods.

(p0.3) CLM-based methods consider the appearance variation around each facial feature point independently. One response map can therefore be calculated from the appearance variation around each facial feature point with the assistance of a corresponding local expert. Facial feature points are then predicted from these response maps refined by a shape prior which is generally learned from training shapes. AAM-based methods model the appearance variation from a holistic perspective. In addition, both the shape and appearance variation model are usually constructed from a linear combination of some bases learned from training shapes and images. Regression-based methods estimate the shape directly from the appearance without learning any shape model or appearance model. There are also other FFPD methods which do not fall into any of the aforementioned categories and are classified into the category of 'other methods'. This category can be further divided into four sub-categories: graphical model-based methods, joint face alignment methods, independent facial feature point detectors, and deep learning-based methods. Table 1 and Fig.2 present the development timeline of the four categories of methods. As shown in the table and figure, the topic has attracted growing interest.
## (s1) Constrained Local Model-Based Methods
(p1.0) CLM-based methods fit an input image for the target shape through optimizing an objective function, which is comprised of two terms: shape prior R(p) and the sum of response maps D i (x i ; I), (i = 1, · · · , N ) obtained from N independent local experts :

(p1.1) A shape model is usually learned from training facial shapes and is taken as the prior refining the configuration of facial feature points. Each local expert is trained from the facial appearance around the corresponding feature point and is utilized to compute the response map which measures detection accuracy. The CLM objective function in the equation (1) can be interpreted from a probabilistic perspective:

(p1.2) where l i ∈ {1, −1} indicates whether the i-th point is aligned or misaligned, R(p) = −ln{p(p)} and D i (x i ; I) = −lnp(l i = 1|x i , I). The CLM objective function (either (1) or (2)) implicitly assumes that N response maps are calculated independently. In the offline phase, a shape model and local experts should be learned from training shapes and corresponding images. Then in the online phase, given an input image, the output shape can be solved from the optimization of equation (1). We will investigate commonly used shape models and local experts sequentially. Finally, methods on how to combine the shape model and local experts for optimization are investigated. Fig. 3 illustrates the statistical distribution of facial feature points sampled from 600 facial images. Regarding the shape prior, multivariate Gaussian distribution is commonly assumed, otherwise known as the point distribution model (PDM) proposed by Cootes and Taylor (Cootes and Taylor, 1992): 
## (s2) Shape Model
(p2.0) where s i (i = 0, · · · , n) can be estimated by the principal component analysis (PCA) on all aligned training shapes. Actually, s 0 is the mean of all these shapes and s 1 , · · · , s n are the eigenvectors corresponding to the n largest eigenvalues of the covariance matrix of all aligned training shapes. n is usually determined by preserving 90% ∼ 98% variance (the ratio between the sum of n largest eigenvalues and sum of all eigenvalues). Mei et al (2008) suggested the above rule to determine whether the value of n is reliable or not and further explored bootstrap stability analysis to improve reliability. To remove the effect of rigid transformation, all training shapes are aligned by Procrustes analysis before learning the shape model. We call this rigid transformation-free shape s in a reference frame. We apply rigid transformation to s to generate a shape x in the image frame:

(p2.1) where t point consists of N replications of t and s point denotes a rearranged 2 × N matrix with each column corresponding to one point in s. Similarly, x is the rearrangement of x point . An eigenspace shown in equation (3) can be represented by a quadruple: mean vector, matrix of eigenvectors, eigenvalues, and the number of observations to construct the eigenspace. Eigenspace fusion (Hall et al, 2000) merges two eigenspaces into one eigenspace, which has great significance for online updating. Butakoff and Frangi (Butakoff and Frangi, 2006) generalized the eigenspace fusion model (Hall et al, 2000) to a weighted version and applied it to merge multiple ASMs (or AAMs). Their experimental results show that fused ASMs have similar performance to full ASMs (model constructed from full set of observations) in terms of both segmentation error and time cost. They also applied the above fusion model to multi-view face segmentation (Butakoff and Frangi, 2010), which can be casted as a two-model fusion problem: the fusion of a frontal view model and a left profile model; and the fusion of a frontal view model and a right profile model. Faces in intermediate view can be interpolated through fusion weight estimation.
## (s3) Local Expert
(p3.0) A local expert functions to compute a response map on the local region around corresponding facial feature points, i.e. we have N local experts in a FFPD model. The region that supports a local expert could be either one-dimensional (i.e. a line) or two-dimensional (such as a rectangular region). A local expert can be a distance metric such as the Mahalanobis distance , a classifier such as linear support vector machine (Wang et al, 2008a), or a regressor (Cristinacce and Cootes, 2007;Saragih et al, 2009c).

(p3.1) Regarding ASM, Cootes et al (1995) defined the support region as the profile normal to the model boundary through each shape model point (see Fig. 4). Along the profile, k pixels are sampled from both sides of the model point in the i-th training image. Then 2k + 1 samples (actually gradients of these pixels) can be concatenated into a column vector g i . After being normalized by the sum of the absolute value of elements in the vector, the meanḡ and the covariance S g can be estimated from all training vectors {g i }. They adopted the multivariate Gaussian distribution assumption for the vectors. The fitting response for a new sample vector g s is given by

(p3.2) which is also known as the Mahalanobis distance of the sample vector from the model mean. The authors then provided a quantitative evaluation of the active shape model search using these local grey-level models (Cootes and Taylor, 1993). The aforementioned Mahalanobis distance-based methods assume that the local appearance is Gaussian distributed. This Gaussian distribution assumption does not always hold and thus may result in inferior performance. Classifier-based local experts separate aligned from misaligned locations, and so they ignore the local appearance variations. These experts are trained from positive image patches (centered at corresponding facial feature points) and negative image patches (with their centers displaced from the correct facial feature point positions). The linear support vector machine is frequently chosen due to its efficiency Wang et al, 2008a). Taking the local expert corresponding to the i-th facial feature point as an example:

(p3.3) where {w i , γ i } denote the gain and the bias, respectively and f represents the normalized patch vector with zero mean and unit variance. To reformulate the output of the classifier in a probability form, the logistic regression is employed to refine the equation (6):

(p3.4) Given an estimated shape, we can calculate the response map within the region around each facial feature point according to equation (7). Fig. 5 shows the response maps of 66 classifiers (specifically, linear support vector machines).

(p3.5) An alternative way to model the local expert is to exploit regressors instead of classifiers. Cristinacce and Cootes (2007) explored GentleBoost (Friedman et al, 2000) to learn a regressor from the local neighborhood appearance to the displacement between the center of the local neighborhood and the true facial feature point location. Saragih et al (2009c) claimed that a fixed mapping function (regressor) would take a complex form to incorporate the issues of generalizability and computational complexity. Considering a fixed mapping function cannot adapt to face variations in identity, pose, illumination and expression, they developed a bilinear model. Cootes et al (2012) introduced random forest (Breiman, 2001) to the CLM framework. Random forest learns response maps taking Haar-like features as the regressor input. PDM statistically models the shape models and regularizes the global shape configuration. The motivation behind the regressor rather than the classifier is that the regressor can potentially provide more useful information, such as the distance of negative patches from a positive patch, while classifiers only determine whether an image patch is positive or negative. However, learning a regressor is more difficult than constructing a classifier.
## (s5) Active Appearance Model
(p5.0) An active appearance model (AAM) (Gao et al, 2010) can be decoupled into a linear shape model and a linear texture model. The linear shape is obtained in the same way as in the CLM framework (see equation (3)). To construct the texture model, all training faces should be warped to the mean-shape frame by triangulation or thin plate spline method; the resultant images should be free of shape variation, called shape-free textures. Each shape-free texture is raster scanned into a greylevel vector z i . To eliminate the effect of global lighting variation, z i is normalized by a scaling u and offset v:

(p5.1) where u and v represent the variance and the mean of the texture z i respectively and 1 is a vector of all 1s with the same length as z i . The texture model can be generated by applying PCA on all normalized textures as follows:
## (s6) Improvements and Extensions
(p6.0) Due to the flexible and simple framework of AAM, it has been extensively investigated and improved. However, many difficulties are encountered when AAM is applied to real applications. These difficulties are generally encountered from the following three aspects: the low efficiency for real-time applications, the less discrimination for classification, and the lack of robustness under inconstant circumstances. As our previous work (Gao et al, 2010) does, we review the developments of AAM from these three aspects.
## (s7) Efficiency
(p7.0) Due to the high-dimensional texture representation and the unconstrained optimization, the original AAM suffers from low efficiency in real-time systems. We investigate improvements from these two aspects respectively. 1) Texture representation To reduce the redundancy information contained in the texture, Cootes et al (1998b) only subsampled a number of pixels. Pixels corresponding to a number of the largest elements in the regression matrix are assumed to be helpful and are preserved. This procedure decreases the dimension of texture representation. However, since the assumption is not always tenable, it cannot be guaranteed to obtain reasonable results.

(p7.1) Since learning the regression matrix in (21) or (23) is time-and memory-consuming, Hou et al (2001) learned the regression from the low-dimensional representation (PCA projection) of texture difference to the position displacement. Moreover, considering that the mapping from the texture to the shape is many-to-one, they proposed to linearly model the relationship between the texture and the shape to cater for this. Tresadern et al (2012) explored Haar-like features to provide a computationally inexpensive linear projection for efficiency to facilitate facial feature point tracking on a mobile device. To provide high accuracy, a hierarchical model that utilizes tailored training data is designed.

(p7.2) 2) Optimization In order to improve the efficiency of the fitting process, Matthews and Baker (2004) considered the AAM as an image alignment problem and optimized it by inverse compositional method (Baker et al, 2003) based on independent AAM. Here, independent AAM indicates that the linear shape model and linear texture model are not combined, as in the original literature. The method aims to minimize the following objective function:

(p7.3) where s (k) denotes any pixel location with the area enclosed by the mean shape s 0 , W W W(s (k) ; p) represents the pixel location after warping s (k) with a warp W W W(·; p), Q Q Q(s (k) ; q) has a similar meaning and a composition relation exists: Q Q Q • W W W(s (k) ; p, q). The advantage of the inverse compositional method is that in the fitting process, many variants such as the Jacobian matrix and the Hessian matrix can be precomputed.

(p7.4) The inverse compositional method has had many variants since its birth. It has been applied to solve the robust and efficient FFPD objective (Tzimiropoulos et al, 2011) which aims to detect points under occlusion and illumination changes. Gross et al (2005) proposed a simultaneous inverse compositional algorithm which simultaneously updates the warp parameters p and the texture parameters β β β. Moreover, they also claimed that:

(p7.5) (1) the person specific AAM is much easier to build and fit than the generic AAM (person-independent AAM) and can also achieve better performance; (2) the generic shape model is far easier to build than the generic texture model; (3) the origin of the idea that fitting the generic AAM is far harder than the person specific AAM lies in fact that the effective dimensionality of the generic shape model is far higher than that of the person-specific shape models. Papandreou and Maragos (2008) presented two improvements to the inverse compositional AAM fitting method to overcome significant appearance variation: fitting algorithm adaptation through fitting matrix adjustment and AAM mean template update by incorporating the prior information to constrain the fitting process. Saragih et al (2008) applied a mixed inverse-compositional-forward-additive parameter update scheme to optimize the objective subject to soft correspondence constraints between the image and the model. Amberg et al (2009) claimed that the inverse compositional method has a small convergence radius and proposed two improvements to enlarge the radius at the expense of it being four times slower and preserving the same time consumption respectively. Lucey et al. (Ashraf et al, 2010;Lucey et al, 2013) extended the inverse compositional method in the Fourier domain for image alignment and applied this method specifically to the case of AAM fitting (Navarathna et al, 2011). Tzimiropoulos et al (2012) proposed a generative model called active orientation model which is as computationally efficient as the standard projectout inverse compositional algorithm. Subsequently, Tzimiropoulos and Pantic (2013) proposed a framework for efficiently solving AAM fitting problem in both forward and inverse coordinate frames. Benefiting from the efficiency of proposed framework, they trained and fitted AAM in-the-wild and the trained model could achieve promising performance. Donner et al (2006) claimed that the multivariate regression technology explored in conventional AAM neglects the correlations between the response variables. This results in slow convergence (more iterations) in the fitting procedure. Since canonical correlation analysis models the correlations between response variables, it is employed to calculate a more accurate gradient matrix. Tresadern et al (2010) utilized an additive update model (boosting model, both linear and nonlinear) to substitute for the original linear predictors in AAM taking Haar-like features as the regression input. They found that the linear additive model is faster than original linear regression (Cootes et al, 1998a) but it preserves comparable accuracy and is also as effective as nonlinear models when close to the true solution. Therefore, they suggested a hybrid AAM which utilizes a nonlinear additive update model at the first several itera-tions and then a linear additive update model in the last several iterations.

(p7.6) Although the linear regression strategy achieves some success in obtaining the updated parameters, it is a coarse approximation of the nonlinear relation between texture residuals and warp parameters. When the parameters are initialized far away from the right place, this linear assumption is invalid. To this end, Saragih and Goecke (2007) deployed a nonlinear boosting procedure to learn the multivariate regression. Each parameter is updated by a strong regressor consisting of an ensemble of weak learners (Friedman, 2001). Each weak learner is fed with Haar-like features to output the parameter. This nonlinear modeling results in a more accurate fitting than linear procedures. Liu (2007Liu ( , 2009) explored GentleBoost classifier (Friedman et al, 2000) to model the nonlinear relationship between texture and parameter updates. A strong classifier consists of an ensemble of weak classifiers (arctangent functions). Haar-like rectangular features are fed into each weak classifier. The goal of the fitting procedure is to find the PDM parameter updates which maximize the score of the strong classifier. Zhang et al (2009) utilized granular features to replace the rectangular Haar-like feature to improve computational efficiency, discriminability and a larger search space. In addition, they explored the evolutionary search process to overcome the deficiency searching problem in the large feature space. Because the weak classifier in Liu (2007Liu ( , 2009) is actually utilized to classify the right PDM parameters from the wrong ones, it cannot guarantee that the fitting objective will converge to the optimum solution. Consequently, instead of discriminatively classifying corrected alignment from incorrect alignment, Wu et al (2008) learned classifiers (GentleBoost) to determine whether to switch from one shape parameters to another parameter corresponding to an improved alignment. Based on the ranking appearance model (Wu et al, 2008), Gao et al (2012) preferred to use gradient boosted regression trees (Friedman, 2001) instead of GentleBoost classifiers. Modified census transform features and pseudo census transform features (Gao et al, 2011) are fed to the regression trees.

(p7.7) Sauer et al (2011) compared the performance of linear predictor, boosting-based predictor and random regression-based predictor. Their experimental results illustrate the random regression-based method achieves the best generalization ability. Furthermore, it can achieve performance that is as efficient as boosting procedures without significant reduction in accuracy.
## (s8) Discrimination
(p8.0) Regarding discrimination, here we mainly refer to the ability to accurately fit a model to an image (Gao et al, 2010). Many aspects may affect this, such as prior knowledge, texture representation and nonlinear modeling the relation between texture residuals and parameters.

(p8.1) Instead of simply minimizing a sum of square measure,  reformulated the AAM problem in a MAP form p(p|I) = p(I|p)p(p) is a zeromean Gaussian with covraiance matrix S −1 p , the MAP problem can be simplified in a log-probability form to minimize the following problem:
## (s10) Regression-Based Methods
(p10.0) The aforementioned categories of methods mostly govern the shape variations through certain parameters, such as PDM coefficient vector α α α in ASM and AAM. By contrast, regression-based methods directly learn a regression function from image appearance (feature) to the target output (shape):

(p10.1) where M denotes the mapping from image appearance (feature) F(I) to the shape x and F is the feature extractor. Haar-like features (Viola and Jones, 2004), SIFT (Lowe, 2004), local binary patterns (LBP) (Ojala et al, 1996) and other gradient-based features are generally used feature types. Zhou and Comaniciu (2007) proposed a shape regression method based on boosting (Freund and Schapire, 1997;Friedman et al, 2000). Their method proceeds in two stages: first, the rigid parameters are found by casting the problem as an object detection problem which is solved by a boosting-based regression method; secondly, a regularized regression function is learned from perturbed training examples to predict the non-rigid shape. Haar-like features are fed to the non-rigid shape regressors. Kozakaya et al. (Kozakaya et al, 2008a,b, 2010 proposed a weighted vector concentration approach to localize facial features without any specific prior assumption on facial shape or facial feature point configuration. In the training phase, grid sampling points are evenly placed on each face image and an extended feature vector is extracted for each sampling point of each training image. The extended feature vector is composed of histograms of oriented gradients (HOG descriptor (Dalal and Triggs, 2005)), N directional vectors from the sampling point to all N feature points, and local likelihood patterns at the feature points. In the detection phase, given an input face image, local descriptors corresponding to each sampling point are extracted. Then a nearest local pattern descriptor can be found for each sampling point of the input image among the descriptors located at the same position of training images using the approximate nearest neighbor search (ANNS) algorithm (Arya et al, 1998). Simultaneously, a group of directional vectors and local likelihood patterns can also be obtained. Finally, feature points are computed from a weighted square distance from the point to the line through sampling points and the directional vector. Each facial feature point can be detected independently after all nearest neighbors are found by the ANNS method. This paper does not take faces with different expressions into consideration in their experiments.

(p10.2) In consideration of the nonlinear property of the facial feature localization problem and generalization ability,  deployed support vector regression to output the target point location from the input local appearance-based features (Haar-like features). To overcome the overfitting problem due to the high dimensionality of the Haar-like features, Adaboost regression is utilized to perform feature selection.

(p10.3) Kazemi and Cullivan (2011) divided a face into four parts: eyes (left and right), nose and mouth. Several regression strategies such as ridge regression, ordinary least squares regression, principal component regression were then explored to regress the local appearance (a variant of HOG descriptor) of each part to the target landmark location. Their experimental results illustrate that these several regression methods, ridge regression achieves the best performance. Moreover, their method has comparable performance as to AAM methods but is more robust.

(p10.4) Cao et al (2012) proposed a two-level cascaded learning framework (see Fig. 6) based on boosted regression (Duffy, 2002). Unlike the above method which learns the regression map of each landmark of those landmarks that correspond to the same component, this method directly learns a vectorial output for all landmarks. Shape-indexed features such as that in (Dollar et al, 2010) are extracted from the whole image and are fed into the regressor. To reduce the complexity of feature selection but still achieve reasonable performance, the authors further proposed a correlation-based feature selection strategy. Each regressor (R t in Fig. 6, t = 1, · · · , T ) in the first level consists of cascaded random fern regressors (r k in Fig. 6, k = 1, · · · , K) (Ozuysal et al, 2010) in the second level. This method achieves state-of-the-art performance in a very efficient manner. In particular, it achieves the highest accuracy on the LFPW database: labeled face parts in the wild database (Belhumeur et al, 2011), images of which are taken under uncontrolled conditions.

(p10.5) Considering the method (Cao et al, 2012) is not robust to occlusions and large shape variations, Burgos-Artizzu et al (2013) improved it from three aspects. First, Cao et al (2012) references pixel by its local coordinates with respect to its closest landmark, which is not enough against large pose variations and shape deformation. Burgos-Artizzu et al (2013) proposed to reference pixels by linear interpolation between two landmarks. Secondly, Burgos-Artizzu et al. presented a strategy to incorporate the occlusion information into the regression which improves the robustness to occlusion. Thirdly, they designed a smart initialization restart scheme to deploying the similarity between different predictions resulted from different initializations. Experimental results on several existing databases in the wild and their newly constructed database illustrate the proposed method achieves state-of-the-art performance.

(p10.6) In view of the boosted regression in Cao et al (2012), which is a greedy method to approximate the function mapping from facial image appearance features to facial shape updates, Xiong and De la Torre (2013) developed the supervised descent method (SDM) to solve a series of linear least squares problems as follows:

(p10.7) where x i * = x i * − x i k is the ground truth difference between the truth shape x i * of the i-th training image d i and the shape x i k obtained from the k-th iteration, φ φ φ i k is the extracted SIFT features around the shape x i k on the training image, R k is called the common descent direction in this paper and b k is a biased term. This method has a natural derivation process based on the Newton method. A series of {R k , b k } are learned in the training stage, and in the testing stage they are applied to the SIFT features extracted from the testing image to update the shape sequentially. SDM efficiently achieves comparable performance to (Cao et al, 2012) on database LFPW (Belhumeur et al, 2011). Martinez et al (2013) believed that each image patch evaluated by the regressors adds evidence to the target location rather than just taking the last estimate (the last iteration) into account and discarding the rest of these estimates. They aggregated all up-to-date local evidence obtained from support vector regression by an unnormalized mixture of Gaussian distributions. LBP is deployed as the local texture descriptor and a correlation-based feature selection method is introduced to reduce the dimensionality of LBP features. Dantone et al (2012) proposed a facial feature point detection by extending the concept of regression forests (Breiman, 2001;Criminisi et al, 2012) to conditional regression forests. They claimed that it is difficult for general regression forests to learn the variations of faces with different head poses. The head pose is evaluated by regression forests. A regression forest is constructed, conditioned on the head pose (i.e. there is one regression forest corresponding to each head pose). In the testing phase, the probabilities of the head pose of an input testing image should be first calculated and, according to this distribution, the number of trees selected from each forest can be determined. Finally the position of each facial feature point can be computed through solving a mean-shift problem. Yang and Patras added structural information into the random regression and proposed a structured-output regression forest-based face parts localization method (Yang and Patras, 2012). Then, they (Yang and Patras, 2013) pro-posed to deploy a cascade of sieves to refine the voting map obtained from random regression forest. Rivera and Martinez (2012) casted the facial feature point detection problem as a regression problem. The input of a regressor consists of features extracted from input images, either pixel intensities or C1 features (Serre et al, 2007). The output of a regressor is PDM coefficients (shape parameters). The regressor is either a kernel ridge regression or -support vector regression. Their experimental results show that kernel ridge regression with pixel intensities achieves the best performance when images have a low resolution.

(p10.8) Considering the fact existing parameterized appearance models do not sample parameter space uniformly, which may result in a biased model, Sanchez-Lozano et al (2012) proposed a continuous regression method to solve this biased learning problem. Instead of discretely sampling the parameter space, this method directly integrates on the parameter space. A closed-form solution can be achieved. To alleviate the small sample size problem, the closed-form solution is further projected onto the principal components.
## (s12) Graphical Model-based Methods
(p12.0) Graphical model-based FFPD methods mainly refer to tree-structure-based methods and Markov random field (MRF)-based methods. Tree-structure-based methods take each facial feature point as a node and all points as a tree. The locations of facial feature points can be optimally solved by dynamic programming. Unlike the tree-structure which has no loop, MRF-based methods model the location of all points with loops. Coughlan and Ferreira (2002) developed a generative Bayesian graphical model that deployed separate models to describe shape variability (shape prior) and appearance variations (appearance likelihood) to find deformable shapes. The shape prior takes the location of each facial feature point and these points' normal orientation as a node in MRF. An edge map and an orientation map are calculated to model the appearance likelihood. A variant of the belief propagation method is utilized to optimize the problem. MRF has been also explored to constrain the relative position of all facial feature points obtained from the regression procedure in ; Martinez et al (2013). Gu et al (2007) learned a sparse Gaussian MRF structure to regularize the spatial configuration of face parts by lasso regression.

(p12.1) Unlike the method in Coughlan and Ferreira (2002) which models the shape prior only in a local neighborhood, Liang et al (2006a) proposed a method that incorporates a global shape prior directly into the Markov network. The local shape prior is enforced by denoting a line segment as a node of the constructed Markov network. Here, line segments draw from one facial point to another neighboring point. Subsequently, Liang et al (2006b) claimed that although CLM-based methods take the global shape prior into account, these methods neglect the neighboring constraint between points since they compute the response map of each point independently. Based on the thought in Liang et al (2006a), Liang et al. further incorporated the PDM shape prior into their model.
## (s13) Joint Face Alignment Methods
(p13.0) Joint face alignment jointly aligns a batch of images undergoing a variety of geometric and appearance variations (Zhao et al, 2011), motivated by the congealingstyle joint alignment method (Learned-Miller, 2006) and sparse and low-rank decomposition method (Peng et al, 2012). Zhao et al (2011) designed a joint AAM by assuming that the images of the same face should lie in the same linear subspace and the person-specific space should be proximate the generic appearance space. The problem is formulated as a nonlinear problem constrained by a rank term which can be transformed to a nuclear norm. An augmented Lagrangian method is explored to optimize the nonlinear problem. Smith and Zhang (2012) stated that the method (Zhao et al, 2011) breaks down under several common conditions, such as significant occlusion or shadow, image degradation, and outliers. Considering the fact that a non-parametric set of global shape models (Belhumeur et al, 2011) results in excellent facial feature point localization accuracy on facial images undergoing significant occlusions, shadows, and pose and expression variation, they introduced the same shape model combined with a local appearance model into the joint alignment framework.

(p13.1) Different from the aforementioned two joint face alignment methods, which both incorporate the rank term into the objective, Zhao et al (2012) proposed a novel two-stage approach to align a set of images of the same person. The initial facial feature point estimation is first computed by an off-the-shelf approach (Gu and Kanade, 2008). To distinguish the "good" alignments from the "bad" ones among all these initial estimations, a discriminative face alignment evaluation metric is designed by virtue of cascaded AdaBoost framework (Viola and Jones, 2004) and Real AdaBoost (Friedman et al, 2000). Selected "good" alignments are utilized to improve the accuracy of "bad" ones through appearance consistency between the "bad" estimate and its selected K neighboring "good" estimates. Tong et al (2009Tong et al ( , 2012 proposed a semi-supervised facial landmark localization approach which utilizes a small number of manually labeled images. Their objective function is to minimize the sum of squared error of two distances: the distance between the labeled and unlabeled images, and the distance between the unlabeled images. To obtain a reasonable shape, an on-line learned PDM shape model is imposed as a constraint.

(p13.2) To further improve the preciseness of the above model, they perform the above procedures in a coarse-to-fine manner, which proceeds by dividing the whole face into patches with different sizes at different levels.
## (s14) Independent Facial Feature Point Detectors
(p14.0) The aforementioned methods predict the locations of all facial feature points or a group of points simultaneously. There are other methods which detect each point independently. Here, methods which do not rely on manually labeled images, such as approach (Asteriadis et al, 2009), are not included. Vukadinovic and Pantic (2005) detected each point by a local expert as utilized in CLM-based methods.

(p14.1) Here, Gabor feature-based boosted classifier is utilized to classify the positive image patch from the negative image patch. The position with the peak response among the response map of each point is the sought location. Shen et al (2013) proposed the detection of each facial feature point through a voting strategy on corresponding points on some exemplar images retrieved from the training dataset. The location corresponding to the peak response in each voting map is the estimated position.
## (s18) Comparisons and Discussions
(p18.0) The distance from the estimated points to the ground truth normalized by the inter-ocular distance and the number of points is a common informative metric for evaluating a facial feature point detection system (named mean normalized error, MNE, in the following text). Sometimes the figure of the proportion of testing images with the increase of MNE is plotted as a comparison metric among different approaches. The performance of facial feature point detection methods cannot be verified by experimenting on each database listed as Table 3 shown since there are too many databases. Table 4 shows the published performance of representative methods of aforementioned categories on several different databases.

(p18.1) To further illustrate the characteristics of various categories of methods, we have collected some software published online and listed them as shown in Table 5. Eight representative methods were chosen for study: DRMF-CLM (Asthana et al, 2013), OPM-CLM (Yu et al, 2013), FF-AAM (Tzimiropoulos and Pantic, 2013), CNN-DL (Sun et al, 2013), the graphical model (GM) method (Zhu and Ramanan, 2012), BorMan-Regression , SDM-Regression (Xiong and De la Torre, 2013), and RCPR-Regression (Burgos-Artizzu et al, 2013). We localized FFPs in three databases, COFW (Burgos-Artizzu et al, 2013), LFPW (Belhumeur et al, 2011), and Helen (Le et al, 2012), using the published software. The 68 re-annotated landmarks of the "300 Faces in-the-Wild Challenge" were used as the ground truth for images in LFPW and Helen. For COFW, we used the augmented version presented in (Burgos-Artizzu et al, 2013), which contains 1345 training images and 507 test images. Examples from these three databases are shown in Fig. 7.

(p18.2) Since only the trained models, and not the source code, were published in some cases, it was difficult to make equitable comparisons (for example, some software contained different face detectors). In addition, different methods labeled different numbers of facial landmarks (see Table 6). In Table 6, "any" denotes that the authors published the training code, and thus the models could be trained for different numbers of facial landmarks. Face detection rates were quantified according to the percentage of detected faces being labeled in the corresponding database. "GM-99" and "GM-1050" indicate graphical models composed of 99 and 1050 parts, respectively. Errors were measured as the percentage of the interocular distance d io , as shown in equation (31), i.e., the mean normalized error (MNE), where x e (i) is the i-th estimated point and x g (i) is its corresponding ground truth: Fig. 8, Fig. 9, and Fig. 10 show the cumulative error curves for the above three databases. It can be seen that CNN (Sun et al, 2013) achieves promising performance on all three databases. There are two main reasons for this: first, deep learning is highly capable of performing feature learning followed by classification or detection, especially when there are many training samples (CNN utilizes approximately ten thousand training samples); secondly, CNN detects five characteristic points: the center of the two pupils, the nose tip, and the two eye corners, which are relatively easy to detect. The cascaded regression method, SDM (Xiong and De la Torre, 2013), also achieves good performance for detecting 49 facial points distributed around the eyebrows, eyes, nose, and mouth, and without points around the outline of the face. RCPR, another cascaded regression method, also appears promising, although inferior to SDM; this is likely to be because SDM fails to detect several difficult test images and detects 49 points without the facial outline. Table 7 shows a comparison of the normalized error of RCPR retrained on 49 points on the same faces detected by SDM. The model could not be retrained on COFW, since 29 points label the CMU Multi-PIE2008 -CMU Multi-PIE (Gross et al, 2010) face database was collected in four sessions between October 2004 and March 2005. It aims to support the development of algorithms for recognition of faces across pose, illumination and expression conditions. This database contains 337 subjects and more than 750,000 images for 305 GB of data. A total of six different expressions are recorded: neutral, smile, surprise, squint, disgust and scream. Subjects were recorded across 15 views and under 19 different illumination conditions. A subset of this database has been labeled either 68 points or 39 points depending on their view but landmarks are not published online. Details on obtaining this dataset can be found at: http://www.multipie.org.

(p18.3) Extended M2VTS database1999 (XM2VTS) -XM2VTS database (Messer et al, 1999) collected 2,360 color images, sound files and 3D face models of 295 people. The database contains four recordings of these 295 subjects taken over a period of four months. Each recording was captured when the subject was speaking or rotating his/her head. This database is available on request at: www.ee.surrey.ac.uk/CVSSP/xm2vtsdb/. These 2,360 color images are labeled with 68 landmarks and are published online: http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/data/xm2vts/ xm2vts_markup.html.

(p18.4) AR1998 -AR database (Martinez and Benavente, 1998) contains over 4,000 color images corresponding to the faces of 126 people (70 men and 56 women). Images were taken under strictly controlled conditions and with different facial expressions, illumination conditions, and occlusions (sunglasses and scarf). Each person appeared in two sessions, separated by two weeks. Ding and Martinez (Ding and Martinez, 2010) manually annotated 130 landmarks on each face image which have been published online with the database: www2.ece.ohio-state.edu/~aleix/ARdatabase.html.
## (s19) Databases in the wild:
(p19.0) BioID2001 -BioID database (Jesorsky et al, 2001) was recorded in an indoor lab environment, but "real world" conditions were used. This database contains 1,521 grey level face images of 23 subjects and each image is labeled with 20 landmarks. This database is available at: http://www.bioid.com/index.php?q=downloads/software/bioid-face-database.html.

(p19.1) LFW2007 -LFW database (Huang et al, 2007a) contains 13,233 face images of 5,749 subjects collected from the web. Each face in the database has been labeled with the name of the person pictured. 1,680 of the people pictured have two or more distinct photos in the data set. The constructors of this database did not provide manually labeled landmarks but there are other available sites: (Michal et al, 2012) http://cmp.felk.cvut.cz/~uricamic/flandmark/(7landmarks); (Dantone et al, 2012) http://www.dantone.me/datasets/facial-features-lfw/(10landmarks).

(p19.2) Annotated Facial Landmarks in the Wild 2011(AFLW) -AFLW database (Kostinger et al, 2011) is a large-scale, multi-view, real-world face database with annotated facial feature points. Images were collected from Flickr using a wide range of face relevant key words such as face, mugshot, and profile face. This database includes 25,993 images in total and each image is labeled with 21 landmarks. It is available at: http://lrs.icg.tugraz.at/research/aflw/.

(p19.3) Labeled Face Parts in the Wild 2011 (LFPW) -LFPW database (Belhumeur et al, 2011) is composed of 1,400 face images (1,100 as the training set and the other 300 images are taken as the testing set) downloaded from the web using simple text queries on websites such as Google.com, Flickr.com, and Yahoo.com. Due to copyright issues, the authors did not distribute image files but provided a list of image URLs. However, some image links are no longer available. 35 landmarks are labeled in total;29 of them are usually utilized in literatures. More information can be found at: http: //homes.cs.washington.edu/~neeraj/databases/lfpw/.

(p19.4) Annotated Faces in the Wild 2012 (AFW) -AFW database (Zhu and Ramanan, 2012) contains 205 images with a highly cluttered background and large variations both in face scale and pose. Each image is labeled with 6 landmarks and the bounding box of the corresponding face. The dataset is available at: http://www.ics.uci.edu/~xzhu/face/.

(p19.5) Helen2012 -Helen database (Le et al, 2012) contains 2,300 high resolution face images collected from Flickr.com. Each face image is labeled with 194 landmarks. More information about this database can be found at: http://www.ifp.illinois. edu/~vuongle2/helen/.

(p19.6) 300 Faces in-the-Wild Challenge (300-W) 2013 -300-W database is a mixed database consisting of face images from several published databases (LFPW, Helen, AFW, and XM2VTS) and a new collected database IBUG. All these images are re-annotated with 68 landmarks. This database is published for the first Automatic Facial Landmark Detection in-the-Wild Challenge (300-W 2013) held in conjunction with the International Conference on Computer Vision 2013. This database is available at: http://ibug.doc.ic.ac.uk/resources/300-W/.

(p19.7) Caltech Occluded Faces in the Wild (COFW) 2013 -COFW database (Burgos-Artizzu et al, 2013) is composed of 1,007 face images showing large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food, hands, microphones, etc.). 29 points are marked for each image. The major difference between this database and other ones is that each landmark is explicitly labeled whether it is occluded. This database presents a great challenging task for facial feature point detection due to the large amount and variety of occlusions and large shape variations. This database is available at: http://www.vision.caltech.edu/xpburgos/ ICCV13/. Valstar  FERET (Phillips et al, 2000)+MMI  (360 + 40) 5.11 22
