# A Survey on Common Threats in npm and PyPi Registries

CorpusID: 237266468 - [https://www.semanticscholar.org/paper/b56f25674afe1140497a8126ab44ddca0059ff86](https://www.semanticscholar.org/paper/b56f25674afe1140497a8126ab44ddca0059ff86)

Fields: Engineering, Computer Science

## (s0) Introduction
(p0.0) The OS movement allows any developer to read or attempt to contribute to the source code hosted in a publicly accessible location, such as GitHub. This trend may have several advantages, such as code reviewers catching software bugs promptly. Furthermore, the creativity behind a software's design is no longer limited to a team of developers as anyone is free to propose their ideas. This advantage allows additional features or functionalities to be implemented, thus possibly making the project the market leader in its area.

(p0.1) Although several advantages are present with the OS movement, there are also disadvantages. One possible disadvantage is that certain individuals in public may have malicious intentions, and the publicly accessible code may raise security concerns. A team of founders of the OS platform may review every pull request to ensure the benign intentions of the commits. However, some environments are known to lack control measures. Some of these environments may include public registries, which are public repositories anyone can upload their packages to for others to use. These repositories intend to serve as collections of libraries to facilitate development tasks with readily available packages. Some public registries do not have any control measurements besides reporting the malicious code once detected by a developer. This issue gives the attacker a certain lifespan to perform malicious activities.

(p0.2) PyPi and npm are two popular public registries. These environments require developers to verify their email addresses without further control, making it much easier for attackers to create malicious accounts. These accounts can publish any package in their public registry, amplifying an attack's impact naturally. Furthermore, attackers can take advantage of several other techniques that benefit from human errors. Some of these techniques are typosquatting and combosquatting. When an attacker intentionally publishes a package in the public registry with a similar name to popular packages, these techniques benefit from a potential typo during the manual installation of dependencies [35]. Typosquatting takes advantage of typos during the installation of packages, while combosquatting hopes to exploit a mistake of the order in the package name, where the name consists of multiple nouns, such as "python-nmap" typed as "nmap-python" [35]. The ultimate goal in both attacks is to get the developer to install the malicious package.
## (s4) Software Supply-Chain Attacks
(p4.0) The goal of supply-chain attacks is to inject malicious code into software or library externally. These attacks modify the targeted program so that it is still validly signed by its owner. The attacker simply injects his code into one of the software's dependencies [21]. Theoretically, an attacker can do this injection in any given node in the software's dependency tree. The ultimate goal is to alter the behavior of the root or a specific node, which is the targeted software product, using a child node, which in this case is a dependency node in the tree [21].
## (s5) Typosquatting and Combosquatting
(p5.0) Typosquatting is an attack methodology where an attacker intentionally publishes a package with a typo in its name, making the name relatively close to a popular package's name. The attacker hopes that a victim would make a typo while downloading dependencies. Thus, the victim would download the malicious package instead of the intended one. One example of this type of attack is the popular lodash package incident, where it had a typosquatting package named loadsh. In the English language, a developer might likely download the loadsh package after making a typo.

(p5.1) Combosquatting is similar to typosquatting; however, some packages consist of multiple words in their name. Sometimes, it would be easy to forget the order of the words, and the attacker intentionally creates a package that contains the same words as the targeted package, but the order of the words will vary. The goal is similar to typosquatting, benefiting from an innocent mistake of a developer. A researcher uploaded typosquatting packages to different repositories, including PyPi, and the package received around 45K downloads over several months [35].
## (s6) Machine Learning
(p6.0) Learning is considered the hallmark of human intelligence, and researchers have worked on making machines intelligent, giving birth to ML [36]. It is the study of methods to allow computers to gain new knowledge and constantly improve themselves [36]. The definition can also be the process that allows computers to convert data to knowledge through certain techniques [36]. The applications of the ML techniques have been quite popular in recent years, and one of these applications is the area of information security. We will incorporate some ML techniques into our proposed security remedies to provide a more reliable method of identifying and preventing malicious actors.
## (s9) Direct, Indirect Dependencies, and Heavy Code Reuse
(p9.0) As stated before, code reuse is common in the npm environment. Projects are either directly or indirectly dependent on each other, meaning that project A that uses code from project B has a direct dependency relationship as A directly depends on B. If B depends on project C, then A has an indirect dependency on C, increasing the attack surface. Indirect dependencies are also referred to as "transitive dependencies" as some of the work in the literature uses that term to build their graphs, such as figure 3 that indicates the average number of dependencies projects use over time.  Figure 3 implies that a developer implicitly trusts around 80 other packages by using a library [41]. This trust also means that a hacker's attack surface includes the accounts that maintain the indirectly dependent packages.

(p9.1) Another term based on transitive dependency is the "Package Reach," which refers to the number of transitive dependencies a specific library has [41]. It has been stated that some popular packages have a package reach of over 100000, drastically increasing the attack surface, and this trend has been observed to be increasing [41]. The package reach also increases the size of the dependency graph. If one of the dependent packages is compromised by either a squatting attack or a stolen credentials of a maintainer account, it would be reasonable to assume that a security risk would arise. The average package reach for a single given library seems to be in an increasing trend for the last couple of years, according to figure 4. Once the attacker injects his code into a dependency graph, he can execute it in various phases. The attack tree that outlines wherein an attacker can execute the malicious script is also illustrated in figure 5.

(p9.2) Zimmerman's 2019 USENIX paper seems to be a leading study examining the interdependent structure of packages and their trends in the npm environment. However, it has also been determined that most papers in this field seem to be The attack tree that outlines the possible phases a malicious script can be executed [21] . less than five years. This finding implies that security experts recently started to investigate the interdependent packaging structure of OS environments. Software supply-chain attacks, on the other hand, seemed to be a more mature field.
## (s10) Technical Lag
(p10.0) Packages relying on vulnerable libraries is not the only issue, as Gonzalez-Barahona et al. introduced the concept of technical lag, which is "the increasing lag between upstream development and the deployed system if no corrective actions are taken" [14,40]. It is essentially the phenomenon where a package lags behind its most recent release [6]. Code reuse only seems to be contributing to the issue as most recent packages can still be outdated if they rely on unpatched components [40].

(p10.1) One may reasonably want the package updates to be automated to solve the issue. Package updates may introduce incompatibility issues with the existing project [40]. Developers should not ignore patches as well due to known vulnerabilities. Patches also introduce new functionalities that a project misses by avoiding incompatibility risks, causing a concept named "technical debt" to arise [14]. However, the issue mainly seems to be a trade-off between security and functionality.

(p10.2) The perspective against technical lag also seems to vary as the literature contains data implying that clients can be safe from technical lag. It has been concluded that one in four dependencies and two in five releases suffer technical lag in the npm environment [6]. After 2015, it has also been observed that the average duration of technical lag in npm was 7 to 9 months, meaning that a specific dependency would be updated 7 to 9 months after a library's new release [6].

(p10.3) Furthermore, benign users might not be the ones to discover a vulnerability first. Most vulnerabilities take a long time to be discovered, especially lowseverity ones [7]. Although project contributors fix most vulnerabilities discovered in the source code in a short amount of time, a non-negligent portion takes longer [7].

(p10.4) To counter the dangers of the technical lag argument, we found one report stating that 73.3% of outdated clients were safe from threats as these clients did not use the specific vulnerable code [38]. A vulnerability tends to be in a specific function, and not every client will have to use that particular function. However, the risk of a client using the vulnerable function persists. Developers who realize the functions they are using are not particularly affected by the stated issue take longer to update their dependencies, increasing the technical lag [38].

(p10.5) In addition, One may reasonably conclude that the older the package is, the less vulnerability it will have. Thus, a possible solution may be to use old packages. However, a study in the literature has found that this claim is wrong as they found that most vulnerabilities are in packages older than 28 months [7]. Older packages tend to have more severe vulnerabilities [7]. We could not establish a relationship between security and the package's age in this study.
## (s11) Squatting Attacks
(p11.0) We found that attackers mainly use two methods to spread malicious code in these environments: stealing the credentials of accounts that maintain certain packages to inject their code into the project and tricking users into downloading their packages by methods such as squatting attacks [35]. The term squatting attack is an umbrella concept used in this paper to represent typosquatting and combosquatting. We will discuss other types of attack methods against the public registries in this paper, but we focus on these two scenarios in this section. In the first scenario, the "ssh-decorate" package was affected as the attackers took control of the maintainer account and injected code into the package. The injected code would send users' ssh credentials to an attacker-controlled remote server [35]. Some popular maintainers can contribute to hundreds of packages, making their accounts critical to a potential compromise [41].
## (s14) Trivial Packages or Micropackages
(p14.0) Trivial packages are small libraries, which a study in the literature found to be less than 35 lines of code, according to 79% of their participants who attempted to classify libraries as trivial [2]. Developers have considered trivial packages good until recent trends pushed code reuse to an extreme [2]. The breakdown of popular web services, such as Facebook, Airbnb, and Netflix in 2016 from a Node.js 11-line trivial package named left-pad, only made the concept be questioned further [1,5].

(p14.1) The incident has been referred to as the case that "almost broke the Internet," which led to many discussions over code reuse sparked by David Haney's blog post "Have We Forgotten How to Program?" [1]. Node.js used to allow developers to unpublish projects that lead to the left-pad incident emerge [1]. Although the origin of the incident is not related to a vulnerability in the package itself, it has raised awareness about trivial packages [1]. Numerous developers agreed with Haney's opinion that developers should implement trivial tasks themselves rather than adding an extra dependency to the project [1]. Since then, people have been working to investigate this issue.

(p14.2) A study has researched the developers' perspectives of trivial packages or micropackages, and the interviewed developers stated that their definition of micropackage is the same across PyPi and npm [2]. It has been found that 16% of packages in the npm and 10.5% of packages in PyPi are trivial packages [2]. The same study also questioned developers to understand the reasons for using such packages. It has been found that developers use these packages due to their belief that the packages are well tested and maintained [2]. Developers stated that they use these packages to increase their productivity [1]. The surveyed software developers also stated that they use trivial packages as they do not want to be concerned with extra indirect dependencies. However, it has also been found that trivial packages do use dependencies in some cases [2]. A study has found that 43.7% of trivial packages have at least one dependency, and 11.5% have more than 20 dependencies [1]. The surveyed developers also thought that trivial packages create a dependency mess, which is hard to update and maintain [1]. The perspective against trivial packages seems to be controversial, as it has been found that 23.9% of the JavaScript developers consider them bad. In comparison, 57.9% of the developers do not consider them to be a bad practice [1]. 70.3% of the Python developers consider trivial packages as bad practice [2].

(p14.3) Zimmermann's paper coins this issue as "micropackages" instead of using the term "trivial package." Still, the concepts are the same: packages with fewer lines of source code than a threshold, although this threshold seems ambiguous across the literature [41]. The specific study explicitly stated that micropackages are insecure as it increases the attack surface and the number of dependencies a project has [41].
## (s15) PyPi Overview
(p15.0) PyPi has limited automated review tools for the uploading process as the npm environment does, making it vulnerable to different kinds of attacks, such as squatting [35]. Furthermore, the moderator and administrator team, who has permission to remove packages from the registry, seems to be less than ten people, limiting the maximum number of code reviews they can conduct [35]. Considering the 400K package owner, each administrator seems to be responsible for 40K people, assuming every administrator performs code review for malicious content, thus providing a lower bound for the number of package owners per moderator ratio. PyPi allows end-users to report malicious packages. Nevertheless, considering each moderator being responsible for at least 40K developers, it would be only reasonable to question the efficiency of the code reviews. When users download packages using the pip, there is no available system that reviews the code to determine its safety aside from a user's antivirus. So, we can outline the process of publishing packages with figure 7 that illustrates a high-level view of the schematics of the PyPi ecosystem. Fig. 7: An overview of the roles in the PyPi ecosystem [35] Spreading malicious code in the wild PyPi is fairly similar to the process we illustrated in the npm environment. Mainly, an attacker can either steal the credentials of an existing account to exploit the current reputation of the project or create a new package by forking the targeted package and modifying the content, or simply creating a brand new package [35]. The latter method can still trick users into downloading the attacker's library by a squatting method.
## (s18) Suggested Countermeasures
(p18.0) We have developed countermeasures based on the ideas we gained from our literature search. We specifically attempted to incorporate ML techniques into our approach. We took this step as the offered solutions will only need minor modifications to features to cover new undiscovered future attacks. This advantage of ML will make the presented techniques more adaptable. Furthermore, although signature-based malware detection is the most extended technique in commercial antivirus, they tend to fail as newer types of malware emerge [24,26]. Artificial intelligence provides a competitive advantage over next-generation malware [26].

(p18.1) Therefore, when designing our countermeasures for public registries, we incorporated ML techniques. We have not tested the solutions yet, so the feasibility of using them may be in question. Another option can be contacting the admin teams of the public registries to gain their inputs. However, we hope that this work would act as a first step to spark interest in this field and capture the attention of the admin teams of OS registries.

(p18.2) Malicious Package Detection Some sources state that current tools for identifying malicious payloads are resource-demanding. However, there are lightweight tools in the literature to mitigate the risk of malware residing in the registries. These tools can be automated scanning programs to analyze the dependency tree for squatting packages [34]. The automation of reviewing published code may prove to be challenging. Nevertheless, allowing any developer to publish code into the public registry, where millions of developers have access, gives much power to any stranger.

(p18.3) Since the source code for every project on an OS registry is public, any user can review the code for each library. This fact is advantageous as the dependency tree and the source code of the project is visible. One antivirus designed for APIs can review packages on the registry to eliminate the time needed for the public to uncover malicious software. A member of the admin team or a volunteer can download batches of packages from the public registry to his local repository and use the antivirus to ensure the safety of each library. However, we have not found an antivirus software specifically designed to scan APIs on the internet. Antiviruses may use features that work on software for end-users. The same features may not prove efficient for libraries designed for implementing programs.

(p18.4) For this purpose, we will explore antivirus software that may be useful specifically for APIs. We will focus on analyzing the binary and the behavior of the library instead of its source code. We decided to cover a broader range of use cases, as we believe analyzing the source code is less challenging than an executable binary. Thus, our project can also cover library binaries for malware detection.

(p18.5) There has been work in the field to efficiently automate the detection of malicious code injection in the distributed artifacts of packages, and the admins may attempt to implement some of these novel tools [34]. One such tool, named Buildwatch, analyzes the third-party dependencies by using the simple assumption that malicious packages introduce more artifacts during installation than benign libraries [22]. This hypothesis has been formulated and tested in the Buildwatch study [22]. Admins can also modify Buildwatch to detect squat-ting packages in the dependency tree with the techniques we will discuss in the upcoming sections.

(p18.6) One approach in detecting suspicious packages comes from a study that uses anomaly detection to identify suspicious software [13]. Their approach first extracts several features from the version when a package gets updated. It then performs k-means clustering to detect outliers for further review. The researchers kept the number of features low to ensure the clustering algorithm takes limited computation power, thus making it lightweight [13]. Some extracted features are the ability to send/receive HTTP requests, create/read/write to file systems, or open/listen/write to sockets [13]. This tool specifically uses anomaly detection, a branch of ML, to tackle the issue of detecting suspicious updates. We believe this tool is very suitable for analyzing third-party libraries.

(p18.7) As image classification becomes more accurate due to the development of robust classifiers such as convolutional neural networks (i.e., CNN), researchers in the field proposed visualization-based techniques to detect malicious packages. This technique would input the executable of the library to visualize its binary for processing. An advantage of binary visualization is that it does not need to disassemble the package to perform static analysis if the source code is not available. The program can directly examine the package binary.

(p18.8) Nataraj et al. [19] proposed a method to convert the malware binary into a gray-scale image by reading the binary as vectors of 8 bits and organizing them into a 2D array. They extracted texture features from the image representation of the malware and performed K-nearest neighbor (i.e., KNN) with Euclidean distance to classify the malware.

(p18.9) Ni et al. [20] converted malware opcode sequences into gray-scale malware fingerprinting images with SimHash encoding and bilinear interpolation and used CNN to train the malware classifier. Their evaluation results revealed that their classifier outperformed support vector machine (SVM) and KNN on the features from 3-grams of the opcode, and another classifier using KNN on GIST features. The classifier reaches around 98% accuracy and only takes 1.41s to recognize a new sample.

(p18.10) Besides the visualization of the binaries of libraries, which is similar to static analysis, there are other ML-based techniques. The ML-based malware detection field seems well-established as several survey papers exist, such as studies from Gandotra et al. and Ucci et al. [12,31]. They compiled various tools from the literature, and Gandotra et al. listed their limitations [12]. These tools are based on static analysis, in which the code is not executed, and dynamic analysis, which focuses on the run-time behavior [12]. One such tool even uses data mining techniques to detect malicious executables that rely on specific features from the binary. [25]. These features are the Portable Executable, strings, and byte sequences [25]. The Naive Bayes model takes in these features and reaches an accuracy rate of 97.11% in the study [25,12]. These surveys contain several examples of malware tools that can be adapted for APIs.

(p18.11) In 2014, Gandotra et al. stated that many researchers lean towards dynamic analysis instead of static [12]. The survey gave examples of dynamic analysis techniques for malware detection [12]. Some examples include the work of Rieck et al. that monitors the behavior of malware on a sandbox environment [12,23]. The observed behavior of the program is embedded into a vector space [23]. They use clustering to identify similar malware behaviors to classify each executable [23].

(p18.12) To conclude the antivirus examples summary for APIs, we believe that the volunteer developers recruited via an advertisement banner on the official npmjs.com website can implement the mentioned tools and ML techniques in exchange of the trust score incentive in the following sections. Afterward, the admin team can design a protocol where a volunteer user can install batches of libraries into his local repository to allow the mentioned programs to review their safety. Developers can even automate this protocol from a web server to prevent the loss of precious human labor on repeated tasks.

(p18.13) If our methods are not feasible for public registries, novel tools still exist in the literature accessed from the Google Scholar platform. We could not find existing antivirus explicitly designed for libraries, but already-tested software would be a better option if admins can access it. Admins can decide to move forward with a novel tool originated from a paper, and reading academic papers can indeed be a daunting task as it is for us. However, most papers have correspondent authors that can clarify ambiguous implementation details. Some papers even have their implementations on public repositories, which will be very helpful to the team of implementers.

(p18.14) Reasonable Constraints, Decentralization, and Trust Score The founding committee of an OS registry might not operate to pursue profit, and they may be short on budget to hire additional developers to implement these tools or reviewers to find malicious packages manually. However, to solve this problem, the admin teams can decentralize the manual code review process to people willing to contribute to the platform's security. Contributors can opt-in to review code and cast votes. After a new library is reviewed and voted to be published by a certain number of developers, it may be available to install on the public registry.

(p18.15) To encourage members of the OS registries to become reviewers, admin teams of the registries can give specific incentives for the time a reviewer invests. These perks, such as early access or discounts to products, can be supplied by companies that sponsor the npm or PyPi environment. As reviewers cast their votes to pass or reject a commit, they can earn points to unlock some advertised perks. The point system can be gamified to build public rankings and hierarchic levels. Each level would give contributors different perks, such as a free course on a partnered educational platform.

(p18.16) To implement the final stage of the voting-based solution, one has to take measures against the possibility of reviewers abusing the scoring system by casting votes without properly reviewing a commit. We believe there are already known solutions for this issue that does not need to be mentioned in this work. Some of these control measures can include comparing the time it takes for each reviewer to cast a vote to ensure each reviewer invests a proper amount of time. Simply observing the number of times a reviewer is an outlier based on the majority of the votes cast for a certain decision can also be observed. These solutions are merely to preserve the quality of the reviewer committees.

(p18.17) The scoring system can be implemented to give more privileges to users gradually besides the mentioned perks. Users can earn trust points as they actively review the contributions to projects, and users may start to publish their packages after their score passes a threshold. Thus, registries can give power to partially trusted users rather than anyone with an internet connection. We will be referencing this trust score in some of the following sections.
## (s19) Securing Critical Accounts
(p19.0) We believe the admin teams should implement a mandatory feature in the registries. A user who maintains a popular library or a popular library that uses the user's package should automatically have multi-factor authentication (MFA) to mitigate the risk of compromised accounts. MFA should be mandatory in highly trusted accounts that pass a preset score threshold as well. Any abnormal behavior, such as login from different machines, should be recorded and processed accordingly to prevent more sophisticated attacks. This process can simply be a secret question or a one-time code.

(p19.1) Another possibility is integrating code authorship identification systems into OS registries, where the program would get triggered whenever a code is published from a known author. Known hackers can be profiled from their public source code to create a database of dangerous code styles that may indicate compromised accounts. This practice is a research field in itself, and it would require high investment to build such an infrastructure. However, there are already systems built to tackle this issue to an extent. One such system is demonstrated in a study that demonstrates a Deep Learning-based Code Authorship Identification System (DL-CAIS), which is language oblivious and resilient against code obfuscation [3]. DL-CAIS uses GitHub repositories as its dataset and reaches an accuracy of 96% [3]. It first preprocesses the input code to build representation models and feeds those models into the deep learning architecture to find more distinctive features [3]. The architecture to be trained consists of several RNN layers and fully connected layers [3]. After the training, researchers use the resulting representation models to construct a random forest classifier [3].

(p19.2) Though limiting ourselves to only known hackers may be an insufficient approach. However, npm and PyPi have the source code of each project, proving to be very advantageous to profile developers. npm or PyPi can use code authorship detection tools to profile each of its developers. Each constructed profile can be associated with a collaborator or maintainer account. If the uploaded code profile in one commit does not match the code profile of the account, the admin can place a temporary hold on the account for further verification. One may still be concerned about the privacy aspect of this approach, but the feature can be optional for non-critical accounts. An automated script can reward users with an extra trust score for opting into the program to provide incentives for this practice. However, we believe critical accounts should be required to participate, as the cost of an account compromise would be significantly higher in their case.

(p19.3) A Proposed Solution for Technical Lag One feature in the npm environment we would like to see removed is the ability to constrain the specific version of a dependency by default in the package.json file [41]. We believe that there should be a special keyword in package.json, such as xxx, for the npm to check the most updated version in its registry and install it automatically to mitigate the issue of technical lag. The keyword npm outdated may be unknown to some developers, or the additional workload it would add can be a deterrent factor. However, the keyword that indicates the most recent version should be a default or auto-filled by the package manager to avoid the user interfering with versions. However, this also presents the danger of an update breaking existing systems. The user should always have the option to fix the dependency to a specific version as some releases will provide long-term support or better stability. Thus, the user can have the freedom to decide if the security or functionality of the project is a priority.

(p19.4) Frameworks have also been developed to measure how outdated a system is, balancing security and functionality [14,40]. Calculating a project's recentness score may prove difficult, as each project will have its priorities, such as performance or stability [14]. Gonzalez et al. name this priority as "gold standard" and defines the gold standard to build a lag function that can output the recentness score [14]. For instance, if the user chooses his gold standard as security, the lag function can focus on the number of new updates related to security [14]. If the gold standard is functionality, the lag function can take the number of new features into account [14]. Gonzales et al. even propose other possible lag functions, such as one that simply measures the differences in lines of source code between the most recent and currently deployed dependency or the number of commits of difference between them [14].

(p19.5) To partially implement the framework in npm or PyPi registries, they can mandate reputable libraries to list and classify each update done with the commit. Users can specify their "gold standard" with a specific command after the creation of their projects. npm can use this gold standard to compare the score of the current dependencies with their most recent versions. Now, the user will know his dependencies' recentness in terms of his priorities. Thus, the user can make more informed decisions on whether he should update.

(p19.6) Registries do not have to undergo significant changes to calculate the current projects' scores, as npm can simply take their score as 0. If all future versions require the contributor to classify new features or bug fixes manually, we believe it will not be challenging to implement the new system. Currently, we do not expect to calculate the significance of each particular update, but the user can be asked to assign a score of significance to each bug fix or new feature. We expect several users to avoid spending time with the classification of new features; however, npm can provide incentives to encourage users to participate in the program, such as with extra perks or trust scores.
