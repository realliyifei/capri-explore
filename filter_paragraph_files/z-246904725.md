# A Survey on Deep Reinforcement Learning-based Approaches for Adaptation and Generalization

CorpusID: 246904725 - [https://www.semanticscholar.org/paper/a11d62fbf84829d01aebbe6bfd5a3f1eed11e3ef](https://www.semanticscholar.org/paper/a11d62fbf84829d01aebbe6bfd5a3f1eed11e3ef)

Fields: Computer Science

## (s0) Introduction
(p0.0) Humans are inherently capable of adapting quickly to changes in their environments and generalizing their past knowledge to tackle future unseen situations. Similarly, recent research in Artificial Intelligence (AI) is progressing at an impressive speed to create intelligent agents that can achieve human-level intelligence [Haenlein and Kaplan, 2019]. AI algorithms aim to possess an ability to adapt to unseen situations and generalize well across various domains. In recent years, Deep Reinforcement Learning (DRL) is gaining traction in the AI community due to a plethora of emerging approaches being used for developing efficient policies to solve complex problems.

(p0.1) AI research works actively use the notions such as Domain Adaptation; Domain Generalization; Task Adaptation; and Task Generalization as their learning goals. The simulated or real-world experiences are utilized in such research to attain these goals. However, there is a rare attempt to provide a clearer distinction among them. In this paper, we attempt to formulate Adaptation and Generalization in the context of Task and Domain and present the recent developments of state-of-the-art AI algorithms aimed at attaining these goals through DRL-based approaches. Fig. 1 shows some complex problems settings representing scenarios that are encountered in the real world. Fig.1 (a) demonstrates a navigation task in which the ant agent (magenta rectangle) has to reach the target destination (green arrow) given the path is obstructed by an object (orange block). The agent learns to push the block sideways and navigate to the target point using hierarchical reinforcement learning . Fig. 1(b) demonstrates a cloth-folding activity which at its base is a complex problem for a robot. [Matas et al., 2018] demonstrated a sim-to-real transfer mechanism to successfully perform this task by training in one domain (simulation) and applying it in another domain (real-world). Fig. 1(c) demonstrates that an autonomous vehicle is given an expert demonstration in a simulated environment which it learns to imitate in a real-world scenario [Codevilla et al., 2018]. Fig. 1(d) demonstrates an agent needs to learn a series of sub-tasks like moving kettle, opening the cabinet door, grasping kettle, and opening microwave to perform a given task of heating water. [Pertsch et al., 2020] accomplished to solve this task by the application of skill discovery in robotic manipulation. Fig. 1(e) demonstrates a complex set of scenarios where the robotic agent needs to perform actions like moving bottle, opening/closing drawer, grabbing spherical object without any well-defined Definition 2 (Domain Adaptation (DA) and Task Adaptation (TA)). DA can be defined as the prediction goal using optimized when the conditions ≠ and = hold true ( represents task in source domain, and represents task in target domain). TA can be defined as the prediction goal using optimized when the conditions = and ≠ hold true.

(p0.2) Definition 3 (Domain Generalization (DG) and Task Generalization (TG)). DG can be defined as the prediction goal using optimized for unseen . Here, unseen refers to that is not seen during training time. TG can be defined as the prediction goal using optimized for unseen .
## (s1) Markov Decision Processes (MDPs)
(p1.0) RL settings are typically formulated using MDP having a tuple (S, A, , ) where S is a discrete or continuous state space; A is a discrete or continuous action space; is a transition function; and is a reward function. and are typically used to represent a model of the environment [Sutton et al., 1998].
## (s2) Policy
(p2.0) A policy ( | ) is a mathematical function that defines the behavior of the agent by mapping a state ϵ S to an action ϵ A. It can be either deterministic or stochastic but owing to the complex structure of the real-world yielding uncertainties in the outcomes promotes stochastic policies over deterministic ones while solving most of the real-world problems [Sutton et al., 1998].
## (s5) Meta Reinforcement Learning (Meta-RL)
(p5.0) Meta-RL is based on the notion of learning to learn [Thrun and Pratt, 1998]. Currently, it is widely used in the development of learning algorithms that are capable of generalization to unseen tasks/domains.  made one of the first attempts to develop a model-agnostic algorithm as opposed to DG-specific models [Khosla et al., 2012;Ghifary et al., 2015], which was demonstrated on RL settings to perform improve DG ability. [Yoon et al., 2018] inspired by the application of Bayesian inference principle in RL [Houthooft et al., 2016], proposed the first Bayesian fast adaptation method to solve the overfitting problem prevailing in vanilla meta-learning algorithms [Finn et al., 2017] to adapt to unseen tasks by approximating the uncertainties in them using a meta-update loss calling it the Chaser loss. The primary motivation behind using Meta-learning methods is to achieve faster adaptation to unseen tasks without learning from scratch by utilizing past experience.
## (s6) Skill Discovery
(p6.0) A skill (or option) in the context of RL is defined as a latent-conditioned policy that can be trained to perform useful tasks in a sparse/unknown reward environment [Sutton et al., 1999]. [Achiam et al., 2018] proposed a variational inference-based option discovery method for training an agent to discover and learn skills through environment interaction without the need of maximizing the cumulative reward for a given task.  proposed a reward-free skill learning method using an information-theoretic objective with a maximum entropy policy to keep the set of learned skills as diverse and discriminable as possible to maintain their usefulness. [Co-Reyes et al., 2018] introduced a novel hierarchical RL algorithm, which is capable of learning the skills in a continuous latent space (low-level). Also, they proposed a model that is capable of predicting the output of the learned skills. Such a capability can play a crucial role in solving more complex tasks at a higher level.
## (s8) Model-based Reinforcement Learning (Model-based RL)
(p8.0) Model-free RL has shown tremendous success for solving the tasks in a simulated environment [Schulman et al., 2015;. However, model-free RL methods are generally considered highly sample-inefficient. It imposes a limitation on those methods in being applicable to real-world environments such as robotics.

(p8.1) In contrast, Model-based RL is able to learn in a sample efficient manner by performing the policy optimization against the learned dynamics of the environment. [Clavera et al., 2018] proposed a model-based meta-policy optimization (MB-MPO) algorithm to allow learning an ensemble of dynamics models and formulating the policy optimization as a meta-learning problem. MB-MPO exhibited low sample complexity on high-dimensional tasks showing its applicability to real-world robotics environments. [Nagabandi et al., 2019] proposed a model-based meta-reinforcement learning algorithm capable of performing online adaptation to dynamic environments. Model-based Meta-RL approach has shown evidence of improved sample efficiency than the model-free meta-RL methods Finn et al., 2017]. The algorithm was experimented on in both simulated and real-world environments and outperformed the standard model-based meta-RL methods, model-free methods, and prior meta-RL methods.

(p8.2) [ Kaiser et al., 2020] proposed a Simulated Policy Learning (SimPLE) algorithm to perform policy optimization within a learned model. It utilizes a convolutional encoder and decoder with video frames as input to develop a model utilizing the discrete latent variables. SimPLE was experimented on several Atari games and is shown to outperform the state-of-the-art Rainbow algorithm [Hessel et al., 2018].
## (s11) Offline Reinforcement Learning (Offline-RL)
(p11.0) Offline-RL enables policy learning from a pre-collected dataset of trajectories/experiences for the tasks/domains where the agent is not allowed to interact with the environment. This approach aims to overcome the practical limitations of online RL such as dangerous or expensive interactions [Dulac-Arnold et al., 2019;.
## (s12) Discussions
(p12.0) Distributional shifts in the data between the training and testing domain is one of the primary causes behind the shortcomings in DRL research. It is found that most of the DRL-based approaches are still focused on achieving TA/DA. Therefore, it is necessary to further develop such approaches to achieve generalization over unseen tasks/domains.

(p12.1) Large and diverse datasets have allowed the application of Offline-RL algorithms to achieve generalization. However, it faces difficulty in scaling beyond laboratory settings because of the longer training time requirements and is mostly applied to problems in simulation or indoor robotics domain. Although there have been efforts for RL datasets development [Mo et al., 2018;. Still, there is a need to curate more advanced datasets that consists of real-world experiences, and also incorporates contextual information about the environment and other agents in it. Doing so will allow the researchers to develop DRL algorithms that can learn from the latent intricacies about the environment.

(p12.2) On the other hand, Meta-RL algorithms require lesser training time to achieve faster adaptability/generalizability. However, its current applications are limited to solving the tasks within indoor environments or controlled outdoor (closed-world) environments. The scaling of Meta-RL algorithms requires measuring their reliability before being deployed in an open-world environment. Similarly, skill discovery algorithms have also demonstrated the ability to perform generalization.

(p12.3) Higher levels of autonomy in Autonomous Vehicles (AVs) are typical examples of open-world environment. Despite great progresses made in DRL research, AVs are still an under-researched domain because current DRL approaches fail to incorporate the notion of safety for undesirable situations. Therefore, future research shall also focus on the development of safety-centric DRL algorithms.
