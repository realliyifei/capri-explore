# Using Natural Language Processing and Networks to Automate Structured Literature Reviews: An Application to Farmers Climate Change Adaptation

CorpusID: 259187797 - [https://www.semanticscholar.org/paper/3c8ae9eb7a77cfa63506971b2dd6134ac5dc7779](https://www.semanticscholar.org/paper/3c8ae9eb7a77cfa63506971b2dd6134ac5dc7779)

Fields: Computer Science, Environmental Science, Agricultural And Food Sciences

## (s0) Introduction
(p0.0) The fast expansion of research articles hinders researchers from keeping track of the emerging findings in their areas of expertise [13]. While systematic literature reviews are labor-intensive, the median time for them to go outdated is 5.5 years [22]. Given the current pace of publishing, it is increasingly challenging to cover the escalating pool of articles and to monitor the latest developments in a particular field. Automated content analysis [18] and extractive text summarization systems [5] have been argued as key tools to support literature synthesis and to keep track of research trends.

(p0.1) Automated content analysis for literature reviews commonly aims to map text into topics using the articles' abstracts. [18] show how automated content analysis can synthesize big volumes of literature's abstracts from ecology and evolutionary biology. In the Climate Change domain, [7] use machine learning and experts evaluations to map key messages relevant to climate change mitigation using around 100,000 articles' abstracts related to demand-side mitigation. Regarding climate change adaptation, [14] use the abstracts and metadata from around 7000 articles to map the evolution of climate change adaptation science across time.

(p0.2) Text summarization systems are normally used for more in-depth literature reviews that use the full articles' text. In this context, a summarization system takes a predefined set of documents and generates a summary, which can be extractive (the system extracts and ranks the sentences from the original text) or abstractive (the system extracts knowledge from the original text and reconstructs them into a new piece) [26]. In this sense, [26] offers a systematic review of the different methodologies for Electronic Health Records and biomedical literature summarizations. Based on their systematic review, 83% of the articles analyzed used at least the full text as input. As summary output, 81% of the articles adopted an extractive approach, while 17% of the articles used an abstractive approach -the remaining 2% compared both outputs. However, automated content analysis and extractive text summarization systems stem from machine learning and deep learning methodologies, commonly referred to as Black Box. This is because they make it difficult to theorize based on their results and to understand how text relationships are built [4,20]. As they tend to rank and re-organize the text, which hinders interpretation. On the one hand, this hinders the understanding of the role of the extracted piece in the sentence [26]. On the other hand, they might contain redundant information, which may result in a deficiency of information [26]. Furthermore, these methodologies can also suffer from opacity [6], namely opacity in terms of what text features the algorithm relied on to perform classifications. This leads to calls to stop using Black Box methodologies in favor of interpretable algorithms [20] grounded in theory and that use tailored databases, as opposed to trained over all the data found in the web [4].
## (s3) Methodology: General Overview of the Algorithm
(p3.0) We build upon [5] extractive text summarization system to perform the NLPsupported systematic literature review. In this work, we followed [5] first six steps and, at the end, introduced three new ones ( Figure 2): 1) PDF Text Extraction; 2) Text Classification and Filtering; 3) Text Normalization; 4) IMRAD Context Detection; 5) Sentence Segmentation; 6) Sentence Filtering; 7) Identification of Verbs used to Report Findings; 8) Extraction of Words Based on Verbs; and 9) Network Visualization. In the following paragraphs, we further explain each step from Figure 2 while using the data from our application as an example.
## (s5) Aesthetics of the Network Visualization
(p5.0) For all the networks, we used the following aesthetics. First, the words are arranged in concentric circles where the central words denote terms that have the highest node degree. First, we measure the words' degree to place the nodes in concentric circles. For this, we only considered whether the word appears in at least one finding sentence of each article, but we did not add up the frequency it appears in the article (i.e., by article, its maximum value is one). Suppose we assume there are: n articles and p words in the database. Then the nodes' degree (D) would be given by Equation 1. In Equation 1, I {.} denotes the indicator function, which returns 1 when the word W j appears in the findings sentences of article A i and 0 in any other case.

(p5.1) Second, the edge weight denotes the weighted frequency the words appeared related in the findings. To measure the edge weight, we use the ratio of the number of times the relationship between the words appeared in one article divided by the number of times the same relation appeared in all the articles. The edge weight (EW ) would be given by Equation 2. In equation 2, A i (W k , W l ) is 1 when there is a findings' sentence that associates words W k and W l and 0 in any other case.
