# Analysis Methods in Neural Language Processing: A Survey

CorpusID: 56657817 - [https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f](https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f)

Fields: Linguistics, Computer Science

## (s3) Linguistic Phenomena
(p3.0) Different kinds of linguistic information have been analyzed, ranging from basic properties like sentence length, word position, word presence, or simple word order, to morphological, syntactic, and semantic information. Phonetic/phonemic information, speaker information, and style and accent information have been studied in neural network models for speech, or in joint audio-visual models. See Table SM1 for references.

(p3.1) While it is difficult to synthesize a holistic picture from this diverse body of work, it appears that neural networks are able to learn a substantial amount of information on various linguistic phenomena. These models are especially successful at capturing frequent properties, while some rare properties are more difficult to learn. Linzen et al. (2016), for instance, found that long short-term memory (LSTM) language models are able to capture subject-verb agreement in many common cases, while direct supervision is required for solving harder cases.
## (s10) Languages
(p10.0) As unfortunately usual in much NLP work, especially neural NLP, the vast majority of challenge sets are in English. This situation is slightly better in MT evaluation, where naturally all datasets feature other languages (see Table SM2). A notable exception is the work by Gulordava et al. (2018), who constructed examples for evaluating number agreement in language modeling in English, Russian, Hebrew, and Italian. Clearly, there is room for more challenge sets in non-English languages. However, perhaps more pressing is the need for large-scale non-English datasets (besides MT) to develop neural models for popular NLP tasks.
## (s11) Scale
(p11.0) The size of proposed challenge sets varies greatly (Table SM2). As expected, datasets constructed by hand are smaller, with typical sizes in the hundreds. Automatically built datasets are much larger, ranging from several thousands to close to a hundred thousand (Sennrich, 2017), or even more than one million examples (Linzen et al., 2016). In the latter case, the authors argue that such a large test set is needed for obtaining a sufficient representation of rare cases. A few manually constructed datasets contain a fairly large number of examples, up to 10 thousand (Burchardt et al., 2017).
## (s14) Adversarial Examples
(p14.0) Understanding a model also requires an understanding of its failures. Despite their success in many tasks, machine learning systems can also be very sensitive to malicious attacks or adversarial examples (Szegedy et al., 2014;Goodfellow et al., 2015). In the vision domain, small changes to the input image can lead to misclassification, even if such changes are indistinguishable by humans.

(p14.1) The basic setup in work on adversarial examples can be described as follows. 13 Given a neural network model f and an input example x, we seek to generate an adversarial example x that will have a minimal distance from x, while being assigned a different label by f :

(p14.2) In the vision domain, x can be the input image pixels, resulting in a fairly intuitive interpretation of this optimization problem: measuring the distance ||x âˆ’ x || is straightforward, and finding x can be done by computing gradients with respect to the input, since all quantities are continuous.

(p14.3) In the text domain, the input is discrete (for example, a sequence of words), which poses two problems. First, it is not clear how to measure the distance between the original and adversarial examples, x and x , which are two discrete objects (say, two words or sentences). Second, minimizing this distance cannot be easily formulated as an optimization problem, as this requires computing gradients with respect to a discrete input.

(p14.4) In the following, we review methods for handling these difficulties according to several criteria: the adversary's knowledge, the specificity of the attack, the linguistic unit being modified, and the task on which the attacked model was trained. 14 Table SM3 (in the supplementary materials) categorizes work on adversarial examples in NLP according to these criteria.
