# A Survey on Practical Applications of Multi-Armed and Contextual Bandits

CorpusID: 128358546 - [https://www.semanticscholar.org/paper/b24e6b0539d6e27d82c60fa7c53a1d0905e41a19](https://www.semanticscholar.org/paper/b24e6b0539d6e27d82c60fa7c53a1d0905e41a19)

Fields: Business, Mathematics, Computer Science

## (s0) Introduction
(p0.0) Sequential decision-making problems, where at each point in time an agent must choose the best action out of several alternatives, are frequently encountered in various practical applications, from clinical trials [Durand et al., 2018] to recommender systems [Mary et al., 2015] and anomaly detection [Ding et al., 2019]. Often, there is a side information, or context, associated with each action (e.g., a user's profile), and the feedback, or reward, is limited to the chosen option. For example, in clinical trials [Durand et al., 2018;Bastani and Bayati, 2015] the context is the patient's medical record (e.g. health condition, family history, etc.), the actions correspond to the treatment options being compared, and the reward represents the outcome of the proposed treatment (e.g., success or failure). An important aspect affecting the long-term success in such settings is finding a good trade-off between exploration (e.g., trying a new drug) and exploitation (choosing the known drug).
## (s2) Healthcare
(p2.0) Clinical trials. Collecting data for assessing treatment effectiveness on animal models during the full range of disease stages can be difficult when using conventional random treatment allocation procedures, since poor treatments can cause deterioration of subject's health. Authors in [Durand et al., 2018] aim to design an adaptive allocation strategy to improve the efficiency of data collection by allocating more samples for exploring promising treatments. They cast this application as a contextual bandit problem and introduce a practical algorithm for exploration vs. exploitation in this framework. The work relies on sub-sampling to compare treatment options using an equivalent amount of information. Precisely, they extend the sub-sampling strategy to contextual bandit setting g by applying sub-sampling within Gaussian Process regression.

(p2.1) Warfarin is the most widely used oral anticoagulant agent in the world; however, dosing it correctly remains a significant challenge, as the appropriate dose can be highly variable among individuals due to various clinical, demographic and genetic factors. Physicians currently follow a fixed-dose strategy: they start patients on 5mg/day (the appropriate dose for the majority of patients) and slowly adjust the dose over the course of a few weeks by tracking the patients anti-coagulation levels. However, an incorrect initial dosage can result in highly adverse consequences such as stroke (if the initial dose is too low) or internal bleeding (if the initial dose is too high). Thus, authors in [Bastani and Bayati, 2015] tackle the problem of learning and assigning an appropriate initial dosage to patients by modeling the problem as a multi-armed bandit with high-dimensional covariates, and propose a novel and efficient bandit algorithm based on the LASSO estimator.

(p2.2) Brain and behavior modeling. Drawing an inspiration from behavioral studies of human decision making in both healthy controls and patients with different mental disorders, authors in [Bouneffouf et al., 2017a] propose a general parametric framework for multi-armed bandit problem which extends the standard Thompson Sampling approach to incorporate reward processing biases associated with several neurological and psychiatric conditions, including Parkinson's and Alzheimer's diseases, attention-deficit/hyperactivity disorder (ADHD), addiction, and chronic pain. They demonstrate empirically, from the behavioral modeling perspective, that their parametric framework can be viewed as a first step towards a unifying computational model capturing reward processing abnormalities across multiple mental conditions.
## (s4) Dynamic Pricing
(p4.0) Online retailer companies are often faced with the dynamic pricing problem: the company must decide on real-time prices for each of its multiple products. The company can run price experiments (make frequent price changes) to learn about demand and maximize long-run profits. The authors in [Misra et al., 2018] propose a dynamic price experimentation policy, where the company has only incomplete demand information. For this general setting, authors derive a pricing algorithm that balances earning an immediate profit vs. learning for future profits. The approach combines multi-armed bandit with partial identification of consumer demand from economic theory. Similar to [Misra et al., 2018], authors in [Mueller et al., 2018] consider high-dimensional dynamic multi-product pricing with an evolving lowdimensional linear demand model. They show that the revenue maximization problem reduces to an online bandit convex optimization with side information given by the observed demands. The approach applies a bandit convex optimization algorithm in a projected low-dimensional space spanned by the latent product features, while simultaneously learning this span via online singular value decomposition of a carefully-crafted matrix containing the observed demands.
## (s6) Influence Maximization
(p6.0) Autors in [Vaswani et al., 2017] consider influence maximization (IM) in social networks, which is the problem of maximizing the number of users that become aware of a product by selecting a set of seed users to expose the product to. They propose a novel parametrization that not only makes the framework agnostic to the underlying diffusion model, but also statistically efficient to learn from data. They give a corresponding monotone, submodular surrogate function, and show that it is a good approximation to the original IM objective. They also consider the case of a new marketer looking to exploit an existing social network, while simultaneously learning the factors governing information propagation. For this, they develop a LinUCB-based bandit algorithm. Authors in [Wen et al., 2017] also study the online influence maximization problem in social networks but under the independent cascade model. Specifically, they try to learn the set of best seeds or influencers in a social network online while repeatedly interacting with it. They address the challenges of combinatorial action space, since the number of feasible influencer sets grows exponentially with the maximum number of influencers, and limited feedback, since only the influenced portion of the network is observed. They propose and analyze IMLinUCB, a computationally efficient UCB-based algorithm.
## (s7) Information Retrieval
(p7.0) Authors in [Losada et al., 2017] argue that Information Retrieval iterative selection process can be naturally modeled as a contextual bandit problem. Casting document judging as a multi-armed bandit problem leads to highly effective adjudication methods. Under this bandit allocation framework, they consider stationary and non-stationary models and propose seven new document adjudication methods (five stationary methods and two non-stationary variants). This comparative study includes existing methods designed for poolingbased evaluation and existing methods designed for metasearch. In mobile information retrieval, authors in [Bouneffouf et al., 2013] introduce an algorithm that tackles this dilemma in Context-Based Information Retrieval (CBIR) area. It is based on dynamic exploration/exploitation and can adaptively balance the two aspects by deciding which users situation is most relevant for exploration or exploitation. Within a deliberately designed online framework they conduct evaluations with mobile users.
## (s8) Dialogue Systems
(p8.0) Dialogue response selection. Dialogue response selection is an important step towards natural response generation in conversational agents. Existing work on conversational models mainly focuses on offline supervised learning using a large set of context-response pairs. In [Liu et al., 2018] authors focus on online learning of response selection in dialog systems. They propose a contextual multi-armed bandit model with a nonlinear reward function that uses distributed representation of text for online response selection. A bidirectional LSTM is used to produce the distributed representations of dialog context and responses, which serve as the input to a contextual bandit. They propose a customized Thompson sampling method that is applied to a polynomial feature space in approximating the reward.
## (s9) Anomaly Detection
(p9.0) Performing anomaly detection on attributed networks concerns with finding nodes whose behaviors deviate significantly from the majority of nodes. Authors in [Ding et al., 2019] investigate the problem of anomaly detection in an interactive setting by allowing the system to proactively communicate with the human expert in making a limited number of queries about ground truth anomalies. Their objective is to maximize the true anomalies presented to the human expert after a given budget is used up. Along with this line, they formulate the problem through the principled multiarmed bandit framework and develop a novel collaborative contextual bandit algorithm, that explicitly models the nodal attributes and node dependencies seamlessly in a joint framework, and handles the explorationexploitation dilemma when querying anomalies of different types. Credit card transactions predicted to be fraudulent by automated detection systems are typically handed over to human experts for verification. To limit costs, it is standard practice to select only the most suspicious transactions for investigation. Authors in [Soemers et al., 2018] claim that a trade-off between ex-ploration and exploitation is imperative to enable adaptation to changes in behavior. Exploration consists of the selection and investigation of transactions with the purpose of improving predictive models, and exploitation consists of investigating transactions detected to be suspicious. Modeling the detection of fraudulent transactions as rewarding, they use an incremental regression tree learner to create clusters of transactions with similar expected rewards. This enables the use of a contextual multi-armed bandit (CMAB) algorithm to provide the exploration/exploitation trade-off.
## (s10) Telecommunication
(p10.0) In [Boldrini et al., 2018], a multi-armed bandit model was used to describe the problem of best wireless network selection by a multi-Radio Access Technology (multi-RAT) device, with the goal of maximizing the quality perceived by the final user. The proposed model extends the classical MAB model in a twofold manner. First, it foresees two different actions: to measure and to use; second, it allows actions to span over multiple time steps. Two new algorithms designed to take advantage of the higher flexibility provided by the muMAB model are also introduced. The first one, referred to as measure-use-UCB1 is derived from the UCB1 algorithm, while the second one, referred to as Measure with Logarithmic Interval, is appositely designed for the new model so to take advantage of the new measure action, while aggressively using the best arm. The authors in [Kerkouche et al., 2018] demonstrate the possibility to optimize the performance of the Long Range Wide Area Network technology. Authors suggest that nodes use multi-armed bandit algorithms, to select the communication parameters (spreading factor and emission power). Evaluations show that such learning methods allow to manage the trade-off between energy consumption and packet loss much better than an Adaptive Data Rate algorithm adapting spreading factors and transmission powers on the basis of Signal to Interference and Noise Ratio values.
## (s13) Algorithm Selection
(p13.0) Algorithm selection is typically based on models of algorithm performance, learned during a separate offline training sequence, which can be prohibitively expensive. In recent work, they adopted an online approach, in which a performance model is iteratively updated and used to guide selection on a sequence of problem instances. The resulting exploration-exploitation trade-off was represented as a bandit problem with expert advice, using an existing solver for this game, but this required using an arbitrary bound on algorithm runtimes, thus invalidating the optimal regret of the solver. In [Gagliolo and Schmidhuber, 2010], a simpler framework was proposed for representing algorithm selection as a bandit problem, using partial information and an unknown bound on losses.
## (s14) Hyperparameter Optimization
(p14.0) Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select optimal hyperparameter configurations, they rather focus on speeding up random search through adaptive resource allocation and early-stopping. [Li et al., 2016] formulated hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resources, such as iterations, data samples, or features are allocated to randomly sampled configurations. This work introduced a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, Hyperband wascmpared with popular Bayesian optimization methods on a suite of hyperparameter optimization problems; it was observed that Hyperband can provide more than an orderof-magnitude speedup over its competitors on a variety of deep-learning and kernel-based learning problems.
## (s16) Bandit for Active Learning
(p16.0) Labelling all training examples in supervised classification setting can be costly. Active learning strategies solve this problem by selecting the most useful unlabelled examples to obtain the label for, and to train a predictive model. The choice of examples to label can be seen as a dilemma between the exploration and the exploitation over the input space. In [Bouneffouf et al., 2014], a novel active learning strategy manages this compromise by modelling the active learning problem as a contextual bandit problem. they propose a sequential algorithm named Active Thompson Sampling (ATS), which, in each round, assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for this sample point label. The authors of [Ganti and Gray, 2013] also propose a multi-armed bandit inspired, pool-based active learning algorithm for the problem of binary classification. They utilize ideas such as lower confidence bounds, and self-concordant regularization from the multi-armed bandit literature to design their proposed algorithm. In each round, the proposed algorithm assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for the label of this sampled point.
## (s18) Reinforcement learning
(p18.0) Autonomous cyber-physical systems play a large role in our lives. To ensure that agents behave in ways aligned with the values of the societies in which they operate, we must develop techniques that allow these agents to not only maximize their reward in an environment, but also to learn and follow the implicit constraints assumed by the society. In [Noothigattu et al., 2018], the authors study a setting where an agent can observe traces of behavior of members of the society but has no access to the explicit set of constraints that give rise to the observed behavior. Instead, inverse reinforcement learning is used to learn such constraints, that are then combined with a possibly orthogonal value function through the use of a contextual bandit-based orchestrator that picks a contextually-appropriate choice between the two policies (constraint-based and environment reward-based) when taking actions. The contextual bandit orchestrator allows the agent to mix policies in novel ways, taking the best actions from either a reward maximizing or constrained policy. The [Laroche and Féraud, 2017] tackles the problem of online RL algorithm selection. A metaalgorithm is given for input a portfolio constituted of several off-policy RL algorithms. It then determines at the beginning of each new trajectory, which algorithm in the portfolio is in control of the behaviour during the next trajectory, in order to maximise the return. A novel metaalgorithm, called Epochal Stochastic Bandit Algorithm Selection. Its principle is to freeze the policy updates at each epoch, and to leave a rebooted stochastic bandit in charge of the algorithm selection.

(p18.1) 3.7 Bandit for Machine Learning:  Table 2 summarizes the types of bandit problems used to solve the machine-learning problems mentioned above. We see, for example, that contextual bandit was not used in feature selection or hyperparameter optimization. This observation could point into a direction for future work, where side information could be employed in feature selection. Also, non-stationary bandit was rarely considered in these problem settings, which is also suggesting possible extensions of current work. For instance, the non-stationary contextual bandit could be useful in the non-stationary feature selection setting, where finding the right features is time-dependent and contextdependent when the environment keeps changing. Our main observation is also that each technique is solving just one machine learning problem at a time; thus, the question is whether a bandit setting and algoritms can be developed to solve multiple machine learning problems simultaneously, and whether transfer and continual learning can be achieved in this setting. One solution could be to model all these problems in a combinatorial bandit framework, where the bandit algorithm would find the optimal solution for each problem at each iteration; thus, combinatorial bandit could be further used as a tool for advancing automated machine learning.
