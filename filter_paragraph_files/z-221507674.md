# Silent Speech Interfaces for Speech Restoration: A Review

CorpusID: 221507674 - [https://www.semanticscholar.org/paper/02b75480b6bb368346648e65bd9c722340c37571](https://www.semanticscholar.org/paper/02b75480b6bb368346648e65bd9c722340c37571)

Fields: Engineering, Computer Science, Medicine

## (s2) A. Aphasia
(p2.0) Aphasia is a disorder that affects the comprehension and formulation of language and is caused by damage to the areas of the brain involved in language [49]. People with aphasia have difficulties with understanding, speaking, reading or writing, but their intelligence is normally unaffected. For instance, aphasic patients struggle in retrieving the words they want to say, a condition known as anomia. The opposite mental process, i.e., the transformation of messages heard or read into an internal message, is also affected in aphasia. Aphasia affects not only spoken but also written communication (reading/writing) and visual language (e.g., sign languages) [49].
## (s6) B. Direct speech synthesis
(p6.0) Direct synthesis techniques are used to model the relationship between speech-related biosignals and the acoustic speech waveform. In its most common form, this relationship is conveniently represented as a mapping f : X → Y between the space X ∈ R Dx of feature vectors extracted from the biosignals and the space Y ∈ R Dy of acoustic feature vectors as follows:

(p6.1) where x t and y t are, respectively, the source and target feature vectors at time t computed from the silent speech and acoustic signals (more details about the computation of the source vectors is provided in Section IV), and t is a zero-mean independent and identically distributed (i.i.d.) error term.

(p6.2) Modelling the mapping function f (·) in (3) presents some challenging problems. This function is known to be nonlinear [128], [129]. Moreover, for some types of biosignals, this mapping is non-unique [129]- [131], that is, the same acoustic features may be associated with multiple realisations of the biosignal. For instance, ventriloquists are able to produce almost the same acoustics with multiple vocal tract configurations. Another reason for this non-uniqueness is that the sensing techniques frequently have a limited spatial or temporal resolution and, as a result, the speech production process is not properly captured and some information is lost.
## (s7) C. Comparison of the two SSI approaches
(p7.0) Each SSI approach has its advantages and disadvantages. Silent speech-to-text has the advantage that speech might be more accurately predicted from the biosignals, thanks to the language and pronunciation lexicon models used in ASR systems. These models impose strong constraints during speech decoding and may help recover some speech features, such as voicing or manner of articulation, which are not well captured by current sensing techniques [20], [29], [102], [174]. However, the use of these models also means that this approach is unable to recognise words that were not considered during training, such as words in a foreign language. The direct speech synthesis approach, in contrast, is not limited to a specific vocabulary and is language-independent. A second limitation of the silent speech-to-text approach is that the paralinguistic features of speech (e.g., speaker identity or mood), which are important for human communication, are lost after ASR, but could be recovered by direct synthesis techniques. Yet another problem of silent speech-to-text is that, in practice, it is difficult to record enough silent speech data to train a large vocabulary ASR system 2 , while direct synthesis systems require less training material (usually just a few hours of training data) because modelling the biosignal-to-speech mapping is arguably easier than training a full-fledged speech recogniser.
## (s9) A. Brain activity
(p9.0) Obtaining biosignals at the origin of speech production has the advantage that a wider range of speech disorders and pathologies can thus be addressed. Brain activity sensing techniques can potentially assist not only persons with voice disorders but also those with dysarthria or apraxia, or even some cases of aphasia. On the other hand, the internal processes of the brain that are involved in speech production are imperfectly understood, and recording brain activity at a high spatiotemporal resolution is still problematic, at best. 1) Neuroanatomy of speech production: The neuroanatomy of language production and comprehension has been a topic of intense investigation for more than 130 years [185]. Historically, the brain's left superior temporal gyrus (STG) has been identified as an important area for these cognitive processes. Studies have shown that patients with lesions to this brain area present deficits in language production and comprehension [186], and that a complex cortical network extending through multiple areas of the brain is involved in these processes [187].

(p9.1) This cortical network has recently been modelled by a dualstream model consisting of a ventral and a dorsal stream [185]. The ventral stream, which involves structures in the superior (i.e., STG) and middle portions of the temporal lobe, is related to speech processing for comprehension, while the dorsal stream maps acoustic speech signals to the frontal lobe articulatory networks, which are responsible for speech production. This dorsal stream is strongly left-hemisphere dominant and involves structures in the posterior dorsal and the posterior frontal lobe, including Broca's area, or inferior frontal gyrus (IFG), which is critically involved in speech production [188].
## (s10) B. Muscle activity
(p10.0) As shown in Fig. 1, during speech production the muscles in the face and larynx are responsible for the movements that will eventually result in the production of the acoustic signal. As mentioned above, the brain controls the activation of these muscles by means of electrical signals transmitted through the motor neurons of the peripheral nervous system. These electrical signals cause muscles to contract and relax, thus producing the required articulatory movements and gestures. EMG measures the electrical potentials generated by depolarisation of the external membrane of the muscle fibres in response to the stimulation of the muscles by the motor neurons [212]. The EMG signal resulting from the application of this technique is complex and dependent on the anatomical and physiological properties of the muscles [213].

(p10.1) Two types of electrodes can be used for EMG signal acquisition: invasive or non-invasive. Invasive methods involve intramuscular electrodes (i.e., needles) inserted through the skin into the muscle. These methods fundamentally measure localised action potentials, but this approach can be problematic when the aim is to measure the characteristics and behaviour of whole muscle signals, as is the case with SSIs. In contrast, non-invasive methods employ superficial electrodes (i.e., sEMG) directly attached to the skin, as shown in Fig.  2. In this case, the sEMG signal is a composite of all the action potentials of the muscle fibres localised beneath the area covered by the sensor. Because of this property and its non-invasiveness, sEMG is the preferred technology in most SSI investigations. The characteristics of sEMG signals are determined by the properties of the tissue separating the signal generating sources from the surface electrodes. In particular, biological tissue acts as a low pass filter affecting the frequency content of the signal and the distance at which it can no longer be detected.

(p10.2) 1) EMG and speech production: In studies of speech production and related applications, EMG electrodes are attached to the subject's face, as illustrated in Fig. 2. Fig. 2a shows a single electrode setup [86], [215] with electrodes connected to certain muscle areas, whereas Fig. 2b shows an electrode array setup [214], [216]. In the latter case, there are two electrode arrays, a large one placed on the cheek and a small one under the chin. The signals thus captured represent the potential differences between two adjacent electrodes. Once amplified, these signals are ready for further signal processing.

(p10.3) Since the speech signal is mainly produced by the activity of the tongue and the facial muscles, the EMG signal patterns resulting from measurements in these muscles provide a means of retrieving the speech signal [217]. Moreover, this effect is maintained even when words are spoken inaudibly, i.e., the acoustic signal is not produced [218]. This represents an important advantage of EMG-based SSI systems when it comes to providing an alternative means of communication for persons with voice disorders (such as laryngectomy patients) or some types of speech disorders (e.g., dysarthria). Another advantage is that EMG signals appear 60 ms before articulatory motion [87], [219], which is an important feature for real-time EMG-to-speech conversion with low latency. Besides its application in SSIs (see next section), EMG is being used in clinical rehabilitation (e.g., for the recovery of facial muscular activity in patients with motor speech disorders [220] and other articulatory disturbances [221]), assistance and as an input device [212]. In particular, these previous studies have reported the benefits of EMG biofeedback in therapy aimed at increasing muscle activity of the oral articulators in dysarthric speakers with neurological conditions [220], [222], [223]. EMG is also a useful tool for speech production research [224], [225].
## (s11) C. Articulator motion capture
(p11.0) Production of the acoustic speech signal requires the movement of different speech articulators. Therefore, monitor-ing the movement of these articulators is a straightforward approach enabling us to capture meaningful biosignals for speech characterisation. In this subsection, we describe different techniques for capturing articulatory movement, using kinematic sensors attached to the vocal tract or by means of imaging techniques to visualise these changes. As most of these techniques do not capture glottal activity, they are best suited to restore communication capabilities for persons with voice disorders, such as laryngectomised patients.

(p11.1) 1) Magnetic articulography: The techniques described in this section employ magnetic tracers attached to the articulators and sense their movement by measuring the changes in the magnetic field generated (or sensed) by these magnets. There are two variants of this technique, EMA and PMA, which differ according to where the generation and sensing of the magnetic field take place. A comparative study of these variants can be found in [238].

(p11.2) The idea of EMA [23], [239] is to attach receiver coils to the main articulators of the vocal tract. These coils are connected by wires attached to external equipment that monitors articulatory activity. Transmitter coils placed near the user's head generate alternating magnetic fields, making it possible to track the spatial position of the coupled receiver coils. The advantages of this technique are its high temporal resolution for modelling the articulatory dynamics and the minimal feature pre-processing required (the captured data directly provides the 3D Cartesian coordinates of the receiver coils and, additionally, their velocity and acceleration). The major drawback is the need for external non-portable transmitters and wired connections, which limits its use to laboratory experiments.

(p11.3) EMA was used in [240] for automatic phoneme recognition without additional audio information. A Carstens AG100 device simultaneously tracked the vertical and horizontal coordinates in the mid-sagittal plane of six receiver coils located at different points of the oro-facial articulators. The EMA parameters were recorded at a sampling frequency of 500 Hz. The articulatory parameters were fused using a multistream HMM decision fusion. This study was conducted using a French corpus of vowel-consonant-vowel (VCV) sequences and additional short and long sentences. Finally, the system was evaluated using different combinations of articulatory parameters and compared with the use of the standalone audio signal or a combination of both audio and EMA data. The recognition results obtained were found to be competitive. DNNs were recently investigated in the context of EMA-based ASR. In [93], bidirectional RNNs were used to capture longrange temporal dependencies in the articulatory movements. Moreover, physiological and data-driven normalisation techniques were considered for speaker-independent silent speech recognition. A silent speech EMA dataset was recorded from twelve healthy and two laryngectomised English speakers. This approach provided state-of-the-art performance in comparison with other ASR models.
## (s13) DIRECTIONS
(p13.0) SSIs have advanced considerably in recent years [19]. The studies reviewed in the previous sections show that it is possible to decode speech, even in real-time in some cases, for a wide range of speech-related biosignals. Nevertheless, this technology is not yet mature enough for useful purposes outside laboratory settings. In particular, most SSIs have been validated only for healthy individuals, and their viability as a clinical tool to assist speech-impaired persons has yet to be determined. Furthermore, to the best of our knowledge, none of the studies we reference report a means by which intelligible speech can be consistently generated, in the context of a large vocabulary and/or conversational speech. In addition, there remains the problem of training data-driven direct synthesis techniques when no acoustic data are available.
## (s14) A. Improved sensing techniques
(p14.0) Most of the sensing techniques described in Section IV have only been validated in laboratory settings under controlled scenarios. Hence, certain issues need to be addressed before final products can be made available to the general public. First, while many techniques are designed to allow some portability and to be generally non-invasive, some problems remain. The equipment is not discreet and/or comfortable enough to be used as a wearable in real-world practice [26], [278] and may be insufficiently robust against sensor misalignment [279], [280]. Second, the linguistic information captured by these devices is often limited. For example, sEMG has difficulty in capturing tongue motions, while EMA/PMA cannot accurately model the phones articulated at the back of the vocal tract due to practical problems that may arise in locating sensors in this area (such as the gag reflex and the danger of the user swallowing the sensors) [102], [174]. These problems might be overcome by combining different types of sensors, each of which is focused on a different region of the vocal tract, thus enabling a broader spectrum of linguistic information to be obtained. Yet another issue is that of how to capture and model supra-segmental features (i.e., prosodic features), which play a key role in oral communication. Prosody is mainly conditioned by the airflow and the vibration of the vocal folds, which in the case of laryngectomised patients is not possible to recover. As a result, most direct synthesis techniques generating a voice from sensed articulatory movements can, at best, recover a monotonous voice with limited pitch variations [101], [281], [282]. The use of complementary information capable of restoring prosodic features is thus an important area for future research.
## (s18) E. Validation in a clinical population
(p18.0) With few exceptions [93], [108], research outcomes in SSIs have been validated only for healthy users. Although clinical populations have participated in studies with implanted brain sensors, in most cases the participants have had the sensors implanted to treat other neurological conditions (e.g., refractory epilepsy). Furthermore, in these cases sensor implantation was driven by clinical needs, with little or no consideration for optimising sensor placement to cover the language-processing areas in the brain.

(p18.1) In most cases, therefore, SSIs have been validated only for healthy users. This is for several reasons: first, SSI development is still in its infancy. Thus, many of the studies reviewed in this paper only seek to show that speech can be decoded from a particular biosignal for healthy individuals. Second, data recording is considerably harder for patients with health conditions than for healthy individuals. In particular, the synchronous recording of parallel audio-and-biosignal data may not be feasible for persons with moderate to severe speech impairments. As mentioned above, the non-availability of parallel training data greatly hampers the use of direct synthesis techniques for this population. Third, persons with speech impairments are likely to display more variability than those with no such impairments. Thus, biosignal variability as the impairment advances is another type of intra-subject variability which should be considered in practical SSIs. However, designing systems that are robust to such variability is no easy task. Finally, difficulties may arise in recruiting patients for the studies and in addressing the ethical questions involved.
## (s19) F. Evaluation in more realistic scenarios
(p19.0) The vast majority of SSIs thus far proposed have been validated using offline analyses with pre-recorded data. In these analyses, a pre-recorded data corpus is used both for system training and for evaluation. While the results of these offline analyses are useful for optimising various system parameters (such as system latency, output quality and system robustness), online analyses are needed in order to evaluate system performance in real-world scenarios. Online analyses assess the efficacy of the SSI while it is in active use, possibly while the user is receiving real-time audio feedback. Ideally, the system should be tested in real-life scenarios, over a prolonged period (i.e., longitudinal analysis) and with an adequate number of users presenting a diversity of speech impairments at different stages of evolution. Regarding the first point, most offline analyses reported to date have been based on a pre-recorded list of words, commands or phonetically-rich sentences. While this type of vocabulary-oriented evaluation can provide insights into SSI accuracy for decoding different phones, it does not reflect the fact that, in most cases, users will employ the SSI to establish a goal-oriented dialogue (e.g., ordering food in a restaurant or asking for help) [325], [326]. In these situations, other factors come into play, such as contextual information and visual clues (e.g., body language), which can help to resolve confusion in word meaning during the dialogue [183].
