# Finding Critical Scenarios for Automated Driving Systems: A Systematic Literature Review

CorpusID: 239016131 - [https://www.semanticscholar.org/paper/7eac16f6e5c63fd983155947b18f1721af87beaf](https://www.semanticscholar.org/paper/7eac16f6e5c63fd983155947b18f1721af87beaf)

Fields: Engineering, Computer Science

## (s7) Scenario
(p7.0) The temporal development between several scenes in a sequence of scenes. [8] Scene A snapshot of the environment including the scenery and movable objects, as well as all actors' and observers' self-representations, and the relationships among those entities. [8] Critical Scenario
## (s8) CSI Method
(p8.0) Methods to find triggering conditions, safetycritical operational situations, or combinations of the two that will lead to harm ODD Operational Design Domain: Operating conditions under which a given ADS or AD feature thereof is specifically designed to function [6]. It contains the set of all the influential factors and the possible combinations of these factors.
## (s9) Functional Scenario
(p9.0) Scenario space representation on a semantic or a high level of abstraction via linguistic notations [22] Logical Scenario Scenario space representation on a state-space level with parameter ranges [22] Concrete Scenario A concretization of a logical scenario with concrete parameter values [22] Fig. 2. Nominal AD functions according to Autoware [24], Apollo [25] and Elektrobit open robinos [26] which are excluded from the scope of this driving automation taxonomy because they do not perform part or all of the DDT on a sustained basis, but rather provide momentary intervention during potentially hazardous situations, such as lane keeping assistance (LKA) systems and automatic emergency braking (AEB) systems.

(p9.1) In our paper, we used the definition of levels of automation from the SAE J3016 [6] to classified the systems in this survey into L3+ (with a human driver), L3-(without a human driver), and active safety system (no continues control of the vehicle). Detailed definitions of these three classes are given in Section 5.1.1.
## (s13) Scenario Representation
(p13.0) Menzel et al. [22] classified scenario representations into three levels of abstraction, namely functional scenario, logical scenario and concrete scenario. Functional scenario and logical scenario describe scenario spaces on two different levels of abstraction, while concrete scenario describes a particular scenario.

(p13.1) Functional scenario: A scenario space representation on a semantic level with linguistic scenario notations. [22] "The vocabulary used for the description of functional scenarios is specific for the use case and the domain and can feature different levels of detail." [22] Logical scenario: A scenario space representation on a state-space level with parameter ranges in the state space. Each parameter correlates to one influential factor. The parameter ranges can optionally be specified with probability distributions. A logical scenario includes a formal notation of the scenario space. [22] Additionally, "the relations of the parameter ranges can optionally be specified with the help of correlations or numeric conditions." [22] In this paper, it is assumed that a logical scenario cannot fully reflect its corresponding functional scenario since relevant parameters cannot be completely listed.

(p13.2) Concrete scenario: A parameterized representation of a particular scenario. Each concrete scenario is an instantiation of a logical scenario, with a concrete value for each parameter. [22] According to a concrete scenario, an executable scenario can be constructed, which can be either a simulation model or a real test. An executable scene refers to an image from a camera or a point cloud from a LiDAR.

(p13.3) In the rest of this paper, functional scenario, logical scenario and concrete scenario are also used to denote a scenario space or a scenario represented with the corresponding levels of abstraction. Fig. 4 depicts the transitions between the three levels of abstraction, which are defined as follows:
## (s16) Related Survey Papers
(p16.0) While there is a vast amount of literature on various aspects of CSI methods, there are much fewer related survey papers. Other related topics, although outside the scope of this review, are briefly introduced in Section 8.

(p16.1) According to our literature search, to the best of our knowledge, related relevant survey papers are [32] and [33]. Neurohr et al. [32] reviewed and analyzed the literature about the scenario-based testing method for automated vehicles. The authors presented fundamental arguments, principles and assumptions of the scenario-based approach. They also proposed a generic framework (from scenario elicitation to test evaluation) for scenario-based testing and analyzed in detail the individual steps based on the reviewed articles. As a result, they presented various considerations for using and implementing scenario-based testing to support automated vehicle homologation.

(p16.2) Riedmaier et al. [33] performed a survey of scenariobased approaches for safety assessment of automated vehicles. The authors provided an overview of various approaches. They also developed a taxonomy for the scenariobased approach and compared the summarized methods with each other. In the end, this paper integrated the formal verification with the scenario-based approach as an alternative concept.
## (s17) LITERATURE REVIEW METHODOLOGY
(p17.0) This literature review follows the guidelines proposed by Keele [34], which divides the whole literature review process into three stages: planning, conducting and reporting. Based on this guideline, our literature review protocol is illustrated in Fig. 7 and detailed in the rest of this section.
## (s19) RQ1:
(p19.0) What would be a taxonomy that allows to systematically categorize and compare state-of-the-art CSI methods for ADS and ADAS? RQ2: What is the current status of CSI methods research with respect to this taxonomy? RQ3: What are the remaining problems and challenges for further investigation?

(p19.1) The taxonomy in RQ1 can provide a systematic structure of common characteristics to enable the classification and comparison of different CSI methods. With this taxonomy, further researchers or engineers can easily pinpoint a CSI method on the big picture, and thereby understand its strength and limitations. The answer to RQ2 presents the state of the art of CSI methods. It can also help new researchers or engineers in this field have a quick and comprehensive understanding of these methods. RQ3 tries to identify the further directions of this research topic.

(p19.2) To clarify the scope of this literature review, as suggested in [34], the research questions are broken down into individual facets (Population, Intervention, Comparison, Outcomes and Context -PICOC [35]) as: The scope of this literature review can be further narrowed down by clarifying the definitions of scenario and criticality. As discussed in Section 2.3.1, the definition of a scenario should follow the one given in [8], and cover at least one layer of the 6-layer model [30]. The included studies must distinguish critical scenarios from other scenarios. General-purpose scenario modeling methods, such as [36], are excluded if they do not consider the identification or generation of critical scenarios. For a similar reason, we also excluded general-purpose data augmentation approaches for training machine learning models (e.g., [37], [38]).
## (s22) Literature search and selection
(p22.0) The primary studies for this literature review are collected through an iterative process with automatic search and snowballing as shown in Fig. 8. This section describes the details of how we conducted each stage. Stage 1: A comprehensive initial search string can reduce the number of iterations to determine the final search string as described in Stage 3. To better define the initial search strings, an initial set of relevant studies was gathered from two sources. The first source was publications from recent relevant research projects including AutoDrive, Prystine, Pegasus, Enable-S3 and AdaptIVe. The second source are the relevant papers included in the two relevant survey papers  Fig. 8. The stages to collect the primary studies. The numbers on the arrows indicate the numbers of studies given to the next stage [32], [33] introduced in Section 2.6. The initial set contains 151 potentially relevant studies. After being filtered by the inclusion and exclusion criteria, 49 studies remained.

(p22.1) Stages 2 and 6: To filter a given set of potentially relevant studies, we reviewed the title abstract and author keywords of each paper with respect to the inclusion and exclusion criteria defined in Section 3.1. After reviewing, each paper was labelled as either "included", "excluded" (together with the violated criterion) or "unclear". Unclear studies were further checked by going through their introduction, conclusion and some other parts. When necessary, a discussion among multiple researchers would be conducted to determine the inclusion of an unclear study.
## (s24) Data Extraction
(p24.0) This task is to answer RQ1. Relevant information needs to be extracted from the primary studies according to a taxonomy. Meanwhile, the taxonomy needs to be updated during the extraction. To start, an initial taxonomy was proposed based on (1) the relevant industrial standards introduced in Section 2, (2) the concepts identified from the initial set of the primary studies, and (3) the previous project experience of the authors. The structure of the taxonomy was inspired by [39]. The initial taxonomy was, thereafter, iteratively  Fig. 9. The stages to extract data from primary studies updated when reviewing the primary studies, following the process illustrated in Fig. 9. This section elaborates how each stage in this process was conducted. Stage 1: All the primary studies were thoroughly read by at least one researcher to extract information according to the taxonomy, and to identify new concepts to update the taxonomy.
## (s30) Usage of the Scenarios:
(p30.0) This category classifies the CSI methods according to how the identified critical scenarios are supposed to be used. Subcategories include 1) the System of Interest (SoI), i.e. the AD system/function whose development is supported by the identified scenarios; and 2) how are the identified critical scenarios used in different development phases. For example, Li et al. [41] proposed a method to find critical scenarios as test cases (purpose) for the verification (phase) of a whole ADS (SoI).

(p30.1) Target ODD: As depicted in Fig. 4, a clear and sufficient ODD definition is necessary for the reasoning of critical functional scenarios and the formalization of a functional scenario to a logical scenario. This category analyzes how ODD is defined and used in each primary study.
## (s32) Level of Abstraction:
(p32.0) This category classifies the studied approaches according to their inputs and outputs in terms of the levels of abstraction of their scenario representation, as described in Section 2.3.2. For example, the approach proposed by Li et al. [41] identifies critical concrete scenarios within a given logical scenario. Therefore this approach is classified as "logical → concrete". Similar approaches to identify critical scenarios according to a scenario with a higher level of abstraction are called deductive approaches. In contrast, inductive approaches find critical scenarios based on a set of lower-level scenarios.
## (s39) Requirement Analysis:
(p39.0) This phase analyzes functional requirements or safety requirements at the vehicle level. In ISO 26262, Hazard Analysis and Risk Assessment (HARA) is an essential step to identify all the potential hazardous events. As depicted in Fig. 3, each hazardous event is a combination of a hazard and an operational condition. With the methods proposed in [30], [61], [62], [63], the identified pre-crash functional scenarios can be used as the set of all the operational conditions for HARA as described in [64].

(p39.1) Most of the studied CSI approaches treat the identified critical scenarios as test cases used in the verification and validation phases. The failed test cases or the identified critical scenarios through simulation can also support the identification of specification insufficiency.

(p39.2) Some of the studied CSI approaches can also support the analysis of influential factors by determining the critical regions (i.e. particular values or value ranges of a set of parameters) where critical scenarios are significantly more probable [57], [58], [59], [60], [65], [66]. The identification of unknown influential factors is out of the scope of this survey but is briefly discussed in Section 8.5.
## (s40) System Design:
(p40.0) This phase decides the system configuration and the decomposition of vehicle level requirements to component level. However, in this survey, no CSI approach was found to support requirement decomposition. System configuration includes the selection of sensors [55] and sensor ranges [67].
## (s42) Component and System Verification:
(p42.0) Nearly 55% of the CSI methods proposed in the primary studies focus on the identification of critical concrete scenarios, which are used in the verification phase as test cases. The generated test cases can be used to verify the whole AD system (e.g., [41] ) or a particular AD function (e.g., [42] ). Detailed analysis of these methods can be found in Sections 6.1 and 6.2.
## (s44) Targeted ODD
(p44.0) As shown in Fig. 4, a clear ODD definition is necessary for the reasoning of critical functional scenarios, and the formulation (together with improvisation) of a functional scenario to a logical scenario. As discussed in Section 3.1, improvisation and formulation are out of the scope of this literature review. Therefore this section only focuses on how ODD supports the reasoning of critical functional scenarios. A clear ODD definition is essential to derive a complete set of critical concrete scenarios. However, none of the primary studies provide an explicit ODD definition. Some of the primary studies explicitly or implicitly provide the scope of the ODD, e.g., driving on a highway [30], [61] and driving on a structured road [62], [63], [73], [74]. Methods in these studies make assumptions about the environment (e.g., the behavior of other vehicles) within the ODD, and propose systematics to reason about critical functional scenarios based on these assumptions.
## (s45) Definitions of Criticality
(p45.0) As discussed in Section 2.4, in this paper, critical scenarios are considered to be more important than non-critical scenarios in terms of safety analysis or verification. The definitions of criticality are tightly connected with the usages of the identified critical scenarios. However, most of the primary studies do not provide an explicit definition of criticality. Instead, they explicitly define the employed surrogate measures for criticality. In this survey, these measures of criticality are classified according to two dimensions, as illustrated in Table 6. This classification is also used to specify the definition of criticality in each primary study. The two dimensions are explained in the rest of this section. The surrogate measures are detailed in Section 6. The first dimension specifies whether the identified scenarios are implementation-specific. An implementationspecific critical scenario is only determined to be critical for a particular implementation of an AD function or system (set of functions). It may or may not be critical for other implementations of the same function or system. In contrast, non-implementation-specific critical scenarios refer to the ones that are critical for most of the implementations of the same function or system. For example, scenarios with a heavy fog are critical for most of the camera-based object detection/classification functions.

(p45.1) The second dimension classifies criticality according to consequence. Scenarios that are highly likely to cause a hazardous event are defined as safety-critical, while scenarios that may lead to a malfunctioning behavior are classified as function-critical. According to Fig. 3, both safety-critical and function-critical scenarios can support the identification of unknown functional insufficiency and the corresponding triggering conditions. There are also primary studies whose criticality definition includes other perspectives such as comfort and traffic impact. These perspectives are out of the scope of this survey.

(p45.2) Function-critical scenarios can be further classified according to the awareness of the consequential malfunctioning behavior. If the consequential malfunctioning behavior is pre-defined, (e.g., a collision or a misclassification of a certain object), the criticality of the identified scenarios is consequence-aware. Approaches to find consequenceunaware critical scenarios tend to find scenarios where malfunctioning behaviors are tend to be triggered. These scenarios may help to find unknown influential scenario factors [115], [116], [121], [124].

(p45.3) Different primary studies may implicitly adopt different definitions of criticality. TABLE 6 explicitly classifies the definitions of criticality according to the aforementioned three dimensions. As shown in TABLE 6, most of the primary studies focus on the identification of safety-critical scenarios. The number of primary studies that find implementationspecific critical scenarios is larger than those finding nonimplementation-specific critical scenarios.
## (s46) Level of Abstraction
(p46.0) Critical scenarios can be identified on different levels of abstraction. As introduced in Section 4.1, this section classifies the CSI methods according to their inputs and outputs in terms of the level of abstraction of scenario description. The classification result is given in Table 7.

(p46.1) Most of the studied CSI methods find concrete critical scenarios within a given logical scenario (i.e., logical → concrete). An important step within this process is to evaluate the criticality of a given concrete scenario (i.e. concrete → criticality). These two classes cover most of the CSI methods introduced in Sections 6.1, 6.2 and 6.5.

(p46.2) As shown in Fig. 4, critical functional scenarios can be reasoned from the ODD definition and/or functional specifications and/or previous project experience, etc. (i.e. others → functional) [30], [32], [61]. In some papers, these reasoned functional scenarios are also formulated into logical scenarios (i.e. others → logical) [62], [63]. These methods are analyzed in Section 6.4.

(p46.3) Critical functional scenarios can also be induced from accident databases. Depending on whether the accidents in the database are described qualitatively or quantitatively, these methods can be classified as functional → functional   [110] or concrete → functional [54]. These methods are introduced in Section 6.3. Another type of functional → functional method is to combine triggering conditions with hazardous operational situations [32]. Studies evaluating the criticality of a logical scenario (i.e. logical → criticality) actually evaluate the failure rate of an SOI under the given logical scenario. These studies estimate the failure rate through importance sampling [42], [80], [81], [82], [83], [85], which is a variant of Monte-Carlo simulation, considering the distribution of critical scenarios in the space of the given logical scenario. It should be noticed that the focus of this survey does not include how the failure rate is evaluated. We only care about how the critical region can be discovered at the first step of the importance sampling.
## (s47) THE SOLUTION CATEGORY
(p47.0) According to the similarities on the problem formulations and solutions, studied CSI approaches are grouped into the following five clusters. As depicted in Fig. 4, a logical scenario can be instantiated into multiple concrete scenarios. The parameters of a logical scenario can be classified as illustrated in Fig. 15. Assumed parameters have fixed values for all the instances (i.e. concrete scenarios), e.g. the number of vehicles in a scenario and the number of lanes. Parameters of interest construct the scenario space to be explored. These parameters include the ones that are constant over time (e.g. the weather condition or the position of a stationary obstacle on the road) and the ones that are variable over time (e.g. the speed of a surrounding vehicle or the perception error). If a parameter is variable over time, it can be represented as a parameter trajectory. Values of the parameters can be either categorical (e.g. weather, color and vehicle model) or numerical. Numerical values can be either continuous (e.g. speed, heading and sensor noise) or discrete (e.g. the number of other vehicles, the number of lanes and speed limit).

(p47.1) The approaches to explore a logical scenario are elaborated in both C1 and C2. The difference between these two clusters is that the logical scenarios in C1 do not contain parameter trajectories. C3 analyzes the methods to induce critical scenarios from different data sets. C4 discusses systematic approaches to deduce critical functional scenarios. Most of the computer-vision (CV) based functions (e.g. object detection or classification) take a scene (i.e. a camera image) as input at each time step. The performance of such a function is mainly affected by the input scene rather than how the scene develops over time. Methods to find critical scenes for CV-based functions are summarized in C5.

(p47.2) The following subsections respectively introduce our analysis of these clusters. Except in C4, CSI approaches in each cluster are summarized with a gated tree, i.e. Fig.  17, 22, 23 and 24. Each node of these trees represents a component of a CSI solution. A parent node is connected with its children through either an "AND" gate or an "OR" gate. The "AND" gate implies that the parent note is constructed with all its children as necessary components. For example in Fig. 17, all the methods in C1 (i.e. the root node) contain three components, namely a logical scenario model, an exploration method and a criticality assessment approach. If they are connected with an "OR" gate, the children will be the alternative solutions of the parent node. In other words, in a particular paper (e.g. [41]), the parent node (e.g. the Exploration Method node in Fig. 17) will choose one of its children (e.g. Combinatorial Testing) as a solution. Not so many papers have been found in C4. Such a systematic figure is not given for C4, since a conclusion drawn from such few samples will not be representative. A systematic classification of CSI methods in C4 will be part of the future work.
## (s48) Exploring Logical Scenarios Without Parameter Trajectory
(p48.0) This section focuses on approaches to explore a logical scenario that does not contain a parameter trajectory. The exploration methods for logical scenarios with parameter trajectories are analyzed in Section 6.2. The exploration processes in this cluster are formulated as Design Space Exploration (DSE) or Search-based Testing (SBT) problems [89], whose flow chart is shown in Fig. 16. Given a logical scenario, a set of concrete scenarios are generated with a parameter space exploration method. Among these generated concrete scenarios, critical ones are identified with a pre-defined criticality assessment method. Criticality assessment can be seen as a function that maps a point in the scenario space to a point in the scoring space. The scoring space is the quantitative evaluation of criticality for concrete scenarios. The evaluation is achieved through the surrogate measures since the criticality can hardly be measured directly. If the criticality is defined on multiple  Fig. 10, all the CSI methods in this cluster are summarized in Fig. 17. The analysis of Fig. 17 follows the taxonomy of Solution category in Fig. 10.

(p48.1) Regarding the analysis in Table 7, most of the approaches in this cluster take logical scenarios as inputs and generate a set of critical concrete scenarios, which are further used to construct test cases or simulation cases. Exceptions include the approaches that focus on the criticality assessment of a given concrete scenario [92], [107], and the refinement of a logical scenario [69], [70]. Criticality assessment is considered in this cluster because it is one of the essential steps to search for critical scenarios, as shown in Fig. 16. If a primary study only focuses on a criticality assessment approach, it usually implies that the given concrete scenario is generated from its previous step. Logical scenario refinement methods are included in this cluster only if they employ search-based methods. Instead of finding individual critical scenarios in the scenario space, they optimize the lower and upper bounds of the parameters of interest to derive the critical region. It can be treated as a prepossessing step of CSI methods to reduce the searching space.
## (s49) Scenario Model
(p49.0) In this cluster, a logical scenario is modeled as a scenario space. Each dimension is a parameter of interest whose value can be either categorical or continuous. A logical scenario also specifies their value ranges or distributions. A concrete scenario is a vector in this scenario space with a fixed value for each dimension. As mentioned in Fig. 3, each parameter of interest correlates to one known scenario factor at one layer, as described in  Fig. 17. Instantiation methods without parameter trajectories • a constant condition including the weather condition (e.g., sunny, foggy, or raining) and the road topology;

(p49.1) • or a parameter of an assumed model including, e.g., the size, field of view, or constant speed of a vehicle model Parameter distributions can help to estimate the likelihood of exposure of a given concrete scenario. 9 out of 45 studies in this cluster consider realistic distributions of parameters ( [42], [45], [79], [80], [81], [82], [83], [85], [92]) during the exploration. Other studies assume that the parameters follow a uniform distribution. The realistic parameter distributions are taken into account mainly for the following reasons:

(p49.2) • To estimate the failure rate of the SoI in all situations: As discussed in Section 5.4, estimating the failure rate with importance sampling needs critical scenarios or critical region as input. In addition, it also needs to know the likelihood of exposure of each sampled concrete scenario, which can be approximated based on the parameter distributions.

(p49.3) • To consider commonality as part of the criticality definition: Compared to rare cases, those with higher probabilities to exposure in the real world are of significant interest. To this end, the commonality of a scenario is introduced into the definition of criticality [42], [79], [92], so that the exploration method can find common and hazardous scenarios. This commonality can be quantified with the help of parameter distributions in real traffic.

(p49.4) Parameter distributions are derived from an existing real-life driving database (i.e., the Naturalistic Driving Study (NDS) or Field Operational Test (FOT) data). The parameters can be assumed either mutually independent [107], [108] or dependent [79]. Akagi et al. [79] considered the parameter distribution and approximate it by a Gaussian Mixture Model (GMM), where parameter probability distributions represent the covariance of variables. Methods to approximate parameter distributions from a given data set are summarized in [126]. A naive approach for scenario exploration is to search randomly or systematically over the scenario space. In other words, samples are mutually independent. Therefore these approaches can be implemented in a parallel manner to reduce the exploration time. However, if critical scenarios are rare, these approaches can be inefficient as the probability of sampling a critical scenario is low. On the other hand, the guided search methods have the potential to be more efficient, since the searching direction at each iteration is adjusted according to the search result of the previous iteration, so as to converge the exploration to critical regions. Each exploration method under these two types is briefly introduced in the rest of this section.
## (s50) Exploration Methods
(p50.0) Sampling: The sampling method instantiates a concrete scenario by randomly assigning each parameter's value in a logical scenario space. A predetermined number of samples are taken statistically based on probability distributions of parameters, and its sampling size is determined by the required coverage and computation time for simulation. The applied sampling methods are summarized in Fig. 18 Fig. 18. Sampling method in scenario exploration the parameter distributions are taken into account. We assume that a uniform distribution is adopted if the parameter distribution is not mentioned. Near-random sampling, such as Latin Hypercube sampling [67], can improve the coverage when the sampling size is small. It splits the multidimensional parameter space into even grids and selects samples in each grid with a given number.
## (s51) Criticality Assessment Methods
(p51.0) After attaining concrete scenarios from the CSI methods mentioned above, most of the primary studies adopt testingbased approaches to verify the criticality of derived scenarios, as shown in Fig. 19. Different definitions of criticality are discussed in Section 5.3. This section discusses how to assess the criticality of a concrete scenario under different criticality definitions. In the criticality assessment phase, most of the studies utilize an X-in-the-loop simulation to estimate the criticality of a concrete scenario, where X represents the model, software, or hardware of the SoI as a black-box. Criticalities are assessed on the simulation results based on the predefined surrogate measures summarized in Table 6. Nevertheless, the criticality can also be assessed without X-in-the-loop simulations. Validation can be realized through real-world testing to analyze the performance of the proposed exploration method [81], [86].

(p51.1) As discussed before, criticality assessment is a function whose input is a concrete scenario, and whose output is a quantified criticality index. Most of the studied CSI methods in this cluster implement this function deductively with analytical approaches, while the others implement it inductively with machine learning approaches [77], [78].

(p51.2) Inductive criticality assessment is generally applied in the data-driven approach. Instead of evaluating scenario criticality by the black-box simulation, it constructs an approximation model to emulate the real system behavior with the database. The database is composed of existing scenario data with labels, which indicates the criticality of the scenarios and is used in the training process of the approximation models. The labels of criticality measures can be attained directly from the training data in the scenario database. It can also be derived by simulation of concrete scenarios, where the simulation result is regarded as the ground truth.

(p51.3) Since criticality can hardly be measured directly, deductive criticality assessment is based on a surrogate measure to evaluate the criticality of a concrete scenario quantitatively. The surrogate measures can be either Boolean or numerical in the scoring space, depending on the types of exploration methods. For naive search methods, criticality measures are Boolean, since each sampled scenario needs to be evaluated as critical or non-critical. For guided search methods, since criticality is a part of the objective function, the criticality measure has to be quantified as numerical. Under each criticality definition, one or more surrogate measures were proposed in different primary studies, as summarized in Table 6. More measures of criticality can be found in [136]. Based on Table 6, the surrogate criticality measures used in this cluster are summarized as follows.

(p51.4) KPI-based measures are commonly used in this cluster due to the simplicity of implementation. It describes the proximity to a critical state through simulations driven by black-box testing. KPI-based methods calculate metrics by assessing a posterior measurement of the vehicle state. The metrics are used to evaluate the criticality in a scene, and the criticality of a scenario can be subsequently indicated by the worst scene. In the context of the safety-critical scenario measurement, KPI metrics refer to Time-to-X metrics (e.g., Time-to-Collision, Time-to-Brake, and Time-to-React), distance-based metrics (e.g., longitudinal and lateral distance), velocity-based metrics (e.g., relative speed), and acceleration-based metrics (e.g., required deceleration). A comprehensive analysis of KPI comparison can be found in [137]. The metrics above are usually used as the measures of a particular function. However, compared to the implementation-specific category, the non-implementationspecific ones can be found only with the generic features of the SoI, for example, a highly abstract simulation model which is generic enough to represent different implementations of the SoI or a criticality measure that is only based on the environmental aspects. The critical scenarios are detected without information of the implemented function, which makes it possible to apply scenarios for system design [107] and early phase of verification [86]. Also, the driveable area [69], [70] can be viewed as a special measure in this category, where the criticality is defined directly on a logical scenario by examining the range of relevant parameters. The SoI (e.g., general motion planning algorithms) will be more critical if the solution space is smaller with a less drivable area. Meanwhile, besides safety-critical applications, KPIbased approaches can also be applied in functional-critical studies. Unlike safety-critical measures, function-critical assessment emphasizes the performance of a particular module of the SoI (e.g., sensor, decision-making, and actuator), which does not necessarily propagate to an accident.

(p51.5) Besides the KPIs mentioned above, in order to increase the efficiency of critical scenario detection, complexity can be regarded as an auxiliary property of criticality in some studies [117], [118], [119], [120]. Compared to traditional software testing, critical scenario generation focuses on finding triggering conditions instead of explicit software bugs. Different value selections correspond to different levels of complexity, which is treated as a priori knowledge before performing the test case generation. By our definition in Section 5.3, they are viewed as non-implementation-specific and consequence-unaware, since the complexity is not exclusively oriented to any particular malfunction type.

(p51.6) Compared to KPI-based methods, collision is a more straightforward measure in the safety-critical assessment. It generates binary output according to whether a crash happens in a simulated scenario. However, some real-world collision scenarios may not manifest on simulation due to the low fidelity of the simulation model. In this cluster, collision analysis only appears in the implementation-specific category since collision avoidance has to be realized with certain types of ADAS/ADS.

(p51.7) A more complex and specific measure in implementation-specific class can be represented by formal specifications such as signal temporal logic (STL) [57], [65]. This kind of method can be applied to either safety-critical or function-critical use cases. The safety-critical approach examines the scenarios that may lead to accidents, while the function-critical measure aims to find anomalies in subsystem-level function and test its robustness.

(p51.8) In addition, implementation-specific criticality can also  Fig. 20. Mechanisms for coverage of scenario space exploration methods be characterized by performance boundary, which can be divided into two types. The first type is the predefined boundary, where the exploration methods try to separate critical scenarios to non-critical ones through the boundary and find avoidable collisions [67]. In the second type, the performance boundary is unknown at the beginning. Several performance modes are derived through clustering the scoring space. Scenarios around the boundaries of the performance modes are of great interest since slight changes of parameter values can contribute to the behavior change.

(p51.9) It is assumed that faults tend to manifest in those scenarios [65]. Moreover, we distinguish the study in [77] as the consequence-unaware type for the function-critical use case, since a scenario is regarded as critical if a small change of its configuration leads to significant changes in the SoI performance. By this definition, consequential malfunctioning behavior is not explicitly given. In the above-mentioned types of studies, the criticality assessment is realized by simulation.

(p51.10) Apart from the safety-critical and function-critical consideration, quality of service (QoS) indices can also be applied as surrogate measures. In this cluster, the normal operations of the vehicle are assumed, and QoS is quantified to judge the performance of a system, such as by fuel consumption [77], passengers' comfort [78], and overall traffic quality [125].
## (s52) Coverage
(p52.0) For safety argumentation, coverage must be considered when exploring a logical scenario. However, not all the primary studies in this cluster explicitly discuss coverage. As shown in Fig. 20, this section summarizes the consideration of coverage and the mechanisms to increase the diversity of the identified critical scenarios in this cluster.

(p52.1) Combinatorial coverage (i.e. N-wise coverage) [41], [57], [65], [117], [138] and sampling size [67], [86], [107] are the two metrics found in the primary studies in this cluster to measure the coverage of the exploration. These two coverage metrics are detailed in Section 6.1.2.

(p52.2) Another definition related to coverage is the diversity of the identified critical scenarios. Similar scenarios have a high potential to reflect the same triggering condition. Diverse critical scenarios can help to identify different functional insufficiencies and hazardous events. The rest of this section discusses mechanisms to increase the diversity of the identified critical scenarios.

(p52.3) In the primary studies, diversity is quantitatively defined based on the distance in the scenario space. For samplingbased exploration methods, distances are generally estimated in the Euclidean space. Different measures, such as Voronoi interpolation [89], can be used at sampling to ensure no tests exist in close proximity. Moreover, various sampling methods, such as Latin Hypercube Sampling [67], can also improve the diversity of samples. It divides each parameter into intervals and ensures the scenario space is evenly covered.

(p52.4) On the other hand, for optimization or learning-based methods, diversity can be considered part of either the objective or the constraint. The diversity can be realized either by examining the distance among scenarios [77], or by exploring multiple local minima [88], [95]. For the latter approach, Wagner et al. [88] extracted key features from critical scenarios by principal component analysis (PCA) and added noise in a lower-dimensional component space to reconstruct more varied critical scenarios. Gangopadhyay et al. [95] applied Bayesian Optimization to identify multiple minima regions from a non-convex function, where critical scenarios are situated in minima regions. Both methods mentioned above aim to find critical scenarios with diverse failure conditions.
## (s56) Scenario model
(p56.0) A scenario model includes a set of parameters. Some of them have a predefined value (assumed parameters) while others (parameters of interest) should be optimized to find a critical concrete scenario, as shown in Fig. 22. Examples of assumed parameters in the primary studies are the number of other actors (moving traffic participants) [91], the number of lanes [69], and the position of the crosswalk [58].

(p56.1) Referring to Fig. 15, parameters of interest in this cluster may include both constant (over time) parameters and parameter trajectories, or only parameter trajectories. An example of constant parameters is the the number of agents in a particular scenario in [91], which is explored within a predefined range.
## (s60) Coverage
(p60.0) Coverage is not mentioned by most of the papers in this section. Some of them include a measure to promote better coverage, but none of them address it as the main focus. In [66], a novelty function is computed based on the Mahalanobis distance to achieve better coverage and avoid local minimum. [59] includes a trajectory dissimilarity reward to promote the discovery of highly diverse failure scenarios.
## (s61) Required information
(p61.0) All the studies analyzed in the section require a simulator. Some studies require access to the internal state of the simulator, as in [58], while others treat them as a blackbox as in [60]. In [46], as mentioned before, a backward search is performed, where their method tries to get to a safe state from a future unsafe state. Due to the impossibility of computing the inverse of the ACC control law, the authors generate random inputs for the ACC vehicle to try to get the vehicle in the previous state. Then they simulate forward to ensure getting into an unsafe state again.
## (s64) Based on various types of data and scenarios
(p64.0) Compared to the previous section, articles [74], [94], [101], [109] in this section do not use accident databases as the data source to find the critical pre-crash scenarios. Their used data type can be mainly grouped into sensor data, traffic data and others.

(p64.1) Data type: The sensor data in this section refers to real sensor data from field operations. This data is mainly used for the virtual assessment of ADAS/AD functions to find critical scenarios. As shown in Fig. 23, traffic data was also used here as a data type, refering to the public map data and traffic flow data. This data was used for microscopic traffic simulation (i.e., each vehicle and its dynamics are modeled individually, no detailed individual sensor models and function inside) parameterization to find critical scenarios. The other data types are concrete scenarios from available analyses or simulations. These data types are mainly used for the generation of new accident scenarios from prerecorded data.
## (s65) Reasoning methods:
(p65.0) The main reasoning methods identified in this section are safety analysis, Virtual Assessment of Automation in Field Operation (VAAFO), simulation and others. Safety analysis was used as a method for scenario analysis and simplification for reducing the number of test scenarios for HAV test and evaluation [101]. First, the concrete scenarios set are analyzed. Second, by analyzing concrete scenarios through the traversal of trajectories, trajectories that lead to collisions or test tasks uncompleted are obtained. By analyzing these trajectories, the SCPs (scenario characteristic parameters) of the corresponding scenario are obtained using functional decomposition [15], combined with fault tree analysis (FTA). By analyzing the overlap or relationship among the SCPs, the inclusion relationship among scenarios is obtained according to the SCPs included in different scenarios. By searching for the combination that contains the fewest scenarios but still covers all the SCPs, and using this set of scenarios to replace the original combination of test scenarios, the redundant evaluation scenarios were deleted. Using simulation as the method, the authors in [74] used public traffic data to calibrate SUMO (simulation tool, Simulation of Urban MObility) to perform traffic simulation. Based on the simulation results, the data is post-processed to extract concrete crash scenarios.

(p65.1) VAAFO [94] is used as a method in parallel with human driving for critical scenario identification. In VAAFO (Virtual Assessment of Automation in Field Operation), the vehicle is driven by a human. It receives information from all sensors, but is not connected to the actuators and instead operates in parallel while the human driver drives the vehicle. AD functions are running (simulating) within the perceived world. The trajectory planned by the AD function is compared with the human driving trajectory. The AD function may take a different decision than the human driver. This difference may consequently affect the behavior of other vehicles. The authors in [94] state that accident scenarios made by human drivers will be recorded by the police. Potential critical scenarios are filtered further to identify real critical scenarios after correction of the world model and criticality metrics. The VAAFO method will identify scenarios that are risky for the AD but not for human drivers.

(p65.2) As another method, the authors in [109] uses Long Short Term Memory (LSTM) networks to generate new collision data from prerecorded data. The data used to train the Recurrent Neural Networks (RNNs) comes from a simulation environment, but it could also make use of real accident data. In the example developed in the study, the data includes speed, the direction of vehicles and traffic light data. Once trained, the network can be used to generate new collision data starting from an initial seed that contains the initial speed, direction and traffic light state.

(p65.3) Criticality assessment: The criticality definitions of the articles in this section are also shown in TABLE 6. The criticality definition in [101] is based on safety analysis. This paper uses the term hazardous scenario, which refers to a scenario that may lead to harm, caused by the functional limitation or failures of the system. A scenario is critical if it is possible to have a collision in this scenario due to FuSa (Functional Safety) or a SOTIF problem with the ego vehicle. In [94], the criticality was defined via the assessment module, the simplest measure is whether a real collision has occurred or not, and the criticality definition also considered SOI. Therefore, it was also implementation-specific and related to a collision. The definitions in papers [74] and [109] do not consider a specific SoI. [74] defined a scenario risk index and [109] used RNN for criticality definition based on the collected data in simulation, and both are therefore non-implementation-specific.
## (s67) Required Information
(p67.0) All the required information in this section relates to sensor data, unstructured accident records, structured accident records, sentence template for NLP, simulator, FTA analysis, pre-crash scenario classes, traffic data and real driving sensor data. The involved databases are Korea National Police Agency Accident Database, NHTSA (National Highway Traffic Safety Administration), IGLAD [155] (Initiative for the Global Harmonization of Accident Data), Second Strategic Highway Research Program (SHRP2), China In-Depth Accident Study (CIDAS) and German In-Depth Accident Study (GIDAS).
## (s68) Deductive Reasoning
(p68.0) This section focuses on the methods that use a deductive reasoning approach based on different sources of knowledge to find critical functional or logical scenarios. Based on this scope, a total of 6 related articles were identified.

(p68.1) Four of these papers [30], [61], [62], [63] focus on finding pre-crash scenarios by systematically considering all the possibilities under a set of explicitly pre-defined assumptions. The identified pre-crash scenarios can be used as safety-critical operational situations as introduced in Fig.  3. These four papers adopt a similar approach. As the first step, they take some available logical scenarios from function/system specification or safety analysis. Then, they define a structured road and initial conditions based on the function specification (e.g., 3 lane highway road) for the reasoning. Afterwards, the scenario is elaborated using the assumptions, e.g., by adding more vehicles and changing the vehicles' behaviors. The assumptions include: potential behaviors of vehicles on the road defined in the ODD, possible collision types, traffic rules, and function (SoI) features. All the identified pre-crash scenarios should be used for system verification and validation.

(p68.2) In addition to finding pre-crash scenarios as critical operational situations, there is also an approach to find complete critical scenarios, which also include a triggering condition. The authors in [64] proposed a method to identify and to quantify the risk of critical scenarios for highly automated driving vehicles, considering both functional insufficiencies and failures. They proposed a new method to combine triggering conditions with the fault tree to deduce critical scenario from a given safety goal. The approach can be summarized into the following steps: The first step is to simulate the automated driving functions as components and interfaces. The second step is to identify potential hazards related to the AD system. Using a HAZOP-like method, they first identify generic vehicle-level hazards that are independent of the underlying implementation. Second, they use a HAZOP-like approach again to identify Functional Insufficiencies with hazardous effects. Each AD function is treated as a black box. Next, an "environmental fault tree" is used to identify the causes of functional inadequacies and the corresponding environmental conditions for triggering a hazardous scenario.

(p68.3) Ponn et al. [111] proposed a methodology for an intelligent selection of relevant scenarios for the certification of automated vehicles. They proposed a two-stage optimization framework to generate concrete scenarios. A detailed optimization method was not proposed. In the first optimization stage, the parameters of Layer one, two and five (refer to the 6-layered model discussed in Sec. 2.3.1) are first optimized by sensor analysis and consideration of driving behavior. In addition, the trajectory of the potential conflict partner (Layer L4) is determined. In the second stage, further objects are defined (to refine the logical scenario) by considering the complexity, and their trajectories are optimized.

(p68.4) Criticality assessment: The criticality definitions of the articles in this section are all shown in TABLE 6. All the articles in this section relate to collision and are nonimplementation specific. No implementation of a specific system is needed for these approaches. Basically, a critical scenario should contain at least one accident threat or collision (i.e., one accident type may potentially happen).
## (s75) Mechanisms for Coverage
(p75.0) Coverage is rarely discussed in the primary studies in this cluster. The only explicitly mentioned coverage is the neuron coverage (i.e., the proportion of neurons activated by the whole test set) [116], which assesses the test adequacy of neural networks. [157] This coverage is implementationspecific, and can not reflect the completeness of the identified scenes. A coverage defined based on the ODD can be a potential future research topic.
## (s80) Online risk assessment
(p80.0) Online risk assessment refers to the methods which evaluate and predict in real time if a situation could be dangerous and result in harm. As summarized in [158], these methods analyze the predicted trajectories of other vehicles and the potential colliding point. The methods presented in section 6.1.3 are mainly offline methods, whereas the methods here are for real-time vehicle application.

(p80.1) Relation: For both online and offline approaches, the identified influential factors as well as the critical scenario based on unexpected behaviors or events can be used. Second, the end-to-end scene risk prediction used in online risk assessment can also be used offline to determine a scene's criticality. Furthermore, risk indicators such as Time-To-Collision and Time-To-React can be used for both methods.

(p80.2) Differences: The online methods can only utilize the real street data whereas the CSI methods can utilize groundtruth environmental information. Offline methods, on the other hand, are not so easily transferred to online methods because the offline approach typically requires exhaustive searches, and computing all the potential trajectories of the vehicles is computationally costly hens not real-time capable.
## (s81) Scenario-based Function Assessment
(p81.0) Function assessment is one of the main reasons for using scenarios in the development, verification and validation of ADSs. It is also one of the primary reasons for finding critical scenarios as discussed throughout this paper. Here we can consider both broad descriptions of scenarios, e.g., [36], [159], used for functional assessment, as well as different ways to achieve coverage of the scenario space in general, e.g., through Monte-Carlo (MC) simulations. Furthermore, [87] suggests a method using a heuristic search and a fitness function to ensure that the scenarios specified are actually tested. It is also possible to do functional assessment using MC simulations of agent and simulation models [160]. In [160] these models are parameterized with different data sources to reflect actual driving scenarios.

(p81.1) Relation: As mentioned, function assessment is a common vein across many of the papers considered in this review, and one of the primary purposes for using scenarios in general is to achieve some kind of assessment of the function. This paper primarily focuses on the assessment of critical scenarios whereas [36], [87], [159], [160] consider scenarios in a broader sense of AD function assessment. Having a "driver's license test" for ADSs would perhaps also be considered a function assessment. This aspect is however not considered in this review nor further in this section.

(p81.2) Differences: The primary difference to the approaches of this section is the lack of explicit discussion about how to find or identify critical scenarios. In [36], [159] scenarios are used as important descriptors to achieve function assessment, but there are no discussions about how to identify critical ones. Ensuring that specified scenarios are indeed tested, as described in [87], is surely relevant, but again outside of the scope of this review as [87] disclose no details with respect to finding critical scenarios. One could however note that the described methods could be used to ensure that critical as well as non-critical scenarios that are specified are executed. Monte-Carlo simulation, in general, is not covered in this review as MC only provides a brute force methodology for covering scenario space. However, importance sampling, as suggested in [161], is covered since it can be used to find the scenarios that are more critical to the system. The authors of [161] have also considered using subset simulation for accelerated testing of ADSs [162]. Similar to importance sampling, subset simulation guides the MC search for relevant scenarios using a KPI. If this KPI is chosen to capture a safety-critical aspect, this approach would also yield critical scenarios.
## (s84) Ontology Design and Influential Factor Analysis
(p84.0) The articles and methods here focus on finding and analyzing the influencing factor, and utilizing a domain ontology to capture and represent the environment of the system under test. As presented in Fig 3, a scenario condition can be modeled as a combination of several scenario factors. The main steps of the ontology design method in the section are similar to the method in the reasoning section regarding both deductive and inductive reasoning. They took input or knowledge from different sources, such as system specification, sensor error type, system knowledge or analysis like FTA, or FMEA analysis [166].

(p84.1) Relation: The articles in this section are not included in the main content. The reason is that they focus mainly on the use of ontology design to find triggering conditions or influencing factors, but these identified influencing factors can be further used in CSI method to find the critical scenarios.
## (s85) Formal Methods
(p85.0) Formal methods are used to define and verify unambiguous specifications of computer systems. These methods rely on a sufficiently complete abstract model of the real world and formal specifications that should resemble the informal requirements that should be imposed on the system. [167] Relation: This literature review covers the papers which use formal methods to find counter examples as critical scenarios.

(p85.1) Differences: It is necessary to represent the relations between the behaviors of the agents and environmental conditions with hybrid automata, which need to be sophisticated enough to reflect reality and simple enough to guarantee computational feasibility.
## (s86) Unknown Unknowns Detection in Computer Vision
(p86.0) Unknown unknowns in computer vision (CV) functions (e.g., object classification) refer to the case where the employed predictive model (e.g., an deep neural network) assign incorrect labels to instances with high confidence [168]. Explainable AI [169] approaches can be adopted to detect and avoid unknown unknowns.

(p86.1) Relation: These unknown unknowns are typically caused by the mismatch between training data and the cases encountered at test time. Since the CV functions are almost blind to such errors, they can be considered as implementation-specific critical scenes.
## (s90) Coverage
(p90.0) Within the context of this paper, coverage can be defined in 3 ways (referred to as coverage types in the following): 1) the coverage of the exploration with respect to the given scenario space; 2) the coverage of all the critical scenarios in the given scenario space (i.e., the proportion of the identified critical scenarios among all the critical scenarios within the given scenario space); and 3) the coverage of all the critical functional insufficiencies and their triggering conditions under a given functional scenario or an ODD. The type 1 coverage evaluates the exploration of the scenario space. The type 2 coverage evaluates the effectiveness of a logical → concrete CSI method. As shown in Fig. 6, identified critical scenarios are used to support the identification of critical functional insufficiencies so as to improve the safety of the intended functionalities. Therefore, the type 3 coverage is essential for safety analysis. These three types of coverage are discussed in the rest of this section.

(p90.1) The type 1 coverage is valid for the CSI methods finding critical concrete scenarios from a given logical scenario. Most of these methods are covered in clusters C1, C2 and C5. Due to the huge size of the scenario space, reaching full coverage is practically impossible. To quantify the level of coverage, coverage metrics can be defined. Adopted coverage metrics include sampling size (for sample-based exploration methods) and combinatorial coverage (for combinatorial testing methods). It is difficult to directly measure the type 2 and type 3 coverage. The type 2 coverage can be indirectly reflected by the coverage metrics defined for the type 1 coverage. As discussed before, the type 3 coverage is of the most importance. However, the relationships between these three types of coverage have not been sufficiently discussed in the primary studies. Understanding these relationships can help to determine a sufficient coverage level (in terms of a particular coverage metric) on the scenario space. It can also help to analyze the completeness of the safety analysis. These relationships can be discussed from the perspective of how critical scenarios may be missed.

(p90.2) Relation between type 1 and type 2 coverage: With potential limitations of an adopted exploration method, it may not be possible to identify all the critical concrete scenarios in a given scenario space (i.e., the given logical scenario). According to Fig. 16, these limitations can stem from the coverage level employed by the instantiation process or the criticality assessment method. Since full coverage is not reached, some critical scenarios may not be covered by the specified coverage level. Even though a critical scenario is reached during the exploration, it may still be assessed as noncritical due to the limitation of the criticality assessment method. If a simulator is used for criticality assessment, its fidelity (i.e. the influential factors in the simulator can represent those factors in real world) will affect the accuracy of the criticality assessment. Increasing the fidelity of simulators is one way to solve this problem. However, a high-fidelity simulator normally entails high computational resources, and thereby increases power and time consumption. Another way to solve this problem is to design surrogate criticality measures that can tolerate some simulation error (i.e. the differences between simulation and real results). For example, instead of collision, Time To Collision (TTC) can be used to find more potentially critical scenarios. However, as analyzed in Section 6.5.3 and shown in Table 6, no such surrogate measure was found in the primary studies to find critical scenes for the CV methods. On the other hand, some critical scenarios may also be filtered out by the limitation of the adopted surrogate criticality measure. For example, if longitudinal TTC is used as the surrogate measure, critical scenarios caused by lateral collision will be missed. Therefore an appropriate approach to determine the criticality measure can be another future research direction. In addition, another way to increase the fidelity of the simulator is to bring the vehicle into the loop of the simulation, such as the methods introduced in [94] and [82].

(p90.3) Relation between type 2 and type 3 coverage: Even if all the critical concrete scenarios within the given logical scenario are identified, one can still not claim a full coverage of the type 3 coverage under the corresponding functional scenario. Due to the potential misalignment between the logical scenario and the functional scenario, there might be critical scenarios that are not covered by the logical scenario. The reason for the misalignment is the assumptions made when formalizing the functional scenario (referring to Fig.  4). For example, some influential factors might be missing, or the models of some influential factors might be too simplified (e.g., using a constant speed model to represent the behavior of another vehicle). Section 8.5 briefly discusses how to find and formulate influential factors. Even though it is not the focus of this survey paper, it is an important topic for the safety analysis in terms of SOTIF. A literature review on influential factor identification could be a valuable future work.
## (s93) Other Sources of Harm
(p93.0) Potential lack of Communication: As shown in [172], some functional insufficiencies cannot be resolved without V2V (Vehicle to Vehicle) and V2I (Vehicle to Infrastructure) communication. Examples of such functional insufficiencies include occlusions, traffic violation by other road users, and the uncertainty of behavior prediction.
