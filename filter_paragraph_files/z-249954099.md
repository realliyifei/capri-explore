# Synthetic Data-Based Simulators for Recommender Systems: A Survey

CorpusID: 249954099 - [https://www.semanticscholar.org/paper/74aa80b9c39724146682a55642de59002ed2460c](https://www.semanticscholar.org/paper/74aa80b9c39724146682a55642de59002ed2460c)

Fields: Computer Science

## (s9) Brief description of simulators
(p9.0) We start this section with a brief description of simulators that shows the versatility of approaches to their development and applications. Note that descriptions of all the simulators under consideration in a short form are given in Table A.5 in Appendix.

(p9.1) RecSim Ie et al. (2019) is a customizable platform for modeling sequential user interactions with RL-based RSs. RecSim provides flexibility in the creation of a modeling environment: for example, it is possible to determine some aspects of user behavior (e.g. user preferences on certain properties of items can change over time), as well as the attribute structures of items offered to users. Note that the framework implementation in the case of modeling a complex system can be complicated because the approach assumes the creation of an abstraction levels platform for modeling certain aspects of user behavior. The positive point is that the authors of RecSim assume the re-usage of the simulator's components in different simulation environments.

(p9.2) In Huang et al. (2020), SOFA (Simulator for OFfline leArning and evaluation), which is based on a method for correction of the interaction bias present in the data, is proposed. The problem of interaction bias is often ignored in simulators, where modeling of user feedback based on historical data is usually used. It is not possible to avoid the usage of historical data in the process of testing RL-based methods, as the online testing process can lead to an unfavorable user experience. A method, that corrects interaction bias present in historical data before this data will be used to construct user behavior models, is proposed in Huang et al. (2020) as a solution to the problem of interaction bias. In addition, the authors present a new method to evaluate the impact of interaction bias on the quality of the recommender algorithm. Both methods (bias correction and evaluation of bias impact) are the basis of the proposed simulator.
## (s18) Models for generating user-item responses
(p18.0) The In UserSim Zhao et al. (2021), a user response model is based on a generative adversarial network, where the generator is used to generate synthetic logs based on the historical data, while the discriminator is utilized to predict users' behavior. The generative adversarial network is also used in CLL Chen et al. (2019) for the simulation of user's behavior (in the form of sequential choices) and learning of the user's reward function.
## (s23) Bias and other negative effects studied by simulation
(p23.0) Let us consider a typical situation in an RS lifecycle. An e-commerce company offers some products to users and exploits an RS for that. Meanwhile, data scientists in the company work on improving the existing RS. They do offline development as well as evaluation and devise a new RS that outperforms the old one in some offline accuracy-based Parapar and Radlinski (2021) metric like nDCG 10 . Then it is a pretty common situation Huzhang et al. (2021) that once the new RS is deployed in production, the online performance of the new RS is worse than the performance of the old one. This is the example of offline-online inconsistency so ,in this section, we explore its origins and the solutions simulators may offer to it.
## (s25) Unrealistic metrics
(p25.0) To evaluate an RS offline, a typical procedure consists of dividing the available data into training and test sets. The RS is trained on the training set, part of which is usually used for validation. After training, RS is used for making predictions on the test set and comparing predictions with real data using some metrics. These metrics are called accuracy-based since they are computed on a test set using the true responses. The most frequent ones are Precision@K, Recall@K, AUC@K, nDCG@K, MAP@K. However, it has been noticed that the users' behavior is more complex than these metrics would imply. In particular, users would like to discover new content, and value diverse and surprising recommendations. Hence, there are beyond accuracy metrics proposed to evaluate an RS output: Novelty, Serendipity, Unexpectedness, Diversity and others Silveira et al. (2019).
## (s64) Brief description of simulators
(p64.0) We start this section with a brief description of simulators that shows the versatility of approaches to their development and applications. Note that descriptions of all the simulators under consideration in a short form are given in Table A.5 in Appendix.

(p64.1) RecSim Ie et al. (2019) is a customizable platform for modeling sequential user interactions with RL-based RSs. RecSim provides flexibility in the creation of a modeling environment: for example, it is possible to determine some aspects of user behavior (e.g. user preferences on certain properties of items can change over time), as well as the attribute structures of items offered to users. Note that the framework implementation in the case of modeling a complex system can be complicated because the approach assumes the creation of an abstraction levels platform for modeling certain aspects of user behavior. The positive point is that the authors of RecSim assume the re-usage of the simulator's components in different simulation environments.

(p64.2) In Huang et al. (2020), SOFA (Simulator for OFfline leArning and evaluation), which is based on a method for correction of the interaction bias present in the data, is proposed. The problem of interaction bias is often ignored in simulators, where modeling of user feedback based on historical data is usually used. It is not possible to avoid the usage of historical data in the process of testing RL-based methods, as the online testing process can lead to an unfavorable user experience. A method, that corrects interaction bias present in historical data before this data will be used to construct user behavior models, is proposed in Huang et al. (2020) as a solution to the problem of interaction bias. In addition, the authors present a new method to evaluate the impact of interaction bias on the quality of the recommender algorithm. Both methods (bias correction and evaluation of bias impact) are the basis of the proposed simulator.
## (s73) Models for generating user-item responses
(p73.0) The In UserSim Zhao et al. (2021), a user response model is based on a generative adversarial network, where the generator is used to generate synthetic logs based on the historical data, while the discriminator is utilized to predict users' behavior. The generative adversarial network is also used in CLL Chen et al. (2019) for the simulation of user's behavior (in the form of sequential choices) and learning of the user's reward function.
## (s78) Bias and other negative effects studied by simulation
(p78.0) Let us consider a typical situation in an RS lifecycle. An e-commerce company offers some products to users and exploits an RS for that. Meanwhile, data scientists in the company work on improving the existing RS. They do offline development as well as evaluation and devise a new RS that outperforms the old one in some offline accuracy-based Parapar and Radlinski (2021) metric like nDCG 10 . Then it is a pretty common situation Huzhang et al. (2021) that once the new RS is deployed in production, the online performance of the new RS is worse than the performance of the old one. This is the example of offline-online inconsistency so ,in this section, we explore its origins and the solutions simulators may offer to it.
## (s80) Unrealistic metrics
(p80.0) To evaluate an RS offline, a typical procedure consists of dividing the available data into training and test sets. The RS is trained on the training set, part of which is usually used for validation. After training, RS is used for making predictions on the test set and comparing predictions with real data using some metrics. These metrics are called accuracy-based since they are computed on a test set using the true responses. The most frequent ones are Precision@K, Recall@K, AUC@K, nDCG@K, MAP@K. However, it has been noticed that the users' behavior is more complex than these metrics would imply. In particular, users would like to discover new content, and value diverse and surprising recommendations. Hence, there are beyond accuracy metrics proposed to evaluate an RS output: Novelty, Serendipity, Unexpectedness, Diversity and others Silveira et al. (2019).
