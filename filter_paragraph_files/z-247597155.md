# A survey on GANs for computer vision: Recent research, analysis and taxonomy

CorpusID: 247597155 - [https://www.semanticscholar.org/paper/4876459cc2abb2189c41a4e2ec23c6407048920e](https://www.semanticscholar.org/paper/4876459cc2abb2189c41a4e2ec23c6407048920e)

Fields: Computer Science

## (s6) Mode collapse
(p6.0) The objective is to generate synthesized data from a latent space, which requires not only quality in the generated data, but generalization and diversity in the different synthesized samples. In other words, GAN models should be able to recreate new unseen data. Mode collapse occurs when the same class outputs are generated by different inputs from the latent space [207].
## (s10) Evaluation metrics
(p10.0) Due to GAN's particularity, there is not an unique metric to measure the quality of the synthesized data [190]. One of the reasons of why there is no consensus among researches is the particularity of each GAN application. As mentioned in previous sections, GANs can be used to replicate any data distribution, but it depends on the particular problem how to measure the differences between the origin and synthesized distributions [17].
## (s12) Multi-scale structural similarity for image quality (MS-SSIM)
(p12.0) is based on the comparison between two image structures, luminance and contrast at different scales [181]. The MS-SSIM provides a metric that compares the similarity between the real and the synthesized dataset. One of the strengths of MS-SSIM is that it correlates closer pixels with strong dependence. In comparison with other metrics such as Mean Squared Error (MSE), that calculates the absolute error of an image,
## (s21) Auxiliary Classifier GAN (ACGAN)
(p21.0) ACGAN [130] modifies the CGAN structure. The D of the ACGAN does not receive the class label c as an input, instead D is used to classify the probability of the image class. To train the model, the loss function must be modified, dividing the objective function in two parts, one for the correct source of data and the other for the class label. ACGAN loss function can be denoted as:
## (s24) Cycle-Consistent GAN (CycleGAN)
(p24.0) Cyclic consistency is the idea that, given a data x from a domain A, if the data is translated to a domain B and translated again to the A domain it should be recovered the data x. In other words, if a sample is translated to a domain and recovered from that domain, it should not change. This process, where a data sample is transformed and recovered, is known as cycle consistency, and it has been widely used during the last decades [162,75].

(p24.1) This idea is the main base of CycleGAN [211]. The main strength of the application of cycles is that paired data is not a requirement. GAN architecture adds a new mapping denoted as F, its function is to do the inverse mapping to retrieve the original data. In other words, the function of F is F (G(x)) = x. To train the architecture, a new cycle consistency loss is proposed to train the so-called forward and backward cycle consistency. The cycle consistency loss is denoted as follows:
## (s29) Self Attention GAN (SAGAN)
(p29.0) SAGAN [199]  SAGAN uses self attention layers [174], these layers are capable to capture structural and geometric features of multiclass datasets. The feature maps of each convolution are split into a 1 Ã— 1 convolution in query, key and value, then they are multiplied to construct the output of the layer. This way the network can learn long-range dependencies. The structure of the self-attention layer can be seen in Fig.5. 
## (s32) A GAN Through Quantum States (QuGAN)
(p32.0) During the last decade, quantum computing has become a hot topic in computer science. Since it was proposed in 1980 [14] it has always been restricted to a few laboratories around the world. Thanks to the progress made recently [114], it has made possible to test the first algorithms, prototypes and ideas [22].

(p32.1) Thanks to quantum computing particularities, problems previously defined can be solved, or are optimized, reducing their computation time. Using quantum superposition, the multiples solutions can be evaluated simultaneously, then by using quantum interference and entanglement the correct answer can be defined.
## (s34) Classification Enhancement GAN (CEGAN)
(p34.0) Data imbalance is a common problem when using real world datasets. Dataset often contains a majority of samples of a certain data class. In the case of GANs using unbalanced datasets, the imbalance problem results in poor quality of the synthesized data of the class with less samples.

(p34.1) CEGAN [160] tries to solve the data imbalance problem in GAN. The objective is to enhance the quality of the synthesized data and to improve the accuracy of the predictions.

(p34.2) The CEGAN architecture consists of 3 different networks, G, D and a new network known as the classifier (C). The training of the CEGAN divides in two steps. In the first step, the architecture is normally trained, using D to differentiate between fake and real samples, C is used to classify the class label of the input sample. Then, in the second step, an augmented training dataset is formed via generating new samples from G, and this new dataset is used to train the C.
## (s35) Mobile Image Enhancement GAN (MIEGAN)
(p35.0) The MIEGAN [135] presents a novel architecture that aims to improve the quality of images taken with a mobile phone. To do so, two new networks are proposed, the so-called multi-mode cascade generative network and the adaptive multi-scale discriminative network. The generative network is composed of an Autoencoder architecture. The encoder of this new generator is divided into two streams, the inclusion of the second encoder is in charge of improving the low luminance areas, where mobile phones particularly lack in their clarity.

(p35.1) The discriminator network has a dual goal. First, the global discriminator ensures overall image quality. Second, a local discriminator maintains the local quality of small areas of the image. To combine both objectives, an adaptative weight allocation module is also proposed that is responsible for balancing the importance of each discriminator.
## (s38) WGAN-GP
(p38.0) In the original paper of WGAN, the authors suggest that weight clipping is "a terrible way to enforce Lipschitz constraints". Weigh clipping is one problem that the original WGAN had, but it worked well enough and its implementation was easy. The WGAN-GP [55] proposes a new technique to substitute the weight clipping that leads to the WGAN with undesired behavior.

(p38.1) The proposed change involves constraining the critic gradient norm output regarding to the input of the network. The constraint is softened via a penalty on the gradient norm. say that the new loss function is denoted as follows:

(p38.2) The new change makes the WGAN-GP optimize its training, stabilizing it with almost no hyperparameter tuning. The new loss function also improves the quality of the generated images over WGAN and converges faster.
## (s40) Least Square GAN (LSGAN)
(p40.0) The new loss function presented in LSGAN [116] aims to reduce the vanishing gradient problem. The main objective of the LSGAN is to punish the synthesized samples that are far from the real data but still in the correct side of the decision boundary. The least squares loss function is denoted as follows:

(p40.1) where a and b are the labels for fake and real data respectively and c is the label that G wants D to believe is real data. It should be noted that the square of both equations is responsible for punishing far from the decision boundary samples.

(p40.2) The LSGAN tries to generate more gradients while penalizing samples that lie a long way from the decision boundary. This way the gradients are forced to be higher, preventing the gradient vanishing problem. Compared to the classical sigmoid cross entropy loss function of GANs, the new least squares loss is flat only at one point as we can see in Fig.8. 
## (s50) GAN applications
(p50.0) As mentioned before, GANs are one of the most popular applications of machine learning of the last years. GANs models can achieve results in fields where previous models could not, in other cases, GANs improve the previous results significantly.

(p50.1) In this section, we will review the most important fields where GAN architectures are applied, paying a special attention to the GAN models related to computer vision tasks and we will compare the different architecture results.

(p50.2) Most of the last researches focus on how to apply GANs to generate new synthesized data, replicating a data distribution. But, as we will review in this section GANs can be applied to other fields, e.g. video game creation [80].
## (s54) Image generation from text
(p54.0) Since the introduction of CGAN the capabilities of GANs were expanded. The possibility of constraint the synthesized information that GANs produced made the networks have a wider range of application. By controlling the output of the generations of the networks the applications of them can be much more specific and interesting. One field were GANs have shown to outperform previous techniques in image generation from text [88].
## (s60) GANs in agriculture
(p60.0) Similar to the medical imaging field, obtaining images to train the computer vision models of agricultural image analysis is not an easy task. These models benefit from having large-scale balanced datasets but the cost of obtaining high quality labelled data makes the data augmentation a crucial task in these datasets.

(p60.1) Many different GAN models have been applied to agricultural data, such as [95,191,69]. These works aim to generate new images of plant with different diseases, augmenting the number of samples by using GAN.

(p60.2) In these cases the use of GAN improves the results of the machine learning models by enlarging the number of available data. The agricultural images have different particularities that make the analysis of them a difficult task. For example the biological variability between two samples of the same specie makes crucial to have many different samples to learn all the modes of the data. In particular, the same leaf of a fruit can drastically differ from one individual to another.

(p60.3) Other important factor is that the labelling of the data can be very costly, specially for specific applications such as the disease detection of certain plant, e.g. tomato leaf [95].

(p60.4) In addition, the environment where the images are taken, most of the time in crops, can lead to many variance in the images, such as lighting changes or object occlusion.
## (s61) Drug discovery using GANs
(p61.0) The process of discovering and designing new drugs has recently been impulsed by the field of Deep Learning [70,32]. In particular, GANs are an useful technique to synthesize new useful samples of data. In the drug environment, the GAN architecture can process the drug compound using graphs or Simplified Molecular Input Line Entry Specification (SMILES), to then generate synthetic samples of drugs.

(p61.1) Due to the flexibility that ANNs have in terms of operating with different data types, it is possible to use the same architectures in different fields. In this case the overall GAN design can be adapted to molecular data, being able to transfer the same principles of the image generation to new data types.

(p61.2) The research followed by Kadurin et al. [73,74] generates new drug compounds for anticancer therapy, using biological and chemical datasets. In particular, in [74] it is used an Adversarial Autoencoder that uses molecular fingerprints as inputs of the network. By using this architecture the researches are able of define the desired properties of the synthesized drugs. Some of the new synthetic drugs discovered by the Deep Learning architecture corresponded with previous known anticancer drugs.
