# A survey of Object Classification and Detection based on 2D/3D data

CorpusID: 170078919 - [https://www.semanticscholar.org/paper/b2c768b7ddc9a038111a1361ef25828adfdcbdf8](https://www.semanticscholar.org/paper/b2c768b7ddc9a038111a1361ef25828adfdcbdf8)

Fields: Engineering, Computer Science

## (s0) Introduction
(p0.0) By using deep neural network based on algorithms, especially convolutional neural networks based algorithms, computer vision systems based on 2D images have been making great achievements in image classification, object detection and semantic segmentation since the year 2012. For some scenarios, deep neural network based algorithms can achieve a similar or even better performance on 2D image classification/detection and semantic segmentation than the human expert, but 3D-based systems are not as developed yet.

(p0.1) The survey is organized as follows: in the second section, 2D-based systems are introduced and in the third section the 3D-based systems are introduced.   [4] demonstrates the tasks of Image classification, Object Detection, Semantic Segmentation and Instance Segmentation. Image classification is recognizing the interesting objects in each image and output the objects categories. Object Detection will not only output the objects categories but also the location of those objects with bounding boxes. Both semantic segmentation and instance segmentation will output the pixel level location of each object. The difference between those two tasks is that semantic segmentation does not distinguish the instance in the same category while the instance segmentation does. Figure 2: An early Neural Network used in [5].
## (s2) Main Networks used for Image Classification
(p2.0) Some traditional algorithms used for the image classification are nearest neighbor and SVM. The features are the flattened pixel values. In the year 1989, the first important application [5] of using BP(Back Propagation) appeared. From this paper, a basic structure is shown in Figure 2. Similar structures are used in the modern neural networks such as AlexNet [6], VGG 16 [7] and ResNet [8]. The basic idea of the CNN was also introduced in [5].
## (s3) Object detection
(p3.0) In the object detection research area, just like in the image classification where the ImageNet dataset is available to train the algorithm, in the object detection, COCO(Common Objects in Context) [4] and VOC dataset can be used to train the algorithms. For the COCO dataset, the bounding box and the mask of the objects are provided. The first important contribution of using deep neural networks to solve the object detection problem is the R-CNN [20]. The region of interest(ROI) of an image is proposed by the selective search(SS) [21] algorithm, and the ROI is cropped to feed a CNN to do the detection. However, as every proposed interesting area will be calculated to predict whether that specified region is an object, the speed of this algorithm is very slow. In order to address this problem, a fast R-CNN algorithm is proposed in [22] by improving the feature map generation efficiency. In another paper which is short for faster RCNN [23], instead of using the SS algorithm to generate ROI, the ROI is proposed by using deep neural network structure. By the combination, the performance of the algorithm can also be improved itself. Figure 11: The 2D center offset box encoding method.
## (s4) Bounding box encoding method
(p4.0) In the object detection task, one import part is estimating the bounding box of the object. The 2D bounding box estimation only focus on axis-aligned boxes. The box encoding method is simply based on a simple center coordinates of the bounding box (x, y) and the offset of the bounding box: width: W and height: H. We are calling this method as 2D center offset box encoding method and it is shown in Figure 11. The RCNN, Fast RCNN, Faster RCNN, YOLO, YOLOv2 and Mask R-CNN are using this kind of bounding box encoding with a slightly difference on the loss calculation. [20], Fast RCNN [22] and Faster RCNN RCNN is an important framework in 2D image detection task which mainly uses the CNN to extract the features for each cropped interesting region. After that the extracted features are fed to a SVM to do the classification and a bounding box regression is followed to improve the bounding box prediction based on the method from [24]. It mainly has two stages: region proposal and detection. As the detection process is computation expensive, the region proposals can make the detection step mainly focus on limited interesting regions (about 2000 regions for a typical image) which greatly reduces the complexity of the whole system and achieves a good performance on the 2D image detection task. This two-stage detection framework is becoming a classical model in both 2D image based object detection and 3D image based object systems. The frame work of RCNN is shown in Figure 12. Figure 13: Fast R-CNN [22] architecture. An input image and multiple regions of interest (RoIs) are input into a fully convolutional network. Each RoI is pooled into a fixed-size feature map and then mapped to a feature vector by fully connected layers (FCs). The network has two output vectors per RoI: softmax probabilities and per-class bounding-box regression offsets. The architecture is trained end-to-end with a multi-task loss.  Fast R-CNN improves the RCNN mainly with respect to three aspects: First, instead of doing convolution operations separately for each proposed region, the Fast RCNN does the convolution operations for the whole image firstly and then uses region proposals from the feature map directly to do the further detection. The feature map level region proposals are projected from the region proposals based on the original image. Second, using the softmax layer to replace the SVM classifier to make the detection under one deep learning framework. Finally, Fast R-CNN is using the Multi-task loss to do the object classification and the bounding box regression. The Fast RCNN framework is shown in Figure 13. In order to have a same size of feature vectors from different size proposed regions, the ROI pooling is used and the ROI pooling is demonstrated in Figure 14. Faster RCNN Figure 15: The three main steps for the Faster RCNN [23], system: head(backbone network), RPN and detection network.
## (s6) HEAD
(p6.0) The main contribution of the Faster RCNN is introducing the region proposal network(RPN) under the deep learning framework. Before the RPN, the regional proposal is done by traditional method such as SS which is used in both RCNN and Fast RCNN. Since traditional methods such as SS and Edge Box are calculated by CPU, the speed is slow. At the same time, RPN can be calculated by GPU, also the convolutional layer of the RPN and the detection network can be shared, the detection speed for the whole framework of Faster RCNN improved a lot. The framework of Faster RCNN is shown in Figure 15. It mainly contains three steps: the head is used to extract the features by using CNN, then the RPN is used to get the region proposal. Finally, it is using the detection network to do the object detection. As shown in Figure 16, 1) the RPN is done based on the feature map instead of the original image by using a sliding window method. The size of feature map is smaller than the original image by the pooling layer of the CNN. For example, when the original image size is 1000×600, if the 4 pooling layers are used, then the size of the feature map will be 62 × 37. The smaller size of feature map makes the regional proposals much faster. The proposal is done by using a k × k sliding window (the k in 3 in Faster RCNN), and different size and ratio anchors are used to get more accurate proposals. For each anchor, the RPN network will output 1) two scores: foreground score and background score 2) proposal bounding box: by using 2D center offset encoding, it will output 4 values. In Faster RCNN, 3 size and 3 ratio are selected. The comparison of the RPN and SS is shown in Table 1. From the result, we can see, the RPN is faster and the performance is better than SS. The YOLO and YOLO v2 systems are one-stage detection systems. They do not have a separately region proposal stage. Instead they divide the original image roughly into a S × S grid and then based on those grid cells will be used as a rough region to do the further processing.     Here in order to make a fair comparison, it is resized by using the same dimension as YOLO.

(p6.1) Another important paper for the object detection is YOLO [26] [27]. For the YOLO algorithm, the speed of detection is faster than the RCNN approach, however, the performance is slightly reduced. The Darknet structure which is used for YOLO is shown in Figure 19. The Network, the bounding box encoding and the performance comparison is given in Table 2. Meanwhile, the comparison of the proposal candidates numbers generated by the two method are compared in Table 3. From the comparison shown in Table  3 we can explain that YOLO is faster because fewer proposals are considered by YOLO. However, this introduces worse performance and unsuccessful detection of small objects.  Table 4: Some common object detection frame work by stages: two-stage methods and one-stage methods one-stage method # candidate objects YOLO v1 [26] 98 YOLO v2 [27] ∼1k OverFeat [34] ∼1-2k SSD [31] ∼8-26k(hard-example mining) RetinaNet [33] ∼100-200k("soft" hard-example mining) From the description above we can have a general understanding that for the object detection, there are two kinds of methods: 1) Two-stage methods Detector 2) One-stage methods. Part of those detection algorithms are organized by those two different stage methods and are shown in Table 4. Actually, there is a competition between the two-stage methods and one-stage methods. Generally, the one-stage method is improved by introducing more proposals as shown in Table 5 and by introducing new loss functions such as Focal loss [33] to get rid of the unbalance between the positive proposals and negative proposals. Meanwhile, two-stage methods are also trying to improve the speed by introducing the lighter head, for example Light Head R-CNN [29]. The comparison of those different stage method's performance is shown in Figure 20 and 21 based on the COCO dataset. 
## (s9) 3D-image based systems
(p9.0) In this section, 3D-image based systems will be introduced. The main highlevel tasks in 3D vision, 3D image data representation and methods used to do the 3D classification and object detection are briefly described. Several import papers in this area are surveyed. Figure 27: A 3D shape reconstruction example: 3D-RecGAN reconstructs a full 3D shape from a single 2.5D depth view [42] Similar to the 2D systems, the main tasks also include 3D object classification, 3D object detection, 3D semantic segmentation and 3D instance segmentation. Also, as 3D images commonly suffer from occlusion such as self occlusion and inter object occlusion. In order to address this issue, a 3D shape reconstruction task is also actively researched in the 3D vision understanding community. This part is important, however, it will not be surveyed. An example of 3D shape reconstruction is shown in Figure 27 3.2 3D image data representation 3D images can have multiple data representations, such as multi-view RGB-D images, volumetric images, polygonal meshes and point clouds.    3D images are becoming more and more important and are widely used in reconstructing architectural models of buildings, navigation of self-driving cars, detection face (such as face ID for iPhone X), preservation of at-risk historical sites, and recreation of virtual environments for film and video game industries. Mainly, there are two basic kinds of hardware available for the 3D data generation in outdoor and indoor environments. For outdoors, one typical hardware is LiDAR( Light Detection and Ranging). The coverage of this equipment can achieve to hundreds and even thousands meters. Most selfdriving cars use a LiDAR scanner. For indoors, in recent years the availability of low-cost sensors such as the Microsoft Kinect have enabled the acquisition of short-range indoor 3D data at the consumer level. Meanwhile, smart phone such as iPhone X a equipped will a depth camera. In Figure 29, one example of the 3D data collected from the outdoor urban LiDAR scanner is shown. In Figure 28, depth map generated by a Kinect camera is provided. Capturing a 3D environment by a robot is shown in Figure 30. A RGB-D camera based object detection system is used to help a robot to grasp objects as shown in Figure 31. Velodyne's HDL-64E LiDAR sensor is commonly used for Self-driving cars. There is a new LiDAR sensor with 128 laser beams released from Velodyne in 2017. The photos of both the 64 beams sensor and the 128 beams sensors is given in Figure 32. The sensor's vertical scan range is shown in Figure 33 while the horizontal scan range is shown in Figure 34. Raw data collected by Velodyne's HDL-64E LiDAR sensor is shown in Figure 35.
## (s11) Classification
(p11.0) The classification in 3D is mainly based on the CAD(Computer-aided design) model. One import dataset is ModelNet [47]. ModelNet has 127915 3D CAD models from 662 categories. ModelNet10 and ModelNet40 are two subsets of the ModelNet. ModelNet10 has 4899 models from 10 categories and Mod-elNet40 has12311 models from 40 categories. For the classification task, the papers will be discussed based on the leaderboard of the ModelNet40. The leaderboard is given next.
## (s16) Novel view point models
(p16.0) RotationNet is an extension of MVCNN [60]. In this paper, multiple views from different angles are explored. Three models of camera views are proposed as shown in Figure 37. The performance of case(i) (the same view points model as MVCNN [60]) and case(ii) are compared. Case (ii) achieves a better performance based on the ModelNet40 task. For the ModelNet40, the case(iii) model is not used.
## (s17) Unsupervised view point estimation
(p17.0) RotationNet is using an unsupervised approach when using the view point information: the view point is not provided directly during the training process (the view point are also not available in most dataset such as Model-Net40) but inferred during the training process. The authors of RotationNet use the following optimization method to find the possible viewpoints. RotationNet is defined as a differentiable multi-layer neural network R(). The final layer of RotationNet is the concatenation of M softmax layers, each of which outputs the category likelihood P (ŷ i , x i , v i = j) where j ∈ {1, ..., M } for each image x i . Here,ŷ i denotes an estimate of the object category label for x i . For the training of RotationNet, the set of images x i M i=1 are used simultaneously and the following optimization problem is solved to get the estimated viewpoints [64]:
## (s18) PointNet [48]
(p18.0) Applications of PointNet PointNet is a new work which is performing 3D vision understanding directly on the raw cloud point data. Applications of PointNet are shown in Figure 40. PointNet consumes raw point clouds (set of points) without voxelization or rendering. It is a unified architecture that learns both global and local point features, providing a simple, efficient and effective approach for a number of 3D classification tasks. Figure 41: PointNet Architecture. The figure is from [48]. Here the "mlp" means multi-layer perceptron.
## (s23) Detection
(p23.0) Similar to 2D-image based object systems, most 3D systems are also using the two-stage methods to do the 3d object detection: first, generate proposals and then do detection. At the same time, the unique properties of the 3D systems, such as different data representation and the availability of both 2D and 3D images, make the 3D detection framework more complicated and more interesting. We will discuss the datasets used for detection and main works in indoor and outdoor scenarios next.     For 3D understanding, the detection output is more complicated than the 2D case. It contains the output of the class and the bounding box. The class output is similar to the 2D detection task. For the bounding box detection, it can output the image plane bounding box, bird's eye view(BEV) bounding box and 3D bounding box. Meanwhile, for the 2D object detection, the bounding boxes are axis aligned, however, the BEV and 3D bounding boxes are not axis aligned. Finally, the orientation of BEV and 3D bounding box will also be detected in some tasks. The detection outputs are summarized in Table 8. Examples of each output are shown in Figure 48. In this survey, we mainly focus on the 3D bounding box detection.  Most frameworks are based on two-stage methods where the proposals are firstly generated and then the detection will be done based on the proposals. F-PointNet [75] is a special case. It has three stages: first, get a frustum proposal from the detected 2D bounding based on the RGB image. The difference of the frustum proposal with the proposal from two-stage methods is it has a class label. second, 3D Instance segmentation is done based on the proposal. Finally, a 3D box is estimated based on the segmentation results. In order to represent a 3D bounding box, different methods are proposed, such as 8-corner method, 3D center offset method and 4-corner-2-height method as shown in Figure 49.  Different input data can be used to detect the 3D bounding box, such as the Monocular image, Stereo image and depth/LiDAR image. The detection system organized by the input data type is given in Table 10. Generally speaking, the 2D image only system including both the monocular and stereo image will perform worse than the 3D only or 2D+3D system. The comparison of the 2D image only and the 2D+3D system is provided in Figure 66.
## (s28) Comparison by performance
(p28.0) The performance comparison of the different systems is provided in Table 16 , 17 and 18 for the outdoor scenario based on the KITTI [69] dataset and in Table 12 for the indoor scenario based on the SUN-RGBD dataset [67].   3D object detection systems can be categorized by the supported application scenarios: indoor only, outdoor only or both. There are two main differences between indoor and outdoor scenarios: first, the range of indoor is small and of outdoor is large. A comparison of the indoor range and outdoor range based on two typical datasets is shown in Table 13. Second, as the distribution of the outdoor objects is more sparse and the categories of interesting objects of the outdoor scenarios is less compared with indoor scenarios, the outdoor scenarios can use BEV to generate the proposals and then do the detection. However, the indoor scenarios, generation only based on BEV will get a bad performance since there might be multiple objects in the vertical direction.

(p28.1) BEV only proposal generation algorithms such as MV3D [72] do not performance well indoors. At the same time, as the space is too large for the outdoor scenario, some 3D CNN based algorithms, such as Deep Sliding Shape [71], can work well for indoors but may have a high possibility to fail for the outdoor scenario without adjustment.  In the rest of this section we will introduce several of the latest papers organized by different application scenarios as shown in Table 14.
## (s30) Deep Sliding Shapes [71]
(p30.0) The TSDF 3D Representation A directional Truncated Signed Distance Function(TSDF) is used to encode 3D shapes. The 3D space is divided into 3D voxel grid and the value in each voxel is defined to be the shortest distance between the voxel center and the surface from the input depth map. To encode the direction of the surface point, a directional TSDF stores a three-dimensional vector [dx, dy, dz] in each voxel in order to record the distance in three directions to the closest surface point instead of a single distance value [71]. The example of the directional TSDF for the RPN and detection network is shown in Figure 50.

(p30.1) The 3D RPN Figure 51: 3D Region Proposal Network: Taking a 3D volume from depth as input, a fully convolutional 3D network extracts 3D proposals at two scales with different receptive fields. Figure and Caption are from [71].

(p30.2) Deep Sliding Shapes [71] is inspired by the Faster RCNN [23] framework. The proposals are generated by a CNN based RPN. The RPN is shown in Figure 51. The network structure shown in Figure 51:
## (s35) FV features for MV3D
(p35.0) MV3D projects the FV into a cylinder plane to generate a dense front view map as in VeloFCN [84]. The front view map is encoded with threechannel features, which are height, distance and intensity as shown in Figure  64. Since KITTI uses a 64-beam Velodyne laser scanner, the size of map for the front view is 64 × 512.  [72] The performance of MV3D is evaluated based on the outdoor KITTI dataset. The performance of 3D object detection based on the test set can be found from the leaderboard. The performance of 3D object detection based on validation dataset is shown in Figures 65 and 66. It only provides the car detection results. Detection results for the pedestrians and cyclists are not provided.   The framework of AVOD is shown in Figure 68. AVOD is using the same encoding method as MV3D for the BEV. In AVOD, the value of M is set as 5 and the range of the LiDAR is [0, 70] × [−40, 40] × [0, 2.5] meters. So the size of the input feature for the BEV is 700 × 800 × 7. AVOD is using both the BEV and image to do the region proposals which is the main difference to the MV3D work.  VoxelNet architecture is shown in Figure 71. The feature learning network takes a raw point cloud as input, partitions the space into voxels, and transforms points within each voxel to a vector representation characterizing the shape information. The space is represented as a sparse 4D tensor. The convolutional middle layers processes the 4D tensor to aggregate spatial context. Finally, a RPN generates the 3D detection. A fixed number, T , of points from voxels containing more than T points are randomly sampled. For each point, a 7-feature is used which is (x, y, z, r, x− v x , y − v y , z − v z ) where x, y, z are the XY Z coordinates for each point. r is the received reflectance and (v x , v y , v z ) is the centroid of points in the voxel. Voxel Feature Encoding is proposed in VoxelNet. The 7-feature for each point is fed into the Voxel feature encoding layer as shown in Figure 72. Fully connected networks are used in the VFE network with element-wise MaxPooling for each point and concatenation between each point and the element-wise MaxPooling output. The input of the VFE is T × 7 and the output will be T × C where C depends on the FC layers of the VFE itself and depends on the whole VFE layers network used. Finally, an element-wise MaxPooling is used again and change the dimension of the output to 1 × C. Then for each voxel we have a one vector with C elements as shown in Figure 71. For the whole framework, we will have an input data with shape of C ×D ×H ×W .
## (s45) Data representation methods summary for 3D system
(p45.0) From the surveyed 3D systems, we can see the importance of the data representation to the performance. Here the pros and cons of different data representation methods to the classification and detection tasks is summarized:

(p45.1) • using Projected multiple view RGB images -Pros : It is a similar to the human being's recognization process for a 3D object by looking from different views. Since it can encode the multiple view info into 2D RGB image, it can take the advantage of well developed 2D image recognization system such as 2D CNN to further classify or detect 3D objects.

(p45.2) -Cons : Sometime, not all the desired multiple-view RGB images are available. In some papers which use multiple views such as RotationNet [48] and MVCNN [60], views from different angles are used to do the classification. This is possible when the whole object's CAD model is available. However, in the real application such as the autonomous cars scenario, from the self-driving cars' perspective, it can only take multiple-view of objects from one side during the driving process. The other side of the objects cannot be observed due to self-occultations. The partial availability of all views will reduce the performance of the algorithms based on all views.
