# Mapping Knowledge Representations to Concepts: A Review and New Perspectives

CorpusID: 255372420 - [https://www.semanticscholar.org/paper/af1477d23c956de9aa0362b4abefc0ac9a9d1f10](https://www.semanticscholar.org/paper/af1477d23c956de9aa0362b4abefc0ac9a9d1f10)

Fields: Computer Science

## (s4) Interventions (What if I do?)
(p4.0) In the reviewed research this type of question is addressed by: exposing typical and atypical images related to a concept (Chen et al. 2020;Lucieri et al. 2020), relate a decision to one of a number of predefined disentangled concepts (Chen et al. 2020) or building simplified models over the decision logic (Elshawi, Sherif, and Sakr 2021;Rabold, Schwalbe, and Schmid 2020). We arrange these systems as aiming for intervention in relation to their input data and their labels since they present both contrastive and non-contrastive explanans thus making it possible for a domain expert to construct what if explanations. Knowledge priors added are, for example, the selection of disentangled concepts or using a decision tree as the structure for a contrastive explanation.

(p4.1) Counterfactuals (What if I had done?) Since counterfactual explanations build on a capability to imagine alternative futures, from an historical point in time, there is a need for temporal data for this type of explanation. In the reviewed work there is one example based on electronic health records and a comparison between how a specific treatment at a certain situation, for example, antibiotics treatment, can be evaluated in comparison to alternative treatment (Mincu et al. 2021).

(p4.2) Explanation categories If we view the reviewed research through the lens of D-N explanations the explanations aims for either a model-data or kind-data relation. Data are in all cases images except in Mincu et al. (2021) where temporal tabular data from electronic health records is used. Below we analyse the work reviewed in relation to their core relation.

(p4.3) In Figure 3, to the left, the structure of a kind-data explanation is presented. As discussed earlier we view the trained model as a law under scrutiny and not a model in D-N sense. For example, in Lucieri et al. (2020), that focuses on identifying skin lesions, the images presented as similar to the image to be explained, together, with the trained model create the explanans that are a selected part of the available explanandum. This explanandum then answers a question of the form: This instance belongs to the concept quality a (a specific skin lesion concept) presenting these similar images as evidence/explanans for the decision (quality b). If a human with domain knowledge finds that these evidence sufficient, perhaps by combining them with other factors as experience, known sub-kind concepts or data not included in the training data then an explanation that builds on causality (if C then D) valid in the real world can be formulated by the domain expert.

(p4.4) In one example by Chen et al. (2020)  This explanation is contrastive from the trained model's perspective in the sense that it explains the classification and that it is likely that images containing a bed and a person will be classified as bedrooms. Here, again, knowledge priors in the form of selection of kinds and sub-kinds, but also training data and model selection, together delimits the explanadum available. So even if the form of the explanation can be classified as intervention the explanation is not causal in the sense that it holds in a real world context, instead it gives insights in how the trained model associate input data with outputs. The same holds for the proof of concept in Mincu et al. (2021) where a system in the hands of a person holding medical expertise can be a tool useful to create counterfactual explanations. The explanans presented by the system, together with other explanantia, can open up to better understand the consequences of an alternative historical decision. For example that it is probable that using a different type of antibiotics on women than men will make women recover faster.

(p4.5) In Figure 3, to the right, the structure of a model-data explanation is presented. We placed the work that compares the human decision process with the ML decision process as a core relation between model and data since the aim is to use an overlap as an explanan and useful model over a trustworthy decision process (Natekar, Kori, and Krishnamurthi 2020;Lucieri et al. 2020;Yeche, Harrison, and Berthier 2020). In Wang et al. (2020) calculations of sufficient and necessary causes are used to explain a decision. Knowledge priors added here are then under which conditions a presupposed value of an alignment to human learning is useful as an explanan and under which conditions it is possible to calculate sufficient and necessary cause in an inductive learning process.

(p4.6) In two reviewed work a model, in a D-N sense, over the decision process is created. In the work by Rabold, Schwalbe, and Schmid (2020) sub-kinds are automatically identified and used to build first order logical rules covering spatial relations in images (the relative placement of EYES, MOUTH and NOSE useful to identify a FACE). In Elshawi, Sherif, and Sakr (2021) a decision tree based on sub-kinds is constructed and used to explain classification of pictures, answering contrastive questions like: Why is this image classified as a COAST and not a MOUNTAIN?.

(p4.7) The work reviewed does not include any research that uses a theory-data relation or a entity-data relation (See Table 1). Related to theory-data it can be argued that using a surrogate model implicitly presumes that the proposed decision can be explained using, in this case, a decision tree or first order logic (Rabold, Schwalbe, and Schmid 2020;Elshawi, Sherif, and Sakr 2021).

(p4.8) There is in the reviewed work no trained model that focus on entity-data relations, relations that a neural network can learn well in a similar fashion as it can learn kind-data relations. Entity-related concepts follows the instance and can be related to ageing or wear and tear, for example, SCRATCHES, SPLINTERS or MARKINGS and can be useful to, for example identify objects and estimate ageing (Holmberg, Generalao, and Hermansson 2021).
