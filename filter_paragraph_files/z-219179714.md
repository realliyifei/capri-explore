# Generate FAIR Literature Surveys with Scholarly Knowledge Graphs

CorpusID: 219179714 - [https://www.semanticscholar.org/paper/3e4de425e4a8db948b94cdc2fe14aed8d0cac6e9](https://www.semanticscholar.org/paper/3e4de425e4a8db948b94cdc2fe14aed8d0cac6e9)

Fields: Computer Science

## (s1) USE CASES
(p1.0) We motivate our work by means of two use cases that underscore the usefulness of a literature survey generation system. In the rst use case, a researcher wants to obtain an overview on state-of-theart research addressing a speci c problem. The second use case describes how a researcher can publish a FAIR compliant literature review with the ORKG.

(p1.1) Familiarize with the state-of-the-art. A state-of-the-art (SoTA) analysis reviews new and emerging research. They are useful for multiple reasons. Firstly, they provide a broad overview of a research problem and support understanding. Secondly, they juxtapose di erent approaches for a problem. Thirdly, they can support claims on why certain research is relevant by giving an overview of the breadth of research addressing a problem. The proposed approach enables automated generation of surveys to quickly obtain an overview of state-of-the-art research as well as sharing of surveys for others to reuse.

(p1.2) Publishing of literature reviews. Literature reviews typically consist of multiple (survey) tables in which di erent approaches from original papers are compared based on a set of properties. These tables can be seen as the main contribution and most informative part of the review paper, since the tables juxtapose and compare existing work. Comparison tables are published in review papers as static content in PDF documents. This presentational format is generated from datasets that typically contain more (structured) information than what is presented in the published table. However, the additional information is not published. It is "dark data" which is not stored or indexed and likely lost over time [12]. Furthermore, published tables are not machine actionable. Their overall low FAIRness hinders reusability of the published content. With the presented service, it is possible to publish a literature survey with high FAIRness, i.e. that is compliant with the FAIR principles to a high degree. Section 3 discusses this aspect in more details.
## (s2) RELATED WORK
(p2.0) The task of comparing research contributions can be reviewed in light of the more general task of comparing resources (or entities) in a knowledge graph. While this is a well-known task in multiple domains (for instance in e-commerce systems [42]), not much work has focused on comparison in knowledge graphs, speci cally. One of the few works with this focus is by Petrova et al. [28] who created a framework for comparing entities in RDF graphs using SPARQL queries. In order to compare contributions, they rst have to be found. Finding is an information retrieval problem. As a well-known technique, TF-IDF [21] can be used for this task. More sophisticated techniques can be used to determine the structural similarity between graphs (e.g., [20]) and matching semantically similar predicates. This relates to dataset interlinking [1] or more generally ontology alignment [34]. For property alignment, techniques of interest include edit distance (e.g., Jaro-Winkler [41] or Levenshtein [19]) and vector distance. Gromann and Declerck [10] found that fastText [4] performs best for ontology alignment.
## (s3) SYSTEM DESIGN
(p3.0) We now present the system design of the literature comparison service. It consists of a methodology that describes how to perform a comparison of research contributions. An early version of this methodology has been presented at the 3rd SciKnow workshop [24]. The methodology consists of ve steps: 1) nding comparison candidates, 2) selecting related statements, 3) aligning contribution descriptions, 4) visualizing comparisons and 5) publishing FAIR comparisons. The methodology is depicted in Figure 1  discuss the data structure of the ORKG, which forms the foundation of the comparison. Then, each step of the methodology is described in more detail. Finally, we discuss the implementation.
## (s4) ORKG ontology
(p4.0) In ORKG, each paper is typed as paper class. A paper consists of at least one research contribution, which addresses at least one research problem. Research contributions consist of contribution data that describe the contribution. For instance, a paper in Computer Science might have descriptions for materials, methods, implementation and results as contribution data. These prede ned core concepts can be easily extended with domain speci c research problems, methods, etc. in ORKG curation using crowdsourcing or other curation approaches. The underlying data structure uses the notion of statements. Statements are triples that consist of a subject, a predicate (also called a property) and an object. The granularity of a comparison is at the research contribution, meaning that contributions are compared rather than papers. For simplicity, we use the terms "paper comparison" and "contribution comparison" interchangeably. Because a comparison happens on contribution level, it is possible to compare speci c elements of a paper instead of the complete paper. The bene t of this is that a comparison does not contain data from irrelevant contributions. The ORKG OWL ontology is available online. 4 
## (s6) Find similar contributions.
(p6.0) Comparing contributions makes only sense when contributions can sensibly be compared. For example, it does not make (much) sense to compare a biology paper to a history paper. We thus argue that it makes only sense to compare contributions that are similar. More speci cally, contributions that share the same (or a similar set of) properties are good comparison candidates. For instance, a paper about question answering has the property orkg:disambiguationTask 5 and another paper is 4 https://gitlab.com/TIBHannover/orkg/orkg-ontology 5 orkg: denotes the ontology of the ORKG system described in Section 4.1  using the same property to describe what disambiguation tasks are performed. Since they share the same property it makes them likely candidates for comparison. Finding similar contributions is therefore based on nding contributions that share the same or similar informative description properties. To achieve this, each comparison contribution is converted into a string by concatenating all properties of the contribution. TF-IDF [21] is used to query these strings with the string of the main contribution as query. The search returns the most similar contributions by weighting the most informative properties higher due to TF-IDF. The top-k contributions are selected and form a set of contributions that are used in the next step. Figure 2 displays how the similar contribution selection is implemented. As depicted, three similar contributions are suggested to the user (with the corresponding similarity percentage being displayed next to paper title). These suggested contributions can be directly compared.
## (s9) Align contribution descriptions
(p9.0) As described in the rst step, comparisons are built using shared or similar properties of contributions. In case the same property has been used between contributions, these properties are grouped and form one comparison row. However, often di erent properties are used to describe the same concept. This occurs for various reasons. The most obvious reason is when two di erent ontologies are used to describe the same property. For example, for describing the population of a city, DBpedia uses dbo:populationTotal while WikiData uses WikiData:population (actually the property identi er is P1082; for the purpose here we use the label). When comparing contributions, these properties should be considered as equivalent. Especially for community-created knowledge graphs, di erently identi ed properties likely exist that are, in fact, equivalent.

(p9.1) To overcome this problem, we use pre-trained fastText [4] word embeddings to determine the similarity of properties. If the similarity is higher than a predetermined threshold τ , the properties are considered equivalent and are grouped. This happens when the similarity threshold τ ≥ 0.9 (also empirically determined). In the end, each group of properties will be visualized as one row in the comparison table. The result of this step is a list of statements for each contribution, where similar properties are grouped. Based on this similarity matrix γ is generated
## (s15) EVALUATION
(p15.0) In this section, we present an evaluation of multiple aspects of the presented comparison methodology and implementation. Firstly, we evaluate information representation. Then, we evaluate the  Table 4 Author name disambiguation 5 https://orkg.org/orkg/c/vDxKdr No Hussain and Asghar [13] Table 5 Author name disambiguation 6 https://orkg.org/orkg/c/XXg8Wg No Hussain and Asghar [13] Table 6 Author name disambiguation 9 https://orkg.org/orkg/c/9rOwPV No Hussain and Asghar [13] Table 7 Author name disambiguation 6 https://orkg.org/orkg/c/mB7kIK No Naidu et al. [23] Figure 6: Partial graph structure of an imported paper. Orange colored resources indicate potentially interesting values for a paper comparison.
## (s19) DISCUSSION & FUTURE WORK
(p19.0) One of the aims of the contribution comparison functionality is to support literature reviews and make this activity less cumbersome and time consuming for researchers. To live up to this aim, more structured contribution descriptions are needed. Existing scholarly knowledge graph initiatives focus primarily on scholarly metadata, while with ORKG we focus on making the actual research contributions machine readable. Currently, the ORKG does not yet contain su cient contribution descriptions in order for the comparison functionally to be practically useful for researchers. Furthermore, for an evaluation of the e ectiveness of certain components of the methodology (such as nding related papers or aligning similar properties), more contribution data is needed. Publishing surveys does not rely on data quantity and is therefore evaluated more extensively in this work. The performance evaluation results indicate that the comparison feature performs well. This means the technical infrastructure is in place for the literature survey service.

(p19.1) In the evaluation, we focused on the aspects of the system that are necessary for researchers to use the system in practice. The information representation evaluation is a straightforward evaluation to see if existing survey tables can be regenerated with the ORKG. This is a minimal requirement for researchers when using the system, since they should at least be able to recreate tables. This evaluation does not give insight to the usefulness and usability of the system, but still provides an indication that the service can be successfully used to publish literature surveys. One of the reasons for using the service is that also "dark data" in comparisons is published (as discussed in Section 2).

(p19.2) Another interesting aspect of the service is that published literature surveys rank high in FAIRness. Therefore, the second part of the evaluation focuses on how the FAIR principles are met. Merely publishing data as RDF is not su cient to fully meet the FAIR principles. Hence, we conducted a more detailed evaluation that describes how the service complies with each sub-principle. Since FAIR is not a standard, the principles are permissive and not prescriptive [22]. No technical requirements are speci ed. Both the implementation and evaluation of the guidelines are therefore subject to interpretation. With respect to data interoperability and reusability, certain aspects of the service can be improved. For example, to improve interoperability, the contribution data should be reusing existing vocabularies where possible. Additionally, although most of FAIRi cation is done by the system, the researcher is responsible for adding correct and relevant metadata while publishing a survey.
