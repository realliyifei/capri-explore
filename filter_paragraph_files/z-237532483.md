# A Survey on Temporal Sentence Grounding in Videos

CorpusID: 237532483 - [https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453](https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453)

Fields: Computer Science

## (s1) METHOD OVERVIEW
(p1.0) We establish the taxonomy of existing approaches based on their characteristics. As shown in Fig. 2, early works adopt a two-stage architecture, i.e., they first scan the whole video and pre-cut various candidate segments (i.e., proposals or moments) via sliding window strategy or proposal generation network, and then rank the candidates according to the ranking scores produced by the cross-modal matching module. However, such a scan-and-localize pipeline is time-consuming due to too much redundant computation of overlapping candidate segments, and the individual pairwise segment-query matching may also neglect the contextual video information.

(p1.1) Considering the above concerns, some researchers start to solve TSGV in an end-to-end manner. It is unnecessary for such end-to-end models to pre-cut candidate moments as the inputs of the model. Instead, multi-scale candidate moments ended at each time step are maintained by LSTM sequentially or convolutional neural networks hierarchically, and such end-to-end methods are named anchor-based methods. Some other end-to-end methods predict the probabilities for each video unit (i.e., frame-level or clip-level) being the start and end point of the target segment, or straightforwardly regress the target start and end coordinates based on the multimodal feature of the providing video and sentence query. These methods do not depend on any candidate proposal generation process, and are named anchor-free methods.

(p1.2) Besides, it is worth noting that some works resort to deep reinforcement learning techniques to address TSGV, taking the sentence localization problem as a sequential decision process, which are also of anchor-free. To reduce intensive labor for annotating the boundaries of groundtruth moments, weakly supervised methods with only video-level annotated descriptions have also emerged. In the following, we will present all the approaches above and perform a deep analysis of the characteristics for each type.   frameworks, two pioneer works that firstly present TSGV task. CTRL uses a joint representation to get the final alignment score and refines the temporal boundaries by location regressor, while MCN tries to minimize the ℓ 2 distance between the language and video representation vectors, figures from [16] and [23].
## (s2) Two-stage method
(p2.0) For a two-stage method, the pre-segmenting of proposal candidates is conducted separately with the model computation. It takes the pre-segmented candidates and the sentence query as inputs of a cross-modal matching module for target segment localization. The two-stage methods can be grouped into two categories based on different ways to generate proposals.

(p2.1) 2.1.1 sliding window-based. Early methods including MCN [23], CTRL [16], ROLE [38], MCF [63], ACRN [37], SLTA [27] and ACL-K [18], adopt multi-scale sliding window sampling strategy for the generation of candidate proposals. There are two pioneering works MCN [23] and CTRL [16] to define TSGV task and construct benchmark datasets. Firstly, Hendricks et al. [23] propose MCN, which samples all the candidate moments (i.e. segments) via sliding window mechanism, and then projects the video moment representation and query representation into a common embedding space. The ℓ 2 distance between the sentence query and the corresponding target video moment in this space is minimized to supervise the model training (c.f ., Fig. 3b). Specifically, MCN encourages the sentence query to be closer to the target moment than negative moments in a shared embedding space. Since the negative moments either come from other segments within the same video (intra-video) or from different videos (inter-video), MCN devises two similar but different ranking loss functions:

(p2.2) where L ( , ) = max(0, − + ), is a margin. As for training sample , the intra-video ranking loss encourages sentence to be closer to the target moment at the location than the negative moments from other possible locations within the same video, while the inter-video ranking loss encourages sentence to be closer to the target one at location than the negative ones from other videos of the same location . The intra-video ranking loss is able to differentiate between subtle difference within a video while the inter-video ranking loss can differentiate between broad semantic concepts. At the same time, Gao et al. [16] propose CTRL, which is the first one to adapt R-CNN [20] methodology from object detection to the TSGV domain. Particularly, CTRL also leverages sliding window to obtain candidate segments of various lengths, and as shown in Fig. 3a, it exploits a multi-modal processing module to fuse the candidate segment representation with the sentence representation by three operators (i.e., add, multiply, and full-connected layer). Then, CTRL feeds the fused representation into another fully-connected layer to predict the alignment score and location offsets between the candidate segment and the target segment. CTRL designs a multi-task loss function to train the model, including visual-semantic alignment loss and location regression loss:

(p2.3) where is the visual-semantic alignment loss considering both aligned (video segment, query) pairs and misaligned pairs. , measures the alignment score between video segment and sentence . The location regression loss is only accounted for aligned pairs to predict the correct coordinates. is a smooth-L1 function.

(p2.4) Compared to above CTRL that treats the query as a whole, Liu et al. [38] further make some improvements by decomposing the query and adaptively get the important textual components according to the temporal video context.

(p2.5) Since CTRL overlooks the spatial-temporal information inside the moment and the query, Liu et al. [37] further propose an attentive cross-modal retrieval network (ACRN). With a memory attention network guided by the sentence query, ACRN adaptively assigns weights to the contextual moment representations for memorization to augment the moment representation. SLTA [27] also devises a spatial and language-temporal attention model to adaptively identify the relevant objects and interactions based on the query information.

(p2.6) Wu and Han [63] propose a multi-modal circulant fusion (MCF) in contrast to the simple fusion ways employed in CTRL including element-wise product, element-wise sum, or concatenation. MCF extends the visual/textual vector to the circulant matrix, which can fully exploit the interactions of the visual and textual representations. By plugging MCF into CTRL, the grounding accuracy is further improved. Previous works like CTRL, ACRN and MCF directly calculate the visual-semantic correlation without explicitly modelling the activity information within two modalities, and the candidate segments fairly sampled by sliding window may contain various meaningless noisy contents which do not contain any activity. Hence, Ge et al. [18] explicitly mine activity concepts from both visual and textual parts as prior knowledge to provide an actionness score for each candidate segment, reflecting how confident it contains activities, which enhances the localization accuracy.

(p2.7) Despite the simplicity and effectiveness of such two-stage sliding window-based methods, they suffer from inefficient computation since there are too many overlapped areas re-computed due to the densely sampling process with predefined multi-scale sliding windows.
## (s3) proposal-generated.
(p3.0) Considering the inevitable drawbacks of sliding window-based methods, some approaches devote to reduce the number of proposal candidates, namely proposal-generated method. Such proposal-generated methods still adopt a two-stage scheme but avoid densely sliding window sampling through different kinds of proposal networks.

(p3.1) QSPN [67] relieves such a computation burden by proposing temporal segments conditioned on the query so as to reduce the number of candidate segments (c.f ., Fig. 4). Specifically, QSPN comprises of a query-guided segment proposal network (SPN) to propose query-specific candidate segments, a fine-grained early-fused similarity model for retrieval and a multi-tasking loss combining retrieval task with an auxiliary captioning task.

(p3.2) As shown in Fig. 4a, the query-guided SPN first incorporates the query embeddings into the video features to get the attention weight for each temporal location, and further integrates the temporal attention weights into the convolutional process for video encoding to propose queryaware representations of candidate segments. Then as shown in Fig. 4b, the generated proposal visual feature from Fig. 4a is incorporated into the sentence embedding process at each time step of the second layer of the two-layer LSTM in a early fusion way. Then QSPN devises a triplet-based retrieval loss which is similar to MCN:

(p3.3) where ( , ) is the positive (sentence, segment) pair while ′ is the sampled negative segment. QSPN also devises an auxiliary captioning task which re-generate the query sentence from the retrieved video segment. The loss for captioning is as follows:

(p3.4) where a standard captioning loss is introduced to maximize the normalized log-likelihood of the words generated at all T unrolled time steps, over all K groundtruth matching sentence-segment pairs. Similarly, SAP proposed by Chen and Jiang [9] integrates the semantic information of sentence queries into the generation process of activity proposals. Specifically, the visual concepts extracted from the query sentence and video frames are used to compute visual-semantic correlation score for every frame. Activity proposals are generated by grouping frames with high visual-semantic correlation scores.

(p3.5) Despite the success of such a two-stage pipeline, it also has some drawbacks. In order to achieve high localization accuracy (i.e., the candidate pool should have at least one proposal that is close to the groundtruth moment), the duration and location distribution of the candidate moments should be diverse, thus inevitably increasing the number of candidates, which leads to inefficient computation of the subsequent matching process.
## (s5) anchor-free.
(p5.0) Instead of ranking a vast number of proposal candidates, the anchor-free methods start from more fine-grained video units such as frames or clips, and aim to predict the probability for each frame/clip being the start and end point of the target segment, or directly regress the start and end points from the global view. The typical methods include ABLR [75], L-Net [6], LGI [44], PMI [8], Rodriguez et al. [48], DEBUG [39], GDP [7], HVTG [10], DRN [76], ExCL [19], and VSLNet [80].

(p5.1) Yuan et al. propose ABLR [75], which solves TSGV from a global perspective without generating anchors. Specifically, as shown in Fig. 9, to preserve the context information, ABLR first encodes both video and sentence via bidirectional LSTM networks. Then, a multi-modal co-attention mechanism is introduced to generate not only video attention which reflects the global video structure, but also sentence attention which highlights the crucial details for temporal localization. Finally, an attention-based coordinates prediction module is designed to regress the temporal coordinates (i.e. the starting timestamp and the ending timestamp ) of sentence query from the former output attentions. Meanwhile, there are two different regression strategies (i.e., attention weight-based regression and attended feature-based regression) with the location regression loss :

(p5.2) where is a smooth L1 function. Besides the location regression loss that aims to minimize the distance between the temporal coordinates of the predicted and the groundtruth segments, ABLR also designs an attention calibration loss to get the video attentions more accurately:
## (s6) Reinforcement learning-based method
(p6.0) As another kind of anchor-free approach, RL-based frameworks view such a task as a sequential decision process. The action space for each step is a set of handcraft-designed temporal transformations (e.g., shifting, scaling). The typical methods include R-W-M [22], SM-RL [62], TripNet [21], STRONG [2], TSP-PRL [65] and AVMR [3].

(p6.1) He et al. [22] first introduce deep reinforcement learning techniques to address the task of TSGV, which formulates TSGV as a sequential decision making problem. As depicted in Fig. 12, at each time step, the observation network outputs the current state of the environment for the actor-critic module to generate an action policy (i.e., the probabilistic distribution of all the actions predefined in the action space), based on which the agent will perform an action to adjust the temporal boundaries. This iterative process will be ended when encountering the STOP action or reaching the maximum number of steps (i.e., ). Specifically, at each step, the current state vector is computed as:

(p6.2) where ( ) is generated by a FC layer whose inputs are the concatenated features including the segment-specific features (i.e., the normalized boundary pair ( −1) = [ ( −1) , ( −1) ] and local segment C3D feature ( −1) ) and global features (i.e., the sentence embedding and entire video C3D feature ). Then the actor-critic module employs GRU to model the sequential decision making process. At each time step, GRU takes ( ) as input and the hidden state is used for policy (denoted as ( ( ) | ( ) , )) generation and state-value (denoted as ( ( ) | )) estimation. The reward for each step is designed to encourage a higher tIoU compared to that of the last step. The accumulated reward function is then defined as ( is a constant discount factor):
## (s9) Datasets
(p9.0) Several datasets for TSGV from different scenarios with their distinct characteristics have been proposed in the past few years. There is no doubt that the effort of creating these datasets and designing corresponding evaluation metrics do promote the development of TSGV. Table 1 provides an overview about the statistics of public datasets, indicating the trend of involving more complicated activities and not being constrained in a narrow and specific scene (e.g., kitchen). We will introduce them more concretely in the following.

(p9.1) DiDeMo [23]. This dataset is collected from Flickr, and consists of various human activities uploaded by personal users. Hendricks et al. [23] split and label video segments from original untrimmed videos by aggregating five-second clip units, which means the lengths of groundtruth segments are times of five seconds. They claim that this trick is for avoiding ambiguity of labeling and accelerating the validation process. However, such a length-fixed issue makes the retrieval task easier since it compresses the searching space into a set with limited candidates. The data split is also provided by [23], with 33008, 4180, and 4022 video-sentence pairs for training, validation, and test, respectively.

(p9.2) TACoS [47]. TACoS is built based on MPII-Compositive dataset [49]. It contains 127 complex videos featuring cooking activities, and each video has several segments being annotated by sentence descriptions illustrating people's cooking actions. The average length of videos in TACoS is around 300s, which is much longer than that of other benchmark datasets. The total amount of sentence-segment pairs is 17,344 in this dataset, and 50%, 25%, 25% of which are used for training, validation, and test, respectively.
## (s10) words per sentence).
(p10.0) ActivityNet Captions [31]. ActivityNet Captions is originally proposed for dense video captioning, and the sentence-segment pairs in this dataset can naturally be utilized for TSGV. ActivityNet Captions contains the largest amount of videos, and it aligns videos with a series of temporally annotated sentence descriptions. On average, each of the 20k videos contains 3.65 temporally localized sentences, resulting in a total of 100k sentences. Each sentence has an average length of 13.48 words. The sentence length is also normally distributed. Since the official test set is withheld for competitions, most TSGV works merge the two available validation subsets "val1" and "val2" as the test set. In summary, there are 10,009 videos and 37,421 sentence-segment pairs in the training set, and 4,917 videos and 34,536 sentence-segment pairs in the test set.
## (s11) Metrics
(p11.0) There are two types of metrics for TSGV, i.e., R@ ,IoU@ and mIoU, both of which are first introduced for TSGV in [16]. Since IoU (Intersection over Union) is widely used in object detection to measure the similarity between two bounding boxes, similarly for TSGV, as illustrated in Fig. 16, many TSGV methods adopt temporal IoU to measure the similarity between the groundtruth moment and the predicted one. The ratio of intersection area over union area ranges from 0 to 1, and it will be equal to 1 when these two moments are totally overlapped.

(p11.1) Thereby, one of the metrics is mIoU (i.e., mean IoU), a simple way to evaluate the results through averaging temporal IoUs of all samples. The other commonly-used metric is R@ , IoU@ [25]. As for sample , it is accounted as positive when there exists one segment out of top retrieved segments whose temporal IoU with the groundtruth segment is over , which can be denoted as ( , , ) = 1. Otherwise, ( , , ) = 0. R@ , IoU@ is the percentage of positive samples over all samples:

(p11.2) The community is accustomed to setting ∈ {1, 5, 10} and ∈ {0.3, 0.5, 0.7}. Usually, = 1 when the method adopts a proposal-free manner (i.e., belongs to either anchor-free or RL-based frameworks). Moreover, it is worth noting that MCN [23] adopts a particular metric with the IoU  threshold = 1 since the groundtruth segments in DiDeMo is generated by aggregating the clip units of 5 seconds, and MCN also employs a matching-based method thus the predicted moment has chance to fully coincide with the target moment, satisfying such a extremely high IoU threshold.
## (s12) Performance Comparison
(p12.0) In this section, we give a thorough performance comparison of the aforementioned approaches based on four benchmark datasets. For convenience and fairness, we uniformly adopt = 1 and ∈ {0.3, 0.5, 0.7} for the metric of R@ ,IoU@ . Table 2 reports the experimental results of twostage methods, Table 3 is presented for end-to-end methods, Table 4 compares the performance of both RL-based and weakly supervised methods, and Table 5 separately reports the experimental results on DiDeMo dataset with MCN-specific metrics.

(p12.1) Two-stage method. As shown in Table 2, the overall performance of two-stage methods seems poorer than other approaches. The possible reasons lie in three folds: (1) Firstly, most of the two-stage methods combine video and sentence features coarsely, and neglect the fine-grained visual and textual interactions for accurate temporal sentence grounding in videos. (2) Secondly, separating the candidate segment generation and sentence-segment matching procedures will make the model unable to be globally optimized, which can also influence the overall performance.

(p12.2) (3) Thirdly, establishing matching relationships between sentence queries and individual segments will make the local video content separate with the global video context, which may also hurt the temporal grounding accuracy.

(p12.3) Specifically, for the sliding window-based methods, all the methods achieve the lowest grounding accuracy on the TACoS dataset compared to the other three datasets. The reason is that the cooking activities in TACoS take place in the same kitchen scene with only some slightly varied cooking objects (e.g. chopping board, knife, and bread). Thus, it is hard to do temporal location predictions for such fine-grained activities. Meanwhile, the lengths of videos in TACoS are also longer, which will greatly increase the target segment searching space and bring more difficulties. ACL-K outperforms the other sliding window-based methods by a large margin on the TACoS and Charades-STA datasets, proving the effectiveness of aligning the activity concepts mined from both textual and visual parts. MCN gets the most inferior results on the Charades-STA dataset, which shows that its simple multimodal matching and ranking strategy for candidate segments cannot deal well with the segments of various and flexible locations. However, CTRL, ACRN, ROLE, SLTA and ACL-K can adjust the candidate segment boundaries based on the model location offsets prediction, which can therefore improve the performances. All of the sliding window-based methods have not conducted experiments on the large-scale ActivityNet Captions dataset, which may due to the extremely expensive computation for multi-scale sliding window sampling.

(p12.4) The proposal-generated methods achieve even better performance than the sliding window-based methods though the number of proposal candidates decreases. QSPN with query-guided segment proposal network and auxiliary captioning loss significantly outperforms other two-stage methods on the Charades dataset, verifying that unlike sliding window-based sampling, the presented query-guided proposal network is able to provide more effective candidate moments with finer temporal granularity. QSPN also conducts experiments on ActivityNet Captions that is comprised of richer scenes and achieves competitive results, which also proves the effectiveness of captioning supervision and query-guided proposals. Since the videos in Charades-STA dataset are of shorter lengths and contain less diverse activities, it is necessary to focus more on the metrics with higher IoU thresholds. SAP consistently outperforms other sliding-window based methods on Charades-STA with a higher IoU threshold, which attributes to its discriminative generated proposals and additional refinement process.

(p12.5) End-to-end method. For anchor-based methods, TGN achieves the lowest performance on TACoS and ActivityNet Captions datasets. CMIN also performs poorly on TACoS. The common inferior accuracy achieved by TGN, CMIN and CBP may attribute to their single-stream anchorbased localization framework. With sequential RNNs, they fail to reason complex cross-modal relations. Instead of employing RNN-based frameworks, both SCDM and MAN use convolutional neural networks to better capture fine-grained interactions and diverse video contents of different temporal granularities, which consistently achieve better performance. To make further improvement, 2D-TAN extends it to 2D feature maps to model the adjacent relations of various candidate moments of multi-anchors. SMIN and Zhang et al. [82] that adopt such a similar 2D structure modelling the relationships of candidate moments, also achieve superior results out of anchor-based methods. Specifically, Zhang et al. [82] performs the best on TACoS while SMIN has surpassed other methods on Charades-STA, which also prove the effectiveness of 2D moment relationship modelling. Furthermore, CSMGAN, FIAN, SMIN and Zhang et al. [82] all achieve superior results on ActivityNet Captions dataset. It is noted that although CSMGAN adopts the similar sequential RNN like TGN but it builds a joint graph for modeling the cross-/self-modal relations which can capture the high-order interactions between two modalities effectively, and FIAN employs a symmetrical iterative attention to obtain more robust cross-modal features for more accurate localization.

(p12.6) For anchor-free methods, reading comprehension-inspired methods including ExCL, VSLNet and Rodriguez et al. [48] outperform other anchor-free methods with a significant gap. Specifically, ExCL performs the best on TACoS and ActivityNet Captions dataset while VSLNet achieves the best performance on Charades-STA dataset, which proves that adopting such mature techniques in Table 3. The performance comparison of end-to-end frameworks (AB:anchor-based,AF:anchor-free,OT:others). reading comprehension area for TSGV is available and effective. However, Rodriguez et al. [48] achieves the lowest performance on ActivityNet Captions. One possible reason is that the subjectivity of annotation is hardest to model for this challenging dataset. The dense anchor-free methods including DRN, GDP and DEBUG outperform the early sparse regression network ABLR, justifying the importance of increasing the number of positive training samples. However, the additional regression-based methods including PMI, HVTG and LGI achieve superior performance on ActivityNet Captions dataset and LGI even performs best on Charades-STA dataset, which may result from more effective interaction between visual and textual contents. It is noted that L-Net has not been included in the table since the original paper [6] did not report the specific experimental values. Additionally, other methods like BPNet, DPIN and CBLN which adopt neither anchor-based nor anchor-free achieve comparable results on three datasets. It is noted that CBLN achieves the best results out of all end-to-end methods on Charades-STA and ActivityNet Captions datasets, which quite highlights the superiority of combining the advances of both anchor-based and anchor-free and its special biaffine-based architecture.

(p12.7) RL-based method. The upper part of Table 4 reports the performance of RL-based methods for TSGV. As we can see, TSP-PRL achieves promising performance on ActivityNet Captions, proving the effectiveness of borrowing the idea of the coarse-to-fine human-decision-making process. STRONG and AVMR achieves the best performance out of the RL-based frameworks on both TACoS and Charades-STA datasets, which proves the effectiveness of spatial RL for scene tracking and the employment of adversarial learning, respectively. R-W-M, TripNet and SM-RL achieve relative inferior performance. Specifically, SM-RL achieves lowest performance on Charades-STA. TripNet keeps the lowest performance on ActivityNet Captions. Although RL-based methods can not reach the performance of end-to-end state-of-the-art methods, they offer brand-new thoughts to address the TSGV task and enhance the ability of interpretability.

(p12.8) Weakly supervised method. The experimental results of Charades-STA and ActivityNet Captions datasets for weakly supervised methods are shown at the bottom part of Table 4. The performance of DiDeMo for weakly supervised methods will be presented later. We cannot tell which framework (i.e., MIL-based or reconstruction-based) has absolute advances according to the overall performance. Specifically, CRM achieves the best performance on Charades-STA and ActivityNet Captions datasets out of all weakly supervised methods. The results are also competitive compared with those of other fully supervised methods DiDeMo evaluation results with particular metrics. As aforementioned, MCN [23] measures the results with the IoU threshold = 1. Some works [5,40,78] also followed MCN using such metrics. We supplementally list the evaluation results (i.e., R@1,m@1 and R@5,m@1) on DiDeMo at Table 5 grouped by whether the method belongs to fully-supervised or weakly supervised.

(p12.9) Specifically, LoGAN achieves the best performance among the weakly supervised methods while TGA [43] achieves the worst. As for fully supervised methods, the performance achieved by MCN and MAN is inferior to that of TGN.
## (s17) Spatio-temporal localization.
(p17.0) Spatial-temporal sentence grounding in videos is another extension from TSGV which mainly localizes the referring object/instance as a continuing spatialtemporal tube (i.e., a sequence of bounding boxes) extracted from an untrimmed video via a natural language description. Since fine-grained labeling process of localizing a tube (i.e., annotate a spatial region for each frame in videos) for STSGV is labor-intensive and complicated, Chen et al. [13] propose to solve this task in a weakly-supervised manner which only needs video-level descriptions, with a newly-constructed VID-sentence dataset. Besides, VOGNet [50] commits to address the task of video object grounding, which grounds objects in videos referred to the natural language descriptions, and constructs a new dataset called ActivityNet-SRL. Tang et al. [56] employ visual transformer to solve a similar task which aims to localize a spatio-temporal tube of the target person from an untrimmed video based on a given textural description with a newly-constructed HC-STVG dataset.
## (s18) 4.2.3
(p18.0) Audio-enhanced localization. The current inputs for TSGV only contain the given sentence along with the untrimmed video. However, the audio signals are not effectively exploited, which may provide extra guidance for video localization, e.g., the loud noise while using electronics in the kitchen or cheers from the audience when the football player kicks a goal. Such various forms of sounds do offer auxiliary but essential clues for more precise localization of the target moments, which has not been explored yet. Moreover, what people speak in videos can be converted into text with the Automated Speech Recognition (ASR) technique. The converted text also provides relevant information for the cross-modal alignment between video and the text query. Nowadays, there has been many works [24,68] in visual-and-language area with audio-enhanced auxiliary proving its effectiveness for performance improvements. Thus, it is a promising future direction to embed the audio information for the TSGV task.
