# Convergence of Edge Computing and Deep Learning: A Comprehensive Survey

CorpusID: 197935335 - [https://www.semanticscholar.org/paper/4cd03cd34e7e94d1b1ee293d5dead8efc24c1a6d](https://www.semanticscholar.org/paper/4cd03cd34e7e94d1b1ee293d5dead8efc24c1a6d)

Fields: Engineering, Computer Science

## (s0) II. FUNDAMENTALS OF EDGE COMPUTING
(p0.0) Edge computing has become an important solution to break the bottleneck of emerging technologies by virtue of its advantages of reducing data transmission, improving service latency and easing cloud computing pressure. The edge computing architecture will become an important complement to the cloud, even replacing the role of the cloud in some scenarios. More detailed information can be found in [8], [20], [21]. 
## (s5) B. Deep Reinforcement Learning (DRL)
(p5.0) As depicted in Fig. 8, the goal of RL is to enable an agent in the environment to take the best action in the current state to maximize long-term gains, where the interaction between the agent's action and state through the environment is modeled as a Markov Decision Process (MDP). DRL is the combination of DL and RL, but it focuses more on RL and aims to solve decision-making problems. The role of DL is to use the powerful representation ability of DNNs to fit the value function or the direct strategy to solve the explosion of state-action space or continuous state-action space problem. By virtue of these characteristics, DRL becomes a powerful solution in robotics, finance, recommendation system, wireless communication, etc [18], [72].  1) Value-based DRL: As a representative of value-based DRL, Deep Q-Learning (DQL) uses DNNs to fit action values, successfully mapping high-dimensional input data to actions [73]. In order to ensure stable convergence of training, experience replay method is adopted to break the correlation between transition information and a separate target network is set up to suppress instability. Besides, Double Deep Q-Learning (Double-DQL) can deal with that DQL generally overestimating action values [74], and Dueling Deep Q-Learning (Dueling-DQL) [75] can learn which states are (or are not) valuable without having to learn the effect of each action at each state.
## (s6) C. Distributed DL Training
(p6.0) At present, training DL models in a centralized manner consumes a lot of time and computation resources, hindering further improving the algorithm performance. Nonetheless, distributed training can facilitate the training process by taking full advantage of parallel servers. There are two common ways to perform distributed training, i.e., data parallelism and model parallelism [80]- [83] as illustrated in Fig. 9.
## (s10) A. Real-time Video Analytic
(p10.0) Real-time video analytic is important in various fields, such as automatic pilot, VR and Augmented Reality (AR), smart surveillance, etc. In general, applying DL for it requires high computation and storage resources. Unfortunately, executing these tasks in the cloud often incurs high bandwidth consumption, unexpected latency, and reliability issues. With the development of edge computing, those problems tend to be addressed by moving video analysis near to the data source, viz., end devices or edge nodes, as the complementary of the cloud. In this section, as depicted in Fig. 10, we summarize related works as a hybrid hierarchical architecture, which is divided into three levels: end, edge, and cloud.  1) End Level: At the end level, video capture devices, such as smartphones and surveillance cameras are responsible for video capture, media data compression [89], image preprocessing, and image segmentation [90]. By coordinating with these participated devices, collaboratively training a domain-aware adaptation model can lead to better object recognition accuracy when used together with a domainconstrained deep model [91]. Besides, in order to appropriately offload the DL computation to the end devices, the edge nodes or the cloud, end devices should comprehensively consider tradeoffs between video compression and key metrics, e.g., network condition, data usage, battery consumption, processing delay, frame rate and accuracy of analytics, and thus determine the optimal offloading strategy [89].

(p10.1) 2) Edge Level: Numerous distributed edge nodes at the edge level generally cooperate with each other to provide better services. For example, LAVEA [92] attaches edge nodes to the same access point or BS as well as the end devices, which ensure that services can be as ubiquitous as Internet access.

(p10.2) In addition, since the cloud is more suitable for DL training while the DL inference can be benefited by executing it on the edge, EdgeEye [93] separates the DL model and allocates these partitions to end devices and edge nodes at different levels. Other than the edge cooperation, compressing the DL model on the edge can also improve holistic performance. As in [94], the resource consumption of the edge layer can be greatly reduced while ensuring the analysis performance, by reducing the unnecessary filters in CNN layers.

(p10.3) 3) Cloud Level: At the cloud level, the cloud is responsible for the integration of DL models among the edge layer and updating parameters of distributed DL models on edge nodes [89]. Since the distributed model training performance on an edge node may be significantly impaired due to its local knowledge, the cloud needs to integrate different well-trained DL models to achieve global knowledge. When the edge is unable to provide the service confidently (e.g., detecting objects with low confidence), the cloud can use its powerful computing power and global knowledge for further processing and assist the edge nodes to update DL models.
## (s11) B. Autonomous Internet of Vehicles (IoVs)
(p11.0) It is envisioned that vehicles can be connected to improve safety, enhance efficiency, reduce accidents, and decrease traffic congestion in transportation systems [95]. There are many information and communication technologies such as networking, caching, edge computing which can be used for facilitating the IoVs, though usually studied respectively. On one hand, edge computing provides low-latency, highspeed communication and fast-response services for vehicles, making automatic driving possible. On the other hand, DL techniques are important in various smart vehicle applications. Further, they are expected to optimize complex IoVs systems.
## (s12) C. Intelligent Manufacturing
(p12.0) Two most important principles in the intelligent manufacturing era are automation and data analysis, the former one of which is the main target and the latter one is one of the most useful tools [98]. In order to follow these principles, intelligent manufacturing should first address response latency, risk control, and privacy protection, and hence requires DL and edge computing. In intelligent factories, edge computing is conducive to expand the computation resources, the network bandwidth, and the storage capacity of the cloud to the IoT edge, as well as realizing the resource scheduling and data processing during manufacturing and production [99]. For autonomous manufacturing inspection, aDeepIns [98] uses DL and edge computing to guarantee performance and process delay respectively. The main idea of this system is partitioning the DL model, used for inspection, and deploying them on the end, edge and cloud layer separately for improving the inspection efficiency.
## (s15) A. Optimization of DL Models in Edge
(p15.0) DL tasks are usually computationally intensive and requires large memory footprints. But in the edge, there are not enough resources to support raw large-scale DL models. Optimizing DL models and quantize their weights can reduce resource costs. In fact, model redundancies are common in DNNs [109], [110] and can be utilized to make model optimization possible. The most important challenge is how to ensure that there is no significant loss in model accuracy after being optimized. In other words, the optimization approach should transform or re-design DL models and make them fit in edge devices, with as little loss of model performance as possible. In this section, optimization methods for different scenarios are discussed: 1) general optimization methods for edge nodes with relatively sufficient resources; 2) fine-grained optimization methods for end devices with tight resource budgets.
## (s16) B. Segmentation of DL Models in Edge
(p16.0) In [12], the delay and power consumption of the most advanced DL models are evaluated on the cloud and edge devices, finding that uploading data to the cloud is the bottleneck of current DL servicing methods (leading to a large overhead of transmitting). Dividing the DL model and performing distributed computation can achieve better endto-end delay performance and energy efficiency. In addition, by pushing part of DL tasks from the cloud to the edge, the throughput of the cloud can be improved. Therefore, the DL model can be segmented into multiple partitions and then allocated to 1) heterogeneous local processors (e.g., GPUs, CPUs) on the end device [133], 2) distributed edge nodes [134], [135], or 3) collaborative 'end-edge-cloud" architecture [12], [46], [136], [137].

(p16.1) Partitioning the DL model horizontally, i.e., along the end, edge and cloud, is the most common segmentation method. The challenge lies in how to intelligently select the partition points. As illustrated in Fig. 13, a general process for determining the partition point can be divided into three steps [12], [136]: 1) measuring and modeling the resource cost of different DNN layers and the size of intermediate data between layers; 2) predicting the total cost by specific layer configurations and network bandwidth; 3) choosing the best one from candidate partition points according to delay, energy requirements, etc. Another kind of model segmentation is vertically partitioning particularly for CNNs [135]. In contrast to horizontal partition, vertical partition fuses layers and partitions them vertically in a grid fashion, and thus divides CNN layers into independently distributable computation tasks.  
## (s17) C. Early Exit of Inference (EEoI)
(p17.0) To reach the best trade-off between model accuracy and processing delay, multiple DL models with different model performance and resource cost can be maintained for each DL service. Then, by intelligently selecting the best model, the desired adaptive inference is achieved [138]. Nonetheless, this idea can be further improved by the emerged EEoI [139].

(p17.1) The performance improvement of additional layers in DNNs is at the expense of increased latency and energy consumption in feedforward inference. As DNNs grow larger and deeper, these costs become more prohibitive for edge devices to run real-time and energy-sensitive DL applications. By additional side branch classifiers, for partial samples, EEoI allows inference to exit early via these branches if with high confidence. For more difficult samples, EEoI will use more or all DNN layers to provide the best predictions.

(p17.2) As depicted in Fig. 14, by taking advantage of EEoI, fast and localized inference using shallow portions of DL models at edge devices can be enabled. By this means, the shallow model on the edge device can quickly perform initial feature extraction and, if confident, can directly give inference results. Otherwise, the additional large DL model deployed in the cloud performs further processing and final inference. Compared to directly offloading DL computation to the cloud, this approach has lower communication costs and can achieve higher inference accuracy than those of the pruned or quantized DL models on edge devices [98], [140]. In addition, since only immediate features rather than the original data are sent to the cloud, it provides better privacy protection. Nevertheless, EEoI shall not be deemed independent to model optimization (Section V-A2) and segmentation (Section V-B). The envision of distributed DL over the end, edge and cloud should take their collaboration into consideration, e.g., developing a collaborative and on-demand co-inference framework [141] for adaptive DNN partitioning and EEoI.  
## (s18) VI. EDGE COMPUTING FOR DEEP LEARNING
(p18.0) Extensive deployment of DL services, especially mobile DL, requires the support of edge computing. This support is not just at the network architecture level, the design, adaptation, and optimization of edge hardware and software are equally important. Specifically, 1) customized edge hardware and corresponding optimized software frameworks and libraries can help DL execution more efficiently; 2) the edge computing architecture can enable the offloading of DL computation; 3) well-designed edge computing systems can better maintain DL services running on the edge; 4) fair platforms for evaluating DL performance on the edge help to further evolve above implementations.

(p18.1) A. Edge Hardware and Software Stacks for DL 1) Mobile CPUs and GPUs: DL applications are more valuable if directly enabled on lightweight edge devices, such as mobile phones, wearable devices and surveillance cameras, near to the location of events. Low-power IoT devices, as a part of edge devices, can be used to undertake lightweight DL computation, and hence avoiding communication with the cloud, but it still need to face limited computation resources, memory footprint and energy consumption. To relax these bottlenecks, in [125], ARM Cortex-M micro-controllers are studied to make them potentially feasible edge hardware for DL. By the developed CMSIS-NN, a collection of efficient NN kernels, the memory footprint of NNs on ARM Cortex-M processor cores can be minimized, and then the DL model can be fitted into IoT devices, meantime achieving normal performance and energy efficiency.

(p18.2) DeepMon [142], as a suite of optimizations (on both hardware and software) for processing CNN layers on mobile GPUs, can largely reduce the inference time by taking advantage of the similarity between consecutive frames in firstperson-view videos. Such a feature incurs a large amount of computation repetition in CNN layers, and thus inspires the idea of caching computation results of CNN layers. In addition, by means of matrix decomposition, high-dimensional matrix operations, particularly multiplications, in CNN layers become available in mobile GPUs and can be accelerated. In view of this work, different kinds of mobile GPUs, already deployed in edge devices, can be potentially explored with specific DL models and play a more important role in enabling edge intelligence.
## (s20) 1) Integral Offloading:
(p20.0) The most natural mode of DL computation offloading is similar to the existed "end-cloud" computing, i.e., the end device sends its computation requests to the cloud for DL inference results (as depicted in Fig. 15(a)). This kind of offloading is straightforward by extricating itself from DL task decomposition and combinatorial problems of resource optimization (may bring about additional computation cost and scheduling delay), and thus simple to implement.

(p20.1) In [148], a distributed infrastructure, that ties together powerful edge nodes with less powerful end devices, is proposed. DL inference can be performed on the end or edge, depending on the tradeoffs between the required DL accuracy, inference latency, battery level, and network conditions. With regard to each DL task, the end device will decide whether locally processing or totally offloading it to the edge node.

(p20.2) Further, the workload optimization among edge nodes should not be ignored in the offloading problem, since edge nodes are commonly resource-restrained compared to the cloud. In order to satisfy the delay and energy requirements of accomplishing a DL task with limited edge resources, providing DL models with different model size and performance in the edge can be adopted to fulfill one kind of tasks. Hence, multiple Virtual Machines (VMs) or containers, undertaking different DL models separately, can be deployed on the edge node to process DL requests. Specifically, when a DL model with lower complexity can meet the requirements, it will be selected as the serving model. By optimizing the workload assignment weights and computing capacities of VMs, an optimization model that aims at reducing the energy cost and delay can be developed [149], while guaranteeing the DL inference accuracy.

(p20.3) 2) Partial Offloading: Partially offloading the DL task to the edge is also feasible (as depicted in Fig. 15(b)). An offloading system can be developed to enable online finegrained partition of a DL task, and determine how to allocate these divided tasks to the end device and the edge node. As exemplified in [150], the proposed MAUI, capable of adaptively partitioning general computer programs, can conserve an order of magnitude energy by optimizing the task allocation strategies, under the network constraints. More importantly, this solution can decompose the whole program at runtime instead of manually partitioning of programmers before program deploying. Though this work does not consider DL applications, it still sheds light on the potential of partial offloading of DL tasks.

(p20.4) It is not realistic to pre-install DL models on any edge nodes for handling every kind of requests from end devices (especially when the mobility are concerned) and it is even impractical to stuff all DL models into every edge node. Therefore, when the DL model is not pre-installed, the end device should first upload its DL model to the edge node. Unfortunately, it can seriously delay the offloaded DL computation due to long uploading time.

(p20.5) To deal with this challenge, an incremental offloading system IONN is proposed in [151]. Differ from packing up the whole DL model for uploading, IONN divides a DL model, prepared for uploading, into multiple partitions and uploads them to the edge node in sequential. On the other hand, the edge node, receiving the partitioned models, incrementally builds the DL model as each partitioned model arrives, while being able to execute the offloaded partial DL computation even before the entire DL model is uploaded. Therefore, the key lies in the determination of best DL partitions and the uploading order. Specifically, on one hand, DNN layers whose performance benefit is high and whose uploading overhead is low are preferred to be uploaded first, and thus making the edge node quickly build a partial DNN in order to achieve the best-expected query performance. On the other hand, unnecessary DNN layers, which cannot bring in any performance increase, will not be uploaded and hence avoiding the offloading.

(p20.6) 3) Vertical Collaboration: Expected offloading strategies among "End-Edge" architecture, as discussed in Section VI-B1 and VI-B2, are feasible for supporting less computationintensive DL services and small-scale concurrent DL queries. However, when a large number of DL queries need to be processed at one time, a single edge node is certainly insufficient.

(p20.7) A natural choice of collaboration is the edge performs data pre-processing and preliminary learning, when the DL tasks are offloaded. Then, the intermediate data, viz., the output of edge architectures, are transmitted to the cloud for further DL computation [152]. Nevertheless, the hierarchical structure of DNNs can be further excavated for fitting the vertical collaboration. In [12], all layers of a DNN are profiled on the end device and the edge node in terms of the data and computation characteristics, in order to generate performance prediction models. Based on these prediction models, wireless conditions and server load levels, the proposed Neurosurgeon evaluates each candidate point in terms of end-to-end latency or mobile energy consumption and partition the DNN at the best one. Then, it decides the allocation of DNN partitions, i.e., which part should be deployed on the end, the edge or the cloud, while achieving best latency and energy consumption of end devices.

(p20.8) By taking advantages of EEoI (Section V-C), vertical collaboration can be more adapted. Partitions of a DNN can be mapped onto a distributed computing hierarchy (i.e., the end, the edge and the cloud) and can be trained with multiple early exit points [140]. Therefore, the end and the edge can perform a portion of DL inference on themselves rather than directly requesting the cloud. Using an exit point after inference, results of DL tasks, the local device is confident about, can be given without sending any information to the cloud. For providing more accurate DL inference, the intermediate DNN output will be sent to the cloud for further inference by using additional DNN layers. Nevertheless, the intermediate output, e.g., high-resolution surveillance video streams, should be carefully designed much smaller than the raw input, therefore drastically reducing the network traffic required between the end and the edge (or the edge and the cloud).

(p20.9) Though vertical collaboration can be considered as an evolution of cloud computing, i.e., "end-cloud" strategy. Compared to the pure "end-edge" strategy, the process of vertical collaboration may possibly be delayed, due to it requires additional communication with the cloud. However, vertical collaboration has its own advantages. One side, when edge architectures cannot afford the flood of DL queries by themselves, the cloud architectures can share partial computation tasks and hence ensure servicing these queries. On the other hand, the raw data must be preprocessed at the edge before they are transmitted to the cloud. If these operations can largely reduce the size of intermediate data and hence reduce the network traffic, the pressure of backbone networks can be alleviated.
## (s21) 4) Horizontal Collaboration:
(p21.0) In Section VI-B3, vertical collaboration is discussed. However, devices among the edge or the end can also be united without the cloud to process resource-hungry DL applications, i.e., horizontal collaboration. By this means, the trained DNN models or the whole DL task can be partitioned and allocated to multiple end devices or edge nodes to accelerate DL computation by alleviating the resource cost of each of them. MoDNN, proposed in [153], executes DL in local distributed mobile computing system over a Wireless Local Area Network (WLAN). Each layer of DNNs is partitioned into slices to increase parallelism and to reduce memory footprint, and these slices are executed layer-by-layer. By the execution parallelism among multiple end devices, the DL computation can be significantly accelerated.

(p21.1) With regard to specific DNN structures, e.g., CNN, a finer grid partitioning can be applied to minimize communication, synchronization, and memory overhead [115]. In [135], a Fused Tile Partitioning (FTP) method, able to divide each CNN layer into independently distributable tasks, is proposed. In contrast to only partitioning the DNN by layers as in [12], FTP can fuse layers and partitions them vertically in a grid fashion, hence minimizing the required memory footprint of participated edge devices regardless of the number of partitions and devices, while reducing communication and task migration cost as well. Besides, to support FTP, a distributed work stealing runtime system, viz., idle edge devices stealing tasks from other devices with active work items [135], can adaptively distribute FTP partitions for balancing the workload of collaborated edge devices.
## (s22) VII. DEEP LEARNING TRAINING AT EDGE
(p22.0) Present DL training (distributed or not) in the cloud data center, namely cloud training, or cloud-edge training [47], viz., training data are preprocessed at the edge and then transmitted to cloud, are not appropriate for all kind of DL services, especially for DL models requiring locality and persistent training. Besides, a significant amount of communication resources will be consumed, and hence aggravating wireless and backbone networks if massive data are required to be continually transmitted from distributed end devices or edge nodes to the cloud. For example, with respect to surveillance applications integrated with object detection and target tracking, if end devices directly send a huge amount of real-time monitoring data to the cloud for persistent training, it will bring about high networking cost. In addition, merging all data into the cloud might violate privacy issues. All these challenges put forward the need for a novel training scheme against existing cloud training.

(p22.1) Naturally, the edge architecture, which consists of a large number of edge nodes with modest computing resources, can cater for alleviating the pressure of networks by processing the data or training at themselves. Training at the edge or potentially among "end-edge-cloud", treating the edge as the core architecture of training, is called as "DL at Edge". Such kind of DL training may require significant resources to digest distributed data and exchange updates.
## (s28) C. Edge Management and Maintenance
(p28.0) Edge computing services are envisioned to be deployed on BSs in cellular networks, as implemented in [214]. Therefore, edge management and maintenance require optimizations from multiple perspectives (including communication perspective). Many works focus on applying DL in wireless communication [215]- [217]. However, there are still other perspectives should be concerned as discussed in this section.

(p28.1) 1) Edge Communication: When edge nodes are serving mobile devices (users), the handover problem of edge devices in edge computing networks should be addressed. DLbased methods can be used to assist the smooth transition of connections between edge nodes [210]. Specifically, the whole handover problem is to minimize the interruptions of a mobile device moving from an edge node to the next one throughout its moving trajectory. An MLP can be used to predict available edge nodes at a given location and time. Moreover, it still needs to evaluate the cost (the latency of servicing a request) for the interaction between the mobile device and each edge node to determine the best edge node the mobile device should associate to. Nonetheless, modeling the cost of these interactions requires a more capable learning model. Therefore, a two-layer stacked RNN with LSTM cells is implemented for modeling the cost of interaction. At last, based on the capability of predicting available edge nodes along with corresponding potential cost, the mobile device can associate with the best edge node, and hence the possibility of disruption is minimized.

(p28.2) To deal with the energy minimization problem in the communication scenario with multiple modes (to serve various IoT services), i.e., C-RAN (Cloud-Radio Access Networks) mode, D2D (Device-to-Device) mode, and FAP (Fog radio Access Point) mode, DQL can be used to control communication modes of edge devices and on-off states of processors throughout the communicating process [211]. Then, when the communication mode and the processors' on-off states of a given edge device are determined by the DQL agent, the whole problem is degraded into an RRH (Remote Radio Head) transmission power minimization problem and solved.

(p28.3) 2) Edge Security: Due to the fact that edge devices are generally equipped with limited computation, energy and radio resources, the transmission between them and the edge node is more vulnerable to various attacks, such as jamming attacks and Distributed Denial of Service (DDoS) attacks, compared to cloud computing. Therefore, the security of edge architectures should be enhanced. Certainly, security protection generally requires additional energy consumption and the overhead of both computation and communication. Consequently, each edge device shall optimize its defense strategies, viz., choosing the transmit power, channel and time, without violating its resource limitation. The optimization is challenging since it is hard to estimate the attack model and the dynamic model of edge computing networks.

(p28.4) DRL-based security solutions can provide secure offloading (from the edge device to the edge node) to against jamming attacks [212] or protect user location privacy and the usage pattern privacy [218]. The edge device observes the status of edge nodes and the attack characteristics, and then determines the defense level and key parameters in security protocols. By setting the reward as the anti-jamming communication efficiency, such as the Signal-to-Interference-plus-Noise Ratio (SINR) of the signals, the Bit Error Rate (BER) of the received messages, and the protection overhead, the DQLbased security agent can be trained to cope with various types of attacks.
## (s30) A. ("DL on Edge") More Promising Applications
(p30.0) With the rapid development of wireless communication techniques, the enabled pervasive connections and the highspeed uplink/downlink rates illuminate the bloom of DL services on edge, and there are many promising applications not dabbled yet, to name a few:

(p30.1) • Cloud-edge gaming and VR application: Both of them or their combination requires faster gaming interactions and video rendering [221]. With more DL techniques are universally embedded in these emerged applications, the introduced processing delay and additional computation cost make the cloud gaming architecture struggle to meet the latency requirements. Edge computing architectures, near to users, can be leveraged with the cloud to form a hybrid gaming architecture. • Autonomous vehicle: Intelligent driving involves speech recognition, image recognition, intelligent decision making, etc. Processing massive data is indispensable in autonomous vehicles, and edge computing is an important tool for real-time data processing. Various applications in intelligent driving, such as collision warning, require edge computing platforms to ensure millisecond-level interaction delay. In addition, edge computing can provide location awareness. Customized perception is more conducive to analyze the traffic environment around the vehicle, thus enhancing the driving safety. • Intelligent transportation: With respect to smart cities, the challenge of building intelligent transportation systems should be met, aiming to lower the probabilities of traffic accidents and to improve traffic efficiency. For example, DRL algorithms can be deployed on the edge, e.g., RSUs, to realize adaptive traffic signal controls [222]. Therefore, it can effectively alleviate urban traffic congestion by dynamically adjusting the traffic light plan to cope with real-time traffic fluctuations, thereby facilitating people's lives. To summarize, if DL and edge are well-integrated, they can offer great potential for the development of innovative applications. There are still many areas to be explored to provide operators, suppliers and third parties with new business opportunities and revenue streams.
## (s31) B. ("DL in Edge") General DL Model for Inference
(p31.0) When deploying DL in edge devices, it is necessary to accelerate DL inference by model optimization. In this section, challenges with respect to model compression, model segmentation and EEoI, used to optimize DL models, is discussed.

(p31.1) 1) Generalization of EEoI: Currently, EEoI can be applied to classification problems in DL [139], but there is no generalized solution for a wider range of DL applications. Furthermore, in order to build an intelligent edge and support edge intelligence, not only DL but also the possibility of applying EEoI to DRL should be explored, since applying DRL to real-time resource management for the edge, as discussed in Section VIII, requires stringent response speed.

(p31.2) 2) Hybrid Model Modification: Coordination issues with respect to model optimization, model segmentation, and EEoI should be thought over. These customized DL models are often used independently to enable "end-edge-cloud" collaboration. Model optimizations, such as model quantification and pruning, may be required on the end and edge sides, but because of the sufficient computation resources, the cloud does not need to take the risk of model accuracy to use these optimizations. Therefore, how to design a hybrid precision scheme, that is, to effectively combine the simplified DL models in the edge with the raw DL model in the cloud is important.

(p31.3) 3) Coordination between training and inference: Pruning, quantizing and introducing EEoI into trained raw DL models require retraining to give them the desired inference performance. In general, customized models can be trained offline in the cloud. However, the advantage of edge computing lies in its response speed and might be neutralized because of belated DL training. Moreover, due to a large number of heterogeneous devices in the edge and the dynamic network environment, the customization requirements of DL models are not monotonous. Then, is this continuous model training requirement reasonable, and will it affect the timeliness of model inference? How to design a mechanism to avoid these side-effects?

(p31.4) C. ("Edge for DL") Complete Edge Architecture for DL Edge intelligence and intelligent edge require a complete system framework, covering data acquisition, service deployment and task processing. In this section, we discuss the efforts that need to be made to build a complete edge computing framework for DL, as summarized in Table VI. 1) Edge for Data Processing: Both pervasively deployed DL services on the edge and DL algorithms for optimizing edge cannot be realized without data acquiring. Edge architecture should be able to efficiently acquire and process the original data, sensed or collected by edge devices, and then feed them to DL models.

(p31.5) Adaptively acquiring data at the edge and then transmitting them to cloud (as done in [7]) is a natural way to alleviate the workload of edge devices and to reduce the potential resource overhead. In addition, it is better to further compress the data, which can alleviate the bandwidth pressure of the network, while the transmission delay can be reduced to provide better QoS. Most existed works focus only on vision applications [89]. However, the heterogeneous data structures and characteristics of a wide variety of DL-based services are not addressed well yet. Therefore, developing a heterogeneous, parallel and collaborative architecture for edge data processing for various DL services will be helpful.

(p31.6) 2) Reduction of DL Computation: Users within the same area might request recognition results of similar interesting objects, and it may introduce redundant DL inference tasks. By caching requesting results and necessary DL models [223] or sharing intermediate DL inference results among different DL models [224], repetitive or similar requests can be directly disposed at the edge. In this manner, the processing delay can be largely reduced without bringing too much extra computation. Nevertheless, similar works [225] [226] concerns DL computation without dabbling DRL applications, which might be essential for the near future.

(p31.7) 3) Microservice for DL Services: Edge and cloud services have recently started undergoing a major shift from monolithic entities to graphs of hundreds of loosely-coupled microservices [227]. Executing DL computations may need a series of software dependencies, and it calls for a solution for isolating different DL services on the shared resources. Naturally, Virtual Machine (VM) and Docker (noted Unikernels [228] and Docker complement each other) are both feasible for this purpose. However, compared to VM, Docker can provide faster deployment, smaller footprint, better performance and finer-grained resource management, which make it potentially more appropriate for edge computing [229]. At present, microservice frameworks deployed on the edge for hosting DL services are still not investigated, due to several important challenges: 1) microservice frameworks should be flexible enough to handle DL deployment and management; 2) the live migration of microservices should be achieved to cater for the user mobilities, in order to reduce migration times and unavailability of DL services; 3) the microservice framework should be able to orchestrate the cloud and distributed edge infrastructures, leveraging computing resources of both sides to achieve better performance as illustrated in Section VI-B3. 4) Incentive and trusty offloading mechanism for DL: Heavy DL computations on resource-limited end devices can be offloaded to nearby edge nodes (Section VI-B). However, there are still several issues, 1) an incentive mechanism should be established for stimulating edge nodes to take over DL computations; 2) the security should be guaranteed to avoid the risks from anonymous edge nodes [230].

(p31.8) Blockchain, as a decentralized public database storing transaction records across participated devices, can avoid the risk of tampering the records [231]. By taking advantage of these characteristics, incentive and trust problems with respect to computation offloading can potentially be tackled. To be specific, all end devices and edge nodes have to first put down deposits to the blockchain to participate. The end device request the help of edge nodes for DL computation, and meantime send a "require" transaction to the blockchain with a bounty. Once an edge nodes complete the computation, it returns results to the end device with sending a "complete" transaction to the blockchain. After a while, other participated edge nodes also execute the offloaded task and validate the former recorded result. At last, for incentives, firstly recorded edge nodes win the game and be awarded [232]. However, this idea about blockchained edge is still in its infancy.
## (s34) E. ("DL for Edge") Deployment and Improvement of Intelligent Edge
(p34.0) At present, there have been many attempts to use DL to optimize and schedule resources in edge computing networks. In this regard, there are many potential areas where DL can be applied, including online content streaming [245], routing and traffic control [246] [247], etc. However, because DL solutions do not rely entirely on accurate modeling of networks and devices, finding a scenario where DL can be applied is not the most important concern. Besides, if DL is applied to the optimization of real-time edge computing networks, the training and inference of DL models or DRL algorithms may bring certain side effects, such as the additional bandwidth consumed by training data transmission and the latency of DL inference.

(p34.1) Existing works mainly concern about solutions of "DL for Edge" at the high level, but overlook the practical feasibility at the low level. Though DL show its performance in these works, as illustrated in Fig. 21, the deployment issues of DL models and DRL algorithms should also be carefully considered:

(p34.2) • Where DL and DRL should be deployed, in view of the resource overhead of them and the requirement of managing edge computing networks in real time? • When using DL to determine caching policies or optimize task offloading, will the benefits of DL be neutralized by the bandwidth consumption and the processing delay brought by DL itself? • How to explore and improve edge computing architectures in Section VI to support "DL for Edge"? • Are the ideas of customized DL models, introduced in Section V, can help to facilitate the practical deployment?
