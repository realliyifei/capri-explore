# Purposeful Communication in Human-Robot Collaboration: A Review of Modern Approaches in Manufacturing

CorpusID: 254438398 - [https://www.semanticscholar.org/paper/f5c01e0bc48ee9cd43de2c4ebdec5c618e0ca8f8](https://www.semanticscholar.org/paper/f5c01e0bc48ee9cd43de2c4ebdec5c618e0ca8f8)

Fields: Engineering, Computer Science

## (s4) B. SEARCH STRATEGY
(p4.0) The preferred reporting items for the systematic review and meta-analysis framework (PRISMA) were followed to perform the literature review [6]. The literature review was performed by searching databases, including IEEE Xplore, ISI Web of Knowledge, Scopus, and the journal of communication studies. Keywords used were the combination of: 'human-robot collaboration,' OR 'robot task planning,' OR 'decision making in human-robot collaboration,' OR 'robot learning in collaborative tasks,' AND 'communication.' The title, abstract, and keywords of the journal articles and conference proceedings were searched. The ones written in English from 2010 to 2021 were selected according to the paper selection criteria mentioned above. The paper selection process in this systematic review is shown in Figure 1. According to Figure 1, a total number of 176 articles from ISI Web of Knowledge, 382 from IEEE Xplore, 289 from Scopus, and 13 articles from the journal of communication studies were identified. The elimination of duplicates was the next step in the identification phase (53 articles); then the title and abstract of the remaining articles were screened to remove unrelated ones (screening). The remaining articles were evaluated for eligibility according to the eligibility criteria defined in the next phase. One hundred articles were thoroughly reviewed and, in the end, forty-nine articles were included in the quantitative analysis.
## (s5) IV. COLLABORATIVE TASKS
(p5.0) The level of human-robot interaction (HRI) has been categorized as fully programmed, co-existence, assistance, cooperation, collaboration, and fully autonomous. The fully programmed level refers to traditional work cells where robots are located inside cages; there are no shared tasks and shared workspace defined, and physical contact is not allowed. At the co-existence level, still shared tasks and work spaces are not defined, and physical contact is not allowed. However, compared to the fully programmed level, robots are not fenced and some technologies, such as laser scans, are used to separate the robot workspace from the human workspace. For the level of HRI assistance, there is no shared task between the human and the robot, as the robot does not have any independent task to perform, while the workspace is shared and physical contact is allowed based on the nature of the assisting task. Both the cooperation and the collaboration levels have shared tasks and shared workspaces. However, physical contact is not allowed at the cooperation level, since humans and robots have the decoupled task (sequential); however, at the collaboration level, physical contact is allowed since the task is supposed to be accomplished simultaneously by the human and the robot. Finally, the fully autonomous level is operator-independent and there is a shared workspace for the human and the robot [7]. For the sake of this article, among the aforementioned levels of HRI, articles with a focus on collaboration, cooperation, and assistance levels depending on the nature of the task have been reviewed.

(p5.1) The type of collaborative task is a determinant factor in designing the HRC framework, and depending on the type of task, the robot and the human need to have access to different kinds of information related to the workspace. As a result, the lack of any appropriate technique for the defined task and workspace will cause uncertainty in the framework. Therefore, the selected articles were reviewed considering the type of collaborative task performed on them [8]. According to the selected articles, collaborative tasks between humans and robots could be classified as a shared workspace, shared manipulation, handover task, sequential task (e.g., assembly and the objective of the task are known), or leaderassistant, which will be discussed in this section (as shown in Tables 1 and 2).
## (s6) B. JOINT REACHING TASK
(p6.0) In Joint reaching collaborative tasks, human users and robots are jointly supposed to perform the task [15]. For example, table clearing is a joint-reaching task that could be done by a robot and a human to clear a table from objects. It would be possible to define a supervisory role for a human user in this type of task and involve human intentions such as trust in the process; If the human trusts the robot, it will let the robot do its task; otherwise, the human will do the task jointly with the robot [16], [17]. Joint reaching tasks could also be implemented in inventory scenarios [18].
## (s7) C. OBJECT HANDING TASK
(p7.0) Object handling is another form of the human-robot collaborative task in that the human and the robot move a jointly grabbed object from one location to a different location. In this type of task, it would be important for the robot to be able to predict the intention of the human user or inform the human about its intention through different communication modalities (e.g., implicit or explicit communication) depending on the design of the HRC framework [19], [20].
## (s8) D. OBJECT HAND-OVER TASK
(p8.0) Finally, object hand-over is the last category in which a robot and a human exchange an item in the collaborative framework [21]. Object hand-over is considered a simple task in HRC but requires some cognitive abilities in a robot, since it needs to predict the intention of the human user while performing the task. Human intent could change during the task and the robot must adjust its behavior based on [22]. Object handover tasks could also be part of an assembly task that is required to be completed in several steps (e.g., it includes several object handover steps). In this type of scenario, the robot may need to select the correct object at each step to hand over according to the human user's desire, so the capability of human intent estimation should be added to the framework [23].
## (s9) V. COMMUNICATION MODALITIES
(p9.0) The robot's ability to build and maintain mental models of other team members (human and robot) facilitates collaborative manufacturing processes. In HRC, human and robot must establish a shared mental model (SMM) to improve positive outcomes, such as team performance and safety in manufacturing processes [24]. Therefore, to build an effective human-robot teaming, just like human-human teaming and establish an SMM, communication is one of the essential steps that should occur between the human and the robot along with coordination and collaboration. Communication is essential in HRC, since purposeful communication helps build a shared mental model, transparency, and trust in a collaborative workspace [25].

(p9.1) In general, explicit and implicit communications are the two common approaches used in HRC. In explicit communication, which is obtained through verbal communication or gesture, the intent is obtained directly via communication. However, the intent is estimated through observed and predicted human behavior in implicit communication, a nonverbal communication method. In addition, a combination of explicit and implicit communication modalities (multi-modal communication) is used in the HRC field. The communication modality or modalities for human-to-robot (HTR) and robot-to-human (RTH) communication are usually chosen based on their reliability, robustness, cognitive load, and delay. Furthermore, some task-related factors, such as the type of task; extent of use, flexibility, duration, and additional classification, are other critical factors in the choice of communication technology in HRC [26].
## (s11) 1) VERBAL COMMUNICATION
(p11.0) Verbal communication is the most straightforward method of explicit communication in HRC. Human and robot could communicate through speech/verbal commands, so that humans give commands to the robot, or the robot replies back to the human user. It is also possible to have bidirectional or two-way communication through speech in HRC; however, there are some challenges in using verbal communication in HRC. For example, it is difficult to establish a foundation only through verbal communication [19]; foundation refers to the fact that speakers understand the messages of others as intended [28]. Moreover, when it comes to time and cognitive resources, verbal communication is considered a costly approach [29].
## (s12) 2) NON-VERBAL COMMUNICATION
(p12.0) The vision system is the commonly used non-verbal communication channel through which humans and robots could have both explicit (e.g., gesture, text) and implicit communication (e.g., gaze). In the final list of articles, most research studies have chosen vision as a channel of communication and information exchange [30]. The vision system is used primarily for object detection, human body tracking, or information display. A vision system could be a set of RGBD sensors and an interactive GUI used to perform an assembly task. 3D RGB sensors are used for action recognition and help the robot understand the status of the environment, while the interactive GUI is used to interact with the user [9]. The vision system could monitor the workspace and recognize objects to reach them and move them to a different location [16], [17]. Furthermore, different types of information related to the environment and workspace could be obtained using the vision system, including the position of the assembly part, the position of the user in the environment, the physical characteristics of the environment, and the position and orientation of the robot end effector [11]. Human user behavior could also be sensed through a vision system (e.g., webcam and Kinect RGB-D sensor) in addition to tracking the positions / orientations of the various objects [31].

(p12.1) Although direct communication (for example, speech or gestures) provides reliable methods to establish a joint intention [23], it can require a human to stop performing another task to communicate with the robot, reducing the efficiency of the team. On the other hand, the estimation of intent allows the human to focus on task completion, resulting in a more intuitive and efficient relationship. Still, it requires the robot to interpret intent from information obtained via measurements (e.g., physiological states). This interpreted information can be used to answer binary yes/no questions about human intent, select between discrete modes of intent, or establish intent along with a set of continuous variables such as future limb trajectories and an approximate level of intent (e.g., based on speed or physical force exertion).

(p12.2) Additional research on task environments and interactions has shown that intent estimation may be improved by leveraging the context of spoken commands or interpreting human/object interactions based on object affordances. Although video-based sensing is often sufficient to recognize the gestures, objects, and approximate motions required to estimate intent, shared control over objects in HRC typically requires more accurate coordination than video processing permits. To this end, HRC systems often estimate intent using inertial measurement units (IMUs), force sensors, and physiological monitoring equipment such as EEG and muscle activation measurement devices (i.e., myography). Physiological signals, such as EMG signals that have information on body movements and record muscle activities, or EEG signals that record brain activities, have recently been used to give a command to a robot, control robot actions and movements, and create a shared control architecture in the context of HRC [32], [33]. However, such measurements can be invasive or uncomfortable and often provide noisy signals that may require significant processing and machine learning (ML) efforts to extract useful information. On the contrary, force sensors and IMUs provide more reliable but less instantaneous information (compared to neurological signals) [34]. In addition, haptic communication is another commonly used approach to accomplish an HRC task.
## (s13) 3) MULTI-MODAL COMMUNICATION
(p13.0) Using multiple communication modalities (that is, explicit and implicit) is another communicative approach in HRC that improves HRC flexibility and robustness, quality of communication, task performance, human safety, and production efficiency [35], [36]. Furthermore, it has been shown that the grounding issue related to the use of only verbal communication can be improved when verbal communication is used with other types of communication modalities, such as haptic communication [19].
## (s14) B. ROBOT-TO-HUMAN COMMUNICATION
(p14.0) In HRI, the human user's understanding of the robot's intentions is important because it can improve trust in the robot. However, compared to HTR communication modalities, RTH communication methods are not widely studied, and recent research efforts have been made to add new communication technologies, such as extended reality (XR), to the HRC context to fill the gap [26].
## (s15) 1) VERBAL COMMUNICATION
(p15.0) In some of the selected articles, verbal commands were used to establish RTH communication and provide different types of information for the human user, such as how the human needs to do a task or why the human user should follow a specific method to do a task to allow human decision-making based on the information provided [19]. Additionally, RTH verbal communication could be a method to inform the human user about the state of the environment, the goal, the plan or action, and asking the human partner to act [18], [41]. Furthermore, the use of verbal commands for HTR and RTH communication could facilitate the creation of bidirectional and two-way communication channels in HRC [8].
## (s16) 2) NON-VERBAL COMMUNICATION
(p16.0) A display installed on the robot, an interactive GUI, robot gaze, robot hand gesture, and robot body gesture are other ways of building RTH communication [9], [23]. The hand gesture is not limited to being used by the human user; robots could use this approach as a way of communication alongside other techniques such as gaze. The gaze of the robot could be an indicator of its readiness to execute a task [42] or to signal planned actions followed by an action [43]. The robot's gaze could help to accomplish two primary purposes, establishing mutual belief (that is, the user is indicated about the action to be taken) and indicating readiness for the next instruction. In this specific case, whenever a robot decides to close or open its hand or reach out to an object, it could look at its hand or look at the object in the task [23]. Robot gestures are an appropriate and informative communication medium in HRC, so more innovative methods, such as zoomorphic gestures, are introduced in the field [44].
## (s18) 4) COMMUNICATION VIA EXTENDED REALITY TECHNIQUES
(p18.0) Extended reality (XR) techniques (i.e. virtual reality (VR), augmented reality (AR), and mixed reality (MR)) bridge the virtual environment to the physical and real environment, and there are many attempts to integrate these techniques with HRC. XR techniques provide four types of solutions for HRC: 1) operator support, 2) simulation, 3) instruction, and 4) manipulation [47]. In this categorization, operator support is provided by enabling communication between the human and the robot through XR techniques. Simulation solutions give users the opportunity to understand the collaborative task and environment. Virtual instructions could provide a chance for the exploitation of virtual and augmented environments, which help the human user follow the hierarchy of tasks. Finally, the manipulation solution examines existing solutions on how these techniques enable the operator to operate and manipulate the robot remotely.
## (s19) VI. ROBOT DECISION-MAKING
(p19.0) HRC frameworks are becoming more intelligent with the introduction of intelligent robots that could make decisions on their own and adapt to the environment that facilitates the human-robot partnership and promote human safety. However, the choice of decision-making algorithm added to the HRC framework depends on multiple factors, such as collaborative task and available communication channel, which all affect the robot learning algorithm, the decisionmaking model, and interaction planning [57], [58]. It is necessary to know the characteristics of the task because it would determine the defined or undefined parameters in the environment. For example, in assembly tasks, tolerance or completion time may be well defined, while other details, such as the preferences of the individuals, may change in different individuals. However, the robot must learn the preferences of each operator and adapt to the situation. This section will describe the different robot decisionmaking algorithms used in the selected articles as one of the key components of HRC frameworks.
## (s20) A. MACHINE LEARNING
(p20.0) Conventional machine learning (ML) techniques are mostly used to create mapping functions to link human physical capacities with HRC design, such as mapping human actions as robot input using support vector machines (SCM) [59].
## (s21) B. DEEP LEARNING
(p21.0) Besides creating mapping functions between human physical capacities and robot computation, another challenge in HRC design is the computational representation of human-related factors that are embedded in multi-modal and multi-scale sensor data. Therefore, there have been attempts to develop new ML methods that could act as a feature extractor that ''transform the raw data of human capacities into a suitable internal representation or a feature vector from which the learning subsystem can be derived'' [61]. As a result, deep learning (DL) algorithms were developed, which are: representation learning methods with multiple levels of representation, obtained by composing simple but nonlinear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level [61]. Convolutional neural networks (CNN) and recurrent neural networks (RNNs) are commonly used DL methods. CNN is designed to process data that come in multiple arrays, such as a color image composed of three 2D arrays containing pixel intensities in the three color channels, and has application in image processing. Numerous research studies have shown that CNN is a promising method for learning representation from human visual perception in the HRC context [62]. On the other hand, RNNs are trained using the backpropagation technique and are suitable for tasks with sequential input, such as speech and language. DL methods are also combined with other neural networks [61] or reinforcement learning (RL) methods to create DRL architectures in real-world HITL settings to study collaborative learning between humans and robots [12].
## (s22) C. PROBABILISTIC GRAPHICAL MODELS
(p22.0) Another challenge of representation learning in HRC is the mental models that are supposed to be shared between humans and robots. In particular, the states and actions of both parties should be learned and represented as a theoretical uniform to achieve shared mental models. Furthermore, the representation of these mental models should be capable of integrating uncertainties to show robustness and tolerance to environmental changes, such as human preferences or task changes. Probabilistic graphical models (PGMs) are currently considered one of the promising methods to solve this problem. PGMs are defined as: ''Probabilistic graphical models (PGM) comprise any model that uses the language of graphs to facilitate the representation and resolution of complex problems that use probability as a representation of uncertainty [63].'' A graph structure consists of several nodes, and the edges represent the probabilistic relationships or conditional dependency/independence among a set of variables in a system. The nodes represent variables, and an edge between two nodes indicates a conditional dependency between the two variables (the absence of edges means conditional independence). PGMs are used for different purposes in HRC, including supervised classification, clustering, abductive reasoning, decision-making, and optimization [63].
## (s23) D. LEARNING FROM DEMONSTRATION
(p23.0) Learning from demonstration (LFD) that could be implemented through kinesthetic teaching, teleoperation, and passive observation is a robot learning method in that the robot learns to do a task by imitating a demonstrator. This method could be considered a supervised learning method, since an expert and a robot provide the information that the tries to follow. In this robot-learning method, even non-experts could interact with the robot, learning could be done using a small number of demonstrations (i.e., data efficiency), robot learning is done in a safe condition, it is a reliable method, and the learned task could be implemented in a different platform (i.e., platform independence). However, there are some limitations, including that it is difficult to demonstrate complex behaviors, labeled data are needed, and learning from sub-optimal and inappropriate demonstrators would not be accurate [66], [67].
## (s26) VII. NOVEL CATEGORIZATION OF HITL SYSTEMS
(p26.0) The reviewed articles had some similarities and differences, but two distinguishing factors were identified that were used to categorize the available HITL systems: 1) level of communication and 2) adaptability in communication and task execution. The first factor focuses on the level of communication in HRC where information related to the task or any of the human and robot's intent could be shared. Information sharing should be done through the appropriate communication modality in the framework that has been discussed in Section V, information sharing could be done through HTR and RTH communication in that humans and robots need to be informed about each other's intention before the execution step. Moreover, communication time and communication type affect the shared understanding of the task and the user's intent. Information sharing will bring caution to humans in a collaborative workspace. The second factor investigates adaptability in task execution as another factor that affects human trust in robot and team performance [20].

(p26.1) Consequently, using the factors mentioned, a new categorization of the available HITL systems was proposed in the context of HRC. Figure 2 shows a schematic of the DHITL and PCHITL architectures.

(p26.2) • Delayed Human-in-the-loop (DHITL): The robot starts doing an action based on the prediction of human intent; the human observes the robot's action and then provides feedback to the system.

(p26.3) • Pre-cautious Human-in-the-loop (PCHITL): In addition to estimating the intent in some steps, the human or robot could inform each other before the execution of the task in a purposeful way embedded in an intelligent architecture. According to Table 1 and Table2, of the 49 articles reviewed in this article, 12 were classified as PCHITL, one as semi-PCHITL, and the rest were considered DHITL. This section will discuss details related to the frameworks identified as PCHITL while considering communication as the deriving factor. In the categorization, the mode of communication was not considered a factor in itself, but the way the communication modalities were used to promote purposeful communication by addressing any of the issues of how to communicate; when to communicate; and what to communicate was the focus of this categorization. 1) Some HRC frameworks allow the robot to decide if, when, and what to communicate while performing collaborative tasks with humans. The robot informs the human on what action it is going to do (e.g., I am going to do an action at a landmark); asks the human counterpart about her intention for the next step (e.g., Where are you going?); and also commands the human counterpart what to do (e.g., please make the next sandwich at landmark).
