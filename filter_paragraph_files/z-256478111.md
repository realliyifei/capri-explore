# Approximate Query Processing: What is New and Where to Go? A Survey on Approximate Query Processing

CorpusID: 256478111 - [https://www.semanticscholar.org/paper/da27323e1456ee41dbc198a14af3d37d45c29946](https://www.semanticscholar.org/paper/da27323e1456ee41dbc198a14af3d37d45c29946)

Fields: Computer Science

## (s0) Introduction
(p0.0) Online analytical processing (OLAP) is a core functionality in data management and analytics systems [33]. The performance of OLAP is crucial for many applications that need to use OLAP to make online decisions, e.g., business intelligence. However, it is rather costly to support OLAP for large datasets, especially big data. Many systems have been proposed to support OLAP on big data, e.g., Pig, Hive, Spark SQL, and they usually take tens of minutes or even hours to answer an OLAP query. However, many applications have online requirement of OLAP that want to get results in seconds.

(p0.1) To alleviate this problem, approximate query processing (AQP) has been proposed, which computes approximate answers (with some quality guarantee) very efficiently to meet the high-performance requirement. Next, we use several examples to show how AQP works. Note that AQP only performs well for aggregate functions such as SUM, AVG, COUNT, MAX and MIN due to it should use statistical tools to give approximate results for numerical types of answers.

(p0.2) AQP Use Cases We consider a database with multiple tables in Fig. 1. For simplicity, we only show three tables, orders O, customers C, states ST, and the relations among the tables. We consider the following three use cases of AQP. However, online sampling without pre-computing may lead to large errors and the quality is uncontrollable. For example, consider a rare group with few tuples (e.g., c 4 ), and the online methods may not get any sample from the group. Thus, the online sampling method fails to provide a high-quality result. To address this problem, we can use offline synopses. synopsis to answer an online query. For example, suppose the queries w.r.t column CustomerID and Profit are frequently used in a query workload, the system builds synopses offline for the two columns and uses them to answer a query. The system uses the error bound and confidence to determine the synopsis size, generates the synopsis, and utilizes the synopsis to answer the query. For example, given a query
## (s1) Online Aggregation Methods
(p1.0) In this section, we survey the online AQP methods. The basic idea is to first select a sample S and then use S to estimate the results. We introduce how to select S in Sect. 2 

(p1.1) In this section, we survey the online AQP methods. The basic idea is to first select a sample S and then use S to estimate the results. We introduce how to select S in Sect. 2 
## (s3) Error Estimation
(p3.0) The confidence interval is widely used to estimate the result quality in most of the random-sampling methods [2], where each confidence interval gives users a numerical interval and a corresponding confidence based on the statistical theory. Initially, a set S of samples is computed based on sampling techniques in Sect. 2.1. Then if the data distribution is known in advance, S can be utilized to estimate the distribution and then the error can be estimated based on the distribution (Sect. 2.2.1). If the data distribution is unknown, it needs to first estimate the distribution of sampling data and then estimate the error (Sect. 2.2.2).

(p3.1) The confidence interval is widely used to estimate the result quality in most of the random-sampling methods [2], where each confidence interval gives users a numerical interval and a corresponding confidence based on the statistical theory. Initially, a set S of samples is computed based on sampling techniques in Sect. 2.1. Then if the data distribution is known in advance, S can be utilized to estimate the distribution and then the error can be estimated based on the distribution (Sect. 2.2.1). If the data distribution is unknown, it needs to first estimate the distribution of sampling data and then estimate the error (Sect. 2.2.2).
## (s11) Database Learning
(p11.0) In traditional database systems, previous query answers are not used to process future queries. If we can use previous query results to answer future queries, we can efficiently estimate an answer. Motivated by such assumption, a new AQP method called "Database Learning" (DBL) has been proposed [87].

(p11.1) DBL uses statistical features (e.g., computing the correlation parameters and covariances between all pairs of past queries snippets) of the dataset to train a model to represent underlying data distribution. When a sample is used to answer the queries, it is hard to know the distribution of the whole data. However, with the help of previous query answers, one can know more about the distribution and then infer answers of new queries based on trained statistical model. The more precise the model, the less need for actual data, the smaller the sample, and consequently, the faster the response time. By increasing previous queries, one can use smaller sample and the system will become smarter and faster to process queries.

(p11.2) Note that DBL is different from using pre-computed views to answer new queries or QCS-based offline synopses which generate summaries for visited query columns. Views aim to use pre-computed results to exactly or approximately answer new queries. For example, if we know the query result of the average profit of orders from the MA state in Fig. 1, we can exactly answer the average profit of orders from c 1 and c 4 and approximately answer the average profit of orders from males in MA. A QCS-based offline synopses shown in Case 3 is based on the assumption on query column sets. However, during the query process, DBL learns model from past observations of past queries results (i.e., training sets) and trains a model, and when a query comes, it uses the trained model to infer the query result. For example, if we know the results of past queries of the profit of orders from c 1 , c 2 , c 3 , c 4 in the MA state and NY state, we can use the results as training set to train a model of relation R. When we need to know the profit of orders from the WA state, we can sample tuples from O and use the trained model to compute the approximation result.

(p11.3) The limitation of DBL is that if the past query result is not accurate, then the quality of a training set of an online learning become worse. Thus, it will become worse and worse and finally it may mislead the approximate result.

(p11.4) In traditional database systems, previous query answers are not used to process future queries. If we can use previous query results to answer future queries, we can efficiently estimate an answer. Motivated by such assumption, a new AQP method called "Database Learning" (DBL) has been proposed [87].

(p11.5) DBL uses statistical features (e.g., computing the correlation parameters and covariances between all pairs of past queries snippets) of the dataset to train a model to represent underlying data distribution. When a sample is used to answer the queries, it is hard to know the distribution of the whole data. However, with the help of previous query answers, one can know more about the distribution and then infer answers of new queries based on trained statistical model. The more precise the model, the less need for actual data, the smaller the sample, and consequently, the faster the response time. By increasing previous queries, one can use smaller sample and the system will become smarter and faster to process queries.

(p11.6) Note that DBL is different from using pre-computed views to answer new queries or QCS-based offline synopses which generate summaries for visited query columns. Views aim to use pre-computed results to exactly or approximately answer new queries. For example, if we know the query result of the average profit of orders from the MA state in Fig. 1, we can exactly answer the average profit of orders from c 1 and c 4 and approximately answer the average profit of orders from males in MA. A QCS-based offline synopses shown in Case 3 is based on the assumption on query column sets. However, during the query process, DBL learns model from past observations of past queries results (i.e., training sets) and trains a model, and when a query comes, it uses the trained model to infer the query result. For example, if we know the results of past queries of the profit of orders from c 1 , c 2 , c 3 , c 4 in the MA state and NY state, we can use the results as training set to train a model of relation R. When we need to know the profit of orders from the WA state, we can sample tuples from O and use the trained model to compute the approximation result.

(p11.7) The limitation of DBL is that if the past query result is not accurate, then the quality of a training set of an online learning become worse. Thus, it will become worse and worse and finally it may mislead the approximate result.
## (s13) Other Works
(p13.0) DAQ DAQ [92] is a variant of OLA which borrows ideas from probabilistic database and iteratively uses the highorder bits of numerical data to compute the approximation. For example, a DAQ scheme stores numbers in column PROFIT in Fig. 1 using "Bitsliced-Index" [92]. If we query MAX on the column PROFIT, DAQ checks the first bit of the numbers in the 16 tuples of O, if there is only one tuple whose first bit is '1', we get the exact answer rather than travel all the bits (e.g., 32 bits); otherwise, we check the next bit until finding the maximum one. Unfortunately, such technique can only support simple queries over numerical columns (such as SUM and AVG) but cannot support general SQL queries.
## (s23) AQP on Complex Data
(p23.0) Besides relational data, many other complex data also become very large, and AQP techniques [106] can be extended to deal with these data. In this section, we take spatial data and trajectory data as examples to show how to enable AQP on these data.

(p23.1) Besides relational data, many other complex data also become very large, and AQP techniques [106] can be extended to deal with these data. In this section, we take spatial data and trajectory data as examples to show how to enable AQP on these data.
## (s29) AQP on Data Cleaning
(p29.0) Data is rather dirty, especially in big data era, and data cleaning and integration are rather important in many applications [26]. For example, in Google Scholar, we want to compute the average citations of database researchers. Since some researchers' Google Scholar pages contain publications that do not belong to them, it is incorrect to directly compute the average citations on the dirty data. A straightforward method first cleans the Google Scholar pages for every researchers and then applies the OLAP queries. Obviously this brute-force method is rather expensive. A smarter way is to utilize AQP techniques, which first cleans a sample data and then uses the sample data to compute the results.
## (s36) Approximate Data Visualization
(p36.0) There are still many problems in approximate data visualization. First, how to quantify the accuracy of visualization is an open problem. Second, selecting proper chart type to fit different AQP methods is difficult. It requires to investigate effective techniques for rapidly generating visualizations for other optimization goals (including outlier detection, trend detection) and other data types (such as large networks). Finding new data visualization applications such as ExploreSample [107] is also promising.

(p36.1) There are still many problems in approximate data visualization. First, how to quantify the accuracy of visualization is an open problem. Second, selecting proper chart type to fit different AQP methods is difficult. It requires to investigate effective techniques for rapidly generating visualizations for other optimization goals (including outlier detection, trend detection) and other data types (such as large networks). Finding new data visualization applications such as ExploreSample [107] is also promising.
## (s49) Introduction
(p49.0) Online analytical processing (OLAP) is a core functionality in data management and analytics systems [33]. The performance of OLAP is crucial for many applications that need to use OLAP to make online decisions, e.g., business intelligence. However, it is rather costly to support OLAP for large datasets, especially big data. Many systems have been proposed to support OLAP on big data, e.g., Pig, Hive, Spark SQL, and they usually take tens of minutes or even hours to answer an OLAP query. However, many applications have online requirement of OLAP that want to get results in seconds.

(p49.1) To alleviate this problem, approximate query processing (AQP) has been proposed, which computes approximate answers (with some quality guarantee) very efficiently to meet the high-performance requirement. Next, we use several examples to show how AQP works. Note that AQP only performs well for aggregate functions such as SUM, AVG, COUNT, MAX and MIN due to it should use statistical tools to give approximate results for numerical types of answers.

(p49.2) AQP Use Cases We consider a database with multiple tables in Fig. 1. For simplicity, we only show three tables, orders O, customers C, states ST, and the relations among the tables. We consider the following three use cases of AQP. However, online sampling without pre-computing may lead to large errors and the quality is uncontrollable. For example, consider a rare group with few tuples (e.g., c 4 ), and the online methods may not get any sample from the group. Thus, the online sampling method fails to provide a high-quality result. To address this problem, we can use offline synopses. synopsis to answer an online query. For example, suppose the queries w.r.t column CustomerID and Profit are frequently used in a query workload, the system builds synopses offline for the two columns and uses them to answer a query. The system uses the error bound and confidence to determine the synopsis size, generates the synopsis, and utilizes the synopsis to answer the query. For example, given a query
## (s50) Online Aggregation Methods
(p50.0) In this section, we survey the online AQP methods. The basic idea is to first select a sample S and then use S to estimate the results. We introduce how to select S in Sect. 2 

(p50.1) In this section, we survey the online AQP methods. The basic idea is to first select a sample S and then use S to estimate the results. We introduce how to select S in Sect. 2 
## (s52) Error Estimation
(p52.0) The confidence interval is widely used to estimate the result quality in most of the random-sampling methods [2], where each confidence interval gives users a numerical interval and a corresponding confidence based on the statistical theory. Initially, a set S of samples is computed based on sampling techniques in Sect. 2.1. Then if the data distribution is known in advance, S can be utilized to estimate the distribution and then the error can be estimated based on the distribution (Sect. 2.2.1). If the data distribution is unknown, it needs to first estimate the distribution of sampling data and then estimate the error (Sect. 2.2.2).

(p52.1) The confidence interval is widely used to estimate the result quality in most of the random-sampling methods [2], where each confidence interval gives users a numerical interval and a corresponding confidence based on the statistical theory. Initially, a set S of samples is computed based on sampling techniques in Sect. 2.1. Then if the data distribution is known in advance, S can be utilized to estimate the distribution and then the error can be estimated based on the distribution (Sect. 2.2.1). If the data distribution is unknown, it needs to first estimate the distribution of sampling data and then estimate the error (Sect. 2.2.2).
## (s60) Database Learning
(p60.0) In traditional database systems, previous query answers are not used to process future queries. If we can use previous query results to answer future queries, we can efficiently estimate an answer. Motivated by such assumption, a new AQP method called "Database Learning" (DBL) has been proposed [87].

(p60.1) DBL uses statistical features (e.g., computing the correlation parameters and covariances between all pairs of past queries snippets) of the dataset to train a model to represent underlying data distribution. When a sample is used to answer the queries, it is hard to know the distribution of the whole data. However, with the help of previous query answers, one can know more about the distribution and then infer answers of new queries based on trained statistical model. The more precise the model, the less need for actual data, the smaller the sample, and consequently, the faster the response time. By increasing previous queries, one can use smaller sample and the system will become smarter and faster to process queries.

(p60.2) Note that DBL is different from using pre-computed views to answer new queries or QCS-based offline synopses which generate summaries for visited query columns. Views aim to use pre-computed results to exactly or approximately answer new queries. For example, if we know the query result of the average profit of orders from the MA state in Fig. 1, we can exactly answer the average profit of orders from c 1 and c 4 and approximately answer the average profit of orders from males in MA. A QCS-based offline synopses shown in Case 3 is based on the assumption on query column sets. However, during the query process, DBL learns model from past observations of past queries results (i.e., training sets) and trains a model, and when a query comes, it uses the trained model to infer the query result. For example, if we know the results of past queries of the profit of orders from c 1 , c 2 , c 3 , c 4 in the MA state and NY state, we can use the results as training set to train a model of relation R. When we need to know the profit of orders from the WA state, we can sample tuples from O and use the trained model to compute the approximation result.

(p60.3) The limitation of DBL is that if the past query result is not accurate, then the quality of a training set of an online learning become worse. Thus, it will become worse and worse and finally it may mislead the approximate result.

(p60.4) In traditional database systems, previous query answers are not used to process future queries. If we can use previous query results to answer future queries, we can efficiently estimate an answer. Motivated by such assumption, a new AQP method called "Database Learning" (DBL) has been proposed [87].

(p60.5) DBL uses statistical features (e.g., computing the correlation parameters and covariances between all pairs of past queries snippets) of the dataset to train a model to represent underlying data distribution. When a sample is used to answer the queries, it is hard to know the distribution of the whole data. However, with the help of previous query answers, one can know more about the distribution and then infer answers of new queries based on trained statistical model. The more precise the model, the less need for actual data, the smaller the sample, and consequently, the faster the response time. By increasing previous queries, one can use smaller sample and the system will become smarter and faster to process queries.

(p60.6) Note that DBL is different from using pre-computed views to answer new queries or QCS-based offline synopses which generate summaries for visited query columns. Views aim to use pre-computed results to exactly or approximately answer new queries. For example, if we know the query result of the average profit of orders from the MA state in Fig. 1, we can exactly answer the average profit of orders from c 1 and c 4 and approximately answer the average profit of orders from males in MA. A QCS-based offline synopses shown in Case 3 is based on the assumption on query column sets. However, during the query process, DBL learns model from past observations of past queries results (i.e., training sets) and trains a model, and when a query comes, it uses the trained model to infer the query result. For example, if we know the results of past queries of the profit of orders from c 1 , c 2 , c 3 , c 4 in the MA state and NY state, we can use the results as training set to train a model of relation R. When we need to know the profit of orders from the WA state, we can sample tuples from O and use the trained model to compute the approximation result.

(p60.7) The limitation of DBL is that if the past query result is not accurate, then the quality of a training set of an online learning become worse. Thus, it will become worse and worse and finally it may mislead the approximate result.
## (s62) Other Works
(p62.0) DAQ DAQ [92] is a variant of OLA which borrows ideas from probabilistic database and iteratively uses the highorder bits of numerical data to compute the approximation. For example, a DAQ scheme stores numbers in column PROFIT in Fig. 1 using "Bitsliced-Index" [92]. If we query MAX on the column PROFIT, DAQ checks the first bit of the numbers in the 16 tuples of O, if there is only one tuple whose first bit is '1', we get the exact answer rather than travel all the bits (e.g., 32 bits); otherwise, we check the next bit until finding the maximum one. Unfortunately, such technique can only support simple queries over numerical columns (such as SUM and AVG) but cannot support general SQL queries.
## (s72) AQP on Complex Data
(p72.0) Besides relational data, many other complex data also become very large, and AQP techniques [106] can be extended to deal with these data. In this section, we take spatial data and trajectory data as examples to show how to enable AQP on these data.

(p72.1) Besides relational data, many other complex data also become very large, and AQP techniques [106] can be extended to deal with these data. In this section, we take spatial data and trajectory data as examples to show how to enable AQP on these data.
## (s78) AQP on Data Cleaning
(p78.0) Data is rather dirty, especially in big data era, and data cleaning and integration are rather important in many applications [26]. For example, in Google Scholar, we want to compute the average citations of database researchers. Since some researchers' Google Scholar pages contain publications that do not belong to them, it is incorrect to directly compute the average citations on the dirty data. A straightforward method first cleans the Google Scholar pages for every researchers and then applies the OLAP queries. Obviously this brute-force method is rather expensive. A smarter way is to utilize AQP techniques, which first cleans a sample data and then uses the sample data to compute the results.
## (s85) Approximate Data Visualization
(p85.0) There are still many problems in approximate data visualization. First, how to quantify the accuracy of visualization is an open problem. Second, selecting proper chart type to fit different AQP methods is difficult. It requires to investigate effective techniques for rapidly generating visualizations for other optimization goals (including outlier detection, trend detection) and other data types (such as large networks). Finding new data visualization applications such as ExploreSample [107] is also promising.

(p85.1) There are still many problems in approximate data visualization. First, how to quantify the accuracy of visualization is an open problem. Second, selecting proper chart type to fit different AQP methods is difficult. It requires to investigate effective techniques for rapidly generating visualizations for other optimization goals (including outlier detection, trend detection) and other data types (such as large networks). Finding new data visualization applications such as ExploreSample [107] is also promising.
