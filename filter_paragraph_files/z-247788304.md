# A Survey on Quantum Computational Finance for Derivatives Pricing and VaR

CorpusID: 247788304 - [https://www.semanticscholar.org/paper/37e273264c38589693c54d332d827a9f57b85739](https://www.semanticscholar.org/paper/37e273264c38589693c54d332d827a9f57b85739)

Fields: Economics, Computer Science

## (s4) Some Models for the Value of Underlying Assets
(p4.0) One of the key ingredients in option pricing is the choice of the dynamics of the underlying stochastic factors. Although there are a lot of possible choices of factors and dynamics that could be taken into account, in this review we will focus on the classical Black and Scholes [13] and Heston dynamics [46] for the underlying asset evolution. This framework can easily be extended to the values of a set of assets when dealing with basket options.
## (s6) Numerical Methods for Options
(p6.0) In order to compute the value of European options with one underlying asset, we can use Girsanov's theorem 4 to work in the risk neutral probability measure Q. Mathematical models for options pricing must take into account the absence of arbitrage hypothesis. For this purpose, the discounted prices of financial products must satisfy the martingale property under certain probability measure, this measure is the so called risk neutral probability measure Q. The dynamics of the underlying asset under this measure is obtained by replacing the drift by the risk free rate r in (5). Under the measure Q, the value of the option is a martingale process, that is, the conditional expected value of the discounted

(p6.1) value of the derivative is constant through time. Next, we use Ito's lemma 5 and the martingale property, so that the option value v t = V(t, S t ) can be obtained as a conditional expectation of the form:
## (s11) Credit Portfolio Management
(p11.0) A source of risk that needs to be measured and managed is the credit risk coming from the risk of default of a number of counterparties. The financial instruments could be simple loans or bonds, or more complex products such as credit derivatives such as Credit Default Swaps or Credit Loan Obligations. It can also include credit exposures stemming from other derivative contracts through counterparty credit risk. Given its complexity and high dimensionality, credit portfolio modelling relies on dimensionality reduction techniques such as factor models and Principal Component Analysis (PCA).

(p11.1) Let us consider a portfolio of N credit products or obligations (e.g. loans, bonds, overdrafts, etc) to a number of counterparties (also known as obligors). Each product is characterized by the exposure at default and the loss given default. The exposure at default is the amount of money that would be due at the time of counterparty default. The loss given default is the actual loss incurred after the recovery process is concluded (this is bounded between zero and 100%). In addition, each counterparty has a known probability of default associated to its credit worthiness. Note that several products can be linked to the same counterparty, however

(p11.2) here we will take the simplifying assumption that each counterparty has only one single product. While the first two parameters will be denoted by E j and P j , j = 1, … , J , the third parameter is assumed to be 100% for all the counterparties. These parameters can be estimated from the capital markets, or from historical credit data.

(p11.3) Assume now that we are in the framework of Merton's firm-value model [65]. In this approach, a counterparty is assumed to go into default if the value of its assets falls below a given default barrier which is linked to the value of its liabilities. In other words, if the value of the assets of a company falls below how much the company owns to its creditors, the model assumes that this firm is in default. Therefore, the combination of asset value and liability barrier determines the credit quality of the counterparty and defines its probability of default. Let V j (t) denote the asset value of instrument j at time t < T , where T is the time horizon (typically one year). The counterparty j defaults when its value at the end of the observation period, V j (T) , falls below barrier j , i.e, V j (T) < j . A default indicator can be defined mathematically as

(p11.4) where Be(p) is a Bernoulli distribution with probability of success p. Given D j , the individual loss of counterparty j is defined as L j = D j ⋅ E j , while the total loss in the portfolio reads

(p11.5) As the credit quality of the counterparty (captured by the asset value in the Merton model) is correlated, this problem is closely related to the basket option covered earlier, but with the additional complexity of having different barriers. In addition, banks portfolios would usually comprise several millions of counterparties, and therefore credit portfolio models typically rely on Monte Carlo techniques to estimate the probability distribution of portfolio losses. Once the probability of portfolio losses L has been estimated, it is possible to compute different risk measures such as those introduced earlier (with PL = L).
## (s13) 3
(p13.0) factors of the systemic part, the model can be classified into the one-or multi-factor class. The power of factor models is that they allow us to reduce the number of parameters needed to capture the portfolio correlations. In the following section we briefly describe some of the most commonly employed models.

(p13.1) One-Factor Models In the one-factor model setting, the credit quality (which in the Merton model is defined as the logarithmic return of the asset value) of counterparty j, X j , at time T is represented by a common, standard normally distributed single factor Y component and an idiosyncratic Gaussian noise component j . The dependence structure between these two latent random variables can be set using copula 6 functions. Thus, these models are also called onefactor copula models. Two models are usually considered in practice. The Gaussian copula model is given by:

(p13.2) where Y and j are i.i.d. standard normal random variables for all j = 1, … , J . Alternatively, as an extension of the model in Eq. (40), the t-copula model was introduced to take into account tail dependence,

(p13.3) , W follows a chi-square distribution 2 ( ) with degrees of freedom and 1 , ⋯ , J , Y and W are mutually independent. Scaling the model in Equation (40) by the factor √ ∕W transforms standard Gaussian random variables into t-distributed random variables with degrees of freedom. For both models, the parameters 1 , … , J ∈ (0, 1) are the correlation coefficients. In the case that j = , for all j = 1, … , J , the parameter is called the common asset correlation.

(p13.4) According to the Merton model described above, counterparty j defaults when the value of its assets falls below the barrier j . The barrier is therefore defined by j ∶= Φ −1 (P j ) or j ∶= Φ −1 (P j ) for the Gaussian and t-copula models respectively, where Φ −1 denotes the inverse of the standard normal cumulative distribution function and Φ −1 is the corresponding inverse distribution function of the t-distribution (with degrees of freedom).

(p13.5) Multi-factor Models Multi-factor models aim to capture more realistic correlation structures, e.g. counterparties in similar industrial sectors and geographies would typically be more correlated. For this, we consider the extension to multiple dimensions of the models presented in Sect. 2.2.2,

(p13.6) i.e., the multi-factor Gaussian copula model and the multifactor t-copula model. The d-factor Gaussian copula model assumes that the covariance structure of [V 1 , … , V J ] is determined by the multi-factor model,

(p13.7) T denotes the systematic risk factors. Note that we represent vectors by bold symbols. Here, a j = a j1 , a j2 , … , a jd T represents the factor loadings satisfying a T j a j < 1 , and j are standard normally distributed random variables representing the idiosyncratic risks, independent of each other and independent of Y . The constant b j , being the factor loading of the idiosyncratic risk factor, is ch o s e n s o t h a t V j h a s u n i t va r i a n c e , i . e . ,

(p13.8) The incentive for considering the multi-factor version of the Gaussian copula model becomes clear when one rewrites it in matrix form, While each j represents the idiosyncratic factor affecting only counterparty j, the common factors Y 1 , Y 2 … , Y d , may affect all (or a certain group of) counterparties. Although the systematic factors are sometimes given economic interpretations (as industry or regional risk factors, for example), their key role is that they allow us to model complicated correlation structures in a non-homogeneous portfolio.
## (s19) Original Amplitude Estimation
(p19.0) The path suggested in the original paper [15] is to adopt an inverse quantum Fourier transform, see Fig. 3. In the picture, the oracle A of Sect. 3.1.1 corresponds to R(P ⊗ ) acting on the first n qubits. The first n qubits are the physical register upon which the block P loads the probability distribution. The block R applies the function whose expected value we want to compute, this is assumed to require one auxiliary qubit. The last m qubits constitute an auxiliary register used for amplitude estimation; it controls different powers of the Grover amplification block Q. Eventually, an inverse of the Quantum Fourier transform on the auxiliary register provides the phase estimation, from which one recovers the amplitude.

(p19.1) The inverse Quantum Fourier transform adopted by the original amplitude estimation algorithm [15] is resource demanding and thus constitutes a serious bottleneck, especially in relation to current or near-future technologies. Said otherwise, Quantum Fourier transform requires a deep and wide quantum circuit. For this reason, some algorithms which need less resources have been proposed.
## (s21) Applications to Option Pricing
(p21.0) As presented in Eq. (8), the option pricing problem can be formulated as the computation of the expected value of a payoff function with respect to a probability distribution: 10 where the domain Ω can be multi-dimensional. In pure computational terms, the problem can be reduced to that of computing an expression like:

(p21.1) where Here we have implicitly defined p, f and X, which are proper discretized versions of p , f and Ω respectively. Recall that, upon the discretization procedure (which we are not specifying to keep the treatment general), the sum in (53) corresponds to the discretized version of the original integral we needed to compute. The problem of defining a suitable (and
## (s27) Quantum Machine Learning
(p27.0) As for many other scientific disciplines, in the last decade machine learning techniques have been intensively applied to diverse problems in quantitative finance. Regressionbased pricing methods, PDE resolution and optimal stochastic control problems are prominent examples. For that reason, the recent advances in the so-called Quantum Machine Learning (QML) front of research can have a great impact when employed on pricing derivatives and risk management or other relevant tasks for the financial industry. The QML explores how to devise and implement quantum algorithms that outperform classical computers on machine learning tasks [12]. Many machine learning classical components have been recently adapted to quantum systems, opening this way a full range of novel applications. Although these new QML algorithms have not been widely employed, so far, for financial applications (to the best of the authors' knowledge), they deserve to be considered in the near future.
## (s31) Discussion
(p31.0) In this section we discuss some of the open problems in quantum computing for option pricing and VaR. As QAMC approaches to solve pricing and VaR problems are the most widely adopted ones in the literature and the ones taking more attention, we focus on them. The main implicit assumption in QAMC is the existence of an efficient oracle which loads the probability distribution. Both for pricing and VaR, loading the distribution means that it is necessary to create a circuit for a unitary operation P such that:

(p31.1) In the case of VaR, the cost of creating such a unitary can be mitigated to some extent using the techniques from Sect. 3.4.1. In the case of pricing, it is much more critical as we discuss below.

(p31.2) In pricing, the distribution to be loaded has to be previously generated through the simulation of a SDE. As it was discussed in the first part of this survey, the simulation of the SDE consumes most of the computing resources. When assessing the overall performance of the QAMC one must take into account this step. Otherwise, the latter comparison of the QAMC and the CMC would be unfair. Indeed, the claims of a quadratic speed-up of the QAMC over the CMC for financial applications-in general-do not take into account the generation step. If we compare both QAMC and CMC under the same conditions, with the approaches proposed in the literature, we will find that there is no rigorous evidence for the quadratic speed-up.

(p31.3) If we assume that we have the probability distribution in the classical case (as it is done for the quantum one), the problem of pricing is reduced to computing the following expectation where p is the probability distribution, f de payoff function and x i are the points where we know the probability

(p31.4) distribution. In this case the number of operations performed in the classical computer is of order N and there is no quadratic speedup for the QAMC. In fact, when adding the costs of loading the probability distribution and the payoff into the quantum state we might end up with a clear disadvantage.

(p31.5) These problems provide concrete examples about possible issues encountered in designing full quantum algorithms able to reach a quantum advantage. Almost any speed-up concentrated in a subroutine of an overarching inefficient algorithm, however interesting, is clearly not sufficient to reach quantum advantage.

(p31.6) We are here implicitly referring (as it often happens) to quantum advantage in terms of scaling of the execution time. This is only a part of a bigger picture which needs to involve other variables such as the energy consumption and cost. Strangely enough, this wider picture is usually not analyzed in the quantum finance literature.

(p31.7) Many ideal algorithms studied in the literature are not viable on current or near-future quantum technology 14 . They usually require either an exceedingly large number of qubits or involve too deep a circuit with respect to the realistic coherence time, or both. The theoretical analysis of algorithms should be always accompanied by a critically explored awareness of current and future technological limitations. In this perspective, an important (negative) claim has been described in [8], where it is argued that a quadratic speed-up is not sufficient to obtain a quantum advantage, mainly due to the-constant but large-resource overheads (needed for error correction). An important overarching suggestion emerging from [8] is that the complexity scaling is in general not enough to properly define an actual threshold for quantum advantage.
