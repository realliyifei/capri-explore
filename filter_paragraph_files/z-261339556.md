# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 A Survey on Multi-Behavior Sequential Recommendation

CorpusID: 261339556 - [https://www.semanticscholar.org/paper/ca9f41ad1c3f0ed51781eef6cfcd035bf4d01d1a](https://www.semanticscholar.org/paper/ca9f41ad1c3f0ed51781eef6cfcd035bf4d01d1a)

Fields: Computer Science

## (s5) C. Challenges
(p5.0) The MBSR problem involves modeling both multiple behaviors and behavioral sequences, contributing to the necessity to consider the existing problems of MBR and SBSR, as well as how to integrate these two kinds of information well. In particular, MBSR has to face the following challenges.

(p5.1) • Sequence modeling of heterogeneous behavioral feedback. In a traditional sequential recommendation problem [12], [29], researchers mostly consider only a single type of behavior, ignoring the potential and importance of other behaviors, especially in instances where the utilized data for the target behavior is sparse. It indicates that it is necessary to model the users' multiple heterogeneous behaviors in the sequential recommendation problem. However, different from SBSR, the uncertainty of users' intention due to heterogeneous behaviors makes it more challenging to predict the user's preference in MBSR. Hence, it is a key and challenging issue to model heterogeneous behaviors well in sequential recommendations without information loss. • Relationship modeling between user behaviors. In the MBSR problem, multiple behaviors of users are often related to each other [26]. For example, in an e-commerce platform, users tend to examine an item, and check the reviews of the item before purchasing it, or purchase an item after examination and adding to cart other items of the same category. Different from MBR, which does not consider the sequential relationship of behaviors, MBSR takes the sequential nature of various behaviors into account. For example, MBR treats both cases the same for users who examine first and then purchase and for users who purchase and then examine, whereas MBSR considers the distinction between the two in modeling. According to the above issues mentioned, there are correlations and transitions among different behaviors within a user-item interaction sequence, which is a great challenge in modeling. • Joint long-term and short-term preference modeling with heterogeneous behaviors of users. Most of the traditional recommendation algorithms statically model the interaction information between users and items [30]- [32], which usually reveals users' long-term stable preferences. However, it ignores the dynamic changes of users' sequential behavior interacted with items. The dynamics of user preferences [33] indicate the user's current shortterm preferences, which can be revealed in the dynamically changing behavioral sequential information of the user. Since the user's interactive behavior information is a behavior sequence that naturally evolved over time, the sequential information can dynamically display the user's long-term stable preferences and short-term needs.

(p5.2) How to take the sequential information into account is the main challenge of SBSR. However, compared with SBSR, the behavioral heterogeneity of MBSR leads to an even greater challenge in modeling a user's long-term and short-term preferences simultaneously. • Related issues such as noise and bias. Some previous works regard unexamined behavior and missing behavior as implicit negative feedback of users, or simply ignore them [34]- [36]. Unexamination does not always represent a negative user preference [37], nor does an examination represent a positive user preference, where there may be mis-examinations. As for bias, since most current recommender systems tend to utilize the implicit feedback of users to make recommendation to users with the goal of more accurate item ranking, there may be a selection bias in implicit feedback data (e.g., a user may examine on an item simply because it is ranked highly). As such, the possible noise and bias in MBSR also deserve our attention.
## (s8) A. Basic Paradigm
(p8.0) In the case with implicit feedback data, the similarity between two items can be calculated using measures such as the Jaccard index and the cosine similarity. Taking item-based collaborative filtering with the Jaccard index as an example, the similarity of items k and j is calculated as follows:

(p8.1) where U i and U i ′ denote the set of users who have interacted with item i and item i ′ , respectively. Based on the calculated similarity, we may select the top-K nearest item set N i ′ for each item i ′ , and then predict the score according to the following formula:

(p8.2) As the predicted rating of an item increases, the possibility that the user will be interested in it increases accordingly. Although there is almost no work for MBSR using neighborhood-based methods, we will introduce BIS [12], a work toward the SBSR problem, to illustrate the idea of the use of similarity in sequential recommendation. It is expected to have some possibilities and inspirations to solve the MBSR problem.
## (s10) IV. MATRIX FACTORIZATION-BASED METHODS
(p10.0) Although neighborhood-based methods may provide interpretability, their aforementioned disadvantages and lower efficiency make them less applicable to MBSR. To address the problem of non-transitivity, a method named matrix factorization has been proposed to connect users who have not purchased common items before [42], [43].
## (s12) B. TransRec++
(p12.0) TransRec++ [44] introduces several behavior transition vectors to capture the sequential relationships between user behaviors and their dynamics, and takes into account some recent preceding items which can learn the weights automatically. The behavior transition vectors contain four types, i.e., from examination to examination e2e, from examination to purchase e2p, from purchase to examination p2e, and from purchase to purchase p2p, which we illustrate in Figure 3. In step ℓ, the overall translation vector of user u to the target item i t u is calculated by the following equation:

(p12.1) where b(·) denotes the behavior type. To achieve a transition of item i t−ℓ u to a future item i t u in step ℓ for a user's sequence, the formula can be calculated as follows:

(p12.2) where V i t−ℓ u and V i t u are the embedding vectors of item i t−ℓ u and item i t u , respectively. The prediction formula is defined as follows:
## (s14) Works
(p14.0) Data Perspective Model Perspective Features RLBL [14] A sequence of (item, behavior) pairs Local Capture the influence of heterogeneous behaviors by utilizing a behavior transition matrix. RIB [26] A sequence of (item, behavior) pairs Local Leverage GRU and attention mechanism simultaneously.

(p14.1) BINN [22] A sequence of (item, behavior) pairs Local Design the CLSTM and the Bi-CLSTM, where the behavior vector is as context in LSTM. CBS [63] Some behavior-specific subsequences of items Local Design of models with and without shared parameters for behaviors simultaneously; towards the next-basket recommendation. DIPN [64] Some behavior-specific subsequences of items Local Leverage GRU and attention mechanism simultaneously; behaviors are specific, including swipe, touch and browse interactive behavior. HUP [27] A sequence of (item, behavior) pairs
## (s15) Local
(p15.0) Design the Behavior-LSTM where adds behavior gate and time gate to the LSTM; leverage attention mechanism; take into account the category of the items. IARS [28] A sequence of (item, behavior) pairs Local Propose Soft-MGRU (a multi-behavior gated recurrent unit) with sharing parameters between behaviors; leverage attention mechanism; take into account the category of the items. DeepRec [62] Some behavior-specific subsequences of items Local + Global Utilizing multi-behavior sequence data to make privacy-preserving recommendation. MBN [65] Some behavior-specific subsequences of items
## (s17) B. GNN-based Learning Architecture
(p17.0) Graph neural network (GNN) [66], [67], utilized to extract features, is a widespread technique in recent years, and there have been many excellent graph neural network models, including GCN [68], GraphSAGE [69], GAT [70] and so on. It can fully exploit the higher-order neighbor information of nodes and performs well on recommender systems.
## (s19) Works
(p19.0) Data Perspective Model Perspective Features DMT [23] Some behavior-specific subsequences of items Local + Global Use target item as query; Consider implicit feedback bias by a bias deep neural network. DFN [84] Some behavior-specific subsequences of items Local + Global Use target item as query; Consider implicit negative feedback noise by an attention network . DUMN [88] Some behavior-specific subsequences of items Local Consider implicit feedback noise; Use memory network to obtain the long-term user preference. FeedRec [25] Some behavior-specific subsequences of items and a sequence of (item, behavior) pairs Local + Global Consider implicit feedback noise by an attention network; Consider multiple patterns of the multi-behavior sequences.

(p19.1) NextIP [86] Some behavior-specific subsequences of items and a sequence of (item, behavior) pairs Local + Global Treat the problem as the item prediction task and the purchase prediction task; Consider multiple patterns of the multibehavior sequences. MB-STR [87] A sequence of (item, behavior) pairs
## (s20) Local
(p20.0) A novel positional encoding function to model multi-behavior sequence relationships. FLAG [85] A behavior-agnostic sequence of items and a sequence of behaviors Local + Global Model user's local preference, local intention and global preference simultaneously. item embedding V target i . Thirdly, the multi-task training model MMoE is used to improve the performance of both CTR and CVR prediction. In particular, DMT considers bias in implicit feedback, such as position and neighboring bias, and utilizes a deep neural network with the ReLU function.

(p20.1) DMT uses a Transformer with unshared parameters to capture the relationships within each behavior and subsequently feeds the different behavioral features into the MMoE module, lacking the explicit modeling of the relationships between the different behaviors. A bias deep neural network is proposed for modeling implicit feedback bias, which is a good modeling solution.

(p20.2) DFN. Deep feedback network (DFN) [84], another work for CTR prediction in ads, models multi-behavior sequences utilizing Transformer from a local modeling perspective and three modules commonly used in industry, i.e., a wide component, an FM component, and a deep component.

(p20.3) We can draw a comparison between DFN and DMT [23]. Firstly, like DMT, DFN employs a Transformer architecture with unshared parameters to capture the relationships within each behavior. It treats multi-behavior sequences as a series of behavior-specific items and inputs them into a multi-head selfattention mechanism. Secondly, DFN also takes into account the implicit feedback noise. Unlike DMT, DFN leverages the attention mechanism to explore the relationship between different behaviors, which can be advantageous. Noting that the implicit negative feedback, i.e., the unexamination sequence S n , is abundant in real life but contains noise. As such, DFN uses implicit positive feedback f e and explicit negative feedback f d to denoise the implicit negative feedback by an attention network. The formula is as follows:

(p20.4) where f e ∈ R 1×d and f d ∈ R 1×d are the keys, and f ne ∈ R 1×d and f nd ∈ R 1×d are the outputs of the two attention networks, respectively. Finally, f e , f d , f n , f nc and f nd are concatenated and fed in the three modules commonly used in industry mentioned above with other features, i.e., item features, user profiles, and recommendation contexts. In addition to DFN, two other works also denoise the implicit feedback by an attention network with the help of the explicit feedback, the first of which, DUMN [88], also utilizes a memory network for modeling users' long-term preferences to perform the CTR prediction task, while the second work FeedRec [25], a work focusing on news recommendation, uses Transformers with shared and unshared parameters to perform user modeling.

(p20.5) NextIP. A dual-task learning approach towards the item prediction task and purchase prediction task (NextIP) [86] utilizes the self-attention mechanism to model multi-behavior sequences from a local modeling perspective and performs the next-item recommendation task. Unlike other methods, NextIP simultaneously treats the multi-behavior sequences as some sequences of behavior-specific items and a sequence of (item, behavior) pairs. Specifically, NextIP treats the multi-behavior sequential recommendation problem as two tasks, i.e., the item prediction task and the purchase prediction task.

(p20.6) In the item prediction task, the embeddings of behaviorspecific and behavior-aware item sequencesare entered into the self-attention block (SAB). Subsequently, NextIP proposes the target-behavior-aware context aggregator (TBCG) to fully model the interplay of different behaviors at different times. Specifically, TBCG takes the representations of the most recent interaction for behavior-specific subsequences as keys and values, takes the user's target behavior embedding as a query, and inputs those into the attention module and mean pooling function with the target behavior representations from the behavior-specific subsequence representations. Finally, the item prediction result is calculated by the inner product between the target item embedding and the representation added by the output of TBCG and the most recent interaction representation of the behavior-aware sequences.

(p20.7) In the purchase prediction task, the user's behavior sequence embeddings are input into the behavior-aware self-attention block, masked depending on user behavior types and behavior distance. Each auxiliary behavior representation from the output of the behavior-aware self-attention block is treated as negative samples to model the user purchase preference.

(p20.8) In summary, NextIP proposes a new perspective on this multi-behavior sequential recommendation problem, by framing it as both an item prediction and a purchase prediction task. This new perspective offers a fresh outlook on the issue at hand, allowing for more accurate and efficient solutions. Moreover, NextIP considers multiple input patterns of the multi-behavior sequences and uses the self-attention network to model multi-behavior sequences with good performance. The contrastive loss function used to train the model also contributes to recommendation performance.

(p20.9) MB-STR. Multi-behavior sequential Transformer recommender (MB-STR) [87] utilizes Transformer to model multibehavior sequences from both global and local modeling perspectives, to address the next-item recommendation problem. MB-STR treats the multi-behavior sequence as a sequence of (item, behavior) pairs and feeds it into the multi-head self-attention network, which considers the sequential pattern and distinguishes it based on the types of behavior. Then a parameter-shared network like MMoE is used to model the behavior-specific information, denoted as a Behavior Aware Prediction (BA-Pred) module. BA-Pred includes two parts, i.e., the parameters-shared experts and the behavior-specific experts, where the latter are shared for the representations of the same behavior.

(p20.10) In summary, MB-STR employs a range of behavior-specific parameters to represent diverse behavioral sequences at a fine-grained level. This approach enables effective modeling of the distinctiveness and interdependence among various behaviors, rendering it a robust tool for behavior modeling. Meanwhile, the total number of parameters in MB-STR is O(|V|d+|B|d 2 +n), and its time complexity is O(n 2 d+nd 2 ), which is moderate compared to other works. Moreover, unlike the positional encoding function of the classical Transformer, MB-STR is inspired by T5 [89] in natural language processing and uses a novel positional encoding function to model multibehavior sequence relationships, which can better capture their positional relationships.

(p20.11) FLAG. Feedback-aware local and global (FLAG) [85] takes into account both user intent and preference complexity in modeling multi-behavior sequences for next-item recommendation. It takes a behavior-agnostic sequence of items and a sequence of behaviors as input, and employs both the global and local modeling perspectives. FLAG has four parts, including a local preference modeling, a global preference modeling, a local intention modeling and a prediction module.

(p20.12) In the local preference modeling, the input matrix X (0) u , composed of the element-wise additions of the item embedding and the position embedding, is fed into the multiple stacked feedback-aware self-attention blocks (FSABs), and then obtains a user's local preference z lp t at time step t from the top FSAB. Specifically, an FSAB successively goes through a feedback-aware input layer with a mask mechanism, a self-attention layer and a feed-forward layer. In the global preference modeling, the authors use a location-based attention layer to model users' global preferences z gp . Given that the preferences of users, both local and global, cannot be effectively modeled through local preference modeling and global preference modeling alone, a feedback-based attention layer (FAL) is proposed for local intention modeling. It receives an input matrix O that takes into account both the examinationspecific and purchase-specific embedding matrices:

(p20.13) where V i t u ∈ R 1×d , p ′ t ∈ R 1×d and F f t u ∈ R 1×d are the itemspecific embedding vector, the position-specific embedding vector and the behavior-specific embedding vector f t u of the item i t u at time step t, respectively. And the next behavior F f t+1 u is treated as a query vector to uncover the user's local intention in the following time step, so as to obtain the final local intention feature z li t . Then an item similarity gating (ISG) module is proposed to achieve a balance between the local and global preferences with a weight factor λ, and then the obtained balanced preference representation z lgp t and the local intention feature z li t are element-wise added to get the final representation z t of the sequence at time step t.

(p20.14) FLAG models the user's local preference, global preference and local intention with acceptable time complexity and space complexity, where the multiple behaviors is utilized as a mask matrix in the local preference learning module, and as part of the input to the module through behavior embedding for better distinguishing the user's different behaviors and consequently improve preference modeling. However, in the local intention learning module, FLAG uses the next real feedback as the query vector during training, which may have a data bias that allows the model to overfitting the historical behavioral data. Furthermore, this approach may not perform well in cold-start settings where there is little historical interaction data.

(p20.15) In summary, Transformer, a sequence-to-sequence model, has demonstrated exceptional performance in recommender systems. Typically, Transformer captures the temporal relationship of behaviors by incorporating positional information in MBSR. Through the utilization of an attention mechanism, it is able to model relationships both within and between behaviors. With superior parallel computing capabilities, an enhanced ability to capture long-term dependencies, and stronger interpretability, Transformer surpasses RNN and GNN in MBSR to some extent.
## (s21) D. Generic-Methods-based Learning Architecture
(p21.0) Since there are a lot of relevant and advanced works in a research area, it is necessary to study a generic framework that can utilize any of the previous relevant works to obtain information. A learning architecture based on a generic method that can employ a particular designed module on a state-of-theart model, combined with some innovative modeling modules to enhance the performance of that model, which is a direction worth further study.

(p21.1) 1) Methods in MBSR: For the MBSR problem, the most important issues to consider are how to model sequences and how to distinguish between different behaviors. As such, the use of generic-methods-based learning architectures can be chosen to improve the recommendation performance by following previous effective models of SBSR or MBR in the modeling of sequences or heterogeneous behaviors. Behavioraware recommendation (BAR) [90] is a generic framework utilized in terms of obtaining sequence representations, which we introduce below.

(p21.2) BAR. BAR proposes a generic learning architecture for modeling multi-behavior sequences from a global modeling perspective, including a behavior attention layer and a taskspecific layer, as we illustrate in Figure 10. In the behavior attention layer, an attention network is used to enhance the presentation of the item embedding. Firstly, the embedding of an item ℓ is added by the behavior embedding B b ℓ u and the position embedding P ℓ . Then an attention network is used to obtain the attention score α ℓ ∈ R representing the relationship between the behavior embedding B b ℓ u and the new presentation of item embedding X ℓ , and is added to the item embedding V i ℓ u to learn the hidden representation at each time step: where RM(·) denotes some important components used in sequential recommendation methods, e.g., recurrent neural network and convolutional neural network. RM(·) reflects the generality of BAR, as any SBSR method like SASRec [78] can be utilized as a module of RM(·) to learn the potential representations of sequences. The task-specific layer is proposed as a solution to address the challenge of unknown whether the behavior is the purchase or not when the model is focused on predicting the next purchased item. It uses an MLP to obtain the connection between the sequential information representation h t−1 and the behavior embedding B b ℓ u . In summary, a general framework like BAR with directly applying the modeling methods used in SBSR possesses better performance and strong generalization capability, but now there are few works aiming to enhance the performance of recommendations. Hence, it could be beneficial to investigate the generalizability of modeling behavior types and transitions or to propose a generic model that incorporates the items' knowledge graph and the social connections among users.
## (s22) E. Hybrid-Methods-based Learning Architecture
(p22.0) Combining multiple technologies for modeling can make use of the advantages of different technologies, and different technologies can also complement each other, leading to the improvement of modeling ability. The effective integration of diverse technologies within different modules is a crucial aspect to be considered when utilizing a hybrid-methods-based learning architecture.

(p22.1) 1) Methods in MBSR: MBSR needs to model the sequence and behavior types at the same time, and it also needs to consider long-term and short-term preferences, as well as local or global information, which provides opportunities for employing different technologies. In MBSR, there are some works utilizing different techniques, including MKM-SR [15], MBGNN [75], MBHT [91], KHGT [92] and TGT [93]. We describe some of them in detail below, and summarize the data MBHT [91] Transformer + GNN A sequence of (item, behavior) pairs Local + Global Model users' short-term and long-term preferences by self-attention network and graph neural network, respectively. TGT [93] Transformer + GNN A sequence of (item, behavior) pairs
## (s23) VI. FUTURE DIRECTIONS
(p23.0) The multi-behavior sequential recommendation (MBSR) problem, which is more representative of real-world recommendation scenarios, has increasingly gained attention from academia and industry in recent years. Although some works with superior recommendation performance towards the MBSR problem have been proposed, there are still many issues worthy of further study. In this section, we discuss some potential future research directions for the MBSR problem, including data, techniques, optimization targets and trustworthiness and responsibility.

(p23.1) Data. In the field of artificial intelligence, a comprehensive understanding of data is crucial for developing models. In the case of MBSR, the complexity of the data also poses various challenges when modeling. First of all, data sparsity has always been the focus of recommendation algorithms [96], and MBSR is no exception. However, excessive data sparsity can undermine the performance of association-based algorithms like collaborative filtering in recommender systems. Additionally, the multiple behaviors of MBSR make the pattern of data sparsity more intricate. In practical situations, such as cold-start settings, where new users or items are seldom interacted with, resolving the data sparsity issue is necessary to generate reasonable recommendations. Secondly, it is essential to explicitly model the data imbalance in MBSR. The data suffers from a heterogeneous behavioral distribution problem similar to MBR and a sequence length problem similar to SBSR. User behavior distribution and interaction sequence lengths often differ in real-world scenarios. For instance, in shopping scenarios, users tend to make fewer purchases than examination behaviors, and users may examine varying item quantities. Thirdly, there are several issues associated with data processing, including periodicity and noise. Periodicity refers to users' inclination to examine items at specific times, and noise refers to users examining items that do not align with their current preferences. While related works have focused on denoising [84], [88], there remains a significant scope for further research, particularly in terms of how to explicitly model various types of specific noise, such as interactions that align with a user's long-term preferences but not their current preferences. As such, it is necessary to further explore how to deal with data sparsity, imbalance, periodicity and noise, etc, so as to improve the effectiveness of recommendations.

(p23.2) Techniques. Technical innovation has been the approach that most works have been focused on to improve the recommendation performance, and there are several challenges to the techniques currently used for MBSR. Firstly, single types of techniques have their own limitations. For example, Transformer can solve the problem of parallel computation that RNN is limited, but is less capable of capturing the local information than RNN due to the point-wise dot-product self-attention utilized [97], [98]. As such, combining multiple complementary components or techniques to solve the MBSR problem is an important research direction. Secondly, efficiency is an essential issue in MBSR due to the complexity of the data. It is worthwhile to investigate how to improve recommendation performance without sacrificing efficiency so as to enable real-time recommendations. Thirdly, how to maintain acceptable time and space complexity when the number of behavior types increases is also a challenging issue. Fourthly, some works propose models that perform well on some datasets but poorly on others during training and prediction [72], [85]. As such, it remains a challenge to improve the generalization of the models for MBSR. In addition, there are difficulties in investigating the MBSR problem utilizing data from different domains, or data with auxiliary information such as item category information, reviews, and knowledge graphs. As interactive conversational recommender systems become more prevalent between users and platforms, future MBSR techniques may need to model multi-behavior sequential data and multiple rounds of conversational text data. In summary, there is much valuable research that can be done on the technical aspects of the MBSR problem, especially in terms of combining methods, improving efficiency, and adaptability of data diversity.
