# Reinforcement Learning for Ridesharing: An Extended Survey *

CorpusID: 246017110
 
tags: #Computer_Science

URL: [https://www.semanticscholar.org/paper/6099eeb3c4d8f8bae466e88075f83c5ee1d9c444](https://www.semanticscholar.org/paper/6099eeb3c4d8f8bae466e88075f83c5ee1d9c444)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Reinforcement Learning for Ridesharing: An Extended Survey *


Zhiwei 
Tony Qin 
Hongtu Zhu 
Jieping Ye 

Lyft Rideshare Labs San Francisco
University of North
Carolina Chapel Hill94107, 27514CA, NC


University of Michigan
48109Ann ArborMI

Reinforcement Learning for Ridesharing: An Extended Survey *

In this paper, we present a comprehensive, in-depth survey of the literature on reinforcement learning approaches to decision optimization problems in a typical ridesharing system. Papers on the topics of rideshare matching, vehicle repositioning, ride-pooling, routing, and dynamic pricing are covered. Most of the literature has appeared in the last few years, and several core challenges are to continue to be tackled: model complexity, agent coordination, and joint optimization of multiple levers. Hence, we also introduce popular data sets and open simulation environments to facilitate further research and development. Subsequently, we discuss a number of challenges and opportunities for reinforcement learning research on this important domain.

# Introduction

The emergence of ridesharing 3 , led by companies such as DiDi, Uber, and Lyft, has revolutionized the form of personal mobility. It is projected that the global rideshare industry will grow to a total market value of $218 billion by 2025 (MarketsAndMarkets 2018). However, how to improve operational efficiency is a major challenge for rideshare platforms, e.g., long passenger waiting time (Smith 2019) and as high as 41% vacant time for ridesharing vehicles in a large city (Brown 2020). The success of ridesharing, from the perspectives of the platforms, drivers, and passengers, requires sophisticated optimization of all the integrated components that collectively deliver the services.

Reinforcement learning (RL) is a machine learning paradigm that trains an agent to take optimal actions (measured by total cumulative reward) through interaction with the environment and getting feedback signals. It is a class of optimization methods for solving sequential decision-making problems with a long-term objective in a stochastic environment. Thanks to the rapid advancement in deep learning research and computing power, the integration of deep neural networks and RL has generated explosive progress in solving complex large-scale decision problems (Silver & Hassabis 2016, Berner et al. 2019, attracting huge amount of renewed interests in the recent years. We are witnessing a similar trend in the ridesharing domain, where the demand and supply are highly stochastic and non-stationary, and the operational decisions are often sequential in nature and have strong spatiotemporal dependency. Simple greedy heuristics that only optimizes for immediate returns tend to produce short-sighted reactive policies, which do not align well with cumulative nature of the true performance measure of interest. The multi-step sequential nature of the decision-making (e.g., pricing, matching, repositioning) and the supply-demand stochasticity in the environment pose enormous challenges to traditional predictive and optimization methods, spanning such issues as forecast accuracy, decision-time computational complexity, and adaptability to real-time changes. RL, on the other hand, presents itself as an excellent promising approach to these ridesharing optimization problems. RL methods are often highly data-driven, making them more suitable to situations where it is hard to build accurate predictive models. They are forward-looking, and yet, they do not explicitly depend on forecasting. And, by design, RL-based policies are dynamic and often light in decision-time complexity.

There are excellent surveys on RL for intelligent transportation (Haydari & Yilmaz 2020, Yau et al. 2017, with in-depth coverage of traffic signals control and autonomous driving. ) offers a broad review of ridesharing systems, whereas Tong et al. (2020) surveys spatial crowdsourcing, which is a more general field than ridesharing. There has been no comprehensive review of the literature on RL for ridesharing, even though the field has attracted much attention and interest from the research communities for both RL and transportation just within the last few years (e.g., (Shah et al. 2020, Al-Kanj et al. 2020, Shou & Di 2020b). This paper aims to fill that gap by surveying the literature of this domain published in top conferences and journals in transportation (e.g., Transportation Research series, IEEE Transactions on Intelligent Transportation Systems, Transportation Science), data mining (e.g., KDD, ICDM, WWW, CIKM), and machine learning/AI (e.g., NeurIPS, AAAI, IJCAI). We describe the research problems associated with the various aspects of a ridesharing system, review the existing RL approaches proposed to tackle each of them, and discuss the challenges and opportunities. Reinforcement learning also has close relationship with approximate dynamic programming. Although it is not our goal in this paper to have a comprehensive review of this class of methods for problems in ridesharing, we aim to point the readers to the representative works so that they can refer to the further literature therein.

This survey is organized as follows: We lay out the ridesharing system architecture in Section 2 and define the scope of the problems to be reviewed. Within this section, as well as the subsequent sections of the survey, we clarify and draw connections among the different names that the problems are referred to, which are often due to the different communities that the authors are from and are easy to confuse by researchers new to ridesharing. In Section 3, we provide a concise review of the RL basics and the major algorithms adopted by the works in this survey. We review in details in Section 4 the literature for each problem described in Section 2 and the relevant data sets and environments. Finally in Section 5, we discuss some challenges and opportunities that we feel crucial in advancing RL for ridesharing.


# Ridesharing

We first describe the architecture of a ridesharing system in this section, followed by explanation and clarification on the scopes of the problem associated with each module.


## Architecture

A ridesharing service, in contrast to taxi hailing, matches passengers with drivers of vehicles for hire using mobile apps. In a typical mobile ridesharing system, there are five major modules: pricing, matching, repositioning, pooling, and routing. Figure 1 illustrates the process and decision modules. When a potential passenger submits a trip request, the pricing module offers a quote, which the passenger either accepts or rejects. Upon acceptance, the matching module attempts to assign the request to an available driver. Depending on driver pool availability, the request may have to wait in the system until a successful match. Pre-match cancellation may happen during this time. The assigned driver then travels to pick up the passenger, during which time post-match cancellation may also occur. The pick-up location is usually where the passenger is making the request or he/she specifies. In some cases, it could be a public designated area, e.g., outside an airport or train station.

After the driver successfully transports the passenger to the destination, she receives the trip fare and becomes available again. The repositioning module guides idle vehicles to specific locations in anticipation of fulfilling more requests in the future. Following the reposition recommendations is usually on a voluntary basis unless it is an autonomous ridesharing setting. Hence, it is common that the platform offers incentives to drivers for completing the repositions. When each driver takes only one passenger request at a time, i.e. only one passenger shares the ride with the driver, this mode is more commonly called 'ride-hailing'. Ridesharing can refer to both ride-hailing and ride-pooling. 4 In the ride-pooling mode, multiple passengers with different trip requests can share one single vehicle, so the pricing, matching, repositioning, and routing problems are different from those for ride-hailing and require specific treatment, in particular, considering the passengers already on board. The routing module provides turn-by-turn guidance on the road network to drivers/vehicles either in service of a passenger request or performing a reposition. The goal is to guide the vehicle to its destination efficiently and safely.


## Problem Scopes

First, we start from the pricing module. Since the trip fare is both the price that the passenger has to pay for the trip and the major factor for the income of the driver, pricing decisions influence both demand and supply distributions through price sensitivities of users, e.g., the use of surge pricing during peak hours. This is illustrated by the solid arrows pointing from the pricing module to orders and idle vehicles respectively in Figure 1. The pricing problem in the ridesharing literature is in most cases dynamic pricing, which adjusts trip prices in real-time in view of the changing demand and supply. The pricing modules sits at the upstream position with respect to the other modules and is a macro-level lever to achieve supply-demand (SD) balance. Although technically, driver pay can be determined by a separate module from pricing and has its own implication on supply elasticity and driver behavior, this paper follows the common setting where driver pay is closely associated (approximately proportional) to the trip fare so that pricing has the dual effect on demand and supply.

The ridesharing matching problem (Yan et al. 2020, Özkan & Ward 2020) may appear under different names in the literature, e.g., order dispatching , trip-vehicle assignment (Bei & Zhang 2018), and on-demand taxi dispatching . It is an online bipartite matching problem where both supply and demand are dynamic, with the uncertainty coming from demand arrivals, travel times, and the entrance-exit behavior of the drivers. Matching can be done continuously in a streaming manner or at fixed review windows (i.e., batching). Sophisticated matching algorithms often leverage demand prediction in some form beyond the actual requests, e.g., the value function in RL. Online request matching is not entirely unique to ridesharing. Indeed, ridesharing matching falls into the family of more general dynamic matching problems for on-demand markets (Hu & Zhou 2022). A distinctive feature of the ridesharing problem is its spatiotemporal nature. A driver's eligibility to match and serve a trip request depends in part on her spatial proximity to the request. Trip requests generally take different amount of time to finish, and they change the spatial states of the drivers, affecting the supply distribution for future matching. The drivers and passengers generally exhibit asymmetric exit behaviors in that drivers usually stay in the system for an extended period of time, whereas passenger requests are lost after a much shorter waiting period in general.

Single-vehicle repositioning may refer to as taxi routing or passenger seeking in the literature. Taxi routing slightly differs in the setting from repositioning a rideshare vehicle in that a taxi typically has to be at a visual distance from the potential passenger to take the request whereas the matching radius of a mobile rideshare request is considerably longer, sometimes more than a mile. System-level vehicle repositioning, also known as driver dispatching, vehicle rebalancing/reallocation, or fleet management, aims to rebalance the global SD distributions by proactively dispatching idle vehicles to different locations. Repositioning and matching are similar to each other in that both relocate a vehicle to a different place as a consequence. In theory, one can treat repositioning as matching a vehicle to a virtual trip request, the destination of which is that of the reposition action, so that both matching and repositioning can be solved in a single problem instance. Typically in practice, these two problems are solved separately because they are separate system modules on most ridesharing platforms with different review intervals and objective metrics among other details. Figure 1: The process flow of ridesharing operations. The solid orange rectangular boxes represent the modules described in Section 2, and the literature on the optimization problems associated with the modules are reviewed in the paper. The blue text and arrow apply exclusively to ride-pooling to account for the fact that order combinations and in-service vehicles are also eligible to participate in matching.

The routing module described in Section 2.1 performs route guidance, which could be dynamic routing or route planning depending on the decision points. Dynamic routing is also called dynamic route choice, and route planning is alternatively referred to as the traffic assignment problem (Shou & Di 2020a). In some cases, the reposition policy directly provides link-level turn-by-turn guidance with the goal of maximizing the driver's income, thus covering the role of dynamic routing albeit with a different objective. Dynamic routing is generally different from the vehicle routing problem (VRP) (Dantzig & Ramser 1959). In VRP, the set of destinations that the vehicle has to visit is known in advance, and hence, it is a static problem. It mainly concerns with the sequence in which the destinations should be visited, considering the estimated travel time from point to point. In contrast, dynamic routing is associated with a road network, and the decision to make is which outgoing road (link) to follow at each intersection (node). The decision is adaptive to the changing traffic condition on the road network in real time. In the context of ride-pooling, there is another emerging problem in which the dynamic routing decisions with passenger(s) on board have to align with the overall objective of ride-pooling.

Some literature refers to the mode of multiple passengers sharing a ride as 'ridesharing'. In this paper, we use term 'ride-pooling' (or 'carpool') to disambiguate the concept, as 'ridesharing' can refer to both single-and multiple-passenger rides. The seminal paper of Alonso-Mora, Samaranayake, Wallar, Frazzoli & Rus (2017) shows that through ride-pooling optimization, the number of taxis required to meet the same trip demand can be significantly reduced with limited impact on passenger waiting times. In a pooling-enabled rideshare system, the matching, repositioning, and pricing modules all have to adapt to additional complexity. Compared to the regular ride-hailing problem, the one with ride-pooling has considerably more complexity due to the more dynamic nature of the problem and the additional constraints and multiple objectives that have to be considered. In this case, the set of available vehicles are augmented, including both idle vehicles and occupied ones not at capacity. It is non-trivial to determine the set of feasible actions (one or more passengers to be assigned to a vehicle) for matching. Every time a new passenger is to be added to a non-idle vehicle, the route has to be recalculated using a VRP solver to account for the additional pick-up and drop-off, the travel times for all the passengers already on board are updated, and the vehicle capacity, the waiting time and detour distance constraints are checked. In-service routing in the context of ride-pooling is discussed in Section 4.5.


# Reinforcement Learning

We briefly review the RL basics and the major algorithms, especially those used in the works reviewed by this survey. For a complete reference, see, e.g., (Sutton & Barto 2018).


## Basics

RL is based on the Markov decision process (MDP) framework, where the agent (the decision-making entity) has a state s (e.g., the location of a vehicle) in the state space S and can perform an action a (e.g., to dispatch or idle) in the action space A. The action is determined by the agent's policy, π(s) : S → A. If the policy is stochastic, then π(a|s) gives the probability of selecting a given s. After executing the action, the agent receives an immediate reward R(s, a) from the environment, and its state changes according to the transition probabilities P (·|s, a). The process repeats until a terminal state or the end of the horizon is reached, giving a sequence of the triplets (s t , a t , r t ) t=T t=0 , where t is the epoch index, T is the final epoch at the terminal state or the end of the horizon, and r t is a sample of R. The objective of the MDP is to maximize the cumulative reward over the horizon.

A key quantity to compute is the value function associated with π,
V π (s) := E π t=T t=0 γ t r t s 0 = s , which satisfies the Bellman equation, V π (s t ) = at π(a t |s t ) st+1,rt P (s t+1 , r t |s t , a t ) r t (s t , a t ) + γV π (s t+1 ) .
(1)

Similarly, we have the action-value function,
Q π (s, a) := E π t=T t=0 γ t r t s 0 = s, a 0 = a ,
which conditions on both s and a. The optimal state-and action-values are denoted by V * and Q * , evaluated at the optimal policy π * . The objective of an MDP is to find an optimal policy π * that maximizes the long-term cumulative discounted reward, i.e., π * := arg max π E s [V π (s)].


## Algorithms

Given P and R, which specifies the MDP, and π, we can compute V π by iteratively applying the Bellman equation (1),
V π (s) ← a π(a|s) s ,r P (s , r|s, a) r + γV π (s ) .(2)
This is called policy evaluation. Using policy evaluation as a sub-routine, we can again iteratively improve the policy through policy iterations, which generate a new policy at each outer iteration by acting greedily with respect to V π , a * ← arg max a s ,r P (s , r|s, a) r + γV π (s ) .

As a special instance, we can collapse the inner policy evaluation loop to a single iteration and compute V * (s), ∀s ∈ S, by the value iterations,
V (s) ← max a s ,r P (s , r|s, a) r + γV (s ) .(4)
If we estimate (P, R) from data, we have a basic model-based RL method. A model-free method learns the value function and optimizes the policy directly from data without learning and constructing a model of the environment. A common example is temporal-difference (TD) learning (Sutton 1988), which iteratively updates V π by TD-errors using π-generated trajectory samples and bootstrapping,
V π (s) ← V π (s) + α r + γV π (s ) − V π (s) ,(5)
where s is the next state in the trajectory after s, α is the step size (or learning rate), and the term that it scales is the TD error. If learning the optimal action-values for control is the goal, we similarly have Q-learning (Watkins & Dayan 1992), which updates the action-value function Q(s, a) to approximate Q * (s, a) by
Q(s, a) ← Q(s, a) + α r + γ max a Q(s , a ) − Q(s, a) .(6)
Q-learning is an off-policy algorithm, where the behavior policy, which collects the experience data and typically involves exploration, is different from the target policy that we are trying to learn and in this case, is the optimal policy. The on-policy counterpart of Q-learning is SARSA, which basically generalizes TD-learning (5) to the action-value function associated with the behavior policy π (same as the target policy):
Q π (s, a) ← Q π (s, a) + α r + γQ π (s , a ) − Q π (s, a) ,(7)
where a is the action executed by the agent at state s in the experience data. The deep Q-network (DQN) (Mnih et al. 2015) approximates Q(s, a) by a neural network Q w parametrized by w along with a few heuristic techniques like experience replay (Lin 1992) and a target network to improve training stability. These techniques are critical to the successful of DQN in playing Atari games and many other applications, due to the deadly triad issue (Sutton & Barto 2018) of reinforcement learning when one tries to combine bootstrapping (i.e., TD-learning and Q-learning), off-policy training (i.e., Q-learning), and function approximations (i.e., neural networks), which may lead to instability and divergence. The algorithms introduced so far are all value-based methods, which focus on learning the value function, and the policy is derived from the learned value function by, e.g., arg max a Q(s, a). Neural network-based value function approximation is important to ridesharing applications because the state is often high dimensional with the incorporation of SD contextual features. Tabular methods suffer from the curse of dimensionality and are not tractable in this case.

A policy-based method directly learns π (which is also called the actor and parametrized by θ) by performing stochastic gradient descent. The central step is computing the policy gradient (PG), the gradient of the cumulative reward J(θ) with respect to the policy parameters θ,
∇ θ J(θ) = s µ(s) a Q π (s, a)∇ θ π(a|s, θ),(8)
where µ is the on-policy distribution under π. A more common (equivalent) form of the PG (8) is s µ(s) a π(a|s, θ)Q π (s, a)∇ θ log π(a|s, θ),

which most of the policy-based methods are based on. REINFORCE (Williams 1992) is a classical PG method, which uses Monte Carlo (MC) rollout to obtains the sample-based update
θ t+1 ← θ t + αG t ∇ θ log π(a|s, θ),(10)
where G t is an MC approximation of Q π . As in a value-based method, we can also use function approximation for the action values. The function approximator (e.g., a neural network) Q w is called the critic, and the resulting algorithm is an actor-critic (AC) method. It is well-recognized that the 'baseline' version of (8),
s µ(s) a π(a|s, θ) Q π (s, a) − b(s) ∇ θ log π(a|s, θ),(11)
where b(s) is an action-independent baseline, reduces the variance in the sample gradient and helps speed up learning. Since a natural choice of such a baseline is the state value, the critic often learns the advantage function Q(s, a) − V (s), and the method is called Advantage Actor Critic (A2C). The evaluation of an action is based on how good it can be with respect to the average over all actions, the benefit of which is to reduce the high variance in the actor and to stabilize the model. Mnih et al. (2016) extend A2C to an asynchronous version (A3C) where independent agents interact with their own copy of the environment and update their model parameters with the master copy asynchronously. This architecture enables much more efficient utilization of the CPU cores through parallel threads and hence accelerates the training. The proximal policy optimization (PPO) (Schulman et al. 2017) optimizes a clipping surrogate objective with respect to the advantage to promote conservative updates to π and is a popular choice of training algorithm for RL problems where policy-based methods are suitable (e.g., with continuous actions).

An MDP can be extended to a Markov game involving multiple agents to form the basis for multiagent RL (MARL). Many MARL algorithms, e.g., Yang et al. (2018), Lowe et al. (2017) focus on agent communication and coordination, in view of the intractable action space in MARL.


## Approximate Dynamic Programming

A family of methods closely related to RL is approximate dynamic programming (ADP) (Powell 2007) for solving stochastic dynamic programs (DP), of which the Bellman equation for MDP is an instance. In ADP methods, unlike that typically seen in RL, a post-decision state s x t is often defined to represent the intermediate state to which the current state s t will transition deterministically given the action a t before the random factors ω t (e.g., demand appearance and cancellation) in the environment realize. With ω t fully realized, the state transitions into the next pre-decision state s t+1 . The value function in an ADP method is defined on the post-decision state and is approximated by a particular functional form. Given the approximated values, the original optimization problem is solved to obtain the decision solution for the current time step. Linear function approximation is popular (e.g., (Simao et al. 2009, Yu & Shen 2019, Al-Kanj et al. 2020) because the dual variables associated with the solution to the current-stage optimization can be used to update the linear function parameters. Then, the state is advanced to the next pre-decision state, and the iteration continues until convergence. By nature, ADP methods are on-policy methods. Recently, neural network-based value function approximation (Shah et al. 2020) has also been adopted and developed due to their higher level of flexibility. In this case, the value function updates largely follow the DQN scheme. The ADP methods for ridesharing reviewed in this survey solve system-level stochastic DP problems (e.g., matching and repositioning) and aim to approximate the system value by decomposing it into local or driver-centric values, and the update schemes employed fall into the family of approximate value iterations.


# Reinforcement Learning for Ridesharing

We review the RL literature for ridesharing in this section grouped by the core operational problems described in Section 2. We first cover pricing, matching, repositioning, and routing in the context of ride-hailing. Then, we will review works on those problems specific to ride-pooling.


## Pricing

RL-based approaches have been developed for dynamic pricing in one-sided retail markets (Raju et al. 2003, Bertsimas & Perakis 2006, where pricing changes only the demand pattern per customers' price elasticity. The ridesharing marketplace, however, is more complex due to its two-sided nature and spatiotemmporal dimensions. In this case, pricing is also a lever to change the supply (driver) distribution if price changes are broadcast to the drivers. Chen et al. (2021) describe examples of such elasticity functions for both demand and supply for their simulation environment.

The challenges in dynamic pricing for ridesharing lie in both its exogeneity and endogeneity. Dynamic pricing on trip inquiries changes the subsequent distribution of the submitted requests through passenger price elasticity. The requests distribution, in turn, influences future supply distribution as drivers fulfill those requests. On the other hand, the trip fares influence the demand for ridesharing services at given locations, and these changes will affect the pool of waiting passengers, which further affects the passengers' expected waiting times. Again, it will influence the demand either through cancellation of the current requests or the conversion of future trip inquiries. Because of its close ties to SD distributions, dynamic pricing is often jointly optimized with order matching or vehicle repositioning. Within the (non-RL) operations research literature, dynamic pricing for ridesharing has already been studied and analyzed in conjunction with matching (Yan et al. 2020, Özkan & Ward 2020) and from the spatiotemporal perspective (Ma et al. 2020, Bimpikis et al. 2019, covering optimality and equilibrium analyses.

The complex interaction between pricing and the SD makes it hard to explicitly develop mathematical models that adapt well to dynamic and stochastic environments, and RL comes in as a promising direction to address these challenges by considering endogeneity and exogeneity as part of the environment dynamics. Table 1 summarizes the reviewed works on RL for dynamic pricing in ridesharing. As one of the early RL works, Wu et al. (2016) consider a simplified ridesharing environment which captures only the two-sidedness of the market but not the spatiotemporal dimensions. The state of the MDP is the current price plus SD information. The action is to set a price, and the reward is the generated profit. A Q-learning agent is trained in a simple simulator, and empirical advantage in the total profit is demonstrated against other heuristic approaches. More recent works leverage the spatiotemporal nature of the pricing actions and take into account the spatiotemporal long-term values in the pricing decisions. Chen, Jiao, Qin, Tang, Li, An, Zhu & Ye (2019) integrate contextual bandits and the spatiotemporal value network developed in ) for matching to jointly optimize pricing and matching decisions. In particular, the pricing actions are the discretized price percentage changes and are selected by a contextual bandits algorithm, where the long-term values learned by the value network are incorporated into the bandit rewards. In (Turan et al. 2020), the RL agent determines both the price for each origin-destination (OD) pairs and the reposition/charging decisions for each electric vehicle in the fleet. The state contains global information such as the electricity price in each zone, the passenger queue length for OD pair, and the number of vehicles in each zone and their energy levels. The reward accounts for trip revenue, penalty for the queues, and operational cost for charging and repositioning. Due to the multi-dimensional continuous action space, PPO is used to train the agent in a simulator. Song et al. (2020) perform a case study of ridesharing in Seoul. They use a tabular Q-learning agent to determine spatiotemporal pricing, and extensive simulations are performed to analyze the impact of surge pricing on alleviating the problem of marginalized zones (areas where it is consistently hard to get a taxi) and on improving spatial equity. Chen et al. (2021) adopt PPO to optimize the spatiotemporal pricing decisions for each hexagonal cell in terms of the per-km rate for the excess mileage beyond a base trip distance and the per-km rate for driver wage, for the objective of maximizing profits (revenue minus wage). The agent is modeled as a global decision-maker with state information of the numbers of open requests, vacant vehicles, occupied vehicles in each grid cell at time t and historical demand at time t − 1. Unlike the works above that focus on the pricing decisions, Mazumdar et al. (2017) study from a different perspective of the pricing problem. The proposed risk-sensitive inverse RL method (Ng et al. 2000) recovers the policies of different types of passengers (risk-averse, risk-neutral, and risk-seeking) in view of surge pricing. The policy determines whether the passenger should wait or take the current ride.

As discussed in Section 2.2, under the setting where the driver pay is associated with the trip fare, the dynamic pricing policy also affects supply elasticity, i.e., drivers' decisions on participation in a given marketplace, working hours, and in some cases, the probability of accepting a given assignment, depending on the rules of the particular ridesharing platform (Chen & Sheldon 2016, Angrist et al. 2021.

Although not yet being widely considered in RL approaches to pricing, supply elasticity is an important piece of system state information that has significant implication to the sequence of pricing decisions. For the closely related topic of driver incentives design, Shang et al. (2019Shang et al. ( , 2021) adopts a learning-based approach to construct a generative model for driver behavior with respect to the incentives policy and subsequently trains an RL agent to optimize the incentives design for system-level metrics. Perhaps this example sheds some light on how RL is able to help improve pricing policies in view of supply-side effects.


## Online Matching

The rideshare matching problem and its generalized forms have been investigated extensively in the field of operations research (see e.g., (Özkan & Ward 2020, Hu & Zhou 2022, Lowalekar et al. 2018 and the references therein). Typically, both the open trip requests and available drivers are batched within time windows of fixed length as they arrive at the system, and they are matched at predefined discrete review times. See Figure 2 for an illustration. Hence, ridesharing matching is an online stochastic problem . Outside the RL literature, Lowalekar et al. (2018) approach the problem through stochastic optimization and use Bender's decomposition to solve it efficiently. To account for the temporal dependency of the decisions, Hu & Zhou (2022) formulate the problem as a stochastic DP and propose heuristic policies to compute the optimal matching decisions. For a related problem, the truckload carriers assignment problem, Simao et al. (2009) also formulate a dynamic DP but with post-decision states so that they are able to solve the problem using ADP. In each iteration, a demand path is sampled, and the value function is approximated in a linear form and updated using the dual variables from the LP solution to the resulting optimization problem.

The RL literature for rideshare matching (see Table 2) typically aims to optimize the total driver income and the service quality over an extended period of time. Service quality can be quantified by response rate and fulfillment rate. Response rate is the ratio of the matched requests to all trip requests. Since the probability of pre-match cancellation is primarily a function of response time (pre-match waiting time), the total response time is an alternative metric to response rate. Fulfillment rate is the ratio of completed requests to all requests and is no higher than the response rate. The gap is due to post-match cancellation, usually because of the waiting for pick-up. Hence, the average pick-up distance is also a relevant quantity to observe. Figure 3 shows the detailed flow of matching a single trip request together with the quantities discussed above.

In terms of the MDP formulation, driver agent is a convenient modeling choice for its straightforward definition of state, action, and reward, in contrast to system-level modeling where the action space is exponential. In this case, the rideshare platform is naturally a multi-agent system with a global objective. A common approach is to crowdsource all drivers' experience trajectories to train a single agent and apply it to all the drivers to generate their matching policies ). Since the system reward is the sum of the drivers' rewards, the system value function does decompose into the individual drivers' value functions computed by each driver's own trajectories. The approximation here is using a single value function learned from all drivers' data. See    This type of single-agent approach avoids dealing explicitly with the multi-agent aspect of the problem and the interaction among the agents during training. Besides simplicity, this strategy has the additional advantage of being able to easily handle a dynamic set of agents (and hence, a changing action space) ). On the other hand, order matching requires strong system-level coordination in that a feasible solution has to satisfy the one-to-one constraints. To address this issue, Xu et al.  2019) take a different approach treating each spatial grid cell as a worker agent and a region of a set of grid cells as a manager agent, and they adopt hierarchical RL to jointly optimize order matching and vehicle repositioning.

In practical settings, the online matching policy often has to balance among multiple objectives (Lyu et al. 2019), e.g., financial metrics and customer experience metrics. The rationale is that persistent negative customer experience will eventually impact long-term financial metrics as users churn the service or switch to competitors. There are two potential ways that one can leverage RL to approach this problem. The explicit approach is to directly learn a policy that dynamically adjusts the weights to combine the multiple objectives into a single reward function. With the abundance of historical experience data, inverse RL can be used to learn the relative importance of multiple objectives under a given unknown policy (Zhou et al. 2021). The implicit approach is to capture the necessary state signals that characterize the impact of the metrics not explicitly in the reward function, so that the learned value function correctly reflect the long-term effect of the multiple metrics. As discussed in Section 5.5, the long feedback loop is a potential challenge here.

Besides the driver-passenger pairing decisions, there are other important levers that can be optimized within the matching module, namely the matching window and the matching radius (Yang, Qin, Ke & Ye 2020). The matching window determines when to match a request (or a batch of requests). A larger window increases pre-match waiting time but may decrease pick-up time for matched requests because of more available drivers. There have been several RL works on the matching window optimization, which can be done from the perspective of a request itself  or the system , Qin, Luo, Yin, Sun & Ye 2021. In , each trip request is an agent. An agent network is trained centrally using pooled experience from all agents to decide whether or not to delay the matching of a request to the next review window, and all the agents share the same policy. To encourage cooperation among the agents, a specially shaped reward function is used to account for both local and global reward feedback. They also modify the RL training framework to address the delayed reward issue by sampling complete trajectories at the end of training epochs to update the network parameters. Wang et al. (2019) take a system's view and propose a Restricted Q-learning algorithm to determine the length of the current review window (or batch size). They show theoretical analysis results on the performance guarantee in terms of competitive ratio for dynamic bipartite graph matching with adaptive windows. Qin, Luo, Yin, Sun & Ye (2021) take a similar modeling perspective but use the AC method with experience replay (ACER) (Wang et al. 2016) that combines on-policy updates (through a queuing-based simulator) with off-policy updates. The matching radius defines how far an idle driver can be from the origin of a given request to be considered in the matching. It can be defined in travel distance or time. A larger matching radius may increase the average pick-up distance but requests are more likely to be matched within a batch window, whereas a smaller radius renders less effective driver availability but it may decrease the average pick-up distance. Both the matching window and radius are trade-offs between pre-match and post-match waiting times (and hence, cancellation). So far, few effort through RL has been devoted to matching radius optimization. The joint optimization of the matching window and radius is certainly another interesting line of research.

Because of its generalizability, matching for ridesharing is closed related to a number of online matching problems in other domains, the RL methods to which are also relevant and can inform the research in rideshare matching. Some examples are training a truck agent using DQN with pooled experience to dispatch trucks for mining tasks (Zhang, Wang, Li & Xu 2020), learning a decentralized value function using PPO with a shaped reward function for cooperation (in similar spirit as ) to dispatch couriers for pick-up services (Chen, Qian, Yao, Wu, Li, Zhou, Hu & Xu 2019), and designing a self-attention, pointer network-based policy network for a system agent to assign participants to tasks in mobile crowdsourcing ).


## Vehicle Repositioning

Vehicle repositioning from a single-driver perspective (i.e., taxi routing) has a relatively long history of research since taxi service has been in existence long before the emergence of rideshare platforms. Likewise, research on RL-based approaches for this problem (see Table 3) also appeared earlier than that on system-level vehicle repositioning.  For the taxi routing problem, each driver is naturally an agent, and the objective thus focuses on optimizing individual reward. Common reward definitions include trip fare (Rong et al. 2016), net profit (income -operational cost) (Verma et al. 2017), idle cruising distance (Garg & Ranu 2018), and ratio of trip mileage to idle cruising mileage (Gao et al. 2018). Earlier works Han et al. (2016), Wen et al. (2017), Verma et al. (2017), Garg & Ranu (2018) optimize the objective within a horizon up to the next successful match (i.e., A-to-B and A-to-C in Figure 4), but it is now more common to consider a long-term horizon, where an episode usually consists of a trajectory over one day (Lin et al. 2018, Shou et al. 2020, Jiao et al. 2021. We illustrate these concepts in Figure 4.

The type of actions of an agent depends on the physical abstraction adopted. A simpler and more common way of representing the spatial world is a grid system, square or hexagonal 5 (Han et al. 2016, Wen et al. 2017, Verma et al. 2017, Gao et al. 2018, Lin et al. 2018, Rong et al. 2016, Jiao et al. 2021, Shou et al. 2020. In this setting, the action space is the set of neighboring cells (often including the current cell). Shou & Di (2020b) explain the justification for this configuration. Determination of the specific destination point is left to a separate process, e.g., pick-up points service (Jiao et al. 2021). The more realistic abstraction is a road network, in which the nodes can be intersections or road segments (Garg & Ranu 2018, Schmoll & Schubert 2020. The action space is the adjacent nodes or edges of the current node. This approach supports a turn-by-turn guiding policy but requires more map information at run time.

Most of the papers adopt a tabular value function, so the state is necessarily low-dimensional, including spatiotemporal information and sometimes additional categorical statuses. Shou et al. (2020) have a boolean in the state to indicate if the driver is assigned to consecutive requests since its setting allows a driver to be matched before completing a trip request. Rong et al. (2016), Zhou et al. (2018) have the direction from which the driver arrives at the current location. For deep RL-based approaches (Wen et al. 2017, Jiao et al. 2021, richer contextual information, such as SD distributions in the neighborhood, can go into the state.

The learning algorithms are fairly diverse but are all value-based. By estimating the various parameters (e.g., matching probability, passenger destination probability) to compute the transition probabilities, The problem formulation most relevant to the ridesharing service provider is system-level vehicle repositioning. Similar to order matching, the ridesharing platform reviews the vehicles' states at fixed time intervals which are significantly longer than those for order matching. See Figure 2 for illustration. Idle vehicles that meet certain conditions, e.g., being idle for sufficiently long time and not in the process of an existing reposition task, are sent reposition recommendations, which specify the desired destinations and the associated time windows. The motivation here is to explicitly modify the current distribution of the available vehicles so that collectively they are better positioned to fulfill more requests more efficiently in the future. Figure 5 explains the idea with a concrete example. If the vehicles reposition independently (following the orange arrows), they both move to the orange-circled area and there will be a surplus of supply while the demand in the green-circled area will not be served. In contrast, if the vehicles coordinate and the one in the south repositions by the blue arrow, both vehicles will be matched, and all the requests are served.

The agent can be either the platform or a vehicle, latter of which calls for a MARL approach. All the works in this formulation have global SD information (each vehicle and request's status or SD distributions) in the state of the MDP, and a vehicle agent will additionally have its spatiotemporal status in the state. The rewards are mostly the same as in the taxi routing case, except that Mao et al. (2020) consider the monetized passenger waiting time. The actions are all based on grid or taxi zone systems.

The system-agent RL formulation has only been studied very recently, in view of the intractability of the joint action space of all the vehicles (see Table 4). To tackle this challenge of scalability, 5 The hexagonal grid system is the industry standard. Figure 4: Illustration of the (single-agent) taxi routing problem on a hexagon grid system. The vehicle at its origin position A has the option to reposition to one of the neighboring and current cells. The black arrows represent reposition (idle cruising), and in the two scenarios, the vehicle is matched to a trip request at B and C respectively. The orange arrows represent trip moves, and the orange flags are where the episodes terminate (for long-term horizons). Feng et al. (2020) decompose the system action into a sequence of atomic actions corresponding to passenger-vehicle matches and vehicle repositions. The MDP encloses a 'sequential decision process' in which all feasible atomic actions are executed to represent one system action, and the MDP advances its state upon complete of the system action. They develop a PPO algorithm for the augmented MDP to determine the sequence of the atomic actions. The system policy in (Mao et al. 2020) produces a reposition plan that specifies the number of vehicles to relocate from zone i to j so that the action space is independent from the number of agents (at the expense of additional work at execution). The agent network, trained by a batch AC method, outputs a value for each OD pair, which after normalization gives the percentage of vehicles from each zone to a feasible destination.

The vehicle-agent approaches have to address the coordination issue among the agents. Lin et al. (2018) develop contextual DQN and AC methods, in which coordination is achieved by masking the action space based on the state context and splitting the reward accrued in a grid cell among the multiple agents within the same cell. Oda & Joe-Wong (2018) treat the global state in grid as image input and develop an independent DQN method. They argue that independent learning, equipped with global state information, works quite well compared to an MPC-based approach. The zone structure in  is constructed by clustering a road-connectivity graph. A single vehicle agent is trained with contextual deep RL and generates sequential actions for the vehicles. Zhang, Wang, Li & Xu (2020) also train a single DQN agent for all agents, but with global KL distance between the SD distributions similar to . The DQN agent is put in tandem with QRewriter, another agent with a Q-table value function that converts the output of DQN to an improved action. Shou & Di (2020b) approach the MARL problem with bilevel optimization: The bottom level is a mean-field AC method  with the reward function coming from a platform reward design mechanism, which is tuned by the top level Bayesian optimization. Agent coordination is done by a central module in (Chaudhari et al. 2020a), where a vehicle agent executes a mix of independent and coordinated actions. The central module determines the need for coordination based on SD gaps, and explicit coordination is achieved by solving an assignment problem to move vehicles from excess zones to deficit zones.

For joint matching and repositioning optimization, one major challenge is the heterogeneous review cadence. Matching and reposition decisions are typically made asynchronously in practice. To address this issue, Tang et al. (2021) allow the two modules to operate independently but share the same spatiotemporal state value function which is updated online. If the two decisions are formulated into the same problem, the action space can be masked depending on the state (Holler et al. 2019).

Existing RL literature on repositioning often assumes the drivers' full compliance to reposition, i.e., the autonomous vehicle setting. How non-compliance affects the overall performance of a reposition algorithm is a natural question to ask when considering a real-world ridesharing system, in which we expect to see a combination of drivers' independent cruising strategies (Urata et al. 2021, Wong et al. 2014) and system-guided idle cruising behavior. It is also interesting and practically necessary to investigate incentives design and strategies that facilitate the repositioning process. In , for example, a mean-field MDP is developed for modeling drivers' strategies, and empirical investigations are performed on how spatiotemporal driver incentives affect driver behavior and the system performance.


## Route Guidance (Navigation)

Routing in this paper refers to low-level navigation decisions on a road network, typically with output of matching and repositioning algorithms as input. The road network, combined with traffic conditions on the links (exhibited as link costs), forms the traffic network which is a non-stationary stochastic network (Mao & Shen 2018). It is known that standard static shortest-path algorithms do not find the path with minimum expected cost in this case, and the optimal route is not a simple route but a policy (Hall 1986, Kim et al. 2005). There are two types of set-up for the routing problem, depending on the decision review time. In the first type of set-up, each vehicle on the road network selects a route for a given OD pair from a set of feasible routes. The decision is only reviewed and revised after a trip is completed. Hence, it is called route planning or route choice. When the routes for all the vehicles are planned together, it is equivalent to assigning the vehicles to each link in the network, and hence, the problem is called traffic assignment problem (TAP), which is typically for transportation planning purposes. In the second type of set-up, the routing decision is made at each intersection to select the next outbound road (link) to enter. These are real-time    adaptive navigation decisions for vehicles to react to the changing traffic state of the road network.

The problem corresponding to this set-up is called dynamic routing, dynamic route choice, or route guidance.

Routing on a road network is a typical multi-agent problem, where the decisions made by one agent has influence on the other agents' performance, simply in that the congestion level of a link depends directly on the number of vehicles passing through that link at a given time and has direct impact on the travel time for all the vehicles on that link within the same time interval. The literature for route planning and TAP often consider the equilibrium property of the algorithms when a population of vehicles adopt them. TAP is typically from a traffic manager's (i.e., system's) perspective. Its goal is to reach system equilibrium (SE, or also often referred to as the system optimum). Some works focus on route planning or TAP from an individual driver's perspective and maximize individual reward. These algorithms try to reach user equilibrium (UE) or Nash equilibrium, under which no agent has the incentive to change its policy because doing so will not achieve higher utility. This is the best that selfish agents can achieve but may not be optimal for the system.

Value-based RL is by far the most common approach for route planning and TAP aiming to reach UE (see Table 5). In the MDP formulation, the agent is a vehicle (or equivalently, a task) with a given OD pair. The objective is to minimize the total travel time for an individual vehicle (task) (Mainali et al. 2008, Ramos et al. 2018, Zhou, Song, Zhao & Liu 2020, i.e., the agent is selfish. The immediate reward is the total travel time of a trip for an individual and a particular run. This MDP is stateless, so strictly speaking, it is a multi-arm bandits or contextual bandits problem (Li et al. 2010) if considering time as a contextual feature. The action to take at decision time is to select a route from the set of feasible routes for the associated OD pair (Ramos et al. 2018, Zhou, Song, Zhao & Liu 2020, Bazzan & Chira 2015. The value function that governs the route choice decisions represents the long-term expected travel time for the trip identified by the given OD pair. Due to the multi-agent nature, the environment w.r.t. each agent is non-stationary in that the reward function is changing with the policy updates from the other agents. Empirical convergence to UE is demonstrated by Ramos et al. (2018). Zhou, Song, Zhao & Liu (2020) further develop a Bush-Mosteller RL scheme for MARL and formally establishes its UE convergence property. We also highlight some unique features of the papers. Ramos et al. (2018) consider a different objective from the common and minimizes the driver's regret. To do that, the Q-learning updates are modified using the estimated action regret, which can be computed by local observations and global travel time information communicated by an app. Bazzan & Chira (2015) propose a hybrid method, with Q-learning for individual agents and Genetic Algorithm for reaching system equilibrium, minimizing the average travel time over different trips in the network. This method is thus able to achieve SE. Mainali et al. (2008) adopt Q-iterations with a model set-up similar to that of dynamic routing to be discussed next.

Most applications of RL to routing concern with the dynamic routing (DR) problem (see Table 5). The MDP is modeled around a vehicle agent. The basic state information is the traffic state of the current node (i.e., intersection). Some works consider state features of the neighboring nodes (Kim et al. 2005, Mao & Shen 2018) so that the agent has a broader view of the environment. The action space comprises the set of outbound links (i.e., roads) or adjacent nodes from the current node, so the policy provides a turn-by-turn navigation guidance until the destination is reached. While it is most common to use travel time on a link as the reward function, Tumer et al. (2008), Grunitzki et al. (2014) stand out by defining a new form called difference reward, which is the difference in average travel time on a link with and without the agent in the system. This applied to only a reward function dependent on the number of agents using the traversed link. In particular, travel distance cannot be used to define a difference reward. Whether solving a specific formulation achieves UE or SE depends on the reward function used. The average travel time on a link is a global reward because it is an aggregate of local rewards (i.e., individual travel times) of all the agents on that link. The difference reward, by definition, is a global reward that also reflects individual effect. If all the agents in the system learn by global reward (Tumer et al. 2008, Grunitzki et al. 2014, Shou & Di 2020a), then the system is expected to achieve SE. Otherwise, the agents learn by their local rewards, and we will have UE or Nash equilibrium (Kim et al. 2005, Yu et al. 2012, Mao & Shen 2018, Bazzan & Grunitzki 2016, Wen et al. 2019.

Most works in the literature adopt Q-learning or its variant as the training algorithm. We report several notable developments. To tackle the sample efficiency issue of online model-free methods, Mao & Shen (2018) propose an offline batch RL approach (fitted Q-iterations) with a tree-based function approximator (Extreme Randomized Trees) that empirically shows good convergence property. Hierarchical methods have also been adopted to address the complexity of a large-scale problem. In (Wen et al. 2019), the global road network is divided into sub-networks by differential evolution-based clustering. The top-level network contains only the boundary nodes of the original network. The top-level policy produces the destination node for a sub-network. The sub-level policy provides link-level guidance to reach its sub-destination. Shou & Di (2020b) adopt a bilevel optimization scheme. At the lower level, a mean-field MARL algorithm solves for the dynamic routing problem for the travelers, while at the upper level, a Bayesian optimization module optimizes the control (i.e., reward parameter of the travelers) by the city planner.


## Ride-pooling (Carpool)

Ride-pooling optimization typically concerns with matching, repositioning, routing (see e.g.,   Tong et al. 2018)). The RL literature has primarily focused on the first two problems. The ride-pooling matching problem differs from that in Section 4.2 in that a combination of multiple passengers, and hence their combined trip, can be matched to a vehicle that may or may not be empty. See stages B and C in Figure 6 from (Alonso-Mora, Samaranayake, Wallar, Frazzoli & Rus 2017) for an illustration. The repositioning problem is similar to the ride-hailing case, except that the objective is to optimize some pooling-specific metrics that we define next. The routing problem solves for the sequence of pick-ups and drop-offs given the assigned passengers for a vehicle. The routing problem could also concern with route guidance on the road network. See stage D in Figure 6.

Many works have multiple objectives and define the reward as a weighted combination of several quantities, with hand-tuned weight parameters. Passenger wait time is the duration between the request time and the pick-up time. Detour delay is the extra time a passenger spends on the vehicle due to the participation in the ride-pooling. In some cases, these two quantities define the feasibility of a potential pooled trip instead of appearing in the reward (Shah et al. 2020). Effective trip distance is the travel distance between the origin and destination of a trip request, should it be fulfilled without ride-pooling. Yu & Shen (2019) consider minimizing passenger wait time, detour delay, and lost demand. Guériau & Dusparic (2018) maximize the number of passengers served. Jindal et al. (2018) maximize the total effective trip distance within an episode, which is just the number of served requests weighted by individual trip distance. Considering a fixed number of requests within an episode (hence fixed maximum effective distance), this metric reflects the efficiency of ride-pooling.   The state of an agent usually consists of global SD information, similar to that for matching and reposition, but the vehicle status contains key additional information of occupancy and OD's of the passengers on board.

The action space depends on whether the agent is modeled at vehicle level or system level. Existing RL-based works all require that a vehicle drops off all the passengers on board according to a planned route before a new round of pooling. An individual vehicle agent can then match to a feasible group of passengers (in terms of capacity and detour delay) (Jindal et al. 2018), reposition to another location (Alabbasi et al. 2019, Haliem et al. 2020, or both (Guériau & Dusparic 2018). A system-level agent has to make action decisions for the entire fleet together (Yu & Shen 2019, Shah et al. 2020.

The feasible combinations of passengers are typically determined by a separate process based on a pairwise shareability graph or a trip-vehicle graph(Alonso-Mora, Samaranayake, Wallar, Frazzoli & Rus 2017). (See illustration in Figure 6.)

Papers with vehicle-level policy commonly train a single agent and apply to all the vehicles independently (see e.g., (Haliem et al. 2021)). DQN is a convenient choice of training algorithm for this setting. It has become increasingly clear that dynamic routing and route planning in the context of ridepooling require specific attention. In particular, there are two aspects unique to ride-pooling. First, the trips are known only at their request times. Hence, the routes taken by the pooled vehicles (i.e., the sequences of pick-ups and drop-offs) have to be updated dynamically to account for the newly joined passengers. Tong et al. (2018), Xu et al. (2020) formulate the route planning problem for ride-pooling and develop efficient DP-based route insertion algorithms for carpool. In Section 4.6, we will see that this is also part of the stochastic dynamic vehicle routing problem. Second, within a given route plan, the route taken by a pooled vehicle from an origin to a destination can affect the chance and quality of its future pooling. Hence, dynamic routing (or route choice) between an OD pair can be optimized in that direction, e.g., Yuen et al. (2019) go beyond the shortest-path to make route recommendations for better chance of pooling. Guériau et al. (2020) evaluate the SAMoD system proposed in (Guériau & Dusparic 2018) in a microscopic environment based on SUMO with a traffic congestion-aware (non-RL) routing component. We expect to see more RL-based algorithms for the ride-pooling dynamic routing problems.


## Vehicle Routing Problem (VRP)

VRP has close connection to the ridesharing problem in that variants of VRP could serve as a subroutine of the ride-pooling problem and could even used to model the entire ridesharing problem itself. The main challenge, in the context of ridesharing, is that new demand (a pair of pick-up and drop-off locations) appears in an online nature and has to be inserted into the existing route dynamically. So reviewing the RL literature for VRP is not only for the completeness of this survey but also essential for one to appreciate the complexity and challenges in tackling ridesharing via RL.

VRP has many variants in its rich literature, so it is important to be clear on their differences and on the variant that each paper claims to solve. The basic setup of a VRP consists of a transportation network G := (V, E, w) and a fleet of K vehicles, where V is the set of nodes (customer and depot locations), and E is the set of edges such that e ij ∈ E indicates that it is possible to travel from node v i to node v j . w(x ij ) := w ij is the edge cost, typically distance or travel time. The depot v 0 ∈ V is a special node where all the vehicles depart and return at. The vanilla VRP is to find an optimal set of disjoint routes (one for each vehicle) that start and end at the depot, that collectively cover all the nodes, and whose total cost (summing over all the edges in those routes) is minimized. A traveling salesman problem (TSP) is a special (simplified) instance of VRP, in which there is no depot, and the fleet consists of a single vehicle. In a capacitated VRP (CVRP), each node has a demand quantity to be fulfilled by one of the vehicles. Each vehicle has a limited capacity, and it starts from the depot with a full load of goods to fulfill the demand of the nodes on its route. The total capacities of the fleet is sufficiently large, and all the demand has to be fulfilled. A CVRP with split delivery allows the demand of each node to be fulfilled by multiple vehicles. In a VRP with time windows, each node has a delivery window within which the node has to be visited or its demand has to be satisfied. If the time window constraints are soft, they can be violated with the price of a penalty that contributes to the total cost function. In some variants of the VRP, the goods for the demand of a node (destination) has to be picked up from another designated non-depot location (origin) before being delivered to it. This is known as a VRP with pick-up and delivery, also known as the dial-a-ride problem (DARP). If the fleet consists of electric vehicles (EV), the set of nodes also include charging stations, and each EV has a limited battery capacity, before the depletion of which the EV has to reach a charging station to recharge. In practical situations, a VRP or variant can be stochastic and dynamic (SDVRP), i.e., its parameters (e.g., demand and travel time) are uncertain, and the requests are not known at the beginning but are revealed sequentially throughout the problem period.

The connection between VRP and ridesharing exists at both local and fundamental levels. As a subproblem in ride-pooling, the rerouting problem after a new passenger is matched to the vehicle is a TSP with pick-up and delivery (TSPPD), which is one-vehicle single-tour instance of CVRP with pick-up and delivery. At a fundamental level, (multi-vehicle) ride-pooling is a stochastic dynamic multi-vehicle CVRP with pick-up and delivery. Although there are no explicit time windows, cancellation may occur if waiting time is too long. Ride-hailing is also a special case where the vehicles all have unit capacity, and in this case, matching and routing merge into one single problem. So the ridesharing problem is an SDCVRP, except that repositioning is an intervention strategy not considered in SDCVRP.

The goal of this section is not to provide a complete survey of the VRP literature but rather to point out the representative or unique works that adopt RL to solve VRPs (see Table 6). A recent review of RL-based methods for solving stochastic dynamic VRP can be found in [Hildebrandt, et al., 2021] and a more general one in (Ulmer et al. 2020).

Single-vehicle v.s. multi-vehicle problems CVRP may appear in different forms, and sometimes the subtle differences may not be stated clearly. Most papers solve the single-vehicle problem where there is only one active vehicle at any time. In the capacitated single vehicle problem, the vehicle can make multiple tours (i.e., passing through the depot multiple times) to fulfill all the demand, but the number of tours is not set in advance. In the multi-vehicle case, a fleet of K vehicles are active simultaneously. For static VRPs, if the number of tours in the single-vehicle case is fixed, then it is equivalent to the multi-vehicle counterpart by treating each tour as a separate vehicle. (For problems with time windows, this equivalence can be achieved by resetting the clock every time a new tour starts.) Otherwise, they are not equivalent in general because the number of tours in the optimal solution for the single-vehicle problem may not be N . For dynamic problems where the requests are not all known a-priori, it is not possible to generate the fixed number of tours in sequence, since one cannot insert a new request to a previous tour. In this case, equivalence can only be achieved by keeping each tour on the same clock and updating the routes with the newly appeared requests at each time step. As we will see below, this would render essentially a multi-vehicle algorithm.

The majority of the RL-based methods for VRP models the agent as a vehicle with the system-state visibility. The state thus consists of two types of information: the vehicle state, which includes the vehicle's current location and remaining capacity (for pick-up and delivery, e.g., (Ulmer et al. 2020, James et al. 2019, Joe & Lau 2020) or inventory (for homogeneous goods delivery, e.g., (Nazari et al. 2018, Kool et al. 2018, Delarue et al. 2020)); the system state, which contains the locations of the customer nodes, the demand at each node, and the unserved customers. For pick-up and delivery problems, the system state instead contains the pick-up and delivery locations of the orders. In the case of EVs, the vehicle state additionally contains the vehicle's battery level, and the system state also includes the locations of the charging stations and the number of vehicles available (not in charging). The action of the agent is to specify the next stop (pick-up/delivery location or charging station in the case EV) to visit for the current vehicle. The sequence of actions form a route for the vehicle. When multiple routes/tours are required, the different routes are separated by the insertion of the depot (Nazari et al. 2018, Duan et al. 2020, Kool et al. 2018, Lin et al. 2021. For (dynamic) multi-vehicle problem where decisions for all the vehicles are made at each time step, the agents would generate their actions sequentially to avoid conflicting actions (James et al. 2019, Zhang, He, Zhang, Lin & Li 2020. Since the objective of VRPs is typically to minimize total travel distance, the reward is naturally defined as the negative travel distance. For problems with (soft) time window constraints, the negative penalty for constraint violation is added to the reward.

Typically, these methods adopt an encoder-decoder agent network architecture. The encoder is responsible for encoding part or all of the state information into an embedding vector (or context), which, potentially with additional input state features, is fed into the decoder to generate the action one at a time. Bello et al. (2016) develop an policy network based on the pointer network (Vinyals et al. 2015), which consists of an RNN encoder and an RNN decoder. The major novelty over a sequence-to-sequence architecture is that the decoder uses attention mechanism to attend over the embeddings of the input nodes to generate the probability distribution over the input space, thus eliminating the distance disparity in the output with respect to the input, a feature that is particularly suitable for solving TSP and VRP. To reduce the complexity of the encoder and avoid imposing a sequence on the input state features (e.g., customer locations), which is unnecessary in routing problems, Nazari et al. (2018) modify the pointer network with a non-sequential encoder which simply embeds each individual input node. They incorporate the policy network into an AC method and validate the design on a CVRP with split delivery. A few more recent works have adopted this network structure. James et al. (2019) use structural graph embedding (Struct2Vec) for the encoder, since their agent's state additionally contains a vehicle tour graph. In (Lin et al. 2021), the encoder has 1D convolution and graph embedding for the input nodes, followed by an attention layer. Duan et al. (2020) include edge features in the state besides the node features of the transportation network. Their encoder is based on graph convolution network with both node and edge inputs. Another work with significant novelty is (Kool et al. 2018), which develops a policy network with a transformer-based encoder and a self-attention-based decoder to use in a PG method (REINFORCE) with the baseline computed from deterministic greedy rollout. This training framework has also been adopted by Zhang, He, Zhang, Lin & Li (2020) for multi-vehicle VRP with soft time windows, Lin et al. (2021) for EV VRP with time windows, and Duan et al. (2020), which jointly train an MLP-based binary classifier on edge encoding with the policy network output as labels. They have tested their method on a CVRP with 400 nodes, the largest among the reviewed works.

For SDVRP, Ulmer et al. (2020) argue that it is a more convenient model, which also aligns better with popular approaches to this problem, that the action contains also the route plan information. They define a new variant of MDP, called route-based MDP, in which the state includes the route plan from the last epoch, and the action contains the updated route plan in addition to the next stop to visit. The 'immediate' reward becomes the difference in route value between the old and new plans. Following this line, Joe & Lau (2020) model a system agent whose state includes the cost for the remaining route for each vehicle, and the agent assigns a new request to a vehicle at each decision epoch. The rerouting after matching is solved by simulated annealing for VRP. Under this framework, one only needs to learn an action-value function to generate the matching decisions. The algorithm is tested on a multi-vehicle SDVRP with pick-up/delivery and time windows. 6 In a somewhat similar spirit but for static CVRP, the MDP action in (Delarue et al. 2020) is to generate one route (tour). The value network consists of dense layers and ReLU activation and is representable by mixed-integer linear constraints so that the action can be computed through solving a Prize Collecting TSP by MIP.


## Data Sets & Environments

The problems in ridesharing are highly practice-oriented, and results from toy data sets or environments may present a very different picture from those in reality. Hence, real-world data sets and realistic simulators backed up by them are instrumental to research in RL algorithms for these problems.  Another taxi data set is the San Francisco data set (Piorkowski et al. 2009), which contains GPS coordinates of approximately 500 taxis collected over 30 days in the San Francisco Bay Area in May 2008. The average time interval between two consecutive location updates is less than 10s.

A more recent rideshare (Transportation Network Providers, TNPs) data set is published by Chicago Data Portal (Portal 2020 Java-based simulation frameworks that also come with graphical user interfaces and visualization tools. They are of more sophisticated engineering architectures albeit with higher programming bars for extension. The evaluation simulation environment for the KDD Cup 2020 competition is available for public access through the DiDi decision intelligence simulation platform (DiDi 2021).

Although not yet open-sourced, this simulation environment supports both matching and vehicle repositioning tasks and accepts input algorithms through a Python API.


# Challenges and Opportunities

Given the state of the current literature, we discuss a few challenges and opportunities that we feel crucial in advancing RL for ridesharing.


## Ride-pooling

As seen in Section 4.5, the reward function in ride-pooling is often a hand-tuned combination of multiple objectives. It is desirable to have a principled way to determine the best weighting scheme automatically, potentially leveraging inverse RL and multi-objective learning techniques (Zou et al. 2021, Arora & Doshi 2021 in a similar sense of the ride-hailing case (Zhou et al. 2021). Methods for learning to make matching decisions are still computationally intensive (Shah et al. 2020, Yu & Shen 2019), in part due to the need to use VRP solver to determine feasible actions (combination of passengers). Moreover, all existing works assume that the action set is pre-determined, and some make only high-level decisions of reposition and serving new passengers or not. A more sophisticated agent may be called for to figure out, for example, how to dynamically determine the desirable passenger combination to match to a vehicle and the routes to take thereafter. Ride-pooling pricing (Ke, Yang, Li, Wang & Ye 2020), a hard pricing problem itself, is tightly coupled with matching. A joint pricing-matching algorithm for ride-pooling is therefore highly pertinent. As mentioned in Section 4.5, it is also highly anticipated to go beyond using generic routing algorithms and to tailor them to ride-pooling with RL.


## Joint Optimization

The rideshare platform is an integrated system, so joint optimization of multiple decision modules leads to better solutions that otherwise unable to realize under separate optimizations, ensuring that different decisions work towards the same goal. RL for joint optimization across multiple modules calls for research on reward function design, state-action representation that facilitates intermodule communication, and the training algorithms. Models and algorithms that allow decentralized execution by the different modules are highly preferred in practice. We have already seen development on RL for joint matching-reposition (Holler et al. 2019) and with ride-pooling (Guériau & Dusparic 2018), pricing-matching (Chen, Jiao, Qin, Tang, Li, An, Zhu & Ye 2019), and pricing-reposition (Turan et al. 2020). An RL-based method for fully joint optimization of all major modules is highly expected. Meanwhile, this also requires readiness from the rideshare platforms in terms of system architecture and organizational structure.


## Heterogeneous Fleet

With the wide adoption of electric vehicles and the emergence of autonomous vehicles, we are facing an increasingly heterogeneous fleet on rideshare platforms. Electric vehicles have limited operational range per their battery capacities. They have to be routed to a charging station when the battery level is low (but sufficiently high to be able to travel to the station). Autonomous vehicles may run within a predefined service geo-fence due to their limited ability (compared to human drivers) to handle complex road situations. For an RL-based approach, a heterogeneous fleet means multiple types of agents with different state and action spaces. The adoption of autonomous vehicles also opens new operational paradigms. Dynamic fleet size inflation (Beirigo et al. 2022), for example, hires idle autonomous vehicles on demand to guarantee service quality contracts in a ridesharing marketplace. Specific studies are required to investigate how to make such a heterogeneous fleet cooperate well to complement each other and maximize the advantage of each type of vehicles to improve overall system efficiency.


## Simulation & Sim2Real

Simulation environments are fundamental infrastructure for successful development of RL methods. Despite those introduced in Section 4.7, simulation continues to be a significant engineering and research challenge. We have rarely seen comparable simulation granularity as that of the environments for traffic management, (e.g., SUMO (Lopez et al. 2018), Flow )) or autonomous driving (e.g., SMARTS (Zhou, Luo, Villela, Yang, Rusu, Miao, Zhang, Alban, Fadakar, Chen et al. 2020), CARLA (Dosovitskiy et al. 2017)). 7 The opportunity is an agent-based microscopic simulation environment for ridesharing that accounts for both ride-hailing and carpool, as well as driver and passenger behavior details, e.g., price sensitivity, cancellation behavior, driver entrance/exit behavior. None of the existing public/open-source simulators supports pricing decisions. Those simulators described in the pricing papers all have strong assumptions on passenger and driver price elasticities. A better way might be to learn those behaviors from data through, e.g., generative adversarial imitation learning (Shang et al. 2019) or inverse RL (Mazumdar et al. 2017).

No publicly known ridesharing simulation environment has sufficiently high fidelity to the real world to allow an agent trained entirely in it to deploy directly to production. Several deployed works , Jiao et al. 2021 in Section 4 have all adopted offline RL for learning the state value functions and online planning. The robotics community has been extensively investigating ways to close the reality gap (Traoré et al. 2019, Mehta et al. 2020. Sim2real transfer algorithms for ridesharing agents are urgently sought after.


## Human Behavior

Central to ridesharing platforms are human participants (passengers and drivers). 8 The impact of human behavior is pervasive in the ridesharing marketplace, e.g., in request conversion, cancellation, idle driver diffusion, driver sign-in and sign-off, rider and driver responses to incentives. Human behavior is inherently stochastic and difficult to model, especially with limited data (in size and features), which introduces errors to optimization and simulation. Compared to traditional approaches from operations research, RL offers potential to better handle these stochasticity issues through its adaptability and data-driven nature.

Unlike cumulative effects induced by spatiotemporal transitions (e.g., matching), human-induced long-term effects from changes in habituation and sentiment on the marketplace are much harder to learn due to the much longer horizon such effects span over. To RL, this is dual challenge and opportunity. The challenge is the long feedback loop and very delayed reward signals, and the opportunities lie in engineering and capturing more refined system state features that capture human behavior characterization better and in designing a richer set of reward signals that facilitate the learning of policies for long-term optimality.


## Non-stationarity

We have seen in Sections 4.2 and 4.3 that RL algorithms deployed to real-world systems generally adopt offline training -once the value function or the policy is deployed, it is not updated until the next deployment. Value functions trained offline using a large amount of historical data are only able to capture recurring patterns resulted from day-on-day SD changes. However, the SD dynamics can be highly non-stationary in that one-time abrupt changes can easily occur due to various events and incidents, e.g., concerts, matches, and even road blocks by traffic accidents. To fully unleash the power of RL, practical mechanisms for real-time on-policy updates of the value function (e.g., , Eshkevari et al. 2022) is required. In view of the low risk tolerance of production systems in general, sample complexity, computational complexity, and robustness are the key challenges that such methods have to address.


## Business Strategies

The research problems in the ridesharing domain are closely associated with how the ridesharing platforms run the operations. Innovation in product and business operations will continue to raise new challenging research problems. There can be multiple alternative product forms to achieve the same goals or address the same challenges, and they inherently define different optimization problems that RL can help tackle.

Surge pricing, for example, is a pricing strategy during the peak hours to address the severe shortage of supply with respect to the surging demand. We have explained its motivation in Section 4.1. While surge pricing is a common practice nowadays, it is not the only strategy that the ridesharing platforms adopt. Passenger requests can be queued if there are no vacant vehicles around to immediately serve the requests (Zhong et al. 2020). The queuing mechanism is perceived in some markets as a more socially acceptable mechanism during the peak hours than surge pricing. Several operational decision questions immediately come up, e.g., how large an area each queue should cover, if the coverage should be dynamically updated, and when the incoming requests should start queuing. These potentially time-varying decisions in a highly stochastic environment are good candidates to be solved for by RL.

Ridesharing platforms often use incentives to stimulate growth on both sides of the marketplace. The forms of incentives are diverse and ever evolving: rider coupons, discounts, target-based challenges, driver bonuses with spatial and temporal constraints, etc. Each incentive strategy changes the behaviors of a certain segment of the marketplace participants in a certain way, and they inevitably interact with the other marketplace levers, e.g., dynamic pricing (Yang, Shao, Wang & Ye 2020). The collective effects of the evolving incentives convolute the environment and dynamics of the marketplace, posing significant challenges to RL and other optimization methods. How to represent and capture these factors or explicitly model them in joint optimization is key to tackle these challenges.

Third-party service integrator allows passengers to simultaneously request orders from multiple ridehailing platforms . Service integrators offer the platforms more access to the demand but also bring competition more explicit by displaying the matching information (e.g., trip fare, pickup distance) side by side. Optimizing pricing and matching policies in a competitive environment with feedback from the service integrator on the competition landscape will be interestingly different from those without a service integrator or in a non-competitive environment. With the added environment complexity, these problems are challenging to solve by traditional methods and could be better tackled by RL.


## General RL

RL provides the necessary tools for the methods reviewed in this survey. Hence, the problems of RL for ridesharing tie closely to the development in RL in general. In the context of ridesharing, we have seen from the literature review above that it is difficult for RL to learn combinatorial actions, e.g., the system matching actions. In the era of deep RL, model interpretability is a long-standing challenge, which hampers investigation of customer experience corner cases. For experience-critical service like ridesharing, policy exploration adds further complication, especially for real-world deployment. In view of these challenges, the future is probably that RL-based and traditional optimization approaches will be complementing each other for a long time. We have seen such combinations in the current literature as  for matching, (Chaudhari et al. 2020a, Jiao et al. 2021 for repositioning, and (Delarue et al. 2020) for VRP, that combine RL with combinatorial optimization, mixed-integer programming, and tree search. The breakthroughs of RL that we are seeing in other domains and the continued development of RL methodology for ridesharing certainly make it exciting to anticipate the future landscape.


# Closing Remarks

We have surveyed the RL literature for the core problems in ridesharing: pricing, dispatching, repositioning, routing, ride-pooling, and VRP. We have also discussed some open challenges and future opportunities pertinent to this area.

The ridesharing system is a complex multi-agent system with multiple decision levers. RL offers a powerful modeling vehicle for optimizing this system, but as we have seen from the current literature, challenges remain in tackling complexity in the learning algorithms, the coordination among the agents, and the joint optimization of multiple levers. Along tackling these challenges, we expect that domain knowledge in ridesharing as well as transportation in general will be increasingly instrumental to the successful adoption of RL. As one may have noticed, most of the literature has just appeared in the last four years, and we expect it to continue growing and updating rapidly.

## Figure 2 :
2for detailed discussions. Specifically, Xu et al. (2018) learn a tabular driver value function using TD(0), and Wang et al. (2018), Tang et al. (2019), Holler et al. (2019) apply DQN-The order matching process with batching from the system perspective (Qin et al. 2020). The assignments are for illustration only.

## Figure 3 :
3The order matching process from a single request's perspective.type of training to learn a value network. In particular,Tang et al. (2019) design a spatiotemporal state-value network using hierarchical coarse coding and cerebellar embedding memories for better state representation and training stability.Holler et al. (2019) develop an action-value network that leverages global SD information, which is embedded into a global context by attention.


(2018), Tang et al. (2019) use the learned state values to populate the edge weights of a bipartite assignment problem to generate a collective-greedy policy (Qin et al. 2020) with respect to the state values. Holler et al. (2019) assume a setting where drivers are matched or repositioned sequentially so that the policy output always satisfies the matching constraints. Leveraging MARL, Li et al. (2019), Jin et al. (2019), Zhou et al. (2019) directly optimize the multiagent system. One significant challenge is scalability since any realistic ridesharing setting can easily involve thousands of agents, precluding the possibility of dealing with an exact joint action space. Li et al. (2019) apply mean-field MARL to make the interaction among agents tractable, by taking the 'average' action of the neighboring agents to approximate the joint actions. Zhou et al. (2019) argue that no explicit communication among agents is required for order matching due to the asynchronous nature of the transitions and propose independent Q-learning with centralized KL divergence (of the supply and demand distributions) regularization. Both Li et al. (2019), Zhou et al. (2019) follow the centralized training decentralized execution paradigm. Jin et al. (


Rong et al. (2016),,Shou et al. (2020),Zhou et al. (2018) adopt a model-based approach and use value iterations to solve the MDP.Shou et al. (2020) further use inverse RL to learn the unit-distance operational cost. Model-free methods are also common, e.g., Monte Carlo learning(Verma et al. 2017), Q-learning(Han et al. 2016, Gao et al. 2018, and DQN(Wen et al. 2017).(Jiao et al. 2021) is a hybrid approach in that it performs an action tree search at the online planning stage using estimated matching probabilities and a separately learned state value network.(Garg & Ranu 2018) is in a similar spirit by augmenting the multi-arm bandits with Monte Carlo tree search.

## Figure 5 :
5Illustration of system-level vehicle repositioning. The requests in the orange-circled and green-circled areas appear in the future w.r.t. the time of repositioning. The empty vehicles are existing ones in the orange-circled area. The orange and blue arrows represent potential reposition moves.

## Figure 6 :
6Illustration of the ride-pooling matching process adapted from (Alonso-Mora, Samaranayake, Wallar, Frazzoli & Rus 2017). Stage A shows the state of the current vehicles and requests. Vehicle 1 has one passenger on board with her destination at the top right-hand corner, while vehicle 2 is empty. The feasible combinations of passengers and the vehicles feasible to serve them are determined at stage B. The corresponding assignment graph is set up and solved at stage C. Stage D shows the resulting routes to fulfill all four new requests as well as the existing trip.


Alabbasi et al. (2019),Haliem et al. (2020),Singh et al. (2021),Haliem et al. (2021) all attempt to minimize the SD mismatch, passenger wait time, reposition time, detour delay, and the number of vehicles used. In addition,Haliem et al. (2020) consider the fleet profit, and Singh et al.(2021)study a more general form of ride-pooling, where a passenger can hop among different vehicles to complete a trip, with each vehicle completing one leg. They further consider the number of hops and the delay due to hopping. on the link Q-iteration. The route is constructed by following the decision at each intersection.


For system-level decision-making, bothYu & Shen (2019) andShah et al. (2020) employ an ADP approach and consider matching decisions only.Yu & Shen (2019) follow a similar strategy as(Simao et al. 2009) and use a linear approximation for the value function. In contrast,Shah et al. (2020) decompose the system value function into vehicle-level ones and adopts a neural network for the individual value function, which is updated by mini-batch stochastic gradient descent similar to that in DQN.

## Table 1 :
1Summary of literature for Pricing.Paper 

Agent 
State 
Action 
Reward 
Algorithm 
Environment 
Wu et al. (2016) 
global decision-
maker 

current trip price 
(same for all 
trips), SD info 

price 
profit 
Q-learning 
no spatiotempo-
ral dimensions 

Chen, Jiao, Qin, Tang, 
Li, An, Zhu & Ye 
(2019) 

global decision-
maker 

features of the 
trip request 

discretized price 
change percent-
age 

profit 
contextual ban-
dits with action 
values 
partly 
computed 
by 
CVNet 

ride-hailing sim-
ulator with pric-
ing module and 
passenger elas-
ticity model 
Turan et al. (2020) 
global decision-
maker 
for 
pricing and EV 
charging 

electricity price 
in each zone, 
passenger queue 
length for each 
OD pair, number 
of vehicles in 
each zone and 
their 
energy 
levels 

price for each 
OD pair, repo-
sition/ charging 
for each vehicle 

trip 
revenue 
-penalty for 
queues -opera-
tional cost for 
charging 
and 
reposition 

PPO 
simulator 

Song et al. (2020) 
global decision-
maker 

location, time 
price for spatial-
temporal grid 
cells 

trip price minus 
penalty 
for 
driver waiting 

Q-learning 
case study: ride-
hailing simula-
tion of Seoul 
Mazumdar et al. 
(2017) 

passenger 
price multiplier, 
time, if a ride has 
completed 

wait, take cur-
rent ride 

trip price to pay 
risk-sensitive in-
verse RL 

historical data 

Chen et al. (2021) 
global decision-
maker 

number of open 
requests, vacant 
vehicles, and oc-
cupied vehicles 
in each grid cell 
at time t, and de-
mand in time t − 
1 

joint actions of 
price (per-km for 
excess mileage) 
and wage (per-
km rate) for each 
grid cell 

profit: revenue 
minus wage 

PPO 
simulation based 
on Hangzhou 
data from DiDi; 
modeling 
on 
both 
supply 
and 
demand 
elasticity 




up distance local reward based on the ultimate outcome of matching (whether or not matched or cancelled): trip value, pickup distance, and match window time Global reward is based on average local reward. Final reward is convex combination of local and global rewards. The rewards have to be updated at the end of the epoch. and vehicles and estimated arrival rates of demand and supply in each cell for each time interval, the negative of total matching wait time for all batched requests and the total pick-up wait time saved (by delaying the current batch)Paper 

Agent 

State 

Action 

Reward 

Algorithm 

Environment 

Notes 

Xu et al. (2018) 

driver 
assignment to a spe-

cific order or idle 

location, time 

trip price 

tabular TD(0) for learning 
state values offline + Hungar-
ian method for generating the 
assignment online 

deployed in production; 
multi-agent, 

agent-level 

simulation 

Wang et al. (2018) 

driver 
assignment to a spe-

cific order or idle 

location, time, SD features 

trip price 

offline DQN for matching, 
CFPT network for transfer 
learning 

single-agent simulation 

single-vehicle problem 

Tang et al. (2019), Qin et al. 
(2020) 

driver 
assignment to a spe-

cific order or idle 

location, time, SD features 

trip price 

CVNet (deep TD-like al-
gorithm) for learning state 
values offline + Hungarian 
method for generating the as-
signment online 

deployed in production; 
multi-agent, 

agent-level 

simulation 

hierarchical 

sparse 

coarse coding, cere-
bellar embedding of 
spatial info, Lipschitz 
regularization 

on 

network 

Holler et al. (2019) 

driver, sys-
tem 
matching a driver to 

an order, reposition a 
driver; matching and 
repositioning done se-
quentially 
global info of all drivers and 
open orders 

trip price, reposition cost 

DQN, PPO 

multi-agent, agent-level sim-

ulation 

attention mechanism to 
extract global state info 
into a context vector 

Li et al. (2019) 

driver 
assignment to a spe-

cific order or idle 

location, time, is_available 

trip price 

mean-field MARL, AC 
method 

homogeneous 

vehicles 

within the same grid cell 

The mean action is 
represented by the 

peers' 

destination 

distribution. 

Jin et al. (2019) 

worker: 

hex 

cell 
manager: 
group of 
hex cells 
(one layer) 
worker: ranking for 
match and reposition 
manager: abstract goal 

for workers 

number of vehicles, orders, 
entropy, reposition-guided 
vehicles, distributions of trip 
prices and durations in the 
given hex cell 
manager: total driver income + specifi-

cally designed quantity to promote high 
order response rate 
worker: intrinsic reward for following 

the goal generated by manager 

hierarchical MARL 

homogeneous 

vehicles 

within the same grid cell 

multiple 

managers, 

each manager commu-
nicates with multiple 
workers 
multi-head 

atten-

tion mechanism for 
coordination 

Zhou et al. (2019) 

driver 
a trip tuple: (origin cell, 
destination cell, trip du-

ration, price) 
cell index, number of idle ve-
hicles, orders, distribution of 
trip destinations in the given 
hex cell 

trip price 

independent learning with 

KL divergence regularization 

homogeneous 

vehicles 

within the same grid cell 

Ke, Yang, Ye et al. (2020) 

trip request 

match or delay 
global: number of idle vehi-
cles, open requests, expected 
arrival rates of requests and 
drivers in each cell 
local: location, cumulative 
waiting time, expected pick-

DQN, PPO, A2C, ACER 
with delayed reward. Whole 
episode trajectories are sam-
pled from replay buffer. 

agent-based simulation with 

delayed matching feature 

Wang et al. (2019) 

system 
expected length of cur-

rent batch 

current nodes in the bipartite 
graph, current batch size 
the sum of the edge weights in the batch 

restricted Q-learning 

DiDi GAIA data set 

The adaptive batch-
based matching has a 
guarantee on competi-

tive ratio 

Qin, Luo, Yin, Sun & Ye 
(2021) 

system 
match the current batch 

or continue to batch 
(decision made at every 

time interval) 

number of batched requests 

ACER that combines on-
policy updates (through a 
queuing-based 

simulator) 

with off-policy updates 

Shanghai taxi data 

Shi et al. (2019) 

vehicle 
remaining battery level 

when available, next 
available time and loca-

tion, global time 

matching, EV (charging) 
trip price -pick-up cost -charging cost 

similar to CVNet (Tang et al. 
2019) 

synthetic data 

assumes decomposibil-

ity of system value into 
vehicle values 

Kullman et al. (2022) 

system 
global time, new re-
quest info, state of each 
vehicle 
joint matching, reposition, 
and charging (EV) for each 
vehicle 

revenue -travel cost 

DQN + attention mecha-
nism over vehicles embed-
dings (similar to (Holler et al. 
2019)) 

NYC taxi data with taxi 
zones 

Decision epoch is ei-

ther a new request ar-
rives or a vehicle be-
comes idle. 

Al-Kanj et al. (2020) 

system 

supply-demand counts 

in spatiotemporal dis-
cretized space 
joint matching and reposi-
tion, EV (charging) 

revenue -charging expense 

ADP with value function ap-
prox. on post-decision states. 
Value function approx. by hi-
erarchical aggregation. 

simulation with New Jersey 
ride-hailing trip data 



## Table 2 :
2Summary of literature for Online Matching.

## Table 3 :
3Summary of literature for Vehicle Repositioning (taxi routing).Paper 

Type 

Agent 
State (in addition to ST 
info) 

Action 

Reward 

Episode 

Algorithm 

Coordination 

Data 

Lin et al. (2018) 

system driver 
global SD contextual 
features in all cells 

neighboring cells in a grid 

system 

trip fare, shared when mul-
tiple agents are in the same 
grid cell 

long-term 

contextual DQN, AC 

contextual state fea-

tures, action space 
pruned by context 

4 weeks of DiDi 
data in Chengdu, 
China 

Oda & Joe-Wong 
(2018) 

system driver 
global SD state dis-
cretized into cells, 
treated as an image 

reachable cells in a grid sys-

tem within the reposition cy-
cle 

weighted number of pick-ups 

-reposition time 

long-term 

independent DQN 

-

NYC taxi data 

Shou & Di (2020b) 

system vehicle -

neighboring cells in a grid 

system 

trip fare 

long-term 
bilevel optimization: top 
Bayesian optimization to up-

date reward param, bottom 
mean-field MARL (AC) 

mean-field MARL 

NYC taxi data 

Jiao et al. (2021) 
system vehicle SD contextual info in 
the current and neigh-
boring cells 

neighboring cells in a grid 

system 

trip fare -reposition cost 

long-term 

deep SARSA 

stochastic 

policy 

through softmax of 
action values, 

SD 

regularization to action 

values 

DiDi ride-hailing 
data 

Mao et al. (2020) 
system system SD info for each zone 

reposition plan: number of 
repositioned vehicles for 

each OD pair in a zone map 

monetized passenger waiting 

time 

long-term 

batch AC 

central 

decision-

making 

NYC taxi data 

Feng et al. (2020) 
system system status of every vehicle 

and request 

atomic action: 

driver-

passenger match or driver-
destination match (repo-

sition); 
system action: 

sequence of atomic actions 

trip fare -operational cost 

long-term (day) 
PPO applied to MDP with 
Sequential Decision Process 
embedded: global actions 
decomposed into sequential 

atomic ones 

central 

decision-

making 

DiDi data: 5 re-
gions, 1000 cars, 
360 minutes 

Liu et al. (2020) 
system vehicle discrete zone structure 

constructed by cluster-

ing a road connectivity 
graph 

neighboring zones 

trip fare 

long-term 
contextual DQN with shared 

value function 

vehicle actions gener-
ated sequentially 

real-world taxi 

data 

Zhang, Wang, Li & Xu 
(2020) 
system vehicle global SD distributions 

neighboring cells in a grid 

system 

global KL distance between 
SD distributions 

long-term 
DQN + Q-table in tandem 
with shared value function 

global state features 

DiDi data 

Chaudhari 

et 

al. 

(2020a) 
system vehicle cell in an ST table 

neighboring cells in a grid 

system, wait 

trip fare -operational cost 

long-term 
SARSA-like policy evalua-

tion 

solve an assignment 
problem of surplus and 
deficits in terms of SD 
gap 

NYC taxi data 



## Table 4 :
4Summary of literature for Vehicle Repositioning (system reposition).

## Table 5 :
5Summary of literature for Route Guidance.


Encoder with 1D conv layer and graph embedding for the nodes and attention layer; LSTMbased decoderThe most commonly used data sets are those made available by NYC TLC (Taxi & Limousine 
Commission) (TLC 2020). This large public data repository contains trip records from several 
different services, Yellow Taxi, Green Taxi, and FHV (For-Hire Vehicle), from 2009 to 2020. The 
Yellow Taxi data is the most frequently used for various studies. The FHV trip records are submissions 
from the TLC-licensed bases (e.g., Uber, Lyft) and have a flag indicating pooled trips offered by Uber 
Pool and Lyft Line. The pick-up and drop-off locations are represented by taxi zones. Manhattan, 
for example, is divided into 64 zones. There is no driver ID associated with the trip records, so 
reconstructing historical driver-based trajectories is not possible. An older version of the NYC 
data set (Donovan & Work 2016), however, does include GPS coordinates for pick-up and drop-off 
locations, and car IDs can be used to track drivers within each year, allowing for more granular and 
diverse analyses. A similar subset of the NYC FHV data is also available at (Kaggle 2017), with GPS 
Paper 

Type 

State 

Action 

Reward 

Network 

Algorithm 

Problem Size 

Nazari et al. (2018) 

single-vehicle 
location and demand of 
each request 

the next request to visit 

negative travel distance 
Non-sequential encoder for the input with 

RNN decoder that attends over over the input 
space (Pointer network without an RNN en-
coder) 

AC 

Single-vehicle Capaci-

tated VRP with split de-
livery: one active vehi-
cle at a time 

Kool et al. (2018) 

single-vehicle 
coordinates and orig-

inal demand of each 
node, remaining de-
mand of each node, re-
maining capacity of the 
vehicle 

next stop for a given ve-
hicle 

negative travel distance 
transformer encoder with input of coordinates 

and demand of each node + self-attention-
based decoder with additional input of remain-
ing demands and capacity at time t 

REINFORCE 

with 

greedy rollout baseline 

TSP and Capacitated 
VRP with split delivery, 
100 nodes 

Balaji et al. (2019) 

single-vehicle 
current pickup loca-
tion, vehicle's location 
and remaining capac-
ity, orders' locations, 
statuses, waiting times, 

and values 

accept an order, pick 

up an accepted order, 
wait 

value of delivered or-
der (accept, pickup, de-
liver) -cost (waiting, 
traveling, penalty) 

two dense layers NN 

APE-X DQN (Horgan 

et al. 2018), a variant of 
a DQN that utilizes dis-
tributed prioritized ex-
perience replay 

stochastic and dynamic 
CVRP 

with 

pick-

up/delivery and time 
windows, 8 x 8 map, 

5 orders 3 pick-up 
locations 

James et al. (2019) 

multi-vehicle 
system state (available 
requests, charging sta-

tion output, vehicles' 
status(location, battery 

levels, next stops)), ve-
hicle tour graph 

next stop for a given ve-
hicle; The vehicles gen-
erate actions sequen-
tially at each time step. 

expected 

objective 

value for a tour 
pointer network for actor, another critic net-
work; Network architecture is similar to 
NeurIPS-18 paper, but with structural graph 
embedding (Struct2Vec) for the encoder. 

A3C 

multi-vehicle dynamic 

VRP with pick-up and 
delivery for EVs: 200 
random requests, 100 
vehicles 

Zhang, He, Zhang, Lin 

& Li (2020) 

multi-vehicle 
same as (Kool et al. 
2018) 

same as (Kool et al. 
2018); The vehicles 
generate actions se-
quentially at each time 

step. 

negative travel distance 

+ negative constraint vi-
olation penalty 
same as (Kool et al. 2018) 

same as (Kool et al. 
2018) 

Multi-vehicle 

VRP 

with soft time windows 
(no split delivery): 150 
nodes, 5 vehicles 

Ulmer et al. (2020) 

single-vehicle 
vehicle location, time, 
num of passengers 
onboard, 

info 

of 

in-process and out-
standing 

requests, 

route plan from last 
epoch 

the next stop to visit 
and the new route plan; 
The paper defines a 
new variant of MDP 
called 

route-based 

MDP. 

difference in route 

value between the old 
route plan and the new 
one 

N.A. 

insert new request s 
into the current route 
and use variable neigh-
borhood search to im-
prove the route 

SDVRP with pick-up 

and delivery (DDARP) 

Joe & Lau (2020) 

multi-vehicle 
includes the cost for 

the remaining route for 
each vehicle 

matching a new order 

to a vehicle. Rerouting 
after matching is done 
by simulated annealing 
for VRP 

cost diff between two 
consecutive decisions 

not specified 

NN-based TD learning 

with experience replay 
(like in (Tang et al. 
2019)) to learn the 
action-value function 

Multi-vehicle 

dy-

namic 

VRP 

with 

pick-up/delivery and 

delivery windows: 48 
nodes, 2 vehicles, avg 

22 orders/day 

Delarue et al. (2020) 

single-vehicle 
the remaining unvisited 
nodes 

to generate one route 
(tour) through solving 

a Prize Collecting TSP 
by MIP 

negative tour distance 
Value network consists of dense layers + ReLU 
activation (representable by mixed-integer lin-

ear constraints) 

MC policy iteration: 
rollout N trajectories, 

fit a new NN 

CVRP: 51 nodes 

Duan et al. (2020) 

single-vehicle 
nodes (location, de-
mand), edges (distance, 
adjacency) 

generate one node at a 

time sequentially; The 
resulting sequence may 

have multiple depot oc-
currences for different 

tours. 

negative travel distance 
GCN-based encoder with both node and edge 
features; GRU-based decoder similar to the 
pointer network as policy network and MLP-
based decoder on the edge encoding as classi-
fier 

REINFORCE 

with 

greedy rollout baseline 
(Kool et al. 2018) to 
train the policy net-
work; Cross-entropy 
loss to train the binary 
classifier of route 
edges with policy 
output as labels 

CVRP: 400 nodes 

Lin et al. (2021) 

multi-vehicle 

For time t, 

the state 

of each vertex (loca-
tion, time window, re-
maining demand), and 
global variables (time, 
battery level of the ac-
tive vehicle, number of 
EVs not in charging) 

Next stop for the 
current route; Unlike 
(James et al. 2019) the 
routes of the vehicles 
are generated sequen-
tially. Every time the 
depot appears in the 
sequence, the system 

time is reset to 0. 

negative travel distance 

+ negative penalties for 
constraint violations 
REINFORCE 

with 

greedy rollout baseline 
(Kool et al. 2018) 

EV with time window 
and charging. Within 
the planning horizon, a 
vehicle can visit the de-

pot only once: C100, 
S12, EV12 



## Table 6 :
6Summary of literature for VRP.coordinates for pick-up and drop-off locations. In addition, travel time data between OD pairs can be obtained through Uber Movement(Uber 2021).


). This data set contains trips, drivers, and vehicles data reported by Transportation Network Providers (TNP, or rideshare companies) in Chicago from 2018. Trip origin and destinations are represented by census tracts. Times are rounded to the nearest 15 minutes. Fares are rounded to the nearest $2.50 and tips are rounded to the nearest $1.00. The driver and vehicle data are not joinable with the trip data.Developing ridesharing simulators has been a line of research itself. Yao & Bekhor (2021) offer a comprehensive review of recent works on ridesharing simulation models, most of them covering a subset of considerations on the number of passengers, the pre-/post-match passenger cancellation behaviors, and driver acceptance/rejection behaviors. In(Yao & Bekhor 2021), a sophisticated eventbased simulation framework is proposed to capture all aspects of the behavior modeling. Although the 'ridesharing' in their paper is known as the hitch service, where the driver is on her own trip as well, the modeling framework is general and accommodates the ridesharing setting in this survey.Chaudhari et al. (2020a) offer a Gym-compatible, open-source ride-hailing environment (Chaudhari et al. 2020b) for training dispatching and repositioning agents. For large-scale simulation on transport networks, AMoDeus (Ruch et al. 2018) and MATSim (W Axhausen et al. 2016) are well-established
In this survey, we do not cover topics on hitch, in which the driver is on his/her own trip with a specific destination.
This method can be regarded as one for the matching problem in ride-pooling described in Section 4.5.
Guériau et al. (2020) evaluate the ridesharing algorithms in SUMO, but the environment is not public. 8 A partial exception is an autonomous ridesharing platform, where the supply side is powered by autonomous vehicles. However, such services are still prototypical at the time of writing and have very limited coverage.

Approximate dynamic programming for planning a ride-hailing system using autonomous fleets of electric vehicles. L Al-Kanj, J Nascimento, W B Powell, European Journal of Operational Research. 2843Al-Kanj, L., Nascimento, J. & Powell, W. B. (2020), 'Approximate dynamic programming for planning a ride-hailing system using autonomous fleets of electric vehicles', European Journal of Operational Research 284(3), 1088-1106.

Deeppool: Distributed model-free algorithm for ride-sharing using deep reinforcement learning. A Alabbasi, A Ghosh, V Aggarwal, IEEE Transactions on Intelligent Transportation Systems. 2012Alabbasi, A., Ghosh, A. & Aggarwal, V. (2019), 'Deeppool: Distributed model-free algorithm for ride-sharing using deep reinforcement learning', IEEE Transactions on Intelligent Transportation Systems 20(12), 4714-4727.

On-demand highcapacity ride-sharing via dynamic trip-vehicle assignment. J Alonso-Mora, S Samaranayake, A Wallar, E Frazzoli, D Rus, Proceedings of the National Academy of Sciences. 1143Alonso-Mora, J., Samaranayake, S., Wallar, A., Frazzoli, E. & Rus, D. (2017), 'On-demand high- capacity ride-sharing via dynamic trip-vehicle assignment', Proceedings of the National Academy of Sciences 114(3), 462-467.

Predictive routing for autonomous mobility-on-demand systems with ride-sharing. J Alonso-Mora, A Wallar, D Rus, IEEE. Alonso-Mora, J., Wallar, A. & Rus, D. (2017), Predictive routing for autonomous mobility-on-demand systems with ride-sharing, in '2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)', IEEE, pp. 3583-3590.

Uber versus taxi: A driver's eye view. J D Angrist, S Caldwell, J V Hall, American Economic Journal: Applied Economics. 133Angrist, J. D., Caldwell, S. & Hall, J. V. (2021), 'Uber versus taxi: A driver's eye view', American Economic Journal: Applied Economics 13(3), 272-308.

A survey of inverse reinforcement learning: Challenges, methods and progress. S Arora, P Doshi, 103500Arora, S. & Doshi, P. (2021), 'A survey of inverse reinforcement learning: Challenges, methods and progress', Artificial Intelligence p. 103500.

Orl: Reinforcement learning benchmarks for online stochastic optimization problems. B Balaji, J Bell-Masterson, E Bilgin, A Damianou, P M Garcia, A Jain, R Luo, A Maggiar, B Narayanaswamy, C Ye, arXiv:1911.10641arXiv preprintBalaji, B., Bell-Masterson, J., Bilgin, E., Damianou, A., Garcia, P. M., Jain, A., Luo, R., Maggiar, A., Narayanaswamy, B. & Ye, C. (2019), 'Orl: Reinforcement learning benchmarks for online stochastic optimization problems', arXiv preprint arXiv:1911.10641 .

A hybrid evolutionary and multiagent reinforcement learning approach to accelerate the computation of traffic assignment. A L Bazzan, C Chira, Bazzan, A. L. & Chira, C. (2015), A hybrid evolutionary and multiagent reinforcement learning approach to accelerate the computation of traffic assignment, in 'AAMAS', pp. 1723-1724.

A multiagent reinforcement learning approach to en-route trip building. A L Bazzan, R Grunitzki, 2016 International Joint Conference on Neural Networks (IJCNN)', IEEE. Bazzan, A. L. & Grunitzki, R. (2016), A multiagent reinforcement learning approach to en-route trip building, in '2016 International Joint Conference on Neural Networks (IJCNN)', IEEE, pp. 5288- 5295.

Algorithms for trip-vehicle assignment in ride-sharing. X Bei, S Zhang, 'Thirty-second AAAI conference on artificial intelligence. Bei, X. & Zhang, S. (2018), Algorithms for trip-vehicle assignment in ride-sharing, in 'Thirty-second AAAI conference on artificial intelligence'.

A business class for autonomous mobility-on-demand: Modeling service quality contracts in dynamic ridesharing systems. B A Beirigo, R R Negenborn, J Alonso-Mora, F Schulte, Transportation Research Part C: Emerging Technologies. 136103520Beirigo, B. A., Negenborn, R. R., Alonso-Mora, J. & Schulte, F. (2022), 'A business class for autonomous mobility-on-demand: Modeling service quality contracts in dynamic ridesharing systems', Transportation Research Part C: Emerging Technologies 136, 103520.

I Bello, H Pham, Q V Le, M Norouzi, S Bengio, arXiv:1611.09940Neural combinatorial optimization with reinforcement learning. arXiv preprintBello, I., Pham, H., Le, Q. V., Norouzi, M. & Bengio, S. (2016), 'Neural combinatorial optimization with reinforcement learning', arXiv preprint arXiv:1611.09940 .

Dota 2 with large scale deep reinforcement learning. C Berner, G Brockman, B Chan, V Cheung, P Dębiak, C Dennison, D Farhi, Q Fischer, S Hashme, C Hesse, arXiv:1912.06680arXiv preprintBerner, C., Brockman, G., Chan, B., Cheung, V., Dębiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C. et al. (2019), 'Dota 2 with large scale deep reinforcement learning', arXiv preprint arXiv:1912.06680 .

Dynamic pricing: A learning approach, in 'Mathematical and computational models for congestion charging. D Bertsimas, G Perakis, SpringerBertsimas, D. & Perakis, G. (2006), Dynamic pricing: A learning approach, in 'Mathematical and computational models for congestion charging', Springer, pp. 45-79.

Spatial pricing in ride-sharing networks. K Bimpikis, O Candogan, D Saban, Operations Research. 673Bimpikis, K., Candogan, O. & Saban, D. (2019), 'Spatial pricing in ride-sharing networks', Opera- tions Research 67(3), 744-769.

The Ride-Hail Utopia That Got Stuck in Traffic. E Brown, Brown, E. (2020), The Ride-Hail Utopia That Got Stuck in Traffic -WSJ. https://www.wsj.com/ articles/the-ride-hail-utopia-that-got-stuck-in-traffic-11581742802.

Learn to earn: Enabling coordination within a ride hailing fleet. H A Chaudhari, J W Byers, E Terzi, Proceedings of IEEE International Conference on Big Data. IEEE International Conference on Big DataChaudhari, H. A., Byers, J. W. & Terzi, E. (2020a), 'Learn to earn: Enabling coordination within a ride hailing fleet', Proceedings of IEEE International Conference on Big Data .

Simulation code for "learn to earn: Enabling coordination within a ride-hailing fleet. H A Chaudhari, J W Byers, E Terzi, Chaudhari, H. A., Byers, J. W. & Terzi, E. (2020b), 'Simulation code for "learn to earn: Enabling coordination within a ride-hailing fleet"'. "https://transparent-framework.github.io/ optimize-ride-sharing-earnings/".

Spatial-temporal pricing for ride-sourcing platform with reinforcement learning. C Chen, F Yao, D Mo, J Zhu, X M Chen, Transportation Research Part C: Emerging Technologies. 130103272Chen, C., Yao, F., Mo, D., Zhu, J. & Chen, X. M. (2021), 'Spatial-temporal pricing for ride-sourcing platform with reinforcement learning', Transportation Research Part C: Emerging Technologies 130, 103272.

Inbede: Integrating contextual bandit with td learning for joint pricing and dispatch of ride-hailing platforms. H Chen, Y Jiao, Z Qin, X Tang, H Li, B An, H Zhu, J Ye, '2019 IEEE International Conference on Data Mining (ICDM)', IEEE. Chen, H., Jiao, Y., Qin, Z., Tang, X., Li, H., An, B., Zhu, H. & Ye, J. (2019), Inbede: Integrating contextual bandit with td learning for joint pricing and dispatch of ride-hailing platforms, in '2019 IEEE International Conference on Data Mining (ICDM)', IEEE, pp. 61-70.

Dynamic pricing in a labor market: Surge pricing and flexible work on the uber platform. M K Chen, M Sheldon, 16455Chen, M. K. & Sheldon, M. (2016), 'Dynamic pricing in a labor market: Surge pricing and flexible work on the uber platform.', Ec 16, 455.

Can sophisticated dispatching strategy acquired by reinforcement learning?-a case study in dynamic courier dispatching system. Y Chen, Y Qian, Y Yao, Z Wu, R Li, Y Zhou, H Hu, Y Xu, arXiv:1903.02716arXiv preprintChen, Y., Qian, Y., Yao, Y., Wu, Z., Li, R., Zhou, Y., Hu, H. & Xu, Y. (2019), 'Can sophisticated dis- patching strategy acquired by reinforcement learning?-a case study in dynamic courier dispatching system', arXiv preprint arXiv:1903.02716 .

The truck dispatching problem. G B Dantzig, J H Ramser, Management science. 61Dantzig, G. B. & Ramser, J. H. (1959), 'The truck dispatching problem', Management science 6(1), 80-91.

Reinforcement learning with combinatorial actions: An application to vehicle routing. A Delarue, R Anderson, C Tjandraatmadja, Advances in Neural Information Processing Systems. 33Delarue, A., Anderson, R. & Tjandraatmadja, C. (2020), 'Reinforcement learning with combinatorial actions: An application to vehicle routing', Advances in Neural Information Processing Systems 33, 609-620.

Didi decision intelligence simulation platform. Didi , DiDi (2021), 'Didi decision intelligence simulation platform'. "https://outreach.didichuxing. com/Simulation".

New york city taxi trip data. B Donovan, D Work, 10.13012/J8PN93H8Donovan, B. & Work, D. (2016), 'New york city taxi trip data (2010-2013)'. URL: https://doi.org/10.13012/J8PN93H8

A Dosovitskiy, G Ros, F Codevilla, A Lopez, V Koltun, PMLRCarla: An open urban driving simulator, in 'Conference on robot learning. Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A. & Koltun, V. (2017), Carla: An open urban driving simulator, in 'Conference on robot learning', PMLR, pp. 1-16.

Efficiently solving the practical vehicle routing problem: A novel joint learning approach. L Duan, Y Zhan, H Hu, Y Gong, J Wei, X Zhang, Y Xu, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningDuan, L., Zhan, Y., Hu, H., Gong, Y., Wei, J., Zhang, X. & Xu, Y. (2020), Efficiently solving the practical vehicle routing problem: A novel joint learning approach, in 'Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining', pp. 3054- 3063.

Reinforcement learning in the wild: Scalable rl dispatching algorithm deployed in ridehailing marketplace. S S Eshkevari, X Tang, Z Qin, J Mei, C Zhang, Q Meng, J Xu, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningEshkevari, S. S., Tang, X., Qin, Z., Mei, J., Zhang, C., Meng, Q. & Xu, J. (2022), Reinforcement learning in the wild: Scalable rl dispatching algorithm deployed in ridehailing marketplace, in 'Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining'.

Scalable deep reinforcement learning for ride-hailing. J Feng, M Gluzman, J Dai, IEEE Control Systems Letters. Feng, J., Gluzman, M. & Dai, J. (2020), 'Scalable deep reinforcement learning for ride-hailing', IEEE Control Systems Letters .

Optimize taxi driving strategies based on reinforcement learning. Y Gao, D Jiang, Y Xu, International Journal of Geographical Information Science. 328Gao, Y., Jiang, D. & Xu, Y. (2018), 'Optimize taxi driving strategies based on reinforcement learning', International Journal of Geographical Information Science 32(8), 1677-1696.

Route recommendations for idle taxi drivers: Find me the shortest route to a customer!. N Garg, S Ranu, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningGarg, N. & Ranu, S. (2018), Route recommendations for idle taxi drivers: Find me the shortest route to a customer!, in 'Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining', pp. 1425-1434.

Individual versus difference rewards on reinforcement learning for route choice. R Grunitzki, G De Oliveira Ramos, A L C Bazzan, IEEEGrunitzki, R., de Oliveira Ramos, G. & Bazzan, A. L. C. (2014), Individual versus difference rewards on reinforcement learning for route choice, in '2014 Brazilian Conference on Intelligent Systems', IEEE, pp. 253-258.

Shared autonomous mobility on demand: A learning-based approach and its performance in the presence of traffic congestion. M Guériau, F Cugurullo, R A Acheampong, I Dusparic, IEEE Intelligent Transportation Systems Magazine. 124Guériau, M., Cugurullo, F., Acheampong, R. A. & Dusparic, I. (2020), 'Shared autonomous mobility on demand: A learning-based approach and its performance in the presence of traffic congestion', IEEE Intelligent Transportation Systems Magazine 12(4), 208-218.

Samod: Shared autonomous mobility-on-demand using decentralized reinforcement learning. M Guériau, I Dusparic, '2018 21st International Conference on Intelligent Transportation Systems (ITSC)', IEEE. Guériau, M. & Dusparic, I. (2018), Samod: Shared autonomous mobility-on-demand using decentral- ized reinforcement learning, in '2018 21st International Conference on Intelligent Transportation Systems (ITSC)', IEEE, pp. 1558-1563.

A distributed model-free ride-sharing algorithm with pricing using deep reinforcement learning, in 'Computer Science in Cars Symposium. M Haliem, G Mani, V Aggarwal, B Bhargava, Haliem, M., Mani, G., Aggarwal, V. & Bhargava, B. (2020), A distributed model-free ride-sharing al- gorithm with pricing using deep reinforcement learning, in 'Computer Science in Cars Symposium', pp. 1-10.

A distributed model-free ride-sharing approach for joint matching, pricing, and dispatching using deep reinforcement learning. M Haliem, G Mani, V Aggarwal, B Bhargava, IEEE Transactions on Intelligent Transportation Systems. 2212Haliem, M., Mani, G., Aggarwal, V. & Bhargava, B. (2021), 'A distributed model-free ride-sharing approach for joint matching, pricing, and dispatching using deep reinforcement learning', IEEE Transactions on Intelligent Transportation Systems 22(12), 7931-7942.

The fastest path through a network with random time-dependent travel times. R W Hall, Transportation science. 203Hall, R. W. (1986), 'The fastest path through a network with random time-dependent travel times', Transportation science 20(3), 182-188.

Routing an autonomous taxi with reinforcement learning. M Han, P Senellart, S Bressan, H Wu, Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. the 25th ACM International on Conference on Information and Knowledge ManagementHan, M., Senellart, P., Bressan, S. & Wu, H. (2016), Routing an autonomous taxi with reinforcement learning, in 'Proceedings of the 25th ACM International on Conference on Information and Knowledge Management', pp. 2421-2424.

Deep reinforcement learning for intelligent transportation systems: A survey. A Haydari, Y Yilmaz, IEEE Transactions on Intelligent Transportation Systems. Haydari, A. & Yilmaz, Y. (2020), 'Deep reinforcement learning for intelligent transportation systems: A survey', IEEE Transactions on Intelligent Transportation Systems .

Deep reinforcement learning for multi-driver vehicle dispatching and repositioning problem. J Holler, R Vuorio, Z Qin, X Tang, Y Jiao, T Jin, S Singh, C Wang, J Ye, 2019 IEEE International Conference on Data Mining (ICDM. J. Wang, K. Shim & X. WuWashington, DCInstitute of Electrical and Electronics EngineersHoller, J., Vuorio, R., Qin, Z., Tang, X., Jiao, Y., Jin, T., Singh, S., Wang, C. & Ye, J. (2019), Deep reinforcement learning for multi-driver vehicle dispatching and repositioning problem, in J. Wang, K. Shim & X. Wu, eds, '2019 IEEE International Conference on Data Mining (ICDM)', Institute of Electrical and Electronics Engineers, Washington, DC, pp. 1090-1095.

. D Horgan, J Quan, D Budden, G Barth-Maron, M Hessel, H Van Hasselt, D Silver, arXiv:1803.00933arXiv preprintDistributed prioritized experience replayHorgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., Van Hasselt, H. & Silver, D. (2018), 'Distributed prioritized experience replay', arXiv preprint arXiv:1803.00933 .

Surge pricing and two-sided temporal responses in ride hailing. B Hu, M Hu, H Zhu, Manufacturing & Service Operations Management. 241Hu, B., Hu, M. & Zhu, H. (2022), 'Surge pricing and two-sided temporal responses in ride hailing', Manufacturing & Service Operations Management 24(1), 91-109.

Dynamic type matching. M Hu, Y Zhou, Manufacturing & Service Operations Management. 241Hu, M. & Zhou, Y. (2022), 'Dynamic type matching', Manufacturing & Service Operations Manage- ment 24(1), 125-142.

Online vehicle routing with neural combinatorial optimization and deep reinforcement learning. J James, W Yu, J Gu, IEEE Transactions on Intelligent Transportation Systems. 2010James, J., Yu, W. & Gu, J. (2019), 'Online vehicle routing with neural combinatorial optimiza- tion and deep reinforcement learning', IEEE Transactions on Intelligent Transportation Systems 20(10), 3806-3817.

Real-world ride-hailing vehicle repositioning using deep reinforcement learning. Y Jiao, X Tang, Z T Qin, S Li, F Zhang, H Zhu, J Ye, Transportation Research Part C: Emerging Technologies. 130103289Jiao, Y., Tang, X., Qin, Z. T., Li, S., Zhang, F., Zhu, H. & Ye, J. (2021), 'Real-world ride-hailing vehi- cle repositioning using deep reinforcement learning', Transportation Research Part C: Emerging Technologies 130, 103289.

Coride: Joint order dispatching and fleet management for multi-scale ride-hailing platforms. J Jin, M Zhou, W Zhang, M Li, Z Guo, Z Qin, Y Jiao, X Tang, C Wang, J Wang, Proceedings of the 28th ACM International Conference on Information and Knowledge Management. the 28th ACM International Conference on Information and Knowledge ManagementJin, J., Zhou, M., Zhang, W., Li, M., Guo, Z., Qin, Z., Jiao, Y., Tang, X., Wang, C., Wang, J. et al. (2019), Coride: Joint order dispatching and fleet management for multi-scale ride-hailing plat- forms, in 'Proceedings of the 28th ACM International Conference on Information and Knowledge Management', pp. 1983-1992.

Optimizing taxi carpool policies via reinforcement learning and spatio-temporal mining. I Jindal, Z T Qin, X Chen, M Nokleby, J Ye, '2018 IEEE International Conference on Big Data (Big Data. Jindal, I., Qin, Z. T., Chen, X., Nokleby, M. & Ye, J. (2018), Optimizing taxi carpool policies via reinforcement learning and spatio-temporal mining, in '2018 IEEE International Conference on Big Data (Big Data)', IEEE, pp. 1417-1426.

Deep reinforcement learning approach to solve dynamic vehicle routing problem with stochastic customers. W Joe, H C Lau, Proceedings of the International Conference on Automated Planning and Scheduling. the International Conference on Automated Planning and Scheduling30Joe, W. & Lau, H. C. (2020), Deep reinforcement learning approach to solve dynamic vehicle routing problem with stochastic customers, in 'Proceedings of the International Conference on Automated Planning and Scheduling', Vol. 30, pp. 394-402.

Uber pickups in new york city -trip data for over 20 million uber (and other for-hire vehicle) trips in nyc. Kaggle, Kaggle (2017), 'Uber pickups in new york city -trip data for over 20 million uber (and other for-hire vehicle) trips in nyc'. "https://www.kaggle.com/fivethirtyeight/ uber-pickups-in-new-york-city".

Pricing and equilibrium in on-demand ride-pooling markets. J Ke, H Yang, X Li, H Wang, J Ye, Transportation Research Part B: Methodological. 139Ke, J., Yang, H., Li, X., Wang, H. & Ye, J. (2020), 'Pricing and equilibrium in on-demand ride-pooling markets', Transportation Research Part B: Methodological 139, 411-431.

Learning to delay in ride-sourcing systems: a multi-agent deep reinforcement learning framework. J Ke, H Yang, J Ye, IEEE Transactions on Knowledge and Data Engineering. Ke, J., Yang, H., Ye, J. et al. (2020), 'Learning to delay in ride-sourcing systems: a multi-agent deep reinforcement learning framework', IEEE Transactions on Knowledge and Data Engineering .

Optimal vehicle routing with real-time traffic information. S Kim, M E Lewis, C C White, IEEE Transactions on Intelligent Transportation Systems. 62Kim, S., Lewis, M. E. & White, C. C. (2005), 'Optimal vehicle routing with real-time traffic information', IEEE Transactions on Intelligent Transportation Systems 6(2), 178-188.

Attention, learn to solve routing problems!. W Kool, H Van Hoof, M Welling, arXiv:1803.08475arXiv preprintKool, W., Van Hoof, H. & Welling, M. (2018), 'Attention, learn to solve routing problems!', arXiv preprint arXiv:1803.08475 .

Dynamic ride-hailing with electric vehicles. N D Kullman, M Cousineau, J C Goodson, J E Mendoza, Transportation Science. 563Kullman, N. D., Cousineau, M., Goodson, J. C. & Mendoza, J. E. (2022), 'Dynamic ride-hailing with electric vehicles', Transportation Science 56(3), 775-794.

A contextual-bandit approach to personalized news article recommendation. L Li, W Chu, J Langford, R E Schapire, Proceedings of the 19th international conference on World wide web. the 19th international conference on World wide webLi, L., Chu, W., Langford, J. & Schapire, R. E. (2010), A contextual-bandit approach to personalized news article recommendation, in 'Proceedings of the 19th international conference on World wide web', pp. 661-670.

Efficient ridesharing order dispatching with mean field multi-agent reinforcement learning. M Li, Z Qin, Y Jiao, Y Yang, Z Gong, J Wang, C Wang, G Wu, J Ye, Proceedings of the 2019 World Wide Web Conference on World Wide Web', International World Wide Web Conferences Steering Committee. the 2019 World Wide Web Conference on World Wide Web', International World Wide Web Conferences Steering CommitteeTo appear inLi, M., Qin, Z., Jiao, Y., Yang, Y., Gong, Z., Wang, J., Wang, C., Wu, G. & Ye, J. (2019), Efficient ridesharing order dispatching with mean field multi-agent reinforcement learning, in 'To appear in Proceedings of the 2019 World Wide Web Conference on World Wide Web', International World Wide Web Conferences Steering Committee.

Deep reinforcement learning for the electric vehicle routing problem with time windows. B Lin, B Ghaddar, J Nathwani, IEEE Transactions on Intelligent Transportation Systems. Lin, B., Ghaddar, B. & Nathwani, J. (2021), 'Deep reinforcement learning for the electric vehicle routing problem with time windows', IEEE Transactions on Intelligent Transportation Systems .

Efficient large-scale fleet management via multi-agent deep reinforcement learning. K Lin, R Zhao, Z Xu, J Zhou, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningLin, K., Zhao, R., Xu, Z. & Zhou, J. (2018), Efficient large-scale fleet management via multi-agent deep reinforcement learning, in 'Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining', pp. 1774-1783.

Self-improving reactive agents based on reinforcement learning, planning and teaching. L.-J Lin, Machine learning. 83-4Lin, L.-J. (1992), 'Self-improving reactive agents based on reinforcement learning, planning and teaching', Machine learning 8(3-4), 293-321.

Context-aware taxi dispatching at city-scale using deep reinforcement learning. Z Liu, J Li, K Wu, IEEE Transactions on Intelligent Transportation Systems. Liu, Z., Li, J. & Wu, K. (2020), 'Context-aware taxi dispatching at city-scale using deep reinforcement learning', IEEE Transactions on Intelligent Transportation Systems .

Microscopic traffic simulation using sumo. P A Lopez, M Behrisch, L Bieker-Walz, J Erdmann, Y.-P Flötteröd, R Hilbrich, L Lücken, J Rummel, P Wagner, E Wießner, The 21st IEEE International Conference on Intelligent Transportation Systems. IEEELopez, P. A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Flötteröd, Y.-P., Hilbrich, R., Lücken, L., Rummel, J., Wagner, P. & Wießner, E. (2018), Microscopic traffic simulation using sumo, in 'The 21st IEEE International Conference on Intelligent Transportation Systems', IEEE.

Online spatio-temporal matching in stochastic and dynamic domains. M Lowalekar, P Varakantham, P Jaillet, Artificial Intelligence. 261Lowalekar, M., Varakantham, P. & Jaillet, P. (2018), 'Online spatio-temporal matching in stochastic and dynamic domains', Artificial Intelligence 261, 71-112.

Multi-agent actor-critic for mixed cooperative-competitive environments', Advances in neural information processing systems 30. R Lowe, Y I Wu, A Tamar, J Harb, O Pieter Abbeel, I Mordatch, Lowe, R., Wu, Y. I., Tamar, A., Harb, J., Pieter Abbeel, O. & Mordatch, I. (2017), 'Multi-agent actor-critic for mixed cooperative-competitive environments', Advances in neural information processing systems 30.

Multi-objective online ride-matching. G Lyu, W C Cheung, C.-P Teo, H Wang, Available at SSRN 3356823Lyu, G., Cheung, W. C., Teo, C.-P. & Wang, H. (2019), 'Multi-objective online ride-matching', Available at SSRN 3356823 .

Spatio-temporal pricing for ridesharing platforms. H Ma, F Fang, D C Parkes, ACM SIGecom Exchanges. 182Ma, H., Fang, F. & Parkes, D. C. (2020), 'Spatio-temporal pricing for ridesharing platforms', ACM SIGecom Exchanges 18(2), 53-57.

Optimal route based on dynamic programming for road networks. M K Mainali, K Shimada, S Mabu, K Hirasawa, Journal of Advanced Computational Intelligence and Intelligent Informatics. 126Mainali, M. K., Shimada, K., Mabu, S. & Hirasawa, K. (2008), 'Optimal route based on dynamic programming for road networks', Journal of Advanced Computational Intelligence and Intelligent Informatics 12(6), 546-553.

Dispatch of autonomous vehicles for taxi services: A deep reinforcement learning approach. C Mao, Y Liu, Z.-J M Shen, Transportation Research Part C: Emerging Technologies. 115102626Mao, C., Liu, Y. & Shen, Z.-J. M. (2020), 'Dispatch of autonomous vehicles for taxi services: A deep reinforcement learning approach', Transportation Research Part C: Emerging Technologies 115, 102626.

A reinforcement learning framework for the adaptive routing problem in stochastic time-dependent network. C Mao, Z Shen, Transportation Research Part C: Emerging Technologies. 93Mao, C. & Shen, Z. (2018), 'A reinforcement learning framework for the adaptive routing problem in stochastic time-dependent network', Transportation Research Part C: Emerging Technologies 93, 179-197.

Ride Sharing Market by Type (E-hailing, Station-Based, Car Sharing & Rental), Car Sharing (P2P, Corporate), Service (Navigation, Payment, Information), Micro-Mobility (Bicycle, Scooter), Vehicle Type, and Region -Global Forecast to. Marketsandmarkets, MarketsAndMarkets (2018), Ride Sharing Market by Type (E-hailing, Station-Based, Car Sharing & Rental), Car Sharing (P2P, Corporate), Service (Navigation, Payment, Information), Micro- Mobility (Bicycle, Scooter), Vehicle Type, and Region -Global Forecast to 2025.

Gradient-based inverse risk-sensitive reinforcement learning. E Mazumdar, L J Ratliff, T Fiez, S S Sastry, 2017 IEEE 56th Annual Conference on Decision and Control (CDC)', IEEE. Mazumdar, E., Ratliff, L. J., Fiez, T. & Sastry, S. S. (2017), Gradient-based inverse risk-sensitive reinforcement learning, in '2017 IEEE 56th Annual Conference on Decision and Control (CDC)', IEEE, pp. 5796-5801.

Curriculum in gradient-based meta-reinforcement learning. B Mehta, T Deleu, S C Raparthy, C J Pal, L Paull, arXiv:2002.07956arXiv preprintMehta, B., Deleu, T., Raparthy, S. C., Pal, C. J. & Paull, L. (2020), 'Curriculum in gradient-based meta-reinforcement learning', arXiv preprint arXiv:2002.07956 .

Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, 'International conference on machine learning. Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D. & Kavukcuoglu, K. (2016), Asynchronous methods for deep reinforcement learning, in 'International conference on machine learning', pp. 1928-1937.

Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 5187540Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G. et al. (2015), 'Human-level control through deep reinforcement learning', Nature 518(7540), 529-533.

Reinforcement learning for solving the vehicle routing problem. M Nazari, A Oroojlooy, L Snyder, M Takác, Advances in Neural Information Processing Systems. Nazari, M., Oroojlooy, A., Snyder, L. & Takác, M. (2018), Reinforcement learning for solving the vehicle routing problem, in 'Advances in Neural Information Processing Systems', pp. 9839-9849.

Algorithms for inverse reinforcement learning. A Y Ng, S J Russell, 12Ng, A. Y., Russell, S. J. et al. (2000), Algorithms for inverse reinforcement learning., in 'Icml', Vol. 1, p. 2.

Movi: A model-free approach to dynamic fleet management. T Oda, C Joe-Wong, 'IEEE INFOCOM 2018-IEEE Conference on Computer Communications. IEEEOda, T. & Joe-Wong, C. (2018), Movi: A model-free approach to dynamic fleet management, in 'IEEE INFOCOM 2018-IEEE Conference on Computer Communications', IEEE, pp. 2708-2716.

Dynamic matching for real-time ride sharing. E Özkan, A R Ward, Stochastic Systems. 101Özkan, E. & Ward, A. R. (2020), 'Dynamic matching for real-time ride sharing', Stochastic Systems 10(1), 29-70.

. M Piorkowski, N Sarafijanovic-Djukic, M Grossglauser, CRAWDAD dataset epfl/mobility (v. 2009-02-24Piorkowski, M., Sarafijanovic-Djukic, N. & Grossglauser, M. (2009), 'CRAWDAD dataset epfl/mobility (v. 2009-02-24)', Downloaded from https://crawdad.org/epfl/mobility/ 20090224.

Chicago transportation network providers (rideshare) data'. C D Portal, Portal, C. D. (2020), 'Chicago transportation network providers (rideshare) data'. "https://data. cityofchicago.org/Transportation/Transportation-Network-Providers-Trips/ m6dm-c72p".

Approximate Dynamic Programming: Solving the curses of dimensionality. W B Powell, John Wiley & Sons703Powell, W. B. (2007), Approximate Dynamic Programming: Solving the curses of dimensionality, Vol. 703, John Wiley & Sons.

Optimizing matching time intervals for ride-hailing services using reinforcement learning. G Qin, Q Luo, Y Yin, J Sun, J Ye, Transportation Research Part C: Emerging Technologies. 129103239Qin, G., Luo, Q., Yin, Y., Sun, J. & Ye, J. (2021), 'Optimizing matching time intervals for ride-hailing services using reinforcement learning', Transportation Research Part C: Emerging Technologies 129, 103239.

Ride-hailing order dispatching at didi via reinforcement learning. Z Qin, X Tang, Y Jiao, F Zhang, Z Xu, H Zhu, J Ye, INFORMS Journal on Applied Analytics. 505Qin, Z., Tang, X., Jiao, Y., Zhang, F., Xu, Z., Zhu, H. & Ye, J. (2020), 'Ride-hailing order dispatching at didi via reinforcement learning', INFORMS Journal on Applied Analytics 50(5), 272-286.

Reinforcement learning for ridesharing: A survey. Z Qin, H Zhu, J Ye, 'appearing in Proceedings of the IEEE Intelligent Transportation Systems Conference. Qin, Z., Zhu, H. & Ye, J. (2021), Reinforcement learning for ridesharing: A survey, in 'appearing in Proceedings of the IEEE Intelligent Transportation Systems Conference'.

Reinforcement learning applications in dynamic pricing of retail markets. C Raju, Y Narahari, K Ravikumar, IEEE International Conference on E-Commerce. IEEERaju, C., Narahari, Y. & Ravikumar, K. (2003), Reinforcement learning applications in dynamic pricing of retail markets, in 'IEEE International Conference on E-Commerce, 2003. CEC 2003.', IEEE, pp. 339-346.

Analysing the impact of travel information for minimising the regret of route choice. G D O Ramos, A L Bazzan, B C Da Silva, Transportation Research Part C: Emerging Technologies. 88Ramos, G. d. O., Bazzan, A. L. & da Silva, B. C. (2018), 'Analysing the impact of travel information for minimising the regret of route choice', Transportation Research Part C: Emerging Technologies 88, 257-271.

The rich and the poor: A markov decision process approach to optimizing taxi driver revenue efficiency. H Rong, X Zhou, C Yang, Z Shafiq, A Liu, Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. the 25th ACM International on Conference on Information and Knowledge ManagementRong, H., Zhou, X., Yang, C., Shafiq, Z. & Liu, A. (2016), The rich and the poor: A markov decision process approach to optimizing taxi driver revenue efficiency, in 'Proceedings of the 25th ACM International on Conference on Information and Knowledge Management', pp. 2329-2334.

Amodeus, a simulation-based testbed for autonomous mobility-on-demand systems. C Ruch, S Hörl, E Frazzoli, '2018 21st International Conference on Intelligent Transportation Systems (ITSC)', IEEE. Ruch, C., Hörl, S. & Frazzoli, E. (2018), Amodeus, a simulation-based testbed for autonomous mobility-on-demand systems, in '2018 21st International Conference on Intelligent Transportation Systems (ITSC)', IEEE, pp. 3639-3644.

Semi-markov reinforcement learning for stochastic resource collection. S Schmoll, M Schubert, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI). the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI)Schmoll, S. & Schubert, M. (2020), 'Semi-markov reinforcement learning for stochastic resource col- lection', Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI) .

J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintSchulman, J., Wolski, F., Dhariwal, P., Radford, A. & Klimov, O. (2017), 'Proximal policy optimiza- tion algorithms', arXiv preprint arXiv:1707.06347 .

Neural approximate dynamic programming for on-demand ride-pooling. S Shah, M Lowalekar, P Varakantham, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Shah, S., Lowalekar, M. & Varakantham, P. (2020), Neural approximate dynamic programming for on-demand ride-pooling, in 'Proceedings of the AAAI Conference on Artificial Intelligence', Vol. 34, pp. 507-515.

Partially observable environment estimation with uplift inference for reinforcement learning based recommendation. W Shang, Q Li, Z Qin, Y Yu, Y Meng, J Ye, Machine Learning. Shang, W., Li, Q., Qin, Z., Yu, Y., Meng, Y. & Ye, J. (2021), 'Partially observable environment estimation with uplift inference for reinforcement learning based recommendation', Machine Learning pp. 1-38.

Environment reconstruction with hidden confounders for reinforcement learning based recommendation. W Shang, Y Yu, Q Li, Z Qin, Y Meng, J Ye, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningShang, W., Yu, Y., Li, Q., Qin, Z., Meng, Y. & Ye, J. (2019), Environment reconstruction with hidden confounders for reinforcement learning based recommendation, in 'Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining', pp. 566-576.

Auxiliary-task based deep reinforcement learning for participant selection problem in mobile crowdsourcing. W Shen, X He, C Zhang, Q Ni, W Dou, Y Wang, Proceedings of the 29th ACM International Conference on Information & Knowledge Management. the 29th ACM International Conference on Information & Knowledge ManagementShen, W., He, X., Zhang, C., Ni, Q., Dou, W. & Wang, Y. (2020), Auxiliary-task based deep reinforcement learning for participant selection problem in mobile crowdsourcing, in 'Proceedings of the 29th ACM International Conference on Information & Knowledge Management', pp. 1355- 1364.

Operating electric vehicle fleet for ride-hailing services with reinforcement learning. J Shi, Y Gao, W Wang, N Yu, P A Ioannou, IEEE Transactions on Intelligent Transportation Systems. 2111Shi, J., Gao, Y., Wang, W., Yu, N. & Ioannou, P. A. (2019), 'Operating electric vehicle fleet for ride-hailing services with reinforcement learning', IEEE Transactions on Intelligent Transportation Systems 21(11), 4822-4834.

Multi-agent reinforcement learning for dynamic routing games: A unified paradigm. Z Shou, X Di, arXiv:2011.10915arXiv preprintShou, Z. & Di, X. (2020a), 'Multi-agent reinforcement learning for dynamic routing games: A unified paradigm', arXiv preprint arXiv:2011.10915 .

Reward design for driver repositioning using multi-agent reinforcement learning', Transportation research part C: emerging technologies 119. Z Shou, X Di, Shou, Z. & Di, X. (2020b), 'Reward design for driver repositioning using multi-agent reinforcement learning', Transportation research part C: emerging technologies 119, 102738.

Optimal passenger-seeking policies on e-hailing platforms using markov decision process and imitation learning. Z Shou, X Di, J Ye, H Zhu, H Zhang, R Hampshire, Transportation Research Part C: Emerging Technologies. 111Shou, Z., Di, X., Ye, J., Zhu, H., Zhang, H. & Hampshire, R. (2020), 'Optimal passenger-seeking poli- cies on e-hailing platforms using markov decision process and imitation learning', Transportation Research Part C: Emerging Technologies 111, 91-113.

Alphago: Mastering the ancient game of go with machine learning. D Silver, D Hassabis, Research Blog 9Silver, D. & Hassabis, D. (2016), 'Alphago: Mastering the ancient game of go with machine learning', Research Blog 9.

An approximate dynamic programming algorithm for large-scale fleet management: A case application. H P Simao, J Day, A P George, T Gifford, J Nienow, W B Powell, Transportation Science. 432Simao, H. P., Day, J., George, A. P., Gifford, T., Nienow, J. & Powell, W. B. (2009), 'An approx- imate dynamic programming algorithm for large-scale fleet management: A case application', Transportation Science 43(2), 178-197.

A distributed model-free algorithm for multi-hop ride-sharing using deep reinforcement learning. A Singh, A O Al-Abbasi, V Aggarwal, IEEE Transactions on Intelligent Transportation Systems. Singh, A., Al-Abbasi, A. O. & Aggarwal, V. (2021), 'A distributed model-free algorithm for multi-hop ride-sharing using deep reinforcement learning', IEEE Transactions on Intelligent Transportation Systems .

Here?s how long you have to wait for an Uber or Lyft in DC. M Smith, Smith, M. (2019), Here?s how long you have to wait for an Uber or Lyft in DC. https://wtop.com/ dc-transit/2019/12/how-long-you-have-to-wait-for-an-uber-or-lyft-in-d-c/.

An application of reinforced learningbased dynamic pricing for improvement of ridesharing platform service in seoul. J Song, Y J Cho, M H Kang, K Y Hwang, Electronics. 9111818Song, J., Cho, Y. J., Kang, M. H. & Hwang, K. Y. (2020), 'An application of reinforced learning- based dynamic pricing for improvement of ridesharing platform service in seoul', Electronics 9(11), 1818.

Model and analysis of labor supply for ride-sharing platforms in the presence of sample self-selection and endogeneity. H Sun, H Wang, Z Wan, Transportation Research Part B: Methodological. 125Sun, H., Wang, H. & Wan, Z. (2019), 'Model and analysis of labor supply for ride-sharing plat- forms in the presence of sample self-selection and endogeneity', Transportation Research Part B: Methodological 125, 76-93.

Learning to predict by the methods of temporal differences. R S Sutton, Machine learning. 31Sutton, R. S. (1988), 'Learning to predict by the methods of temporal differences', Machine learning 3(1), 9-44.

Reinforcement learning: An introduction. R S Sutton, A G Barto, MIT pressSutton, R. S. & Barto, A. G. (2018), Reinforcement learning: An introduction, MIT press.

A deep value-network based approach for multi-driver order dispatching. X Tang, Z Qin, F Zhang, Z Wang, Z Xu, Y Ma, H Zhu, J Ye, Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining. the 25th ACM SIGKDD international conference on knowledge discovery & data miningTang, X., Qin, Z., Zhang, F., Wang, Z., Xu, Z., Ma, Y., Zhu, H. & Ye, J. (2019), A deep value-network based approach for multi-driver order dispatching, in 'Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining', pp. 1780-1790.

Value function is all you need: A unified learning framework for ride hailing platforms. X Tang, F Zhang, Z Qin, Y Wang, D Shi, B Song, Y Tong, H Zhu, J Ye, Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining', KDD '21. the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining', KDD '21New York, NY, USAAssociation for Computing MachineryTang, X., Zhang, F., Qin, Z., Wang, Y., Shi, D., Song, B., Tong, Y., Zhu, H. & Ye, J. (2021), Value function is all you need: A unified learning framework for ride hailing platforms, in 'Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining', KDD '21, Association for Computing Machinery, New York, NY, USA, p. 3605-3615.

Nyc taxi & limousine commission trip record data. TLC. TLC (2020), 'Nyc taxi & limousine commission trip record data'. "https://www1.nyc.gov/site/ tlc/about/tlc-trip-record-data.page".

Combinatorial optimization meets reinforcement learning: Effective taxi order dispatching at large-scale. Y Tong, D Shi, Y Xu, W Lv, Z Qin, X Tang, IEEE Transactions on Knowledge and Data Engineering. Tong, Y., Shi, D., Xu, Y., Lv, W., Qin, Z. & Tang, X. (2021), 'Combinatorial optimization meets reinforcement learning: Effective taxi order dispatching at large-scale', IEEE Transactions on Knowledge and Data Engineering .

A unified approach to route planning for shared mobility. Y Tong, Y Zeng, Z Zhou, L Chen, J Ye, K Xu, Proceedings of the VLDB Endowment. 11111633Tong, Y., Zeng, Y., Zhou, Z., Chen, L., Ye, J. & Xu, K. (2018), 'A unified approach to route planning for shared mobility', Proceedings of the VLDB Endowment 11(11), 1633.

Spatial crowdsourcing: a survey. Y Tong, Z Zhou, Y Zeng, L Chen, C Shahabi, The VLDB Journal. 291Tong, Y., Zhou, Z., Zeng, Y., Chen, L. & Shahabi, C. (2020), 'Spatial crowdsourcing: a survey', The VLDB Journal 29(1), 217-250.

Continual reinforcement learning deployed in real-life using policy distillation and sim2real transfer. R Traoré, H Caselles-Dupré, T Lesort, T Sun, N Díaz-Rodríguez, D Filliat, arXiv:1906.04452arXiv preprintTraoré, R., Caselles-Dupré, H., Lesort, T., Sun, T., Díaz-Rodríguez, N. & Filliat, D. (2019), 'Continual reinforcement learning deployed in real-life using policy distillation and sim2real transfer', arXiv preprint arXiv:1906.04452 .

Aligning social welfare and agent preferences to alleviate traffic congestion. K Tumer, Z T Welch, A Agogino, Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems. the 7th international joint conference on Autonomous agents and multiagent systemsCiteseer2Tumer, K., Welch, Z. T. & Agogino, A. (2008), Aligning social welfare and agent preferences to alleviate traffic congestion, in 'Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems-Volume 2', Citeseer, pp. 655-662.

Dynamic pricing and fleet management for electric autonomous mobility on demand systems. B Turan, R Pedarsani, M Alizadeh, Transportation Research Part C: Emerging Technologies. 121102829Turan, B., Pedarsani, R. & Alizadeh, M. (2020), 'Dynamic pricing and fleet management for electric autonomous mobility on demand systems', Transportation Research Part C: Emerging Technologies 121, 102829.

Uber movement. Uber, Uber (2021), 'Uber movement'. "https://movement.uber.com/?lang=en-US".

On modeling stochastic dynamic vehicle routing problems. M W Ulmer, J C Goodson, D C Mattfeld, B W Thomas, EURO Journal on Transportation and Logistics. 92100008Ulmer, M. W., Goodson, J. C., Mattfeld, D. C. & Thomas, B. W. (2020), 'On modeling stochastic dynamic vehicle routing problems', EURO Journal on Transportation and Logistics 9(2), 100008.

Learning ride-sourcing drivers' customer-searching behavior: A dynamic discrete choice approach. J Urata, Z Xu, J Ke, Y Yin, G Wu, H Yang, J Ye, Transportation Research Part C: Emerging Technologies. 130103293Urata, J., Xu, Z., Ke, J., Yin, Y., Wu, G., Yang, H. & Ye, J. (2021), 'Learning ride-sourcing drivers' customer-searching behavior: A dynamic discrete choice approach', Transportation Research Part C: Emerging Technologies 130, 103293.

Augmenting decisions of taxi drivers through reinforcement learning for improving revenues, in 'Twenty-Seventh International Confer. T Verma, P Varakantham, S Kraus, H C Lau, Verma, T., Varakantham, P., Kraus, S. & Lau, H. C. (2017), Augmenting decisions of taxi drivers through reinforcement learning for improving revenues, in 'Twenty-Seventh International Confer- ence on Automated Planning and Scheduling'.

O Vinyals, M Fortunato, N Jaitly, Pointer networks, in 'Advances in Neural Information Processing Systems. Vinyals, O., Fortunato, M. & Jaitly, N. (2015), Pointer networks, in 'Advances in Neural Information Processing Systems', pp. 2692-2700.

The multi-agent transport simulation MATSim. K W Axhausen, A Horni, K Nagel, Ubiquity PressW Axhausen, K., Horni, A. & Nagel, K. (2016), The multi-agent transport simulation MATSim, Ubiquity Press.

Ridesourcing systems: A framework and review. H Wang, H Yang, Transportation Research Part B: Methodological. 129Wang, H. & Yang, H. (2019), 'Ridesourcing systems: A framework and review', Transportation Research Part B: Methodological 129, 122-155.

Adaptive dynamic bipartite graph matching: A reinforcement learning approach. Y Wang, Y Tong, C Long, P Xu, K Xu, W Lv, 2019 IEEE 35th International Conference on Data Engineering (ICDE)', IEEE. Wang, Y., Tong, Y., Long, C., Xu, P., Xu, K. & Lv, W. (2019), Adaptive dynamic bipartite graph matching: A reinforcement learning approach, in '2019 IEEE 35th International Conference on Data Engineering (ICDE)', IEEE, pp. 1478-1489.

Sample efficient actor-critic with experience replay. Z Wang, V Bapst, N Heess, V Mnih, R Munos, K Kavukcuoglu, N De Freitas, arXiv:1611.01224arXiv preprintWang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K. & de Freitas, N. (2016), 'Sample efficient actor-critic with experience replay', arXiv preprint arXiv:1611.01224 .

Deep reinforcement learning with knowledge transfer for online rides order dispatching. Z Wang, Z Qin, X Tang, J Ye, H Zhu, 'International Conference on Data Mining. IEEEWang, Z., Qin, Z., Tang, X., Ye, J. & Zhu, H. (2018), Deep reinforcement learning with knowledge transfer for online rides order dispatching, in 'International Conference on Data Mining', IEEE.

Q-learning. C J Watkins, P Dayan, Machine learning. 83-4Watkins, C. J. & Dayan, P. (1992), 'Q-learning', Machine learning 8(3-4), 279-292.

Hierarchical sarsa learning based route guidance algorithm. F Wen, X Wang, X Xu, Journal of Advanced Transportation. Wen, F., Wang, X. & Xu, X. (2019), 'Hierarchical sarsa learning based route guidance algorithm', Journal of Advanced Transportation 2019.

Rebalancing shared mobility-on-demand systems: A reinforcement learning approach. J Wen, J Zhao, P Jaillet, 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)', Ieee. Wen, J., Zhao, J. & Jaillet, P. (2017), Rebalancing shared mobility-on-demand systems: A reinforce- ment learning approach, in '2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)', Ieee, pp. 220-225.

Simple statistical gradient-following algorithms for connectionist reinforcement learning. R J Williams, Machine learning. 83-4Williams, R. J. (1992), 'Simple statistical gradient-following algorithms for connectionist reinforce- ment learning', Machine learning 8(3-4), 229-256.

A cell-based logit-opportunity taxi customer-search model. R Wong, W Szeto, S Wong, Transportation Research Part C: Emerging Technologies. 48Wong, R., Szeto, W. & Wong, S. (2014), 'A cell-based logit-opportunity taxi customer-search model', Transportation Research Part C: Emerging Technologies 48, 84-96.

Flow: Architecture and benchmarking for reinforcement learning in traffic control. C Wu, A Kreidieh, K Parvate, E Vinitsky, A M Bayen, arXiv:1710.0546510arXiv preprintWu, C., Kreidieh, A., Parvate, K., Vinitsky, E. & Bayen, A. M. (2017), 'Flow: Architecture and benchmarking for reinforcement learning in traffic control', arXiv preprint arXiv:1710.05465 p. 10.

Automated pricing agents in the on-demand economy. T Wu, A D Joseph, S J Russell, Berkeley, CA, USAUniversity of California at BerkeleyWu, T., Joseph, A. D. & Russell, S. J. (2016), 'Automated pricing agents in the on-demand economy', University of California at Berkeley: Berkeley, CA, USA .

An efficient insertion operator in dynamic ridesharing services. Y Xu, Y Tong, Y Shi, Q Tao, K Xu, W Li, IEEE Transactions on Knowledge and Data Engineering. Xu, Y., Tong, Y., Shi, Y., Tao, Q., Xu, K. & Li, W. (2020), 'An efficient insertion operator in dynamic ridesharing services', IEEE Transactions on Knowledge and Data Engineering .

Largescale order dispatch in on-demand ride-hailing platforms: A learning and planning approach. Z Xu, Z Li, Q Guan, D Zhang, Q Li, J Nan, C Liu, W Bian, J Ye, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningACMXu, Z., Li, Z., Guan, Q., Zhang, D., Li, Q., Nan, J., Liu, C., Bian, W. & Ye, J. (2018), Large- scale order dispatch in on-demand ride-hailing platforms: A learning and planning approach, in 'Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining', ACM, pp. 905-913.

Dynamic pricing and matching in ride-hailing platforms. C Yan, H Zhu, N Korolko, D Woodard, Naval Research Logistics (NRL). 678Yan, C., Zhu, H., Korolko, N. & Woodard, D. (2020), 'Dynamic pricing and matching in ride-hailing platforms', Naval Research Logistics (NRL) 67(8), 705-724.

Optimizing matching time interval and matching radius in on-demand ride-sourcing markets. H Yang, X Qin, J Ke, J Ye, Transportation Research Part B: Methodological. 131Yang, H., Qin, X., Ke, J. & Ye, J. (2020), 'Optimizing matching time interval and matching radius in on-demand ride-sourcing markets', Transportation Research Part B: Methodological 131, 84-105.

Integrated reward scheme and surge pricing in a ridesourcing market. H Yang, C Shao, H Wang, J Ye, Transportation Research Part B: Methodological. 134Yang, H., Shao, C., Wang, H. & Ye, J. (2020), 'Integrated reward scheme and surge pricing in a ridesourcing market', Transportation Research Part B: Methodological 134, 126-142.

Mean field multi-agent reinforcement learning. Y Yang, R Luo, M Li, M Zhou, W Zhang, J Wang, PMLR'International Conference on Machine Learning. Yang, Y., Luo, R., Li, M., Zhou, M., Zhang, W. & Wang, J. (2018), Mean field multi-agent reinforce- ment learning, in 'International Conference on Machine Learning', PMLR, pp. 5571-5580.

A ridesharing simulation platform that considers dynamic supplydemand interactions. R Yao, S Bekhor, arXiv:2104.13463arXiv preprintYao, R. & Bekhor, S. (2021), 'A ridesharing simulation platform that considers dynamic supply- demand interactions', arXiv preprint arXiv:2104.13463 .

A survey on reinforcement learning models and algorithms for traffic signal control. K.-L A Yau, J Qadir, H L Khoo, M H Ling, P Komisarczuk, ACM Comput. Surv. 503Yau, K.-L. A., Qadir, J., Khoo, H. L., Ling, M. H. & Komisarczuk, P. (2017), 'A survey on reinforce- ment learning models and algorithms for traffic signal control', ACM Comput. Surv. 50(3).

Q value-based dynamic programming with sarsa learning for real time route guidance in large scale road networks. S Yu, J Zhou, B Li, S Mabu, K Hirasawa, The 2012 International Joint Conference on Neural Networks (IJCNN)', IEEE. Yu, S., Zhou, J., Li, B., Mabu, S. & Hirasawa, K. (2012), Q value-based dynamic programming with sarsa learning for real time route guidance in large scale road networks, in 'The 2012 International Joint Conference on Neural Networks (IJCNN)', IEEE, pp. 1-7.

A markov decision process approach to vacant taxi routing with e-hailing. X Yu, S Gao, X Hu, H Park, Transportation Research Part B: Methodological. 121Yu, X., Gao, S., Hu, X. & Park, H. (2019), 'A markov decision process approach to vacant taxi routing with e-hailing', Transportation Research Part B: Methodological 121, 114-134.

An integrated decomposition and approximate dynamic programming approach for on-demand ride pooling. X Yu, S Shen, IEEE Transactions on Intelligent Transportation Systems. 219Yu, X. & Shen, S. (2019), 'An integrated decomposition and approximate dynamic programming approach for on-demand ride pooling', IEEE Transactions on Intelligent Transportation Systems 21(9), 3811-3820.

Beyond shortest paths: Route recommendations for ride-sharing. C F Yuen, A P Singh, S Goyal, S Ranu, A Bagchi, The World Wide Web Conference. Yuen, C. F., Singh, A. P., Goyal, S., Ranu, S. & Bagchi, A. (2019), Beyond shortest paths: Route recommendations for ride-sharing, in 'The World Wide Web Conference', pp. 2258-2269.

Multi-vehicle routing problems with soft time windows: A multi-agent reinforcement learning approach. K Zhang, F He, Z Zhang, X Lin, M Li, Transportation Research Part C: Emerging Technologies. 121102861Zhang, K., He, F., Zhang, Z., Lin, X. & Li, M. (2020), 'Multi-vehicle routing problems with soft time windows: A multi-agent reinforcement learning approach', Transportation Research Part C: Emerging Technologies 121, 102861.

Dynamic fleet management with rewriting deep reinforcement learning. W Zhang, Q Wang, J Li, C Xu, IEEE Access. 8Zhang, W., Wang, Q., Li, J. & Xu, C. (2020), 'Dynamic fleet management with rewriting deep reinforcement learning', IEEE Access 8, 143333-143341.

Order dispatch in price-aware ridesharing. L Zheng, L Chen, J Ye, Proceedings of the VLDB Endowment. 118Zheng, L., Chen, L. & Ye, J. (2018), 'Order dispatch in price-aware ridesharing', Proceedings of the VLDB Endowment 11(8), 853-865.

Queueing versus surge pricing mechanism: Efficiency, equity, and consumer welfare', Equity, and Consumer Welfare. Y Zhong, Z Wan, Z.-J M Shen, Zhong, Y., Wan, Z. & Shen, Z.-J. M. (2020), 'Queueing versus surge pricing mechanism: Efficiency, equity, and consumer welfare', Equity, and Consumer Welfare (September 24, 2020) .

A reinforcement learning scheme for the equilibrium of the in-vehicle route choice problem based on congestion game. B Zhou, Q Song, Z Zhao, T Liu, Applied Mathematics and Computation. 371124895Zhou, B., Song, Q., Zhao, Z. & Liu, T. (2020), 'A reinforcement learning scheme for the equilibrium of the in-vehicle route choice problem based on congestion game', Applied Mathematics and Computation 371, 124895.

Multi-objective distributional reinforcement learning for large-scale order dispatching. F Zhou, C Lu, X Tang, F Zhang, Z Qin, J Ye, H Zhu, '2021 IEEE International Conference on Data Mining (ICDM)', IEEE. Zhou, F., Lu, C., Tang, X., Zhang, F., Qin, Z., Ye, J. & Zhu, H. (2021), Multi-objective distributional reinforcement learning for large-scale order dispatching, in '2021 IEEE International Conference on Data Mining (ICDM)', IEEE, pp. 1541-1546.

Multi-agent reinforcement learning for order-dispatching via order-vehicle distribution matching. M Zhou, J Jin, W Zhang, Z Qin, Y Jiao, C Wang, G Wu, Y Yu, J Ye, Proceedings of the 28th ACM International Conference on Information and Knowledge Management. the 28th ACM International Conference on Information and Knowledge ManagementZhou, M., Jin, J., Zhang, W., Qin, Z., Jiao, Y., Wang, C., Wu, G., Yu, Y. & Ye, J. (2019), Multi-agent reinforcement learning for order-dispatching via order-vehicle distribution matching, in 'Proceed- ings of the 28th ACM International Conference on Information and Knowledge Management', pp. 2645-2653.

Smarts: Scalable multi-agent reinforcement learning training school for autonomous driving. M Zhou, J Luo, J Villela, Y Yang, D Rusu, J Miao, W Zhang, M Alban, I Fadakar, Z Chen, arXiv:2010.09776arXiv preprintZhou, M., Luo, J., Villela, J., Yang, Y., Rusu, D., Miao, J., Zhang, W., Alban, M., Fadakar, I., Chen, Z. et al. (2020), 'Smarts: Scalable multi-agent reinforcement learning training school for autonomous driving', arXiv preprint arXiv:2010.09776 .

Optimizing taxi driver profit efficiency: A spatial network-based markov decision process approach. X Zhou, H Rong, C Yang, Q Zhang, A V Khezerlou, H Zheng, Z Shafiq, A X Liu, IEEE Transactions on Big Data. 61Zhou, X., Rong, H., Yang, C., Zhang, Q., Khezerlou, A. V., Zheng, H., Shafiq, Z. & Liu, A. X. (2018), 'Optimizing taxi driver profit efficiency: A spatial network-based markov decision process approach', IEEE Transactions on Big Data 6(1), 145-158.

Competition and third-party platformintegration in ride-sourcing markets. Y Zhou, H Yang, J Ke, H Wang, X Li, Transportation Research Part B: Methodological. 159Zhou, Y., Yang, H., Ke, J., Wang, H. & Li, X. (2022), 'Competition and third-party platform- integration in ride-sourcing markets', Transportation Research Part B: Methodological 159, 76- 103.

A mean-field markov decision process model for spatialtemporal subsidies in ride-sourcing markets. Z Zhu, J Ke, H Wang, Transportation Research Part B: Methodological. 150Zhu, Z., Ke, J. & Wang, H. (2021), 'A mean-field markov decision process model for spatial- temporal subsidies in ride-sourcing markets', Transportation Research Part B: Methodological 150, 540-565.

Dynamic multiobjective optimization driven by inverse reinforcement learning. F Zou, G G Yen, C Zhao, Information Sciences. 575Zou, F., Yen, G. G. & Zhao, C. (2021), 'Dynamic multiobjective optimization driven by inverse reinforcement learning', Information Sciences 575, 468-484.