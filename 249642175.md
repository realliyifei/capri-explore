# Multimodal Learning with Transformers: A Survey

CorpusID: 249642175
 
tags: #Medicine, #Computer_Science

URL: [https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7](https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

Multimodal Learning with Transformers: A Survey


Peng Xu 
Xiatian Zhu 
David A Clifton 
Multimodal Learning with Transformers: A Survey
1Index Terms-Multimodal LearningTransformerIntroductoryTaxonomyDeep LearningMachine Learning
Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and big data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal big data era, (2) a systematic review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community.

# INTRODUCTION

The initial inspiration of Artificial Intelligence (AI) is to imitate human perception, e.g., seeing, hearing, touching, smelling. In general, a modality is often associated with a specific sensor that creates a unique communication channel, such as vision and language [1]. In humans, a fundamental mechanism in our sensory perception is the ability to leverage multiple modalities of perception data collectively in order to engage ourselves properly with the world under dynamic unconstrained circumstances, with each modality serving as a distinct information source characterized by different statistical properties. For example, an image gives the visual appearance of an "elephants playing in water" scene via thousands of pixels, whilst the corresponding text describes this moment with a sentence using discrete words. Fundamentally, a multimodal AI system needs to ingest, interpret, and reason about multimodal information sources to realize similar human level perception abilities. Multimodal learning (MML) is a general approach to building AI models that can extract and relate information from multimodal data [1].

This survey focuses on multimodal learning with Transformers [2] (as demonstrated in Figure 1), inspired by their intrinsic advantages and scalability in modelling different modalities (e.g., language, visual, auditory) and tasks (e.g., language translation, image recognition, speech recognition) with fewer modality-specific architectural assumptions (e.g., translation invariance and local grid attention bias in vision) [3]. Concretely, the input to a Transformer could encompass one or multiple sequences of tokens, and each sequence's attribute (e.g., the modality label, the sequential order), naturally allowing for MML without architectural modification [4]. Further, learning per-modal specificity and inter-modal • This paper is accepted by IEEE TPAMI. • Peng Xu is with Tsinghua University. Xiatian Zhu is with the University of Surrey. David A. Clifton is with the University of Oxford, UK, and also with Oxford Suzhou Centre for Advanced Research, Suzhou, PRC. • Corresponding author: David A. Clifton correlation can be simply realized by controlling the input pattern of self-attention. Critically, there is a recent surge of research attempts and activities across distinct disciplines exploring the Transformer architectures, resulting in a large number of novel MML methods being developed in recent years, along with significant and diverse advances in various areas [4], [5], [6], [7], [8]. This calls for a timely review and summary of representative methods to enable researchers to understand the global picture of the MML field across related disciplines and more importantly to capture a holistic structured picture of current achievements as well as major challenges.

Taxonomy For better readability and reachability from and across different disciplines, we adopt a two-tier structured taxonomy based on the application and challenge dimensions respectively. This has several benefits: (1) Researchers with expertise in specific applications can find those applications appropriate to their own research domain before connecting to other related domains. (2) Similar model designs and architectures developed in different domains can be summarized in an abstract, formula-driven perspective so that the mathematical ideas of various models formed in different applications can be correlated and contrasted on common ground, crossing domain-specific restrictions. Crucially, our taxonomy offers an interesting stereo-view of individual works with the insights in both application specificity and formulation generality. It is hoped that this can help to break down domain boundaries and foster more effective idea communication and exchange across modalities. By using the prompt modelling strategy [9], [10] as a basis for investigation, we also include the classical classification problem (e.g., image classification) -usually regarded as a single modality learning application in conventional MML surveys [1], [11], [12] -as a special MML application. This has the potential to significantly enrich MML, as the classification problem is an AI topic amongst the most extensive studies in the literature [13]. Scope This survey will discuss the multimodality specific designs of Transformer architecture including, but not limited to, the following modalities: RGB image [5], depth image [14], multispectral image [15], video [7], audio/speech/music [14], [16], [17], table [18], scene graph/layout [19], [20], [21], [22], pose skeleton [23], SQL [24], [25], recipe [26], programming language [27], sign language [28], [29], [30], point cloud [31], symbolic knowledge (graph) [32], [33], multimodal knowledge graph [34], sketch drawing [35], [36], [37], [38], 3D object/scene [39], [40], [41], document [42], [43], programming code [44] and Abstract Syntax Tree (AST) -a kind of graph [45], optical flow [46], medical knowledge (e.g., diagnosis code ontology [47]). Note that this survey will not discuss the multimodal papers where Transformer is used simply as the feature extractor without multimodal designs.


## Related Surveys

We relate this paper to existing surveys of the two specific dimensions MML and Transformers. There exist a few MML surveys [1], [11], [12]. In particular, [1] proposed a structured, acknowledged taxonomy by five challenges, which we also adopt as part of our structure. Unlike [1], [11], and [12], which review general machine learning models, we instead focus on Transformer architectures and their self-attention mechanisms. Several surveys dedicated to Transformers have been recently introduced, with a range of emphases including general Transformers [48], efficient designs [49], visualization [50], computer vision tasks [51], [52], [53], [54], medical imaging [55], video tasks [56], and vision language pretraining [57]. While [51], [53], [54], [55] consider MML, their reviews are somewhat limited in the scope, taxonomy, and coverage. To our knowledge, only a few surveys on video-language pretraining (VLP) [57], [58], [59] are relevant to MML. However, VLP is only a subdomain of MML. In this survey, we focus solely on the intersection of multimodal learning and Transformers.

Features To our knowledge, this paper is the first comprehensive review of the state of Transformer based multimodal machine learning. The major features of this survey include (1) We highlight that Transformers have the advantage that they can work in a modality-agnostic way. Thus, they are compatible with various modalities (and combinations of modalities). To support this view, we, for the first time, offer an understanding of the intrinsic traits of Transformers in a multimodal context from a geometrically topological perspective. We suggest that self-attention be treated as a graph style modelling, which models the input sequence (both uni-modal and multimodal) as a fully-connected graph. Specifically, self-attention models the embedding of arbitrary tokens from an arbitrary modality as a graph node.

(2) We discuss the key components of Transformers in a multimodal context as mathematically as possible.

(3) Based on Transformers, cross-modal interactions (e.g., fusion, alignment) are essentially processed by self-attention and its variants. In this paper, we extract the mathematical essence and formulations of Transformer based MML practices, from the perspective of self-attention designs.

Contributions Having presented our review of the landscape of multimodal learning, Transformer ecosystem, and multimodal big data era in Section 2, we summarize our main contributions as the follows.

(1) In Section 3, we present a systematic reviewing of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective.

(2) We contribute a taxonomy for Transformer based MML from two complementary perspectives, i.e., application based and challenge based. In Section 4, we provide a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks. In Section 5, we summarize the common challenges and designs shared by the various multimodal Transformer models and applications.

(3) In Section 6, we discuss current bottlenecks, existing problems, and potential research directions for Transformer based MML.


# BACKGROUND


## Multimodal Learning (MML)

MML [1], [60], [61] has been an important research area in recent decades; an early multimodal application -audiovisual speech recognition was studied in 1980s [62]. MML is key to human societies. The world we humans live in is a multimodal environment, thus both our observations and behaviours are multimodal [63]. For instance, an AI navigation robot needs multimodal sensors to perceive the real-world environment [64], [65], [66], e.g., camera, LiDAR, radar, ultrasonic, GNSS, HD Map, odometer. Furthermore, human behaviours, emotions, events, actions, and humour are multimodal, thus various human-centred MML tasks are widely studied, including multimodal emotion recognition [67], multimodal event representation [68], understanding multimodal humor [69], face-body-voice based video person-clustering [70], etc.

Thanks to the development of the internet and a wide variety of intelligent devices in recent years, increasing amounts of multimodal data are being transmitted over the internet, thus an increasing number of multimodal application scenarios are emerging. In modern life, we can see various multimodal applications, including commercial services (e.g., e-commerce/commodity retrieval [71], visionand-language navigation (VLN) [72], [73], [74], [75], [76]), communication (e.g., lip reading [77], sign language translation [28], [29]), human-computer interaction [78], healthcare AI [79], [80], surveillance AI [81], etc.

Moreover, in the era of Deep Learning, deep neural networks greatly promote the development of MML, and Transformers [2] are a highly competitive architecture family, bringing new challenges and opportunities to MML. In particular, the recent success of large language models and their multimodal derivatives [82], [83], [84], [85], [86] further demonstrates the potential of Transformers in multimodal foundation models.


## Transformers: a Brief History and Milestones

Transformers are emerging as promising learners. Vanilla Transformer [2] benefits from a self-attention mechanism, and is a breakthrough model for sequence-specific representation learning that was originally proposed for NLP, achieving the state-of-the-art on various NLP tasks. Following the great success of Vanilla Transformer, a lot of derivative models have been proposed, e.g., BERT [4], BART [87], GPT [88], Longformer [43], Transformer-XL [89], XLNet [90].

Transformers currently stand at the dominant position in NLP domains, and this motivates researchers try to apply Transformers to other modalities, such as visual domains. In early attempts for visual domain, the general pipeline is "CNN features + standard Transformer encoder", and researchers achieved BERT-style pretraining, via preprocessing raw images by resizing to a low resolution and reshaping into a 1D sequence [91].

Vision Transformer (ViT) [5] is a seminal work that contributes an end-to-end solution by applying the encoder of Transformer to images. Both ViT and its variants have been widely applied to various computer vision tasks, including low-level tasks [92], recognition [93], detection [94], segmentation [95], etc, and also work well for both supervised [93] and self-supervised [96], [97], [98] visual learning. Moreover, some recently-released works provide further theoretical understanding for ViT, e.g., its internal representation robustness [99], the continuous behaviour of its latent representation propagation [100], [101]. Motivated by the great success of Transformer, VideoBERT [7] is a breakthrough work that is the first work to extend Transformer to the multimodal tasks. VideoBERT demonstrates the great potential of Transformer in multimodal context. Following VideoBERT, a lot of Transformer based multimodal pretraining models (e.g., ViLBERT [102], LXMERT [103], VisualBERT [104], VL-BERT [105], UNITER [106], CBT [107], Unicoder-VL [108], B2T2 [109], VLP [110], 12-in-1 [111], Oscar [112], Pixel-BERT [113], ActBERT [114], ImageBERT [115], HERO [116], UniVL [117]) have become research topics of increasing interest in the field of machine learning.

In 2021, CLIP [9] was proposed. It is a new milestone that uses multimodal pretraining to convert classification as a retrieval task that enables the pretrained models to tackle zero-shot recognition. Thus, CLIP is a successful practice that makes full use of large-scale multimodal pretraining to enable zero-shot learning. Recently, the idea of CLIP is further studied, e.g., CLIP pretrained model based zero-shot semantic segmentation [118], ALIGN [119], CLIP-TD [120], ALBEF [121], and CoCa [122].


## Multimodal Big Data

In the past decade, with the rapid development of internet applications such as social media and online retail, massive multimodal datasets have been proposed, e.g., Conceptual Captions [123], COCO [124], VQA [125], Visual Genome [126], SBU Captions [127], Cooking312K [7], LAIT [115], e-SNLI-VE [128], ARCH [129], Adversarial VQA [130], OTT-QA [18], MULTIMODALQA (MMQA) [131], VALUE [132], Fashion IQ [133], LRS2-BBC [134], ActivityNet [135], VisDial [136].

Some emergent new trends among the recently released multimodal datasets are:

(1) Data scales are larger. Various recently released datasets are million-scale, e.g., Product1M [137], Conceptual 12M [138], RUC-CAS-WenLan [139] (30M), HowToVQA69M [140], HowTo100M [141], ALT200M [142], LAION-400M [143].

(2) More modalities. In addition to the general modalities of vision, text, and audio, further diverse modalities are emerging, e.g., Pano-AVQA [144] -the first large-scale spatial and audio-visual question answering dataset on 360 • videos, YouTube-360 (YT-360) [145] (360 • videos), AIST++ [146] (a new multimodal dataset of 3D dance motion and music), Artemis [147] (affective language for visual arts). In particular, MultiBench [148] provides a dataset including 10 modalities.

(3) More scenarios. In addition to common caption and QA datasets, more applications and scenarios have been studied, e.g., CIRR [149] (real-life images), Product1M [137], Bed and Breakfast (BnB) [150] (vision-and-language navigation), M3A [151] (financial dataset), X-World [152] (autonomous drive).

(4) Tasks are more difficult. Beyond the straightforward tasks, more abstract multimodal tasks are proposed, e.g., MultiMET [153] (a multimodal dataset for metaphor understanding), Hateful Memes [154] (hate speech in multimodal memes).

(5) Instructional videos have become increasingly popular, e.g., cooking video YouCookII [155]. Aligning a sequence of instructions to a video of someone carrying out a task is an example of a powerful pretraining pretext task [7], [156]. Pretext tasks are pre-designed problems to force the models to learn representation by solving them.

Similar to other deep neural network architectures, Transformers are also data hungry. Therefore, their highcapacity models and multimodal big data basis co-create the prosperity of the Transformer based multimodal machine learning. For instance, big data bring zero-shot learning capability to VLP Transformer models.


# TRANSFORMERS

In this section, we use mathematical formulations to review the key techniques of Vanilla Transformer [2], Vision Transformer [5], and multimodal Transformers 1 , including tokenized inputs, self-attention, multi-head attention, basic Transformer layers/blocks, etc. We highlight that Vanilla Transformers can be understood from a geometrically topological perspective [157], because due to the self-attention mechanism, given each tokenized input from any modalities, Vanilla self-attention (Transformer) can model it as a fully-connected graph in topological geometry space [158]. Compared with other deep networks (for instance, CNN is restricted in the aligned grid spaces/matrices), Transformers intrinsically have a more general and flexible modelling space. This is a notable advantage of Transformers for multimodal tasks. Sections 3.1, 3.2, and 3.3 will review the key designs of Vanilla Transformer, Vision Transformer, and multimodal Transformers, respectively.


## Vanilla Transformer

Vanilla Transformer has an encoder-decoder structure and is the origin of the Transformer-based research field. It takes tokenized input (see Section 3.1.1). Both its encoder and decoder are stacked by the Transformer layers/blocks, as demonstrated in Figure 1. Each block has two sub-layers, i.e., a multi-head self-attention (MHSA) layer (see Section 3.1.2) and a position-wise fully-connected feed-forward network (FFN) (see Section 3.1.3). To help the back propagation of the gradient, both MHSA and FFN use Residual Connection [159] (given an input x, the residual connection of any mapping f (·) is defined as x ← f (x) + x), followed by normalization layer. Thus, assuming that the input tensor is Z, the output of MHSA and FFN sub-layers can be formulated as:
Z ← N (sublayer(Z) + Z),(1)
where sublayer(·) is the mapping implemented by the sublayer itself and N (·) denotes normalization, e.g., BN (·) [160], LN (·) [161]. Discussion There is an important unsolved problem that is post-normalization versus pre-normalization. The original Vanilla Transformer uses post-normalization for each MHSA and FFN sub-layer. However, if we consider this from the mathematical perspective, pre-normalization makes more sense [162]. This is similar to the basic principle of the theory of matrix, that normalization should be performed before projection, e.g., Gram-Schmidt process 2 . This problem should be studied further by both theoretical research and experimental validation.


### Input Tokenization

Tokenization Vanilla Transformer was originally proposed for machine translation as a sequence-to-sequence model, thus it is straightforward to take the vocabulary sequences as input. As mentioned previously, the original selfattention can model an arbitrary input as a fully-connected graph, independently of modalities. Specifically, both Vanilla and variant Transformers take in the tokenized sequences, where each token can be regarded as a node of the graph. 1. In this survey, "multimodal Transformer" means "Transformer in multimodal learning context".

2. https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt process Special/Customized Tokens In Transformers, various special/customized tokens can be semantically defined as place-holders in the token sequences, e.g., mask token [MASK] [4]. Some common special tokens are summarized in appendix. Special tokens can be used in both uni-modal and multimodal Transformers.

Position Embedding Position embeddings are added to the token embeddings to retain positional information [4]. Vanilla Transformer uses sine and cosine functions to produce position embedding. To date, various implementations of position embedding have been proposed. The concrete solutions are outside the focus of this survey.


## Discussion

The main advantages of input tokenization include the following:

(1) Tokenization is a more general approach from a geometrically topological perspective, achieved by minimizing constraints caused by different modalities. In general, every modality has intrinsic constraints on modelling. For instance, sentences have sequential structures that are well-suited by RNN, and photos are restricted in aligned grid matrices that CNN works well for. Tokenization helps Transformers inherently to process different modalities universally via irregular sparse structures. Thus even Vanilla Transformer can encode multimodal inputs flexibly by just concatenation, weighted summation, even without any multimodal tailor-made modifications.

(2) Tokenization is a more flexible approach to organize the input information via concatenation/stack, weighted summation, etc. Vanilla Transformer injects temporal information to the token embedding by summing position embedding. For instance, when use Transformer to model freehand sketch drawing [163], each input token can integrate various drawing stroke patterns, e.g., stroke coordinates, stroke ordering, pen state (start/end).

(3) Tokenization is compatible with the task-specific customized tokens, e.g., [MASK] token [4] for Masked Language Modelling, [CLASS] token [5] for classification.


## Discussion How to understand position embedding to

Transformers is an open problem. It can be understood as a kind of implicit coordinate basis of feature space, to provide temporal or spatial information to the Transformer. For cloud point [164] and sketch drawing stroke [163], their token element is already a coordinate, meaning that position embedding is optional, not necessary. Furthermore, position embedding can be regarded as a kind of general additional information. In other words, from a mathematical point of view, any additional information can be added, such as detail of the manner of position embedding, e.g., the pen state of sketch drawing stroke [163], cameras and viewpoints in surveillance [165]. There is a comprehensive survey [166] discussing the position information in Transformers. For both sentence structures (sequential) and general graph structures (sparse, arbitrary, and irregular), position embeddings help Transformers to learn or encode the underlying structures. Considered from the mathematical perspective of self-attention, i.e., scaled dot-product attention, attentions are invariant to the positions of words (in text) or nodes (in graphs), if position embedding information is missing. Thus, in most cases, position embedding is necessary for Transformers.


### Self-Attention and Multi-Head Self-Attention

The core component of Vanilla Transformer is the Self-Attention (SA) operation [2] that is also termed "Scaled Dot-Product Attention". Assume that X = [x 1 , x 2 , · · · ] ∈ R N ×d is an input sequence of N elements/tokens, and an optional preprocessing is positional encoding by point-wise summation Z ← X⊕P ositionEmbedding or concatenation Z ← concat(X, P ositionEmbedding).


## Self-Attention (SA)

After preprocessing, embedding Z will go through three projection matrices (W Q ∈ R d×dq , W K ∈ R d×d k , and W V ∈ R d×dv , d q = d k ) to generate three embeddings Q (Query), K (Key), and V (Value):
Q = ZW Q , K = ZW K , V = ZW V .(2)
The output of self-attention is defined as
Z = SA(Q, K, V) = Sof tmax QK d q V.(3)
Given an input sequence, self-attention allows each element to attend to all the other elements, so that self-attention encodes the input as a fully-connected graph. Therefore, the encoder of Vanilla Transformer can be regarded as a fullyconnected GNN encoder, and the Transformer family has the non-local ability of global perception, similar to the Non-Local Network [167].

Masked Self-Attention (MSA) In practice, modification of self-attention is needed to help the decoder of Transformer to learn contextual dependence, to prevent positions from attending to subsequent positions, as
Z = M SA(Q, K, V) = Sof tmax QK d q M V,(4)
where M is a masking matrix. For instance, in GPT [88], an upper triangular mask to enable look-ahead attention where each token can only look at the past tokens. Masking can be used in both encoder [163], [168] and decoder of Transformer, and has flexible implementations, e.g., 0-1 hard mask [163], soft mask [168].

In both uni-modal and multimodal practices, specific masks are designed based on domain knowledge and prior knowledge. Essentially, MSA is used to inject additional knowledge to Transformer models, e.g., [24], [163], [169], [170].


## Multi-Head Self-Attention (MHSA)

In practice, multiple self-attention sub-layers can be stacked in parallel and their concatenated outputs are fused by a projection matrix W, to form a structure named Multi-Head Self-Attention:
Z = M HSA(Q, K, V) = concat(Z 1 , · · · , Z H )W,(5)
where each head
Z h = SA(Q h , K h V h ) and h ∈ [1, H],
and W is a linear projection matrix. The idea of MHSA is a kind of ensemble. MHSA helps the model to jointly attend to information from multiple representation sub-spaces.


### Feed-Forward Network

The output of the multi-head attention sub-layer will go through the position-wise Feed-Forward Network (FFN) that consists of successive linear layers with non-linear activation. For instance, a two-layer FFN can be formulated as
F F N (Z) = σ(ZW 1 + b 1 )W 2 + b 2 ,(6)
where W 1 , b 1 , W 2 , and b 2 denote the weights and biases of the two linear transformations, while σ(·) is nonlinear activation, e.g., ReLU(·) [171], GELU (·) [172]. In some Transformer literature, FFN is also termed Multi-Layer Perceptron (MLP).


## Vision Transformer

Vision Transformer (ViT) [5] has an image-specific input pipeline in which the input image must be split into fixedsize (e.g., 16 × 16, 32 × 32) patches. After going through the linearly embedded layer and adding the position embeddings, all the patch-wise sequences will be encoded by a standard Transformer encoder. Given an image X ∈ R H×W ×C (H height, W width, C channels), ViT needs to reshape X into a sequence of flattened 2D patches:
x p ∈ R N×(P 2 ·C) , where (P × P )
is the patch resolution and N = HW/P 2 . To perform classification, a standard approach is to prepend an extra learnable embedding "classification token" [CLASS] to the sequence of embedded patches:
Z ← concat([CLASS], XW),(7)
where W denotes the projection.


## Multimodal Transformers

Recently, a large number of Transformers have been studied extensively for various multimodal tasks, and shown to be compatible with various modalities in both discriminative and generative tasks.

In this section, we will review the key techniques/designs of the existing multimodal Transformer models, from the perspectives of multimodal input (Section 3.3.1), self-attention variants (Section 3.3.2), and network architectures (Section 3.3.3).


### Multimodal Input

The Transformer family is a general architecture that can be formulated as a type of general graph neural network. Specifically, self-attention can process each input as a fullyconnected graph, by attending to the global (non-local) patterns. Therefore, this intrinsic trait helps Transformers can work in a modality agnostic pipeline that is compatible with various modalities by treating the embedding of each token as a node of the graph.


## Tokenization and Embedding Processing

Given an input from an arbitrary modality, users only need to perform two main steps, (1) tokenize the input, and (2) select an embedding space to represent the tokens, before inputting the data into Transformers. In practice, both the tokenizing input and selecting embedding for the token are vital for Transformers but highly flexible, with many alternatives. For instance, given an image, the solution of tokenizing and embedding is not unique. Users can choose or design tokenization at multiple granularity levels -coarse-grained vs. fine-grained. e.g., use ROIs (obtained by an object detector) and CNN features as tokens and token embeddings [102], use patches and linear projection as tokens and token embeddings [5], or use graph node (obtained by object detector and graph generator) and GNN features as tokens and token embeddings [181]. Given a tokenization plan, the subsequent embedding approaches can be diverse. For example, for video input, a common tokenization is to treat the non-overlapping windows (down-sampled) over the video as tokens, and their embeddings can then be extracted by various 3D CNNs, e.g., VideoBERT [7], CBT [107], and UniVL [117] use S3D [186], ActBERT uses ResNet-3D [187]. Table 1 summarizes some common practices of multimodal inputs for Transformers, including RGB, video, audio/speech/music, text, graph, etc.

Discussion When considered from the perspective of geometric topology, each of the modalities listed in Table 1 can be regarded as a graph. An RGB image is essentially a neat grid graph in the pixel space. Both video and audio are clip/segment based graphs over a complex space involving temporal and semantic patterns. Both 2D and 3D drawing sketches [78], [163] are a kind of sparse graph if we consider their key points along the drawing strokes. Similar to sketches, the human pose also is a kind of graph. 3D point cloud is a graph in which each coordinate is a node. Other abstract modalities also can be interpreted as graphs, e.g., source code [44], data flow of source code [44], table [18], SQL database schema [25], text question graph [24], and electronic health records (EHRs) [184].


## Token Embedding Fusion

In practice, Transformers allow each token position to contain multiple embeddings. This is essentially a kind of early-fusion of embeddings, for both uni-modal and multimodal Transformer models.

(This will be discussed further in subsequent sections.) The most common fusion is the token-wise summing of the multiple embeddings, e.g., a specific token embedding ⊕ position embedding. Similar to the flexible tokenization, token embedding fusion is also flexible and widely applied to both uni-modal and multimodal Transformer applications. In [81], token-wise weighted summing is used to perform early-fusion of RGB and grey-scale images for multimodal surveillance AI. In particular, token embedding fusion has an important role in multimodal Transformer applications as various embeddings can be fused by tokenwise operators, e.g., in VisualBERT [104] and Unicoder-VL [108], segment embeddings are token-wise added to indicate which modality (vision or language) each token is from, VL-BERT [105] injects global visual context to linguistic domain by "linguistic token embedding ⊕ full image visual feature embedding", InterBERT [188] adds location information for ROI by "ROI embedding ⊕ location embedding", in Im-ageBERT [115], five kinds of embeddings are fused "image embedding ⊕ position embedding ⊕ linguistic embedding ⊕ segment embedding ⊕ sequence position embedding".


### Self-Attention Variants in Multimodal Context

In multimodal Transformers, cross-modal interactions (e.g., fusion, alignment) are essentially processed by self-attention and its variants. Thus, in this section, we will review the main multimodal modelling practices of Transformers, from a perspective of self-attention designs, including (1) early summation (token-wise, weighted), (2) early concatenation, (3) hierarchical attention (multi-stream to one-stream), (4) hierarchical attention (one-stream to multi-stream), (5) TABLE 2 Self-attention variants for multi-modal interaction/fusion. α and β denote weightings. "Att.": Attention; "Concat."/"Con.": Concatenation; "Tfs":

Transformer layers. N (A) and N (B) denote the token sequence lengths of two modalities.


## Self-Attention Definitions Streams Formulations Complexities References

Early Summation token-wise sum before Tfs 1
Z ← T f (αZ (A) ⊕ βZ (B) ) O(N 2 (A) ) [46], [81] Early Concat.
token sequence concat. before Tfs 1
Z ← T f (C(Z (A) , Z (B) )) O((N (A) + N (B) ) 2 ) [7]
, [44], [178], [180] Hierarchical Att.

2-stream Tfs followed by concat.
2 → 1 Z ← T f3(C(T f1(Z (A) ), T f2(Z (B) ))) O((N (A) + N (B) ) 2 ) [146],
Hierarchical Att. early concat. followed by 2-stream Tfs
1 → 2      C(Z (A) , Z (B) ) ← T f1(C(Z (A) , Z (B) )), Z (A) ← T f2(Z (A) ), Z (B) ← T f3(Z (B) ). O((N (A) + N (B) ) 2 ) [188]
Cross-Attention exchange query 2
Z (A) ← M HSA(QB, KA, VA) Z (B) ← M HSA(QA, KB, VB) O(N 2 (A) ) [102]
, [144] Cross-Att. to Con. 2-stream cross-att. followed by concat. [137], [189] cross-attention, and (6) cross-attention to concatenation. See Table 2 and Figure 2. For brevity, we will state and compare the mathematical formulations in two-modality cases. Please note that all discussed self-attention and its variants are such flexible that can be extended to multiple modality cases. Specifically, the following formulations are modality-, tokenization-, and embedding-agnostic, as self-attention models the embedding of arbitrary token from arbitrary modality as a node of a graph.
2 → 1      Z (A) ← M HSA(QB, KA, VA) Z (B) ← M HSA(QA, KB, VB) Z ← T f (C(Z (A) , Z (B) )) O((N (A) + N (B) ) 2 ) [69]
Given inputs X A and X B from two arbitrary modalities, Z (A) and Z (B) denote their respective token embeddings. Let Z denoting the token embedding (sequence) produced by the multimodal interactions. T f (·) stands for the processing of Transformer layers/blocks.

(1) Early Summation In practice, early summation [46], [81] is a simple and effective multimodal interaction, where the token embeddings from multiple modalities can be weighted summed at each token position and then processed by Transformer layers:
Z ← T f (αZ (A) ⊕ βZ (B) ) = M HSA(Q (AB) , K (AB) , V (AB) ),(8)
where ⊕ is element-wise sum, and α and β are weightings. Concretely,
Q (AB) = (αZ (A) ⊕ βZ (B) )W Q (AB) , K (AB) = (αZ (A) ⊕ βZ (B) )W K (AB) , and V (AB) = (αZ (A) ⊕ βZ (B) )W V (AB)
. Its main advantage is that it does not increase computational complexity. However, its main disadvantage is due to the manually set weightings. As discussed in Section 3.1.1 and 3.3.1, summing position embedding is intrinsically a case of early summation.

(2) Early Concatenation Another straightforward solution is early concatenation [7], [44], [178], [180] that the token embedding sequences from multiple modalities are concatenated and input into Transformer layers as
Z ← T f (C(Z (A) , Z (B) )).(9)
Thus, all the multimodal token positions can be attended as a whole sequence, such that the positions of each modality can be encoded well by conditioning the context of other modalities. VideoBERT [7] is the one of the first multimodal Transformer works, where video and text are fused via early concatenation that can encode the global multimodal context well [188]. However, the longer sequence after concatenation will increase computational complexity. Early concatenation is also termed "all-attention" or "Co-Transformer" [137].

(3) Hierarchical Attention (multi-stream to one-stream) Transformer layers can be combined hierarchically to attend to the cross-modal interactions. A common practice is that multimodal inputs are encoded by independent Transformer streams and their outputs are concatenated and fused by another Transformer [146]:
Z ← T f 3 (C(T f 1 (Z (A) ), T f 2 (Z (B) ))).(10)
This kind of hierarchical attention is an implementation of late interaction/fusion, and can be treated as a special case of early concatenation.

(4) Hierarchical Attention (one-stream to multi-stream) In-terBERT [188] is another good practice of hierarchical attention where concatenated multimodal inputs are encoded by a shared single-stream Transformer that is followed by two separate Transformer streams. This flow can be formulated as
     C(Z (A) , Z (B) ) ← T f 1 (C(Z (A) , Z (B) )), Z (A) ← T f 2 (Z (A) ), Z (B) ← T f 3 (Z (B) ).(11)
This method perceives the cross-modal interactions and meanwhile preserves the independence of uni-modal representation.


## (5) Cross-Attention

For two-stream Transformers, if the Q (Query) embeddings are exchanged/swapped in a crossstream manner, the cross-modal interactions can also be perceived. This method is termed cross-attention or coattention [190], which was first proposed in VilBERT [102]:
Z (A) ← M HSA(Q B , K A , V A ), Z (B) ← M HSA(Q A , K B , V B ).(12)
Cross-attention attends to each modality conditioned on the other and does not cause higher computational complexity, however if considered for each modality, this method fails to perform cross-modal attention globally and thus loses the whole context. As discussed in [188], two-stream crossattention can learn cross-modal interaction, whereas there is no self-attention to the self-context inside each modality.
Q K V TL (a) Q K V TL (b) TL Q K V TL Q K V Q K V TL (c) Q K V TL TL Q K V TL Q K V (d) TL V K Q TL Q K V (e) TL V K Q TL Q K V Q K V TL (f)

## (6) Cross-Attention to Concatenation

The two streams of cross-attention [102] can be further concatenated and processed by another Transformer to model the global context. This kind of hierarchically cross-modal interaction is also widely studied [137], [189], and alleviates the drawback of cross-attention.
     Z (A) ← M HSA(Q B , K A , V A ), Z (B) ← M HSA(Q A , K B , V B ), Z ← T f (C(Z (A) , Z (B) )).(13)
Discussion All these aforementioned self-attention variants for multimodal interactions are modality-generic, and can be applied in flexible strategies and for multi-granular tasks. Specifically, these interactions can be flexibly combined and nested. For instance, multiple cross-attention streams are used in hierarchical attention (one-stream to multi-stream) that in a two-stream decoupled model [191] T f 2 and T f 3 of Eq. 11 are implemented by cross-attention defined in Eq. 12. Moreover, they can be extended to multiple (≥ 3) modalities. TriBERT [183] is a tri-modal cross-attention (coattention) for vision, pose, and audio, where given a Query embedding, its Key and Value embeddings are the concatenation from the other modalities. Cross-attention to concatenation is applied to three modalities (i.e., language, video, and audio) in [189].


### Network Architectures

Essentially, various multimodal Transformers work due to their internal multimodal attentions that are the aforementioned self-attention variants. Meanwhile, as illustrated in Figure 2, these attentions determine the external network structures of the multimodal Transformers where they are embedded.

In general, if we consider from the angle of network structures, (1) early summation and early concatenation work in single-stream, (2) cross-attention work in multistreams, (3) hierarchical attention and cross-attention to concatenation work in hybrid-streams. Thus, multimodal Transformers can be divided into single-stream (e.g., Uniter [106], Visualbert [104], Vl-bert [105] , Unified VLP [110]), multi-stream (e.g., ViLBERT [102], Lxmert [103], ActBERT [114]), hybrid-stream (e.g., InterBERT [188]), etc.

From the perspective of timing of interaction, these multimodal attentions fall into three categories, i.e., early interaction: early summation, early concatenation, and hierarchical attention (one-stream to multi-stream), late interaction: hierarchical attention (multi-stream to one-stream), or throughout interaction: cross-attention, cross-attention to concatenation.

As demonstrated in Figure 2 in [192], the multimodal Transformer models have another architecture taxonomy based on the computational size of the components.


# APPLICATION SCENARIOS

In this section we survey multimodal Transformers based on the application scenarios. We consider two important paradigms: (1) Transformers for multimodal pretraining (Section 4.1, including both task-agnostic (Section 4.1.1) and task-specific (Section 4.1.2) multimodal pretraining), and (2) Transformers for specific multimodal tasks (Section 4.2).


## Transformers for Multimodal Pretraining

Inspired by the great success of Transformer based pretraining in NLP community, Transformers are also widely studied for multimodal pretraining as the various large-scale multimodal corpora is emerging. Recent work has demonstrated that if pretrained on large scale multimodal corpora Transformer based models [7], [102], [103], [104], [105], [106], [110] clearly outperform other competitors in a wide range of multimodal down-stream tasks, and moreover achieve the zero-shot generalization ability. These superiorities have led Transformer-based multimodal pretraining to become a hot topic, which has two main directions, i.e., general pretraining for agnostic down-stream tasks (Section 4.1.1), goal-oriented pretraining for specific down-stream tasks (Section 4.1.2).

We focus on these key points: (1) What trends are emerging? (2) Where/how do the cross-modal interactions take place during pretraining? (3) How to sort out and understand the pretraining pretext objectives? How can they drive Transformers to learn the cross-modal interactions?


### Task-Agnostic Multimodal Pretraining

Recently Transformer-oriented pretraining has been widely studied involving diverse modality combinations, e.g., video-text [7], [107], [117], image-text [102], [103], [104], [193], [194], [195], acoustic-text [180].

Among existing work, the following main trends are emerging:

(1) Vision-language pretraining (VLP) is a major research problem in this field. VLP is including both "image + language" and "video + language", also termed visual-linguistic pretraining. A great deal of excellent work has been proposed, e.g., VideoBERT [7], ViLBERT [102], LXMERT [103], VisualBERT [104], VL-BERT [105], UNITER [106], CBT [107], Unicoder-VL [108], B2T2 [109], VLP [110], 12-in-1 [111], Oscar [112], Pixel-BERT [113], ActBERT [114], ImageBERT [115], HERO [116], UniVL [117], SemVLP [196].

(2) Speech can be used as text. Thanks to recent advances in automatic speech recognition (ASR) techniques, in a multimodal context, speech can be converted to text by the offthe-shelf speech recognition tools. For instance, VideoBERT [7] and CBT [107] make full use of speech rather than lowlevel sounds as a source of cross-modal supervision, by extracting high-level semantic text.

(3) Overly dependent on the well-aligned multimodal data. A majority of Transformer-based multimodal pretraining works in a self-supervised manner, however, it is overly dependent on the well-aligned multimodal sample pairs/tuples. For instance, large amount of imagelanguage pretraining Transformer models are pretrained on large-scale image-text pairs, e.g., VisualBERT [104], VL-BERT [105], ViLBERT [102], LXMERT [103], UNITER [106]. For another example, the instructional videos (e.g., cooking) 3 are widely used as the pretraining corpora, e.g., HowToVQA69M [140], HowTo100M [141], as in general, their visual clues/content and the spoken words have a higher probability to align with each other, if compared with other videos. However, using cross-modal alignment as cross-modal supervision is costly for large-scale applications. Thus, how to use the weakly-aligned or even unpaired/unaligned multimodal data as the pretraining corpora is still understudied. Some recent attempts [137], [199] study the use of weakly-aligned cross-modal supervision to train Transformers to learn the cross-modal interactions.

(4) Most of the existing pretext tasks transfer well across modalities. For instance, Masked Language Modelling (MLM) in the text domain has been applied to audio and image, e.g., Masked Acoustic Modelling [180], [200], Masked Image Region Prediction [190], while both Sentence Ordering Modelling (SOM) [201] in text domain and Frame Ordering Modelling (FOM) [116] in video domain share the same idea. We will further discuss the pretext tasks for multimodal Transformer pretraining in the follows.

(5) Model structures are mainly in three categories. Essentially, in multimodal pretraining scenarios, Transformer models work based on those self-attention variants that are discussed in Section 3.3.2. Thus, if considered from the perspective of model structures, the existing Transformers for multimodal pretraining are also mainly in three categories, i.e., single-stream, multi-stream, hybrid-stream.

(6) Cross-modal interactions can perform within various components/levels in the pretraining pipelines. For Transformer based multimodal pretraining, the key is to drive the Transformer (encoder w/, w/o decoder) to learn the cross-modal interactions. In the existing Transformerbased multimodal pretraining practices, the cross-modal interactions are flexible, which can perform within various components/levels in the pretraining pipelines. In general, Transformer-based multimodal pretraining pipelines have three key components, from bottom to top, i.e., tokenization, Transformer representation, objective supervision. For not only the multimodal pretraining but also the specific multimodal tasks, the cross-modal interactions can perform within arbitrary component(s) of the three. As discussed in Section 3.3.2, because self-attention models the embedding of an arbitrary token from an arbitrary modality as a node of a graph, the existing pretraining pipelines can, in general, be transferred independently across modalities, unless considered with modality-specific objectives.

Discussion Vision Language Pretraining (VLP) follows two general pipelines: two-stage (need object detector, e.g., Faster R-CNN [202]) (e.g., LXMERT [103], ViLBert [102], VL-Bert [105], UNITER [106]) and end-to-end (e.g., Pixel-Bert [113], SOHO [203], KD-VLP [204], Simvlm [199]). Two-stage pipelines have a main advantage -object-aware perceiving, by using the supervised pre-trained visual detectors, however these are based on a strong assumption that the visual representations can be fixed.


## Discussion

How to look for more corpora that intrinsically have well-aligned cross-modal supervision, such as instructional videos, is still an open problem. However, weaklyaligned cross-modal samples are popular in the real-life scenarios, for instance, enormous weakly aligned multimodal data samples are emerging in e-commerce [137], due to fine-grained categories, complex combinations, and fuzzy correspondence. Well labelled/aligned cross-modal datasets are very costly in collecting and annotating; how to use weakly-aligned or even unaligned corpora crawled from the web is a promising question. Some recently successful practice [9], [199], [205] used weakly aligned image-text pairs to perform pretraining, and achieve both competitive performance and zero-shot learning capability for image classification, image-text retrieval, and open-ended visual question answering, etc. Because these practices in weak supervision make full use of large-scale pretraining corpora, they yield greater promise of zero-shot generalization.


## Pretext Tasks

In Transformer based multimodal pretraining, the pretraining tasks/objectives are also termed pretext tasks/objectives. To date, various pretext tasks have been studied, e.g., masked language modelling (MLM) [137], masked image region prediction/classification (also termed masked object classification (MOC)) [137], [190], masked region regression (MRR) [115], visual-linguistic matching (VLM) (e.g., image-text matching (ITM) [188], image text matching (ITM), phrase-region alignment (PRA) [204], word-region alignment (WRA) [106], video-subtitle matching (VSM) [116]), masked frame modelling (MFM) [116], frame order modelling (FOM) [116], next sentence prediction (NSP) [4], [102], [190], masked sentence generation (MSG) [191], masked group modelling (MGM) [188], prefix language modelling (PrefixLM) [199], video conditioned masked language model [117], text conditioned masked frame model [117], visual translation language modelling (VTLM) [206], and image-conditioned masked language modelling (also termed image-attended masked language modelling) [207]. These down-stream task -agnostic pretext pretraining is optional, and the down-stream task objectives can be trained directly, which will be discussed in Section 4.1.2. Table 3 provides the common and representative pretext tasks for Transformer based multimodal pretraining. In practice, pretext tasks can be combined, and some representative cases are summarized in Table 3 of [57], Table 2 of [58].

The pretext tasks have multiple taxonomies:

(1) Supervision. The common multimodal pretraining Transformers use well-aligned, weakly-aligned, and even unaligned multimodal sample pairs/tuples, to work in supervised, weakly-supervised, and unsupervised manners, respectively. Meanwhile, if we consider the definitions of their pretext tasks/objectives from supervision, the pretexts can be sorted into unsupervised/self-supervised (e.g., masked language modelling (MLM) [7], [137]) and supervised (e.g., image-text matching (ITM) [188] [102], [103], [104], [106], [209]), etc. Nowadays, self-supervised attempts are the majority.

(2) Modality. Considering the mathematical formulations, some pretexts are defined on single modality, e.g., masked language modelling [7], masked acoustic modelling [200], masked region regression (MRR) [115], while other pretexts are defined on multiple modalities, e.g., imageconditioned masked language modelling (IMLM) [208], image-text matching (ITM) [188], video-subtitle matching (VSM) [116]. Thus, from this mathematical view, the pretext tasks can be divided into two categories, i.e., uni-modal and multimodal.

However, this classification is not really accurate. It should be highlighted that in multimodal pretraining Transformer models, even if the pretext objective formulations only include uni-modal elements, pretexts can still involve other modalities, essentially conditioned on the clues from other modalities, by (a) prepositive token level interactions and/or Transformer level interactions, (b) co-training with other pretexts that involve other modalities. For instance, VL-BERT [105] uses two dual pretext tasks, i.e., masked language modelling and masked RoI classification.

(3) Motivation. If consider their motivations, the pretext tasks include masking, describing, matching, ordering, etc.

Some recent surveys focus on VLP and compare the existing VLP Transformer models from the angles of domain (image-text or video-text), vision feature extraction, language feature extraction, architecture (single-or dualstream), decoder (w/, w/o), pretext tasks/objectives, pretraining datasets, and down-stream tasks, e.g., Table 3 of [57], Table 2 of [58]. Different from these views, in this survey, we would propose our comparisons from some new perspectives. Specifically: (1) The core of Transformer ecosystem is self-attention, thus we would compare the existing multimodal pretraining Transformer models from the angles of how and when the self-attention or its variants perform cross-modal interactions. (2) Considering from a geometrically topological perspective, self-attention helps Transformers intrinsically work in a modality agnostic pipeline that is compatible with various modalities by taking in the embedding of each token as a node of graph, thus we would highlight that the existing VLP can be applied to other modalities, beyond visual and linguistic domains.

(3) We suggest to treat the Transformer-based multimodal pretraining pipelines having three key components, from bottom to top, i.e., tokenization, Transformer representation, objective supervision.


## Discussion

In spite of the recent advances, multimodal pretraining Transformer methods still have some obvious bottlenecks. For instance, as discussed by [208] in VLP field, while the BERT-style cross-modal pretraining models produce excellent results on various down-stream visionlanguage tasks, they fail to be applied to generative tasks directly. As discussed in [208], both VideoBERT [7] and CBT [107] have to train a separate video-to-text decoder for video captioning. This is a significant gap between the pretraining models designed for discriminative and generative tasks, as the main reason is discriminative task oriented pretraining models do not involve the decoders of Transformer. Therefore, how to design more unified pipelines that can work for both discriminative and generative down-stream tasks is also an open problem to be solved. Again for instance, common multimodal pretraining models often underperform for fine-grained/instance-level tasks as discussed by [137].

Discussion As discussed in [208], the masked language and region modelling as pre-training task have a main advantage that the Transformer encoder learned from these supervisions can encode both vision and language patterns based on bidirectional context and it is naturally fit for the semantic understanding tasks, e.g., VQA, image-text retrieval.

Discussion How to boost the performance for multimodal pretraining Transformers is an open problem. Some practices demonstrate that multi-task training (by adding auxiliary loss) [111], [137] and adversarial training [210] improve multimodal pretraining Transformers to further boost the performance. Meanwhile, overly compound pretraining objectives potentially upgrade the challenge of balancing among different loss terms, thus complicate the training optimization [199]. Moreover, the difficulty of the pretexts is also worth discussing. In general, if aim to learn more explicit object concepts, more complex pretext losses will be used [204]. However, for pretexts, whether more complexity is better remains a question.


### Task-Specific Multimodal Pretraining

In practices of multimodal Transformers, the aforementioned down-stream task -agnostic pretraining is optional, not necessary, and down-stream task specific pretraining is also widely studied [150], [190], [208], [211]. The main reasons include: (1) Limited by the existing technique, it is extremely difficult to design a set of highly universal network architectures, pretext tasks, and corpora that work for all the various down-stream applications. (2) There are nonnegligible gaps among various down-stream applications, e.g., task logic, data form, making it difficult to transfer from pretraining to down-stream applications.

Therefore, a large number of down-stream tasks still need tailor-made pretraining to improve the performance. Guhur et al. [150] propose in-domain pretraining for visionand-language navigation, as the general VLP focuses on TABLE 3 Pretext task comparison of multi-modal pretraining Transformer models (for agnostic down-stream tasks). "C-M Loss": cross-modal loss; "Con.

Loss": loss conditioned on other modality/modalities.


## Types (Motivations) Tasks C-M Loss Con. Loss References

Masking Masked Language Modelling (MLM) [7], [137] Image-Conditioned Masked Language Modelling (IMLM) [206], [207] [208] Text-Conditioned Masked Region Prediction [206] Masked Acoustic Modelling [180], [200] Masked Image Region Regression [115] Masked Image Region Prediction [190] Masked Frame Modelling (MFM) [116] Masked Sentence Generation (MSG) [191] Video Conditioned Masked Language Model [117] Text Conditioned Masked Frame Model [117] Describing Image-conditioned Denoising Autoencoding (IDA) [208] Text-conditioned Image Feature Generation (TIFG) [208] Prefix Language Modelling (PrefixLM) [199] Matching Image-Text Matching (ITM) [188] [102], [103], [104], [106], [209], Phrase-Region Alignment (PRA) [204] Word-Region Alignment (WRA) [106], [192] Video-Subtitle Matching (VSM) [116] Next Sentence Prediction (NSP) [4], [102], [190] Ordering Sentence Ordering Modelling (SOM) [201] Frame Ordering Modelling (FOM) [116] learning vision-language correlations, not designed for sequential decision making as required in embodied VLN. Murahari et al. [190] present a visual dialogue oriented approach to leverage pretraining on general vision-language datasets. XGPT [208] is tailor-made for image captioning, to overcome the limitation that BERT-based cross-modal pretrained models fail to be applied to generative tasks directly. ERNIE-ViLG [211] is designed for bidirectional image-text generation with Transformers. Special modalities have their own unique domain knowledge that can be used to design the specific pretrain pretexts. GraphCodeBERT [44] uses two structure-aware pretext tasks (i.e., predict where a variable is identified from, data flow edge prediction between variables) for programming source code. To learn from the spatial cues in 360 • video, Morgado et al. [145] propose to perform contrastive audio-visual spatial alignment of 360 • video and spatial audio. Med-BERT [184] is a contextualized embedding model pretrained on a structured electronic health record dataset of two million patients. Kaleido-BERT [212] is a VLP Transformer model tailor-made for the fashion domain.


## Transformers for Specific Multimodal Tasks

Recent work has demonstrated that Transformer models can encode various multimodal inputs in both classical and novel discriminative applications, e.g., RGB & optical flow [46], RGB & depth [213], RGB & point cloud [214], RGB & LiDAR [215], [216], textual description & point cloud [31], acoustic & text [180], audio & visual observation for Audio-Visual Navigation [76], speech query & schema of SQL database [25], text question/query & the schema SQL database [24], audio & tags [217], multimodal representation for video [218], [ [44], image & text for retrieval [222].

Meanwhile, Transformers also contribute to various multimodal generative tasks, including single-modality to single-modality (e.g., raw audio to 3D mesh sequence [39], RGB to 3D scene [40], single image to 3D human texture estimation [223], RGB to scene graph [19], [224], [225], [226], graph to graph [33], knowledge graph to text [227], video to scene graph [228], video to caption [229], [230], [231], [232], image to caption [233], [234], [235], [236], [237], text to speech [238], text to image [205], [239], text to shape [240], RGB to 3D human pose and mesh [41], music to dance [241]), multimodality to single modality (e.g., image & text to scene graph [242], Video Dialogue (text & audio & visual to text) [243], Mono Audio & Depth to Binaural Audio [14], music piece & seed 3D motion to long-range future 3D motions [146], X-raying image & question to answer [244], video & text & audio to text [245]), and multimodality to multimodality (e.g., [246]).


# CHALLENGES AND DESIGNS

Complementing the application scenario taxonomy discussed in Section 4, we further survey prior work from the perspective of technical challenges. We discuss seven challenges of Transformer based multimodal learning, including fusion, alignment, transferability, efficiency, robustness, universalness, and interpretability. This further extends the taxonomy introduced in [1] to tackle the higher diversity and wider scopes of existing Transformer based MML works in recent years.


## Fusion

In general, MML Transformers fuse information across multiple modalities primarily at three levels: input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion). Common early fusion based MML Transformer models [7], [104], [108] are also known as one-stream architecture, allowing the adoption of the merits of BERT due to minimal architectural modification. The main difference between these one-stream models is the usage of problem-specific modalities with variant masking techniques. With attention operation, a noticeable fusion scheme is introduced based on a notion of bottleneck tokens [175]. It applies for both early and middle fusion by simply choosing to-be-fused layers. We note that the simple prediction-based late fusion [247], [248] is less adopted in MML Transformers. This makes sense considering the motivations of learning stronger multimodal contextual representations and great advance of computing power. For enhancing and interpreting the fusion of MML, probing the interaction and measuring the fusion between modalities [249] would be an interesting direction to explore.


## Alignment

Cross-modal alignment is the key to a number of real-world multimodal applications. Transformer based cross-modal alignment has been studied for various tasks, e.g., speaker localization in multi-speaker videos [250], speech translation [180], text-to-speech alignment [251], text-to-video retrieval [252], [253], [254], and visual grounding of natural language [255], [256], [257], [258], [259]. Recently, Transformer based alignment [9], [119], [260], [261], [262] has led to a surge of leveraging large quantities of web data (e.g., image-text pairs) for vision and language tasks.

A representative practice is to map two modalities into a common representation space with contrastive learning over paired samples. The models based on this idea are often enormous in size and expensive to optimize from millions or billions of training data. Consequently, successive works mostly exploit pretrained models for tackling various downstream tasks [120], [263], [264], [265], [266]. These alignment models have the ability of zero-shot transfer particularly for image classification via prompt engineering [267]. This novel perspective is mind-blowing, given that image classification is conventionally regarded as a unimodal learning problem and zero-shot classification remains an unsolved challenge despite extensive research [268]. This has been studied for more challenging and fine-grained tasks (e.g., object detection [269], visual question answering [103], [106], [112], [263], and instance retrieval [222], [263]) by imposing region (semantic parts such as objects) level alignment. Finegrained alignment will however incur more computational costs from explicit region detection and how to eliminate this whilst keeping the region-level learning capability becomes a challenge. Several ideas introduced recently include random sampling [113], learning concept dictionary [203], uniform masking [270], patch projection [192], joint learning of a region detector [271], and representation aligning before mask prediction [263].


## Transferability

Transferability is a major challenge for Transformer based multimodal learning, involving the question of how to transfer models across different datasets and applications.

Data augmentation and adversarial perturbation strategies help multimodal Transformers to improve the generalization ability. VILLA [210] is a two-stage strategy (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning) that improves VLP Transformers.

In practice, the distribution gap between training data and practical data is noticeable. For instance, supervised data samples (well-labelled, well-aligned) are costly in practical applications, thus how to transfer the supervised multimodal Transformers pretrained on well-aligned cross-modal pairs/tuples to the weakly aligned test bed is challenging [137]. CLIP [9] is an inspiring solution that transfers knowledge across modalities by learning a shared multimodal embedding space, enabling zero-shot transfer of the model to down-stream tasks. The main inspiration that CLIP presents the community is that the pretrained multimodal (image and text) knowledge can be transferred to down-stream zero-shot image prediction by using a prompt template "A photo of a {label}." to bridge the distribution gap between training and test datasets.

Over-fitting is a major obstacle to transfer. Multimodal Transformers can be overly fitted to the dataset biases during training, due to the large modelling capability. Some recent practices exploit how to transfer the oracle model trained on noiseless dataset to real dataset. For instance, Kervadec et al. [272], [273] explore how transferable reasoning patterns are in VQA, and demonstrate that for LXMERT [103]/BERT-like reasoning patterns can be partially transferred from an ideal dataset to a real dataset.

Cross-task gap is another major obstacle to transfer [208], [274], due to the different reasoning and input-output workflows, e.g., how to use multimodal datasets to finetune the language pretrained model is difficult [274]. In real applications, multimodal pretrained Transformers sometimes need to handle the uni-modal data at inference stage due to the issue of missing modalities. One solution is using knowledge distillation, e.g., distilling from multimodal to uni-modal attention in Transformers [275], distilling from multiple uni-modal Transformer teachers to a shared Transformer encoder [276]. There is a huge gap across discriminative and generative multimodal tasks. As discussed in [208], the BERT-like encoder-only multimodal Transformers (e.g., VideoBERT [7], CBT [107]) need separately to train decoders for generation tasks. This could create a pretrain-finetune discrepancy detrimental to the generality. Recently, more and more attempts study this issue further, e.g., GilBERT [222] is a generative VLP models for a discriminative task, i.e., image-text retrieval.

Cross-lingual gap also should be considered for the transferability of Transformer based multimodal learning, e.g., universal cross-lingual generalization from English to non-English multimodal contexts [206], [277].


## Efficiency

Multimodal Transformers suffer from two major efficiency issues: (1) Due to the large model parameter capacity, they are data hungry and thus dependent on huge scale training datasets. (2) They are limited by the time and memory complexities that grow quadratically with the input sequence length, which are caused by the self-attention. In multimodal contexts, calculation explosion will become worse due to jointly high dimension representations. These two bottlenecks are interdependent and should be considered together.

To improve the training and/or inferring efficiency for multimodal Transformers, recent efforts have attempted to find various solutions, to use fewer training data and/or parameters. The main ideas can be summarized as the follows.

(1) Knowledge distillation. Distill the knowledge from the trained larger Transformers to smaller Transformers [93]. Miech et al. [278] conduct distillation from a slower model (early concatenation based Transformers, O((N (A) +N (B) ) 2 )) to a faster one (independently dual branch Transformers, O(N 2 (A) )). (2) Simplifying and compressing model. Remove the components to simplify the pipelines. Taking the VLP Transformer models as an example, two-stage pipeline is costly as they need object detector. One simplifying is processing the visual input in convolution-free manner, e.g., E2E-VLP [271], ViLT [192]. DropToken [174] reduces the training complexity via random dropping a portion of the video and audio tokens from input sequence during training. DropToken can be treated as an implementation of dropout or adversarial training. Weight-sharing is also a common practice for simplifying multimodal Transformer models. Wen et al. [279] present a weight-sharing Transformer on top of the visual and textual encoders to align text and image. Lee et al. [280] propose a novel parameter sharing scheme based on lowrank approximation.

(3) Asymmetrical network structures. Assign different model capacities and computational size properly for different modalities, to save parameters. See Figure 2 in [192].

(4) Improving utilization of training samples. Liu et al. [281] train a simplified LXMERT by making full use of fewer samples at different granularities. Li et al. [282] use fewer data to train CLIP by fully mining the potential self-supervised signals of (a) self-supervision within each modality, (b) multi-view supervision across modalities, and (c) nearest-neighbour supervision from other similar pairs.

(5) Compressing and pruning model. Search the optimal sub-structures/sub-networks of multimodal Transformers, e.g., playing Lottery Tickets with the VLP Transformer models [283], adaptively freezing some layers during training [284].

(6) Optimizing the complexity of self-attention. Transformers cost time and memory that grows quadratically with the input sequence length [285]. One potential solution is optimizing the O(N 2 ) complexity, e.g., Child et al. [286] present sparse factorizations of the attention matrix to reduce the quadratical complexity to O(n √ n), Transformer-LS [287] is an efficient Transformer for both language and vision long sequence, with linear computational and memory complexity.

(7) Optimizing the complexity of self-attention based multimodal interaction/fusion. Nagrani et al. [175] propose Fusion via Attention Bottlenecks (FSN, fusion bottleneck) to improve the early concatenation based multimodal interaction. FSN passes on the messages through a small number of bottleneck latents, thus requiring the model to purify the most necessary information from each modality for crossmodal sharing. This strategy uses the fusion bottleneck as a bridge, and not only improves fusion performance, but also reduces computational cost.

(8) Optimizing other strategies. Use optimal strategies to perform the common Transformer based multimodal interactions. Given the quadratic complexity of self-attention, using early concatenation based multimodal interaction to synchronously fuse the inputs from multiple modalities/views is costly. Yan et al. [288] present an efficient solution that sequentially fuses information between all pairs of two adjacent views in ascending order of sequence length. This is intrinsically a greedy strategy.


## Robustness

Multimodal Transformers pretrained on large-scale corpora achieve the state-of-the-art for various multimodal applications, while their robustness is still unclear and understudied. This at least involves two key challenges, i.e., how to theoretically analyse the robustness, how to improve the robustness.

Although that recent attempts [99], [182], [289], [290] study and evaluate how the Transformer components/sublayers contribute to the robustness, the main bottleneck is that the community lacks theoretical tools to analyse the Transformer family. Recently, the common practices to analyse robustness are mainly based on experiment evaluations [291], e.g., cross-dataset evaluations, perturbationbased evaluations. Thus, some multimodal datasets [130], [292] are proposed for evaluating the robustness.

Recent attempts mainly use two straightforward methods to improve the robustness for multimodal Transformer models: (1) augmentation and adversarial learning based strategies [293], [294], (2) fine-grained loss functions [295]. For instance: VILLA [210] is a generic adversarial training framework that can be applied to various multimodal Transformers. Akula et al. [292] empirically demonstrate that ViL-BERT fails to exploit linguistic structure, and they propose two methods to improve the robustness of ViLBERT, one based on contrastive learning and the other based on multitask learning.


## Universalness

Due to the highly diversity of tasks and modalities of multimodal learning, universalness is an important problem for multimodal Transformer models. A large amount of recent attempts [117], [296], [297], [298] study how to use as unified as possible pipelines to handle various modalities and multimodal tasks. Ideally, the unified multimodal Transformers can be compatible with various data (e.g., aligned and unaligned, uni-modal and multimodal) and tasks (e.g., supervised and unsupervised, uni-modal and multimodal, discriminative and generative), and meanwhile have either few-shot or even zero-shot generalization ability. Thus, the current solutions for universalness goal for multimodal Transformers are preliminary probes.

The currently unifying-oriented attempts mainly include:

(1) Unifying the pipelines for both uni-modal and multimodal inputs/tasks. As discussed Section 5.3, in practical scenarios, multimodal Transformers need to handle unimodal data due to the issue of missing modalities. Distilling multimodal knowledge into small models that are adaptable to uni-modal data and tasks is a successful practice [275], [276].

(2) Unifying the pipelines for both multimodal understanding and generation. In general, for multimodal Transformer pipelines, understanding and discriminative tasks require Transformer encoders only, while generation/generative tasks require both Transformer encoders and decoders. Existing attempts use multi-task learning to combine the understanding and generation workflows, where two kinds of workflows are jointly trained by multitask loss functions. From the perspective of model structures, typical solutions include: (a) encoder + decoder, e.g., E2E-VLP [271]. (b) separate encoders + cross encoder + decoder, e.g., UniVL [117], CBT [107]. (c) single unified/combined encoder-decoder, e.g., VLP [110]. (d) twostream decoupled design [191].

(3) Unifying and converting the tasks themselves, e.g., CLIP [9] converts zero-shot recognition to retrieval, thus reduces the costs of modifying the model. However, the aforementioned practices suffer some obvious challenges and bottlenecks, at least including:

(1) Due to modality and task gaps, universal models should consider the trade-off between universalness and cost. Unifying the pipelines of different modalities and tasks generally cause larger or more complicated model configuration, whereas for a specific modality or task, some components are redundant.

(2) Multi-task loss functions increase the complexity of training. How to co-train multiple objectives properly and effectively is challenging, due to that different objectives generally should be optimized in different strategies.


## Interpretability

Why and how Transformers perform so well in multimodal learning has been investigated [106], [299], [300], [301], [302], [303], [304], [305], [306]. These attempts mainly use probing task and ablation study. Cao et al. [299] design a set of probing tasks on UNITER [106] and LXMERT [103], to evaluate what patterns are learned in pretraining. Hendricks et al. [301] probe the image-language Transformers by fine-grained image-sentence pairs, and find that verb understanding is harder than subject or object understanding. Chen et al. [106] examine the optimal combination of pretraining tasks via ablation study, to compare how different pretexts contribute to the Transformers. Despite these attempts, the interpretability of multimodal Transformers is still under-studied to date.


# DISCUSSION AND OUTLOOK

Designing the universal MML models to excel across all the unimodal and multimodal down-stream tasks with different characteristics simultaneously [115], [299] is a non-trivial challenge. For instance, two-stream architectures [9], [263] are typically preferred over one-stream ones for cross-modal retrieval-like tasks in efficiency, since the representation of each modality can be pre-computed beforehand and reused repeatedly. That being said, how to design task-agnostic MML architectures is still an open challenge, in addition to other design choices such as pretext and objective loss functions. Furthermore, a clear gap remains between the state-of-the-art and this ultimate goal. In general, existing multimodal Transformer models [9], [199], [263] are superior only for specific MML tasks, as they are designed specifically for only a subset of specific tasks [137], [142], [212], [249], [260], [261], [265], [266]. Encouragingly, several recent studies towards universal modality learning in terms of modality-agnostic network design [3] and more task-generic architecture design [307], [308], [309] have been introduced, and it is hoped this will spark further investigation. To that end, instead of exhaustively exploring the vast model design space, seeking in-depth understanding and interpretation of a MML model's behaviour might be insightful for superior algorithm design, even though the interactions and synergy across different modalities are intrinsically complex and even potentially inconsistent over tasks [249].

For more fine-grained MML, it is widely acknowledged that discovering the latent semantic alignments across modalities is critical. An intuitive strategy is to leverage semantic parts (e.g., objects) pre-extracted by an off-the-shelf detector for MML [103], [104], [105], [106], [112], [204], [310]. This, however, is not only complex and error-prone, but computationally costly [207]. Several remedies introduced recently include random sampling [113], learning concept dictionary [203], jointly learning a region detector [271], and representation aligning before mask prediction [263]. Given the scale of MML training data, exploring this direction needs exhaustive computational costs, and it is supposed that industrial research teams with rich resources are more likely to afford. Ideally, a favourable MML method would leave fine-grained semantic alignment across modalities to emerge on its own, which is worthy of careful investigation in the future.

As the learning scale expands exponentially, the training data become inevitably noisy and heterogeneous [9], [199], [263]. It has been recently shown that properly tackling the noise issue is useful [263], [309]. Another related facet is training strategy, e.g., how many stages of training is superior over the common one-stage policy [115]. Further, the quadratic complexity with Transformers becomes more acute for multimodal data due to longer input. Despite extensive research on efficient variants [49], dedicated efficiency study for MML is still underestimated even empirically and call for more investigation.

Identifying the strengths of Transformers for multimodal machine learning is a big open problem. The following main points can be summarized from the literature: (1) Transformers can encode implicit knowledge [32]. (2) The multi-head brings multiple modelling sub-spaces that can further enhance the expressive ability of the model. Ideally, multiple heads after training are good and different. This is essentially a good practice of ensemble learning.

(3) Transformers intrinsically have a nature of global aggregation that perceives the non-local patterns. (4) Thanks to the large model capacity, Transformer models handle the challenging domain gaps and shifts (e.g., linguistic and visual) better via effective pretraining on large-scale corpora [294]. (5) Transformers can represent the inputs as graphs, which are intrinsically compatible with more modalities, e.g., table and SQL. (6) For modelling series and sequence patterns (e.g., time-series), Transformers have better training and inference efficiency against RNN-based models, thanks to their parallel computation in training and/or inference. Transformers are inherently permutation invariant for processing a sequence of points, e.g., well-suited for point cloud learning [164]. (7) Tokenization makes Transformers flexible to organize multimodal inputs, as discussed in Section 3.1.1.


# CONCLUSION

This survey focuses on multimodal machine learning with Transformers. We reviewed the landscape by introducing the Transformer designs and training in the multimodal contexts. We summarized the key challenges and solutions for this emerging and exciting field. Moreover, we discussed open problems and potential research directions. We hope that this survey gives a helpful and detailed overview for new researchers and practitioners, provides a convenient reference for relevant experts (e.g., multimodal machine learning researchers, Transformer network designers), and encourages future progress.


## APPENDIX

Notations and Abbreviations Throughout this survey, unless specified otherwise, mathematical symbols and abbreviated terms follow the conventions in Table 4.   TABLE 4 Notation and abbreviations used in this survey.


## Notations

Descriptions M, M T matrix M and its transpose Q Query matrix of Self-Attention K Key matrix of Self-Attention V Value matrix of Self-Attention SA(· · · ) Self-Attention M SA(· · · ) Masked Self-Attention F F N (·)

Feed-Forward Network M HSA(· · · )

Multi-Head Self-Attention M LP (· · · )

Multi-Layer Perceptron concat(· · · ), C(· · · ) concatenation operator T ransf (·), T f (·)

Transformer Special/Customized Tokens In both uni-modal and multimodal Transformers, various special/customized tokens are semantically defined as place-holders in the token sequences. Some common special tokens are summarized in Table 5. 


## Tokens

Definitions References

[CLS] class [44], [47], [110], [114] [SEP]

separate [44], [47], [110], [ fusion bottleneck [175] 

## Fig. 1 .
1Overview of Transformer[2].

## Fig. 2 .
2Transformer-based cross-modal interactions: (a) Early Summation, (b) Early Concatenation, (c) Hierarchical Attention (multi-stream to one-stream), (d) Hierarchical Attention (one-stream to multi-stream), (e) Cross-Attention, and (f) Cross-Attention to Concatenation. "Q": Query embedding; "K": Key embedding; "V": Value embedding. "TL": Transformer Layer. Best viewed in colour.

## TABLE 1
1Tokenization and token embedding comparison for multi-modal inputs for Transformers. "ICD": International Classification of Diseases.Modalities 
Tokenization 
Token Embeddings 
References 
RGB 
RoI 
CNN embedding 
ViLBERT [102], LXMERT [103], 
RGB 
patch 
linear projection 
ViT [5] 
video 
clip of sampled frames 
3D CNN embedding 
VideoBERT [7], CBT [107], ActBERT [114] 
video 
sampled frame 
2D CNN embedding 
[173] 
video 
voxel of sampled frames 
linear projection 
VATT [174] 
video 
patch of sampled frame 
linear projection 
MBT [175] 
360 • video 
clip of sampled frames 
3D CNN embedding 
AVSA [145] 
audio 
frame (mel-spectrogram) 
CNN embedding 
[173], AVSA [145] 
audio 
waveform segment 
linear projection 
VATT [174] 
audio 
spectrogram patch 
linear projection 
MBT [175] 
speech/audio 
waveform segment 
1D-CNN (TCN) embedding 
[176], FaceFormer [39] 
speech 
frame (mel-spectrogram) 
linear projection and gated CNN embedding 
Meta-StyleSpeech [177] 
speech 
frame (log-Mel filterbanks) 
linear projection 
AV-HuBERT [178] 
speech/audio 
frame (log power spectrum) 
linear projection 
VSET [179] 
speech 
frame (log-Mel filterbanks) 
2D-CNN embedding 
FAT-MLM [180] 
music 
frame (35-dim music feature) 
linear projection 
FACT [146] 
text 
word 
learned embedding 
Vanilla Transformer [2] 
text 
word 
GNN embedding 
MGNNS [181] 
SQL database schema 
table node, column node 
GNN embedding 
SpeechSQLNet [25] 
textual question-graph 
word node 
GNN embedding 
SADGA [24] 
sketch 
key point of stroke 
linear projection and learnable embedding 
Multi-Graph Transformer [163] 
sketch 
patch of picture 
linear projection 
RVT [182] 
table 
cell 
learned embedding 
[18] 
3D point cloud 
point 
non-linear projection 
Point Cloud Transformer [164] 
source code 
code 
learned embedding 
GraphCodeBERT [44] 
data flow of source code 
variable 
learned embedding 
GraphCodeBERT [44] 
pose 
key point 
GCN embedding 
TriBERT [183] 
electronic health records (EHRs) 
ICD code 
GNN embedding 
G-BERT [47] 
electronic health records (EHRs) 
ICD code 
learned embedding 
Med-BERT [184] 
Gigapixel Whole Slide Images 
patch 
CNN embedding 
MCAT [185] 




BN (·) Batch Normalization ReLU, ReLU (·) Rectified Linear Unit GELU, GELU (·)Gaussian Error Linear Unit VLP Visual-Linguistic (Vision-Language) Pretraining VidLVideo-and-Language RoI region-of-interestlayer operator 
R 
real number 
Hadamard product 
⊕ 
element-wise sum 
Abbreviated 
Terms 
Descriptions 

CNN 
Convolutional Neural Network 
TCN 
Temporal Convolutional Neural Network 
GNN 
Graph Neural Network 
GCN 
Graph Convolutional Network 
BERT 
Bidirectional Encoder Representations from Transformers 
N orm(·) 
normalization 
LN, LN (·) 
Layer Normalization 
BN, 

## TABLE 5
5Special/customized tokens.
. Note that instructional videos also have weakly aligned cases[197],[198].

Multimodal machine learning: A survey and taxonomy. T Baltrušaitis, C Ahuja, L.-P Morency, TPAMI. T. Baltrušaitis, C. Ahuja, and L.-P. Morency, "Multimodal ma- chine learning: A survey and taxonomy," TPAMI, 2018.

Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, NeurIPSA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," in NeurIPS, 2017.

Perceiver: General perception with iterative attention. A Jaegle, F Gimeno, A Brock, A Zisserman, O Vinyals, J Carreira, ICML. A. Jaegle, F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, and J. Carreira, "Perceiver: General perception with iterative atten- tion," in ICML, 2021.

Bert: Pretraining of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, arXivJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre- training of deep bidirectional transformers for language under- standing," arXiv, 2018.

A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, An image is worth 16x16 words: Transformers for image recognition at scale. arXivA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., "An image is worth 16x16 words: Transformers for image recognition at scale," arXiv, 2020.

End-to-end object detection with transformers. N Carion, F Massa, G Synnaeve, N Usunier, A Kirillov, S Zagoruyko, ECCV. N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, "End-to-end object detection with transformers," in ECCV, 2020.

Videobert: A joint model for video and language representation learning. C Sun, A Myers, C Vondrick, K Murphy, C Schmid, ICCV. C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid, "Videobert: A joint model for video and language representation learning," in ICCV, 2019.

Speech-t: Transducer for text to speech and beyond. J Chen, X Tan, Y Leng, J Xu, G Wen, T Qin, T.-Y Liu, NeurIPS. J. Chen, X. Tan, Y. Leng, J. Xu, G. Wen, T. Qin, and T.-Y. Liu, "Speech-t: Transducer for text to speech and beyond," NeurIPS, 2021.

Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, arXivA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agar- wal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., "Learning transferable visual models from natural language supervision," arXiv, 2021.

M Li, R Xu, S Wang, L Zhou, X Lin, C Zhu, M Zeng, H Ji, S.-F Chang, Clip-event: Connecting text and images with event structures. arXivM. Li, R. Xu, S. Wang, L. Zhou, X. Lin, C. Zhu, M. Zeng, H. Ji, and S.-F. Chang, "Clip-event: Connecting text and images with event structures," arXiv, 2022.

Multimodal intelligence: Representation learning, information fusion, and applications. C Zhang, Z Yang, X He, L Deng, JSTSP. C. Zhang, Z. Yang, X. He, and L. Deng, "Multimodal intelligence: Representation learning, information fusion, and applications," JSTSP, 2020.

Multimodal co-learning: Challenges, applications with datasets, recent advances and future directions. A Rahate, R Walambe, S Ramanna, K Kotecha, Information FusionA. Rahate, R. Walambe, S. Ramanna, and K. Kotecha, "Multi- modal co-learning: Challenges, applications with datasets, recent advances and future directions," Information Fusion, 2022.

The elements of statistical learning: data mining, inference, and prediction. T Hastie, R Tibshirani, J H Friedman, J H Friedman, Springer2T. Hastie, R. Tibshirani, J. H. Friedman, and J. H. Friedman, The elements of statistical learning: data mining, inference, and prediction. Springer, 2009, vol. 2.

Beyond mono to binaural: Generating binaural audio from mono audio with depth and cross modal attention. K K Parida, S Srivastava, G Sharma, WACVK. K. Parida, S. Srivastava, and G. Sharma, "Beyond mono to binaural: Generating binaural audio from mono audio with depth and cross modal attention," in WACV, 2022.

Cross-modality fusion transformer for multispectral object detection. F Qingyun, H Dapeng, W Zhaokui, arXivF. Qingyun, H. Dapeng, and W. Zhaokui, "Cross-modality fusion transformer for multispectral object detection," arXiv, 2021.

wav2vec 2.0: A framework for self-supervised learning of speech representations. A Baevski, Y Zhou, A Mohamed, M Auli, NeurIPS. A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, "wav2vec 2.0: A framework for self-supervised learning of speech representa- tions," NeurIPS, 2020.

Speech2action: Cross-modal supervision for action recognition. A Nagrani, C Sun, D Ross, R Sukthankar, C Schmid, A Zisserman, CVPR. A. Nagrani, C. Sun, D. Ross, R. Sukthankar, C. Schmid, and A. Zisserman, "Speech2action: Cross-modal supervision for ac- tion recognition," in CVPR, 2020.

W Chen, M.-W Chang, E Schlinger, W Wang, W W Cohen, Open question answering over tables and text. arXivW. Chen, M.-W. Chang, E. Schlinger, W. Wang, and W. W. Cohen, "Open question answering over tables and text," arXiv, 2020.

From general to specific: Informative scene graph generation via balance adjustment. Y Guo, L Gao, X Wang, Y Hu, X Xu, X Lu, H T Shen, J Song, ICCV. Y. Guo, L. Gao, X. Wang, Y. Hu, X. Xu, X. Lu, H. T. Shen, and J. Song, "From general to specific: Informative scene graph generation via balance adjustment," in ICCV, 2021.

Layouttransformer: Layout generation and completion with self-attention. K Gupta, J Lazarow, A Achille, L Davis, V Mahadevan, A Shrivastava, arXivK. Gupta, J. Lazarow, A. Achille, L. Davis, V. Mahadevan, and A. Shrivastava, "Layouttransformer: Layout generation and com- pletion with self-attention," arXiv, 2020.

Layouttransformer: Scene layout generation with conceptual and spatial diversity. C.-F Yang, W.-C Fan, F.-E Yang, Y.-C F Wang, CVPR. C.-F. Yang, W.-C. Fan, F.-E. Yang, and Y.-C. F. Wang, "Layout- transformer: Scene layout generation with conceptual and spatial diversity," in CVPR, 2021.

Sgtr: End-to-end scene graph generation with transformer. R Li, S Zhang, X He, CVPR. R. Li, S. Zhang, and X. He, "Sgtr: End-to-end scene graph generation with transformer," in CVPR, 2022.

Taming transformers for high-resolution image synthesis. P Esser, R Rombach, B Ommer, CVPR. P. Esser, R. Rombach, and B. Ommer, "Taming transformers for high-resolution image synthesis," in CVPR, 2021.

Sadga: Structure-aware dual graph aggregation network for text-to-sql. R Cai, J Yuan, B Xu, Z Hao, NeurIPS. R. Cai, J. Yuan, B. Xu, and Z. Hao, "Sadga: Structure-aware dual graph aggregation network for text-to-sql," NeurIPS, 2021.

Speech-tosql: Towards speech-driven sql query generation from natural language question. Y Song, R C Wong, X Zhao, D Jiang, arXivY. Song, R. C.-W. Wong, X. Zhao, and D. Jiang, "Speech-to- sql: Towards speech-driven sql query generation from natural language question," arXiv, 2022.

Revamping cross-modal recipe retrieval with hierarchical transformers and self-supervised learning. A Salvador, E Gundogdu, L Bazzani, M Donoser, CVPR. A. Salvador, E. Gundogdu, L. Bazzani, and M. Donoser, "Re- vamping cross-modal recipe retrieval with hierarchical trans- formers and self-supervised learning," in CVPR, 2021.

Proto: Program-guided transformer for program-guided tasks. Z Zhao, K Samel, B Chen, NeurIPSZ. Zhao, K. Samel, B. Chen et al., "Proto: Program-guided trans- former for program-guided tasks," in NeurIPS, 2021.

Improving sign language translation with monolingual data by sign backtranslation. H Zhou, W Zhou, W Qi, J Pu, H Li, CVPR. H. Zhou, W. Zhou, W. Qi, J. Pu, and H. Li, "Improving sign language translation with monolingual data by sign back- translation," in CVPR, 2021.

Read and attend: Temporal localisation in sign language videos. G Varol, L Momeni, S Albanie, T Afouras, A Zisserman, CVPR. G. Varol, L. Momeni, S. Albanie, T. Afouras, and A. Zisser- man, "Read and attend: Temporal localisation in sign language videos," in CVPR, 2021.

H Bull, T Afouras, G Varol, S Albanie, L Momeni, A Zisserman, Aligning subtitles in sign language videos. arXivH. Bull, T. Afouras, G. Varol, S. Albanie, L. Momeni, and A. Zis- serman, "Aligning subtitles in sign language videos," arXiv, 2021.

3dvg-transformer: Relation modeling for visual grounding on point clouds. L Zhao, D Cai, L Sheng, D Xu, ICCV. L. Zhao, D. Cai, L. Sheng, and D. Xu, "3dvg-transformer: Relation modeling for visual grounding on point clouds," in ICCV, 2021.

Krisp: Integrating implicit and symbolic knowledge for opendomain knowledge-based vqa. K Marino, X Chen, D Parikh, A Gupta, M Rohrbach, CVPR. K. Marino, X. Chen, D. Parikh, A. Gupta, and M. Rohrbach, "Krisp: Integrating implicit and symbolic knowledge for open- domain knowledge-based vqa," in CVPR, 2021.

Learning knowledge graphbased world models of textual environments. P Ammanabrolu, M O , arXivP. Ammanabrolu and M. O. Riedl, "Learning knowledge graph- based world models of textual environments," arXiv, 2021.

Multi-modal knowledge graph construction and application: A survey. X Zhu, Z Li, X Wang, X Jiang, P Sun, X Wang, Y Xiao, N J Yuan, arXivX. Zhu, Z. Li, X. Wang, X. Jiang, P. Sun, X. Wang, Y. Xiao, and N. J. Yuan, "Multi-modal knowledge graph construction and application: A survey," arXiv, 2022.

Sketchmate: Deep hashing for million-scale human sketch retrieval. P Xu, Y Huang, T Yuan, K Pang, Y.-Z Song, T Xiang, T M Hospedales, Z Ma, J Guo, CVPR. P. Xu, Y. Huang, T. Yuan, K. Pang, Y.-Z. Song, T. Xiang, T. M. Hospedales, Z. Ma, and J. Guo, "Sketchmate: Deep hashing for million-scale human sketch retrieval," in CVPR, 2018.

Deep selfsupervised representation learning for free-hand sketch. P Xu, Z Song, Q Yin, Y.-Z Song, L Wang, TCSVTP. Xu, Z. Song, Q. Yin, Y.-Z. Song, and L. Wang, "Deep self- supervised representation learning for free-hand sketch," TCSVT, 2020.

Fine-grained instance-level sketch-based video retrieval. P Xu, K Liu, T Xiang, T M Hospedales, Z Ma, J Guo, Y.-Z Song, TCSVTP. Xu, K. Liu, T. Xiang, T. M. Hospedales, Z. Ma, J. Guo, and Y.-Z. Song, "Fine-grained instance-level sketch-based video retrieval," TCSVT, 2020.

Y Vinker, E Pajouheshgar, J Y Bo, R C Bachmann, A H Bermano, D Cohen-Or, A Zamir, A Shamir, Clipasso: Semantically-aware object sketching. arXivY. Vinker, E. Pajouheshgar, J. Y. Bo, R. C. Bachmann, A. H. Bermano, D. Cohen-Or, A. Zamir, and A. Shamir, "Clipasso: Semantically-aware object sketching," arXiv, 2022.

Faceformer: Speech-driven 3d facial animation with transformers. Y Fan, Z Lin, J Saito, W Wang, T Komura, arXivY. Fan, Z. Lin, J. Saito, W. Wang, and T. Komura, "Faceformer: Speech-driven 3d facial animation with transformers," arXiv, 2021.

3d scene reconstruction with multi-layer depth and epipolar transformers. D Shin, Z Ren, E B Sudderth, C C Fowlkes, ICCV. D. Shin, Z. Ren, E. B. Sudderth, and C. C. Fowlkes, "3d scene re- construction with multi-layer depth and epipolar transformers," in ICCV, 2019.

End-to-end human pose and mesh reconstruction with transformers. K Lin, L Wang, Z Liu, CVPR. K. Lin, L. Wang, and Z. Liu, "End-to-end human pose and mesh reconstruction with transformers," in CVPR, 2021.

Layoutlmv2: Multi-modal pre-training for visually-rich document understanding. Y Xu, Y Xu, T Lv, L Cui, F Wei, G Wang, Y Lu, D Florencio, C Zhang, W Che, arXivY. Xu, Y. Xu, T. Lv, L. Cui, F. Wei, G. Wang, Y. Lu, D. Florencio, C. Zhang, W. Che et al., "Layoutlmv2: Multi-modal pre-training for visually-rich document understanding," arXiv, 2020.

Longformer: The longdocument transformer. I Beltagy, M E Peters, A Cohan, arXivI. Beltagy, M. E. Peters, and A. Cohan, "Longformer: The long- document transformer," arXiv, 2020.

D Guo, S Ren, S Lu, Z Feng, D Tang, S Liu, L Zhou, N Duan, A Svyatkovskiy, S Fu, Graphcodebert: Pre-training code representations with data flow. arXivD. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, A. Svyatkovskiy, S. Fu et al., "Graphcodebert: Pre-training code representations with data flow," arXiv, 2020.

Language-agnostic representation learning of source code from structure and context. D Zügner, T Kirschstein, M Catasta, J Leskovec, S Günnemann, arXivD. Zügner, T. Kirschstein, M. Catasta, J. Leskovec, and S. Günnemann, "Language-agnostic representation learning of source code from structure and context," arXiv, 2021.

Actortransformers for group activity recognition. K Gavrilyuk, R Sanford, M Javan, C G Snoek, CVPR. K. Gavrilyuk, R. Sanford, M. Javan, and C. G. Snoek, "Actor- transformers for group activity recognition," in CVPR, 2020.

Pre-training of graph augmented transformers for medication recommendation. J Shang, T Ma, C Xiao, J Sun, arXivJ. Shang, T. Ma, C. Xiao, and J. Sun, "Pre-training of graph aug- mented transformers for medication recommendation," arXiv, 2019.

A survey of transformers. T Lin, Y Wang, X Liu, X Qiu, arXivT. Lin, Y. Wang, X. Liu, and X. Qiu, "A survey of transformers," arXiv, 2021.

Efficient transformers: A survey. Y Tay, M Dehghani, D Bahri, D Metzler, arXivY. Tay, M. Dehghani, D. Bahri, and D. Metzler, "Efficient trans- formers: A survey," arXiv, 2020.

Visualizing transformers for nlp: a brief survey. A M Braşoveanu, R Andonie, International Conference Information Visualisation (IV). 2020A. M. Braşoveanu and R. Andonie, "Visualizing transformers for nlp: a brief survey," in International Conference Information Visualisation (IV), 2020.

S Khan, M Naseer, M Hayat, S W Zamir, F S Khan, M Shah, Transformers in vision: A survey. arXivS. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah, "Transformers in vision: A survey," arXiv, 2021.

A survey of visual transformers. Y Liu, Y Zhang, Y Wang, F Hou, J Yuan, J Tian, Y Zhang, Z Shi, J Fan, Z He, arXivY. Liu, Y. Zhang, Y. Wang, F. Hou, J. Yuan, J. Tian, Y. Zhang, Z. Shi, J. Fan, and Z. He, "A survey of visual transformers," arXiv, 2021.

K Han, Y Wang, H Chen, X Chen, J Guo, Z Liu, Y Tang, A Xiao, C Xu, Y Xu, A survey on vision transformer. arXivK. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu, Y. Xu et al., "A survey on vision transformer," arXiv, 2020.

Transformers in computational visual media: A survey. Y Xu, H Wei, M Lin, Y Deng, K Sheng, M Zhang, F Tang, W Dong, F Huang, C Xu, Computational Visual Media. Y. Xu, H. Wei, M. Lin, Y. Deng, K. Sheng, M. Zhang, F. Tang, W. Dong, F. Huang, and C. Xu, "Transformers in computational visual media: A survey," Computational Visual Media, 2022.

F Shamshad, S Khan, S W Zamir, M H Khan, M Hayat, F S Khan, H Fu, Transformers in medical imaging: A survey. arXivF. Shamshad, S. Khan, S. W. Zamir, M. H. Khan, M. Hayat, F. S. Khan, and H. Fu, "Transformers in medical imaging: A survey," arXiv, 2022.

Video transformers: A survey. J Selva, A S Johansen, S Escalera, K Nasrollahi, T B Moeslund, A Clapés, arXivJ. Selva, A. S. Johansen, S. Escalera, K. Nasrollahi, T. B. Moeslund, and A. Clapés, "Video transformers: A survey," arXiv, 2022.

Survey: Transformer based video-language pre-training. L Ruan, Q Jin, arXivL. Ruan and Q. Jin, "Survey: Transformer based video-language pre-training," arXiv, 2021.

F Chen, D Zhang, M Han, X Chen, J Shi, S Xu, B Xu, Vlp: A survey on vision-language pre-training. arXivF. Chen, D. Zhang, M. Han, X. Chen, J. Shi, S. Xu, and B. Xu, "Vlp: A survey on vision-language pre-training," arXiv, 2022.

Vision-language intelligence: Tasks, representation learning, and large models. F Li, H Zhang, Y.-F Zhang, S Liu, J Guo, L M Ni, P Zhang, L Zhang, arXivF. Li, H. Zhang, Y.-F. Zhang, S. Liu, J. Guo, L. M. Ni, P. Zhang, and L. Zhang, "Vision-language intelligence: Tasks, representation learning, and large models," arXiv, 2022.

Multimodal integration-a statistical view. L Wu, S L Oviatt, P R Cohen, TMML. Wu, S. L. Oviatt, and P. R. Cohen, "Multimodal integration-a statistical view," TMM, 1999.

Deep multimodal representation learning: A survey. W Guo, J Wang, S Wang, IEEE Access. W. Guo, J. Wang, and S. Wang, "Deep multimodal representation learning: A survey," IEEE Access, 2019.

Integration of acoustic and visual speech signals using neural networks. B P Yuhas, M H Goldstein, T J Sejnowski, IEEE Communications Magazine. B. P. Yuhas, M. H. Goldstein, and T. J. Sejnowski, "Integration of acoustic and visual speech signals using neural networks," IEEE Communications Magazine, 1989.

Multimodal behavior therapy. A A Lazarus, SpringerA. A. Lazarus et al., Multimodal behavior therapy. Springer, 1976.

Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges. D Feng, C Haase-Schütz, L Rosenbaum, H Hertlein, C Glaeser, F Timm, W Wiesbeck, K Dietmayer, TITS. D. Feng, C. Haase-Schütz, L. Rosenbaum, H. Hertlein, C. Glaeser, F. Timm, W. Wiesbeck, and K. Dietmayer, "Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges," TITS, 2020.

Multimodal motion prediction with stacked transformers. Y Liu, J Zhang, L Fang, Q Jiang, B Zhou, CVPR. Y. Liu, J. Zhang, L. Fang, Q. Jiang, and B. Zhou, "Multimodal motion prediction with stacked transformers," in CVPR, 2021.

Soat: A scene-and object-aware transformer for vision-andlanguage navigation. A Moudgil, A Majumdar, H Agrawal, S Lee, D Batra, NeurIPS. A. Moudgil, A. Majumdar, H. Agrawal, S. Lee, and D. Batra, "Soat: A scene-and object-aware transformer for vision-and- language navigation," NeurIPS, 2021.

Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences. F Lv, X Chen, Y Huang, L Duan, G Lin, CVPR. F. Lv, X. Chen, Y. Huang, L. Duan, and G. Lin, "Progressive modality reinforcement for human multimodal emotion recog- nition from unaligned multimodal sequences," in CVPR, 2021.

R Zellers, X Lu, J Hessel, Y Yu, J S Park, J Cao, A Farhadi, Y Choi, Merlot: Multimodal neural script knowledge models. arXivR. Zellers, X. Lu, J. Hessel, Y. Yu, J. S. Park, J. Cao, A. Farhadi, and Y. Choi, "Merlot: Multimodal neural script knowledge models," arXiv, 2021.

Humor knowledge enriched transformer for understanding multimodal humor. M K Hasan, S Lee, W Rahman, A Zadeh, R Mihalcea, L.-P Morency, E Hoque, AAAI. M. K. Hasan, S. Lee, W. Rahman, A. Zadeh, R. Mihalcea, L.- P. Morency, and E. Hoque, "Humor knowledge enriched trans- former for understanding multimodal humor," in AAAI, 2021.

Face, body, voice: Video person-clustering with multiple modalities. A Brown, V Kalogeiton, A Zisserman, arXivA. Brown, V. Kalogeiton, and A. Zisserman, "Face, body, voice: Video person-clustering with multiple modalities," arXiv, 2021.

Commercemm: Large-scale commerce multimodal representation learning with omni retrieval. L Yu, J Chen, A Sinha, M M Wang, H Chen, T L Berg, N Zhang, arXivL. Yu, J. Chen, A. Sinha, M. M. Wang, H. Chen, T. L. Berg, and N. Zhang, "Commercemm: Large-scale commerce multimodal representation learning with omni retrieval," arXiv, 2022.

Topological planning with transformers for vision-andlanguage navigation. K Chen, J K Chen, J Chuang, M Vázquez, S Savarese, CVPR. K. Chen, J. K. Chen, J. Chuang, M. Vázquez, and S. Savarese, "Topological planning with transformers for vision-and- language navigation," in CVPR, 2021.

Vln bert: A recurrent vision-and-language bert for navigation. Y Hong, Q Wu, Y Qi, C Rodriguez-Opazo, S Gould, CVPR. Y. Hong, Q. Wu, Y. Qi, C. Rodriguez-Opazo, and S. Gould, "Vln bert: A recurrent vision-and-language bert for navigation," in CVPR, 2021.

Curriculum learning for visionand-language navigation. J Zhang, J Fan, J Peng, NeurIPSJ. Zhang, J. Fan, J. Peng et al., "Curriculum learning for vision- and-language navigation," in NeurIPS, 2021.

The road to know-where: An object-and-room informed sequential bert for indoor vision-language navigation. Y Qi, Z Pan, Y Hong, M.-H Yang, A Van Den Hengel, Q Wu, ICCV. Y. Qi, Z. Pan, Y. Hong, M.-H. Yang, A. van den Hengel, and Q. Wu, "The road to know-where: An object-and-room informed sequential bert for indoor vision-language navigation," in ICCV, 2021.

Semantic audio-visual navigation. C Chen, Z Al-Halah, K Grauman, CVPR. C. Chen, Z. Al-Halah, and K. Grauman, "Semantic audio-visual navigation," in CVPR, 2021.

Learning from the master: Distilling cross-modal advanced knowledge for lip reading. S Ren, Y Du, J Lv, G Han, S He, CVPR. S. Ren, Y. Du, J. Lv, G. Han, and S. He, "Learning from the master: Distilling cross-modal advanced knowledge for lip reading," in CVPR, 2021.

Deep learning for free-hand sketch: A survey. P Xu, T M Hospedales, Q Yin, Y.-Z Song, T Xiang, L Wang, TPAMIP. Xu, T. M. Hospedales, Q. Yin, Y.-Z. Song, T. Xiang, and L. Wang, "Deep learning for free-hand sketch: A survey," TPAMI, 2022.

Behrt: transformer for electronic health records. Y Li, S Rao, J R A Solares, A Hassaine, R Ramakrishnan, D Canoy, Y Zhu, K Rahimi, G Salimi-Khorshidi, Scientific reports. Y. Li, S. Rao, J. R. A. Solares, A. Hassaine, R. Ramakrishnan, D. Canoy, Y. Zhu, K. Rahimi, and G. Salimi-Khorshidi, "Behrt: transformer for electronic health records," Scientific reports, 2020.

A comparison of pre-trained visionand-language models for multimodal representation learning across medical images and reports. Y Li, H Wang, Y Luo, BIBM. Y. Li, H. Wang, and Y. Luo, "A comparison of pre-trained vision- and-language models for multimodal representation learning across medical images and reports," in BIBM, 2020.

Deepchange: A large long-term person reidentification benchmark with clothes change. P Xu, X Zhu, arXivP. Xu and X. Zhu, "Deepchange: A large long-term person re- identification benchmark with clothes change," arXiv, 2021.

Multimodal few-shot learning with frozen language models. M Tsimpoukelli, J Menick, S Cabi, S Eslami, O Vinyals, F Hill, NeurIPS. M. Tsimpoukelli, J. Menick, S. Cabi, S. Eslami, O. Vinyals, and F. Hill, "Multimodal few-shot learning with frozen language models," NeurIPS, 2021.

Vl-adapter: Parameterefficient transfer learning for vision-and-language tasks. Y.-L Sung, J Cho, M Bansal, CVPR. Y.-L. Sung, J. Cho, and M. Bansal, "Vl-adapter: Parameter- efficient transfer learning for vision-and-language tasks," in CVPR, 2022.

Flamingo: a visual language model for few-shot learning. J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, NeurIPSJ.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds et al., "Flamingo: a visual language model for few-shot learning," NeurIPS, 2022.

Image as a foreign language: Beit pretraining for all vision and vision-language tasks. W Wang, H Bao, L Dong, J Bjorck, Z Peng, Q Liu, K Aggarwal, O K Mohammed, S Singhal, S Som, arXivW. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. Mohammed, S. Singhal, S. Som et al., "Image as a foreign language: Beit pretraining for all vision and vision-language tasks," arXiv, 2022.

Pali: A jointly-scaled multilingual language-image model. X Chen, X Wang, S Changpinyo, A Piergiovanni, P Padlewski, D Salz, S Goodman, A Grycner, B Mustafa, L Beyer, arXivX. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer et al., "Pali: A jointly-scaled multilingual language-image model," arXiv, 2022.

Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, arXivM. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, "Bart: Denoising sequence-to-sequence pre-training for natural language genera- tion, translation, and comprehension," arXiv, 2019.

Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, "Im- proving language understanding by generative pre-training," 2018.

Transformer-xl: Attentive language models beyond a fixedlength context. Z Dai, Z Yang, Y Yang, J Carbonell, Q V Le, R Salakhutdinov, arXivZ. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdi- nov, "Transformer-xl: Attentive language models beyond a fixed- length context," arXiv, 2019.

Xlnet: Generalized autoregressive pretraining for language understanding. Z Yang, Z Dai, Y Yang, J Carbonell, R R Salakhutdinov, Q V Le, NeurIPS. Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, "Xlnet: Generalized autoregressive pretraining for language understanding," NeurIPS, 2019.

Generative pretraining from pixels. M Chen, A Radford, R Child, J Wu, H Jun, D Luan, I Sutskever, ICML. M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever, "Generative pretraining from pixels," in ICML, 2020.

Pre-trained image processing transformer. H Chen, Y Wang, T Guo, C Xu, Y Deng, Z Liu, S Ma, C Xu, C Xu, W Gao, CVPR. H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, C. Xu, and W. Gao, "Pre-trained image processing transformer," in CVPR, 2021.

Training data-efficient image transformers & distillation through attention. H Touvron, M Cord, M Douze, F Massa, A Sablayrolles, H Jégou, ICML. H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou, "Training data-efficient image transformers & distilla- tion through attention," in ICML, 2021.

Toward transformer-based object detection. J Beal, E Kim, E Tzeng, D H Park, A Zhai, D Kislyuk, arXivJ. Beal, E. Kim, E. Tzeng, D. H. Park, A. Zhai, and D. Kislyuk, "Toward transformer-based object detection," arXiv, 2020.

Swin transformer: Hierarchical vision transformer using shifted windows. Z Liu, Y Lin, Y Cao, H Hu, Y Wei, Z Zhang, S Lin, B Guo, arXivZ. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, "Swin transformer: Hierarchical vision transformer using shifted windows," arXiv, 2021.

An empirical study of training selfsupervised vision transformers. X Chen, S Xie, K He, arXivX. Chen, S. Xie, and K. He, "An empirical study of training self- supervised vision transformers," arXiv, 2021.

Emerging properties in self-supervised vision transformers. M Caron, H Touvron, I Misra, H Jégou, J Mairal, P Bojanowski, A Joulin, arXivM. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin, "Emerging properties in self-supervised vision transformers," arXiv, 2021.

Beit: Bert pre-training of image transformers. H Bao, L Dong, F Wei, arXivH. Bao, L. Dong, and F. Wei, "Beit: Bert pre-training of image transformers," arXiv, 2021.

Vision transformers are robust learners. S Paul, P.-Y. Chen, arXivS. Paul and P.-Y. Chen, "Vision transformers are robust learners," arXiv, 2021.

Do vision transformers see like convolutional neural networks?. M Raghu, T Unterthiner, S Kornblith, C Zhang, A Dosovitskiy, NeurIPS. M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang, and A. Doso- vitskiy, "Do vision transformers see like convolutional neural networks?" NeurIPS, 2021.

How to understand masked autoencoders. S Cao, P Xu, D A Clifton, arXivS. Cao, P. Xu, and D. A. Clifton, "How to understand masked autoencoders," arXiv, 2022.

Vilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks. J Lu, D Batra, D Parikh, S Lee, arXivJ. Lu, D. Batra, D. Parikh, and S. Lee, "Vilbert: Pretraining task- agnostic visiolinguistic representations for vision-and-language tasks," arXiv, 2019.

Lxmert: Learning cross-modality encoder representations from transformers. H Tan, M Bansal, arXivH. Tan and M. Bansal, "Lxmert: Learning cross-modality encoder representations from transformers," arXiv, 2019.

Visualbert: A simple and performant baseline for vision and language. L H Li, M Yatskar, D Yin, C.-J Hsieh, K.-W Chang, arXivL. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang, "Visualbert: A simple and performant baseline for vision and language," arXiv, 2019.

Vl-bert: Pre-training of generic visual-linguistic representations. W Su, X Zhu, Y Cao, B Li, L Lu, F Wei, J Dai, arXivW. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai, "Vl-bert: Pre-training of generic visual-linguistic representations," arXiv, 2019.

Uniter: Universal image-text representation learning. Y.-C Chen, L Li, L Yu, A El Kholy, F Ahmed, Z Gan, Y Cheng, J Liu, ECCV. Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng, and J. Liu, "Uniter: Universal image-text representation learn- ing," in ECCV, 2020.

C Sun, F Baradel, K Murphy, C Schmid, Learning video representations using contrastive bidirectional transformer. arXivC. Sun, F. Baradel, K. Murphy, and C. Schmid, "Learning video representations using contrastive bidirectional transformer," arXiv, 2019.

Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining. G Li, N Duan, Y Fang, M Gong, D Jiang, AAAI. G. Li, N. Duan, Y. Fang, M. Gong, and D. Jiang, "Unicoder-vl: A universal encoder for vision and language by cross-modal pre- training," in AAAI, 2020.

C Alberti, J Ling, M Collins, D Reitter, Fusion of detected objects in text for visual question answering. arXivC. Alberti, J. Ling, M. Collins, and D. Reitter, "Fusion of detected objects in text for visual question answering," arXiv, 2019.

Unified vision-language pre-training for image captioning and vqa. L Zhou, H Palangi, L Zhang, H Hu, J Corso, J Gao, AAAI. L. Zhou, H. Palangi, L. Zhang, H. Hu, J. Corso, and J. Gao, "Unified vision-language pre-training for image captioning and vqa," in AAAI, 2020.

12-in-1: Multi-task vision and language representation learning. J Lu, V Goswami, M Rohrbach, D Parikh, S Lee, CVPR. J. Lu, V. Goswami, M. Rohrbach, D. Parikh, and S. Lee, "12-in- 1: Multi-task vision and language representation learning," in CVPR, 2020.

Oscar: Object-semantics aligned pretraining for vision-language tasks. X Li, X Yin, C Li, P Zhang, X Hu, L Zhang, L Wang, H Hu, L Dong, F Wei, ECCV. X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei et al., "Oscar: Object-semantics aligned pre- training for vision-language tasks," in ECCV, 2020.

Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. Z Huang, Z Zeng, B Liu, D Fu, J Fu, arXivZ. Huang, Z. Zeng, B. Liu, D. Fu, and J. Fu, "Pixel-bert: Aligning image pixels with text by deep multi-modal transformers," arXiv, 2020.

Actbert: Learning global-local video-text representations. L Zhu, Y Yang, CVPR. L. Zhu and Y. Yang, "Actbert: Learning global-local video-text representations," in CVPR, 2020.

Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data. D Qi, L Su, J Song, E Cui, T Bharti, A Sacheti, arXivD. Qi, L. Su, J. Song, E. Cui, T. Bharti, and A. Sacheti, "Image- bert: Cross-modal pre-training with large-scale weak-supervised image-text data," arXiv, 2020.

Hero: Hierarchical encoder for video+ language omni-representation pre-training. L Li, Y.-C Chen, Y Cheng, Z Gan, L Yu, J Liu, arXivL. Li, Y.-C. Chen, Y. Cheng, Z. Gan, L. Yu, and J. Liu, "Hero: Hierarchical encoder for video+ language omni-representation pre-training," arXiv, 2020.

Univl: A unified video and language pre-training model for multimodal understanding and generation. H Luo, L Ji, B Shi, H Huang, N Duan, T Li, J Li, T Bharti, M Zhou, arXivH. Luo, L. Ji, B. Shi, H. Huang, N. Duan, T. Li, J. Li, T. Bharti, and M. Zhou, "Univl: A unified video and language pre-training model for multimodal understanding and generation," arXiv, 2020.

A simple baseline for zero-shot semantic segmentation with pretrained vision-language model. M Xu, Z Zhang, F Wei, Y Lin, Y Cao, H Hu, X Bai, arXivM. Xu, Z. Zhang, F. Wei, Y. Lin, Y. Cao, H. Hu, and X. Bai, "A simple baseline for zero-shot semantic segmentation with pre- trained vision-language model," arXiv, 2021.

Scaling up visual and visionlanguage representation learning with noisy text supervision. C Jia, Y Yang, Y Xia, Y.-T Chen, Z Parekh, H Pham, Q V Le, Y Sung, Z Li, T Duerig, arXivC. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung, Z. Li, and T. Duerig, "Scaling up visual and vision- language representation learning with noisy text supervision," arXiv, 2021.

Clip-td: Clip targeted distillation for vision-language tasks. Z Wang, N Codella, Y.-C Chen, L Zhou, J Yang, X Dai, B Xiao, H You, S.-F Chang, L Yuan, arXivZ. Wang, N. Codella, Y.-C. Chen, L. Zhou, J. Yang, X. Dai, B. Xiao, H. You, S.-F. Chang, and L. Yuan, "Clip-td: Clip targeted distillation for vision-language tasks," arXiv, 2022.

Align before fuse: Vision and language representation learning with momentum distillation. J Li, R Selvaraju, A Gotmare, S Joty, C Xiong, S C H Hoi, NeurIPS. J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H. Hoi, "Align before fuse: Vision and language representation learning with momentum distillation," NeurIPS, 2021.

Coca: Contrastive captioners are image-text foundation models. J Yu, Z Wang, V Vasudevan, L Yeung, M Seyedhosseini, Y Wu, arXivJ. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu, "Coca: Contrastive captioners are image-text foundation models," arXiv, 2022.

Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. P Sharma, N Ding, S Goodman, R Soricut, ACL. P. Sharma, N. Ding, S. Goodman, and R. Soricut, "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning," in ACL, 2018.

. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Dollár, C L Zitnick, Microsoft coco: Common objects in context," in ECCVT.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, "Microsoft coco: Common objects in context," in ECCV, 2014.

Vqa: Visual question answering. S Antol, A Agrawal, J Lu, M Mitchell, D Batra, C L Zitnick, D Parikh, ICCV. S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh, "Vqa: Visual question answering," in ICCV, 2015.

Visual genome: Connecting language and vision using crowdsourced dense image annotations. R Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz, S Chen, Y Kalantidis, L.-J Li, D A Shamma, R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma et al., "Visual genome: Connecting language and vision using crowdsourced dense image annotations," IJCV, 2017.

Im2text: Describing images using 1 million captioned photographs. V Ordonez, G Kulkarni, T Berg, NeurIPS. V. Ordonez, G. Kulkarni, and T. Berg, "Im2text: Describing im- ages using 1 million captioned photographs," NeurIPS, 2011.

e-vil: A dataset and benchmark for natural language explanations in vision-language tasks. M Kayser, O.-M Camburu, L Salewski, C Emde, V Do, Z Akata, T Lukasiewicz, arXivM. Kayser, O.-M. Camburu, L. Salewski, C. Emde, V. Do, Z. Akata, and T. Lukasiewicz, "e-vil: A dataset and benchmark for natural language explanations in vision-language tasks," arXiv, 2021.

Multiple instance captioning: Learning representations from histopathology textbooks and articles. J Gamper, N Rajpoot, CVPR. J. Gamper and N. Rajpoot, "Multiple instance captioning: Learn- ing representations from histopathology textbooks and articles," in CVPR, 2021.

Adversarial vqa: A new benchmark for evaluating the robustness of vqa models. L Li, J Lei, Z Gan, J Liu, arXivL. Li, J. Lei, Z. Gan, and J. Liu, "Adversarial vqa: A new benchmark for evaluating the robustness of vqa models," arXiv, 2021.

A Talmor, O Yoran, A Catav, D Lahav, Y Wang, A Asai, G Ilharco, H Hajishirzi, J Berant, Multimodalqa: Complex question answering over text, tables and images. arXivA. Talmor, O. Yoran, A. Catav, D. Lahav, Y. Wang, A. Asai, G. Ilharco, H. Hajishirzi, and J. Berant, "Multimodalqa: Complex question answering over text, tables and images," arXiv, 2021.

Value: A multi-task benchmark for video-and-language understanding evaluation. L Li, J Lei, Z Gan, L Yu, Y.-C Chen, R Pillai, Y Cheng, L Zhou, X E Wang, W Y Wang, arXivL. Li, J. Lei, Z. Gan, L. Yu, Y.-C. Chen, R. Pillai, Y. Cheng, L. Zhou, X. E. Wang, W. Y. Wang et al., "Value: A multi-task benchmark for video-and-language understanding evaluation," arXiv, 2021.

Fashion iq: A new dataset towards retrieving images by natural language feedback. H Wu, Y Gao, X Guo, Z Al-Halah, S Rennie, K Grauman, R Feris, CVPR. H. Wu, Y. Gao, X. Guo, Z. Al-Halah, S. Rennie, K. Grauman, and R. Feris, "Fashion iq: A new dataset towards retrieving images by natural language feedback," in CVPR, 2021.

Deep audio-visual speech recognition. T Afouras, J S Chung, A Senior, O Vinyals, A Zisserman, TPAMIT. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, "Deep audio-visual speech recognition," TPAMI, 2018.

Dense-captioning events in videos. R Krishna, K Hata, F Ren, L Fei-Fei, J Carlos Niebles, ICCV. R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. Carlos Niebles, "Dense-captioning events in videos," in ICCV, 2017.

A Das, S Kottur, K Gupta, A Singh, D Yadav, J M Moura, D Parikh, D Batra, Visual dialog. CVPRA. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. Moura, D. Parikh, and D. Batra, "Visual dialog," in CVPR, 2017.

Product1m: Towards weakly supervised instance-level product retrieval via cross-modal pretraining. X Zhan, Y Wu, X Dong, Y Wei, M Lu, Y Zhang, H Xu, X Liang, ICCV. X. Zhan, Y. Wu, X. Dong, Y. Wei, M. Lu, Y. Zhang, H. Xu, and X. Liang, "Product1m: Towards weakly supervised instance-level product retrieval via cross-modal pretraining," in ICCV, 2021.

Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. S Changpinyo, P Sharma, N Ding, R Soricut, CVPR. S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts," in CVPR, 2021.

Wenlan: Bridging vision and language by large-scale multi-modal pre-training. Y Huo, M Zhang, G Liu, H Lu, Y Gao, G Yang, J Wen, H Zhang, B Xu, W Zheng, arXivY. Huo, M. Zhang, G. Liu, H. Lu, Y. Gao, G. Yang, J. Wen, H. Zhang, B. Xu, W. Zheng et al., "Wenlan: Bridging vision and language by large-scale multi-modal pre-training," arXiv, 2021.

Just ask: Learning to answer questions from millions of narrated videos. A Yang, A Miech, J Sivic, I Laptev, C Schmid, ICCV. A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid, "Just ask: Learning to answer questions from millions of narrated videos," in ICCV, 2021.

Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. A Miech, D Zhukov, J.-B Alayrac, M Tapaswi, I Laptev, J Sivic, ICCV. A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic, "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips," in ICCV, 2019.

Scaling up vision-language pre-training for image captioning. X Hu, Z Gan, J Wang, Z Yang, Z Liu, Y Lu, L Wang, arXivX. Hu, Z. Gan, J. Wang, Z. Yang, Z. Liu, Y. Lu, and L. Wang, "Scaling up vision-language pre-training for image captioning," arXiv, 2021.

Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. C Schuhmann, R Vencu, R Beaumont, R Kaczmarczyk, C Mullis, A Katta, T Coombes, J Jitsev, A Komatsuzaki, arXivC. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki, "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs," arXiv, 2021.

Pano-avqa: Grounded audio-visual question answering on 360deg videos. H Yun, Y Yu, W Yang, K Lee, G Kim, ICCV. H. Yun, Y. Yu, W. Yang, K. Lee, and G. Kim, "Pano-avqa: Grounded audio-visual question answering on 360deg videos," in ICCV, 2021.

Learning representations from audio-visual spatial alignment. P Morgado, Y Li, N Vasconcelos, arXivP. Morgado, Y. Li, and N. Vasconcelos, "Learning representations from audio-visual spatial alignment," arXiv, 2020.

Ai choreographer: Music conditioned 3d dance generation with aist++. R Li, S Yang, D A Ross, A Kanazawa, ICCV. R. Li, S. Yang, D. A. Ross, and A. Kanazawa, "Ai choreographer: Music conditioned 3d dance generation with aist++," in ICCV, 2021.

Artemis: Affective language for visual art. P Achlioptas, M Ovsjanikov, K Haydarov, M Elhoseiny, L J Guibas, CVPR. P. Achlioptas, M. Ovsjanikov, K. Haydarov, M. Elhoseiny, and L. J. Guibas, "Artemis: Affective language for visual art," in CVPR, 2021.

Multibench: Multiscale benchmarks for multimodal representation learning. P P Liang, Y Lyu, X Fan, Z Wu, Y Cheng, J Wu, L Y Chen, P Wu, M A Lee, Y Zhu, arXivP. P. Liang, Y. Lyu, X. Fan, Z. Wu, Y. Cheng, J. Wu, L. Y. Chen, P. Wu, M. A. Lee, Y. Zhu et al., "Multibench: Multiscale bench- marks for multimodal representation learning," arXiv, 2021.

Image retrieval on real-life images with pre-trained vision-and-language models. Z Liu, C Rodriguez-Opazo, D Teney, S Gould, ICCV. Z. Liu, C. Rodriguez-Opazo, D. Teney, and S. Gould, "Image retrieval on real-life images with pre-trained vision-and-language models," in ICCV, 2021.

Airbert: In-domain pretraining for vision-and-language navigation. P.-L Guhur, M Tapaswi, S Chen, I Laptev, C Schmid, ICCV. P.-L. Guhur, M. Tapaswi, S. Chen, I. Laptev, and C. Schmid, "Air- bert: In-domain pretraining for vision-and-language navigation," in ICCV, 2021.

Multimodal multi-speaker merger & acquisition financial modeling: A new task, dataset, and neural baselines. R Sawhney, M Goyal, P Goel, P Mathur, R Shah, ACL-IJCNLP. R. Sawhney, M. Goyal, P. Goel, P. Mathur, and R. Shah, "Multi- modal multi-speaker merger & acquisition financial modeling: A new task, dataset, and neural baselines," in ACL-IJCNLP, 2021.

X-world: Accessibility, vision, and autonomy meet. J Zhang, M Zheng, M Boyd, E Ohn-Bar, ICCV. J. Zhang, M. Zheng, M. Boyd, and E. Ohn-Bar, "X-world: Acces- sibility, vision, and autonomy meet," in ICCV, 2021.

Multimet: A multimodal dataset for metaphor understanding. D Zhang, M Zhang, H Zhang, L Yang, H Lin, ACL-IJCNLP. D. Zhang, M. Zhang, H. Zhang, L. Yang, and H. Lin, "Multimet: A multimodal dataset for metaphor understanding," in ACL- IJCNLP, 2021.

The hateful memes challenge: Detecting hate speech in multimodal memes. D Kiela, H Firooz, A Mohan, V Goswami, A Singh, P Ringshia, D Testuggine, arXivD. Kiela, H. Firooz, A. Mohan, V. Goswami, A. Singh, P. Ringshia, and D. Testuggine, "The hateful memes challenge: Detecting hate speech in multimodal memes," arXiv, 2020.

Towards automatic learning of procedures from web instructional videos. L Zhou, C Xu, J J Corso, AAAIL. Zhou, C. Xu, and J. J. Corso, "Towards automatic learning of procedures from web instructional videos," in AAAI, 2018.

What's cookin'? interpreting cooking videos using text, speech and vision. J Malmaud, J Huang, V Rathod, N Johnston, A Rabinovich, K Murphy, arXivJ. Malmaud, J. Huang, V. Rathod, N. Johnston, A. Rabinovich, and K. Murphy, "What's cookin'? interpreting cooking videos using text, speech and vision," arXiv, 2015.

M M Bronstein, J Bruna, T Cohen, P Veličković, Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. arXivM. M. Bronstein, J. Bruna, T. Cohen, and P. Veličković, "Geometric deep learning: Grids, groups, graphs, geodesics, and gauges," arXiv, 2021.

A generalization of transformer networks to graphs. V P Dwivedi, X Bresson, arXivV. P. Dwivedi and X. Bresson, "A generalization of transformer networks to graphs," arXiv, 2020.

Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in CVPR, 2016.

Batch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, ICML. S. Ioffe and C. Szegedy, "Batch normalization: Accelerating deep network training by reducing internal covariate shift," in ICML, 2015.

Layer normalization. J L Ba, J R Kiros, G E Hinton, arXivJ. L. Ba, J. R. Kiros, and G. E. Hinton, "Layer normalization," arXiv, 2016.

On layer normalization in the transformer architecture. R Xiong, Y Yang, D He, K Zheng, S Zheng, C Xing, H Zhang, Y Lan, L Wang, T Liu, ICML. R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu, "On layer normalization in the transformer architecture," in ICML, 2020.

Multigraph transformer for free-hand sketch recognition. P Xu, C K Joshi, X Bresson, TNNLS. P. Xu, C. K. Joshi, and X. Bresson, "Multigraph transformer for free-hand sketch recognition," TNNLS, 2021.

Pct: Point cloud transformer. M.-H Guo, J.-X Cai, Z.-N Liu, T.-J Mu, R R Martin, S.-M Hu, Computational Visual Media. M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu, "Pct: Point cloud transformer," Computational Visual Media, 2021.

Transreid: Transformer-based object re-identification. S He, H Luo, P Wang, F Wang, H Li, W Jiang, ICCV. S. He, H. Luo, P. Wang, F. Wang, H. Li, and W. Jiang, "Transreid: Transformer-based object re-identification," in ICCV, 2021.

Position information in transformers: An overview. P Dufter, M Schmitt, H Schütze, arXivP. Dufter, M. Schmitt, and H. Schütze, "Position information in transformers: An overview," arXiv, 2021.

Non-local neural networks. X Wang, R Girshick, A Gupta, K He, CVPR. X. Wang, R. Girshick, A. Gupta, and K. He, "Non-local neural networks," in CVPR, 2018.

End-toend dense video captioning with masked transformer. L Zhou, Y Zhou, J J Corso, R Socher, C Xiong, CVPR. L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong, "End-to- end dense video captioning with masked transformer," in CVPR, 2018.

Ratsql: Relation-aware schema encoding and linking for text-to-sql parsers. B Wang, R Shin, X Liu, O Polozov, M Richardson, arXivB. Wang, R. Shin, X. Liu, O. Polozov, and M. Richardson, "Rat- sql: Relation-aware schema encoding and linking for text-to-sql parsers," arXiv, 2019.

Sgeitl: Scene graph enhanced imagetext learning for visual commonsense reasoning. Z Wang, H You, L H Li, A Zareian, S Park, Y Liang, K.-W Chang, S.-F Chang, arXivZ. Wang, H. You, L. H. Li, A. Zareian, S. Park, Y. Liang, K.-W. Chang, and S.-F. Chang, "Sgeitl: Scene graph enhanced image- text learning for visual commonsense reasoning," arXiv, 2021.

Deep sparse rectifier neural networks. X Glorot, A Bordes, Y Bengio, AISTATS. X. Glorot, A. Bordes, and Y. Bengio, "Deep sparse rectifier neural networks," in AISTATS, 2011.

Gaussian error linear units (gelus). D Hendrycks, K Gimpel, arXivD. Hendrycks and K. Gimpel, "Gaussian error linear units (gelus)," arXiv, 2016.

Exploring cross-video and cross-modality signals for weaklysupervised audio-visual video parsing. Y.-B Lin, H.-Y Tseng, H.-Y Lee, Y.-Y. Lin, M.-H Yang, NeurIPSY.-B. Lin, H.-Y. Tseng, H.-Y. Lee, Y.-Y. Lin, and M.-H. Yang, "Exploring cross-video and cross-modality signals for weakly- supervised audio-visual video parsing," NeurIPS, 2021.

Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. H Akbari, L Yuan, R Qian, W.-H Chuang, S.-F Chang, Y Cui, B Gong, arXivH. Akbari, L. Yuan, R. Qian, W.-H. Chuang, S.-F. Chang, Y. Cui, and B. Gong, "Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text," arXiv, 2021.

Attention bottlenecks for multimodal fusion. A Nagrani, S Yang, A Arnab, A Jansen, C Schmid, C Sun, NeurIPS. A. Nagrani, S. Yang, A. Arnab, A. Jansen, C. Schmid, and C. Sun, "Attention bottlenecks for multimodal fusion," NeurIPS, 2021.

Multimodal and multilingual embeddings for large-scale speech mining. P.-A Duquenne, H Gong, H Schwenk, NeurIPS. P.-A. Duquenne, H. Gong, and H. Schwenk, "Multimodal and multilingual embeddings for large-scale speech mining," NeurIPS, 2021.

Meta-stylespeech: Multi-speaker adaptive text-to-speech generation. D Min, D B Lee, E Yang, S J Hwang, arXivD. Min, D. B. Lee, E. Yang, and S. J. Hwang, "Meta-stylespeech: Multi-speaker adaptive text-to-speech generation," arXiv, 2021.

Learning audio-visual speech representation by masked multimodal cluster prediction. B Shi, W.-N Hsu, K Lakhotia, A Mohamed, arXivB. Shi, W.-N. Hsu, K. Lakhotia, and A. Mohamed, "Learning audio-visual speech representation by masked multimodal clus- ter prediction," arXiv, 2022.

Vset: A multimodal transformer for visual speech enhancement. K Ramesh, C Xing, W Wang, D Wang, X Chen, ICASSP. K. Ramesh, C. Xing, W. Wang, D. Wang, and X. Chen, "Vset: A multimodal transformer for visual speech enhancement," in ICASSP, 2021.

Fused acoustic and text encoding for multimodal bilingual pretraining and speech translation. R Zheng, J Chen, M Ma, L Huang, arXivR. Zheng, J. Chen, M. Ma, and L. Huang, "Fused acoustic and text encoding for multimodal bilingual pretraining and speech translation," arXiv, 2021.

Multimodal sentiment detection based on multi-channel graph neural networks. X Yang, S Feng, Y Zhang, D Wang, ACL-IJCNLP. X. Yang, S. Feng, Y. Zhang, and D. Wang, "Multimodal sentiment detection based on multi-channel graph neural networks," in ACL-IJCNLP, 2021.

X Mao, G Qi, Y Chen, X Li, R Duan, S Ye, Y He, H Xue, Towards robust vision transformer. arXivX. Mao, G. Qi, Y. Chen, X. Li, R. Duan, S. Ye, Y. He, and H. Xue, "Towards robust vision transformer," arXiv, 2021.

Tribert: Human-centric audiovisual representation learning. T Rahman, M Yang, L Sigal, NeurIPS. T. Rahman, M. Yang, and L. Sigal, "Tribert: Human-centric audio- visual representation learning," NeurIPS, 2021.

Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. L Rasmy, Y Xiang, Z Xie, C Tao, D Zhi, NPJ digital medicine. L. Rasmy, Y. Xiang, Z. Xie, C. Tao, and D. Zhi, "Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction," NPJ digital medicine, 2021.

Multimodal co-attention transformer for survival prediction in gigapixel whole slide images. R J Chen, M Y Lu, W.-H Weng, T Y Chen, D F Williamson, T Manz, M Shady, F Mahmood, ICCV. R. J. Chen, M. Y. Lu, W.-H. Weng, T. Y. Chen, D. F. Williamson, T. Manz, M. Shady, and F. Mahmood, "Multimodal co-attention transformer for survival prediction in gigapixel whole slide im- ages," in ICCV, 2021.

Rethinking spatiotemporal feature learning for video understanding. S Xie, C Sun, J Huang, Z Tu, K Murphy, arXivS. Xie, C. Sun, J. Huang, Z. Tu, and K. Murphy, "Rethinking spatiotemporal feature learning for video understanding," arXiv, 2017.

A closer look at spatiotemporal convolutions for action recognition. D Tran, H Wang, L Torresani, J Ray, Y Lecun, M Paluri, CVPR. D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri, "A closer look at spatiotemporal convolutions for action recogni- tion," in CVPR, 2018.

Interbert: Vision-and-language interaction for multi-modal pretraining. J Lin, A Yang, Y Zhang, J Liu, J Zhou, H Yang, arXivJ. Lin, A. Yang, Y. Zhang, J. Liu, J. Zhou, and H. Yang, "Interbert: Vision-and-language interaction for multi-modal pretraining," arXiv, 2020.

Multimodal transformer for unaligned multimodal language sequences. Y.-H H Tsai, S Bai, P P Liang, J Z Kolter, L.-P Morency, R Salakhutdinov, ACL. Y.-H. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-P. Morency, and R. Salakhutdinov, "Multimodal transformer for unaligned multimodal language sequences," in ACL, 2019.

Large-scale pretraining for visual dialog: A simple state-of-the-art baseline. V Murahari, D Batra, D Parikh, A Das, ECCV. V. Murahari, D. Batra, D. Parikh, and A. Das, "Large-scale pre- training for visual dialog: A simple state-of-the-art baseline," in ECCV, 2020.

Scheduled sampling in vision-language pretraining with decoupled encoder-decoder network. Y Li, Y Pan, T Yao, J Chen, T Mei, AAAI. Y. Li, Y. Pan, T. Yao, J. Chen, and T. Mei, "Scheduled sampling in vision-language pretraining with decoupled encoder-decoder network," in AAAI, 2021.

Vilt: Vision-and-language transformer without convolution or region supervision. W Kim, B Son, I Kim, ICML. W. Kim, B. Son, and I. Kim, "Vilt: Vision-and-language trans- former without convolution or region supervision," in ICML, 2021.

Vision-language pre-training with triple contrastive learning. J Yang, J Duan, S Tran, Y Xu, S Chanda, L Chen, B Zeng, T Chilimbi, J Huang, arXivJ. Yang, J. Duan, S. Tran, Y. Xu, S. Chanda, L. Chen, B. Zeng, T. Chilimbi, and J. Huang, "Vision-language pre-training with triple contrastive learning," arXiv, 2022.

Grounded language-image pre-training. L H Li, P Zhang, H Zhang, J Yang, C Li, Y Zhong, L Wang, L Yuan, L Zhang, J.-N Hwang, CVPR. L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan, L. Zhang, J.-N. Hwang et al., "Grounded language-image pre-training," in CVPR, 2022.

Glipv2: Unifying localization and vision-language understanding. H Zhang, P Zhang, X Hu, Y.-C Chen, L Li, X Dai, L Wang, L Yuan, J.-N Hwang, J Gao, NeurIPSH. Zhang, P. Zhang, X. Hu, Y.-C. Chen, L. Li, X. Dai, L. Wang, L. Yuan, J.-N. Hwang, and J. Gao, "Glipv2: Unifying localization and vision-language understanding," NeurIPS, 2022.

Semvlp: Vision-language pre-training by aligning semantics at multiple levels. C Li, M Yan, H Xu, F Luo, W Wang, B Bi, S Huang, arXivC. Li, M. Yan, H. Xu, F. Luo, W. Wang, B. Bi, and S. Huang, "Semvlp: Vision-language pre-training by aligning semantics at multiple levels," arXiv, 2021.

End-to-end learning of visual representations from uncurated instructional videos. A Miech, J.-B Alayrac, L Smaira, I Laptev, J Sivic, A Zisserman, CVPR. A. Miech, J.-B. Alayrac, L. Smaira, I. Laptev, J. Sivic, and A. Zis- serman, "End-to-end learning of visual representations from uncurated instructional videos," in CVPR, 2020.

Temporal alignment networks for long-term video. T Han, W Xie, A Zisserman, CVPR. T. Han, W. Xie, and A. Zisserman, "Temporal alignment networks for long-term video," in CVPR, 2022.

Simvlm: Simple visual language model pretraining with weak supervision. Z Wang, J Yu, A W Yu, Z Dai, Y Tsvetkov, Y Cao, arXivZ. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao, "Simvlm: Simple visual language model pretraining with weak supervision," arXiv, 2021.

Mam: Masked acoustic modeling for end-to-end speech-to-text translation. J Chen, M Ma, R Zheng, L Huang, arXivJ. Chen, M. Ma, R. Zheng, and L. Huang, "Mam: Masked acoustic modeling for end-to-end speech-to-text translation," arXiv, 2020.

Using bert encoding and sentence-level language model for sentence ordering. M Golestani, S Z Razavi, Z Borhanifard, F Tahmasebian, H Faili, TSD. M. Golestani, S. Z. Razavi, Z. Borhanifard, F. Tahmasebian, and H. Faili, "Using bert encoding and sentence-level language model for sentence ordering," in TSD, 2021.

Faster r-cnn: Towards realtime object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, NeurIPS. S. Ren, K. He, R. Girshick, and J. Sun, "Faster r-cnn: Towards real- time object detection with region proposal networks," NeurIPS, 2015.

Seeing out of the box: End-to-end pre-training for vision-language representation learning. Z Huang, Z Zeng, Y Huang, B Liu, D Fu, J Fu, CVPR. Z. Huang, Z. Zeng, Y. Huang, B. Liu, D. Fu, and J. Fu, "Seeing out of the box: End-to-end pre-training for vision-language rep- resentation learning," in CVPR, 2021.

Kdvlp: Improving end-to-end vision-and-language pretraining with object knowledge distillation. Y Liu, C Wu, S Tseng, V Lal, X He, N Duan, arXivY. Liu, C. Wu, S.-y. Tseng, V. Lal, X. He, and N. Duan, "Kd- vlp: Improving end-to-end vision-and-language pretraining with object knowledge distillation," arXiv, 2021.

Zero-shot text-to-image generation. A Ramesh, M Pavlov, G Goh, S Gray, C Voss, A Radford, M Chen, I Sutskever, arXivA. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, "Zero-shot text-to-image generation," arXiv, 2021.

Uc2: Universal cross-lingual cross-modal vision-and-language pre-training. M Zhou, L Zhou, S Wang, Y Cheng, L Li, Z Yu, J Liu, CVPR. M. Zhou, L. Zhou, S. Wang, Y. Cheng, L. Li, Z. Yu, and J. Liu, "Uc2: Universal cross-lingual cross-modal vision-and-language pre-training," in CVPR, 2021.

Towards learning a generic agent for vision-and-language navigation via pretraining. W Hao, C Li, X Li, L Carin, J Gao, CVPR. W. Hao, C. Li, X. Li, L. Carin, and J. Gao, "Towards learning a generic agent for vision-and-language navigation via pre- training," in CVPR, 2020.

Xgpt: Cross-modal generative pretraining for image captioning. Q Xia, H Huang, N Duan, D Zhang, L Ji, Z Sui, E Cui, T Bharti, M Zhou, NLPCC. Q. Xia, H. Huang, N. Duan, D. Zhang, L. Ji, Z. Sui, E. Cui, T. Bharti, and M. Zhou, "Xgpt: Cross-modal generative pre- training for image captioning," in NLPCC, 2021.

L Yao, R Huang, L Hou, G Lu, M Niu, H Xu, X Liang, Z Li, X Jiang, C Xu, Filip: Fine-grained interactive languageimage pre-training. arXivL. Yao, R. Huang, L. Hou, G. Lu, M. Niu, H. Xu, X. Liang, Z. Li, X. Jiang, and C. Xu, "Filip: Fine-grained interactive language- image pre-training," arXiv, 2021.

Largescale adversarial training for vision-and-language representation learning. Z Gan, Y.-C Chen, L Li, C Zhu, Y Cheng, J Liu, arXivZ. Gan, Y.-C. Chen, L. Li, C. Zhu, Y. Cheng, and J. Liu, "Large- scale adversarial training for vision-and-language representation learning," arXiv, 2020.

Ernie-vilg: Unified generative pre-training for bidirectional vision-language generation. H Zhang, W Yin, Y Fang, L Li, B Duan, Z Wu, Y Sun, H Tian, H Wu, H Wang, arXivH. Zhang, W. Yin, Y. Fang, L. Li, B. Duan, Z. Wu, Y. Sun, H. Tian, H. Wu, and H. Wang, "Ernie-vilg: Unified generative pre-training for bidirectional vision-language generation," arXiv, 2021.

Kaleido-bert: Vision-language pre-training on fashion domain. M Zhuge, D Gao, D.-P Fan, L Jin, B Chen, H Zhou, M Qiu, L Shao, CVPR. M. Zhuge, D. Gao, D.-P. Fan, L. Jin, B. Chen, H. Zhou, M. Qiu, and L. Shao, "Kaleido-bert: Vision-language pre-training on fashion domain," in CVPR, 2021.

Multimodal token fusion for vision transformers. Y Wang, X Chen, L Cao, W Huang, F Sun, Y Wang, CVPR. Y. Wang, X. Chen, L. Cao, W. Huang, F. Sun, and Y. Wang, "Multimodal token fusion for vision transformers," in CVPR, 2022.

Bridged transformer for vision and point cloud 3d object detection. Y Wang, T Ye, L Cao, W Huang, F Sun, F He, D Tao, CVPR. Y. Wang, T. Ye, L. Cao, W. Huang, F. Sun, F. He, and D. Tao, "Bridged transformer for vision and point cloud 3d object detec- tion," in CVPR, 2022.

Multi-modal fusion transformer for end-to-end autonomous driving. A Prakash, K Chitta, A Geiger, CVPR. A. Prakash, K. Chitta, and A. Geiger, "Multi-modal fusion trans- former for end-to-end autonomous driving," in CVPR, 2021.

Transfusion: Robust lidar-camera fusion for 3d object detection with transformers. X Bai, Z Hu, X Zhu, Q Huang, Y Chen, H Fu, C.-L Tai, CVPR. X. Bai, Z. Hu, X. Zhu, Q. Huang, Y. Chen, H. Fu, and C.-L. Tai, "Transfusion: Robust lidar-camera fusion for 3d object detection with transformers," in CVPR, 2022.

Learning contextual tag embeddings for cross-modal alignment of audio and tags. X Favory, K Drossos, T Virtanen, X Serra, ICASSP. X. Favory, K. Drossos, T. Virtanen, and X. Serra, "Learning contextual tag embeddings for cross-modal alignment of audio and tags," in ICASSP, 2021.

Multi-modal transformer for video retrieval. V Gabeur, C Sun, K Alahari, C Schmid, ECCV. V. Gabeur, C. Sun, K. Alahari, and C. Schmid, "Multi-modal transformer for video retrieval," in ECCV, 2020.

Everything at once -multi-modal fusion transformer for video retrieval. N Shvetsova, B Chen, A Rouditchenko, S Thomas, B Kingsbury, R S Feris, D Harwath, J Glass, H Kuehne, CVPR. N. Shvetsova, B. Chen, A. Rouditchenko, S. Thomas, B. Kings- bury, R. S. Feris, D. Harwath, J. Glass, and H. Kuehne, "Ev- erything at once -multi-modal fusion transformer for video retrieval," in CVPR, 2022.

Multiquery video retrieval. Z Wang, Y Wu, K Narasimhan, O Russakovsky, arXivZ. Wang, Y. Wu, K. Narasimhan, and O. Russakovsky, "Multi- query video retrieval," arXiv, 2022.

End-to-end referring video object segmentation with multimodal transformers. A Botach, E Zheltonozhskii, C Baskin, arXivA. Botach, E. Zheltonozhskii, and C. Baskin, "End-to-end refer- ring video object segmentation with multimodal transformers," arXiv, 2021.

Gilbert: Generative vision-language pre-training for image-text retrieval. W Hong, K Ji, J Liu, J Wang, J Chen, W Chu, SIGIR. W. Hong, K. Ji, J. Liu, J. Wang, J. Chen, and W. Chu, "Gilbert: Generative vision-language pre-training for image-text retrieval," in SIGIR, 2021.

. X Xu, C C Loy, 3d human texture estimation from a single image with transformers," in ICCVX. Xu and C. C. Loy, "3d human texture estimation from a single image with transformers," in ICCV, 2021.

Gps-net: Graph property sensing network for scene graph generation. X Lin, C Ding, J Zeng, D Tao, CVPR. X. Lin, C. Ding, J. Zeng, and D. Tao, "Gps-net: Graph property sensing network for scene graph generation," in CVPR, 2020.

Topic scene graph generation by attention distillation from caption. W Wang, R Wang, X Chen, ICCV. W. Wang, R. Wang, and X. Chen, "Topic scene graph generation by attention distillation from caption," in ICCV, 2021.

Context-aware scene graph generation with seq2seq transformers. Y Lu, H Rai, J Chang, B Knyazev, G Yu, S Shekhar, G W Taylor, M Volkovs, ICCV. Y. Lu, H. Rai, J. Chang, B. Knyazev, G. Yu, S. Shekhar, G. W. Taylor, and M. Volkovs, "Context-aware scene graph generation with seq2seq transformers," in ICCV, 2021.

Jointgt: Graph-text joint representation learning for text generation from knowledge graphs. P Ke, H Ji, Y Ran, X Cui, L Wang, L Song, X Zhu, M Huang, arXivP. Ke, H. Ji, Y. Ran, X. Cui, L. Wang, L. Song, X. Zhu, and M. Huang, "Jointgt: Graph-text joint representation learning for text generation from knowledge graphs," arXiv, 2021.

Target adaptive context aggregation for video scene graph generation. Y Teng, L Wang, Z Li, G Wu, ICCV. Y. Teng, L. Wang, Z. Li, and G. Wu, "Target adaptive context aggregation for video scene graph generation," in ICCV, 2021.

Tvt: Two-view transformer network for video captioning. M Chen, Y Li, Z Zhang, S Huang, in ACML. M. Chen, Y. Li, Z. Zhang, and S. Huang, "Tvt: Two-view trans- former network for video captioning," in ACML, 2018.

Swinbert: End-to-end transformers with sparse attention for video captioning. K Lin, L Li, C.-C Lin, F Ahmed, Z Gan, Z Liu, Y Lu, L Wang, arXivK. Lin, L. Li, C.-C. Lin, F. Ahmed, Z. Gan, Z. Liu, Y. Lu, and L. Wang, "Swinbert: End-to-end transformers with sparse attention for video captioning," arXiv, 2021.

Sketch, ground, and refine: Top-down dense video captioning. C Deng, S Chen, D Chen, Y He, Q Wu, CVPR. C. Deng, S. Chen, D. Chen, Y. He, and Q. Wu, "Sketch, ground, and refine: Top-down dense video captioning," in CVPR, 2021.

Endto-end dense video captioning with parallel decoding. T Wang, R Zhang, Z Lu, F Zheng, R Cheng, P Luo, ICCV. T. Wang, R. Zhang, Z. Lu, F. Zheng, R. Cheng, and P. Luo, "End- to-end dense video captioning with parallel decoding," in ICCV, 2021.

Attention on attention for image captioning. L Huang, W Wang, J Chen, X.-Y. Wei, ICCV. L. Huang, W. Wang, J. Chen, and X.-Y. Wei, "Attention on attention for image captioning," in ICCV, 2019.

X-linear attention networks for image captioning. Y Pan, T Yao, Y Li, T Mei, CVPR. Y. Pan, T. Yao, Y. Li, and T. Mei, "X-linear attention networks for image captioning," in CVPR, 2020.

Causal attention for visionlanguage tasks. X Yang, H Zhang, G Qi, J Cai, CVPR. X. Yang, H. Zhang, G. Qi, and J. Cai, "Causal attention for vision- language tasks," in CVPR, 2021.

Dual-level collaborative transformer for image captioning. Y Luo, J Ji, X Sun, L Cao, Y Wu, F Huang, C.-W Lin, R Ji, arXivY. Luo, J. Ji, X. Sun, L. Cao, Y. Wu, F. Huang, C.-W. Lin, and R. Ji, "Dual-level collaborative transformer for image caption- ing," arXiv, 2021.

Towards accurate text-based image captioning with content diversity exploration. G Xu, S Niu, M Tan, Y Luo, Q Du, Q Wu, CVPR. G. Xu, S. Niu, M. Tan, Y. Luo, Q. Du, and Q. Wu, "Towards accurate text-based image captioning with content diversity ex- ploration," in CVPR, 2021.

Neural speech synthesis with transformer network. N Li, S Liu, Y Liu, S Zhao, M Liu, AAAIN. Li, S. Liu, Y. Liu, S. Zhao, and M. Liu, "Neural speech synthesis with transformer network," in AAAI, 2019.

M Ding, Z Yang, W Hong, W Zheng, C Zhou, D Yin, J Lin, X Zou, Z Shao, H Yang, Cogview: Mastering text-toimage generation via transformers. arXivM. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin, X. Zou, Z. Shao, H. Yang et al., "Cogview: Mastering text-to- image generation via transformers," arXiv, 2021.

Clip-forge: Towards zero-shot text-to-shape generation. A Sanghi, H Chu, J G Lambourne, Y Wang, C.-Y Cheng, M Fumero, arXivA. Sanghi, H. Chu, J. G. Lambourne, Y. Wang, C.-Y. Cheng, and M. Fumero, "Clip-forge: Towards zero-shot text-to-shape generation," arXiv, 2021.

Dance revolution: Long-term dance generation with music via curriculum learning. R Huang, H Hu, W Wu, K Sawada, M Zhang, D Jiang, arXivR. Huang, H. Hu, W. Wu, K. Sawada, M. Zhang, and D. Jiang, "Dance revolution: Long-term dance generation with music via curriculum learning," arXiv, 2020.

Learning to generate scene graph from natural language supervision. Y Zhong, J Shi, J Yang, C Xu, Y Li, ICCV. Y. Zhong, J. Shi, J. Yang, C. Xu, and Y. Li, "Learning to generate scene graph from natural language supervision," in ICCV, 2021.

Dynamic graph representation learning for video dialog via multi-modal shuffled transformers. S Geng, P Gao, M Chatterjee, C Hori, J Le Roux, Y Zhang, H Li, A Cherian, AAAI. S. Geng, P. Gao, M. Chatterjee, C. Hori, J. Le Roux, Y. Zhang, H. Li, and A. Cherian, "Dynamic graph representation learn- ing for video dialog via multi-modal shuffled transformers," in AAAI, 2021.

Visqa: X-raying vision and language reasoning in transformers. T Jaunet, C Kervadec, R Vuillemot, G Antipov, M Baccouche, C Wolf, TVCGT. Jaunet, C. Kervadec, R. Vuillemot, G. Antipov, M. Baccouche, and C. Wolf, "Visqa: X-raying vision and language reasoning in transformers," TVCG, 2021.

Vx2text: End-to-end learning of video-based text generation from multimodal inputs. X Lin, G Bertasius, J Wang, S.-F Chang, D Parikh, L Torresani, CVPR. X. Lin, G. Bertasius, J. Wang, S.-F. Chang, D. Parikh, and L. Tor- resani, "Vx2text: End-to-end learning of video-based text gener- ation from multimodal inputs," in CVPR, 2021.

M6: Multi-modality-to-multi-modality multitask mega-transformer for unified pretraining. J Lin, R Men, A Yang, C Zhou, Y Zhang, P Wang, J Zhou, J Tang, H Yang, KDD. J. Lin, R. Men, A. Yang, C. Zhou, Y. Zhang, P. Wang, J. Zhou, J. Tang, and H. Yang, "M6: Multi-modality-to-multi-modality multitask mega-transformer for unified pretraining," in KDD, 2021.

Audio-visual integration in multimodal communication. T Chen, R R Rao, Proceedings of the IEEE. the IEEET. Chen and R. R. Rao, "Audio-visual integration in multimodal communication," Proceedings of the IEEE, 1998.

Audio-visual scene analysis with self-supervised multisensory features. A Owens, A A Efros, ECCV. A. Owens and A. A. Efros, "Audio-visual scene analysis with self-supervised multisensory features," in ECCV, 2018.

Probing inter-modality: Visual parsing with self-attention for vision-and-language pre-training. H Xue, Y Huang, B Liu, H Peng, J Fu, H Li, J Luo, NeurIPS. H. Xue, Y. Huang, B. Liu, H. Peng, J. Fu, H. Li, and J. Luo, "Probing inter-modality: Visual parsing with self-attention for vision-and-language pre-training," NeurIPS, 2021.

The right to talk: An audio-visual transformer approach. T.-D Truong, C N Duong, H A Pham, B Raj, N Le, K Luu, ICCV. T.-D. Truong, C. N. Duong, H. A. Pham, B. Raj, N. Le, K. Luu et al., "The right to talk: An audio-visual transformer approach," in ICCV, 2021.

Multispeech: Multi-speaker text to speech with transformer. M Chen, X Tan, Y Ren, J Xu, H Sun, S Zhao, T Qin, T.-Y Liu, arXivM. Chen, X. Tan, Y. Ren, J. Xu, H. Sun, S. Zhao, T. Qin, and T.-Y. Liu, "Multispeech: Multi-speaker text to speech with trans- former," arXiv, 2020.

Coot: Cooperative hierarchical transformer for video-text representation learning. S Ging, M Zolfaghari, H Pirsiavash, T Brox, arXivS. Ging, M. Zolfaghari, H. Pirsiavash, and T. Brox, "Coot: Co- operative hierarchical transformer for video-text representation learning," arXiv, 2020.

Support-set bottlenecks for videotext representation learning. M Patrick, P.-Y Huang, Y Asano, F Metze, A Hauptmann, J Henriques, A Vedaldi, arXivM. Patrick, P.-Y. Huang, Y. Asano, F. Metze, A. Hauptmann, J. Henriques, and A. Vedaldi, "Support-set bottlenecks for video- text representation learning," arXiv, 2020.

Masking modalities for cross-modal video retrieval. V Gabeur, A Nagrani, C Sun, K Alahari, C Schmid, WACVV. Gabeur, A. Nagrani, C. Sun, K. Alahari, and C. Schmid, "Masking modalities for cross-modal video retrieval," in WACV, 2022.

Video object grounding using semantic roles in language description. A Sadhu, K Chen, R Nevatia, CVPR. A. Sadhu, K. Chen, and R. Nevatia, "Video object grounding using semantic roles in language description," in CVPR, 2020.

Explainable semantic space by grounding language to vision with cross-modal contrastive learning. Y Zhang, M Choi, K Han, Z Liu, NeurIPS. Y. Zhang, M. Choi, K. Han, and Z. Liu, "Explainable semantic space by grounding language to vision with cross-modal con- trastive learning," NeurIPS, 2021.

End-to-end multi-modal video temporal grounding. Y.-W Chen, Y.-H Tsai, M.-H Yang, NeurIPS. Y.-W. Chen, Y.-H. Tsai, and M.-H. Yang, "End-to-end multi-modal video temporal grounding," NeurIPS, 2021.

Multi-modal dynamic graph transformer for visual grounding. S Chen, B Li, CVPR. S. Chen and B. Li, "Multi-modal dynamic graph transformer for visual grounding," in CVPR, 2022.

Tubedetr: Spatio-temporal video grounding with transformers. A Yang, A Miech, J Sivic, I Laptev, C Schmid, CVPR. A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid, "Tubedetr: Spatio-temporal video grounding with transformers," in CVPR, 2022.

Videoclip: Contrastive pre-training for zero-shot video-text understanding. H Xu, G Ghosh, P.-Y Huang, D Okhonko, A Aghajanyan, F Metze, L Zettlemoyer, C Feichtenhofer, arXivH. Xu, G. Ghosh, P.-Y. Huang, D. Okhonko, A. Aghajanyan, F. Metze, L. Zettlemoyer, and C. Feichtenhofer, "Videoclip: Con- trastive pre-training for zero-shot video-text understanding," arXiv, 2021.

Less is more: Clipbert for video-and-language learning via sparse sampling. J Lei, L Li, L Zhou, Z Gan, T L Berg, M Bansal, J Liu, CVPR. J. Lei, L. Li, L. Zhou, Z. Gan, T. L. Berg, M. Bansal, and J. Liu, "Less is more: Clipbert for video-and-language learning via sparse sampling," in CVPR, 2021.

Taco: Token-aware cascade contrastive learning for video-text alignment. J Yang, Y Bisk, J Gao, ICCV. J. Yang, Y. Bisk, and J. Gao, "Taco: Token-aware cascade con- trastive learning for video-text alignment," in ICCV, 2021.

Align and prompt: Video-and-language pre-training with entity prompts. D Li, J Li, H Li, J C Niebles, S C Hoi, arXivD. Li, J. Li, H. Li, J. C. Niebles, and S. C. Hoi, "Align and prompt: Video-and-language pre-training with entity prompts," arXiv, 2021.

Clip4clip: An empirical study of clip for end to end video clip retrieval. H Luo, L Ji, M Zhong, Y Chen, W Lei, N Duan, T Li, arXivH. Luo, L. Ji, M. Zhong, Y. Chen, W. Lei, N. Duan, and T. Li, "Clip4clip: An empirical study of clip for end to end video clip retrieval," arXiv, 2021.

H Fang, P Xiong, L Xu, Y Chen, Clip2video: Mastering video-text retrieval via image clip. arXivH. Fang, P. Xiong, L. Xu, and Y. Chen, "Clip2video: Mastering video-text retrieval via image clip," arXiv, 2021.

Clip-it! languageguided video summarization. M Narasimhan, A Rohrbach, T Darrell, NeurIPS. M. Narasimhan, A. Rohrbach, and T. Darrell, "Clip-it! language- guided video summarization," NeurIPS, 2021.

Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, ACM Computing Surveys. P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, "Pre- train, prompt, and predict: A systematic survey of prompting methods in natural language processing," ACM Computing Sur- veys, 2023.

Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly. Y Xian, C H Lampert, B Schiele, Z Akata, TPAMIY. Xian, C. H. Lampert, B. Schiele, and Z. Akata, "Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly," TPAMI, 2018.

Open-vocabulary object detection via vision and language knowledge distillation. X Gu, T.-Y Lin, W Kuo, Y Cui, arXivX. Gu, T.-Y. Lin, W. Kuo, and Y. Cui, "Open-vocabulary object detection via vision and language knowledge distillation," arXiv, 2021.

Xlxmert: Paint, caption and answer questions with multi-modal transformers. J Cho, J Lu, D Schwenk, H Hajishirzi, A Kembhavi, EMNLP. J. Cho, J. Lu, D. Schwenk, H. Hajishirzi, and A. Kembhavi, "X- lxmert: Paint, caption and answer questions with multi-modal transformers," in EMNLP, 2020.

E2e-vlp: End-to-end vision-language pre-training enhanced by visual learning. H Xu, M Yan, C Li, B Bi, S Huang, W Xiao, F Huang, arXivH. Xu, M. Yan, C. Li, B. Bi, S. Huang, W. Xiao, and F. Huang, "E2e-vlp: End-to-end vision-language pre-training enhanced by visual learning," arXiv, 2021.

Supervising the transfer of reasoning patterns in vqa. C Kervadec, C Wolf, G Antipov, M Baccouche, M Nadri, arXivC. Kervadec, C. Wolf, G. Antipov, M. Baccouche, and M. Nadri, "Supervising the transfer of reasoning patterns in vqa," arXiv, 2021.

C Kervadec, T Jaunet, G Antipov, M Baccouche, R Vuillemot, C Wolf, How transferable are reasoning patterns in vqa?" in CVPR. C. Kervadec, T. Jaunet, G. Antipov, M. Baccouche, R. Vuillemot, and C. Wolf, "How transferable are reasoning patterns in vqa?" in CVPR, 2021.

Integrating multimodal information in large pretrained transformers. W Rahman, M K Hasan, S Lee, A Zadeh, C Mao, L.-P Morency, E Hoque, ACL. W. Rahman, M. K. Hasan, S. Lee, A. Zadeh, C. Mao, L.-P. Morency, and E. Hoque, "Integrating multimodal information in large pretrained transformers," in ACL, 2020.

From multimodal to unimodal attention in transformers using knowledge distillation. D Agarwal, T Agrawal, L M Ferrari, F Bremond, AVSSD. Agarwal, T. Agrawal, L. M. Ferrari, and F. Bremond, "From multimodal to unimodal attention in transformers using knowl- edge distillation," in AVSS, 2021.

Q Li, B Gong, Y Cui, D Kondratyuk, X Du, M.-H Yang, M Brown, Towards a unified foundation model: Jointly pretraining transformers on unpaired images and text. arXivQ. Li, B. Gong, Y. Cui, D. Kondratyuk, X. Du, M.-H. Yang, and M. Brown, "Towards a unified foundation model: Jointly pre- training transformers on unpaired images and text," arXiv, 2021.

M3p: Learning universal representations via multitask multilingual multimodal pre-training. M Ni, H Huang, L Su, E Cui, T Bharti, L Wang, D Zhang, N Duan, CVPR. M. Ni, H. Huang, L. Su, E. Cui, T. Bharti, L. Wang, D. Zhang, and N. Duan, "M3p: Learning universal representations via multitask multilingual multimodal pre-training," in CVPR, 2021.

Thinking fast and slow: Efficient text-to-visual retrieval with transformers. A Miech, J.-B Alayrac, I Laptev, J Sivic, A Zisserman, CVPR. A. Miech, J.-B. Alayrac, I. Laptev, J. Sivic, and A. Zisserman, "Thinking fast and slow: Efficient text-to-visual retrieval with transformers," in CVPR, 2021.

Cookie: Contrastive cross-modal knowledge sharing pre-training for visionlanguage representation. K Wen, J Xia, Y Huang, L Li, J Xu, J Shao, ICCV. K. Wen, J. Xia, Y. Huang, L. Li, J. Xu, and J. Shao, "Cookie: Con- trastive cross-modal knowledge sharing pre-training for vision- language representation," in ICCV, 2021.

Parameter efficient multimodal transformers for video representation learning. S Lee, Y Yu, G Kim, T Breuel, J Kautz, Y Song, arXivS. Lee, Y. Yu, G. Kim, T. Breuel, J. Kautz, and Y. Song, "Param- eter efficient multimodal transformers for video representation learning," arXiv, 2020.

Multi-stage pre-training over simplified multimodal pre-training models. T Liu, F Feng, X Wang, arXivT. Liu, F. Feng, and X. Wang, "Multi-stage pre-training over simplified multimodal pre-training models," arXiv, 2021.

Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. Y Li, F Liang, L Zhao, Y Cui, W Ouyang, J Shao, F Yu, J Yan, arXivY. Li, F. Liang, L. Zhao, Y. Cui, W. Ouyang, J. Shao, F. Yu, and J. Yan, "Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm," arXiv, 2021.

Playing lottery tickets with vision and language. Z Gan, Y.-C Chen, L Li, T Chen, Y Cheng, S Wang, J Liu, arXivZ. Gan, Y.-C. Chen, L. Li, T. Chen, Y. Cheng, S. Wang, and J. Liu, "Playing lottery tickets with vision and language," arXiv, 2021.

Pipetransformer: Automated elastic pipelining for distributed training of large-scale models. C He, S Li, M Soltanolkotabi, S Avestimehr, ICML. C. He, S. Li, M. Soltanolkotabi, and S. Avestimehr, "Pipetrans- former: Automated elastic pipelining for distributed training of large-scale models," in ICML, 2021.

Flashattention: Fast and memory-efficient exact attention with io-awareness. T Dao, D Fu, S Ermon, A Rudra, C Ré, NeurIPS. T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Ré, "Flashattention: Fast and memory-efficient exact attention with io-awareness," NeurIPS, 2022.

Generating long sequences with sparse transformers. R Child, S Gray, A Radford, I Sutskever, arXivR. Child, S. Gray, A. Radford, and I. Sutskever, "Generating long sequences with sparse transformers," arXiv, 2019.

Long-short transformer: Efficient transformers for language and vision. C Zhu, W Ping, C Xiao, M Shoeybi, T Goldstein, A Anandkumar, B Catanzaro, NeurIPSC. Zhu, W. Ping, C. Xiao, M. Shoeybi, T. Goldstein, A. Anandku- mar, and B. Catanzaro, "Long-short transformer: Efficient trans- formers for language and vision," in NeurIPS, 2021.

Multiview transformers for video recognition. S Yan, X Xiong, A Arnab, Z Lu, M Zhang, C Sun, C Schmid, CVPR. S. Yan, X. Xiong, A. Arnab, Z. Lu, M. Zhang, C. Sun, and C. Schmid, "Multiview transformers for video recognition," in CVPR, 2022.

Rethinking the value of transformer components. W Wang, Z Tu, COLING. W. Wang and Z. Tu, "Rethinking the value of transformer com- ponents," in COLING, 2020.

Are multimodal transformers robust to missing modality?" in CVPR. M Ma, J Ren, L Zhao, D Testuggine, X Peng, M. Ma, J. Ren, L. Zhao, D. Testuggine, and X. Peng, "Are multimodal transformers robust to missing modality?" in CVPR, 2022.

Robust visual reasoning via language guided neural module networks. A Akula, V Jampani, S Changpinyo, S.-C Zhu, NeurIPS. A. Akula, V. Jampani, S. Changpinyo, and S.-C. Zhu, "Robust visual reasoning via language guided neural module networks," NeurIPS, 2021.

Words aren't enough, their order matters: On the robustness of grounding visual referring expressions. A R Akula, S Gella, Y Al-Onaizan, S.-C Zhu, S Reddy, arXivA. R. Akula, S. Gella, Y. Al-Onaizan, S.-C. Zhu, and S. Reddy, "Words aren't enough, their order matters: On the robustness of grounding visual referring expressions," arXiv, 2020.

A closer look at the robustness of vision-and-language pre-trained models. L Li, Z Gan, J Liu, arXivL. Li, Z. Gan, and J. Liu, "A closer look at the robustness of vision-and-language pre-trained models," arXiv, 2020.

Domain-robust vqa with diverse datasets and methods but no target labels. M Zhang, T Maidment, A Diab, A Kovashka, R Hwa, CVPR. M. Zhang, T. Maidment, A. Diab, A. Kovashka, and R. Hwa, "Domain-robust vqa with diverse datasets and methods but no target labels," in CVPR, 2021.

Contrast and classify: Training robust vqa models. Y Kant, A Moudgil, D Batra, D Parikh, H , ICCV. Y. Kant, A. Moudgil, D. Batra, D. Parikh, and H. Agrawal, "Contrast and classify: Training robust vqa models," in ICCV, 2021.

Omninet: A unified architecture for multi-modal multi-task learning. S Pramanik, P Agrawal, A Hussain, arXivS. Pramanik, P. Agrawal, and A. Hussain, "Omninet: A unified architecture for multi-modal multi-task learning," arXiv, 2019.

Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. P Wang, A Yang, R Men, J Lin, S Bai, Z Li, J Ma, C Zhou, J Zhou, H Yang, arXivP. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and H. Yang, "Unifying architectures, tasks, and modal- ities through a simple sequence-to-sequence learning frame- work," arXiv, 2022.

Omnivore: A single model for many visual modalities. R Girdhar, M Singh, N Ravi, L Van Der Maaten, A Joulin, I Misra, arXivR. Girdhar, M. Singh, N. Ravi, L. van der Maaten, A. Joulin, and I. Misra, "Omnivore: A single model for many visual modalities," arXiv, 2022.

Behind the scene: Revealing the secrets of pre-trained vision-and-language models. J Cao, Z Gan, Y Cheng, L Yu, Y.-C Chen, J Liu, ECCV. J. Cao, Z. Gan, Y. Cheng, L. Yu, Y.-C. Chen, and J. Liu, "Behind the scene: Revealing the secrets of pre-trained vision-and-language models," in ECCV, 2020.

Decoupling the role of data, attention, and losses in multimodal transformers. L A Hendricks, J Mellor, R Schneider, J.-B Alayrac, A Nematzadeh, TACLL. A. Hendricks, J. Mellor, R. Schneider, J.-B. Alayrac, and A. Ne- matzadeh, "Decoupling the role of data, attention, and losses in multimodal transformers," TACL, 2021.

Probing image-language transformers for verb understanding. L A Hendricks, A Nematzadeh, arXivL. A. Hendricks and A. Nematzadeh, "Probing image-language transformers for verb understanding," arXiv, 2021.

Vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers. S Frank, E Bugliarello, D Elliott, arXivS. Frank, E. Bugliarello, and D. Elliott, "Vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers," arXiv, 2021.

Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers. H Chefer, S Gur, L Wolf, arXivH. Chefer, S. Gur, and L. Wolf, "Generic attention-model explain- ability for interpreting bi-modal and encoder-decoder transform- ers," arXiv, 2021.

Valse: A task-independent benchmark for vision and language models centered on linguistic phenomena. L Parcalabescu, M Cafagna, L Muradjan, A Frank, I Calixto, A Gatt, arXivL. Parcalabescu, M. Cafagna, L. Muradjan, A. Frank, I. Calixto, and A. Gatt, "Valse: A task-independent benchmark for vision and language models centered on linguistic phenomena," arXiv, 2021.

Vlchecklist: Evaluating pre-trained vision-language models with objects, attributes and relations. T Zhao, T Zhang, M Zhu, H Shen, K Lee, X Lu, J Yin, arXivT. Zhao, T. Zhang, M. Zhu, H. Shen, K. Lee, X. Lu, and J. Yin, "Vl- checklist: Evaluating pre-trained vision-language models with objects, attributes and relations," arXiv, 2022.

Vl-interpret: An interactive visualization tool for interpreting vision-language transformers. E Aflalo, M Du, S.-Y Tseng, Y Liu, C Wu, N Duan, V Lal, CVPR. E. Aflalo, M. Du, S.-Y. Tseng, Y. Liu, C. Wu, N. Duan, and V. Lal, "Vl-interpret: An interactive visualization tool for interpreting vision-language transformers," in CVPR, 2022.

Slip: Self-supervision meets language-image pre-training. N Mu, A Kirillov, D Wagner, S Xie, arXivN. Mu, A. Kirillov, D. Wagner, and S. Xie, "Slip: Self-supervision meets language-image pre-training," arXiv, 2021.

Vlm: Task-agnostic videolanguage model pre-training for video understanding. H Xu, G Ghosh, P.-Y Huang, P Arora, M Aminzadeh, C Feichtenhofer, F Metze, L Zettlemoyer, arXivH. Xu, G. Ghosh, P.-Y. Huang, P. Arora, M. Aminzadeh, C. Feicht- enhofer, F. Metze, and L. Zettlemoyer, "Vlm: Task-agnostic video- language model pre-training for video understanding," arXiv, 2021.

Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. J Li, D Li, C Xiong, S Hoi, arXivJ. Li, D. Li, C. Xiong, and S. Hoi, "Blip: Bootstrapping language- image pre-training for unified vision-language understanding and generation," arXiv, 2022.

Vinvl: Revisiting visual representations in visionlanguage models. P Zhang, X Li, X Hu, J Yang, L Zhang, L Wang, Y Choi, J Gao, CVPR. Tsinghua University. Previously ; Department of Engineering Science at the University of OxfordPeng Xu is a lecturer in the Department of Electronic EngineeringP. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi, and J. Gao, "Vinvl: Revisiting visual representations in vision- language models," in CVPR, 2021. Peng Xu is a lecturer in the Department of Elec- tronic Engineering, Tsinghua University. Previ- ously, he was a postdoctoral research assistant in the Department of Engineering Science at the University of Oxford.

Xiatian Zhu is a Senior Lecturer at the Surrey Institute for People-Centred Artificial Intelligence, and Centre for Vision, Speech and Signal Processing (CVSSP). Faculty of Engineering and Physical Sciences, University of SurreyXiatian Zhu is a Senior Lecturer at the Surrey In- stitute for People-Centred Artificial Intelligence, and Centre for Vision, Speech and Signal Pro- cessing (CVSSP), Faculty of Engineering and Physical Sciences, University of Surrey.

Clifton is a Professor of Clinical Machine Learning and leads the Computational Health Informatics (CHI) Lab in the. A David, Department of Engineering Science at the University of OxfordDavid A. Clifton is a Professor of Clinical Ma- chine Learning and leads the Computational Health Informatics (CHI) Lab in the Department of Engineering Science at the University of Ox- ford.