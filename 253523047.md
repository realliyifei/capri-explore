# Graphical Abstract A survey of the European Open Science Cloud services for ex- panding the capacity and capabilities of multidisciplinary scientific applications

CorpusID: 253523047
 
tags: #Engineering, #Environmental_Science, #Computer_Science

URL: [https://www.semanticscholar.org/paper/885a2eae0359c6f76235dd73b4222a1dbb527e05](https://www.semanticscholar.org/paper/885a2eae0359c6f76235dd73b4222a1dbb527e05)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Graphical Abstract A survey of the European Open Science Cloud services for ex- panding the capacity and capabilities of multidisciplinary scientific applications


Amanda Calatrava 
Hernán Asorey 
Jan Astalos 
Alberto Azevedo 
Francesco Benincasa 
Ignacio Blanquer 
Martin Bobak 
Francisco Brasileiro 
Laia Codó 
Laura Del Cano 
Borja Esteban 
Meritxell Ferret 
Josef Handl 
Tobias Kerzen-Macher 
Valentin Kozlov 
Aleš Křenek 
Ricardo Martins 
Manuel Pavesio 
Antonio Juan Rubio-Montero 
Juan Sánchez-Ferrero 

Instituto de Instrumentacón para Imagen Molecular (I3M)
Universitat Politècnica de València
46022ValenciaSpain


Federal University of Campina Grande (UFCG)
Campina GrandeBrazil


Laboratório Nacional de Engenharia Civil (LNEC)
LisbonPortugal


Institute for Meteorology and Climate Research-Atmospheric Trace Gases and Remote Sensing (IMK-ASF)
Karlsruhe Institute of Technology (KIT)
KarlsruheGermany


Steinbuch Centre for Computing (SCC)
Karlsruhe Institute of Technology (KIT)
KarlsruheGermany


Institute of Informatics
Slovak Academy of Sciences
BratislavaSlovakia


Centro Nacional de Biotecnologia
CSIC
MadridSpain


BrnoCzech Republic


Barcelona Supercomputing Center (BSC)
BarcelonaSpain


Centro de Investigaciones Energéticas
Medioambientales y Tecnológicas (CIEMAT)
Av. Complutense 4028040Madrid, MadridSpain


Medical Physics Department
Centro Atómico Bariloche
Comisión Nacional de Energía Atómica (CNEA)
San Carlos de Bariloche
8400Río NegroArgentina


Instituto de Tecnología en Detección y Astropartículas (ITeDA
CNEA/CONICET/UNSAM)
Centro Atómico Constituyentes
Villa Maipú1450Buenos AiresArgentina


Control, Observation and tracking systems
Space Management Area (Indra Sistemas SA)
Ctra. de Loeches, 928850Torrejón de Ardoz, MadridSpain

Graphical Abstract A survey of the European Open Science Cloud services for ex- panding the capacity and capabilities of multidisciplinary scientific applications



• The European Open Science Cloud (EOSC) is an initiative aiming to offer a virtual environment for open access to services to store, share, process and reuse research data and other research digital objects, such as software.

• The adaptation, improvement and quality assessment of thematic services on a Federated Data Infrastructure strongly aligns with the objectives of EOSC.

• A key factor for the success of EOSC is performance and acknowledgement by the users.

• We present an analysis of the adoption of services from the EOSC catalogue that provide feedback on the usability and relevance of the model.

• The ten thematic services analysed provide different experiences on enhancing community-oriented data services to be released in an Open Science environment.

Open Science is a paradigm in which scientific data, procedures, tools and results are shared transparently and reused by society as a whole. The initiative known as the European Open Science Cloud (EOSC) is an effort in Europe to provide an open, trusted, virtual and federated computing environment to execute scientific applications, and to store, share and re-use research data across borders and scientific disciplines. Additionally, scientific services are becoming increasingly data intensive, not only in terms of computationally intensive tasks but also in terms of storage resources. Computing paradigms such as High Performance Computing (HPC) and Cloud Computing are applied to e-science applications to meet these demands. However, adapting applications and services to these paradigms is not a trivial task, commonly requiring a deep knowledge of the underlying technologies, which often constitutes a barrier for its uptake by scientists in general. In this context, EOSC-SYNERGY, a collaborative project involving more than 20 institutions from eight European countries pooling their knowledge and experience to enhance EOSC's capabilities and capacities, aims to bring EOSC closer to the scientific communities. This article provides a summary analysis of the adaptations made in the ten thematic services of EOSC-SYNERGY to embrace this paradigm. These services are grouped into four categories: Earth Observation, Environment, Biomedicine, and Astrophysics. The analysis will lead to the identification of commonalities, best practices and common requirements, regardless of the thematic area of the service. Experience gained from the thematic services could be transferred to new services for the adoption of the EOSC ecosystem framework.

Keywords: Open Science, Cloud Computing, federated infrastructure, multidisciplinary, EOSC


## Introduction

e-Science studies, enacts, and improves the ongoing process of innovation in computationally-intensive or data-intensive research methods [1]; typically this is carried out collaboratively, often using distributed infrastructures. Open Science [2] is the practice of science in such a way that others can collaborate and contribute, with research data, lab notes and other research processes freely available, under terms that enable reuse, redistribution and reproduction of the research and its underlying data and methods.

Scientific applications place higher demands on computing power every

year. The need for large-scale computing resources, including specific hardware needs such as GPUs, together with the increasing demand for storage resources due to the large amount of data generated by these types of applications, are a challenge for both researchers and computer scientists. e-Science makes use of e-Infrastructures [3] which are collaborative virtual environments that provide digital services and tools to meet this resource demand. e-Infrastructures are based on distributed backends ranging from High-Performance Computing to Cloud Computing. However, adapting already existing software applications to these paradigms is not trivial [4]. This process commonly requires an in-depth knowledge of the underlying technologies to truly take advantage of their benefits, as it usually requires refactoring the architecture of the application. This adaption becomes even more challenging on the Cloud computing paradigm, due to the complexity of virtualization and elasticity behaviour together with the vast range of services available and the variety of resource types. This fact can be a barrier for scientists and researchers from fields outside of computer science to adopt these solutions, thus complicating the path of research innovation.

Another obstacle in adopting the e-Science and Open Science paradigms is the fulfilment of the FAIR [5] (Findability, Accessibility, Interoperability and Reusability) principles, which imply permanent and discoverable identifiers for fully annotated data and metadata. Specifically, in Europe, we can find the European Open Science Cloud (EOSC) [6], a European initiative co-funded by the European Commission that aims to facilitate the deployment and consolidation of an open, trusted, virtual, federated environment in Europe to store, share and re-use research data across borders and scientific disciplines promoting open science practices and providing access to a rich array of related services.

As part of this initiative, the EOSC-Synergy [7] project aims to increase the uptake of EOSC through the capacity and capability building using the experience, efforts and resources of national publicly funded digital infrastructures. The project has identified ten thematic services in four scientific domains (Earth Observation, Environment, Bio-medicine, and Astrophysics) to increase the uptake of EOSC services. These thematic services are heterogeneous and cover a wide range of requirements, maturity levels, user targets and usage models.

The ten thematic services also provide helpful best practices for future new services to be developed or adapted to this environment, as they address challenges on federated Authentication and Authorization Infrastructures (AAI), elastic data processing, interoperability with data infrastructures, metadata management and accounting, that apply to many other applications in same or different scientific domains. The analysis of the limitations of these thematic services along with the design of the new architecture of the thematic services will be of particular interest for new scientific applications that want to embrace this paradigm.

In this work, we present each of the thematic services of EOSC-Synergy and then analyse the gaps and bottlenecks in terms of authentication and authorisation services, resource provisioning, workload management, and data storage. This section is followed by the analysis of the different tools and services provided in the EOSC Marketplace [8] that can meet the needs of the thematic services. We then present the adoption of services, tools and technologies used by the thematic services to address their needs, and then model a generic scientific application that can be the starting point for new scientific thematic services. Next, to better illustrate the work done in the adoption of tools and services by the thematic services, we discuss the adoption issues in the service instantiation section. Finally, we analyze the state of the art to identify the most important work related to the implementation of thematic services in the e-Science paradigm, and conclude the paper with the main observations.


## Thematic Services in EOSC Synergy

In this section we briefly describe each one of the ten thematic services of EOSC Synergy, grouped by scientific discipline. Moreover, the expected outcome of the integration with EOSC Synergy for each one of the thematic services are pointed out in this section.


### Thematic Services in Earth Observation

In the field of Earth Observation, three thematic services deal with analysing large satellite imagery, from monitoring coastal changes and inundations, to estimating forest masses and crops. They are addressing different types of targets. Specifically, the three services are:

• WORSICA (Water Monitoring Sentinel Cloud Platform) [9,10]: A service for the detection of water using satellites, Unmanned Aerial Vehicles & in-situ data. WORSICA can be used for coastline detection, inland water bodies detection and water leak detection on irrigation networks. WORSICA aims at integrating multiple-source remote sensing and in-situ data to determine the presence of water in coastal and inland areas. WORSICA enables the research communities to generate maps of water presence and water delimitation lines in coastal and inland regions. These products can be useful for emergency and planning methodologies in case of inundations or reservoir leaks. In the frame of EOSC, the service will be scaled up to a European level to reach all interested research communities.

• SAPS (Surface Energy Balance Automated Processing Service) [11]: Used to estimate Evapotranspiration and other environmental data that can be applied, for example, on water management and the analysis of the evolution of forest masses and crops. SAPS allows the integration of Energy Balance algorithms to compute the estimations that are of special interest for researchers in Agriculture Engineering and Environment. These algorithms can be used to increase the knowledge on the impact of human and environmental actions on vegetation, leading to better forest management and analysis of risks. SAPS is being developed in Brazil, but with the adoption of EOSC services it is expected to facilitate European scientists to exploit the evapotranspiration estimation services from remote sensing imagery.

• G-Core (Acquisition, cataloguing and processing EOS data) [12], [13], [14]: G-Core is a production-ready technology used as a service at ESA's and national programs that provides a Data Manager for spatial and non-spatial purposes and a framework for third-party processors. G-Core is a service for the acquisition, storing, cataloguing and processing data from several Earth Observing System (EOS) missions. Its two main functionalities are: i) a Data Manager for spatial and non-spatial purposes; and ii) a Processing framework to host external processors developed by third parties to generate added value products based on Satellite imagery. The main goal of its integration in the EOSC ecosystem is to offer the service as a Payload Data Ground Segment (PDGS) in the cloud for future ground segment space missions or as a processing framework to plug in different processors that can make use of the Copernicus resources or private data in order to produce different levels of products to be delivered to the users. Thus, the expected impact of the adaptation of the service is to democratize the usage of Earth Ob-servation (EO) data out of the scope of nominal fields. It will help to define new products and services mixing Earth Observation data with other types of data for scientific and social environments.


### Thematic Services in Biomedicine

In this area, the thematic services cover the benchmarking of Genomic data processing tools and the processing of Cryo-electron microscopy imaging. The services are:

• SCIPION (CryoEM data processing for Structural Biology [15]): Cryo-Electron Microscopy Service is an image processing framework used to obtain 3D maps of macromolecular complexes using cryo Electron Microscopy. It has been developed as a plugin-based workflow management system that integrates many important software packages available in the field. The integration of Scipion with Cloud services allows users from the Instruct Research Infrastructure to deploy a dynamic cluster in the cloud to keep processing the data acquired at an Electron Microscopy facility. This cluster has all cryoem packages and software needed to obtain a 3D structure and is powered by EOSC compute resources on the back-end. This means that scientists with minimal computational background (or compute resources of their own) can access the latest tools as well as powerful computational resources to obtain a refined 3D structure to be published and shared with the community.

• OpenEBench [16] (ELIXIR [17] benchmarking and technical monitoring platform): Used to evaluate Life Sciences research software, OpenEBench is an observatory for software quality based on the automated monitoring of FAIR for research software metrics and indicators. The OpenEBench platform supports both the technical monitoring of scientific software and scientific benchmarking activities carried out by Life Sciences Communities. Its architecture has three different engagement levels that allow communities at different maturity stages to make use of the platform. It also connects with ELIXIR Core Data Resources and Deposition databases to use data needed by the Scientific Communities activities. The expected impact of the integration with EOSC services is that Life Science researchers will have semantically annotated, up-to-date collections of benchmarked analytical workflows and tools, organized by scientific communities for specific topics, which can be deployed across heterogeneous systems.


### Thematic Services in Astrophysics

In Astrophysics, the LAGO thematic service sets up a European service for the Latin American Giant Observatory.

• LAGO, the Latin American Giant Observatory [18,19], is an extended cosmic ray observatory, consisting of a wide network of water Cherenkov detectors currently deployed at 10 countries in Latin America, from the south of Mexico to the Antarctic Peninsula. The geographic distribution of LAGO allows the realization of diverse astrophysics studies at a regional scale. LAGO is mainly oriented to perform basic research focusing on three main areas: high energy phenomena, the measurement of atmospheric radiation at ground level and space weather and climate monitoring. All the LAGO analyses are supported by data-intensive computational frameworks, that integrate different simulations tools with own designed data-analysis codes to determine in a very precise way, the signals measured or expected at any detector of any type, in any particular site around the World, and under realistic atmospheric and geomagnetic time-evolving conditions. The final purpose of the LAGO Thematic Service [20] is to enable the universal profit and contribution of this research, within and outside LAGO Collaboration, through a sustainable Virtual Observatory and standardised computational model.


### Thematic Services in the Environmental domain

Finally, in the area of Environment, the fourth group of thematic services include sand and dust storm forecasting, untargeted mass-spectrometry analysis for toxins, water network distribution simulation and the monitoring of stratospheric ozone in climate models. Specifically, these services are as follows:

• SDS-WAS (A Service related to the mineral dust forecast) [21,22]: SDS-WAS is a service that aims at improving capabilities for more reliable sand and dust storm (SDS) forecasts. It supports institutional entities to warn about possible dust events and to foster the study of dust-related phenomena. The framework collects numerical model outputs and observational data from a wide set of worldwide partners plus internally developed. A wide set of post-processed analysis and statistics are generated, and results in form of plots, tables or numerical (binary) data are disseminated to a variety of users (e.g. public institutions, researchers). The integration of the framework in EOSC will increase the volume of data hosted and processed, to reach a wider set of end-users; improve compliance to data FAIR principles and reinforce the robustness of the whole service infrastructure.

• UMSA (Untargeted Mass-Spectrometry Analysis) [23]: UMSA aims at processing mass spectrometry data to correlating the whole spectra (ie. all the present compounds) with other data (social, medical, other sample analyses, etc.) to work with more complex hypotheses on the impact of environment in human health. The data are unrecoverable, therefore long-term data storage is required, together with appropriate data curation. By means of the integration in EOSC, uniform access to data and computing resources are provided, scaling the service to the target European-wide user community.

• MSWSS [24] (Water Supply Systems modeling and analysis): MSWSS is a service for modeling and analyzing Water Supply Systems which integrates the analysis of toxins in drinking-water supply networks with water distribution network simulation. It allows water infrastructure operators and researchers to analyse hazardous events (e.g. toxin propagation within a pipe system) and may be used for preparation of risk management plans for water utilities. The integration with EOSC computing infrastructure and data sharing services will enable modelling more complex water supply systems and increase the number of scenarios for the analysis.

• O3AS (Ozone Assessment Service) [25]: The O3AS service provides an invaluable tool to extract ozone (O 3 ) trends from extensive climate prediction model data to produce figures of stratospheric ozone and figures of dates by when depleted O 3 recovers to pre-ozone hole levels. This service is conceived to assist scientists in visualising ozone data from large climate models by calculating dates for the recovery of the ozone layer and providing trends of the ozone abundance in the atmosphere to produce results in the form of figures in publication quality.

The integration of the service in EOSC has increased its capacity to process large volumes of data (in terms of TBs) and to facilitate the management of the complex workflow to generate key metrics. Climate model data has many fields of physical quantities. The relevant quantities for the ozone column calculation has to be extracted and processed in order to be visualized efficiently. A pre-processor is running on an HPC system reducing the large data set so that it can be read by an REST API to produce figures without noticeable delays.


## Gaps and Bottlenecks analysis

Before starting the integration of the thematic services in EOSC, we have performed a deep analysis with each one of them to properly identify in advance the needs and requirements to increase the capacity, performance, reliability and/or functionality of the services. Thus, every thematic service has made an analysis of the technical services used for managing the users, computing and data. Table 1 shows a summary of the limitations, lacks and needs identified by the Thematic services of the project.

In a preliminary analysis of the results, several technical commonalities and differences have been identified. All thematic services share the importance of using a robust AAI compatible with the ones used by the target institutions. With respect to resource management, all services have the interest of dynamically provisioning processing resources, most of the cases on demand. However, thematic services have different needs: from a dynamic dedicated cloud backend to an elastic cluster that shrinks or grows according to the workload, and even need to access external High-Performance Computing (HPC) and High Throughput Computing resources for massive Batch jobs execution. Regarding job Management, most thematic services use batch queues (like SLURM [26] Batch queues or Galaxy [27]), which could be extended to support containerised jobs. The usage of Kubernetes to orchestrate microservices and job queues of containers is also considered.

The most challenging part is the management of data. Thematic services have identified important issues on transferring and accessing large volumes of data and require smart caching, advanced data transferring and massive persistent data storage. There are two main approaches to cope with storage, from deploying their own datastore, e.g. DATAVERSE [28] instance; to the integration of external Data Infrastructures, like EGI DataHub [29] and EUDAT [30]. Moreover, services need to ensure the compliance to the FAIR principles to facilitate the access, cataloging and reusage of the data Thematic Service

Limitations and needs WORSICA -Improve download speed and number of concurrent downloads of satellite images.

-Increase storage of the images needed for the algorithm.

-Increase computational resources: GPU and RAM to speedup the image processing.

-Seamless authentication and authorization for end users.


## SAPS

-Need for a larger-scale deployment: computing, storage and data access.

-Scalability and standardisation of services -Integrated and widely supported AAI GCore -Overcome limited access to data repository due to network bandwidth restrictions.

-Infrastructure resources for processing and reprocessing large data sets.

-Data delivery volume. Increasing size of files to be delivered to users.


## SCIPION

-Insufficient Cloud resources for the workflow: GPUs, CPUs and RAM -Need of a Resource Management able to optimize the use of cloud resources.

-Storage limitations and data transfer performance: 1-3 TB raw data.

-Distributed and shared file system.

OpenEBench -Need to work on heterogeneous systems to reach Life Sciences Communities -Need to efficiently store processed data and workflows in a FAIR manner.


## LAGO

-Limitations on data preprocessing.

-Needs data storage that copes with FAIR, curation and harvesting; -Need for computing power for simulations, together with optimal scheduling.


## SDS-WAS

-Lack of services needed for Data storage and curation.

-Lack of computing power for data analysis on-demand.

-Lack of reliability of data sources, especially about observations UMSA -Long-term data storage is required, together with appropriate data curation.

-Tracking provenance of the secondary (derived) datasets.

-Need for reimplementing UMSA algorithms to deal with sparse data.


## MSWSS

-Needs data protection measures because of the usage of confidential data.

-The data has to be stored in a private storage only.

-Implement security policies to protect VMs.


## O3AS

-Requires larger storage resources, specially improving data availability -Fast handling of big data generated by their services. Thus, a storage service able to manage, together with the data, metadata and unique data identifiers will be required. Note that not all the services have identified gaps in all the previous aspects so each thematic service will focus the adaptation in the aspects that are more relevant according to the bottlenecks. Solutions available in the EOSC marketplace will be studied and prototyped in the next section before adapting them into the thematic services.


## Analysis of the EOSC Portal Catalogue and Marketplace

In this section, our goal is to identify the key EOSC tools and services that can address the issues and needs analyzed above. Considering the gaps and bottlenecks identified, this analysis also considers potential alternatives of the current technical services used by the thematic services to overcome such issues.

The EOSC Portal Catalog & Marketplace [8] has been developed from the perspective of users, identifying the needs to be supported and facilitating all the actors involved in implementing an open approach to science in a sustainable way. The catalog has more than 320 entries registered by the end of 2021, covering resources from several categories. According to a functional perspective, we can organise them into six categories:

• Access physical & eInfrastructures: offering generalist resources like virtual machines and containers as well as storage and network transport connectivity. By the end of 2021, 62 resources are listed under this topic. This category include Compute resource providers, workload managers, Resource orchestrators and data providers. Some of the services were thematic (e.g. discipline-specific). We identified 6 generic services that could address the requirements of the thematic services: B2SAFE for long-term data preservation, EGI Cloud Compute to provide IaaS cloud resources, EGI DataHubto provide online cloud storage resources, EGI High-Throughput Compute for batch workloads, EGI Workload Manager to orchestrate multi-site batch resources, and Infrastructure Manager -IM to deploy virtual infrastructures on top of cloud offerings), according to the following criteria: Generic purpose, interoperability and support. A brief description of the services is provided next:

-EGI Cloud Compute [31], an IaaS from the EGI Federated Cloud that enables the user to deploy and scale virtual machines ondemand.

-EGI HTC Compute [32] enables running computational jobs at scale on the EGI infrastructure, which is provided by a distributed network of computing centres and offers more than 1,000,000 cores of installed capacity, supporting over 1.6 million computing jobs per day.

-EGI Workload Manager [33], a service to manage and distribute your computing tasks in an efficient way while maximising the usage of computational resources.

-EGI DataHub [34], a service that brings data close to the computing to exploit it efficiently and can be used to publish a dataset and make it available to a specific community or worldwide across federated sites.

-B2SAFE [35] is a service for long-term preservation of the EUDAT Data Collaborative Infrastructure, one of the largest e-infrastructures in Europe offering permanent storage capacity and integrated management services for research communities. EUDAT also provides other services such as B2SHARE, B2FIND, B2ACCESS.

-Infrastructure Manager (IM) [36] • Aggregators & Integrators , where we can find several tools and utilities to facilitate the access to services and resources, by means of indexing and annotation. Out of the 22 resources available in this category, we identified the Dynamic DNS service [37], to easily add a DNS name to an instance deployed in the virtual infrastructure of EOSC, the EGI Fedcloud client [38], that facilitates the access to the federated cloud computing platform, and B2FIND [39], another EUDAT's service, to annotate research objects, considering the requirements of the thematic services and similar criteria as in the first item.

• Processing & Analysis mainly aimed at facilitating the management of computational resources and the scheduling the execution of workloads. Despite this is the category that accumulates the highest number of services, most of them are discipline-specific. Moreover, some of the resources where already listed under the first item. Here we can find Elastic Compute Clusters in the Cloud (EC3) [40], a tool to deploy virtual elastic clusters on top of IaaS clouds, and B2Handle (another EUDAT's service) to provide persistent identifiers to resources.

• Security & Operations aims at guaranteeing that the overall system and the services operate securely and according to standard. In this case, thematic instances for authentication and authorisation may be preferred as researchers in the community already have acquire credentials. For this purpose we identify the EGI Check-In [41] service, the B2ACCESS [42] and EduTeams [43], along with ELIXIR AAI [44] which is not listed in the catalogue.

• Sharing & Discovery relates both to services that produce data relevant to specific disciplines and horizontal services for data deposit and annotation. Only the service B2SHARE to enable sharing and publishing research data is considered. The catalogue also includes an instance of Dataverse, although we have decided to deploy our own instance. The Dataverse Project, developed by the Harvard's Institute for Quantitative Social Science (IQSS), along with many collaborators and contributors worldwide, is an open-source web application to share, preserve, cite, explore, and analyze research data.

• Training & Support aims at facilitating the access to high quality technical information and tailored training materials. Services in this category are not considered.

All these services have two different access modalities:

• Direct access. This model is used by services which are instantiated upfront and which do not require intensive access to resources, or resources are provided directly by the user (e.g. EGI Check-in or Infrastructure Manager). Users are automatically forwarded to the service endpoints.

• Access through orders. This model is used in services that require a non-trivial amount of resources (e.g. EGI Cloud Compute or B2SAFE). In this case, the user normally has to choose between different offerings, which may end up into costs.


## Adoption of EOSC services

As we have shown in section 3, the ten thematic services have complementary requirements and features. However, in general they share needs on four different categories:

Authentication and Authorization Infrastructure (AAI). All services require users to be authenticated and authorised. In some cases, there is a need for delegation from the users that access the platform for accessing data or processing resources. In those cases, it is mandatory to have a coherent single-sign on mechanism. Other cases may require an AAI linked to popular scientific IdPs and implement the authentication via Virtual Organization membership. From the tools and services we identified from the TSs, EGI Check-in has revealed to be a widely accepted choice. Another option analyzed is B2Access, mainly for interacting with the infrastructure. There are also few cases in which users will use federated credentials to access the services -mainly related to storage.

Workload Management. Most of the cases deal with the execution of a set of batch jobs. In those cases, workload managers should be integrated to better take advantage of the computing resources. This will provide the capability to deal with a larger capacity and larger workloads. The options here range from using a standard batch queue (SLURM), that can be eventually powered up with automatic elasticity, to the usage of Kubernetes for the orchestration of a container-oriented approach.

Resource Management. Most of the thematic services require deploying a virtual infrastructure where the services that provide the functionality and the processing will take place. In most cases, the use of Infrastructure Manager (IM) or Elastic Compute Clusters in the Cloud (EC3) client have been identified by most of the thematic services as a technology capable of filling in this gap. Both tools could provide the capability of defining a virtual infrastructure as code and deploying it on the cloud. IM (for static infrastructures) or EC3 (for dynamic infrastructures) together with recipes for K8s, Slurm or Galaxy clusters on top of a dynamic dedicated cloud backend is the preferred solution. Moreover, some thematic services require links to external HPC resources (like Marenostrum in BSC) and HTC resources (EGI HTC compute) for the execution of massive Batch jobs.

Data Storage. The services need to have a storage connected to the processing that can be efficiently accessed. In this case, there is a wide range of different solutions proposed or implemented in the thematic services, rang-ing from external solutions like EGI-DataHub, B2SAFE and B2SHARE to local solutions based on Nextcloud, Dataverse, Elasticsearch and WebDav, where tipically the resource manager will be also in charge of deploying and configuring their own Datastore instance (e.g. DATAVERSE instance). Table 2 summarizes the selection of tools and services performed by each thematic service for the fourth categories detected. As a summary, three different (although compatible among them) AAI methods have been integrated (EGI Checkin, B2ACCESS, Life-Sciences AAI and eduTEAMS). Job scheduling ranges from solutions based on containers (using Kubernetes) to solutions using batch queues (mainly based on SLURM), supported in some cases by workflow frameworks such as Galaxy and instantiated through EC3. For the interaction with cloud resources, TOSCA [45] and RADL recipes have been developed for Infrastructure Manager. Finally, data access is performed through different solutions such as Dataverse, EGI DataHub One-Data, B2SHARE and B2SAFE, which clearly states the complexity of the data management issue and the wide range of solutions.

To sum up this section, with the adoption of all the services and technologies depicted in 2, thematic services have experienced all these improvements:

• Integration of standardized AAI IdPs to facilitate user management.

• Improvement of processing backends by replacing single computing instances with batch job queues, container management platforms or clients to high-throughput computing backends.

• Publishing the output results in persistent repositories.

• Improving repeatability and platform-agnosticism by describing the application topologies as code using standard TOSCA language.

• Self-management of resources to reduce maintenance costs.

• Persistent Identifiers (PID) annotation of output data and integration in official harvesters.

Thanks to the rich analysis of the experience of these ten thematic services in the adoption of several tools, services and technologies to improve and solve their needs, the path to follow for a new scientific use case is far more easy. However, to clarify even more this process, and to easily identify the key services and technologies selected, we present in next section a generic application integrated in the EOSC ecosystem.  


## Application Modelling

This section uses as input the experience of the ten thematic services of EOSC-SYNERGY to define a canonical generic application architecture leveraging the services identified in the EOSC Marketplace catalogue in section 4. Thematic services that have similar requirements as those described in section 3 can use as basis this architecture that relies on several tools and services from the EOSC ecosystem, together with well-known frameworks and technologies of the cloud computing paradigm, all of them carefully selected taking into account the selection made by the thematic services.  First of all, after analyzing our ten use cases, we identified two different deployment scenarios: i) a single instance of the service shared by several users or communities, offered as a web portal able to manage users, data access, processing and visualization, supported by a shared or dedicated pool of resources (e.g. WORSICA or OpenEBench); and ii) an instance of the service deployed on demand, where each user deploys his/her own instance of the service on Cloud resources based on a combination of TOSCA recipes with Docker containers (e.g. SAPS, SCIPION).

Regardless of the approach chosen, Figure 1 shows the architecture of this generic application that relies its deployment on top of the EGI Cloud Compute platform. The first layer that has been considered as essential by the thematic services is the authentication and authorization infrastructure. For that, EOSC offers the EGI Check-In service, that has been the most popular solution adopted by the thematic services. This service can be easily integrated with a service that is exposed to users through a web portal, and it will be used by both the application manager and the end-users to access the EOSC resources and the service itself. Once the user has been properly authenticated, and depending on the usage model that the service wants to use, he/she will access to the Application User Interface of the scientific application itself or to the portal of the resource manager that will facilitate the deployment of the scientific application instance. In the second scenario, the user will be redirected to the portal of IM or EC3 to deploy a virtual cluster configured on demand for his own usage. The selection between IM or EC3 has to be taken depending on the needs in terms of resource consumption. If elasticity is required, the tool to be used will be EC3. Otherwise, IM is the best tool to provide a static infrastructure configured on demand. Both solutions will require the preparation of a recipe where the application manager specifies the required steps and commands to properly install, configure and deploy the scientific service, together with the credentials to access the cloud provider.

In order to take advantage of the virtual infrastructure where the scientific application is running, we need to rely on a workload manager. From the analysis we have made, we have identified two different approaches: i) a traditional batch job queue, managed by the well-known SLURM scheduler; or a solution based on the containerization of jobs, where kubernetes has proven to be the most popular scheduler. Both options are feasible, depending on the approach that the scientific service wants to follow. However, the adoption of one of these workload managers might require an effort adapting the architecture of the tool to it, so this duty has to be consciously analyzed.

Finally, for the data storage, our recommendation is to use a solution that supports metadata to comply with FAIR principles, i.e. to make data Findable, Accessible, Interoperable, and Reusable (like Dataverse or B2SHARE). No matter if the storage solution will work locally or it will be an external service, one of the most important aspects is the support to metadata to properly index and facilitate reusing the data generated by the services, specially if this will be of interest to other researchers of the area.


## Service instantiation

In this section we want to exemplify how the adoption of the new EOSC tools and services can address the gaps and bottlenecks detected by some of the project's thematic services. Specifically, we present seven examples from seven different scientific services showing the integration in each one of the categories described in section 5. We have omitted the integration with the authentication and authorization service of EOSC (EGI Check-in), because this is a well-known process that is properly documented [46]. Next subsections cover the cases for the rest of categories.


### Workload Management

During the analysis of the thematic services, we have detected two main needs for the workload management: i) services that use a more traditional approach relying on local resource management systems based on queues of jobs; and ii) services that encapsulate the tasks in a container and rely on a container-based management system. To exemplify these two models, we analyze both Scipion and O3AS.


#### Batch job oriented

The Scipion service aims to facilitates the life to users from the Instruct-ERIC [47] Research community for processing their electron microscope data. Users who obtained her data through an Instruct granted project in an Electron Microscopy facility can request the use of the Scipion service by contacting the Instruct Image Processing Center (I2PC). Then, the request is reviewed and if the available quota permits it, the I2PC administrator will deploy a cluster using the Infrastructure Manager (IM). The user will then receive an email with instructions on how to access the front-end node and how to copy the data to start processing. She will be able to use the service for a maximum time of one month although if no other service requests are pending this time might be extended. The cluster will be destroyed by the I2PC team after the granted period finishes, giving the user enough time to download her results.

As shown in Fig. 2 the cluster is defined in a TOSCA description that includes Ansible recipes and docker images to launch and configure the different nodes in the cluster. This TOSCA description is deployed through the IM. Once the cluster is running the user will be able to transfer the data to the front-end node and access the service using either a VNC client or NOVNC through a web browser. The front-end host node runs a SLURM master and a Docker container that includes Scipion and related Cryoem packages configured to launch their jobs through SLURM. Once a job is sent to the queue a Docker container is run in one of the worker nodes to run the Scipion command.

Hardware resources in which the cluster is deployed are part of the EOSC EGI Cloud Compute service that control access through Virtual Organizations (VO). In the case of the Scipion service deployment is only granted to members of the cryoem.instruct-eric.eu VO. The cluster shared storage is currently based on a local Ceph disk.


#### Container based

O3AS (using Kubernetes) O3as service is composed of three main components (Fig. 3): o3skim to reduce the original data to the parameters of interest, o3api to provide an API-based access to the skimmed data, and o3webapp for a user-friendly web interface (in development). O3skim runs on the local HPC system, while o3api and o3webapp are deployed in the Kubernetes system for the container scalability ensuring a fast enough response to user requests. If no Kubernetes system is available for the service providers, it can easily be instantiated by the means of Infrastructure Manager (IM). Then the following steps are applied: 1. Install and configure cluster issuer to handle Certificates for secure HTTP connections, e.g from LetsEncrypt. 2. Initialize Ingress resource to route external traffic. 3. Deploy o3api component as a container with the pre-configured Prior-ityClass. 4. Add Horizontal Pod Autoscaler (HPA) to respond with more containers on higher than usual loads. 5. Finally, instantiate o3api as a service.

If an already existing Kubernetes cluster is used instead, steps 1-5 have to be adjusted accordingly.

The output of the services is a set of projections of the ozone distribution. Example figures for climate models with projections for ozone until the year 2100 are shown in Figure 4.


### Resource Management

Regarding resource management in cloud computing providers, we have identified two different approaches: i) static infrastructures, where the number of nodes that compose the platform remains constant during the lifetime of the service; and ii) dynamic infrastructures, where elasticity models are applied to the platform to adapt its size to the workload, thus allowing to reduce costs and wasting resources. As in the previous section, we have chosen two thematic services to exemplify the adoption of these two different solutions. 


#### On-demand fixed infrastructures

The flexibility for choosing the computing platform was one of the objectives of LAGO TS. Beyond the elasticity or the automatic management, the priority is providing resources that accomplish the needs of every specific calculation, and so these requirements may be as variable as its parametrization. Some of these simulations may face intensive requirements, such as scratching up to several TBs of data; accessing to many files through Internet; continuously processing data in batch mode; or even sporadic calculations for demonstration purposes and scholars. Public clouds such as EOSC EGI Cloud Compute can tackle many of these tasks, but require an upfront reservation of the resources by demand and fix their environment. To be able to face all this different approaches, three different services were integrated: the Infrastructure Manager (IM) service, the software encapsulation in standardised Docker images and the profiting of the ubiquity of the OneData cloud storage (EGI DataHub sevice). These technologies allow dynamically instantiating the virtual infrastructures needed, which are maintained fixed over days or weeks [20].

The resulting architecture is shown in Figure 5. To fix a temporal infrastructure on public clouds, researchers dynamically apply for virtual ma- chines or batch clusters through the IM service. Users can create any kind of cluster following, not only their own preferences, even the needs of specific calculations. In this sense, SLURM was a good choice as it is commonly used by many LAGO collaborators and the behaviour could be similar to those used by other TS, like, e.g., Scipion, as it is described in the subsection 7.1.1. However, there are scientists would rather prefer to manage other implementations, such as Kubernetes, more suitable for Docker instances. Additionally, when tasks require scratching at the order of TBs, the user would not be allowed to spend the space accumulated by several computing nodes. In these cases, single virtual machines, are the most suitable choice. All these infrastructures are deployed by IM after a few clicks in its website.

On the other hand, calculations can be arbitrarily performed by researchers by running the official LAGO docker images stored at DockerHub, which are periodically released by the CD/CI pipeline built on the JePL service [48]. Thanks to the virtualised approach, the software encapsulated in these images can be run on any platform supporting Docker. However, as the FAIR paradigm has to be fulfilled, all the LAGO software is always bound to the OneData cloud storage (DataHub) and it comply with the AAI procedure for the LAGO VO in every run. Thus, independently the computing platform, results are always identified by PIDs and they are browsable at the DataHub portal and findable by public harvesters such as B2FIND.

Therefore, the on-demand provisioning of adaptable infrastructures supporting Docker through the IM service, jointly the cloud storage via DataHub, allow users accomplishing their research without depending on other services.

As an example, we deployed a SLURM virtual cluster counting on 10 nodes with 16 Intel Xeon E7 cores and 250 GB of shared memory and disk. Then, we simulated the expected flux of the atmospheric radiation during the interaction of cosmic rays with the Earth atmosphere for every LAGO detector [49]. We computed from 1 to 7 days of the expected flux at high altitude or antarctic sites, reaching up to 1 year of the energetic flux that is accounted for volcanic risk studies. These new integration times is an enormous statistical improvement when it is compared with previous results obtained in LAGO. Note that, e.g., a 24-hour flux in one of the high latitude (Antarctic) detector involves the simulation of ∼ 1.9 × 10 9 different cascades. Thus, we simulated the impressive figure of > 10 12 particles spending > 300 kCPU-hours, generating > 5 TB of synthetic data and metadata.   Results shown in Figure 6 allow estimating radiation doses at different altitudes, which are currently used for designing new detectors; shielding in-struments (e.g., HPC facilities); calculating the reference HEP flux for underground laboratories, volcanic risk assessments and mining prospecting [50].


#### Elastic infrastructures

For cloud applications with varying load the static infrastructures might end up with resource waste. In order to adapt to the dynamic demand for computing capacity, the MSWSS service uses Elastic Cloud Computing Cluster (EC3) tool [40] to create an elastic virtual cluster on top of the EOSC computing resources. EC3 CLI tool provides a set of pre-defined templates which can be combined and customised. It also allows to define custom templates with integrated Ansible scripts. This is used to create a template with strengthened security settings and additional configuration commands specific for the MSWSS service.   Figure 7 shows the architecture of the MSWSS service with the interaction of the service operator and users. The users interact with the service using Galaxy portal where they can manage their data and submit jobs to the elastic virtual cluster for processing. The output data are stored in within MSWSS service and can be downloaded for post-processing tasks.


## EOSC Cloud IaaS

The service operator deploys the service using EC3 tool using the customised template. Once the MSWSS service is deployed the CLUES service monitors SLURM batch system and deploys new virtual worker nodes as needed and automatically re-configures the batch system. The deployment and configuration of worker nodes is performed using the Infrastructure Manager (IM) [51] service. To speed-up the deployment process the worker nodes are instantiated from a snapshot of fully deployed worker node (golden image). This allows to decrease the start-up time from 21 to 5 minutes. It also helps to solve the issue with pending security updates with respect to the vanilla image and potential need to reboot the worker node for the updates to be applied properly. The golden image is maintained by the service operator in up-to-date state. The security is important also for the communication inside the virtual cluster. OpenVPN system is used to create secure connections inside the cluster and protect the data transfers. It also allows to span the virtual cluster over the resources from different Cloud providers.


### Data Storage

Finally, in the field of data storage, we have also observed three different approaches: i) local storage; ii) external storage and iii) hybrid approach. The next subsections analyze the use cases of WORSICA, OpenEBench and UMSA to illustrate the three approaches.


#### Local storage

WORSICA uses the Dataverse application to manage the data produced by the service and disseminate it to the research community and the public in general. The first version of WORSICA's data manager was developed as a local repository, using the architecture presented in Figure 8. This approach raised several constraints to the adoption of a FAIR compliant data management paradigm, such as i) the lack of a unique global identifier for the datasets produced; ii) the data was not accessible and stored in multiple places internal to the service; iii) nonexistence of metadata for each dataset; and also iv) the access to the data did not follow the controlled vocabularies that apply to FAIR principles. In order to surpass the previous oversights, a Dataverse REST API that allows running all necessary operations efficiently, like:

1. the ability to implement in any language only being dependent on the provided interface without any library requirements; 2. the capability to easily maintain the WORSICA code in parallel with Dataverse service updates; 3. Moreover, provide the features required to share sensitive data with the public.

The current architecture of WORSICA's data manager evolved and can be seen in Figure 9. The WORSICA service is now working with Dataverse to automate the data availability and use services for and distribute credit to the data creator. Dataverse allows the creation of multiple virtual archives called Dataverse collections. Each Dataverse collection contains datasets, and each dataset contains descriptive metadata and data files. Therefore, this version of WORSICA enables datasets to be linked in Dataverse to the appropriate ontologies to increase interoperability and data FAIRness. Variable names can also be included in datasets metadata in the native language (Portuguese) and get Universal Resource Identifier (URI) for those entities in controlled vocabularies (e.g., in the case of WORSICA, a DOI -Digital Object Identifier is created). Furthermore, standardized metadata fields are available in Linked Open Data Cloud through standard machine-to-machine interfaces available in Dataverse.


#### External storage

OpenEBench makes use of B2SHARE for long-term availability and storage of scientific benchmarking datasets. The adoption of EUDAT's technical standards, data models and policies helps OpenEbench to further enforce the FAIR-compliant data management of the platform. One of the major capabilities gained through the integration with EUDAT is the minting of Digital Object Identifiers (DOIs) for the benchmarking data collections generated in OpenEBench.

A variety of dataset types are involved in the benchmarking workflows at OpenEBench. Those datasets cover the reference data used as gold standard data, the predictions submitted by participants as well as new datasets like the actual results scoring and ranging participants, provenance reports and metrics' plots. In this way, a compact and human-readable set of data is ready to be referred to in scientific publications, promoting transparency, reproducibility and data reuse. Furthermore, EUDAT registries provide rich metadata fields for easing data discovery, so thus submitted collections could be annotated with cross-links to OpenEBench for further insights. Actually, OpenEBench is one of the EUDAT registered research communities benefiting from a particular extended metadata model and customized access rules. It facilitated a better integration with OpenEBench, implemented through a REST-based programmatic data publication workflow triggered from the platform Web GUI.

Within OpenEBench ecosystem, benchmarking data is accessible on the Web or via specific REST and GraphQL APIs. Nextcloud and MongoDB are the technologies used to store datasets and metadata respectively, always operated under a FAIR-compliant data governance plan that considers, for example, unique and accessible identifiers, document versioning, provenance preserving metadata, strict publication rules or a formal benchmarking data model. When a benchmarking events manager or developer participating in a given event is willing to publish its data outside the platform, they register their B2SHARE API token in OpenEBench and initiate the publication process to B2SHARE via the Web GUI:

• OpenEBench composes the data collection with a specific metadata form, validates it, and programmatically submits both, the data and metadata, to the B2SHARE server.

• Over HTTPS and on behalf of the user, the platform implements the full EUDAT data publication workflow through the B2SHARE REST API. The outcome is a DOI associated with the new registry, which is captured and saved in OpenEBench to keep both systems cross-linked. Eventually, published benchmarking datasets can be consumed using both B2SHARE and OpenBench platforms.

The OpenEBench data flow can be seen in Figure 10.


#### Hybrid approach

UMSA leverages three different storage classes for different purposes. First, data acquired by the instruments (mass spectrometers) are stored on a traditional POSIX filesystem, implemented as RAID disk arrays and cluster of GPFS servers, re-exporting the volumes via NFS and CIFS to the clients. This is a technology with limited scaling (up to petabytes per filesystem), however, it is proven for decades and stable, suitable for the primary experimental data that are irrecoverable otherwise. This storage is also backed up Figure 10: OpenEBench data flow. Users execute evaluation benchmarking workflows, which use and produce datasets. That data is locally stored in MongoDB (for metadata) and Nextcloud. It can be then submitted to B2SHARE, using their API REST, which will lead to minting a DOI. Then, it is mapped and saved to the OpenEBench database for cross-linking purposes. Published benchmarking data can be consumed using both B2SHARE and OpenBench platforms.

weekly to a remote site provided by the CZ national e-Infrastructure 1 .

The UMSA service itself is based on Galaxy, which mounts the primary data filesystem (above) read-only, and it exposes the datafiles to the users via its Data libraries 2 , with appropriate access control, and without the need to copy the large files.

Standard Galaxy setup requires a shared filesystem mounted at both head and worker nodes (alternate setups require lot of file transfers for each job, which we cannot afford). It can be either a single-tier storage, or it can serve as the first tier (cache) of object storage (the second tier). We use the two-tier setup, with a NFS-mounted SSD-only shared filesystem, with typical usage up to dozens of TB only, fast enough not to slow down data processing with I/O, and S3 object storage (provided by the national e-Infrastructure again), which scales up to many PB easily.


## Related Work

This section describes other alternative thematic services and their differences with respect to the provisioning or performing model of the services in this article. Each thematic service has a dedicated paragraph to better analyze its case.

The WORSICA thematic service aims to study the evolution of coastal zones and water in inland areas for better coastal, estuarine, and natural resource management. Other services study these subjects, such as [52,53,54,55]; however, the WORSICA service has the particularity of being freely accessible to the scientific community and performing simulations on-demand for the entire globe. Furthermore, the development of a service like WOR-SICA can only be achieved through the collaborative work of the EOSC network regarding SaaS infrastructures and products with federated access, described in Section 4, due to its need for a robust and scalable computational infrastructure to serve its users.

GCore thematic service leverages cloud computing power to enhance the processing resources available in a Space Ground Segment. The adoption of a dynamic cloud backend is a trend in different EO missions. Programs as Copernicus envisage the use of cloud computing to change the current on-premises processing approach to a on-the-cloud approach and offer the environments as a service. Entities as EUMETSAT (European Organization for the Exploitation of Meteorological Satellites) envisage a new design of a multimission processing infrastructure using the cloud advantages in order to allow extending the resources and made them available to several missions. GCore follows these approaches in order to extend the functionality with the use of cloud resources in order to break the bottleneck that a on premise classical system can obtain during the re-processing tasks associated to a particular mission for example. During this task a massive use of processing power is needed in the nominal platform being able to affect to the nominal operations of the mission. The capacity of GCore to deploy additional processors on the cloud is used to reduce the impact of such a task. This approach can also be used for data archiving and cataloguing the products resulted from the processing in the cloud making them available to perform higher processing levels directly on cloud using as a service processors specifically defined published previously in a market place.

SAPS applies algorithms to estimate evapotranspiration (ET), a technique that has been widely used in several countries, and in areas subject to different weather conditions [56], [57]. The execution of these algorithms suppose high computational demands, both CPU and memory-intensive, and the archival of the output data generated consumes a substantial amount of storage resources [58]. Recently, several software packages and libraries with implementations of ET estimation have been made publicly available, such as the ILWIS software [59] or the 'Water' procedure [60]. However, those implementations have been developed as standalone artifacts made available for individual use by researchers, typically executed in their personal computers, that complicate considerably the sharing of processed data and might suppose limitations on the computing capacity locally available. A different approach is taken by the Google Earth Engine (GEE) platform [61] an initiative to facilitate the implementation and execution of scientific workflows that consume satellite imagery as input. However, this service is under service conditions imposed by Google, that might not always fit the community needs [62]. Conversely, SAPS in combination with EOSC services and resources, follows an innovative approach based on the deployment of ondemand SAPS instances on top of federated resources. The service is highly configurable, allowing the selection of the algorithms used to estimate ET, and provides a standard output data and metadata that can be easily shared among researchers of the area.

The ScipionCloud service offers a ready-to-use infrastructure in the cloud to users that aim to process their CryoEM data. A similar approach is followed by Stion [63], a web application that provides on-demand access to GPU instances on AWS for biomedical researchers to process Cryo-EM data. This solution offers automatic deploying of instances (virtual machines) but does not integrate a batch system. It also sets up an auto-shutdown mechanism to power off instances after a certain period of time, which is risky and insufficient from our point of view. On the contrary plans to integrate ScipionCloud with EC3 guarantees a much better way to optimize the use of cloud resources than Stion's implementation. Besides, to use Stion it is mandatory to provide your own AWS account which might be a drawback for many users both in terms of complexity and cost. Another approach found in the CryoEM world is the work done by Cianfrocco's lab at the University of Michigan, which offers several tools in this area. The first interesting tool [64] is the integration of a 'AWS batch system' in one of the most popular software packages used for CryoEM processing; Relion. This batch system allows to send Relion commands to AWS instances, which includes deploying, running and shutting down the instance. This approach was only integrated in an old version of Relion which might imply that it was not a big success in the CryoEM community. The second tool is COSMIC [65], a freely available web platform for submitting cryo-EM jobs through the cloud to Comet, a cluster at the San Diego Supercomputer Center. Although the use of COSMIC has no cost access and this tool is available for everybody users might prefer to work on their own server instead of preparing their processing workflows to be sent step by step to a cluster.

Regarding the use of external storages for sustainability, OpenEBench is not the only platform designed around FAIR principles that ensures the availability of its data and metadata through EUDAT services or similar data infrastructures. WorkflowHub [66] is applying a similar strategy for publishing scientific workflows, wrapping them into enriched Research Object Crates (RO-Crate 1.1 specification), which include data resources, semantic annotations and all additional information that guarantees the workflow is reusable and reproducible. In this case, Zenodo is the infrastructure minting the DOI, and the publication process follows DataCite [67] guidelines, one of the broadest cross-domain metadata standards available. Among EOSC resources, other platforms like ROHub [68] propose the use of EUDAT services as an internal storage solution. ROHub is a platform that enables the management, sharing and preservation of research data as Research Objects. It is integrating B2DROP [69] as the underlying technology for the researcher's personal storage space and B2SHARE for DOI minting and sharing.

Software needed for LAGO simulations is managed by ARTI [49], a dataintensive and highly-complex framework designed to calculate the expected flux of signals in any site around the World and under realistic geomagnetic, atmospheric and detector conditions. As the simulated phenomena, i.e., the interaction of cosmic rays with the atmosphere and the detector response to the resulting flux secondary particles, is, essentially, a sequence of stochastic processes, the simulation performed need to integrate the flux over long periods to reduce the impact of statistical fluctuations. Such large times, from several hours to days and even years for some applications such as volcano risk assessments, require the usage of large computing facilities and storage. Suitable simulations will typically spend tens of weeks in current CPUs and they will output several TB of data. Examples of that are the results pre-sented in Subsection 7.2.1, and recently in [20,50]. Moreover, all these data needs to be properly identified, catalogued and curated to accomplish the FAIR principles. Although previous attempts were made to adapt ARTI to its use in distributed high-performance computing infrastructures [70] and for the adoption of data curation standards [71], it was only thanks to the development of OnedataSim [20] within the LAGO TS that it was possible to achieve sufficiently long integration times while complying FAIR principles.

SDS-WAS provide, a part of a huge quantity of materials on dust storms forecast, services of data storage, download and data analysis based on EOSC cloud services (B2SAFE, B2HANDLE). As far as we know there aren't comparable services related to dust storms, in terms of collecting a bunch of numerical models outputs, observational data (in situ and satellite), providing derived graphical (plots) and numerical (skill scores) products with an interactive dashboard application. Those services are related to the areas of Northern Africa, Middle-East and Europe. We considered as "competitors" the other SDS-WAS regional nodes (Asian and Panamerican), but also other services (not only provided by Research Institutions, but also by companies) related not directly to dust, but to air quality forecast or similar. The aim is to see how they approach the information and solve user's needs. While the Asian 3 and Panamerican 4 SDS-WAS nodes are still in a preliminary stage, as they provide a limited set of services, there are some others with a good feedback. We analysed strengths and weaknesses of products visual presentation of following services: METOFFICE 5 , Breezometer 6 , WINDY 7 , Plume Labs 8 . Although most of them have an attractive design, they only offer visual products of their data, not numerical binaries, and the amount of data they provides seems to be not big as what our SDS-WAS service does. Finally, some of them have advertisements and banners that affects significantly the user experience.

UMSA is dedicated to storage and processing mass-spectrometry data, focusing on GC-MS and untargeted analysis of low-abundant compounds. Numerous tools to process MS data exist, ranging from proprietary software by laboratory equipment vendors, third party commercial software, and open source tools of widely varying quality and maturity. Several sub-area specific reviews exist [72,73]. Being based on Galaxy, UMSA can leverage from dozens of mass-spec related tools published by the community. There is ongoing effort to provide Galaxy-based environments focusing on specific MS applications, e.g. [74]. According to our knowledge, there is no such effort matching the UMSA purpose. Another approach can be seen with GNPS [75], which is a web-based computational environment and data treatment environment, centered aroundmolecular networking-visual display of chemical spaces and relationships among compounds. Vast majority of the methods leverage on MS 2 data, which is not the focus of UMSA. On the other hand, publication and storage of the results of the analyses is fairly well organized around the MassBank project [76].

The MSWSS is a cloud-based service capable to exploit the resources from the EOSC cloud infrastructure and FAIR data repositories. There are several approaches for modelling water supply systems offering various features. Integrated Tool for Water Supply Systems Management [77] puts together QGIS database, Epanet hydraulic model, and Google Maps. A web-based EPANET model catalogue and execution environment [78] focuses on model sharing and a model viewing. Another example of integration GIS and the hydraulic model is a tool for an integrated water and wastewater management system in municipal enterprises [79]. According to our knowledge these services do not provide cloud-based elastic computational back-end and they do not implement FAIR principles for data sharing.

O3as has been inspired by the need of having the large plethora of climate model data available in a consistent manner. Because of the ozone assessment 9 that culminates in a publication every four years [80] scientists need to have a quick way on looking at ozone data from climate data to estimate the time when the amount of ozone in the stratosphere has reached a level pre-ozone hole. Often the models have to be collected, the analysis program redone and the values recalculated [81,82]. The O3as thematic service is very modular and can therefore be extended so that not only zonally (i.e. across longitudes) averaged values of ozone data will be shown, but also trends and ozone return rates for different positions on the Earth. Also atmospheric trace gases other than ozone could in the future be included into the analysis.


## Conclusions

EOSC-SYNERGY is building capacities in EOSC through the development of ten data-intensive thematic services oriented to four different scientific disciplines. The adaptation, improvement and quality assessment of those services on a federated data infrastructure strongly aligns with the objectives of EOSC [83]. A key factor for the success of EOSC [84] is performance, i.e, how EOSC as an ecosystem operates and how the resources are used and acknowledged by the users. All the services consume services from the EOSC catalogue, which will provide feedback on the usability and relevance of the model.

However, the characterisation of new applications to join is key to evaluate the rightmost services to be used. This is why the EOSC-Synergy project is developing best practices and experiences to promote EOSC adoption by the research communities by expanding and building knowledge on common interfaces, standards and best practices. This paper presented an application modelling proposal, based on the previous analysis of gaps and bottlenecks performed by ten different services from four different disciplines.

The ten thematic services have been analysed concerning four dimensions (authentication and authorisation; resource management and offering model; workload management including containerisation; and data storage and preservation), identifying gaps and bottlenecks. Those requirements are common to many other scientific services. We selected ten services from the EOSC Marketplace to address these requirements. In a nutshell, AAI solutions such EGI Checkin, eduTEAMS, D2Access and life-science AAI are mature enough to provide a coherent authentication model for a whole application. Applications that require a dynamic backend or on-demand deployment found Infrastructure Manager (IM) and Elastic Compute Clusters in the Cloud (EC3) as reasonable solutions to describe their infrastructure as code and deploy resources according to their workload. Depending on the workload type, job management is driven by SLURM batch queues or Kubernetes services. Preservation of data is obtained through EGI DataHub or EUDAT's B2SAFE and B2SHARE storage services, registering persistent identifiers through B2Handle. Finally, services needing local storage used Dataverse as an OAI-PMH on-premise storage.

The impact of EOSC in the thematic services of EOSC Synergy is mainly composed of three main facts. Firstly, the capacity expansion through the federation of computing, storage, and data resources aligned with the EOSC and FAIR policies and practices. Secondly, software and service quality evaluation of the thematic services is critical to improve robustness and reliance and therefore increase user's experience. EOSC-SYNERGY also focuses on transverse training to facilitate the adoption of technologies and the use of thematic services. Finally the cross-fertilization between different thematic areas has allowed the collaboration between thematic services to take advantage of the developments, solutions, experiences and best practices on authentication and authorization, data storage, resource and workload management.

## Figure 1 :
1Architecture of the proposed solution.

## Figure 2 :
2Scipion service usage and architecture.

## Figure 3 :
3O3as service overview: it consists of three main components: o3webapp (to come), o3api, and o3skim.

## Figure 4 :
4Left: O 3 return dates for a recovery to ozone values in 1980. Right: Timeseries of total O 3 column data showing a decline of O 3 and the subsequent recovery of O 3 in the future.

## Figure 5 :
5On-demand deployment of fixed infrastructures and running LAGO software.

## Figure 6 :
6Left: the energy spectrum of the flux of atmospheric radiation expected at La Serena (LSC) detector in Chile (at sea level) is used for designing, characterizing and calibrate new detectors and sites. Right: the expected flux of >TeV/c particles reaching the summit (5450 m a.s.l.) of the mountain where the ANDES underground laboratory will be installed. Since these particles are capable to traverse up to thousands of meters of rock, being the background signals for neutrino physics experiments and dark matter searches.

## Figure 7 :
7Architecture of the MSWSS service.

## Figure 8 :
8First WORSICA's architecture for managing the data produced by the service.

## Figure 9 :
9Current architecture of the data manager implemented in the WORSICA service.

## Table 1 :
1Analysis of the limitations and needs of the Thematic Services.

## Table 2 :## Introduction

e-Science studies, enacts, and improves the ongoing process of innovation in computationally-intensive or data-intensive research methods [1]; typically this is carried out collaboratively, often using distributed infrastructures. Open Science [2] is the practice of science in such a way that others can collaborate and contribute, with research data, lab notes and other research processes freely available, under terms that enable reuse, redistribution and reproduction of the research and its underlying data and methods.

Scientific applications place higher demands on computing power every

year. The need for large-scale computing resources, including specific hardware needs such as GPUs, together with the increasing demand for storage resources due to the large amount of data generated by these types of applications, are a challenge for both researchers and computer scientists. e-Science makes use of e-Infrastructures [3] which are collaborative virtual environments that provide digital services and tools to meet this resource demand. e-Infrastructures are based on distributed backends ranging from High-Performance Computing to Cloud Computing. However, adapting already existing software applications to these paradigms is not trivial [4]. This process commonly requires an in-depth knowledge of the underlying technologies to truly take advantage of their benefits, as it usually requires refactoring the architecture of the application. This adaption becomes even more challenging on the Cloud computing paradigm, due to the complexity of virtualization and elasticity behaviour together with the vast range of services available and the variety of resource types. This fact can be a barrier for scientists and researchers from fields outside of computer science to adopt these solutions, thus complicating the path of research innovation.

Another obstacle in adopting the e-Science and Open Science paradigms is the fulfilment of the FAIR [5] (Findability, Accessibility, Interoperability and Reusability) principles, which imply permanent and discoverable identifiers for fully annotated data and metadata. Specifically, in Europe, we can find the European Open Science Cloud (EOSC) [6], a European initiative co-funded by the European Commission that aims to facilitate the deployment and consolidation of an open, trusted, virtual, federated environment in Europe to store, share and re-use research data across borders and scientific disciplines promoting open science practices and providing access to a rich array of related services.

As part of this initiative, the EOSC-Synergy [7] project aims to increase the uptake of EOSC through the capacity and capability building using the experience, efforts and resources of national publicly funded digital infrastructures. The project has identified ten thematic services in four scientific domains (Earth Observation, Environment, Bio-medicine, and Astrophysics) to increase the uptake of EOSC services. These thematic services are heterogeneous and cover a wide range of requirements, maturity levels, user targets and usage models.

The ten thematic services also provide helpful best practices for future new services to be developed or adapted to this environment, as they address challenges on federated Authentication and Authorization Infrastructures (AAI), elastic data processing, interoperability with data infrastructures, metadata management and accounting, that apply to many other applications in same or different scientific domains. The analysis of the limitations of these thematic services along with the design of the new architecture of the thematic services will be of particular interest for new scientific applications that want to embrace this paradigm.

In this work, we present each of the thematic services of EOSC-Synergy and then analyse the gaps and bottlenecks in terms of authentication and authorisation services, resource provisioning, workload management, and data storage. This section is followed by the analysis of the different tools and services provided in the EOSC Marketplace [8] that can meet the needs of the thematic services. We then present the adoption of services, tools and technologies used by the thematic services to address their needs, and then model a generic scientific application that can be the starting point for new scientific thematic services. Next, to better illustrate the work done in the adoption of tools and services by the thematic services, we discuss the adoption issues in the service instantiation section. Finally, we analyze the state of the art to identify the most important work related to the implementation of thematic services in the e-Science paradigm, and conclude the paper with the main observations.


## Thematic Services in EOSC Synergy

In this section we briefly describe each one of the ten thematic services of EOSC Synergy, grouped by scientific discipline. Moreover, the expected outcome of the integration with EOSC Synergy for each one of the thematic services are pointed out in this section.


### Thematic Services in Earth Observation

In the field of Earth Observation, three thematic services deal with analysing large satellite imagery, from monitoring coastal changes and inundations, to estimating forest masses and crops. They are addressing different types of targets. Specifically, the three services are:

• WORSICA (Water Monitoring Sentinel Cloud Platform) [9,10]: A service for the detection of water using satellites, Unmanned Aerial Vehicles & in-situ data. WORSICA can be used for coastline detection, inland water bodies detection and water leak detection on irrigation networks. WORSICA aims at integrating multiple-source remote sensing and in-situ data to determine the presence of water in coastal and inland areas. WORSICA enables the research communities to generate maps of water presence and water delimitation lines in coastal and inland regions. These products can be useful for emergency and planning methodologies in case of inundations or reservoir leaks. In the frame of EOSC, the service will be scaled up to a European level to reach all interested research communities.

• SAPS (Surface Energy Balance Automated Processing Service) [11]: Used to estimate Evapotranspiration and other environmental data that can be applied, for example, on water management and the analysis of the evolution of forest masses and crops. SAPS allows the integration of Energy Balance algorithms to compute the estimations that are of special interest for researchers in Agriculture Engineering and Environment. These algorithms can be used to increase the knowledge on the impact of human and environmental actions on vegetation, leading to better forest management and analysis of risks. SAPS is being developed in Brazil, but with the adoption of EOSC services it is expected to facilitate European scientists to exploit the evapotranspiration estimation services from remote sensing imagery.

• G-Core (Acquisition, cataloguing and processing EOS data) [12], [13], [14]: G-Core is a production-ready technology used as a service at ESA's and national programs that provides a Data Manager for spatial and non-spatial purposes and a framework for third-party processors. G-Core is a service for the acquisition, storing, cataloguing and processing data from several Earth Observing System (EOS) missions. Its two main functionalities are: i) a Data Manager for spatial and non-spatial purposes; and ii) a Processing framework to host external processors developed by third parties to generate added value products based on Satellite imagery. The main goal of its integration in the EOSC ecosystem is to offer the service as a Payload Data Ground Segment (PDGS) in the cloud for future ground segment space missions or as a processing framework to plug in different processors that can make use of the Copernicus resources or private data in order to produce different levels of products to be delivered to the users. Thus, the expected impact of the adaptation of the service is to democratize the usage of Earth Ob-servation (EO) data out of the scope of nominal fields. It will help to define new products and services mixing Earth Observation data with other types of data for scientific and social environments.


### Thematic Services in Biomedicine

In this area, the thematic services cover the benchmarking of Genomic data processing tools and the processing of Cryo-electron microscopy imaging. The services are:

• SCIPION (CryoEM data processing for Structural Biology [15]): Cryo-Electron Microscopy Service is an image processing framework used to obtain 3D maps of macromolecular complexes using cryo Electron Microscopy. It has been developed as a plugin-based workflow management system that integrates many important software packages available in the field. The integration of Scipion with Cloud services allows users from the Instruct Research Infrastructure to deploy a dynamic cluster in the cloud to keep processing the data acquired at an Electron Microscopy facility. This cluster has all cryoem packages and software needed to obtain a 3D structure and is powered by EOSC compute resources on the back-end. This means that scientists with minimal computational background (or compute resources of their own) can access the latest tools as well as powerful computational resources to obtain a refined 3D structure to be published and shared with the community.

• OpenEBench [16] (ELIXIR [17] benchmarking and technical monitoring platform): Used to evaluate Life Sciences research software, OpenEBench is an observatory for software quality based on the automated monitoring of FAIR for research software metrics and indicators. The OpenEBench platform supports both the technical monitoring of scientific software and scientific benchmarking activities carried out by Life Sciences Communities. Its architecture has three different engagement levels that allow communities at different maturity stages to make use of the platform. It also connects with ELIXIR Core Data Resources and Deposition databases to use data needed by the Scientific Communities activities. The expected impact of the integration with EOSC services is that Life Science researchers will have semantically annotated, up-to-date collections of benchmarked analytical workflows and tools, organized by scientific communities for specific topics, which can be deployed across heterogeneous systems.


### Thematic Services in Astrophysics

In Astrophysics, the LAGO thematic service sets up a European service for the Latin American Giant Observatory.

• LAGO, the Latin American Giant Observatory [18,19], is an extended cosmic ray observatory, consisting of a wide network of water Cherenkov detectors currently deployed at 10 countries in Latin America, from the south of Mexico to the Antarctic Peninsula. The geographic distribution of LAGO allows the realization of diverse astrophysics studies at a regional scale. LAGO is mainly oriented to perform basic research focusing on three main areas: high energy phenomena, the measurement of atmospheric radiation at ground level and space weather and climate monitoring. All the LAGO analyses are supported by data-intensive computational frameworks, that integrate different simulations tools with own designed data-analysis codes to determine in a very precise way, the signals measured or expected at any detector of any type, in any particular site around the World, and under realistic atmospheric and geomagnetic time-evolving conditions. The final purpose of the LAGO Thematic Service [20] is to enable the universal profit and contribution of this research, within and outside LAGO Collaboration, through a sustainable Virtual Observatory and standardised computational model.


### Thematic Services in the Environmental domain

Finally, in the area of Environment, the fourth group of thematic services include sand and dust storm forecasting, untargeted mass-spectrometry analysis for toxins, water network distribution simulation and the monitoring of stratospheric ozone in climate models. Specifically, these services are as follows:

• SDS-WAS (A Service related to the mineral dust forecast) [21,22]: SDS-WAS is a service that aims at improving capabilities for more reliable sand and dust storm (SDS) forecasts. It supports institutional entities to warn about possible dust events and to foster the study of dust-related phenomena. The framework collects numerical model outputs and observational data from a wide set of worldwide partners plus internally developed. A wide set of post-processed analysis and statistics are generated, and results in form of plots, tables or numerical (binary) data are disseminated to a variety of users (e.g. public institutions, researchers). The integration of the framework in EOSC will increase the volume of data hosted and processed, to reach a wider set of end-users; improve compliance to data FAIR principles and reinforce the robustness of the whole service infrastructure.

• UMSA (Untargeted Mass-Spectrometry Analysis) [23]: UMSA aims at processing mass spectrometry data to correlating the whole spectra (ie. all the present compounds) with other data (social, medical, other sample analyses, etc.) to work with more complex hypotheses on the impact of environment in human health. The data are unrecoverable, therefore long-term data storage is required, together with appropriate data curation. By means of the integration in EOSC, uniform access to data and computing resources are provided, scaling the service to the target European-wide user community.

• MSWSS [24] (Water Supply Systems modeling and analysis): MSWSS is a service for modeling and analyzing Water Supply Systems which integrates the analysis of toxins in drinking-water supply networks with water distribution network simulation. It allows water infrastructure operators and researchers to analyse hazardous events (e.g. toxin propagation within a pipe system) and may be used for preparation of risk management plans for water utilities. The integration with EOSC computing infrastructure and data sharing services will enable modelling more complex water supply systems and increase the number of scenarios for the analysis.

• O3AS (Ozone Assessment Service) [25]: The O3AS service provides an invaluable tool to extract ozone (O 3 ) trends from extensive climate prediction model data to produce figures of stratospheric ozone and figures of dates by when depleted O 3 recovers to pre-ozone hole levels. This service is conceived to assist scientists in visualising ozone data from large climate models by calculating dates for the recovery of the ozone layer and providing trends of the ozone abundance in the atmosphere to produce results in the form of figures in publication quality.

The integration of the service in EOSC has increased its capacity to process large volumes of data (in terms of TBs) and to facilitate the management of the complex workflow to generate key metrics. Climate model data has many fields of physical quantities. The relevant quantities for the ozone column calculation has to be extracted and processed in order to be visualized efficiently. A pre-processor is running on an HPC system reducing the large data set so that it can be read by an REST API to produce figures without noticeable delays.


## Gaps and Bottlenecks analysis

Before starting the integration of the thematic services in EOSC, we have performed a deep analysis with each one of them to properly identify in advance the needs and requirements to increase the capacity, performance, reliability and/or functionality of the services. Thus, every thematic service has made an analysis of the technical services used for managing the users, computing and data. Table 1 shows a summary of the limitations, lacks and needs identified by the Thematic services of the project.

In a preliminary analysis of the results, several technical commonalities and differences have been identified. All thematic services share the importance of using a robust AAI compatible with the ones used by the target institutions. With respect to resource management, all services have the interest of dynamically provisioning processing resources, most of the cases on demand. However, thematic services have different needs: from a dynamic dedicated cloud backend to an elastic cluster that shrinks or grows according to the workload, and even need to access external High-Performance Computing (HPC) and High Throughput Computing resources for massive Batch jobs execution. Regarding job Management, most thematic services use batch queues (like SLURM [26] Batch queues or Galaxy [27]), which could be extended to support containerised jobs. The usage of Kubernetes to orchestrate microservices and job queues of containers is also considered.

The most challenging part is the management of data. Thematic services have identified important issues on transferring and accessing large volumes of data and require smart caching, advanced data transferring and massive persistent data storage. There are two main approaches to cope with storage, from deploying their own datastore, e.g. DATAVERSE [28] instance; to the integration of external Data Infrastructures, like EGI DataHub [29] and EUDAT [30]. Moreover, services need to ensure the compliance to the FAIR principles to facilitate the access, cataloging and reusage of the data Thematic Service

Limitations and needs WORSICA -Improve download speed and number of concurrent downloads of satellite images.

-Increase storage of the images needed for the algorithm.

-Increase computational resources: GPU and RAM to speedup the image processing.

-Seamless authentication and authorization for end users.


## SAPS

-Need for a larger-scale deployment: computing, storage and data access.

-Scalability and standardisation of services -Integrated and widely supported AAI GCore -Overcome limited access to data repository due to network bandwidth restrictions.

-Infrastructure resources for processing and reprocessing large data sets.

-Data delivery volume. Increasing size of files to be delivered to users.


## SCIPION

-Insufficient Cloud resources for the workflow: GPUs, CPUs and RAM -Need of a Resource Management able to optimize the use of cloud resources.

-Storage limitations and data transfer performance: 1-3 TB raw data.

-Distributed and shared file system.

OpenEBench -Need to work on heterogeneous systems to reach Life Sciences Communities -Need to efficiently store processed data and workflows in a FAIR manner.


## LAGO

-Limitations on data preprocessing.

-Needs data storage that copes with FAIR, curation and harvesting; -Need for computing power for simulations, together with optimal scheduling.


## SDS-WAS

-Lack of services needed for Data storage and curation.

-Lack of computing power for data analysis on-demand.

-Lack of reliability of data sources, especially about observations UMSA -Long-term data storage is required, together with appropriate data curation.

-Tracking provenance of the secondary (derived) datasets.

-Need for reimplementing UMSA algorithms to deal with sparse data.


## MSWSS

-Needs data protection measures because of the usage of confidential data.

-The data has to be stored in a private storage only.

-Implement security policies to protect VMs.


## O3AS

-Requires larger storage resources, specially improving data availability -Fast handling of big data generated by their services. Thus, a storage service able to manage, together with the data, metadata and unique data identifiers will be required. Note that not all the services have identified gaps in all the previous aspects so each thematic service will focus the adaptation in the aspects that are more relevant according to the bottlenecks. Solutions available in the EOSC marketplace will be studied and prototyped in the next section before adapting them into the thematic services.


## Analysis of the EOSC Portal Catalogue and Marketplace

In this section, our goal is to identify the key EOSC tools and services that can address the issues and needs analyzed above. Considering the gaps and bottlenecks identified, this analysis also considers potential alternatives of the current technical services used by the thematic services to overcome such issues.

The EOSC Portal Catalog & Marketplace [8] has been developed from the perspective of users, identifying the needs to be supported and facilitating all the actors involved in implementing an open approach to science in a sustainable way. The catalog has more than 320 entries registered by the end of 2021, covering resources from several categories. According to a functional perspective, we can organise them into six categories:

• Access physical & eInfrastructures: offering generalist resources like virtual machines and containers as well as storage and network transport connectivity. By the end of 2021, 62 resources are listed under this topic. This category include Compute resource providers, workload managers, Resource orchestrators and data providers. Some of the services were thematic (e.g. discipline-specific). We identified 6 generic services that could address the requirements of the thematic services: B2SAFE for long-term data preservation, EGI Cloud Compute to provide IaaS cloud resources, EGI DataHubto provide online cloud storage resources, EGI High-Throughput Compute for batch workloads, EGI Workload Manager to orchestrate multi-site batch resources, and Infrastructure Manager -IM to deploy virtual infrastructures on top of cloud offerings), according to the following criteria: Generic purpose, interoperability and support. A brief description of the services is provided next:

-EGI Cloud Compute [31], an IaaS from the EGI Federated Cloud that enables the user to deploy and scale virtual machines ondemand.

-EGI HTC Compute [32] enables running computational jobs at scale on the EGI infrastructure, which is provided by a distributed network of computing centres and offers more than 1,000,000 cores of installed capacity, supporting over 1.6 million computing jobs per day.

-EGI Workload Manager [33], a service to manage and distribute your computing tasks in an efficient way while maximising the usage of computational resources.

-EGI DataHub [34], a service that brings data close to the computing to exploit it efficiently and can be used to publish a dataset and make it available to a specific community or worldwide across federated sites.

-B2SAFE [35] is a service for long-term preservation of the EUDAT Data Collaborative Infrastructure, one of the largest e-infrastructures in Europe offering permanent storage capacity and integrated management services for research communities. EUDAT also provides other services such as B2SHARE, B2FIND, B2ACCESS.

-Infrastructure Manager (IM) [36] • Aggregators & Integrators , where we can find several tools and utilities to facilitate the access to services and resources, by means of indexing and annotation. Out of the 22 resources available in this category, we identified the Dynamic DNS service [37], to easily add a DNS name to an instance deployed in the virtual infrastructure of EOSC, the EGI Fedcloud client [38], that facilitates the access to the federated cloud computing platform, and B2FIND [39], another EUDAT's service, to annotate research objects, considering the requirements of the thematic services and similar criteria as in the first item.

• Processing & Analysis mainly aimed at facilitating the management of computational resources and the scheduling the execution of workloads. Despite this is the category that accumulates the highest number of services, most of them are discipline-specific. Moreover, some of the resources where already listed under the first item. Here we can find Elastic Compute Clusters in the Cloud (EC3) [40], a tool to deploy virtual elastic clusters on top of IaaS clouds, and B2Handle (another EUDAT's service) to provide persistent identifiers to resources.

• Security & Operations aims at guaranteeing that the overall system and the services operate securely and according to standard. In this case, thematic instances for authentication and authorisation may be preferred as researchers in the community already have acquire credentials. For this purpose we identify the EGI Check-In [41] service, the B2ACCESS [42] and EduTeams [43], along with ELIXIR AAI [44] which is not listed in the catalogue.

• Sharing & Discovery relates both to services that produce data relevant to specific disciplines and horizontal services for data deposit and annotation. Only the service B2SHARE to enable sharing and publishing research data is considered. The catalogue also includes an instance of Dataverse, although we have decided to deploy our own instance. The Dataverse Project, developed by the Harvard's Institute for Quantitative Social Science (IQSS), along with many collaborators and contributors worldwide, is an open-source web application to share, preserve, cite, explore, and analyze research data.

• Training & Support aims at facilitating the access to high quality technical information and tailored training materials. Services in this category are not considered.

All these services have two different access modalities:

• Direct access. This model is used by services which are instantiated upfront and which do not require intensive access to resources, or resources are provided directly by the user (e.g. EGI Check-in or Infrastructure Manager). Users are automatically forwarded to the service endpoints.

• Access through orders. This model is used in services that require a non-trivial amount of resources (e.g. EGI Cloud Compute or B2SAFE). In this case, the user normally has to choose between different offerings, which may end up into costs.


## Adoption of EOSC services

As we have shown in section 3, the ten thematic services have complementary requirements and features. However, in general they share needs on four different categories:

Authentication and Authorization Infrastructure (AAI). All services require users to be authenticated and authorised. In some cases, there is a need for delegation from the users that access the platform for accessing data or processing resources. In those cases, it is mandatory to have a coherent single-sign on mechanism. Other cases may require an AAI linked to popular scientific IdPs and implement the authentication via Virtual Organization membership. From the tools and services we identified from the TSs, EGI Check-in has revealed to be a widely accepted choice. Another option analyzed is B2Access, mainly for interacting with the infrastructure. There are also few cases in which users will use federated credentials to access the services -mainly related to storage.

Workload Management. Most of the cases deal with the execution of a set of batch jobs. In those cases, workload managers should be integrated to better take advantage of the computing resources. This will provide the capability to deal with a larger capacity and larger workloads. The options here range from using a standard batch queue (SLURM), that can be eventually powered up with automatic elasticity, to the usage of Kubernetes for the orchestration of a container-oriented approach.

Resource Management. Most of the thematic services require deploying a virtual infrastructure where the services that provide the functionality and the processing will take place. In most cases, the use of Infrastructure Manager (IM) or Elastic Compute Clusters in the Cloud (EC3) client have been identified by most of the thematic services as a technology capable of filling in this gap. Both tools could provide the capability of defining a virtual infrastructure as code and deploying it on the cloud. IM (for static infrastructures) or EC3 (for dynamic infrastructures) together with recipes for K8s, Slurm or Galaxy clusters on top of a dynamic dedicated cloud backend is the preferred solution. Moreover, some thematic services require links to external HPC resources (like Marenostrum in BSC) and HTC resources (EGI HTC compute) for the execution of massive Batch jobs.

Data Storage. The services need to have a storage connected to the processing that can be efficiently accessed. In this case, there is a wide range of different solutions proposed or implemented in the thematic services, rang-ing from external solutions like EGI-DataHub, B2SAFE and B2SHARE to local solutions based on Nextcloud, Dataverse, Elasticsearch and WebDav, where tipically the resource manager will be also in charge of deploying and configuring their own Datastore instance (e.g. DATAVERSE instance). Table 2 summarizes the selection of tools and services performed by each thematic service for the fourth categories detected. As a summary, three different (although compatible among them) AAI methods have been integrated (EGI Checkin, B2ACCESS, Life-Sciences AAI and eduTEAMS). Job scheduling ranges from solutions based on containers (using Kubernetes) to solutions using batch queues (mainly based on SLURM), supported in some cases by workflow frameworks such as Galaxy and instantiated through EC3. For the interaction with cloud resources, TOSCA [45] and RADL recipes have been developed for Infrastructure Manager. Finally, data access is performed through different solutions such as Dataverse, EGI DataHub One-Data, B2SHARE and B2SAFE, which clearly states the complexity of the data management issue and the wide range of solutions.

To sum up this section, with the adoption of all the services and technologies depicted in 2, thematic services have experienced all these improvements:

• Integration of standardized AAI IdPs to facilitate user management.

• Improvement of processing backends by replacing single computing instances with batch job queues, container management platforms or clients to high-throughput computing backends.

• Publishing the output results in persistent repositories.

• Improving repeatability and platform-agnosticism by describing the application topologies as code using standard TOSCA language.

• Self-management of resources to reduce maintenance costs.

• Persistent Identifiers (PID) annotation of output data and integration in official harvesters.

Thanks to the rich analysis of the experience of these ten thematic services in the adoption of several tools, services and technologies to improve and solve their needs, the path to follow for a new scientific use case is far more easy. However, to clarify even more this process, and to easily identify the key services and technologies selected, we present in next section a generic application integrated in the EOSC ecosystem.  


## Application Modelling

This section uses as input the experience of the ten thematic services of EOSC-SYNERGY to define a canonical generic application architecture leveraging the services identified in the EOSC Marketplace catalogue in section 4. Thematic services that have similar requirements as those described in section 3 can use as basis this architecture that relies on several tools and services from the EOSC ecosystem, together with well-known frameworks and technologies of the cloud computing paradigm, all of them carefully selected taking into account the selection made by the thematic services.  First of all, after analyzing our ten use cases, we identified two different deployment scenarios: i) a single instance of the service shared by several users or communities, offered as a web portal able to manage users, data access, processing and visualization, supported by a shared or dedicated pool of resources (e.g. WORSICA or OpenEBench); and ii) an instance of the service deployed on demand, where each user deploys his/her own instance of the service on Cloud resources based on a combination of TOSCA recipes with Docker containers (e.g. SAPS, SCIPION).

Regardless of the approach chosen, Figure 1 shows the architecture of this generic application that relies its deployment on top of the EGI Cloud Compute platform. The first layer that has been considered as essential by the thematic services is the authentication and authorization infrastructure. For that, EOSC offers the EGI Check-In service, that has been the most popular solution adopted by the thematic services. This service can be easily integrated with a service that is exposed to users through a web portal, and it will be used by both the application manager and the end-users to access the EOSC resources and the service itself. Once the user has been properly authenticated, and depending on the usage model that the service wants to use, he/she will access to the Application User Interface of the scientific application itself or to the portal of the resource manager that will facilitate the deployment of the scientific application instance. In the second scenario, the user will be redirected to the portal of IM or EC3 to deploy a virtual cluster configured on demand for his own usage. The selection between IM or EC3 has to be taken depending on the needs in terms of resource consumption. If elasticity is required, the tool to be used will be EC3. Otherwise, IM is the best tool to provide a static infrastructure configured on demand. Both solutions will require the preparation of a recipe where the application manager specifies the required steps and commands to properly install, configure and deploy the scientific service, together with the credentials to access the cloud provider.

In order to take advantage of the virtual infrastructure where the scientific application is running, we need to rely on a workload manager. From the analysis we have made, we have identified two different approaches: i) a traditional batch job queue, managed by the well-known SLURM scheduler; or a solution based on the containerization of jobs, where kubernetes has proven to be the most popular scheduler. Both options are feasible, depending on the approach that the scientific service wants to follow. However, the adoption of one of these workload managers might require an effort adapting the architecture of the tool to it, so this duty has to be consciously analyzed.

Finally, for the data storage, our recommendation is to use a solution that supports metadata to comply with FAIR principles, i.e. to make data Findable, Accessible, Interoperable, and Reusable (like Dataverse or B2SHARE). No matter if the storage solution will work locally or it will be an external service, one of the most important aspects is the support to metadata to properly index and facilitate reusing the data generated by the services, specially if this will be of interest to other researchers of the area.


## Service instantiation

In this section we want to exemplify how the adoption of the new EOSC tools and services can address the gaps and bottlenecks detected by some of the project's thematic services. Specifically, we present seven examples from seven different scientific services showing the integration in each one of the categories described in section 5. We have omitted the integration with the authentication and authorization service of EOSC (EGI Check-in), because this is a well-known process that is properly documented [46]. Next subsections cover the cases for the rest of categories.


### Workload Management

During the analysis of the thematic services, we have detected two main needs for the workload management: i) services that use a more traditional approach relying on local resource management systems based on queues of jobs; and ii) services that encapsulate the tasks in a container and rely on a container-based management system. To exemplify these two models, we analyze both Scipion and O3AS.


#### Batch job oriented

The Scipion service aims to facilitates the life to users from the Instruct-ERIC [47] Research community for processing their electron microscope data. Users who obtained her data through an Instruct granted project in an Electron Microscopy facility can request the use of the Scipion service by contacting the Instruct Image Processing Center (I2PC). Then, the request is reviewed and if the available quota permits it, the I2PC administrator will deploy a cluster using the Infrastructure Manager (IM). The user will then receive an email with instructions on how to access the front-end node and how to copy the data to start processing. She will be able to use the service for a maximum time of one month although if no other service requests are pending this time might be extended. The cluster will be destroyed by the I2PC team after the granted period finishes, giving the user enough time to download her results.

As shown in Fig. 2 the cluster is defined in a TOSCA description that includes Ansible recipes and docker images to launch and configure the different nodes in the cluster. This TOSCA description is deployed through the IM. Once the cluster is running the user will be able to transfer the data to the front-end node and access the service using either a VNC client or NOVNC through a web browser. The front-end host node runs a SLURM master and a Docker container that includes Scipion and related Cryoem packages configured to launch their jobs through SLURM. Once a job is sent to the queue a Docker container is run in one of the worker nodes to run the Scipion command.

Hardware resources in which the cluster is deployed are part of the EOSC EGI Cloud Compute service that control access through Virtual Organizations (VO). In the case of the Scipion service deployment is only granted to members of the cryoem.instruct-eric.eu VO. The cluster shared storage is currently based on a local Ceph disk.


#### Container based

O3AS (using Kubernetes) O3as service is composed of three main components (Fig. 3): o3skim to reduce the original data to the parameters of interest, o3api to provide an API-based access to the skimmed data, and o3webapp for a user-friendly web interface (in development). O3skim runs on the local HPC system, while o3api and o3webapp are deployed in the Kubernetes system for the container scalability ensuring a fast enough response to user requests. If no Kubernetes system is available for the service providers, it can easily be instantiated by the means of Infrastructure Manager (IM). Then the following steps are applied: 1. Install and configure cluster issuer to handle Certificates for secure HTTP connections, e.g from LetsEncrypt. 2. Initialize Ingress resource to route external traffic. 3. Deploy o3api component as a container with the pre-configured Prior-ityClass. 4. Add Horizontal Pod Autoscaler (HPA) to respond with more containers on higher than usual loads. 5. Finally, instantiate o3api as a service.

If an already existing Kubernetes cluster is used instead, steps 1-5 have to be adjusted accordingly.

The output of the services is a set of projections of the ozone distribution. Example figures for climate models with projections for ozone until the year 2100 are shown in Figure 4.


### Resource Management

Regarding resource management in cloud computing providers, we have identified two different approaches: i) static infrastructures, where the number of nodes that compose the platform remains constant during the lifetime of the service; and ii) dynamic infrastructures, where elasticity models are applied to the platform to adapt its size to the workload, thus allowing to reduce costs and wasting resources. As in the previous section, we have chosen two thematic services to exemplify the adoption of these two different solutions. 


#### On-demand fixed infrastructures

The flexibility for choosing the computing platform was one of the objectives of LAGO TS. Beyond the elasticity or the automatic management, the priority is providing resources that accomplish the needs of every specific calculation, and so these requirements may be as variable as its parametrization. Some of these simulations may face intensive requirements, such as scratching up to several TBs of data; accessing to many files through Internet; continuously processing data in batch mode; or even sporadic calculations for demonstration purposes and scholars. Public clouds such as EOSC EGI Cloud Compute can tackle many of these tasks, but require an upfront reservation of the resources by demand and fix their environment. To be able to face all this different approaches, three different services were integrated: the Infrastructure Manager (IM) service, the software encapsulation in standardised Docker images and the profiting of the ubiquity of the OneData cloud storage (EGI DataHub sevice). These technologies allow dynamically instantiating the virtual infrastructures needed, which are maintained fixed over days or weeks [20].

The resulting architecture is shown in Figure 5. To fix a temporal infrastructure on public clouds, researchers dynamically apply for virtual ma- chines or batch clusters through the IM service. Users can create any kind of cluster following, not only their own preferences, even the needs of specific calculations. In this sense, SLURM was a good choice as it is commonly used by many LAGO collaborators and the behaviour could be similar to those used by other TS, like, e.g., Scipion, as it is described in the subsection 7.1.1. However, there are scientists would rather prefer to manage other implementations, such as Kubernetes, more suitable for Docker instances. Additionally, when tasks require scratching at the order of TBs, the user would not be allowed to spend the space accumulated by several computing nodes. In these cases, single virtual machines, are the most suitable choice. All these infrastructures are deployed by IM after a few clicks in its website.

On the other hand, calculations can be arbitrarily performed by researchers by running the official LAGO docker images stored at DockerHub, which are periodically released by the CD/CI pipeline built on the JePL service [48]. Thanks to the virtualised approach, the software encapsulated in these images can be run on any platform supporting Docker. However, as the FAIR paradigm has to be fulfilled, all the LAGO software is always bound to the OneData cloud storage (DataHub) and it comply with the AAI procedure for the LAGO VO in every run. Thus, independently the computing platform, results are always identified by PIDs and they are browsable at the DataHub portal and findable by public harvesters such as B2FIND.

Therefore, the on-demand provisioning of adaptable infrastructures supporting Docker through the IM service, jointly the cloud storage via DataHub, allow users accomplishing their research without depending on other services.

As an example, we deployed a SLURM virtual cluster counting on 10 nodes with 16 Intel Xeon E7 cores and 250 GB of shared memory and disk. Then, we simulated the expected flux of the atmospheric radiation during the interaction of cosmic rays with the Earth atmosphere for every LAGO detector [49]. We computed from 1 to 7 days of the expected flux at high altitude or antarctic sites, reaching up to 1 year of the energetic flux that is accounted for volcanic risk studies. These new integration times is an enormous statistical improvement when it is compared with previous results obtained in LAGO. Note that, e.g., a 24-hour flux in one of the high latitude (Antarctic) detector involves the simulation of ∼ 1.9 × 10 9 different cascades. Thus, we simulated the impressive figure of > 10 12 particles spending > 300 kCPU-hours, generating > 5 TB of synthetic data and metadata.   Results shown in Figure 6 allow estimating radiation doses at different altitudes, which are currently used for designing new detectors; shielding in-struments (e.g., HPC facilities); calculating the reference HEP flux for underground laboratories, volcanic risk assessments and mining prospecting [50].


#### Elastic infrastructures

For cloud applications with varying load the static infrastructures might end up with resource waste. In order to adapt to the dynamic demand for computing capacity, the MSWSS service uses Elastic Cloud Computing Cluster (EC3) tool [40] to create an elastic virtual cluster on top of the EOSC computing resources. EC3 CLI tool provides a set of pre-defined templates which can be combined and customised. It also allows to define custom templates with integrated Ansible scripts. This is used to create a template with strengthened security settings and additional configuration commands specific for the MSWSS service.   Figure 7 shows the architecture of the MSWSS service with the interaction of the service operator and users. The users interact with the service using Galaxy portal where they can manage their data and submit jobs to the elastic virtual cluster for processing. The output data are stored in within MSWSS service and can be downloaded for post-processing tasks.


## EOSC Cloud IaaS

The service operator deploys the service using EC3 tool using the customised template. Once the MSWSS service is deployed the CLUES service monitors SLURM batch system and deploys new virtual worker nodes as needed and automatically re-configures the batch system. The deployment and configuration of worker nodes is performed using the Infrastructure Manager (IM) [51] service. To speed-up the deployment process the worker nodes are instantiated from a snapshot of fully deployed worker node (golden image). This allows to decrease the start-up time from 21 to 5 minutes. It also helps to solve the issue with pending security updates with respect to the vanilla image and potential need to reboot the worker node for the updates to be applied properly. The golden image is maintained by the service operator in up-to-date state. The security is important also for the communication inside the virtual cluster. OpenVPN system is used to create secure connections inside the cluster and protect the data transfers. It also allows to span the virtual cluster over the resources from different Cloud providers.


### Data Storage

Finally, in the field of data storage, we have also observed three different approaches: i) local storage; ii) external storage and iii) hybrid approach. The next subsections analyze the use cases of WORSICA, OpenEBench and UMSA to illustrate the three approaches.


#### Local storage

WORSICA uses the Dataverse application to manage the data produced by the service and disseminate it to the research community and the public in general. The first version of WORSICA's data manager was developed as a local repository, using the architecture presented in Figure 8. This approach raised several constraints to the adoption of a FAIR compliant data management paradigm, such as i) the lack of a unique global identifier for the datasets produced; ii) the data was not accessible and stored in multiple places internal to the service; iii) nonexistence of metadata for each dataset; and also iv) the access to the data did not follow the controlled vocabularies that apply to FAIR principles. In order to surpass the previous oversights, a Dataverse REST API that allows running all necessary operations efficiently, like:

1. the ability to implement in any language only being dependent on the provided interface without any library requirements; 2. the capability to easily maintain the WORSICA code in parallel with Dataverse service updates; 3. Moreover, provide the features required to share sensitive data with the public.

The current architecture of WORSICA's data manager evolved and can be seen in Figure 9. The WORSICA service is now working with Dataverse to automate the data availability and use services for and distribute credit to the data creator. Dataverse allows the creation of multiple virtual archives called Dataverse collections. Each Dataverse collection contains datasets, and each dataset contains descriptive metadata and data files. Therefore, this version of WORSICA enables datasets to be linked in Dataverse to the appropriate ontologies to increase interoperability and data FAIRness. Variable names can also be included in datasets metadata in the native language (Portuguese) and get Universal Resource Identifier (URI) for those entities in controlled vocabularies (e.g., in the case of WORSICA, a DOI -Digital Object Identifier is created). Furthermore, standardized metadata fields are available in Linked Open Data Cloud through standard machine-to-machine interfaces available in Dataverse.


#### External storage

OpenEBench makes use of B2SHARE for long-term availability and storage of scientific benchmarking datasets. The adoption of EUDAT's technical standards, data models and policies helps OpenEbench to further enforce the FAIR-compliant data management of the platform. One of the major capabilities gained through the integration with EUDAT is the minting of Digital Object Identifiers (DOIs) for the benchmarking data collections generated in OpenEBench.

A variety of dataset types are involved in the benchmarking workflows at OpenEBench. Those datasets cover the reference data used as gold standard data, the predictions submitted by participants as well as new datasets like the actual results scoring and ranging participants, provenance reports and metrics' plots. In this way, a compact and human-readable set of data is ready to be referred to in scientific publications, promoting transparency, reproducibility and data reuse. Furthermore, EUDAT registries provide rich metadata fields for easing data discovery, so thus submitted collections could be annotated with cross-links to OpenEBench for further insights. Actually, OpenEBench is one of the EUDAT registered research communities benefiting from a particular extended metadata model and customized access rules. It facilitated a better integration with OpenEBench, implemented through a REST-based programmatic data publication workflow triggered from the platform Web GUI.

Within OpenEBench ecosystem, benchmarking data is accessible on the Web or via specific REST and GraphQL APIs. Nextcloud and MongoDB are the technologies used to store datasets and metadata respectively, always operated under a FAIR-compliant data governance plan that considers, for example, unique and accessible identifiers, document versioning, provenance preserving metadata, strict publication rules or a formal benchmarking data model. When a benchmarking events manager or developer participating in a given event is willing to publish its data outside the platform, they register their B2SHARE API token in OpenEBench and initiate the publication process to B2SHARE via the Web GUI:

• OpenEBench composes the data collection with a specific metadata form, validates it, and programmatically submits both, the data and metadata, to the B2SHARE server.

• Over HTTPS and on behalf of the user, the platform implements the full EUDAT data publication workflow through the B2SHARE REST API. The outcome is a DOI associated with the new registry, which is captured and saved in OpenEBench to keep both systems cross-linked. Eventually, published benchmarking datasets can be consumed using both B2SHARE and OpenBench platforms.

The OpenEBench data flow can be seen in Figure 10.


#### Hybrid approach

UMSA leverages three different storage classes for different purposes. First, data acquired by the instruments (mass spectrometers) are stored on a traditional POSIX filesystem, implemented as RAID disk arrays and cluster of GPFS servers, re-exporting the volumes via NFS and CIFS to the clients. This is a technology with limited scaling (up to petabytes per filesystem), however, it is proven for decades and stable, suitable for the primary experimental data that are irrecoverable otherwise. This storage is also backed up Figure 10: OpenEBench data flow. Users execute evaluation benchmarking workflows, which use and produce datasets. That data is locally stored in MongoDB (for metadata) and Nextcloud. It can be then submitted to B2SHARE, using their API REST, which will lead to minting a DOI. Then, it is mapped and saved to the OpenEBench database for cross-linking purposes. Published benchmarking data can be consumed using both B2SHARE and OpenBench platforms.

weekly to a remote site provided by the CZ national e-Infrastructure 1 .

The UMSA service itself is based on Galaxy, which mounts the primary data filesystem (above) read-only, and it exposes the datafiles to the users via its Data libraries 2 , with appropriate access control, and without the need to copy the large files.

Standard Galaxy setup requires a shared filesystem mounted at both head and worker nodes (alternate setups require lot of file transfers for each job, which we cannot afford). It can be either a single-tier storage, or it can serve as the first tier (cache) of object storage (the second tier). We use the two-tier setup, with a NFS-mounted SSD-only shared filesystem, with typical usage up to dozens of TB only, fast enough not to slow down data processing with I/O, and S3 object storage (provided by the national e-Infrastructure again), which scales up to many PB easily.


## Related Work

This section describes other alternative thematic services and their differences with respect to the provisioning or performing model of the services in this article. Each thematic service has a dedicated paragraph to better analyze its case.

The WORSICA thematic service aims to study the evolution of coastal zones and water in inland areas for better coastal, estuarine, and natural resource management. Other services study these subjects, such as [52,53,54,55]; however, the WORSICA service has the particularity of being freely accessible to the scientific community and performing simulations on-demand for the entire globe. Furthermore, the development of a service like WOR-SICA can only be achieved through the collaborative work of the EOSC network regarding SaaS infrastructures and products with federated access, described in Section 4, due to its need for a robust and scalable computational infrastructure to serve its users.

GCore thematic service leverages cloud computing power to enhance the processing resources available in a Space Ground Segment. The adoption of a dynamic cloud backend is a trend in different EO missions. Programs as Copernicus envisage the use of cloud computing to change the current on-premises processing approach to a on-the-cloud approach and offer the environments as a service. Entities as EUMETSAT (European Organization for the Exploitation of Meteorological Satellites) envisage a new design of a multimission processing infrastructure using the cloud advantages in order to allow extending the resources and made them available to several missions. GCore follows these approaches in order to extend the functionality with the use of cloud resources in order to break the bottleneck that a on premise classical system can obtain during the re-processing tasks associated to a particular mission for example. During this task a massive use of processing power is needed in the nominal platform being able to affect to the nominal operations of the mission. The capacity of GCore to deploy additional processors on the cloud is used to reduce the impact of such a task. This approach can also be used for data archiving and cataloguing the products resulted from the processing in the cloud making them available to perform higher processing levels directly on cloud using as a service processors specifically defined published previously in a market place.

SAPS applies algorithms to estimate evapotranspiration (ET), a technique that has been widely used in several countries, and in areas subject to different weather conditions [56], [57]. The execution of these algorithms suppose high computational demands, both CPU and memory-intensive, and the archival of the output data generated consumes a substantial amount of storage resources [58]. Recently, several software packages and libraries with implementations of ET estimation have been made publicly available, such as the ILWIS software [59] or the 'Water' procedure [60]. However, those implementations have been developed as standalone artifacts made available for individual use by researchers, typically executed in their personal computers, that complicate considerably the sharing of processed data and might suppose limitations on the computing capacity locally available. A different approach is taken by the Google Earth Engine (GEE) platform [61] an initiative to facilitate the implementation and execution of scientific workflows that consume satellite imagery as input. However, this service is under service conditions imposed by Google, that might not always fit the community needs [62]. Conversely, SAPS in combination with EOSC services and resources, follows an innovative approach based on the deployment of ondemand SAPS instances on top of federated resources. The service is highly configurable, allowing the selection of the algorithms used to estimate ET, and provides a standard output data and metadata that can be easily shared among researchers of the area.

The ScipionCloud service offers a ready-to-use infrastructure in the cloud to users that aim to process their CryoEM data. A similar approach is followed by Stion [63], a web application that provides on-demand access to GPU instances on AWS for biomedical researchers to process Cryo-EM data. This solution offers automatic deploying of instances (virtual machines) but does not integrate a batch system. It also sets up an auto-shutdown mechanism to power off instances after a certain period of time, which is risky and insufficient from our point of view. On the contrary plans to integrate ScipionCloud with EC3 guarantees a much better way to optimize the use of cloud resources than Stion's implementation. Besides, to use Stion it is mandatory to provide your own AWS account which might be a drawback for many users both in terms of complexity and cost. Another approach found in the CryoEM world is the work done by Cianfrocco's lab at the University of Michigan, which offers several tools in this area. The first interesting tool [64] is the integration of a 'AWS batch system' in one of the most popular software packages used for CryoEM processing; Relion. This batch system allows to send Relion commands to AWS instances, which includes deploying, running and shutting down the instance. This approach was only integrated in an old version of Relion which might imply that it was not a big success in the CryoEM community. The second tool is COSMIC [65], a freely available web platform for submitting cryo-EM jobs through the cloud to Comet, a cluster at the San Diego Supercomputer Center. Although the use of COSMIC has no cost access and this tool is available for everybody users might prefer to work on their own server instead of preparing their processing workflows to be sent step by step to a cluster.

Regarding the use of external storages for sustainability, OpenEBench is not the only platform designed around FAIR principles that ensures the availability of its data and metadata through EUDAT services or similar data infrastructures. WorkflowHub [66] is applying a similar strategy for publishing scientific workflows, wrapping them into enriched Research Object Crates (RO-Crate 1.1 specification), which include data resources, semantic annotations and all additional information that guarantees the workflow is reusable and reproducible. In this case, Zenodo is the infrastructure minting the DOI, and the publication process follows DataCite [67] guidelines, one of the broadest cross-domain metadata standards available. Among EOSC resources, other platforms like ROHub [68] propose the use of EUDAT services as an internal storage solution. ROHub is a platform that enables the management, sharing and preservation of research data as Research Objects. It is integrating B2DROP [69] as the underlying technology for the researcher's personal storage space and B2SHARE for DOI minting and sharing.

Software needed for LAGO simulations is managed by ARTI [49], a dataintensive and highly-complex framework designed to calculate the expected flux of signals in any site around the World and under realistic geomagnetic, atmospheric and detector conditions. As the simulated phenomena, i.e., the interaction of cosmic rays with the atmosphere and the detector response to the resulting flux secondary particles, is, essentially, a sequence of stochastic processes, the simulation performed need to integrate the flux over long periods to reduce the impact of statistical fluctuations. Such large times, from several hours to days and even years for some applications such as volcano risk assessments, require the usage of large computing facilities and storage. Suitable simulations will typically spend tens of weeks in current CPUs and they will output several TB of data. Examples of that are the results pre-sented in Subsection 7.2.1, and recently in [20,50]. Moreover, all these data needs to be properly identified, catalogued and curated to accomplish the FAIR principles. Although previous attempts were made to adapt ARTI to its use in distributed high-performance computing infrastructures [70] and for the adoption of data curation standards [71], it was only thanks to the development of OnedataSim [20] within the LAGO TS that it was possible to achieve sufficiently long integration times while complying FAIR principles.

SDS-WAS provide, a part of a huge quantity of materials on dust storms forecast, services of data storage, download and data analysis based on EOSC cloud services (B2SAFE, B2HANDLE). As far as we know there aren't comparable services related to dust storms, in terms of collecting a bunch of numerical models outputs, observational data (in situ and satellite), providing derived graphical (plots) and numerical (skill scores) products with an interactive dashboard application. Those services are related to the areas of Northern Africa, Middle-East and Europe. We considered as "competitors" the other SDS-WAS regional nodes (Asian and Panamerican), but also other services (not only provided by Research Institutions, but also by companies) related not directly to dust, but to air quality forecast or similar. The aim is to see how they approach the information and solve user's needs. While the Asian 3 and Panamerican 4 SDS-WAS nodes are still in a preliminary stage, as they provide a limited set of services, there are some others with a good feedback. We analysed strengths and weaknesses of products visual presentation of following services: METOFFICE 5 , Breezometer 6 , WINDY 7 , Plume Labs 8 . Although most of them have an attractive design, they only offer visual products of their data, not numerical binaries, and the amount of data they provides seems to be not big as what our SDS-WAS service does. Finally, some of them have advertisements and banners that affects significantly the user experience.

UMSA is dedicated to storage and processing mass-spectrometry data, focusing on GC-MS and untargeted analysis of low-abundant compounds. Numerous tools to process MS data exist, ranging from proprietary software by laboratory equipment vendors, third party commercial software, and open source tools of widely varying quality and maturity. Several sub-area specific reviews exist [72,73]. Being based on Galaxy, UMSA can leverage from dozens of mass-spec related tools published by the community. There is ongoing effort to provide Galaxy-based environments focusing on specific MS applications, e.g. [74]. According to our knowledge, there is no such effort matching the UMSA purpose. Another approach can be seen with GNPS [75], which is a web-based computational environment and data treatment environment, centered aroundmolecular networking-visual display of chemical spaces and relationships among compounds. Vast majority of the methods leverage on MS 2 data, which is not the focus of UMSA. On the other hand, publication and storage of the results of the analyses is fairly well organized around the MassBank project [76].

The MSWSS is a cloud-based service capable to exploit the resources from the EOSC cloud infrastructure and FAIR data repositories. There are several approaches for modelling water supply systems offering various features. Integrated Tool for Water Supply Systems Management [77] puts together QGIS database, Epanet hydraulic model, and Google Maps. A web-based EPANET model catalogue and execution environment [78] focuses on model sharing and a model viewing. Another example of integration GIS and the hydraulic model is a tool for an integrated water and wastewater management system in municipal enterprises [79]. According to our knowledge these services do not provide cloud-based elastic computational back-end and they do not implement FAIR principles for data sharing.

O3as has been inspired by the need of having the large plethora of climate model data available in a consistent manner. Because of the ozone assessment 9 that culminates in a publication every four years [80] scientists need to have a quick way on looking at ozone data from climate data to estimate the time when the amount of ozone in the stratosphere has reached a level pre-ozone hole. Often the models have to be collected, the analysis program redone and the values recalculated [81,82]. The O3as thematic service is very modular and can therefore be extended so that not only zonally (i.e. across longitudes) averaged values of ozone data will be shown, but also trends and ozone return rates for different positions on the Earth. Also atmospheric trace gases other than ozone could in the future be included into the analysis.


## Conclusions

EOSC-SYNERGY is building capacities in EOSC through the development of ten data-intensive thematic services oriented to four different scientific disciplines. The adaptation, improvement and quality assessment of those services on a federated data infrastructure strongly aligns with the objectives of EOSC [83]. A key factor for the success of EOSC [84] is performance, i.e, how EOSC as an ecosystem operates and how the resources are used and acknowledged by the users. All the services consume services from the EOSC catalogue, which will provide feedback on the usability and relevance of the model.

However, the characterisation of new applications to join is key to evaluate the rightmost services to be used. This is why the EOSC-Synergy project is developing best practices and experiences to promote EOSC adoption by the research communities by expanding and building knowledge on common interfaces, standards and best practices. This paper presented an application modelling proposal, based on the previous analysis of gaps and bottlenecks performed by ten different services from four different disciplines.

The ten thematic services have been analysed concerning four dimensions (authentication and authorisation; resource management and offering model; workload management including containerisation; and data storage and preservation), identifying gaps and bottlenecks. Those requirements are common to many other scientific services. We selected ten services from the EOSC Marketplace to address these requirements. In a nutshell, AAI solutions such EGI Checkin, eduTEAMS, D2Access and life-science AAI are mature enough to provide a coherent authentication model for a whole application. Applications that require a dynamic backend or on-demand deployment found Infrastructure Manager (IM) and Elastic Compute Clusters in the Cloud (EC3) as reasonable solutions to describe their infrastructure as code and deploy resources according to their workload. Depending on the workload type, job management is driven by SLURM batch queues or Kubernetes services. Preservation of data is obtained through EGI DataHub or EUDAT's B2SAFE and B2SHARE storage services, registering persistent identifiers through B2Handle. Finally, services needing local storage used Dataverse as an OAI-PMH on-premise storage.

The impact of EOSC in the thematic services of EOSC Synergy is mainly composed of three main facts. Firstly, the capacity expansion through the federation of computing, storage, and data resources aligned with the EOSC and FAIR policies and practices. Secondly, software and service quality evaluation of the thematic services is critical to improve robustness and reliance and therefore increase user's experience. EOSC-SYNERGY also focuses on transverse training to facilitate the adoption of technologies and the use of thematic services. Finally the cross-fertilization between different thematic areas has allowed the collaboration between thematic services to take advantage of the developments, solutions, experiences and best practices on authentication and authorization, data storage, resource and workload management.

## Figure 1 :
1Architecture of the proposed solution.

## Figure 2 :
2Scipion service usage and architecture.

## Figure 3 :
3O3as service overview: it consists of three main components: o3webapp (to come), o3api, and o3skim.

## Figure 4 :
4Left: O 3 return dates for a recovery to ozone values in 1980. Right: Timeseries of total O 3 column data showing a decline of O 3 and the subsequent recovery of O 3 in the future.

## Figure 5 :
5On-demand deployment of fixed infrastructures and running LAGO software.

## Figure 6 :
6Left: the energy spectrum of the flux of atmospheric radiation expected at La Serena (LSC) detector in Chile (at sea level) is used for designing, characterizing and calibrate new detectors and sites. Right: the expected flux of >TeV/c particles reaching the summit (5450 m a.s.l.) of the mountain where the ANDES underground laboratory will be installed. Since these particles are capable to traverse up to thousands of meters of rock, being the background signals for neutrino physics experiments and dark matter searches.

## Figure 7 :
7Architecture of the MSWSS service.

## Figure 8 :
8First WORSICA's architecture for managing the data produced by the service.

## Figure 9 :
9Current architecture of the data manager implemented in the WORSICA service.

## Table 1 :
1Analysis of the limitations and needs of the Thematic Services.

## Table 2 :
2Adoption of technologies for each Thematic Service
http://du.cesnet.cz/ 2 https://galaxyproject.org/data-libraries/
http://www.asdf-bj.net/
https://csl.noaa.gov/assessments/ozone/index.html [2022-02-09]

I Staff, IEEE 17th International Conference on EScience. I. Staff, 2021 IEEE 17th International Conference on EScience (eScience), IEEE, 2021. URL: https://books.google.es/books?id= o924zgEACAAJ.

Open science definition. Foster, Foster, Open science definition, https://www.fosteropenscience.eu/ foster-taxonomy/open-science-definition, 2016.

Commission, e-infrastructures definition. E , E. Commission, e-infrastructures definition, https://ec.europa.eu/ digital-single-market/en/e-infrastructures, 2016.

Requirements of scientific applications in cloud offerings. I Blanquer, G Brasche, D Lezzi, Proceedings of the 2012 Sixth Iberian Grid Infrastructure Conference. the 2012 Sixth Iberian Grid Infrastructure Conference12I. Blanquer, G. Brasche, D. Lezzi, Requirements of scientific applica- tions in cloud offerings, in: Proceedings of the 2012 Sixth Iberian Grid Infrastructure Conference, IBERGRID, volume 12, 2012, pp. 173-182.

FAIR Data Maturity Model. Specification and Guidelines. F D M M W Group, 10.15497/rda00050doi:10.15497/rda00050F. D. M. M. W. Group, FAIR Data Maturity Model. Specifica- tion and Guidelines, 2020. URL: https://doi.org/10.15497/rda00050. doi:10.15497/rda00050.

. E Commission, 2020E. Commission, Eosc european partnership proposal, https://bit.ly/ 2RKYmak, 2020.

E Synergy, Eosc synergy portal. E. Synergy, Eosc synergy portal, https://www.eosc-synergy.eu/, 2020.

Eosc portal catalogue & marketplace. E Enhance, E Future, 2021E. Enhance, E. Future, Eosc portal catalogue & marketplace, https: //marketplace.eosc-portal.eu/services, 2021.

Worsica, Water Monitoring Sentinel Cloud Platform. WORSICA, LNEC Portal -Water Monitoring Sentinel Cloud Platform, http://worsica.lnec.pt, 2021.

Monitoring Sentinel Cloud Platform. Water Worsica, WORSICA, Water Monitoring Sentinel Cloud Platform, https:// worsica.incd.pt/index/, 2021.

A high-throughput shared service to estimate evapotranspiration using landsat imagery. J Cunha, T E Pereira, E Pereira, I Rufino, C Galvão, F Valente, F Brasileiro, 10.1016/j.cageo.2019.104341Computers & Geosciences. 134104341J. Cunha, T. E. Pereira, E. Pereira, I. Rufino, C. Galvão, F. Valente, F. Brasileiro, A high-throughput shared service to estimate evapotranspiration using landsat imagery, Com- puters & Geosciences 134 (2020) 104341. URL: https:// www.sciencedirect.com/science/article/pii/S0098300419302961. doi:https://doi.org/10.1016/j.cageo.2019.104341.

SMOS Fast Reprocessing Platform at ESAC, shorturl.at/ lwyQ2, 2019. Living planet symposium. M Zapata, Milan, ItalyM. Zapata, SMOS Fast Reprocessing Platform at ESAC, shorturl.at/ lwyQ2, 2019. Living planet symposium, 13-17 May 2019, Milan, Italy.

M Rodriguez, shorturl.at/dfoIQ. M. Rodriguez, shorturl.at/dfoIQ, 2018.

. Gcore Gcore, Synergy, GCore, GCore overview at EOSC Synergy, https://www.eosc- synergy.eu/thematic-services/g-core/, 2022.

Scipion: A software framework toward integration, reproducibility and validation in 3d electron microscopy. J De La Rosa-Trevín, A Quintana, L Cano, A Zaldívar, I Foche, J Gutiérrez, J Gómez-Blanco, J Burguet-Castell, J Cuenca-Alba, V Abrishami, J Vargas, J Otón, G Sharov, J Vilas, J Navas, P Conesa, M Kazemi, R Marabini, C Sorzano, J Carazo, 10.1016/j.jsb.2016.04.010Journal of Structural Biology. 195J. de la Rosa-Trevín, A. Quintana, L. del Cano, A. Zaldívar, I. Foche, J. Gutiérrez, J. Gómez-Blanco, J. Burguet-Castell, J. Cuenca-Alba, V. Abrishami, J. Vargas, J. Otón, G. Sharov, J. Vilas, J. Navas, P. Conesa, M. Kazemi, R. Marabini, C. Sorzano, J. Carazo, Scipion: A software framework toward integration, reproducibility and validation in 3d electron microscopy, Jour- nal of Structural Biology 195 (2016) 93-99. URL: https:// www.sciencedirect.com/science/article/pii/S104784771630079X. doi:https://doi.org/10.1016/j.jsb.2016.04.010.

Lessons learned: Recommendations for establishing critical periodic scientific benchmarking. 10.1101/181677Lessons learned: Recommendations for establishing criti- cal periodic scientific benchmarking, bioRxiv (2017). URL: https://www.biorxiv.org/content/early/2017/08/31/181677. doi:10.1101/181677.

LAGO: The Latin American giant observatory. I Sidelnik, H Asorey, L Collaboration, 10.1016/j.nima.2017.02.069Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment. 876I. Sidelnik, H. Asorey, L. Collaboration, et al., LAGO: The Latin Amer- ican giant observatory, Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associ- ated Equipment 876 (2017) 173-175. doi:10.1016/j.nima.2017.02.069.

Latin American Giant Observatory. LAGO Collaboration, Latin American Giant Observatory, http:// lagoproject.net, Accessed, December 2021.

A J Rubio-Montero, R Pagán-Muñoz, R Mayo-García, A Pardo-Díaz, I Sildelnik, H Asorey, A Novel Cloud-based Framework for Standardized Simulations in the Latin American Giant Observatory (LAGO), in: 2021 Winter Simulation Conference (WSC). Phoenix, USAIEEEPrintA. J. Rubio-Montero, R. Pagán-Muñoz, R. Mayo-García, A. Pardo-Díaz, I. Sildelnik, H. Asorey, A Novel Cloud-based Framework for Standard- ized Simulations in the Latin American Giant Observatory (LAGO), in: 2021 Winter Simulation Conference (WSC), IEEE, Phoenix, USA., 13-17 Dec. 2021. In Print.

The WMO SDS-WAS Regional. S Basart, S Nickovic, E Terradellas, E Cuevas, C García-Pando, G García-Castrillo, E Werner, F Benincasa, 10.1051/e3sconf/20199904008E3S Web of Conferences. 994008Center for Northern Africa, Middle East and EuropeS. Basart, S. Nickovic, E. Terradellas, E. Cuevas, C. Pérez García-Pando, G. García-Castrillo, E. Werner, F. Benincasa, The WMO SDS-WAS Regional Center for Northern Africa, Middle East and Europe, in: E3S Web of Conferences, volume 99 of E3S Web of Conferences, 2019, p. 04008. doi:10.1051/e3sconf/20199904008.

Baldasano, The Barcelona Dust Forecast Center: The first WMO regional meteorological center specialized on atmospheric sand and dust forecast. S Basart, E Terradellas, E Cuevas, O Jorba, F Benincasa, J M , EGU General Assembly Conference Abstracts. 13309S. Basart, E. Terradellas, E. Cuevas, O. Jorba, F. Benincasa, J. M. Bal- dasano, The Barcelona Dust Forecast Center: The first WMO regional meteorological center specialized on atmospheric sand and dust fore- cast, in: EGU General Assembly Conference Abstracts, EGU General Assembly Conference Abstracts, 2015, p. 13309.

. Umsa, Synergy, UMSA, UMSA overview at EOSC Synergy, https://www.eosc- synergy.eu/thematic-services/umsa/, 2022.

Modelling service for water supply systems. MSWSS. MSWSS, Modelling service for water supply systems, https:// mswss.ui.savba.sk:8443, 2021.

. O3AS Portal -Ozone Assessment Cloud Platform. O3ASO3AS, O3AS Portal -Ozone Assessment Cloud Platform, https:// o3as.data.kit.edu, 2022.

Simple linux utility for resource management. A B Yoo, M A Jette, M Grondona, Slurm, Job Scheduling Strategies for Parallel Processing. D. Feitelson, L. Rudolph, U. SchwiegelshohnBerlin Heidelberg; Berlin, HeidelbergSpringerA. B. Yoo, M. A. Jette, M. Grondona, Slurm: Simple linux utility for resource management, in: D. Feitelson, L. Rudolph, U. Schwiegelshohn (Eds.), Job Scheduling Strategies for Parallel Processing, Springer Berlin Heidelberg, Berlin, Heidelberg, 2003, pp. 44-60.

galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences. J Goecks, A Nekrutenko, J Taylor, T , Genome Biol. 1186J. Goecks, A. Nekrutenko, J. Taylor, T.(2010) galaxy: a comprehen- sive approach for supporting accessible, reproducible, and transparent computational research in the life sciences, Genome Biol 11 (????) R86.

An introduction to the dataverse network as an infrastructure for data sharing. G King, Sociological Methods and Research. 36G. King, An introduction to the dataverse network as an infrastructure for data sharing, Sociological Methods and Research 36 (2007) 173-199.

Towards european open science commons: The egi open data platform and the egi datahub. M Viljoen, Lukasz Dutka, B Kryza, Y Chen, 10.1016/j.procs.2016.08.2942nd International Conference on Cloud Forward: From Distributed to Complete Computing. 97M. Viljoen, Lukasz Dutka, B. Kryza, Y. Chen, Towards european open science commons: The egi open data platform and the egi datahub, Procedia Computer Science 97 (2016) 148-152. URL: https:// www.sciencedirect.com/science/article/pii/S187705091632110X. doi:https://doi.org/10.1016/j.procs.2016.08.294, 2nd Interna- tional Conference on Cloud Forward: From Distributed to Complete Computing.

Eudat: a new cross-disciplinary data infrastructure for science. D Lecarpentier, P Wittenburg, W Elbers, A Michelini, R Kanso, P Coveney, R Baxter, International Journal of Digital Curation. 8D. Lecarpentier, P. Wittenburg, W. Elbers, A. Michelini, R. Kanso, P. Coveney, R. Baxter, Eudat: a new cross-disciplinary data infras- tructure for science, International Journal of Digital Curation 8 (2013) 279-287.

. Egi Egi, Cloud Compute, Service, EGI, Egi cloud compute service, https://www.egi.eu/services/ cloud-compute/, 2021.

. Egi Egi, High-Throughput Compute, EGI, Egi high-throughput compute, https://www.egi.eu/services/ high-throughput-compute/, 2021.

. Egi Egi, Manager, EGI, Egi workload manager, https://www.egi.eu/services/ workload-manager/, 2021.

. Egi Egi, Hub, EGI, Egi data hub, https://www.egi.eu/services/datahub/, 2021.

B2safe Eudat, keep research data safe via data management policies. EUDAT, B2safe, keep research data safe via data management policies, https://sp.eudat.eu/catalog/resources/5d81cb5b-3640- 4430-b46e-fc652e06a4db, 2021.

Dynamic Management of Virtual Infrastructures. M Caballer, I Blanquer, G Moltó, C Alfonso, 10.1007/s10723-014-9296-5Journal of Grid Computing. 13M. Caballer, I. Blanquer, G. Moltó, C. de Alfonso, Dynamic Manage- ment of Virtual Infrastructures, Journal of Grid Computing 13 (2015) 53-70. doi:10.1007/s10723-014-9296-5.

Dynamic dns for vms in egi cloud. EGI. EGI, Dynamic dns for vms in egi cloud, https://docs.egi.eu/users/ cloud-compute/dynamic-dns/, 2021.

Fedcloud client documentation. V Tran, 2021V. Tran, Fedcloud client documentation, https:// fedcloudclient.fedcloud.eu/, 2021.

EUDAT, B2find official webpage, find research data, research data portal. EUDAT, B2find official webpage, find research data, research data portal, https://sp.eudat.eu/catalog/resources/33bc21d5- f53d-4eed-9a15-56f98f5c7f69, 2021.

Self-managed cost-efficient virtual elastic clusters on hybrid cloud infrastructures. A Calatrava, E Romero, G Moltó, M Caballer, J M Alonso, 10.1016/j.future.2016.01.018Future Generation Computer Systems. 61A. Calatrava, E. Romero, G. Moltó, M. Caballer, J. M. Alonso, Self-managed cost-efficient virtual elastic clusters on hybrid cloud infrastructures, Future Generation Computer Systems 61 (2016) 13-25. URL: https://www.sciencedirect.com/science/ article/pii/S0167739X16300024. doi:https://doi.org/10.1016/ j.future.2016.01.018.

. Egi Egi, Check, EGI, Egi check-in, https://www.egi.eu/services/check-in/, 2021.

Official webpage b2access identity & authorisation. EUDATEUDAT, Official webpage b2access identity & authorisation, https://sp.eudat.eu/catalog/resources/d04af0f5-2253-4ee4- 8181-3a5a961ccd49, 2021.

Common elixir service for researcher authentication and authorisation. L , 10.12688/f1000research.15161.1Linden M, Procházka M. 7L. I. e. a. Linden M, Procházka M, Common elixir service for researcher authentication and authorisation, F1000Research 7 (2018). doi:https: //doi.org/10.12688/f1000research.15161.1.

TOSCA: Portable Automated Deployment and Management of Cloud Applications. T Binz, U Breitenbücher, O Kopp, F Leymann, 10.1007/978-1-4614-7535-4_22doi:10.1007/978- 1-4614-7535-4 22SpringerNew York, New York, NYT. Binz, U. Breitenbücher, O. Kopp, F. Leymann, TOSCA: Portable Automated Deployment and Management of Cloud Applications, Springer New York, New York, NY, 2014, pp. 527-549. URL: https://doi.org/10.1007/978-1-4614-7535-4 22. doi:10.1007/978- 1-4614-7535-4 22.

EGI, Check-in guide for Service Providers. EGI, Check-in guide for Service Providers, https://docs.egi.eu/ providers/check-in/sp/, 2022.

Instruct-Eric, Instruct eric structural biology web site. 2021INSTRUCT-ERIC, Instruct eric structural biology web site, https:// instruct-eric.eu/, 2021.

Jenkins pipeline library -official documentation. S B , Pablo Orviz, S. B. Pablo Orviz, Jenkins pipeline library -official documen- tation, https://indigo-dc.github.io/jenkins-pipeline-library/ 2.0.0/index.html, 2022.

Preliminary Results From the Latin American Giant Observatory Space Weather Simulation Chain. H Asorey, L A Núñez, M Suárez-Durán, 10.1002/2017SW001774Space Weather. 16H. Asorey, L. A. Núñez, M. Suárez-Durán, Preliminary Results From the Latin American Giant Observatory Space Weather Simulation Chain, Space Weather 16 (2018) 461-475. doi:10.1002/2017SW001774.

The EOSC-Synergy cloud services implementation for the Latin American Giant Observatory (LAGO). A J Rubio-Montero, R Pagán-Muñoz, R Mayo-García, A Pardo-Díaz, I Sildelnik, H Asorey, 10.22323/1.395.026137th International Cosmic Ray Conference (ICRC2021). PoS(ICRC2021)261, Proceedings of Science (PoS). Berlin, GermanySISSAA. J. Rubio-Montero, R. Pagán-Muñoz, R. Mayo-García, A. Pardo-Díaz, I. Sildelnik, H. Asorey, The EOSC-Synergy cloud services implemen- tation for the Latin American Giant Observatory (LAGO), in: 37th International Cosmic Ray Conference (ICRC2021). PoS(ICRC2021)261, Proceedings of Science (PoS), SISSA, Berlin, Germany, 12-23 July 2021. doi:10.22323/1.395.0261.

Dynamic management of virtual infrastructures. M Caballer, I Blanquer, G Moltó, C Alfonso, 10.1007/s10723-014-9296-5doi:10.1007/s10723- 014-9296-5J. Grid Comput. 13M. Caballer, I. Blanquer, G. Moltó, C. Alfonso, Dynamic manage- ment of virtual infrastructures, J. Grid Comput. 13 (2015) 53-70. URL: https://doi.org/10.1007/s10723-014-9296-5. doi:10.1007/s10723- 014-9296-5.

. G Australia, G. Australia, DEA coastlines, https://cmi.ga.gov.au/data- products/dea/581/dea-coastlines, Accessed, January 2022.

Sub-pixel waterline extraction: Characterising accuracy and sensitivity to indices and spectra. R Bishop-Taylor, S Sagar, L Lymburner, I Alam, J Sixsmith, 10.3390/rs11242984Remote Sensing. 11R. Bishop-Taylor, S. Sagar, L. Lymburner, I. Alam, J. Sixsmith, Sub-pixel waterline extraction: Characterising accuracy and sensitiv- ity to indices and spectra, Remote Sensing 11 (2019). URL: https: //www.mdpi.com/2072-4292/11/24/2984. doi:10.3390/rs11242984.

Mapping australia's dynamic coastline at mean sea level using three decades of landsat imagery. R Bishop-Taylor, R Nanson, S Sagar, L Lymburner, 10.1016/j.rse.2021.112734Remote Sensing of Environment. 267112734R. Bishop-Taylor, R. Nanson, S. Sagar, L. Lymburner, Map- ping australia's dynamic coastline at mean sea level using three decades of landsat imagery, Remote Sensing of Environment 267 (2021) 112734. URL: https://www.sciencedirect.com/science/ article/pii/S0034425721004545. doi:https://doi.org/10.1016/ j.rse.2021.112734.

. Dias Copernicus, Services, AccessedCopernicus, DIAS services, https://www.copernicus.eu/en/access- data/dias, Accessed, January 2022.

Improvements to a modis global terrestrial evapotranspiration algorithm. Q Mu, M Zhao, S W Running, 10.1016/j.rse.2011.02.019Remote Sensing of Environment. 115Q. Mu, M. Zhao, S. W. Running, Improvements to a modis global terrestrial evapotranspiration algorithm, Remote Sens- ing of Environment 115 (2011) 1781-1800. URL: https:// www.sciencedirect.com/science/article/pii/S0034425711000691. doi:https://doi.org/10.1016/j.rse.2011.02.019.

Water balance-based actual evapotranspiration reconstruction from ground and satellite observations over the conterminous united states. Z Wan, K Zhang, X Xue, Z Hong, Y Hong, J Gourley, 10.1002/2015WR017311Water Resources Research. 51cited By 56Z. Wan, K. Zhang, X. Xue, Z. Hong, Y. Hong, J. Gourley, Water balance-based actual evapotranspiration reconstruction from ground and satellite observations over the conterminous united states, Water Resources Research 51 (2015) 6485-6499. URL: https://www.scopus.com/inward/record.uri?eid=2-s2.0- 84941996457&doi=10.1002%2f2015WR017311&partnerID=40&md5= 2cb0bc3c5ec45c7de97fb17f45b09ce8. doi:10.1002/2015WR017311, cited By 56.

Geoquery: Integrating hpc systems and public web-based geospatial data tools. S Goodman, A Benyishay, Z Lv, D Runfola, 10.1016/j.cageo.2018.10.009Computers & Geosciences. 122S. Goodman, A. BenYishay, Z. Lv, D. Runfola, Geoquery: Inte- grating hpc systems and public web-based geospatial data tools, Computers & Geosciences 122 (2019) 103-112. URL: https:// www.sciencedirect.com/science/article/pii/S0098300418305326. doi:https://doi.org/10.1016/j.cageo.2018.10.009.

A high performance gpu implementation of surface energy balance system (sebs) based on cuda-c. M Abouali, J Timmermans, J E Castillo, B Z Su, 10.1016/j.envsoft.2012.12.005Environmental Modelling & Software. 41M. Abouali, J. Timmermans, J. E. Castillo, B. Z. Su, A high performance gpu implementation of surface energy balance system (sebs) based on cuda-c, Environmental Modelling & Software 41 (2013) 134-138. URL: https://www.sciencedirect.com/science/ article/pii/S1364815212003106. doi:https://doi.org/10.1016/ j.envsoft.2012.12.005.

Water: actual evapotranspiration with energy balance models, R Package Version 0. G Olmedo, S Ortega-Farias, D Fonseca-Luengo, D De La Fuente-Saiz, F Peñailillo, G. Olmedo, S. Ortega-Farias, D. Fonseca-Luengo, D. de la Fuente-Saiz, F. Peñailillo, Water: actual evapotranspiration with energy balance models, R Package Version 0.6 (2017).

Google earth engine: A planetary-scale geo-spatial analysis platform. G E E Team, 2022G. E. E. Team, Google earth engine: A planetary-scale geo-spatial anal- ysis platform., https://earthengine.google.com/, 2022.

Using google's cloud-based platform for digital soil mapping. J Padarian, B Minasny, A Mcbratney, 10.1016/j.cageo.2015.06.023Computers & Geosciences. 83J. Padarian, B. Minasny, A. McBratney, Using google's cloud-based platform for digital soil mapping, Computers & Geosciences 83 (2015) 80-88. URL: https://www.sciencedirect.com/science/ article/pii/S009830041530008X. doi:https://doi.org/10.1016/ j.cageo.2015.06.023.

Stion -a software as a service for cryo-em data processing on aws. S Bhatkar, S. Bhatkar, Stion -a software as a service for cryo-em data process- ing on aws, https://aws.amazon.com/blogs/hpc/stion-a-saas-for- cryo-em-data-processing-on-aws, 2021.

cryoem-cloudtools: A software platform to deploy and manage cryo-em jobs in the cloud. M A Cianfrocco, I Lahiri, F Dimaio, A Leschziner, J. Struct. Biol. 203M. A. Cianfrocco, I. Lahiri, F. DiMaio, A. Leschziner, cryoem-cloud- tools: A software platform to deploy and manage cryo-em jobs in the cloud, J. Struct. Biol. 203 (2018) 230-235.

Cosmic2: A science gateway for cryo-electron microscopy structure determination. M A Cianfrocco, M Wong-Barnum, C Youn, R Wagner, A Leschziner, Practice and Experience in Advanced Research Computing. M. A. Cianfrocco, M. Wong-Barnum, C. Youn, R. Wagner, A. Leschziner, Cosmic2: A science gateway for cryo-electron microscopy structure determination, in: Practice and Experience in Advanced Re- search Computing 2017., 2017, pp. 1-5.

Workflowhub: Community framework for enabling scientific workflow research and development. R Ferreira, 10.1109/WORKS51914.2020.00012doi:10.1109/WORKS51914.2020.00012IEEE. R. Ferreira, et al., Workflowhub: Community framework for enabling scientific workflow research and development, IEEE (2020). URL: http://dx.doi.org/10.1109/WORKS51914.2020.00012. doi:10.1109/WORKS51914.2020.00012.

Locate, identify, and cite research data with the leading global provider of dois for research data. Datacite, DataCite, Locate, identify, and cite research data with the leading global provider of dois for research data., https://datacite.org/, ????

ROHub -A Digital Library of Research Objects Supporting Scientists Towards Reproducible Science. R Palmaemail, 10.1007/978-3-319-12024-9_9doi:10.1007/978-3-319-12024-9 9R. PalmaEmail, et al., ROHub -A Digital Library of Re- search Objects Supporting Scientists Towards Reproducible Sci- ence, 2014. URL: https://link.springer.com/chapter/10.1007/978- 3-319-12024-9 9. doi:10.1007/978-3-319-12024-9 9.

B2drop Eudat, Sync and share research data. 2021EUDAT, B2drop: Sync and share research data, https://eudat.eu/ services/userdoc/b2drop, 2021.

The Latin American Giant Observatory: A Successful Collaboration in Latin America Based on Cosmic Rays and Computer Science Domains. H Asorey, L A Núñez, M Suárez-Durán, L A Torres-Niño, M Rodríguez-Pascual, A J Rubio-Montero, R Mayo-García, 10.1109/CCGrid.2016.11016th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid). Cartagena, ColombiaIEEEH. Asorey, L. A. Núñez, M. Suárez-Durán, L. A. Torres-Niño, M. Rodríguez-Pascual, A. J. Rubio-Montero, R. Mayo-García, The Latin American Giant Observatory: A Successful Collaboration in Latin America Based on Cosmic Rays and Computer Science Domains, in: 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid), IEEE, Cartagena, Colombia, 16-19 May 2016, pp. 707-711. doi:10.1109/CCGrid.2016.110.

M Rodríguez-Pascual, G Larocca, C Kanellopoulo, C Carrubba, G Inserra, R Ricceri, H Asorey, A J Rubio-Montero, E Núñez-González, L A Núñez, O Prnjat, R Barbera, R Mayo-García, 10.1109/CSE.2015.27A Resilient Methodology for Accessing and Exploiting Data and Scientific Codes on Distributed Environments. Porto, PortugalIEEE18th IEEE International Conference on Computational Science and Engineering (CSE)M. Rodríguez-Pascual, G. LaRocca, C. Kanellopoulo, C. Carrubba, G. Inserra, R. Ricceri, H. Asorey, A. J. Rubio-Montero, E. Núñez- González, L. A. Núñez, O. Prnjat, R. Barbera, R. Mayo-García, A Resilient Methodology for Accessing and Exploiting Data and Scientific Codes on Distributed Environments, in: 18th IEEE International Con- ference on Computational Science and Engineering (CSE), IEEE, Porto, Portugal., 21-23 Oct. 2015, pp. 319-323. doi:10.1109/CSE.2015.27.

Bioinformatics methods for mass spectrometry-based proteomics data analysis. C Cand, H Jand, J Tanner, J Cheng, 10.3390/ijms21082873Int J Mol Sci. 212873C. Cand, H. Jand, J. Tanner, J. Cheng, Bioinformatics methods for mass spectrometry-based proteomics data analysis, Int J Mol Sci 21 (2020) 2873. doi:10.3390/ijms21082873.

Chemometric methods in data processing of mass spectrometry-based metabolomics: A review. L Yi, N Dong, Y Yun, B Deng, D Ren, S Liu, Y Liang, 10.1016/j.aca.2016.02.001Analytica Chimica Acta. 914L. Yi, N. Dong, Y. Yun, B. Deng, D. Ren, S. Liu, Y. Liang, Chemometric methods in data processing of mass spectrometry-based metabolomics: A review, Analytica Chimica Acta 914 (2016) 17-34. doi:https: //doi.org/10.1016/j.aca.2016.02.001.

Create, run, share, publish, and reference your LC-MS, FIA-MS, GC-MS, and NMR data analysis workflows with the Work-flow4Metabolomics 3.0 Galaxy online infrastructure for metabolomics. Y Guitton, M Tremblay-Franco, G L Corguillé, J.-F Martin, M Pétéra, 10.1016/j.biocel.2017.07.002The International Journal of Biochemistry & Cell Biology. 93Y. Guitton, M. Tremblay-Franco, G. L. Corguillé, J.-F. Martin, M. Pétéra, et al., Create, run, share, publish, and reference your LC-MS, FIA-MS, GC-MS, and NMR data analysis workflows with the Work- flow4Metabolomics 3.0 Galaxy online infrastructure for metabolomics, The International Journal of Biochemistry & Cell Biology 93 (2017) 89--101. doi:10.1016/j.biocel.2017.07.002.

Sharing and community curation of mass spectrometry data with global natural products social molecular networking. M Wang, J J Carver, V V Phelan, L M Sanchez, N Garg, Y Peng, D D Nguyen, 27504778Nature biotechnology. 34M. Wang, J. J. Carver, V. V. Phelan, L. M. Sanchez, N. Garg, Y. Peng, D. D. Nguyen, et al., Sharing and community curation of mass spec- trometry data with global natural products social molecular networking, Nature biotechnology 34 (2016). PMID: 27504778.

MassBank: a public repository for sharing mass spectral data for life sciences. H Horai, 10.1002/jms.1777doi:10.1002/ jms.1777J Mass Spectrom. 45H. Horai, et al., MassBank: a public repository for sharing mass spectral data for life sciences, J Mass Spectrom 45 (2010) 703-14. doi:10.1002/ jms.1777.

Opensource application for water supply system management: Implementation in a water transmission system in southern spain. J Pérez-Padillo, J G Morillo, E C Poyato, P Montesinos, 10.3390/w13243652Water. 133652J. Pérez-Padillo, J. G. Morillo, E. C. Poyato, P. Montesinos, Open- source application for water supply system management: Implementa- tion in a water transmission system in southern spain, Water 13 (2021) 3652. doi:10.3390/w13243652.

Design and development of a web-based epanet model catalogue and execution environment. T Bayer, D P Ames, T G Cleveland, 10.1080/19475683.2021.1936171Annals of GIS. 27T. Bayer, D. P. Ames, T. G. Cleveland, Design and development of a web-based epanet model catalogue and execution environment, Annals of GIS 27 (2021) 247-260. doi:10.1080/19475683.2021.1936171.

Computer modeling of water supply and sewerage networks as a tool in an integrated water and wastewater management system in municipal enterprises. W Kruszyński, J Dawidowicz, 10.12911/22998993/117533Journal of Ecological Engineering. 21W. Kruszyński, J. Dawidowicz, Computer modeling of water supply and sewerage networks as a tool in an integrated water and wastewa- ter management system in municipal enterprises, Journal of Ecological Engineering 21 (2020) 261-266. doi:10.12911/22998993/117533.

Scientific assessment of ozone depletion. W W , W. W. M. Organization), Scientific assessment of ozone depletion: 2018, 2018.

Estimates of ozone return dates from chemistry-climate model initiative simulations. S S Dhomse, D Kinnison, M P Chipperfield, R J Salawitch, I Cionni, M I Hegglin, N L Abraham, H Akiyoshi, A T Archibald, E M Bednarz, S Bekki, P Braesicke, N Butchart, M Dameris, M Deushi, S Frith, S C Hardiman, B Hassler, L W Horowitz, R.-M Hu, P Jöckel, B Josse, O Kirner, S Kremser, U Langematz, J Lewis, M Marchand, M Lin, E Mancini, V Marécal, M Michou, O Morgenstern, F M O&apos;connor, L Oman, G Pitari, D A Plummer, J A Pyle, L E Revell, E Rozanov, R Schofield, A Stenke, K Stone, K Sudo, S Tilmes, D Visioni, Y Yamashita, G Zeng, 10.5194/acp-18-8409-2018Atmospheric Chemistry and Physics. 18S. S. Dhomse, D. Kinnison, M. P. Chipperfield, R. J. Salawitch, I. Cionni, M. I. Hegglin, N. L. Abraham, H. Akiyoshi, A. T. Archibald, E. M. Bednarz, S. Bekki, P. Braesicke, N. Butchart, M. Dameris, M. Deushi, S. Frith, S. C. Hardiman, B. Hassler, L. W. Horowitz, R.-M. Hu, P. Jöckel, B. Josse, O. Kirner, S. Kremser, U. Langematz, J. Lewis, M. Marchand, M. Lin, E. Mancini, V. Marécal, M. Michou, O. Morgenstern, F. M. O'Connor, L. Oman, G. Pitari, D. A. Plum- mer, J. A. Pyle, L. E. Revell, E. Rozanov, R. Schofield, A. Stenke, K. Stone, K. Sudo, S. Tilmes, D. Visioni, Y. Yamashita, G. Zeng, Estimates of ozone return dates from chemistry-climate model initia- tive simulations, Atmospheric Chemistry and Physics 18 (2018) 8409- 8438. URL: https://acp.copernicus.org/articles/18/8409/2018/. doi:10.5194/acp-18-8409-2018.

Evaluating stratospheric ozone and water vapour changes in cmip6 models from 1850 to 2100. J Keeble, B Hassler, A Banerjee, R Checa-Garcia, G Chiodo, S Davis, V Eyring, P T Griffiths, O Morgenstern, P Nowack, G Zeng, J Zhang, G Bodeker, S Burrows, P Cameron-Smith, D Cugnet, C Danek, M Deushi, L W Horowitz, A Kubin, L Li, G Lohmann, M Michou, M J Mills, P Nabat, D Olivié, S Park, Ø Seland, J Stoll, K.-H Wieners, T Wu, 10.5194/acp-21-5015-2021Atmospheric Chemistry and Physics. 21J. Keeble, B. Hassler, A. Banerjee, R. Checa-Garcia, G. Chiodo, S. Davis, V. Eyring, P. T. Griffiths, O. Morgenstern, P. Nowack, G. Zeng, J. Zhang, G. Bodeker, S. Burrows, P. Cameron-Smith, D. Cugnet, C. Danek, M. Deushi, L. W. Horowitz, A. Kubin, L. Li, G. Lohmann, M. Michou, M. J. Mills, P. Nabat, D. Olivié, S. Park, Ø. Seland, J. Stoll, K.-H. Wieners, T. Wu, Evaluating stratospheric ozone and water vapour changes in cmip6 models from 1850 to 2100, Atmospheric Chemistry and Physics 21 (2021) 5015-5061. URL: https://acp.copernicus.org/ articles/21/5015/2021/. doi:10.5194/acp-21-5015-2021.

European Open Science Cloud Partnership, Draft proposal for the European Open Science Cloud (EOSC) Partnership, shorturl.at/tEIMS, Accessed. European Open Science Cloud Partnership, Draft proposal for the Eu- ropean Open Science Cloud (EOSC) Partnership, shorturl.at/tEIMS, Accessed, December 2021.

Solutions for a sustainable EOSC : a FAIR Lady (olim Iron Lady) report from the EOSC Sustainability Working Group, Publications Office. E Commission, D.-G Research, Innovation, 10.2777/870770doi:doi/10.2777/ 870770E. Commission, D.-G. for Research, Innovation, Solutions for a sustain- able EOSC : a FAIR Lady (olim Iron Lady) report from the EOSC Sus- tainability Working Group, Publications Office, 2020. doi:doi/10.2777/ 870770.