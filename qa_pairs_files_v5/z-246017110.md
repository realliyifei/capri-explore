# Reinforcement Learning for Ridesharing: An Extended Survey *

CorpusID: 246017110 - [https://www.semanticscholar.org/paper/6099eeb3c4d8f8bae466e88075f83c5ee1d9c444](https://www.semanticscholar.org/paper/6099eeb3c4d8f8bae466e88075f83c5ee1d9c444)

Fields: Computer Science

## (s12) Route Guidance (Navigation)
Number of References: 3

(p12.0) Routing in this paper refers to low-level navigation decisions on a road network, typically with output of matching and repositioning algorithms as input. The road network, combined with traffic conditions on the links (exhibited as link costs), forms the traffic network which is a non-stationary stochastic network (Mao & Shen 2018). It is known that standard static shortest-path algorithms do not find the path with minimum expected cost in this case, and the optimal route is not a simple route but a policy (Hall 1986, Kim et al. 2005). There are two types of set-up for the routing problem, depending on the decision review time. In the first type of set-up, each vehicle on the road network selects a route for a given OD pair from a set of feasible routes. The decision is only reviewed and revised after a trip is completed. Hence, it is called route planning or route choice. When the routes for all the vehicles are planned together, it is equivalent to assigning the vehicles to each link in the network, and hence, the problem is called traffic assignment problem (TAP), which is typically for transportation planning purposes. In the second type of set-up, the routing decision is made at each intersection to select the next outbound road (link) to enter. These are real-time    adaptive navigation decisions for vehicles to react to the changing traffic state of the road network.
## (s18) Joint Optimization
Number of References: 4

(p18.0) The rideshare platform is an integrated system, so joint optimization of multiple decision modules leads to better solutions that otherwise unable to realize under separate optimizations, ensuring that different decisions work towards the same goal. RL for joint optimization across multiple modules calls for research on reward function design, state-action representation that facilitates intermodule communication, and the training algorithms. Models and algorithms that allow decentralized execution by the different modules are highly preferred in practice. We have already seen development on RL for joint matching-reposition (Holler et al. 2019) and with ride-pooling (Gu√©riau & Dusparic 2018), pricing-matching (Chen, Jiao, Qin, Tang, Li, An, Zhu & Ye 2019), and pricing-reposition (Turan et al. 2020). An RL-based method for fully joint optimization of all major modules is highly expected. Meanwhile, this also requires readiness from the rideshare platforms in terms of system architecture and organizational structure.
## (s24) General RL
Number of References: 3

(p24.0) RL provides the necessary tools for the methods reviewed in this survey. Hence, the problems of RL for ridesharing tie closely to the development in RL in general. In the context of ridesharing, we have seen from the literature review above that it is difficult for RL to learn combinatorial actions, e.g., the system matching actions. In the era of deep RL, model interpretability is a long-standing challenge, which hampers investigation of customer experience corner cases. For experience-critical service like ridesharing, policy exploration adds further complication, especially for real-world deployment. In view of these challenges, the future is probably that RL-based and traditional optimization approaches will be complementing each other for a long time. We have seen such combinations in the current literature as  for matching, (Chaudhari et al. 2020a, Jiao et al. 2021 for repositioning, and (Delarue et al. 2020) for VRP, that combine RL with combinatorial optimization, mixed-integer programming, and tree search. The breakthroughs of RL that we are seeing in other domains and the continued development of RL methodology for ridesharing certainly make it exciting to anticipate the future landscape.
