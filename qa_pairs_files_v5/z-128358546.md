# A Survey on Practical Applications of Multi-Armed and Contextual Bandits

CorpusID: 128358546 - [https://www.semanticscholar.org/paper/b24e6b0539d6e27d82c60fa7c53a1d0905e41a19](https://www.semanticscholar.org/paper/b24e6b0539d6e27d82c60fa7c53a1d0905e41a19)

Fields: Mathematics, Business, Computer Science

## (s0) Introduction
Number of References: 5

(p0.0) Sequential decision-making problems, where at each point in time an agent must choose the best action out of several alternatives, are frequently encountered in various practical applications, from clinical trials [Durand et al., 2018] to recommender systems [Mary et al., 2015] and anomaly detection [Ding et al., 2019]. Often, there is a side information, or context, associated with each action (e.g., a user's profile), and the feedback, or reward, is limited to the chosen option. For example, in clinical trials [Durand et al., 2018;Bastani and Bayati, 2015] the context is the patient's medical record (e.g. health condition, family history, etc.), the actions correspond to the treatment options being compared, and the reward represents the outcome of the proposed treatment (e.g., success or failure). An important aspect affecting the long-term success in such settings is finding a good trade-off between exploration (e.g., trying a new drug) and exploitation (choosing the known drug).
## (s4) Dynamic Pricing
Number of References: 3

(p4.0) Online retailer companies are often faced with the dynamic pricing problem: the company must decide on real-time prices for each of its multiple products. The company can run price experiments (make frequent price changes) to learn about demand and maximize long-run profits. The authors in [Misra et al., 2018] propose a dynamic price experimentation policy, where the company has only incomplete demand information. For this general setting, authors derive a pricing algorithm that balances earning an immediate profit vs. learning for future profits. The approach combines multi-armed bandit with partial identification of consumer demand from economic theory. Similar to [Misra et al., 2018], authors in [Mueller et al., 2018] consider high-dimensional dynamic multi-product pricing with an evolving lowdimensional linear demand model. They show that the revenue maximization problem reduces to an online bandit convex optimization with side information given by the observed demands. The approach applies a bandit convex optimization algorithm in a projected low-dimensional space spanned by the latent product features, while simultaneously learning this span via online singular value decomposition of a carefully-crafted matrix containing the observed demands.
## (s6) Influence Maximization
Number of References: 2

(p6.0) Autors in [Vaswani et al., 2017] consider influence maximization (IM) in social networks, which is the problem of maximizing the number of users that become aware of a product by selecting a set of seed users to expose the product to. They propose a novel parametrization that not only makes the framework agnostic to the underlying diffusion model, but also statistically efficient to learn from data. They give a corresponding monotone, submodular surrogate function, and show that it is a good approximation to the original IM objective. They also consider the case of a new marketer looking to exploit an existing social network, while simultaneously learning the factors governing information propagation. For this, they develop a LinUCB-based bandit algorithm. Authors in [Wen et al., 2017] also study the online influence maximization problem in social networks but under the independent cascade model. Specifically, they try to learn the set of best seeds or influencers in a social network online while repeatedly interacting with it. They address the challenges of combinatorial action space, since the number of feasible influencer sets grows exponentially with the maximum number of influencers, and limited feedback, since only the influenced portion of the network is observed. They propose and analyze IMLinUCB, a computationally efficient UCB-based algorithm.
## (s7) Information Retrieval
Number of References: 2

(p7.0) Authors in [Losada et al., 2017] argue that Information Retrieval iterative selection process can be naturally modeled as a contextual bandit problem. Casting document judging as a multi-armed bandit problem leads to highly effective adjudication methods. Under this bandit allocation framework, they consider stationary and non-stationary models and propose seven new document adjudication methods (five stationary methods and two non-stationary variants). This comparative study includes existing methods designed for poolingbased evaluation and existing methods designed for metasearch. In mobile information retrieval, authors in [Bouneffouf et al., 2013] introduce an algorithm that tackles this dilemma in Context-Based Information Retrieval (CBIR) area. It is based on dynamic exploration/exploitation and can adaptively balance the two aspects by deciding which users situation is most relevant for exploration or exploitation. Within a deliberately designed online framework they conduct evaluations with mobile users.
## (s9) Anomaly Detection
Number of References: 2

(p9.0) Performing anomaly detection on attributed networks concerns with finding nodes whose behaviors deviate significantly from the majority of nodes. Authors in [Ding et al., 2019] investigate the problem of anomaly detection in an interactive setting by allowing the system to proactively communicate with the human expert in making a limited number of queries about ground truth anomalies. Their objective is to maximize the true anomalies presented to the human expert after a given budget is used up. Along with this line, they formulate the problem through the principled multiarmed bandit framework and develop a novel collaborative contextual bandit algorithm, that explicitly models the nodal attributes and node dependencies seamlessly in a joint framework, and handles the explorationexploitation dilemma when querying anomalies of different types. Credit card transactions predicted to be fraudulent by automated detection systems are typically handed over to human experts for verification. To limit costs, it is standard practice to select only the most suspicious transactions for investigation. Authors in [Soemers et al., 2018] claim that a trade-off between ex-ploration and exploitation is imperative to enable adaptation to changes in behavior. Exploration consists of the selection and investigation of transactions with the purpose of improving predictive models, and exploitation consists of investigating transactions detected to be suspicious. Modeling the detection of fraudulent transactions as rewarding, they use an incremental regression tree learner to create clusters of transactions with similar expected rewards. This enables the use of a contextual multi-armed bandit (CMAB) algorithm to provide the exploration/exploitation trade-off.
## (s10) Telecommunication
Number of References: 2

(p10.0) In [Boldrini et al., 2018], a multi-armed bandit model was used to describe the problem of best wireless network selection by a multi-Radio Access Technology (multi-RAT) device, with the goal of maximizing the quality perceived by the final user. The proposed model extends the classical MAB model in a twofold manner. First, it foresees two different actions: to measure and to use; second, it allows actions to span over multiple time steps. Two new algorithms designed to take advantage of the higher flexibility provided by the muMAB model are also introduced. The first one, referred to as measure-use-UCB1 is derived from the UCB1 algorithm, while the second one, referred to as Measure with Logarithmic Interval, is appositely designed for the new model so to take advantage of the new measure action, while aggressively using the best arm. The authors in [Kerkouche et al., 2018] demonstrate the possibility to optimize the performance of the Long Range Wide Area Network technology. Authors suggest that nodes use multi-armed bandit algorithms, to select the communication parameters (spreading factor and emission power). Evaluations show that such learning methods allow to manage the trade-off between energy consumption and packet loss much better than an Adaptive Data Rate algorithm adapting spreading factors and transmission powers on the basis of Signal to Interference and Noise Ratio values.
## (s16) Bandit for Active Learning
Number of References: 2

(p16.0) Labelling all training examples in supervised classification setting can be costly. Active learning strategies solve this problem by selecting the most useful unlabelled examples to obtain the label for, and to train a predictive model. The choice of examples to label can be seen as a dilemma between the exploration and the exploitation over the input space. In [Bouneffouf et al., 2014], a novel active learning strategy manages this compromise by modelling the active learning problem as a contextual bandit problem. they propose a sequential algorithm named Active Thompson Sampling (ATS), which, in each round, assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for this sample point label. The authors of [Ganti and Gray, 2013] also propose a multi-armed bandit inspired, pool-based active learning algorithm for the problem of binary classification. They utilize ideas such as lower confidence bounds, and self-concordant regularization from the multi-armed bandit literature to design their proposed algorithm. In each round, the proposed algorithm assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for the label of this sampled point.
