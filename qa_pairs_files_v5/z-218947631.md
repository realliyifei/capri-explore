# IEEE INTERNET OF THINGS JOURNAL 1 Sensor-based Continuous Authentication of Smartphones' Users Using Behavioral Biometrics: A Contemporary Survey

CorpusID: 218947631 - [https://www.semanticscholar.org/paper/0891588e9c22af82fa92e0376dc8e4401bff2289](https://www.semanticscholar.org/paper/0891588e9c22af82fa92e0376dc8e4401bff2289)

Fields: Computer Science, Engineering

## (s2) A. Used Biometric Modalities
Number of References: 2

(p2.0) Several modalities are used for biometric-based authentication, including physiological biometrics (e.g., face, fingerprint, iris, etc.) and behavioral biometrics (e.g., keystroke dynamics, touch gestures, voice, motions, etc.). Figure 1 shows a categorization of used modalities for user authentication tasks. Figure  2 shows the modalities and features of several behavioral biometrics that are commonly used for user authentication tasks. All these modalities are made possible by the embedded mobile sensors, e.g., camera, microphone, accelerometers, and gyroscopes, which contribute to the enrolment phase and the verification part of the authentication process. Such sensors provide sufficient information for accurate and secure authentication, and adopting the proper utilization mechanism would play an essential role in delivering efficient and usable user authentication [41]. Using biometrics for authentication, there are enormous studies that demonstrated the benefits and security aspects of using such information to explore "on-themove biometry" [42].
## (s6) III. MOTION-BASED AUTHENTICATION
Number of References: 15

(p6.0) Most of today's mobile devices are equipped with motion sensors such as accelerometers and gyroscopes, which can be a valid source for modeling users' behavior. The accelerometer provides the gravitational acceleration in three spatial dimensions (axes), x, y, and z, measured in meter per second squared, where the axes denote the vertical, and left-to-right dimensions [86]. The gyroscope measures the angular rotation in three dimensions, x, y, and z, in radians per second along the axes [77]. Such sensory data provides a feature space that enables the modeling of users' movement and usage; therefore, a variety of methods revolve around utilizing such data for authentication and security.

(p6.1) Early exploitation of motion sensors includes air-written signatures [44], [78] for which the user holds the device and performs an air-written signature as the application is running and recording the user's motion. Traditionally, signatures are well-known behavioral biometric commonly used for conducting official or commercial transactions [87]- [89]. However, air-written signatures, while providing a valid method for user authentication, they operate as a point-of-entry authentication and fail to offer covert, transparent, or continuous authentication. Laghari et al. [44] showed that a motion-based signature had achieved a 1.46% FAR and 6.87% FRR when tested on a dataset collected from motion sensors of ten participants' smartphones. While such methods are robust against shoulder surfing attacks [90], they 1 require the user input and engagement once authentication is required, 2 fail to offer a continuous transparent authentication, and 3 are secretand knowledge-based since the user must memorize the used signature. Similar implementations include waving gestures [70], free-form gestures [77], and "picking-up" movement (i.e., picking the phone and raising it for answering a call) [71].

(p6.2) Ehatisham et al. [28] proposed a continuous authentication system that identifies mobile users based on their activity patterns using embedded sensors, i.e., accelerometer, gyroscope, and magnetometer. The authors reported an analysis of the system performance when the smartphone is placed at five different locations on the user's body. Amini et al. [67] introduced DeepAuth, an LSTM-based user authentication method, which uses sensory data extracted from the accelerometer and gyroscope to model users' behavioral patterns. The experiments, which were carried out on data collected from 47 users with 10-13 minutes each, have shown an average accuracy of 96.7% for 20 seconds authentication window. Zhu et al. [91] introduced a technique based on users' phone-skating behavior captured by motion sensors. The experiments reported an average EER of 1.2% using data of 20 users. Lee et al. [82] introduced an SVM-based system for user authentication using readings from three motion sensors to achieve an average accuracy of 90% when using data collected from four participants.
## (s7) IV. GAIT-BASED AUTHENTICATION
Number of References: 25

(p7.0) Gait recognition has gained increased interest in recent years, especially with the vast adoption of mobile and wearable sensors. Gait recognition is defined as the process of identifying an individual by the manner of walking using computer vision and/or sensory data collected from environmental and wearable sensors [120]. Computer vision approaches for gait recognition include segmenting the individual's images while walking and capturing the features that enable accurate recognition [92]. While using sensory data, including 1 adopting floor senors where the gait-related features are captured once the person walks on them [93], [94], 2 adopting wearable sensors that aims to collect information that enables gait recognition [93]. For mobile security and authentication, gait recognition is usually done using wearable sensors, especially the reading of the motion sensors (e.g., accelerometer) of the mobile device, to enable continuous transparent authentication.

(p7.1) The general approach to gait recognition includes four steps, 1 data acquisition step in which the device is placed in a certain way that enables the walk activity recording, 2 data preprocessing step for reducing the introduced noise by the data collection method or other environmental factors, 3 walk detection step using either traditional cycle or machine learning techniques, and 4 analysis step [86]. Handling the data acquisition process requires accurate readings from motion sensors as the user places the device in a predefined manner such as carrying the device inside of a pouch [100], in the pants pocket [86], [101], or in hand [102]. Studies conducted for mobile security using gait-based biometrics usually include data collection from a population of size equal to or less than 50 participants [100]- [102], and processed in controlled conditions to minimize the effects of outside factors [103]. Even though some studies have attempted to capture gaitrelated metrics from a real-world collection of sensory data, such as the study by Nickel and Busch [103], generally, the data collection requires an ideal setting at least in one aspect (e.g., walking patterns or floor condition) [3].

(p7.2) The second step after acquiring the data, the preprocessing step takes place to clean, reduce the noise, and normalize the data. The major task in this regard is the noise reduction considering various possible noise sources, such as environmental and gravitational factors, floor conditions, and the users' shoes or other wearable materials. Since the gait-related features rely heavily on readings from motion sensors, such as the accelerometer, which are very sensitive, the adopted method should account for further noise [101]. Such noises can be handled using linear interpolation and filtering techniques, while environmental noise adds much complexity to the walk detection task, which can be minimized using activity recognition to remove any irrelevant data [100]. For the walk detection, cycles (i.e., the time between two paces bounded by maximum and minimum threshold across the three axes) or machine learning techniques are both utilized in the literature. Cycle-based approaches are commonly used since the average cycle length is easily and simply calculated to detect cycles by moving forward or backward in intervals of the average cycle length with some correction measurement. On the other hand, machine learning-based approaches have shown to be accurate for automatic walk detection [103]. Such techniques require readings of the sensory data, preprocessing phase to reduce the noise and normalize the data, and a walk detection model that leverages the lowest and highest values for thresholding and the decision.

(p7.3) The final step of gait recognition is the analysis of the time intervals, frequencies, or both. Using time intervals analysis, some metrics can be extracted and studied, such as cycle statistics, including the minimum, average, maximum acceleration values, and cycle lengths and frequencies. Moreover, cycle variance and stability are measured by acceleration moments [86], [120]. Using frequency analysis, usually conducted using Discrete or Fast Fourier Transforms, it has been shown that the first few coefficients resulting from each conversion are highly relevant for detecting distinctive gait patterns [86].

(p7.4) Wang et al. [92] and Gafurov et al. [93] used a k-NN model to classify legitimate users using gait-based features, where Wang et al. uses the camera to capture the user movement, and Gafurov et al. captures the user movement using cyclic rotation metric device attached to different places of the body (ankle, hip, pocket, arm). Both studies achieved an accuracy of above 85%, with EER of 3.54% and 5%, respectively. Multiple studies used accelerometer as a standalone sensor to capture user movement for user authentication task [86], [95]- [103].
## (s8) V. KEYSTROKE-BASED AUTHENTICATION
Number of References: 17

(p8.0) One of the earliest behavioral authentication methods is based on studying the keystroke dynamics. Most keystroke dynamics-based methods are cost-effective and do not require additional modules to operate [24]. During the usage of the device, when a key input is required (e.g., texting), the keystroke dynamics-based authentication method continuously validates the user since behavioral dynamics can be distinctive across users. Conducting authentication via keystroke dynamics requires analyzing and capturing the distinctive features and patterns of users' keystrokes when using the device [22], [23]. Common features include: 1 Keypress frequency, which calculates the frequency of keypress events. 2 Key release frequency, which calculates the frequency of key release events. 3 Latency and hold time, which calculates the rates of press-to-press, press-to-release (which is also known as the hold time), release-to-release, and release-to-press events. 4 Finger's pressure while touching the screen. 5 Pressed area size by the user's fingers. 6 Error rate, which is the frequency of using backspaces or deletion option.

(p8.1) Using keystroke dynamics for authentication or user validation has been adopted on traditional computers before their application to smartphones [109]. Even though it seems to be an easier task to implement a keystroke dynamics-based authentication on computers due to the less complex feature space, Joyce and Gupta [110] showed the uniqueness of both written signatures and typing behavior are originated from the physiology of the neurological system.

(p8.2) Recent application of keystroke dynamics takes advantage of embedded sensors (e.g., motion sensor on smartphones), to improve the authentication accuracy, especially when there the key-based input is unavailable [121], [122]. Another distinction between applying keystroke dynamics-based methods on smartphones and computers is the large space of keybased input in the smartphone since it includes touches and swipes that meant for interacting with the applications without typing textual content [111]. Several studies have addressed the generalization of these methods to different types of input. For instance, McLoughlin et al. [111] showed that using key press and release frequencies and the latency between two presses contribute greatly to establishing distinctive keystroke behavior for users. The authors showed that the application should account for the inconsistencies in recorded data by introducing weights based on the variance of data (i.e., lower variance gets higher weights). Their results show an accuracy of more than 90%, establishing the validity of using keystroke dynamics as a biometric for authentication with minimal computational overhead and increased usability.

(p8.3) Buriro et al. [123] designed an authentication scheme based on the user's hand movements and timing features as they enter ten keystrokes. The authors conducted experiments using data collected from 97 participants and reported an authentication accuracy of 85.77% and FAR of 7.32%. Similarly, Zahid et al. [112] studied keystroke behavior of 25 users, including features such as the hold time, error rate, and latency. The authors suggested a fuzzy classifier to account for the diffused features space and argued that presenting the classification task of keystroke behavior as an optimization problem benefits the robustness of the model when compared to similarity-based methods [113]. Using a fuzzy classifier with Particle Swarm Optimization and Genetic Algorithms, their proposed method showed 0% FRR and 2% FAR, suggesting high security and usability potential. However, keystroke dynamics are often incorporated with other modalities for improving performance and accuracy. For instance, Hwang et al. [114] suggested including rhythm and tempo as components for studying keystroke dynamics, i.e., a user is required to follow a distinct and consistent timing pattern for accurate keystroke-based authentication. For example, a given term can be entered digit by digit separated with subsequent short and long pauses that are controlled by tempo cues, e.g., a metronome for counting pause intervals. In their study, the authors showed an average improvement of about 4% in the EER evaluation metric when using artificial rhythmic input with tempo cues in comparison to natural rhythms. However, adopting such methods adds complexity to the usability aspect.

(p8.4) Using smartphone embedded sensors to support keystroke dynamics-based authentication has been repeatedly suggested to improve the performance and to provide transparent authentication. [115] proposed incorporating velocity-related metrics to reach an accuracy of 98.6% for classifying data from ten users using an SVM classifier. Similarly, Giuffrida et al. [116] proposed incorporating keystroke data with motion sensors data, namely, accelerometer and gyroscope, to conclude that metrics obtained from the accelerometer data are more useful than those obtained from the gyroscope. The authors showed that combining features from motion sensors with keystroke metrics provides similar results as adopting only the motion sensors-related features alone, i.e., the study shows that sensorrelated features can be more useful than keystroke dynamics in terms of authentication. However, obtaining and analyzing high-frequency sensory data can be power consuming. Table IV shows a list of authentication methods based on keystroke dynamics. The proposed approaches show a promising direction for using this modality for user authentication, achieving an accuracy of up to 99% by Cilia et al. [19]. Insights and Challenges. Keystroke dynamics-based methods have several advantages, such as (a) their high authentica-tion accuracy that can reach up to 99%, (b) high powerefficiency in comparison with other methods, and (c) hardware independence, since these methods can operate with either physical or on-screen keyboards. However, implementing a keystroke dynamics-based approach can be challenging for several reasons. 1 User Behavioral Changes: Capturing keystroke dynamics as a behavioral modality under uncontrolled conditions, e.g., user's activity (standing, walking, etc.), user's emotional or physical state change, and the in-use application, is challenging and requires testing under these non-trivial scenarios. 2 Feature Extraction and Selection:
## (s9) VI. TOUCH GESTURE-BASED AUTHENTICATION
Number of References: 20

(p9.0) Using touch gestures as a biometric modality extend landscape of transparent authentication applications to include a variety of devices with touchscreen unit (e.g., smartwatches, digital cameras, navigation systems, and monitors) [3]. Several studies have investigated the touch gestures as a behavioral biometrics for continuous authentication since it can be convenient and cost-effective. Touch gestures include swipes [124], [141], flicks [125], [126], [129], slides [127]), and handwriting [142]. The distinction between keystroke dynamics and touch gestures can be summarized in the input form for users and the method of input. The commonalities between the two modalities are the space of improvement when accounting for motion sensors [125], [128]. Therefore, many studies have incorporated motion-based features to gesture-based methods [129]. Considering features from touch gestures enables accurate authentication with an accuracy reaching to 99% and minimal EER such as 0.03% when applying k-Nearest Neighbors classifier or other distance-based classifiers [128].

(p9.1) Leveraging the abundance of information generated by the operating system of smartphones, a large number of features can be extracted from touch gestures such as the reading from the accelerometer, pressure, gravity, velocity, touch area, and time-related measurements. Such features allow for accurate calculation of the gesture statistics and developing patterns for user authentication [130]- [133], [143]. Antal et al. [134] extended the feature space of swipe gestures to include touch duration, trajectory length, acceleration, average speed, touch pressure, touch area, and gravity readings. Using data from 40 users, including 58 samples, the authors performed one and two-class classification using multiple classifiers such as Bayes Net, k-Nearest Neighbor, and Random Forests. The authors reported that Random Forests showed an EER of 0.004%. Their results showed that the device motion and positioning are important factors in distinguishing users.

(p9.2) Since touch gestures are commonly known as soft biometrics that could enable the recognition of gender and proportional measures such as physical attributes including hand size, forearm length, and height, they are beneficial in criminal investigations. Miguel et al. [144] proposed studying the swipe gesture for gender prediction using a variety of features including the swipe's length, width, touch area, pressure, velocity, acceleration, start-to-end angle, and others. The authors showed that applying a multi-linear logistic regression classifier for gender prediction achieves an accuracy of 71% when the direction of the swipe is down-to-up. Using a fusion of swipe direction-based decision, the accuracy reaches 78%. Similarly, Bevan and Fraser [145] investigated the relationship between swipe gestures, thumb length, and gender. Using data from 178 users performing one-hand gestures using the thumb, the authors collected 21,360 samples of swipes in various directions. Among the calculated features, the results showed a strong correlation between thumb length and gestures, and they reflected in the velocity, acceleration, and completion time. Moreover, the study also showed that male users completed the gestures at a higher speed than female users.

(p9.3) The landscape of using touch gestures as behavioral biometrics for user authentication includes devices designed for users with disabilities. For example, Azenkot et al. [146] proposed PassChords, which designed for authenticating users with vision impairments using a predefined sequence of screen taps. Another application is proposed by [53] for users with finger injuries, which uses the finger's trajectory and posture before touching the screen using its positioning and proximity. For this application, the direct touch gesture (i.e., the contact with the screen) is not fully required, and only the proximityrelated measurement is possibly feasible to authenticate users.
