# Ear Biometrics Using Deep Learning: A Survey

CorpusID: 251654604 - [https://www.semanticscholar.org/paper/efb14c753c5500b76fe3914894403eed351d9176](https://www.semanticscholar.org/paper/efb14c753c5500b76fe3914894403eed351d9176)

Fields: Computer Science

## (s0) Introduction
Number of References: 4

(p0.0) e ear begins to develop on a fetus amid the fth and seventh weeks of pregnancy [1]. At this stage of the pregnancy, the face acquires a more distinguishable shape as the mouth, nostrils, and ears begin to form. ere is still no exact timeline at which the outer ear is created during pregnancy, but it is accepted that a cluster of embryonic cells connect to establish the ear. ese are called auricular hillocks, which begin to grow in the lower portion of the neck. e auricular hillocks broaden and intertwine within the seventh week to deliver the ear's shape. Within the ninth week, the hillocks move to the ear canal and are more noticeable as the ear [1]. e external anatomy of the ear can be seen in Figure 1. e growth of the ear in the rst four months after birth is linear. e ear is then stretched in development between the ages of four months and eight years. After this, the ear size and shape are constant until the age of seventy, when they increase in size again.
## (s4) Building Block for Convolutional Neural Networks
Number of References: 2

(p4.0) is layer is a set of learnable filters or kernels used to slide over the entire input volume, performing a dot product between entries of the filter and the input layer [5]. e convolutional operation first extracts patches from its information in a sliding window fashion and then applies the same linear transformation to all the areas. e output of the convolutional operation is referred to as a feature map. e network will learn filters and then recognise the visual patterns that are in the input data. is is often shown asx l ij
## (s7) Fully Connected Layer.
Number of References: 4

(p7.0) e fully connected layer is used as a feature extractor. e features produced are then passed to the fully connected layers for classification. Each unit in the fully connected layer is connected to all the units in the previous layers. e last layer is usually a classifier that produces a probability map over the different classes. All the features are converted into one-dimensional feature vectors before passing into the fully connected layer. e reason that this is carried out is that spatial information in the image data is lost, has a high computational cost, and can only work with images that are of the same size [6]. is is often shown as

(p7.1) e fully connected layer is used as a feature extractor. e features produced are then passed to the fully connected layers for classification. Each unit in the fully connected layer is connected to all the units in the previous layers. e last layer is usually a classifier that produces a probability map over the different classes. All the features are converted into one-dimensional feature vectors before passing into the fully connected layer. e reason that this is carried out is that spatial information in the image data is lost, has a high computational cost, and can only work with images that are of the same size [6]. is is often shown as
## (s14) Cascaded Architecture.
Number of References: 2

(p14.0) In the cascaded architecture, the output of the CNN is concatenated with another [9]. ere are many variations with this architecture within the literature, but the input cascade is prominent. In this architecture, the output of the CNN becomes a direct input of another CNN. e input cascade is employed to concatenate the contextual information to the second CNN as additional image channels. Cascaded architecture is an improvement to the only pathway that performs multiscale label prediction separately.
## (s15) UNET.
Number of References: 4

(p15.0) UNET improves a convolutional network that resembles an encoder and decoder network designed to do biomedical image segmentation [10]. e network consists of a contracting path and an expansive path, which provides it with the u-shaped architecture. e contracting path consists of the repeated application of two convolutional layers, followed by a rectified linear measure and a top pooling layer that goes along the trail to scale back the spatial information while feature information is increased. e expansive path consists of upsampling operations combined with high-resolution features from the contraction path through skip connections.

(p15.1) UNET improves a convolutional network that resembles an encoder and decoder network designed to do biomedical image segmentation [10]. e network consists of a contracting path and an expansive path, which provides it with the u-shaped architecture. e contracting path consists of the repeated application of two convolutional layers, followed by a rectified linear measure and a top pooling layer that goes along the trail to scale back the spatial information while feature information is increased. e expansive path consists of upsampling operations combined with high-resolution features from the contraction path through skip connections.
## (s17) Visual Geometry Group
Number of References: 4

(p17.0) Architecture. Visual geometry group architecture is a network created by Visual Graphics Group researchers at Oxford University [12]. It is characterised by a pyramidal shape because it comprises a group of convolutional layers followed by pooling layers; these pooling layers make the layers narrower in shape. e benefits include keeping a good architecture used for benchmarking for any task. e pretrained networks of the VGG are also primarily used for different applications but require numerous computational resources and are slow to coach, above all when training the dataset from scratch.

(p17.1) Architecture. Visual geometry group architecture is a network created by Visual Graphics Group researchers at Oxford University [12]. It is characterised by a pyramidal shape because it comprises a group of convolutional layers followed by pooling layers; these pooling layers make the layers narrower in shape. e benefits include keeping a good architecture used for benchmarking for any task. e pretrained networks of the VGG are also primarily used for different applications but require numerous computational resources and are slow to coach, above all when training the dataset from scratch.
## (s18) GoogLeNet Architecture.
Number of References: 4

(p18.0) e GoogLeNet architecture is referred to as the inception network and was created by Google researchers [13]. It is made from twenty-two layers with two options that these layers can either convolute or pool the input. e architecture contains many beginning modules stacked over each other, allowing joint and parallel training, which helps with faster convergence. e benefits are that there is speedier training, which reduces the size. It , however, possesses an Xception network, which could increase the point for the divergence of the beginning module.

(p18.1) e GoogLeNet architecture is referred to as the inception network and was created by Google researchers [13]. It is made from twenty-two layers with two options that these layers can either convolute or pool the input. e architecture contains many beginning modules stacked over each other, allowing joint and parallel training, which helps with faster convergence. e benefits are that there is speedier training, which reduces the size. It , however, possesses an Xception network, which could increase the point for the divergence of the beginning module.
## (s20) ResNeXt Architecture.
Number of References: 4

(p20.0) ResNeXt architecture is the present state-of-the-art technique for visual perception, which is a hybridisation between inception and ResNeXt architectures [15]. ResNeXt is referred to as the aggregated residual transform network, but it is an improvement over the inception network. It splits the concept and transforms and merges in a commanding but easy way by bringing in cardinality. It uses residual learning, which will enhance the joining of the deep and wide networks. ResNeXt uses many transformations within a split, transform, and merge blocks; and the transformations in cardinality define these. ResNeXt used a mixture of VGG topology and GoogLeNet architecture to correct the spatial resolution using 3 × 3 filters within the split, transform, and merge blocks. e increase in cardinality improves the performance and produces a different and improved architecture.

(p20.1) ResNeXt architecture is the present state-of-the-art technique for visual perception, which is a hybridisation between inception and ResNeXt architectures [15]. ResNeXt is referred to as the aggregated residual transform network, but it is an improvement over the inception network. It splits the concept and transforms and merges in a commanding but easy way by bringing in cardinality. It uses residual learning, which will enhance the joining of the deep and wide networks. ResNeXt uses many transformations within a split, transform, and merge blocks; and the transformations in cardinality define these. ResNeXt used a mixture of VGG topology and GoogLeNet architecture to correct the spatial resolution using 3 × 3 filters within the split, transform, and merge blocks. e increase in cardinality improves the performance and produces a different and improved architecture.
## (s21) Advance Inception Network.
Number of References: 8

(p21.0) e advance inception network includes Inception-V3, Inception-V4, and Inception-ResNet. is is often an improved version of Inception-V1, Inception-V2, and GoogLeNet [16]. Inception-V3 reduces the computational cost of deep networks but does not affect generalisation. Szegedy et al. [17] replaced large-sized filters (5 × 5 and 7 × 7) with small and unequal filters (1 × 7 and 1 × 5) and used 1 × 1 convolution as a blockage before the vast filters. Inception-ResNet combines the strength of the residual learning and starting block.

(p21.1) e advance inception network includes Inception-V3, Inception-V4, and Inception-ResNet. is is often an improved version of Inception-V1, Inception-V2, and GoogLeNet [16]. Inception-V3 reduces the computational cost of deep networks but does not affect generalisation. Szegedy et al. [17] replaced large-sized filters (5 × 5 and 7 × 7) with small and unequal filters (1 × 7 and 1 × 5) and used 1 × 1 convolution as a blockage before the vast filters. Inception-ResNet combines the strength of the residual learning and starting block.
## (s23) Xception Architecture.
Number of References: 4

(p23.0) Xception architecture is referred to as risky inception architecture that overdoes depthwise separable convolution [19]. e first inception block is modified by making it more complete and substituting different spatial dimensions (1 × 1, 5 × 5, and 3 × 3) with one dimension (3 × 3) followed by a 1 × 1 convolution to achieve computational complexity. It makes the network computationally efficient by uncoupling spatial and feature map channels.

(p23.1) Xception architecture is referred to as risky inception architecture that overdoes depthwise separable convolution [19]. e first inception block is modified by making it more complete and substituting different spatial dimensions (1 × 1, 5 × 5, and 3 × 3) with one dimension (3 × 3) followed by a 1 × 1 convolution to achieve computational complexity. It makes the network computationally efficient by uncoupling spatial and feature map channels.
## (s25) Fully Convolutional Network.
Number of References: 12

(p25.0) A fully convolutional network [21] is a set of convolutional and pooling layers. Bi et al. [22] developed a multistage fully convolutional network with the parallel integration method for segmentation. [23] may be a particular sort of artificial neural network that builds on a pyramidal structure by utilising skip connections that skip some convolutional layers. It is composed mainly of multiple convolutional layers.

(p25.1) A fully convolutional network [21] is a set of convolutional and pooling layers. Bi et al. [22] developed a multistage fully convolutional network with the parallel integration method for segmentation. [23] may be a particular sort of artificial neural network that builds on a pyramidal structure by utilising skip connections that skip some convolutional layers. It is composed mainly of multiple convolutional layers.
## (s27) Convolutional and Deconvolutional Neural Networks.
Number of References: 4

(p27.0) is architecture is formed from two significant parts: convolutional and deconvolutional networks [24]. Deconvolutional networks are CNNs that operate during a reversed process, and networks extract discriminated features. e deconvolutional layers are applied for smothering the segmentation maps to get the ultimate high-resolution output.

(p27.1) is architecture is formed from two significant parts: convolutional and deconvolutional networks [24]. Deconvolutional networks are CNNs that operate during a reversed process, and networks extract discriminated features. e deconvolutional layers are applied for smothering the segmentation maps to get the ultimate high-resolution output.
## (s28) Residual Attention Neural.
Number of References: 4

(p28.0) Zhou et al. [25] designed residual attention neural that improves CNNs feature representation by incorporating attention modules into CNN and forms a network capable of learning object-aware features. It employs a feed-forward CNN that stacks residual blocks with an attention module. It combines two different learning strategies into the eye module that permits fast feedforward processing and top-down attention feedback during a single feed-forward process to supply dense features that infer each pixel. e bottom-up feed-forward structure produces low-resolution feature maps with reliable semantic information. e top-down learning strategy globally optimises the network such that it gradually outputs the maps to input during the training process. Table 2 shows a summary of the deep convolutional neural network architecture used for ear identification.

(p28.1) Zhou et al. [25] designed residual attention neural that improves CNNs feature representation by incorporating attention modules into CNN and forms a network capable of learning object-aware features. It employs a feed-forward CNN that stacks residual blocks with an attention module. It combines two different learning strategies into the eye module that permits fast feedforward processing and top-down attention feedback during a single feed-forward process to supply dense features that infer each pixel. e bottom-up feed-forward structure produces low-resolution feature maps with reliable semantic information. e top-down learning strategy globally optimises the network such that it gradually outputs the maps to input during the training process. Table 2 shows a summary of the deep convolutional neural network architecture used for ear identification.
## (s32) e University of Beira Ear (UBEAR) Database.
Number of References: 4

(p32.0) e University of Beira presented the UBEAR database [27]. e database comprises 4429 images of 126 subjects, and these were of both males and females. e images were taken under varying lighting conditions and angles, and partial occlusions were present. ese images are of the ear, both the left-and right-hand side ear images were provided.

(p32.1) e University of Beira presented the UBEAR database [27]. e database comprises 4429 images of 126 subjects, and these were of both males and females. e images were taken under varying lighting conditions and angles, and partial occlusions were present. ese images are of the ear, both the left-and right-hand side ear images were provided.
## (s33) e Annotated Web Ear (AWE) Database.
Number of References: 8

(p33.0) e AWE ear database [28] was a set of public figures from web images. e database was formed from 1000 images of 100 6 Applied Computational Intelligence and Soft Computing different subjects, whose sizes varied and were tightly cropped. Both the left-and right-hand sides of the ears were taken.

(p33.1) 3.5. EarVN1.0. e EarVN1.0 database [29] comprises 28412 images of 164 Asian male and female subjects, and left-and right-hand sides of the ear were captured. It was collected during 2018 and is formed from unconstrained conditions, including camera systems and lighting conditions. e pictures are cropped from facial images to obtain the ears, and the pictures have significant variations in pose, scale, and illumination.

(p33.2) e AWE ear database [28] was a set of public figures from web images. e database was formed from 1000 images of 100 6 Applied Computational Intelligence and Soft Computing different subjects, whose sizes varied and were tightly cropped. Both the left-and right-hand sides of the ears were taken.

(p33.3) 3.5. EarVN1.0. e EarVN1.0 database [29] comprises 28412 images of 164 Asian male and female subjects, and left-and right-hand sides of the ear were captured. It was collected during 2018 and is formed from unconstrained conditions, including camera systems and lighting conditions. e pictures are cropped from facial images to obtain the ears, and the pictures have significant variations in pose, scale, and illumination.
## (s34) e Western Pomeranian University of Technology Ear (WPUTE) Database.
Number of References: 4

(p34.0) e Western Pomeranian University of Technology Ear (WPUTE) database [32] was obtained in the year 2010 to gauge the ear recognition performance for images obtained in the wild. e database contains 2071 ear images belonging to 501 subjects. e images were of various sizes and held both the left-and right-hand sides of the ear and were taken under different indoor lighting conditions and rotations. ere were some occlusions included in the database. ese were the headset, earrings, and hearing aids.

(p34.1) e Western Pomeranian University of Technology Ear (WPUTE) database [32] was obtained in the year 2010 to gauge the ear recognition performance for images obtained in the wild. e database contains 2071 ear images belonging to 501 subjects. e images were of various sizes and held both the left-and right-hand sides of the ear and were taken under different indoor lighting conditions and rotations. ere were some occlusions included in the database. ese were the headset, earrings, and hearing aids.
## (s36) In the Wild Ear (ITWE) Database.
Number of References: 4

(p36.0) e In the Wild Ear (ITWE) database [33] was created for recognition evaluation and has 2058 total images, including 231 male and female subjects. A boundary box obtained these images of the ear. e coordinates of those boundary boxes were released with the gathering. e pictures contained cluttered backgrounds and were of variable size and determination. e database includes both the left-and right-hand sides of the ear, but no differentiation was given about the ears.

(p36.1) e In the Wild Ear (ITWE) database [33] was created for recognition evaluation and has 2058 total images, including 231 male and female subjects. A boundary box obtained these images of the ear. e coordinates of those boundary boxes were released with the gathering. e pictures contained cluttered backgrounds and were of variable size and determination. e database includes both the left-and right-hand sides of the ear, but no differentiation was given about the ears.
## (s37) e University of Science and Technology, Beijing (USTB) Ear Database.
Number of References: 4

(p37.0) e University of Science and Technology Beijing (USTB) Ear Database [30] contained cropped ear and head profile images of male and female subjects split into four sets. Dataset one includes 60 subjects and has 180 images of right-close-up ears during 2002. ese images were taken under different lighting, experiencing some shearing and rotation. Dataset two contains 77 subjects and has 308 images of the right-hand side ear, approximately 2 m away from the ear, and the images were taken in 2004. ese images were taken under different lighting conditions. Dataset three contains 103 subjects and has 1600 images. ese images were taken during the year 2004. e images are on the proper and left rotation, and therefore, the images are of the dimensions 768 × 576. e dataset contains 25500 images of 500 subjects; these were obtained from 2007 to 2008; the subject was in the centre of the camera circle. e images were taken when the subject looked upwards, downwards, and at eye level. e images in this dataset contained different yaw and pitch poses. e databases are available on request and accessible for research.

(p37.1) e University of Science and Technology Beijing (USTB) Ear Database [30] contained cropped ear and head profile images of male and female subjects split into four sets. Dataset one includes 60 subjects and has 180 images of right-close-up ears during 2002. ese images were taken under different lighting, experiencing some shearing and rotation. Dataset two contains 77 subjects and has 308 images of the right-hand side ear, approximately 2 m away from the ear, and the images were taken in 2004. ese images were taken under different lighting conditions. Dataset three contains 103 subjects and has 1600 images. ese images were taken during the year 2004. e images are on the proper and left rotation, and therefore, the images are of the dimensions 768 × 576. e dataset contains 25500 images of 500 subjects; these were obtained from 2007 to 2008; the subject was in the centre of the camera circle. e images were taken when the subject looked upwards, downwards, and at eye level. e images in this dataset contained different yaw and pitch poses. e databases are available on request and accessible for research.
## (s43) e University of Notre Dame (UND) Database.
Number of References: 4

(p43.0) e University of Notre Dame (UND) database contains [37] many subsets of 2D and 3D ear images. ese images were appropriated for a period from 2003 to 2005. e database contains 3480 3D images from 952 male and female subjects and 464 2D images from 114 male and female subjects. ese images were taken in different lighting conditions, yaw, pitch poses, and angles. e images are only of the left-hand side ear.

(p43.1) e University of Notre Dame (UND) database contains [37] many subsets of 2D and 3D ear images. ese images were appropriated for a period from 2003 to 2005. e database contains 3480 3D images from 952 male and female subjects and 464 2D images from 114 male and female subjects. ese images were taken in different lighting conditions, yaw, pitch poses, and angles. e images are only of the left-hand side ear.
## (s44) e Face Recognition Technology (FERET) Database.
Number of References: 4

(p44.0) e Face Recognition Technology (FERET) database [38] is a sizeable facial image database and was obtained between the years 1995 and 1996. It contains 1564 subjects and has a total of 14126 images. ese images were collected for face recognition and were of the left-and right-hand profile images, which made them perfect for 2D ear recognition.

(p44.1) e Face Recognition Technology (FERET) database [38] is a sizeable facial image database and was obtained between the years 1995 and 1996. It contains 1564 subjects and has a total of 14126 images. ese images were collected for face recognition and were of the left-and right-hand profile images, which made them perfect for 2D ear recognition.
## (s70) Introduction
Number of References: 4

(p70.0) e ear begins to develop on a fetus amid the fth and seventh weeks of pregnancy [1]. At this stage of the pregnancy, the face acquires a more distinguishable shape as the mouth, nostrils, and ears begin to form. ere is still no exact timeline at which the outer ear is created during pregnancy, but it is accepted that a cluster of embryonic cells connect to establish the ear. ese are called auricular hillocks, which begin to grow in the lower portion of the neck. e auricular hillocks broaden and intertwine within the seventh week to deliver the ear's shape. Within the ninth week, the hillocks move to the ear canal and are more noticeable as the ear [1]. e external anatomy of the ear can be seen in Figure 1. e growth of the ear in the rst four months after birth is linear. e ear is then stretched in development between the ages of four months and eight years. After this, the ear size and shape are constant until the age of seventy, when they increase in size again.
## (s74) Building Block for Convolutional Neural Networks
Number of References: 2

(p74.0) is layer is a set of learnable filters or kernels used to slide over the entire input volume, performing a dot product between entries of the filter and the input layer [5]. e convolutional operation first extracts patches from its information in a sliding window fashion and then applies the same linear transformation to all the areas. e output of the convolutional operation is referred to as a feature map. e network will learn filters and then recognise the visual patterns that are in the input data. is is often shown asx l ij
## (s77) Fully Connected Layer.
Number of References: 4

(p77.0) e fully connected layer is used as a feature extractor. e features produced are then passed to the fully connected layers for classification. Each unit in the fully connected layer is connected to all the units in the previous layers. e last layer is usually a classifier that produces a probability map over the different classes. All the features are converted into one-dimensional feature vectors before passing into the fully connected layer. e reason that this is carried out is that spatial information in the image data is lost, has a high computational cost, and can only work with images that are of the same size [6]. is is often shown as

(p77.1) e fully connected layer is used as a feature extractor. e features produced are then passed to the fully connected layers for classification. Each unit in the fully connected layer is connected to all the units in the previous layers. e last layer is usually a classifier that produces a probability map over the different classes. All the features are converted into one-dimensional feature vectors before passing into the fully connected layer. e reason that this is carried out is that spatial information in the image data is lost, has a high computational cost, and can only work with images that are of the same size [6]. is is often shown as
## (s84) Cascaded Architecture.
Number of References: 2

(p84.0) In the cascaded architecture, the output of the CNN is concatenated with another [9]. ere are many variations with this architecture within the literature, but the input cascade is prominent. In this architecture, the output of the CNN becomes a direct input of another CNN. e input cascade is employed to concatenate the contextual information to the second CNN as additional image channels. Cascaded architecture is an improvement to the only pathway that performs multiscale label prediction separately.
## (s85) UNET.
Number of References: 4

(p85.0) UNET improves a convolutional network that resembles an encoder and decoder network designed to do biomedical image segmentation [10]. e network consists of a contracting path and an expansive path, which provides it with the u-shaped architecture. e contracting path consists of the repeated application of two convolutional layers, followed by a rectified linear measure and a top pooling layer that goes along the trail to scale back the spatial information while feature information is increased. e expansive path consists of upsampling operations combined with high-resolution features from the contraction path through skip connections.

(p85.1) UNET improves a convolutional network that resembles an encoder and decoder network designed to do biomedical image segmentation [10]. e network consists of a contracting path and an expansive path, which provides it with the u-shaped architecture. e contracting path consists of the repeated application of two convolutional layers, followed by a rectified linear measure and a top pooling layer that goes along the trail to scale back the spatial information while feature information is increased. e expansive path consists of upsampling operations combined with high-resolution features from the contraction path through skip connections.
## (s87) Visual Geometry Group
Number of References: 4

(p87.0) Architecture. Visual geometry group architecture is a network created by Visual Graphics Group researchers at Oxford University [12]. It is characterised by a pyramidal shape because it comprises a group of convolutional layers followed by pooling layers; these pooling layers make the layers narrower in shape. e benefits include keeping a good architecture used for benchmarking for any task. e pretrained networks of the VGG are also primarily used for different applications but require numerous computational resources and are slow to coach, above all when training the dataset from scratch.

(p87.1) Architecture. Visual geometry group architecture is a network created by Visual Graphics Group researchers at Oxford University [12]. It is characterised by a pyramidal shape because it comprises a group of convolutional layers followed by pooling layers; these pooling layers make the layers narrower in shape. e benefits include keeping a good architecture used for benchmarking for any task. e pretrained networks of the VGG are also primarily used for different applications but require numerous computational resources and are slow to coach, above all when training the dataset from scratch.
## (s88) GoogLeNet Architecture.
Number of References: 4

(p88.0) e GoogLeNet architecture is referred to as the inception network and was created by Google researchers [13]. It is made from twenty-two layers with two options that these layers can either convolute or pool the input. e architecture contains many beginning modules stacked over each other, allowing joint and parallel training, which helps with faster convergence. e benefits are that there is speedier training, which reduces the size. It , however, possesses an Xception network, which could increase the point for the divergence of the beginning module.

(p88.1) e GoogLeNet architecture is referred to as the inception network and was created by Google researchers [13]. It is made from twenty-two layers with two options that these layers can either convolute or pool the input. e architecture contains many beginning modules stacked over each other, allowing joint and parallel training, which helps with faster convergence. e benefits are that there is speedier training, which reduces the size. It , however, possesses an Xception network, which could increase the point for the divergence of the beginning module.
## (s90) ResNeXt Architecture.
Number of References: 4

(p90.0) ResNeXt architecture is the present state-of-the-art technique for visual perception, which is a hybridisation between inception and ResNeXt architectures [15]. ResNeXt is referred to as the aggregated residual transform network, but it is an improvement over the inception network. It splits the concept and transforms and merges in a commanding but easy way by bringing in cardinality. It uses residual learning, which will enhance the joining of the deep and wide networks. ResNeXt uses many transformations within a split, transform, and merge blocks; and the transformations in cardinality define these. ResNeXt used a mixture of VGG topology and GoogLeNet architecture to correct the spatial resolution using 3 × 3 filters within the split, transform, and merge blocks. e increase in cardinality improves the performance and produces a different and improved architecture.

(p90.1) ResNeXt architecture is the present state-of-the-art technique for visual perception, which is a hybridisation between inception and ResNeXt architectures [15]. ResNeXt is referred to as the aggregated residual transform network, but it is an improvement over the inception network. It splits the concept and transforms and merges in a commanding but easy way by bringing in cardinality. It uses residual learning, which will enhance the joining of the deep and wide networks. ResNeXt uses many transformations within a split, transform, and merge blocks; and the transformations in cardinality define these. ResNeXt used a mixture of VGG topology and GoogLeNet architecture to correct the spatial resolution using 3 × 3 filters within the split, transform, and merge blocks. e increase in cardinality improves the performance and produces a different and improved architecture.
## (s91) Advance Inception Network.
Number of References: 8

(p91.0) e advance inception network includes Inception-V3, Inception-V4, and Inception-ResNet. is is often an improved version of Inception-V1, Inception-V2, and GoogLeNet [16]. Inception-V3 reduces the computational cost of deep networks but does not affect generalisation. Szegedy et al. [17] replaced large-sized filters (5 × 5 and 7 × 7) with small and unequal filters (1 × 7 and 1 × 5) and used 1 × 1 convolution as a blockage before the vast filters. Inception-ResNet combines the strength of the residual learning and starting block.

(p91.1) e advance inception network includes Inception-V3, Inception-V4, and Inception-ResNet. is is often an improved version of Inception-V1, Inception-V2, and GoogLeNet [16]. Inception-V3 reduces the computational cost of deep networks but does not affect generalisation. Szegedy et al. [17] replaced large-sized filters (5 × 5 and 7 × 7) with small and unequal filters (1 × 7 and 1 × 5) and used 1 × 1 convolution as a blockage before the vast filters. Inception-ResNet combines the strength of the residual learning and starting block.
## (s93) Xception Architecture.
Number of References: 4

(p93.0) Xception architecture is referred to as risky inception architecture that overdoes depthwise separable convolution [19]. e first inception block is modified by making it more complete and substituting different spatial dimensions (1 × 1, 5 × 5, and 3 × 3) with one dimension (3 × 3) followed by a 1 × 1 convolution to achieve computational complexity. It makes the network computationally efficient by uncoupling spatial and feature map channels.

(p93.1) Xception architecture is referred to as risky inception architecture that overdoes depthwise separable convolution [19]. e first inception block is modified by making it more complete and substituting different spatial dimensions (1 × 1, 5 × 5, and 3 × 3) with one dimension (3 × 3) followed by a 1 × 1 convolution to achieve computational complexity. It makes the network computationally efficient by uncoupling spatial and feature map channels.
## (s95) Fully Convolutional Network.
Number of References: 12

(p95.0) A fully convolutional network [21] is a set of convolutional and pooling layers. Bi et al. [22] developed a multistage fully convolutional network with the parallel integration method for segmentation. [23] may be a particular sort of artificial neural network that builds on a pyramidal structure by utilising skip connections that skip some convolutional layers. It is composed mainly of multiple convolutional layers.

(p95.1) A fully convolutional network [21] is a set of convolutional and pooling layers. Bi et al. [22] developed a multistage fully convolutional network with the parallel integration method for segmentation. [23] may be a particular sort of artificial neural network that builds on a pyramidal structure by utilising skip connections that skip some convolutional layers. It is composed mainly of multiple convolutional layers.
## (s97) Convolutional and Deconvolutional Neural Networks.
Number of References: 4

(p97.0) is architecture is formed from two significant parts: convolutional and deconvolutional networks [24]. Deconvolutional networks are CNNs that operate during a reversed process, and networks extract discriminated features. e deconvolutional layers are applied for smothering the segmentation maps to get the ultimate high-resolution output.

(p97.1) is architecture is formed from two significant parts: convolutional and deconvolutional networks [24]. Deconvolutional networks are CNNs that operate during a reversed process, and networks extract discriminated features. e deconvolutional layers are applied for smothering the segmentation maps to get the ultimate high-resolution output.
## (s98) Residual Attention Neural.
Number of References: 4

(p98.0) Zhou et al. [25] designed residual attention neural that improves CNNs feature representation by incorporating attention modules into CNN and forms a network capable of learning object-aware features. It employs a feed-forward CNN that stacks residual blocks with an attention module. It combines two different learning strategies into the eye module that permits fast feedforward processing and top-down attention feedback during a single feed-forward process to supply dense features that infer each pixel. e bottom-up feed-forward structure produces low-resolution feature maps with reliable semantic information. e top-down learning strategy globally optimises the network such that it gradually outputs the maps to input during the training process. Table 2 shows a summary of the deep convolutional neural network architecture used for ear identification.

(p98.1) Zhou et al. [25] designed residual attention neural that improves CNNs feature representation by incorporating attention modules into CNN and forms a network capable of learning object-aware features. It employs a feed-forward CNN that stacks residual blocks with an attention module. It combines two different learning strategies into the eye module that permits fast feedforward processing and top-down attention feedback during a single feed-forward process to supply dense features that infer each pixel. e bottom-up feed-forward structure produces low-resolution feature maps with reliable semantic information. e top-down learning strategy globally optimises the network such that it gradually outputs the maps to input during the training process. Table 2 shows a summary of the deep convolutional neural network architecture used for ear identification.
## (s102) e University of Beira Ear (UBEAR) Database.
Number of References: 4

(p102.0) e University of Beira presented the UBEAR database [27]. e database comprises 4429 images of 126 subjects, and these were of both males and females. e images were taken under varying lighting conditions and angles, and partial occlusions were present. ese images are of the ear, both the left-and right-hand side ear images were provided.

(p102.1) e University of Beira presented the UBEAR database [27]. e database comprises 4429 images of 126 subjects, and these were of both males and females. e images were taken under varying lighting conditions and angles, and partial occlusions were present. ese images are of the ear, both the left-and right-hand side ear images were provided.
## (s103) e Annotated Web Ear (AWE) Database.
Number of References: 8

(p103.0) e AWE ear database [28] was a set of public figures from web images. e database was formed from 1000 images of 100 6 Applied Computational Intelligence and Soft Computing different subjects, whose sizes varied and were tightly cropped. Both the left-and right-hand sides of the ears were taken.

(p103.1) 3.5. EarVN1.0. e EarVN1.0 database [29] comprises 28412 images of 164 Asian male and female subjects, and left-and right-hand sides of the ear were captured. It was collected during 2018 and is formed from unconstrained conditions, including camera systems and lighting conditions. e pictures are cropped from facial images to obtain the ears, and the pictures have significant variations in pose, scale, and illumination.

(p103.2) e AWE ear database [28] was a set of public figures from web images. e database was formed from 1000 images of 100 6 Applied Computational Intelligence and Soft Computing different subjects, whose sizes varied and were tightly cropped. Both the left-and right-hand sides of the ears were taken.

(p103.3) 3.5. EarVN1.0. e EarVN1.0 database [29] comprises 28412 images of 164 Asian male and female subjects, and left-and right-hand sides of the ear were captured. It was collected during 2018 and is formed from unconstrained conditions, including camera systems and lighting conditions. e pictures are cropped from facial images to obtain the ears, and the pictures have significant variations in pose, scale, and illumination.
## (s104) e Western Pomeranian University of Technology Ear (WPUTE) Database.
Number of References: 4

(p104.0) e Western Pomeranian University of Technology Ear (WPUTE) database [32] was obtained in the year 2010 to gauge the ear recognition performance for images obtained in the wild. e database contains 2071 ear images belonging to 501 subjects. e images were of various sizes and held both the left-and right-hand sides of the ear and were taken under different indoor lighting conditions and rotations. ere were some occlusions included in the database. ese were the headset, earrings, and hearing aids.

(p104.1) e Western Pomeranian University of Technology Ear (WPUTE) database [32] was obtained in the year 2010 to gauge the ear recognition performance for images obtained in the wild. e database contains 2071 ear images belonging to 501 subjects. e images were of various sizes and held both the left-and right-hand sides of the ear and were taken under different indoor lighting conditions and rotations. ere were some occlusions included in the database. ese were the headset, earrings, and hearing aids.
## (s106) In the Wild Ear (ITWE) Database.
Number of References: 4

(p106.0) e In the Wild Ear (ITWE) database [33] was created for recognition evaluation and has 2058 total images, including 231 male and female subjects. A boundary box obtained these images of the ear. e coordinates of those boundary boxes were released with the gathering. e pictures contained cluttered backgrounds and were of variable size and determination. e database includes both the left-and right-hand sides of the ear, but no differentiation was given about the ears.

(p106.1) e In the Wild Ear (ITWE) database [33] was created for recognition evaluation and has 2058 total images, including 231 male and female subjects. A boundary box obtained these images of the ear. e coordinates of those boundary boxes were released with the gathering. e pictures contained cluttered backgrounds and were of variable size and determination. e database includes both the left-and right-hand sides of the ear, but no differentiation was given about the ears.
## (s107) e University of Science and Technology, Beijing (USTB) Ear Database.
Number of References: 4

(p107.0) e University of Science and Technology Beijing (USTB) Ear Database [30] contained cropped ear and head profile images of male and female subjects split into four sets. Dataset one includes 60 subjects and has 180 images of right-close-up ears during 2002. ese images were taken under different lighting, experiencing some shearing and rotation. Dataset two contains 77 subjects and has 308 images of the right-hand side ear, approximately 2 m away from the ear, and the images were taken in 2004. ese images were taken under different lighting conditions. Dataset three contains 103 subjects and has 1600 images. ese images were taken during the year 2004. e images are on the proper and left rotation, and therefore, the images are of the dimensions 768 × 576. e dataset contains 25500 images of 500 subjects; these were obtained from 2007 to 2008; the subject was in the centre of the camera circle. e images were taken when the subject looked upwards, downwards, and at eye level. e images in this dataset contained different yaw and pitch poses. e databases are available on request and accessible for research.

(p107.1) e University of Science and Technology Beijing (USTB) Ear Database [30] contained cropped ear and head profile images of male and female subjects split into four sets. Dataset one includes 60 subjects and has 180 images of right-close-up ears during 2002. ese images were taken under different lighting, experiencing some shearing and rotation. Dataset two contains 77 subjects and has 308 images of the right-hand side ear, approximately 2 m away from the ear, and the images were taken in 2004. ese images were taken under different lighting conditions. Dataset three contains 103 subjects and has 1600 images. ese images were taken during the year 2004. e images are on the proper and left rotation, and therefore, the images are of the dimensions 768 × 576. e dataset contains 25500 images of 500 subjects; these were obtained from 2007 to 2008; the subject was in the centre of the camera circle. e images were taken when the subject looked upwards, downwards, and at eye level. e images in this dataset contained different yaw and pitch poses. e databases are available on request and accessible for research.
## (s113) e University of Notre Dame (UND) Database.
Number of References: 4

(p113.0) e University of Notre Dame (UND) database contains [37] many subsets of 2D and 3D ear images. ese images were appropriated for a period from 2003 to 2005. e database contains 3480 3D images from 952 male and female subjects and 464 2D images from 114 male and female subjects. ese images were taken in different lighting conditions, yaw, pitch poses, and angles. e images are only of the left-hand side ear.

(p113.1) e University of Notre Dame (UND) database contains [37] many subsets of 2D and 3D ear images. ese images were appropriated for a period from 2003 to 2005. e database contains 3480 3D images from 952 male and female subjects and 464 2D images from 114 male and female subjects. ese images were taken in different lighting conditions, yaw, pitch poses, and angles. e images are only of the left-hand side ear.
## (s114) e Face Recognition Technology (FERET) Database.
Number of References: 4

(p114.0) e Face Recognition Technology (FERET) database [38] is a sizeable facial image database and was obtained between the years 1995 and 1996. It contains 1564 subjects and has a total of 14126 images. ese images were collected for face recognition and were of the left-and right-hand profile images, which made them perfect for 2D ear recognition.

(p114.1) e Face Recognition Technology (FERET) database [38] is a sizeable facial image database and was obtained between the years 1995 and 1996. It contains 1564 subjects and has a total of 14126 images. ese images were collected for face recognition and were of the left-and right-hand profile images, which made them perfect for 2D ear recognition.
