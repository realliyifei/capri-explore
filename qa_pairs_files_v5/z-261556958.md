# Robust Recommender System: A Survey and Future Directions

CorpusID: 261556958 - [https://www.semanticscholar.org/paper/73d4a4e39eafc5884247f6cacf42228476ae3b54](https://www.semanticscholar.org/paper/73d4a4e39eafc5884247f6cacf42228476ae3b54)

Fields: Computer Science

## (s3) Taxonomy of Robustness in Recommender System
Number of References: 9

(p3.0) Recommender systems function as highly interactive platforms, making them vulnerable to various forms of abnormal data. These anomalies can originate from malicious activities such as injecting fake users and tampering with item information, or from natural noise which typically arises due to human errors or ambiguities in user behavior. For malicious attacks, attackers often aim to promote/nuke specific items or damage the performance of recommender systems. Generally, adversarial scenarios often restrict the attackers' abilities to manipulate a user's historical behavior. There are typically two types of attacks in recommendation scenarios, as shown in Figure 2(b): (1) Item side information modification: Attackers manipulate item side information to artificially augment the popularity of specific items [33], as depicted in the top of Figure 2(b). (2) Fake user injection (shilling attack): Introducing fake users to inflate or deflate the exposure of certain items or degrade the overall performance of the recommender systems [30,120], as shown in the bottom of Figure 2(b). It is essential to note that,  even with limitations on potential attack, several defense mechanisms against interaction-level attacks have been proposed in the literature [24,83,156,158]. For a comprehensive understanding, these defense methodologies will be elaborated upon in the subsequent sections, particularly in Section 3.2.4.

(p3.1) Natural noise mainly stems from user-generated factors such as human errors, uncertainty, and ambiguity in user behavior [151]. For instance, it may manifest in the historical interactions between users and items, such as misclicks or gifts reflecting others' tastes, as illustrated in Figure 2(c). Note that, noise can also occur in user or item side information, such as incorrect personal information or item tags. However, due to the paucity of research addressing these types of noise in the context of recommender systems [82], Figure 2(c) does not show this category.
## (s5) Fraudster Detection
Number of References: 6

(p5.0) The detection of fraudsters is paramount for ensuring the robustness of recommender systems, especially against malicious attacks. Such attacks often involve fraudsters providing misleading feedback to manipulate the system [30,120]. Thus, fraudster detection aims to identify and eliminate these users to maintain reliable recommendations. Fraudster detection approaches can be categorized into three types, corresponding to when detection occurs: pre-processing, in-processing, and post-processing detection ( Figure 5(a)). Pre-processing detection [148,177] aims to find fraudsters in the original data before training, eliminating their impact from the source. In-processing detection [165] further leverages model feedback during training to detect and weaken fraudster impact. Post-processing detection [20] identifies and corrects bad recommendations caused by fraudsters. Each stage provides tailored detection strategies. Pre-processing is the mainstream approach, while in-processing and post-processing are more recent.
## (s11) Discussion of Fraudster Detection.
Number of References: 4

(p11.0) In the context of fraudster detection within recommender systems, there are three principal detection strategies: pre-processing, in-processing, and postprocessing detection. Pre-processing detection functions outside of model training, offering the advantage of computational efficiency during the training phase [4,74]. In-processing detection integrates model insights throughout training to achieve better accuracy [165]. Meanwhile, postprocessing detection aims to filter out bad recommendations for next items [20]. However, its performance can be compromised when the filtered objects are recommendation lists.
## (s16) Adversarial Perturbation on Interaction.
Number of References: 2

(p16.0) In the realm of recommender systems, there's a growing interest in methods that employ adversarial training on interaction graphs. Such methods focus on introducing adversarial perturbations within these graphs. The core motivation behind this is to bolster the robustness of recommender systems against potential threats targeting the interaction data between users and items. Recent studies, such as Conv-GCF [156] and AdvGraph [24], delve into embedding adversarial perturbations directly into the interaction matrix. This integration can be modeled as the optimization problem described by the following objective function:
## (s20) Adjustment of Training Process
Number of References: 2

(p20.0) Orthogonal Mapping where , is the ground truth rating andˆ, is predicted rating computing by Θ. The 1 -norm offers enhanced robustness against outliers or anomalous data in comparison to the Euclidean distance (i.e., 2 -norm). By integrating the 1 -norm regularization, the model prioritizes essential features over noise, leading to more refined recommendations. In recent times, researchers continue to innovate regularization strategies to improve the robustness of recommender systems in the presence of natural noise. As an illustration, Chen et al. [23] meld Jacobian regularization [68] with the transformer block in sequential recommender systems. This regularization enables a significant reduction in the model's susceptibility to noisy sequences, consequently delivering more consistent and trustworthy recommendations amidst noise.
## (s29) Certifiable robustness.
Number of References: 4

(p29.0) Certifiable robustness focuses on finding the robustness boundary for a given instance in the recommender systems model. Traditional methods for certifiable robustness [32,75] can be categorized into two approaches: randomized smoothing and directly finding the worst perturbation. Randomized smoothing is a technique that smoothes the input by applying random noise, aiming to find an adversarial boundary that causes the model to produce incorrect outputs. However, in the recommender systems scenario, it is difficult to smooth the input of the model due to the semantics and discreteness of the features.

(p29.1) Directly finding the worst perturbation involves searching for the worst perturbation that can lead to an incorrect prediction for a given input. Liu et al. [83] provide both non-robust certification and robust certification by approximately calculating the worst perturbation for the FM model. For a given FM model , input sample , which includes historical interaction and other features, and the perturbation budget , let ′ denote the perturbed instance corresponding to . Recall the formulation of the FM model ( ) in Eq. 14, Liu et al. [83] formulate the prediction shift as:
## (s33) From Perspective of Applications
Number of References: 3

(p33.0) E-commerce Recommender Systems: E-commerce recommender systems, a crucial feature on platforms like Amazon and eBay, are designed to simplify product discovery for customers [116]. They function based on the historical browsing or purchasing patterns of customers, leaning heavily on user-item interaction data and item-side information. Despite their effectiveness, these systems confront several robustness concerns. One significant issue is the natural noise in user data through unintentional clicks or purchases, which may not necessarily represent the user's genuine preferences [151]. Furthermore, the system's integrity can be compromised by attackers manipulating item side information to push certain products or fabricate user profiles to manipulate a product's exposure [33]. In an e-commerce context, considerations of recommender systems' robustness are comprehensive. Beyond the strategies delineated in Sections 3 and 4, real-world applications necessitate these systems to adapt to the dynamic nature of product trends, cope with extensive product catalogs, and manage the persistent relevance of products over time. Collectively, these requisites pose new challenges in upholding robustness.
## (s38) Robustness v.s. Fairness
Number of References: 6

(p38.0) Fairness in machine learning typically signifies that an algorithm or model provides impartial and unbiased predictions across different groups or individuals [97,134]. Many methods interpret fairness as a model's invariant prediction to alterations in sensitive attributes (e.g., gender) within the input data [69,97]. This objective shares a resemblance with adversarial training where small perturbations in input data shouldn't alter predictions [157]. Nonetheless, a distinction persists: fairness concentrates explicitly on modifications to select attributes without imposing constraints on the extent of these changes. In contrast, robustness isn't tied to specific attributes but stipulates that the magnitude of changes should remain moderate to avoid distorting the ground truth label. Recent studies suggest that enhancing a model's robustness can indirectly improve its fairness [99].
## (s40) Mitigating Gap between Defense Assumption and Attack Goal
Number of References: 2

(p40.0) A significant challenge that arises in the realm of recommender systems is the gap between defense assumptions and the actual objectives of attacks. Attacks, especially shilling attacks, often aim to promote or diminish product exposure, rather than merely undermining recommendation performance. However, many prevailing defense strategies are predicated on the notion that attackers principally aim to degrade the functionality of recommender systems. This is particularly evident in adversarial training scenarios. For instance, He et al. [56] incorporate adversarial perturbations to model parameters during its training phase. Yet, these perturbations often don't align with real-world attack patterns, meaning certain adversarial examples optimized during training might be ineffective for defense. In another approach, Wu et al. [137] positively introduce empirical risk minimizing users to counterbalance the impacts of malicious users, who are often perceived as threats to recommendation quality.
## (s41) Improving Generalization of Defense Methods
Number of References: 6

(p41.0) The generalization of defense methods in recommender systems presents a considerable challenge. A significant portion of current defense strategies is on specific constraints. For example, the approach proposed by Liu et al. [83] is tailored exclusively for models based on the Factorization Machine, while the methodology introduced by Cao et al. [20] is designed for models grounded in Reinforcement Learning. Moreover, numerous pre-processing detection techniques hinge on predefined features for detecting malicious users [11,74]. Some even resort to specific attack methods for generating supervised data [149,162]. Such specialization limits the applicability of defense methods across diverse scenarios, posing challenges to effectively countering adversarial threats in large-scale, real-world recommender systems.
