# A SURVEY ON HUMAN ACTION RECOGNITION

CorpusID: 255942153 - [https://www.semanticscholar.org/paper/200a067b06bde6d5b684ed47edf17f9e67283539](https://www.semanticscholar.org/paper/200a067b06bde6d5b684ed47edf17f9e67283539)

Fields: Computer Science

## (s1) RGB Modality
Number of References: 8

(p1.0) Looking back at the development of HAR, a large number of studies are conducted based on RGB data, and among which the representative third-person HAR methods are shown in the figure 2. In the first few years, extensive studies focusing on hand-crafted feature-based approaches have been proposed. With more advanced computers and networks, the proliferation of video data, as well as the rapid development of deep learning, a growing number of studies of HAR is conducted based on the deep learning framework. The results show that the HAR methods under the deep learning have superior performance, which gradually replace the traditional methods and have become the mainstream research. These representative deep learning frameworks are elaborated later, including two-stream CNN, RNN, 3D CNN, and Transformer. Hand-crafted feature methods. Before the rise of deep learning, researchers used traditional recognition methods, i.e., manually extracting features through machine learning and classifying them by relevant algorithms. There are three main categories, spatio-temporal volume based methods [8], spatio-temporal interest point (STIP) based methods [72] and trajectory based methods [138,139]. Bobicket et al. [8] proposed to use the motion-energy images (MEI) and motion-history images (MHI) by projecting the human body along the time axis in a 3D cube, then classify behavior through template matching method. However, this method is not applicable to complex scenes. The classical STIP method was mainly proposed by Laptev [72], whose main idea is to extend the feature detection technology from 2D image to 3D spatio-temporal and calculate its feature descriptors, but this method ignores many details of video, and has weak generalization ability as well. Wang et al. [138,139] proposed the dense trajectory (DT) and improved trajectory (IDT). Spatial feature points are detected on each frame of image, and these feature points are tracked individually at each scale to form a trajectory of fixed length, which is finally described by descriptors. The advantage of IDT method lies in the estimation of camera motion by matching SURF descriptors and dense optical flow feature points between the front and back frames, thus eliminating the effect caused by camera motion. It is classified finally by a support vector machine after feature extraction. DT and IDT, although being traditional methods, are comparable to some deep learning methods, meanwhile the method can achieve fantastic results when combined with deep learning framework. The disadvantage is the speed of this algorithm is slow and it needs to accurately track feature points, which is challenging for computers.
## (s9) RGB and Inertial Sensors
Number of References: 8

(p9.0) In addition to vision-based sensors, inertial sensors have been used for human action recognition, allowing recognition beyond the limited field of view of vision-based sensors. Inertial sensors contain accelerometers and gyroscopes that provide acceleration and angular velocity signals for HAR, and a survey [45] details the performance of current deep learning models and future challenges for sensor-based action recognition. The wearable inertial sensors provide 3D motion data, consisting of the three -axis acceleration of the accelerometer and the three-axis angular velocity of the gyroscope. Inertial and video data in [150,149,148] are captured simultaneously by inertial sensors and video cameras and converted into 2D and 3D images. These images are then fed into 2D and 3D CNNs to fuse their decisions in order to detect and identify a specific set of actions of interest from a continuous stream of actions. In [149,148], a decision-level fusion method is considered. In [150], both feature-level fusion and decision-level fusion are tested, and decision-level fusion achieves higher accuracy. In [86], visual and inertial sensor integration algorithms were proposed for efficient and accurate generic abnormal behavior detection among the elderly, which closely cooperate to achieve high accuracy and real-time performance.
## (s10) RGB-D and Inertial Sensors
Number of References: 23

(p10.0) In recent years, many studies have improved the accuracy of HAR by fusing features extracted from depth and inertia sensor data and using co-representation classifiers. Better accuracy results have been obtained due to the complementarity of the data from two modalities. In the vast majority of the work on action or gesture recognition, it is assumed that the action of interest has been separated from the action stream [91,97,17,19,111]. In [97], decision-level fusion between depth camera data and wearable sensor data is performed to increase the ability of the robot to recognize human actions. In [91], the depth and inertia data are effectively fused to train the Hidden Markov Model to improve the accuracy and robustness of gesture recognition. For continuous action flow, Dawar et al. [29] detected and recognized human actions from continuous action flow by fusing both depth and inertia sensing modalities.

(p10.1) In addition, many deep learning methods have been proposed recently [38,1,30,28,2,3,45]. In [1], a shared feature layer is used after multimodal feature-level fusion, and then support vector machines or softmax classifiers are used to recognize actions based on combined features. Considering that deep inertia training data is limited, Dawar et al. [30] proposed a data enhancement framework based on depth and inertia modalities, which are fed to CNN and LSTM, respectively, and then the scores of the two models are fused during testing for better classification. Given that deep learning model allows to extract features at all levels of the structure so that rich multi-layer features are obtained, while existing methods do not take advantage of these rich multi-level information. Specifically, the main drawback of existing deep learning-based HAR fusion methods based on depth and inertia sensors is that the fusion is performed at a single level or stage, either at the feature level or at the decision level, and therefore the true semantic information of the data cannot be mapped to the classifier. By designing different two-stream CNN architectures, several deep inertial fusion techniques are also studied in [2,3], where inertial signals are converted into images using the techniques in [62]. Three new deep multilevel multimodal (M 2 ) fusion frameworks are proposed in [2] to take advantage of different fusion strategies at different stages. Ahmad et al. [3] proposed a new Multistage Gated Average Fusion (MGAF) network, which extracts and fuses features from all layers of CNN. Recently, Ijaz et al. [59] proposed a multimodal Transformer for nursing action recognition, in which the correlation between skeleton and acceleration data is modeled by Transformer.
## (s16) RGB and Wearable Inertial Sensors
Number of References: 10

(p16.0) Ozcan et al. [103] utilized histograms of edge orientations together with the gradient local binary patterns for fall detection, which then combined with three-axis signal of the accelerometer. Experimental results show that the proposed fusion method not only has higher sensitivity, but also significantly reduces the number of false alarms compared with the accelerometer and camera only methods. In [121], a multi-stream convolutional network is extended to analysis activity in egocentric videos, meanwhile, a novel multi-stream LSTM is proposed for classifying wearable sensor data. Finally, two score fusion techniques, namely average pooling and maximum pooling, are evaluated to obtain recognition results. Song et al. [122] proposed a new technique to integrate temporal information into sensor data with similar trajectories. Moreover, the Fisher Kernel framework is applied to fuse sensor and video data for EAR with Multimodal Fisher Vector (MFV). In the work [34], features are extracted by applying a sliding window to video and inertial data, Whereafter, using majority voting to fuse the results. For the classification task, methods of Random Forest and Support Vector Machine are taken into consideration. The methods in [33,32] are extensions of [34], which use time and frequency domain features for acceleration data, and object information encoding hand interaction for visual data. Experiments are carried out on both feature-level fusion and decision-level fusion, and the latter achieves higher accuracy. In [156], a hierarchical fusion framework based on deep learning is developed and implemented, where LSTM and CNN are used for EAR based on motion sensor data and photo streams at different levels, respectively. Experimental results show that the proposed model enables motion sensor data and photo streams to work in the most suitable classification mode, so as to effectively eliminate the negative impact of sensor differences on the fusion results. A novel framework is proposed in [4], where multi-kernel learning (MKL) is used to fuse multimodal features in order to adaptively weighs the visual, audio, and sensor features, additionally, feature and kernel weighting and recognition tasks are performed simultaneously. Huang et al. [56] proposed a first-view multimodal framework based on knowledge driven, GCN and LSTM, which improve the performance of EAR under conditions of few samples and ultra-small datasets.
## (s22) RGB-D and Inertial Sensors
Number of References: 9

(p22.0) Dawar et al. [29] 86.3%(SS) 2018 Dawar et al. [30] 89.20%(SG) 2018 Dawar et al. [28] 92.80% 2018 Fuad et al. [45] 95.00%(SG) 2018 Ahmad et al. [1] 98.70%(SS) 2018 Javed et al. [38] 98.30%(SG) 2019 Imran et al. [60] 97.90% 2020 MGAF [3] 96.80%(SS) 2020 (M 2 ) fusion [2] 99.20%(SS) 2020
