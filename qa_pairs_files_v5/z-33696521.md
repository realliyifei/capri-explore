# A Review on Automatic Text Summarization Approaches

CorpusID: 33696521 - [https://www.semanticscholar.org/paper/f9638a59852efd53fcc97a5f11d39c007081fab0](https://www.semanticscholar.org/paper/f9638a59852efd53fcc97a5f11d39c007081fab0)

Fields: Computer Science

## (s0) Introduction
Number of References: 6

(p0.0) It has been more than 50 years since Luhn started his initial investigation on automatic text summarization (Luhn, 1958). Since then, various techniques have been successfully used to extract the important contents from text document to represent document summary (Gupta and Lehal, 2010;Nenkova and McKeown, 2011;Saggion and Poibeau, 2013). The aim of automatic text summarization is similar to the reason why we humans create summaries; i.e., to produce a shorter representation of the original text. Through these years, a number of researchers have defined the definition of summary from their own perspective. For instance, Sparck Jones defines a summary as a "reductive transformation of source text to summary text through content reduction by selection and generalization on what is important in the source" (Jones, 1999). Hovy defines a summary as "a text that is produced from one or more texts, that convey important information in the original text(s) and that is no longer than half of the original text (s) and usually significantly less than that" (Hovy, 2005).
## (s5) Feature Based Approach
Number of References: 2

(p5.0) One of the natural way to determine the importance of a sentence is to identify the features that reflects the relevance of that sentence. Edmundson (1969) defined three features deemed indicative to sentence relevance i.e., sentence position, presence of title word and cue words. For example, the beginning sentences in a document usually describes the main information concerning the document. Therefore, selecting sentences based on its position could be a reasonable strategy. The following features are commonly used to determine sentence relevance (Gupta and Lehal, 2010).
## (s13) A. Naive Bayes
Number of References: 2

(p13.0) One of the early works that incorporated machine learning was the study done by Kupiec et al. (1995). They used a Naive Bayes classifier for learning from the data (corpus of document/summary pairs). Their method uses the features that were derived from Edmundson (1969), where the features were independent of each other. Given a sentence s, the probability of it being chosen to be included in the summary is:
## (s16) Medical Summarization
Number of References: 5

(p16.0) The study on automatic summarization was found to be very useful to the medical field. Summarization can help doctors to obtain relevant information about a particular disease or information from the patient records (Becher et al., 2002). It will also be beneficial to patients or users whom turn online to find information pertinent to their health problems (Kaicker et al., 2010). Furthermore, there are extensive resources that provide access to medical information and medical-related databases. For instance, there are over 20 million articles in MEDLINE; a biomedical database. Summarization is thus essential in such condition to treat the problem of information overload. An early summarization system that has been built for medical knowledge is the Centrifuser (Elhadad et al., 2005;Kan et al., 2001). The Centrifuser is a summarizer that helps consumers by producing query-driven summaries in their search for healthcare information. It represents document topics by a tree data structure and perform query mapping from the topic trees to retrieve relevant sentences. Another medical summarizer, proposed by Fiszman et al. (2009), was built to generate summaries based on semantic abstraction to assist physicians find the most salient information in MEDLINE citations for some specified diseases.
