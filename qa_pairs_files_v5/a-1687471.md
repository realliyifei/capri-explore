# A Survey on FPGA-Based Sensor Systems: Towards Intelligent and Reconfigurable Low-Power Sensors for Computer Vision, Control and Signal Processing

CorpusID: 1687471 - [https://www.semanticscholar.org/paper/c2f32f171231da883a7c579e7da8f7b7384579ef](https://www.semanticscholar.org/paper/c2f32f171231da883a7c579e7da8f7b7384579ef)

Fields: Medicine, Computer Science, Engineering

## (s2) Control Systems
Number of References: 15

(p2.0) The use of FPGA in industrial control systems is of great interest due to the increasing level of controllers' requirements [13]. The use of FPGAs allows implementing a dedicated parallel architecture that can be adapted to the plant needs in runtime. FPGAs have already been used with success in different sensor control systems, which requires the implementations of fuzzy logic controllers [21,22], motion controllers [23,24], neural network [25][26][27][28], control of asynchronous motors [29], power converter controls [30], mechatronic systems [31], etc.

(p2.1) The hardware implementation of a control system can improve the speed performance. However, the FPGA resources are limited and the control systems' algorithms must be refined. This last aspect is an important research topic devoted to optimize the FPGA resources in the implementation of control systems algorithms. For example, in [21] a model-based design method for the synthesis of embedded fuzzy controllers for the joint development of hardware and software components is proposed. Although it is possible to implement FPGA sensor-based controllers with floating point arithmetic [32], the required recourses are not optimized with respect to fixed-point calculations. Coordinate Rotation Digital Computer (CORDIC) is a well-known algorithm used to approximate iteratively some transcendental functions by using adders/subtractors and shifters. This approach has been used by several authors in order to refine and optimize a control system to be implemented in an FPGA [33]. Consequently, when control systems must be developed in an FPGA, a compromise between control performance and complexity of the hardware architecture must be achieved. In the next three subsections, the main FPGA-based controller applications are classified in image-based controllers, advanced control approaches and monitoring systems.

LLM judge: NO

The analysis reveals that the content violates several of the provided criteria. 
- It is not entirely self-contained as it makes numerous references to external sources ([13], [21], [22], etc.) without providing enough context or explanation about these sources. This makes understanding the full scope of the application and implications of FPGAs in industrial control without access to the referenced material difficult.
- The passage mentions specific instances of FPGA application (such as fuzzy logic controllers, motion controllers, neural networks, etc.) and specific algorithms (like CORDIC) but does not explain these in detail, relying on the reader to have prior knowledge or access to cited works for full comprehension.
- The text is rich in jargon and specific references (like "floating point arithmetic" and "fixed-point calculations") that can be confusing without additional explanation or context.
- Finally, coherence and logical connection among paragraphs could be improved to offer an easier flow for readers not familiar with FPGA or control systems.

## (s3) Image-Based Controllers
Number of References: 7

(p3.0) As previously described, image information can take advantage of the parallel processing capabilities on FPGAs [4]. This information provides global information about the workspace and is progressively integrated in the control systems. In [34], a neuro-inspired mobile robot with a double spike-based control mechanism for two DC motors is proposed. All the image processing issues are also carried out in an FPGA (capture, processing and line tracking). A similar approach is presented in [35] where an address-event representation is employed for visual sensing, processing and finally actuating a robot. In [36], a hardware/software design and implementation for localization of robot in Mars rover missions is presented. This last paper proposes a system architecture implemented on a Xilinx Virtex-6 FPGA to process the obtained images, perform the visual slam, 3D map reconstruction and to obtain the location of the rover at the map. In [37], a high precision automatic system for liquid level measurement in membrane distillation applications is presented. This approach is based on the laser triangulation principle using two lasers and a camera. The level measurement is obtained by an FPGA that performs the image processing. In [38,39] the Simple Network Robot Protocol (SNRP), which permits the integration of network robots and sensors, is defined. In this case, an FPGA has been used to implement a real-time vision system that provides SNRP services to the network. Using the FPGA computer vision module and the SNRP protocol it is possible to implement visual servoing algorithms for industrial robots.

LLM judge: NO

The content does not meet several of the criteria needed for it to be suitable. Specifically:

1. The content does not offer comprehensive context but rather enumerates examples of how FPGA technology has been applied in different scenarios. This enumeration gives a fragmented view rather than a cohesive argument or explanation.
2. The text seems to rely heavily on references for conveying information about each application (e.g., [4], [34], [35]), which suggests that full understanding might require access to those external sources.
3. It is a compilation of summaries from various studies rather than presenting a self-contained explanation or argument about a singular topic.
4. The logical connection between the paragraphs is weak, as it jumps from one application to another without a clear, overarching narrative or conclusion that ties them all together.

Additionally, the content lists applications without delving into how FPGA technology enhances these systems, lacking the depth needed for a question within a scientific Q&A dataset that meets the outlined criteria.

## (s5) Monitoring Systems and Control
Number of References: 6

(p5.0) The use of FPGA also allows the reduction of delays in the control system feedback. Highly demanding data throughputs can take advantage of the ever-increasing density of the chips in FPGAs [46]. Several applications require not only the capture of sensor information in the feedback but also to process such information in order to obtain the required data to be compared with the system reference. Within this topic, one can mention the work described in [47] where a monitoring infrastructure based on FPGA is proposed. In [37], a computer vision system is presented for liquid level measurement in membrane distillation applications. Another monitoring system is presented in [48][49][50]. In this case, thermal sensors are employed and they can be used to detect, for example, if a given device dissipates excessive power or does not work correctly.

LLM judge: NO

The content provided does not meet the criteria for suitability in a scientific question-answering dataset:

1. The text heavily relies on references ([46], [47], [37], [48][49][50]) which does not allow the content to stand alone without access to the full paper, failing the self-contained requirement.
2. The coherence and connection between the applications mentioned (monitoring infrastructure, computer vision system, thermal sensors) are not explained, making it challenging to create a single, comprehensive question that summarizes the extract effectively.
3. The mention of specific applications and technologies (e.g., FPGA, computer vision system for liquid level measurement, thermal sensors for power dissipation and malfunction detection) does provide relevant context, but the logical connection and comprehensive context criteria are only partially met, as the text reads more like a list of examples rather than a cohesive explanation of FPGA applications.
4. It does not present a clear, uninterrupted narrative due to the insertion of references and listing of applications without detailed explanations or context.

## (s6) Smart Sensors
Number of References: 10

(p6.0) The demand for small sized, high accuracy and low consumption smart sensors has grown over time. The term smart sensor is frequently employed for sensors that integrate several functions in a single portable device such as communications capability, self-diagnostics, decision-making and some -intelligenceâ€–. Therefore, the different topics described throughout this paper can be considered as part of a smart sensor: network sensors, control, signal processing, etc. These options are commonly integrated in an embedded FPGA-based device when the term smart sensor is employed. The use of FPGAs and their reconfigurability feature allows the addition of different capabilities such as signal conditioning and signal processing [16,51,52]. Furthermore, a smart sensor not only provides the sensory information but also performs additional functions for error compensation or for obtaining complex data from that measurement (see e.g., [53] where resistance and capacitance information is extracted from the sensor data or [48,49] where FPGAs are employed to include additional features to thermal sensors [54]).

(p6.1) The term smart camera is currently employed for cameras that combine video sensing, processing, and communication on a single embedded platform [17]. The integration of the hardware and software components of a computer vision system in a single portable smart camera is a challenging task. The capacity of the FPGAs to process large image data has allowed the integration of low and mid-level vision algorithms in an embedded smart camera [55]. In this case, the camera does not provide an image but processes data from the image. This approach is optimal for high-speed applications or those that requires the processing of a large amount of data such as tactile information [56].

LLM judge: YES

What functionalities do smart sensors and smart cameras share, and how do FPGAs contribute to their capabilities?

## (s7) Sensor Networks
Number of References: 6

(p7.0) A sensor network consists of a set of autonomous devices (sensor nodes) connected to a network and distributed in an area susceptible of study. These devices use sensors to monitor physical or environmental conditions, having restrictions on computing power, communication and energy concerns. The term WSNs, already defined in the Introduction section, refers to a sensor network that employs wireless communication. Currently, the number of applications for WSNs has grown hugely in several areas (automation, image processing, security, telemedicine, robotics, domotics, etc.) [11]. The main feature demanded for these applications is reduction of the power consumption because the nodes are usually low-cost sensors operating in an environment with limited processing power and restricted battery autonomy. Therefore, low energy WSNs are needed in engineering fields in order to get the longest lifetime possible. For that end, dynamic reconfigurable devices such as FPGAs allow important improvements concerning energy efficiency, because of their efficient use of the communication channels. Moreover, in this case the FPGAs work as distributed reconfigurable devices that permit the implementation of different functionalities everywhere using remote resources. Most of the contributions in the scientific world try to make the most of the FPGAs in order to reduce the transmission of data among the sensor nodes [57,58], to change dynamically the frequency [59,60] and to turn on the radio transceiver selectively [61]. This subsection describes the main approaches developed in Spain concerning the use of FPGAs in sensor networks, where it will be seen that they are related with the purposes above commented.

LLM judge: YES

How do dynamic reconfigurable devices like FPGAs contribute to enhancing energy efficiency in wireless sensor networks (WSNs) and what are their main applications?

## (s8) Signal Processing
Number of References: 10

(p8.0) Embedded signal processing is another topic of interest in the use of FPGAs. Until the appearance of FPGAs in the electronic world, DSPs were the key devices for signal processing. Currently, for highly demanding tasks, FPGAs have superseded DSPs due to the high efficiency given by their architectural flexibility (parallelism, on-chip memory, etc.) [69], reconfigurability [70] and massive performance in the development of algorithms [71]. This subsection provides a brief explanation about the main Spanish approaches in the use of FPGAs for signal processing in sensor systems.

(p8.1) In most cases, FPGAs are used for the implementation of sensor data processing. In this context, in [14], the design of WSNs to get the data of a set of pulse oximeters is presented. In this paper, pulse and oxygen values are processed in the FPGA and the obtained values are sent in real time to the Database Server via a WSN. Another contribution to mention is the presented in [72], where some spike-based band-pass filters have been synthesized for FPGA devices.

(p8.2) Low-level processing of ultrasonic signals is another issue which is being implemented with FPGA devices in order to increase scan rate, precision, and reliability [15,20,73,74]. In this context, using Time-Of-Flight (TOF) measurements given by the transducers, some drawbacks such as cross-talk problems, specular reflection and echo discrimination can arise and generate errors in the distance computation. In order to solve these problems, multimode techniques such as Golay sequences [15] are employed. The implementation of this algorithm in an FPGA device permits adaptation to the distance of the reflector in the environment, simultaneous emissions and simultaneous reception in all transducers being able to discriminate the emitter of the echo.

LLM judge: YES

How have FPGAs advanced the field of embedded signal processing in sensor systems, surpassing DSPs in efficiency and addressing specific challenges in sensor data processing?

## (s11) Low-Level Vision Tasks
Number of References: 24

(p11.0) FPGAs are ideal for image processing, particularly for low-level and mid-level tasks where parallelism is exploited [9]. Most of the works found in the literature related to computer vision and FPGAs describe a parallelism version of a classical sequential computer vision algorithm [9,10]. For a pipelined architecture, a different hardware block is built for each image processing operation. The block implementing the image processing operation passes its processed data to the next block, which performs a different operation. When the system is not synchronous, intermediate buffers between operations are required. These buffers handle the variations in the data flow. As stated before, building multiple copies of implemented operations and assigning different partitions of the image to each copy can exploit spatial parallelism. A full spatial parallelism can be achieved by building a processor for each pixel. In practice, high image resolution of modern cameras makes this unlikely.

(p11.1) Logical parallelism is the overall parallelism contained in a program, i.e., all the computations that may, according to the semantics of the programming language, be executed in parallel. The logical parallelism within an image processing operation fits into an implementation on the FPGA. This is where most of the image processing algorithms can significantly improve performance. To do so, inner loops are unrolled. Thus, operations are performed in parallel hardware instead of sequentially. Figure 3 depicts a scheme of a low-level to mid-level vision task implemented over an FPGA. Parallel skills have effects on the construction of the vision system [9]. Implementing a pipelined architecture in an FPGA permits operating at the same frequency pixels are served. Given that power consumption is directly related to the clock frequency, a lower frequency implies a lower power demand by the system. The vision task described in Figure 3 is a typical FPGA approximation to an image processing task.  Normally, the image data goes serially, which fits perfectly in a hardware implementation, especially if it is possible to interface directly to the camera. Anyway, a block (represented in Figure 3 as an I/O interface directly connected to the camera) performs the communication with the camera to receive the flow of pixels from the sensor. This block is responsible for implementing the required protocol (I2C, Camera link, etc.) to communicate with the capture device, configure it and get the image stream. Once configured and initiated the transmission of data, the flow of pixels is driven into the basic image processing block (Point operations green block in Figure 3). This block represents a low-level vision task block. Point operations have widely used in terms of contrast enhancement, segmentation, color filtering, change detection, masking and many other applications. These operations contain the peculiarity that the output pixel depends only on the value of the input's pixel. Figure 4 depicts an example of this kind of module, where a simple contrast enhancement operation to the input image is performed. The constants a and b with two simple math operations over input pixel value provide a new luminance value. This value may exceed the range of representable values. Thus, the result must be clipped. In Figure 4, this clipping operation is performed over the output value. Operating with the input value may improve the performance in a parallel scheme because both, math operations and logic comparisons can be processed concurrently with two processors. The result of this module can be stored in some kind of device memory (DDR2 RAM in Figure 3). This last step is not strictly necessary. A buffer storage is required only for system synchronization. Point operations are just the basic low-level vision tasks. Normally, from the enhanced image obtained by a point operation module, the computer vision system performs other low-level operations like an image average filter. Filters or blob tracking operations have in common that they need more information besides the value of the pixel being processed. To do so, providing with the necessary architecture to obtain such information (vector structures, intermediate buffers, etc.) is essential. Figure 5 shows some iterations of an image filter computed in an FPGA. On the left the input image is represented for each iteration. The red grid remarks the convolution mask employed to compute the central point in the correspondent iteration, whereas row buffers are depicted in a darker blue and green. Row buffers values are also shown in the right scheme of each iteration. The window mask buffers are represented in orange. Row buffers and window buffers are updated by iterating over the image stream. Parallelism is exploited thanks to these buffers. From window buffer, the simple average can be computed at each iteration. The outcome is a valid pixel value of the output filtered image.  One of the basic low-level vision tasks is an image convolution. This was also one of the first processing image issues to be implemented in an FPGA [6,33,85]. In this work, images provided by a high-resolution sensor were passed to the FPGA. Then, the program embedded on the FPGA applied a convolution with a mask over the image and afterwards transmitted that preprocessed image to a PC. Recently, this basic operation was used to obtain object's edges of an image provided by a spiking system [86]. A spike system also called Address-Event-Representation systems (AER) is a camera sensor that computes internally the movement of the objects in the scene. When a pixel changes its luminance, an event is generated and this is the information transmitted by the camera to the computer vision system. In this paper, Linares-Barranco et al. present two FPGA implementations of AER-based convolution processors. In [87] a design based of FPGA device is described, used in spiking systems for real time image processing. In this case, the AER device described is a synthetic AER retina emulator, used to simulate spiking retina behavior getting as video source a standard video composite source. This design has been synthesized into synchronous and asynchronous FPGA devices to compare their capabilities. Another project related to AER sensors that uses an FPGA is the described in [45]. In this project, the FPGA can perform five different functions: turn a sequence of frames into AER in real time; histogram AER events into sequences of frames in real time; remap addresses using lookup tables; capture and time-stamp events for offline analysis; and reproduce time-stamped sequences of events in real time. In [88] an FPGA is used to develop a real-time high-definition Bayer to RGB converter. Two image processing operations were parallelized in order to obtain this converter: bilinear interpolation and a new median filter scheme that does not require extra memory and is able to work in real time.

(p11.2) Motion estimation represents a highly descriptive visual cue that can be used for applications such as time interpolation of image sequences, video compression, segmentation from motion or tracking. Optical-flow algorithms have been widely employed for motion estimation using FPGAs [32,[89][90][91]. Different approaches to the subject include image block-matching, gradient constraints, phase conservation, and energy models. In [55] Botella et al. present a work developed over a Xilinx board that performs two low-level vision tasks: gradient family optical flow estimation and variant orthogonal moments. These two blocks are then used for a mid-level task (tracking). The system described in [2,32] shows how an optical flow estimation circuit can be implemented using an FPGA platform to achieve real-time computation. The difference in this proposal lies in the fact that authors implement a classical gradient Lucas and Kanade model [92]. They compare different optical flow estimation methods to evaluate the performance of the system.

(p11.3) Adaptive fovea imagers define non-concentric reconfigurable structures for rectangular fields of view. Following procedures used in vision pyramids, from the uniform resolution images supplied by the camera, the upper levels are computed progressively reducing resolution and data volume. In [93,94] adaptive fovea imagers are implemented in an FPGA. Each pixel of the full resolution image is averaged in a low-level vision task. Another interesting image processing application where an FPGA increased the performance is in on-line fingerprint matching [95]. In [96] the FPGA implementation of the structural analysis algorithm consists of a finite state machine core block responsible for managing the neighbourhood analysis. In order to accelerate the computation of distances and angles among minutia points a CORDIC coprocessor is implemented. CORDIC is commonly used when no hardware multiplier is available since the only operations it requires are addition, subtraction, bitshift and table lookup [97].

LLM judge: YES

How do FPGAs enhance image processing performance in computer vision systems, and what are some applications of this technology?

