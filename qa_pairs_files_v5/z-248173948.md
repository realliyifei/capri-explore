# A Comprehensive Review of Various Diabetic Prediction Models: A Literature Survey

CorpusID: 248173948 - [https://www.semanticscholar.org/paper/bf1a3b9a295dc31c9d71cad4ab29ca115415f037](https://www.semanticscholar.org/paper/bf1a3b9a295dc31c9d71cad4ab29ca115415f037)

Fields: Computer Science, Medicine

## (s0) Introduction
Number of References: 18

(p0.0) Diabetes is a disease that is caused due to excessive amount of blood sugar in it. Our body needs energy, and glucose is one of the main sources of energy to build the muscles and tissues of the body. Generally, unhealthy lifestyle and lack of exercise are the main causes of type 2 diabetes in people. e presence of a large amount of sugar in the blood causes diabetes. Sometimes, the pancreas is unable to convert the food into insulin; thus, sugar remains unabsorbed, which causes diabetes. Diabetes can affect kidney, eyes, nervous system, blood vessels, and so on. Diabetes is of three types. First is juvenile diabetes [1], which occurs mostly in children and destroys the cells which produce insulin in the pancreas. Second is type 2 diabetes, which generally happens after the age of 40 because of lack of exercise and unhealthy lifestyle. Diabetes is a type of disease that cannot be reversed but can be controlled with the help of medications, regular walk and exercise, and a proper diet. Type 2 diabetes [2] is also known as insulin-independent diabetes since patients are not injected with insulin after a gap of regular intervals, but in the case of type 1 diabetes, insulin is injected at a regular interval of time to the patient, so this is also known as insulin-dependent diabetes. e third type of diabetes [3] is gestations, which occurs during pregnancy due to the change of hormones, and this generally disappears after the delivery. ere is one more condition, that is, prediabetes, in which the intake levels of sugar are on the borderline, and this condition can be reversed with the help of regular exercise and healthy lifestyle. In this paper, we have tried to predict diabetes using machine learning. Machine learning is a branch of artificial intelligence in which the machine tries to predict the outcome based on certain data and previous outcomes. Machine learning is of two types. First is supervised learning, in which data act as a teacher and the model is built around the dataset. Second is unsupervised learning, in which data trains itself by finding certain patterns in the dataset and labeling them. In recent years, many authors have published and presented their work on diabetes prediction by using machine learning algorithms. In this paper, we have studied various diabetes prediction methods using machine learning and presented a comparative study of a few methods in our paper. e objectives of our study are as follows:

(p0.1) (1) To enrich ourselves with the various diabetic prediction models. (2) To evaluate and discuss the existing models based on classification accuracy. (3) To discuss the various attributes required for the prediction of diabetes. (4) To identify the research gaps in the existing literature. (5) To present a comparative study of various diabetic prediction models. (6) To collect more and more information about the prediction of diabetes in the primitive stage.
## (s4) Quan Zou's Method.
Number of References: 3

(p4.0) Quan Zou worked simultaneously on two datasets. One dataset is the Pima Indians Diabetes Dataset, and another dataset is from a local Hospital in Luzhou, China, which contains 14 attributes and approximately 68994 patient's data. e authors employed a twophase detection method where the dataset was trained and two feature selection methods, namely, principal component analysis, minimum redundancy, and maximum relevance. ey used three classifiers, that is, decision tree J48, random forest, and neural network. Decision tree classifier and random forest were run on Weka 3.9.4 to evaluate the prediction result while neural network model was implemented using MATLAB [16]. A fivefold cross-validation technique was employed by the authors to train and test each value.
## (s5) Nishith Kumar's Method.
Number of References: 9

(p5.0) In this paper, the authors have assumed the medical data to be inherently structured, nonnormal, and nonlinear and therefore made use of three kernel-based Gaussian process classification against naïve Bayes, linear discriminant analysis, and quadratic discriminant analysis. ree kernels are linear, polynomial, and radial basis kernel [26], and then a comparative analysis of three kernels in the GPC and then the GPC is compared against naïve Bayes, LDA, and QDA. Evaluation parameters taken by the authors are sensitivity, specificity, accuracy, positive predictive value, negative predictive value, and receiver operating characteristics. Analysis of three types of kernels for a Gaussian process model has been done using Laplace approximation. A generalization of the logistic function is Gaussian process classification, and the authors have used the activation function as logistic regression [39]. Since there is no noise in the Gaussian process, it can be combined with an activation function; in this case, the authors have used the activation function as logistic regression. But it is too difficult to calculate the likelihood function using a logistic function. For this, the Laplace approximation to solve the binary class problem of the Gaussian process [24] has been used by the author. For binary class problems, a sigmoid function is used in the study, which is defined in equation (3) and t is the variable for which function is being computed.
## (s6) Maniruzzaman's Method.
Number of References: 99

(p6.0) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p6.1) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p6.2) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.

(p6.3) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p6.4) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p6.5) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.

(p6.6) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p6.7) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p6.8) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.
## (s7) V. Jackins Method.
Number of References: 54

(p7.0) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p7.1) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.

(p7.2) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p7.3) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.

(p7.4) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p7.5) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.
## (s9) Saumendra Mohapatra's Method.
Number of References: 6

(p9.0) In this article, the authors have made use of only one classification algorithm, that is, multilayer perceptron, to detect diabetes at an early stage. Multilayer perceptron uses backward propagation method for classification of diseases. e authors have done preprocessing of data as a first step in the classification process. Pima Indians Diabetes Dataset has been used for classification and testing purposes. Division of the dataset is done in a manner that 70% of the dataset is used for training purposes and 30% for testing. Multilayer perceptron [16] makes reorganizing patterns for the classification of inputs and prediction of the stated problem. Before the dataset has been trained by the author, some random weights are used, and neurons of the neural network learn from the training dataset. e authors have left the missing value as missing only, and they were not replaced by any mean or median. 550 rows were used for training, and 218 were used for testing purposes. e authors have trained the network using the training data from the Pima Indians Diabetes Dataset. Eight input layers and four hidden layers were fed to the training network. en, the testing and validation of the model is followed after training. Multilayer perceptron [45] is applied to the testing dataset, and then the authors measure the accuracy of the machine learning algorithm. Accuracy is calculated in (6) using the following formula:
## (s11) M Orabi's Method.
Number of References: 3

(p11.0) In this paper, the authors have made use of regression prediction to predict whether or not a person could be a candidate for having diabetes and at what age. To randomize the task of testing [49] and training, a rotation mechanism has been used by the author. e average of each iteration is calculated for comparison purposes. e dataset used by the authors was from the Egyptian National Research Centre. A questionnaire was prepared, which consisted of several questions for prediction purposes, and then the data was extracted using the SPSS tool and then imported into Excel sheets. e dataset contains 23 features which are age, gender, education, diabetic family member, smoker, cigarette number, exercising status, frequent exercise per week, exercise type, food type, healthy food status, number of basic meals, snacks status, snacks number, snacks type, regime status, blood pressure status, blood fat status, foot complications, neurocomplications, low vision status, and wound recovery status. e authors have preprocessed the dataset by cleaning, data reduction, and normalization of the dataset.
## (s12) O.M. Alade's Method.
Number of References: 36

(p12.0) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.

(p12.1) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.

(p12.2) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.
## (s16) Conclusion
Number of References: 9

(p16.0) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p16.1) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p16.2) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.
## (s19) Introduction
Number of References: 18

(p19.0) Diabetes is a disease that is caused due to excessive amount of blood sugar in it. Our body needs energy, and glucose is one of the main sources of energy to build the muscles and tissues of the body. Generally, unhealthy lifestyle and lack of exercise are the main causes of type 2 diabetes in people. e presence of a large amount of sugar in the blood causes diabetes. Sometimes, the pancreas is unable to convert the food into insulin; thus, sugar remains unabsorbed, which causes diabetes. Diabetes can affect kidney, eyes, nervous system, blood vessels, and so on. Diabetes is of three types. First is juvenile diabetes [1], which occurs mostly in children and destroys the cells which produce insulin in the pancreas. Second is type 2 diabetes, which generally happens after the age of 40 because of lack of exercise and unhealthy lifestyle. Diabetes is a type of disease that cannot be reversed but can be controlled with the help of medications, regular walk and exercise, and a proper diet. Type 2 diabetes [2] is also known as insulin-independent diabetes since patients are not injected with insulin after a gap of regular intervals, but in the case of type 1 diabetes, insulin is injected at a regular interval of time to the patient, so this is also known as insulin-dependent diabetes. e third type of diabetes [3] is gestations, which occurs during pregnancy due to the change of hormones, and this generally disappears after the delivery. ere is one more condition, that is, prediabetes, in which the intake levels of sugar are on the borderline, and this condition can be reversed with the help of regular exercise and healthy lifestyle. In this paper, we have tried to predict diabetes using machine learning. Machine learning is a branch of artificial intelligence in which the machine tries to predict the outcome based on certain data and previous outcomes. Machine learning is of two types. First is supervised learning, in which data act as a teacher and the model is built around the dataset. Second is unsupervised learning, in which data trains itself by finding certain patterns in the dataset and labeling them. In recent years, many authors have published and presented their work on diabetes prediction by using machine learning algorithms. In this paper, we have studied various diabetes prediction methods using machine learning and presented a comparative study of a few methods in our paper. e objectives of our study are as follows:

(p19.1) (1) To enrich ourselves with the various diabetic prediction models. (2) To evaluate and discuss the existing models based on classification accuracy. (3) To discuss the various attributes required for the prediction of diabetes. (4) To identify the research gaps in the existing literature. (5) To present a comparative study of various diabetic prediction models. (6) To collect more and more information about the prediction of diabetes in the primitive stage.
## (s23) Quan Zou's Method.
Number of References: 3

(p23.0) Quan Zou worked simultaneously on two datasets. One dataset is the Pima Indians Diabetes Dataset, and another dataset is from a local Hospital in Luzhou, China, which contains 14 attributes and approximately 68994 patient's data. e authors employed a twophase detection method where the dataset was trained and two feature selection methods, namely, principal component analysis, minimum redundancy, and maximum relevance. ey used three classifiers, that is, decision tree J48, random forest, and neural network. Decision tree classifier and random forest were run on Weka 3.9.4 to evaluate the prediction result while neural network model was implemented using MATLAB [16]. A fivefold cross-validation technique was employed by the authors to train and test each value.
## (s24) Nishith Kumar's Method.
Number of References: 9

(p24.0) In this paper, the authors have assumed the medical data to be inherently structured, nonnormal, and nonlinear and therefore made use of three kernel-based Gaussian process classification against naïve Bayes, linear discriminant analysis, and quadratic discriminant analysis. ree kernels are linear, polynomial, and radial basis kernel [26], and then a comparative analysis of three kernels in the GPC and then the GPC is compared against naïve Bayes, LDA, and QDA. Evaluation parameters taken by the authors are sensitivity, specificity, accuracy, positive predictive value, negative predictive value, and receiver operating characteristics. Analysis of three types of kernels for a Gaussian process model has been done using Laplace approximation. A generalization of the logistic function is Gaussian process classification, and the authors have used the activation function as logistic regression [39]. Since there is no noise in the Gaussian process, it can be combined with an activation function; in this case, the authors have used the activation function as logistic regression. But it is too difficult to calculate the likelihood function using a logistic function. For this, the Laplace approximation to solve the binary class problem of the Gaussian process [24] has been used by the author. For binary class problems, a sigmoid function is used in the study, which is defined in equation (3) and t is the variable for which function is being computed.
## (s25) Maniruzzaman's Method.
Number of References: 99

(p25.0) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p25.1) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p25.2) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.

(p25.3) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p25.4) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p25.5) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.

(p25.6) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p25.7) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p25.8) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.
## (s26) V. Jackins Method.
Number of References: 54

(p26.0) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p26.1) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.

(p26.2) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p26.3) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.

(p26.4) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p26.5) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.
## (s28) Saumendra Mohapatra's Method.
Number of References: 6

(p28.0) In this article, the authors have made use of only one classification algorithm, that is, multilayer perceptron, to detect diabetes at an early stage. Multilayer perceptron uses backward propagation method for classification of diseases. e authors have done preprocessing of data as a first step in the classification process. Pima Indians Diabetes Dataset has been used for classification and testing purposes. Division of the dataset is done in a manner that 70% of the dataset is used for training purposes and 30% for testing. Multilayer perceptron [16] makes reorganizing patterns for the classification of inputs and prediction of the stated problem. Before the dataset has been trained by the author, some random weights are used, and neurons of the neural network learn from the training dataset. e authors have left the missing value as missing only, and they were not replaced by any mean or median. 550 rows were used for training, and 218 were used for testing purposes. e authors have trained the network using the training data from the Pima Indians Diabetes Dataset. Eight input layers and four hidden layers were fed to the training network. en, the testing and validation of the model is followed after training. Multilayer perceptron [45] is applied to the testing dataset, and then the authors measure the accuracy of the machine learning algorithm. Accuracy is calculated in (6) using the following formula:
## (s30) M Orabi's Method.
Number of References: 3

(p30.0) In this paper, the authors have made use of regression prediction to predict whether or not a person could be a candidate for having diabetes and at what age. To randomize the task of testing [49] and training, a rotation mechanism has been used by the author. e average of each iteration is calculated for comparison purposes. e dataset used by the authors was from the Egyptian National Research Centre. A questionnaire was prepared, which consisted of several questions for prediction purposes, and then the data was extracted using the SPSS tool and then imported into Excel sheets. e dataset contains 23 features which are age, gender, education, diabetic family member, smoker, cigarette number, exercising status, frequent exercise per week, exercise type, food type, healthy food status, number of basic meals, snacks status, snacks number, snacks type, regime status, blood pressure status, blood fat status, foot complications, neurocomplications, low vision status, and wound recovery status. e authors have preprocessed the dataset by cleaning, data reduction, and normalization of the dataset.
## (s31) O.M. Alade's Method.
Number of References: 36

(p31.0) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.

(p31.1) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.

(p31.2) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.
## (s35) Conclusion
Number of References: 9

(p35.0) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p35.1) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p35.2) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.
## (s38) Introduction
Number of References: 18

(p38.0) Diabetes is a disease that is caused due to excessive amount of blood sugar in it. Our body needs energy, and glucose is one of the main sources of energy to build the muscles and tissues of the body. Generally, unhealthy lifestyle and lack of exercise are the main causes of type 2 diabetes in people. e presence of a large amount of sugar in the blood causes diabetes. Sometimes, the pancreas is unable to convert the food into insulin; thus, sugar remains unabsorbed, which causes diabetes. Diabetes can affect kidney, eyes, nervous system, blood vessels, and so on. Diabetes is of three types. First is juvenile diabetes [1], which occurs mostly in children and destroys the cells which produce insulin in the pancreas. Second is type 2 diabetes, which generally happens after the age of 40 because of lack of exercise and unhealthy lifestyle. Diabetes is a type of disease that cannot be reversed but can be controlled with the help of medications, regular walk and exercise, and a proper diet. Type 2 diabetes [2] is also known as insulin-independent diabetes since patients are not injected with insulin after a gap of regular intervals, but in the case of type 1 diabetes, insulin is injected at a regular interval of time to the patient, so this is also known as insulin-dependent diabetes. e third type of diabetes [3] is gestations, which occurs during pregnancy due to the change of hormones, and this generally disappears after the delivery. ere is one more condition, that is, prediabetes, in which the intake levels of sugar are on the borderline, and this condition can be reversed with the help of regular exercise and healthy lifestyle. In this paper, we have tried to predict diabetes using machine learning. Machine learning is a branch of artificial intelligence in which the machine tries to predict the outcome based on certain data and previous outcomes. Machine learning is of two types. First is supervised learning, in which data act as a teacher and the model is built around the dataset. Second is unsupervised learning, in which data trains itself by finding certain patterns in the dataset and labeling them. In recent years, many authors have published and presented their work on diabetes prediction by using machine learning algorithms. In this paper, we have studied various diabetes prediction methods using machine learning and presented a comparative study of a few methods in our paper. e objectives of our study are as follows:

(p38.1) (1) To enrich ourselves with the various diabetic prediction models. (2) To evaluate and discuss the existing models based on classification accuracy. (3) To discuss the various attributes required for the prediction of diabetes. (4) To identify the research gaps in the existing literature. (5) To present a comparative study of various diabetic prediction models. (6) To collect more and more information about the prediction of diabetes in the primitive stage.
## (s42) Quan Zou's Method.
Number of References: 3

(p42.0) Quan Zou worked simultaneously on two datasets. One dataset is the Pima Indians Diabetes Dataset, and another dataset is from a local Hospital in Luzhou, China, which contains 14 attributes and approximately 68994 patient's data. e authors employed a twophase detection method where the dataset was trained and two feature selection methods, namely, principal component analysis, minimum redundancy, and maximum relevance. ey used three classifiers, that is, decision tree J48, random forest, and neural network. Decision tree classifier and random forest were run on Weka 3.9.4 to evaluate the prediction result while neural network model was implemented using MATLAB [16]. A fivefold cross-validation technique was employed by the authors to train and test each value.
## (s43) Nishith Kumar's Method.
Number of References: 9

(p43.0) In this paper, the authors have assumed the medical data to be inherently structured, nonnormal, and nonlinear and therefore made use of three kernel-based Gaussian process classification against naïve Bayes, linear discriminant analysis, and quadratic discriminant analysis. ree kernels are linear, polynomial, and radial basis kernel [26], and then a comparative analysis of three kernels in the GPC and then the GPC is compared against naïve Bayes, LDA, and QDA. Evaluation parameters taken by the authors are sensitivity, specificity, accuracy, positive predictive value, negative predictive value, and receiver operating characteristics. Analysis of three types of kernels for a Gaussian process model has been done using Laplace approximation. A generalization of the logistic function is Gaussian process classification, and the authors have used the activation function as logistic regression [39]. Since there is no noise in the Gaussian process, it can be combined with an activation function; in this case, the authors have used the activation function as logistic regression. But it is too difficult to calculate the likelihood function using a logistic function. For this, the Laplace approximation to solve the binary class problem of the Gaussian process [24] has been used by the author. For binary class problems, a sigmoid function is used in the study, which is defined in equation (3) and t is the variable for which function is being computed.
## (s44) Maniruzzaman's Method.
Number of References: 99

(p44.0) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p44.1) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p44.2) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.

(p44.3) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p44.4) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p44.5) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.

(p44.6) In this article, the authors adopted four machine learning algorithms, that is, random forest [20], AdaBoost [23], naïve Bayes [11], and decision tree [13] on NHANES (National Health and Nutrition examination survey) dataset. is dataset contains 9858 patient's data, amongst which 760 are diabetic and 9098 are nondiabetic.

(p44.7) ere were certain missing values in the dataset, which the authors dropped from the dataset as a cleaning process. After dropping the missing values, there were 6561 respondents, amongst which 5904 were nondiabetic and 657 were diabetic. e authors selected the important features from the dataset by making use of the logistic regression model, P value, and odds ratio [40]. e probability of response is calculated from logistic regression [28] by making use of one or more predictions. e relationship between predictor and response is being measured through a logistic regression model, and a logit function is estimated. e equation of a logit function is defined as follows:

(p44.8) where P j is the probability of a patient being diabetic, 1 − P j is the probability of being a nondiabetic, i � 0, 1, 2, . . ., k, unknown regression coefficient, and k is the total number of predictors or attributes (in this case, 14). Using these regression coefficients in (4), the authors have found the P value and odds ratio of each of the features. P value is calculated from t-test for continuous variables and chi-square [31] test for discrete variables. Data is analysed by the authors through Stata version 14.10. Only those features are selected from the dataset by the authors whose P value is less than 0.005. e next step implemented by the authors was the splitting of the dataset into training and validation set. A 10-fold cross-validation model [29] is used in which the dataset is divided into ten equal parts where the nine parts are used as a training set, and the remaining one is used as a validation or testing set. is entire process is repeated 20 times, and the classification accuracy is calculated at each step. en, an average of all classification accuracy is taken by the author. After selecting the significant features from the dataset and following a cross-validation model, the authors have applied four classifiers to the dataset, that is, naïve Bayes, random forest, decision tree, and AdaBoost. Evaluation parameters taken by the authors are accuracy, positive predictive value [24], negative predictive value [27], and f-measure. After conducting a few experiments, the accuracy achieved for K2, K5, and K10 models was 92.54%, 92.33%, and 92.75, respectively. e authors concluded that the best accuracy was given by random forest along with the logistic regression feature selection method.
## (s45) V. Jackins Method.
Number of References: 54

(p45.0) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p45.1) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.

(p45.2) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p45.3) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.

(p45.4) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.

(p45.5) e authors have checked to see if any categorical data in the form of true or false is present. If present, true is replaced by 1 and false by 0. When the missing values are filled by null values, the authors made use of the correlation coefficient [41], which can effectively measure the amount of corelationship between two variables. When the two attributes of the dataset are highly correlated, one of the attributes can be neglected so as to avoid repetition. After the calculation of the correlation of different attributes of the dataset, the authors created a correlogram matrix [13]. In the matrix, the blue colour represents positive coefficients while the red colour represents negative coefficients. e correlation coefficient and intensity of the colours are highly proportional. e range of the correlation coefficient is from −1 to +1. Value of +1 correlation coefficient indicates perfect positive correlation coefficient, and −1 indicates perfect negative correlation coefficient. After the calculation of the correlation coefficient, a confusion matrix is created by the author. Evaluation parameters on which the diabetes is predicted are accuracy, precision, recall, and f1-score. e accuracy achieved by the authors for the naïve Bayes algorithm is 76.72 and 74.46 for training and testing data, respectively, while for the random forest, it is 98.88 and 74.03 for training and testing data. e method applied by the authors consists of applying preprocessing techniques [42] by replacing missing values with null values and deleting those attributes from the dataset, which are highly correlated to each other. Classification algorithm random forest and naïve Bayes [11] are applied to the preprocessed dataset, and efficiency is calculated on the basis of accuracy. e performance of the algorithm proposed by the authors is compared with density-based spatial clustering of applications with noise (DBSCAN) algorithm and k-means clustering algorithm to measure the effectiveness of the algorithm. After comparison, the authors found the proposed algorithm to be better than K-means and DBSCAN. e main disadvantage of the method proposed by Jackins is the processing time, as a large amount of the data is taken for training and testing purposes. e advantage of the method is that it helps in diagnosing the disease with more accuracy.
## (s47) Saumendra Mohapatra's Method.
Number of References: 6

(p47.0) In this article, the authors have made use of only one classification algorithm, that is, multilayer perceptron, to detect diabetes at an early stage. Multilayer perceptron uses backward propagation method for classification of diseases. e authors have done preprocessing of data as a first step in the classification process. Pima Indians Diabetes Dataset has been used for classification and testing purposes. Division of the dataset is done in a manner that 70% of the dataset is used for training purposes and 30% for testing. Multilayer perceptron [16] makes reorganizing patterns for the classification of inputs and prediction of the stated problem. Before the dataset has been trained by the author, some random weights are used, and neurons of the neural network learn from the training dataset. e authors have left the missing value as missing only, and they were not replaced by any mean or median. 550 rows were used for training, and 218 were used for testing purposes. e authors have trained the network using the training data from the Pima Indians Diabetes Dataset. Eight input layers and four hidden layers were fed to the training network. en, the testing and validation of the model is followed after training. Multilayer perceptron [45] is applied to the testing dataset, and then the authors measure the accuracy of the machine learning algorithm. Accuracy is calculated in (6) using the following formula:
## (s49) M Orabi's Method.
Number of References: 3

(p49.0) In this paper, the authors have made use of regression prediction to predict whether or not a person could be a candidate for having diabetes and at what age. To randomize the task of testing [49] and training, a rotation mechanism has been used by the author. e average of each iteration is calculated for comparison purposes. e dataset used by the authors was from the Egyptian National Research Centre. A questionnaire was prepared, which consisted of several questions for prediction purposes, and then the data was extracted using the SPSS tool and then imported into Excel sheets. e dataset contains 23 features which are age, gender, education, diabetic family member, smoker, cigarette number, exercising status, frequent exercise per week, exercise type, food type, healthy food status, number of basic meals, snacks status, snacks number, snacks type, regime status, blood pressure status, blood fat status, foot complications, neurocomplications, low vision status, and wound recovery status. e authors have preprocessed the dataset by cleaning, data reduction, and normalization of the dataset.
## (s50) O.M. Alade's Method.
Number of References: 36

(p50.0) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.

(p50.1) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.

(p50.2) In this paper, the authors have worked on the Pima Indians Diabetes Dataset and made use of an artificial neural network [21] on the dataset for predicting diabetes. e authors have designed a neural network using the Bayesian algorithm and backpropagation method, and the neural network consists of four layers. To avoid any overfitting in the dataset, they made use of the Bayesian regulation algorithm [50], and for training purposes, the backpropagation [51] method was used. 70% of the dataset is used for training purposes, 15% for testing, and the remaining 15% for validation. Dataset is trained using MATLAB software, and the data is trained until a single and accurate output is displayed using regression graphs. e dataset contains 8 attributes; these 8 attributes form the 8 neurons of the input layer. Eight input neurons are the number of times a woman is pregnant, body mass index, diabetes pedigree function [9], age, plasma glucose concentration, blood pressure, insulin, and skin thickness. en, two hidden layers with ten neurons are used. A hidden layer is a layer that receives input from the input layer and forwards output to the output layer. e output layer represents the results, and there is only one neuron present in the output layer. If the neuron in the output layer has a value equal to or greater than 0.5, the person is suffering from diabetes mellitus, otherwise not. After making use of the neural network to predict diabetes, a web-based graphical user interface was developed by the authors using JavaScript and NodeJS, where a user can enter their basic details and get to know whether they are suffering from disease or not.
## (s54) Conclusion
Number of References: 9

(p54.0) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p54.1) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p54.2) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.
