# Opportunities and challenges in machine learning-based newborn screening-A systematic literature review

CorpusID: 247687025 - [https://www.semanticscholar.org/paper/f7024b07482d7d1bec34d18ab14f68afc3e7c8f9](https://www.semanticscholar.org/paper/f7024b07482d7d1bec34d18ab14f68afc3e7c8f9)

Fields: Computer Science, Medicine

## (s0) | INTRODUCTION
Number of References: 10

(p0.0) For more than 50 years, newborn screening (NBS) programs aim at early, ideally presymptomatic, identification of treatable rare diseases with significant health burden to reduce morbidity and mortality. With the introduction of tandem mass spectrometry (MS/MS) 1,2 and recently genetic methods, NBS panels expanded worldwide 3,4 and include many inherited metabolic diseases as well as endocrine, hematological, immune and neurological disorders, and cystic fibrosis. NBS programs refer to the screening principles of Wilson and Jungner 5 which demand a very high sensitivity (ideally 100%) to avoid false negatives and very high specificity (at least 99.5%) to keep the number of false positives low. This is especially challenging in NBS because birth prevalences of the target diseases are very low (1:10 000-<1:1 000 000). 6 Traditional cut-offbased approaches in NBS integrate only a fraction of the available information and focus on the primary variables of the metabolic pathway affected in a particular metabolic disease. Here, laboratory physicians are needed to evaluate these findings and workload directly depends on the number of false positives. Moreover, cut-off-based methods cannot deal with complex relationships among metabolites. 7 To improve the diagnostic specificity of NBS programs an increasing number of second and multiple tier strategies have been developed combining different biochemical 8,9 as well as biochemical and genetic methods. 10,11 In contrast to these analytical improvements, mathematical-based methods are still rarely used to exploit the complete information of NBS test results to improve specificity and positive prediction of NBS. Thanks to advances in data mining and machine learning (ML) as well as the computing landscape in recent years, new opportunities have been created to examine large datasets with high dimensional feature spaces by implementing a ML pipeline for NBS ( Figure 1). ML-based NBS aims at building a classification model, which is part of the essential classification models module to predict the outcome of unknown test data and reduce the number of false positive classifications. The high data imbalance caused by the low birth prevalences of the target diseases makes this task very challenging. Thus, often data preprocessing methods such as data sampling, feature construction, and feature selection are applied before classification. 12 Furthermore, pattern recognition techniques help to detect hidden metabolic interactions within the data. 13 Hence, the goal of this systematic literature review is to present and evaluate current approaches of ML-based NBS, to find an overall consensus on its applicability, and to provide future research directions.
## (s13) | Data imbalance
Number of References: 7

(p13.0) Common methods to overcome data imbalance are sampling methods, which either increase (oversampling) or decrease (undersampling) the data 31 ( Figure 3). In NBS, informed sampling is applied to include special subsets of healthy patients. The inclusion is mainly based on clinical criteria such as healthy patients with elevated primary markers, 12 particularly removing samples close to the decision boundary, 7 one-sided selection, 7 or healthy patients with varying birth weight and gestational age. 27 Other inclusion criteria are based on Tomek links and edited nearest neighbors. 7 Random sampling is applied to change the data imbalance to ratios, for instance, between 1:4 18 and 1:25 14 by randomly excluding data points. In contrast, oversampling methods are applied rarely and create synthetic data samples from the minority class by applying randomness or cluster-based methods such as synthetic minority oversampling technique (SMOTE) 7 and Borderline-SMOTE. 7 Furthermore, spiked blood samples which are designed to resemble sick blood samples are added to enrich the datasets 19 and mixed models such as SMOTE + ENN 7 were applied. For studies that applied ML in second tier analysis, the data was less imbalanced since it only contained false positive screening results from the first tier. 25 
## (s26) | Preprocessing methods
Number of References: 5

(p26.0) Sampling methods are a promising approach to handle the data imbalance in NBS. However, oversampling methods could pose a problem since it cannot be verified whether the synthetically created samples correspond to a positive confirmation diagnosis. Moreover, sampling methods artificially change the sick-to-control ratio of a patient dataset, which could change the model's accuracy on a real population. 13,19 Hence, sampling methods should be chosen carefully and evaluated on real populations to verify performance measures in real settings.

(p26.1) Feature selection is applied to support the classification method by identifying relevant features. When deciding which method to choose, several criteria have to be taken into consideration. Prealgorithm methods are independent of the classification method and its respective computational costs. However, they do not take into account the biases of the classifiers which can be problematic when classification methods are highly sensitive to the feature selection procedure. 36 In contrast, postalgorithm methods depend on the specific biases and heuristics of the classification method. This can make them computationally more expensive, as wrapper methods for instance iterate through subsets of all features. Wrapper methods such as mean decrease in accuracy can also be used to rank the relative importance of individual features in a random forest model for pattern recognition. 22,25 Furthermore, the applicability for NBS has to be evaluated based on its specific data requirements. NBS has numerical input and categorical output data. However, Ï‡ 2 and mutual information expect a categorical input and Pearson's correlation coefficient expects numerical output values whereas ANOVA expects numerical input and categorical output values, which would be most appropriate for NBS. Informed methods allow to include expert knowledge into the feature selection process which can be beneficial for well-studied diseases but lowers the chances of discovering new metabolic patterns.
## (s28) | Conclusion
Number of References: 2

(p28.0) Through technical advances, ML-based NBS enables new opportunities in reducing false positive rates and identifying so far unknown metabolic patterns by relying on complex feature combinations instead of predefined cutoff values. These mathematical strategies should be regarded as complementary to the combined use of biochemical and genetic tests aiming at improving the diagnostic specificity of NBS programs through second and multiple tier analysis. However, due to the variety of diseases and methods, a general recommendation for a single ML method in NBS is currently not possible. Instead, a thorough analysis of different methods is necessary for all applications. Among the presented, LRA and SVM seem to be valuable candidates for NBS classification since they are often applied, achieve high performance in general and in comparative studies, and handle multidimensional data. Comparing both methods, LRA is interpretable on a modular level, whereas SVM is not and therefore, LRA might be more applicable for NBS. Yet, with the rise of ensemble and deep learning methods, also noninterpretable extensions of these methods such as Ridge-LRA and Bagging-SVM showed promising results. 19,24 In combination with explainable artificial intelligence methods, these noninterpretable methods could be applied more frequently, which will be investigated in comprehensive future studies.
