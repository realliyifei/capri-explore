# Challenges in Deploying Machine Learning: a Survey of Case Studies

CorpusID: 227053929 - [https://www.semanticscholar.org/paper/a178a0bdee7549d87402b6c6128c569109128458](https://www.semanticscholar.org/paper/a178a0bdee7549d87402b6c6128c569109128458)

Fields: Business, Computer Science

## (s2) DATA MANAGEMENT
Number of References: 2

(p2.0) Data is an integral part of any machine learning solution. The overall effectiveness of the solution depends on the training and test data as much as on the algorithm. The process of creating quality datasets is usually the very first stage in any production ML pipeline. Unsurprisingly, practitioners face a range of issues while working with data as reported by Polyzotis et al. [18]. Consequently, this stage consumes time and energy that is often not anticipated beforehand. In this section, we describe issues concerning four steps within data management: data collection, data preprocessing, data augmentation and data analysis. Note that we consider storage infrastructure challenges, such as setting up databases and query engines, beyond the scope of this survey. We refer readers to the survey by Cai et al. [19] for further discussion of big data storage.
## (s6) Data analysis
Number of References: 2

(p6.0) Data needs to be analyzed to uncover potential biases or unexpected distribution shifts in it. Availability of high quality tools is essential for conducting any kind of data analysis. One area that practitioners find particularly challenging in that regard is visualization for data profiling [36]. Data profiling refers to all activities associated with troubleshooting data quality, such as missing values, inconsistent data types and verification of assumptions. Despite obvious relevance to the fields of databases and statistics, there are still too few tools that enable the efficient execution of these data mining tasks. The need for such tools becomes apparent considering that, according to the survey conducted by Microsoft [37], data scientists think data issues are the main reason to doubt the quality of the overall work.
## (s10) Hyper-parameter selection
Number of References: 3

(p10.0) In addition to parameters that are learned during the training process, many ML models also require hyper-parameters. Examples of such hyper-parameters are the depth of a decision tree, the number of hidden layers in a neural network or the number of neighbors in k-Nearest Neighbors classifier. Hyper-parameter optimization (HPO) is the process of choosing the optimal setting of these hyper-parameters. Most HPO techniques involve multiple training cycles of the ML model. This is computationally challenging because in the worst case the size of the HPO task grows exponentially: each new hyper-parameter adds a new dimension to the search space. As discussed by Yang and Shami [57], these considerations make HPO techniques very expensive and resourceheavy in practice, especially for applications of deep learning. Even approaches like Hyperband [58] or Bayesian optimization [59], that are specifically designed to minimize the number of training cycles needed, are not yet able to deal with the high dimensional searches that emerge when many hyper-parameters are involved. Large datasets complicate matters by leading to long training times for each search.
