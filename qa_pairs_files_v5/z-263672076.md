# Towards Unified Deep Image Deraining: A Survey and A New Benchmark

CorpusID: 263672076 - [https://www.semanticscholar.org/paper/1a36e49fa5b01bfa2b67ed85beb040027d3b7965](https://www.semanticscholar.org/paper/1a36e49fa5b01bfa2b67ed85beb040027d3b7965)

Fields: Computer Science

## (s15) GAN-based Network Design.
Number of References: 10

(p15.0) Driven by GAN in the image generation task [98], Qian et al. [28] incorporated a GANbased architecture, where the generative network employs an attentive-recurrent network to generate an attention map.This attention map, along with the input image, is then utilized in a contextual autoencoder to generate a rain-free result.Afterwards, Zhang et al. [48] proposed a conditional GAN-based architecture with a densely-connected generator and a multi-scale discriminator.To achieve heavy rain image restoration, the method of Li et al. [43] consists of a two-stage network architecture: an initial physics-based sub-network followed by a depth-guided GAN refinement sub-network.Based on the consistency between the estimated results of the physical model and the observed image, Pan et al. [61] proposed a GAN-based network constrained by a physics model to remove rain.Motivated by the image disentanglement strategy [105], Ye et al. [71] presented a CycleGAN-based joint rain generation and removal framework by performing the translations on the simpler rain space.Ni et al. [72] put forward a rain intensity controlling GAN, which comprises three sub-networks: a main controlling network, a high-frequency rain-streak elimination network, and a background extraction network, which enables seamless control over rain intensities by leveraging interpolation techniques within the deep feature space.With the popularity of generative models [106], Wang et al. [70] introduced a variational rain generation network to implicitly infer the underlying statistical distribution of rain.
## (s22) Outdoor-Rain
Number of References: 13

(p22.0) Rain13K [18] is the mixed datasets collected from multiple previous datasets, which consists of 13,712 image pairs for training and 4,298 test images.There are five test sets, i.e., TABLE 2: Summarization of public datasets for the single image deraining task."Syn" and "Real" denote the synthetic and real-world rainy datasets."RS", "RD", and "RA" represent the rain streak, raindrop and rain accumulation effect, respectively.Note that DDN-Data [35] and DID-Data [36] are also termed as Rain1400 and Rain1200 in some papers.

(p22.1) Rain100H [8], Rain100L [8], Test100 [5], Test2800 [35], and Test1200 [35].

(p22.2) RainKITTI2012 and RainKITTI2015 [63]  RainDS [29] is divided into two subsets: RainDS-Syn and RainDS-Real.Based on first-person view driving scenarios from autonomous driving datasets, RainDS-Syn is a synthetic dataset made up of 3,600 image pairs corrupted by raindrops and rain streaks.By manually mimicking rainfall with a sprinkler, RainDS-Real is a collection of 750 realworld images degraded by raindrops and rain streaks.

(p22.3) RainDirection [74] contains 2,920 high-resolution synthetic image for training and 430 ones for test, where the clear images are selected from the Flick2K and DIV2K dataset [116].Each rainy image is assigned with a direction label.[16] is the large-scale dataset with real paired data captured diverse rain effects.It contains 31,524 rainy and clear frame pairings, which are divided into 26,124 training frames, 3,300 validation frames, and 2,100 test frames.
