# Deep Learning Approaches to Lexical Simplification: A Survey

CorpusID: 258833368 - [https://www.semanticscholar.org/paper/a9429423352c9b15531d32a43b5cd560519a3214](https://www.semanticscholar.org/paper/a9429423352c9b15531d32a43b5cd560519a3214)

Fields: Linguistics, Computer Science

## (s0) Introduction
Number of References: 2

(p0.0) LS improves the readability of any given text with the aim of helping vocabulary and literacy development. LS achieves this by replacing complex words in a sentence with simpler alternatives. LS returns a simplified sentence which can be passed to a TS system for further syntactic and grammatical simplification. The replaced complex words are those words which a general or targeted population found to be hard to read, interpret, or understand. Previous LS systems have been designed to simplify complex words for children, second language learners, individuals with reading disabilities or low-literacy (Paetzold and Specia, 2017b). LS therefore provides both developers and users with a degree of personalization that is unattainable through seq2seq or generative TS systems (Yeung and Lee and Yeung, 2018a).
## (s3) Deep Learning Approaches
Number of References: 3

(p3.0) Prior to deep learning approaches, lexicon, rulebased, statistical, n-gram, and word embedding models were state-of-the-art for SG, SS, and SR. As previously mentioned, Paetzold and Specia (2017b) have provided a comprehensive survey detailing these approaches, their performances, as well as their impact on LS literature. The following sections provide an extension of the work carried out by Paetzold and Specia (2017b). We introduce new deep learning approaches for LS and begin our survey of the LS pipeline at the SG phase. The recent developments in the CWI step of the pipeline have been extensively surveyed by North et al. (2022b).
## (s12) Discussion and Conclusion
Number of References: 2

(p12.0) Since the 2017 survey on LS (Paetzold and Specia, 2017b), deep learning approaches have provided new headway within the field. MLM is now the go to method for SG, with the majority of recent LS studies having employed a MLM objective. The casual language model: GPT-3, surpasses the performance of all other approaches when subjected to prompt learning, especially when an ensemble of prompts are taken into consideration (Table 3). The prediction scores of MLM or casual language modeling have replaced various SS and SR techniques. LS systems that employ minimal SS and no SR apart from ranking their LLM's prediction scores, have outperformed more technical, featureoriented, and unsupervised ranking methods (Table  3). However, an exception is made with regards to equivalence score (Li et al., 2022), which has been shown to be effective at SR.
