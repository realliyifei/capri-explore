# Purposeful Communication in Human-Robot Collaboration: A Review of Modern Approaches in Manufacturing

CorpusID: 254438398 - [https://www.semanticscholar.org/paper/f5c01e0bc48ee9cd43de2c4ebdec5c618e0ca8f8](https://www.semanticscholar.org/paper/f5c01e0bc48ee9cd43de2c4ebdec5c618e0ca8f8)

Fields: Computer Science, Engineering

## (s8) D. OBJECT HAND-OVER TASK
Number of References: 3

(p8.0) Finally, object hand-over is the last category in which a robot and a human exchange an item in the collaborative framework [21]. Object hand-over is considered a simple task in HRC but requires some cognitive abilities in a robot, since it needs to predict the intention of the human user while performing the task. Human intent could change during the task and the robot must adjust its behavior based on [22]. Object handover tasks could also be part of an assembly task that is required to be completed in several steps (e.g., it includes several object handover steps). In this type of scenario, the robot may need to select the correct object at each step to hand over according to the human user's desire, so the capability of human intent estimation should be added to the framework [23].
## (s11) 1) VERBAL COMMUNICATION
Number of References: 3

(p11.0) Verbal communication is the most straightforward method of explicit communication in HRC. Human and robot could communicate through speech/verbal commands, so that humans give commands to the robot, or the robot replies back to the human user. It is also possible to have bidirectional or two-way communication through speech in HRC; however, there are some challenges in using verbal communication in HRC. For example, it is difficult to establish a foundation only through verbal communication [19]; foundation refers to the fact that speakers understand the messages of others as intended [28]. Moreover, when it comes to time and cognitive resources, verbal communication is considered a costly approach [29].
## (s12) 2) NON-VERBAL COMMUNICATION
Number of References: 6

(p12.0) The vision system is the commonly used non-verbal communication channel through which humans and robots could have both explicit (e.g., gesture, text) and implicit communication (e.g., gaze). In the final list of articles, most research studies have chosen vision as a channel of communication and information exchange [30]. The vision system is used primarily for object detection, human body tracking, or information display. A vision system could be a set of RGBD sensors and an interactive GUI used to perform an assembly task. 3D RGB sensors are used for action recognition and help the robot understand the status of the environment, while the interactive GUI is used to interact with the user [9]. The vision system could monitor the workspace and recognize objects to reach them and move them to a different location [16], [17]. Furthermore, different types of information related to the environment and workspace could be obtained using the vision system, including the position of the assembly part, the position of the user in the environment, the physical characteristics of the environment, and the position and orientation of the robot end effector [11]. Human user behavior could also be sensed through a vision system (e.g., webcam and Kinect RGB-D sensor) in addition to tracking the positions / orientations of the various objects [31].
## (s16) 2) NON-VERBAL COMMUNICATION
Number of References: 6

(p16.0) A display installed on the robot, an interactive GUI, robot gaze, robot hand gesture, and robot body gesture are other ways of building RTH communication [9], [23]. The hand gesture is not limited to being used by the human user; robots could use this approach as a way of communication alongside other techniques such as gaze. The gaze of the robot could be an indicator of its readiness to execute a task [42] or to signal planned actions followed by an action [43]. The robot's gaze could help to accomplish two primary purposes, establishing mutual belief (that is, the user is indicated about the action to be taken) and indicating readiness for the next instruction. In this specific case, whenever a robot decides to close or open its hand or reach out to an object, it could look at its hand or look at the object in the task [23]. Robot gestures are an appropriate and informative communication medium in HRC, so more innovative methods, such as zoomorphic gestures, are introduced in the field [44].
## (s19) VI. ROBOT DECISION-MAKING
Number of References: 2

(p19.0) HRC frameworks are becoming more intelligent with the introduction of intelligent robots that could make decisions on their own and adapt to the environment that facilitates the human-robot partnership and promote human safety. However, the choice of decision-making algorithm added to the HRC framework depends on multiple factors, such as collaborative task and available communication channel, which all affect the robot learning algorithm, the decisionmaking model, and interaction planning [57], [58]. It is necessary to know the characteristics of the task because it would determine the defined or undefined parameters in the environment. For example, in assembly tasks, tolerance or completion time may be well defined, while other details, such as the preferences of the individuals, may change in different individuals. However, the robot must learn the preferences of each operator and adapt to the situation. This section will describe the different robot decisionmaking algorithms used in the selected articles as one of the key components of HRC frameworks.
## (s21) B. DEEP LEARNING
Number of References: 5

(p21.0) Besides creating mapping functions between human physical capacities and robot computation, another challenge in HRC design is the computational representation of human-related factors that are embedded in multi-modal and multi-scale sensor data. Therefore, there have been attempts to develop new ML methods that could act as a feature extractor that ''transform the raw data of human capacities into a suitable internal representation or a feature vector from which the learning subsystem can be derived'' [61]. As a result, deep learning (DL) algorithms were developed, which are: representation learning methods with multiple levels of representation, obtained by composing simple but nonlinear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level [61]. Convolutional neural networks (CNN) and recurrent neural networks (RNNs) are commonly used DL methods. CNN is designed to process data that come in multiple arrays, such as a color image composed of three 2D arrays containing pixel intensities in the three color channels, and has application in image processing. Numerous research studies have shown that CNN is a promising method for learning representation from human visual perception in the HRC context [62]. On the other hand, RNNs are trained using the backpropagation technique and are suitable for tasks with sequential input, such as speech and language. DL methods are also combined with other neural networks [61] or reinforcement learning (RL) methods to create DRL architectures in real-world HITL settings to study collaborative learning between humans and robots [12].
## (s22) C. PROBABILISTIC GRAPHICAL MODELS
Number of References: 2

(p22.0) Another challenge of representation learning in HRC is the mental models that are supposed to be shared between humans and robots. In particular, the states and actions of both parties should be learned and represented as a theoretical uniform to achieve shared mental models. Furthermore, the representation of these mental models should be capable of integrating uncertainties to show robustness and tolerance to environmental changes, such as human preferences or task changes. Probabilistic graphical models (PGMs) are currently considered one of the promising methods to solve this problem. PGMs are defined as: ''Probabilistic graphical models (PGM) comprise any model that uses the language of graphs to facilitate the representation and resolution of complex problems that use probability as a representation of uncertainty [63].'' A graph structure consists of several nodes, and the edges represent the probabilistic relationships or conditional dependency/independence among a set of variables in a system. The nodes represent variables, and an edge between two nodes indicates a conditional dependency between the two variables (the absence of edges means conditional independence). PGMs are used for different purposes in HRC, including supervised classification, clustering, abductive reasoning, decision-making, and optimization [63].
