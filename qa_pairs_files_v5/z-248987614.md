# Deep Learning for Visual Speech Analysis: A Survey

CorpusID: 248987614 - [https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785](https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785)

Fields: Computer Science

## (s16) Visual Quality.
Number of References: 5

(p16.0) To evaluate the quality of the synthesized video frames, reconstruction error measurement (e.g., Mean Squared Error) is a natural evaluation way. However, reconstruction error only focuses on pixel-wise alignments and ignores global visual quality. Therefore, existing works usually adopt Peak Signal-to-Noise Ratio (PSNR) and Structure Similarity Index Measure (SSIM) to evaluate the global visual quality of generated frames. More recently, Prajwal et al. [38] introduced Fr√©chet Inception Distance (FID) to measure the distance between synthetic and real data distributions, as FID is more consistent with human perception evaluation. Besides, Cumulative Probability Blur Detection (CPBD) [115], a nonreference measure, is also widely used to evaluate the loss of sharpness during video generation.

(p16.1) Audio-visual Semantic Consistency. Semantic consistency of the generated video and the driving source mainly contains audio-visual synchronization and speech consistency. For, audio-visual synchronization, Landmark Distance (LMD) [116] computes the Euclidean distance of the lip region landmarks between the synthesized video frames and ground truth frames. The other synchronization evaluation metric is to use a pre-trained audio-to-video synchronisation network [48] to predict the offset of generated frames and the ground truth. For the speech consistency, Chen et al. [42] proposed a lipsynchronization evaluation metric, i.e., Lip-Reading Similarity Distance (LRSD), which measures the Euclidean distance of semantic-level speech embeddings obtained by lip reading networks. For better evaluation of speech consistency, lip reading results (accuracy, CER, or WER) comparisons of the generated frames and ground truth are also used as consistency evaluation metrics.
## (s20) Visual frontend network
Number of References: 10

(p20.0) As shown in Fig. 6, there are mainly three types of input data: mouth-centered videos, dense optical flow, and landmark points. Among them, mouth-centered videos and dense optical flow are regular grid data, so CNNs are the most suitable and commonly used backbone architectures for them. On the other hand, as landmark points are irregular data, some existing works [53,54,118] adopted Graph Convolution Networks (GCNs) to extract visual features from landmark points. Next, we review these backbone architectures.

(p20.1) CNN-based Architectures. CNNs have been becoming one of the most common architectures in the field of deep learning. Since AlexNet [119] was proposed in 2012, researchers have invented a variety of deeper, wider, and lighter CNN models [120]. Representative CNN architectures, such as VGG [121], ResNet [122], MobileNet [123], DenseNet [124], ShuffleNet [125] etc, have been widely used in learning visual representation for VSR.
## (s28) VISUAL SPEECH GENERATION
Number of References: 10

(p28.0) Visual Speech Generation (VSG), also known as lip sequence generation, aims to synthesize a lip sequence corresponding to the driving source (a clip of audio or a piece of text). Traditional VSG methods suffer from severe practical challenges [45], such as complex generation pipelines, constrained applicable environments, over-reliance on finegrained viseme (phoneme) annotations, etc. To realize mapping driving sources to lip dynamics, representative traditional VSG methods mainly adopted cross-modal retrieval approaches [16,103,155,156] and HMM-based approaches [157,158]. For example, Thies et al. [103] introduced a typical image-based mouth synthesis approach that generates a realistic mouth interior by retrieving and warping best-matching mouth shapes from offline samples. However, retrieval-based methods are static text-phoneme-viseme mappings and do not really consider the contextual information of the speech. Meanwhile, retrieval-based methods are pretty sensitive to head pose changes. HMM-based methods also suffer from some drawbacks, such as the limitation of the prior assumptions (e.g., Gaussian Mixture Model (GMM) and its diagonal covariance). As deep learning technologies have extensively promoted the developments of VSG, we focus on reviewing deep learning based VSG methods in this section.

(p28.1) To make the scope of VSG clear for readers, we first explain the relationship and difference between VSG and another hot topic, i.e., Talking Face Generation (TFG) 1 [71,159].
## (s33) 2D Coefficient based.
Number of References: 2

(p33.0) Active Appearance Model (AAM) is one of the most commonly used facial coefficient models, representing both the shape and texture variations and their correlation. Fan et al. [26] utilized a two-layer BiLSTM network to estimate AAM coefficients of the mouth area based on the overlapped triphone input, which then is transferred to a face image to produce a photo-realistic talking head. The experiments show that the BiLSTM network has superior performance to previous HMM-based approaches. Similarly, as shown in Fig. 8(e), Taylor et al. [66] introduced a simple and effective DNN as a sliding window predictor to automatically learn AAM coefficients based on the fixed-length phoneme sequence. Furthermore, the model can be retargeted to drive other face models with the help of an effective retargeting approach. The main practical limitation of AAM coefficients is that the reference face AAM parameterization may cause potential errors when retargeting to a new subject.
## (s37) Other Methods
Number of References: 3

(p37.0) In addition, some other one-stage VSG schemes have also been proposed. Inspired by the success of the neural radiance field (NeRF) [200], Guo et al. [73] proposed the audio-driven neural radiance fields (AD-NeRF) model for VSG. As shown in Fig. 8(k), AD-NeRF takes DeepSpeech audio features as conditional input, learning an implicit neural scene representation function to map audio features to dynamic neural radiance fields for talking face rendering. Furthermore, AD-NeRF models not only the head region but also the upper body via learning two individual neural radiance fields. However, AD-NeRF does not generalize well on mismatched driving audios and speakers. As shown in Fig. 8(l), unlike the previous concatenation-based feature fusion strategy, Ye et al. [74] presented a full convolutional neural network with dynamic convolution kernels (DCKs) for crossmodal feature fusion, which extracts features from audio and reshapes features as DCKs of the fully convolutional network. Due to the simple yet effective network architecture, the realtime performance of VSG is significantly improved.
