# LITERATURE REVIEW OF ATTRIBUTE LEVEL AND STRUCTURE LEVEL DATA LINKAGE TECHNIQUES

CorpusID: 9815708 - [https://www.semanticscholar.org/paper/2ee3a601957528b0d50b68bc145c7086a14c15a6](https://www.semanticscholar.org/paper/2ee3a601957528b0d50b68bc145c7086a14c15a6)

Fields: Computer Science

## (s3) Exact Matching Strategies
Number of References: 3

(p3.0) Unlike SQL Matching, Exact Matching techniques give more insight into the content and meaning of schema elements [25]. Exact matching uses a unique identifier present in both datasets being compared. The unique identifier can only be linked to one individual item, or an event (for example, a driver's license number). The Exact Matching technique is helpful in situations where the data linkage to be performed belongs to one data source. For example, consider a company with a recent system crash willing to perform data linkage between the production data source file and the most recent tape backup file to trace transactions. In such situations, Exact Matching would likely suffice in performing data linkage. A specific variation of exact matching discovered In this research is the Squirrel System [31], using a declarative specification language, ISL, to specific matching criteria which will match one record in a given table, with one record in another table. However, exact matching approach leaves no room for uncertainty; records are either classified as a match or as a non-match. Problems often arise when the quality of the variables does not sufficiently guarantee the unique identifier is valid [16]. Exact matching comparison does not suffice for matching records when the data contains errors, for example typographical mistakes, or when the data have multiple representations, such as through the use of abbreviations or synonyms [10].
## (s7) Rule/Regular expression
Number of References: 2

(p7.0) The Rule / Regular expression [40] approach uses rules or set of predefined regular expressions and perform matching on tuples. Regular Expression Pattern as proposed in [40] is more flexible than regular expression alone, which is built from alphabetical elements. This is also because the Regular Expression Pattern is built from patterns over a data element, allowing the use of constructs such as "wildcards" or pattern variables. Regular Expression Pattern is quite useful when manipulating strings, and can be used in conjunction with basic pattern matching. However, the problem with this approach lies in the fact that it is relatively domain specific and tends to only work well on strings.
## (s9) String distance
Number of References: 3

(p9.0) String distance methods, also known as character-based similarity metrics [34] are used to perform data linkage based on the cost associated within the comparing strings. The cost is estimated on the number of characters which needs to be inserted, replaced or deleted for a possible string match. For example, Fig. 3 shows the cost associated in editing string "Aussie" to "Australian" (the "+" sign shows addition, the "-" sign shows deletion, and the "x" sign shows replacement). Experimental results in [34] have shown that the different distance based methodologies discovered so far are efficient under different circumstances. Some of the commonly recommended distance based metrics include Levenstein distance, Needleman-Wunsch distance, Smith-Waterman distance, Affine-gap distance, Jaro metric, Jaro and Jaro-Winkler metric, Qgram distance, and positional Q-grams distance. Through the various methods, costs are assigned to compensate for pitfalls in the system. Yet, overall, string distance pattern is most effective for typographical errors, but is hardly useful outside of this area [34].
## (s10) Term frequency
Number of References: 4

(p10.0) Term frequency [43] approach determines the frequency of strings in relation and to favour matches of less common strings, and penalizes more common strings. The Term frequency methods allow for more commonly used strings to be left out of the similarity equation. TF-IDF [43] (Term Frequency-Inverse Document Frequency) is a method using the commonality of the term (TF) along with the overall importance of the term (IDF). TF-IDF is commonly used in conjunction with cosine similarity in the vector space model. Soft TF-IDG [44] adds similar token pairs to the cosine similarity computation. According to the researchers in [44], TF-IDF can be useful for similarity computations due to its ability to give proportionate token weights. However, this approach fails to make distinctions between the similarity level of two records with the same token or weight, and is essentially unable to determine which record is more relevant.
## (s15) Gram sequence
Number of References: 7

(p15.0) Gram sequence based techniques compare the sequence of grams of one string with the sequence of grams of another string. n-grams is a gram based comparison function which calculates the common characters in a sequence, but is only effective for strings that have a small number of missing characters [46]. For example, the strings "Uni" and "University" have the same 2-gram {un, ni}. q-gram [85] involves generating short substrings of length q using a sliding window at the beginning and end of a string [85]. The q-gram method can be used in corporate databases without making any significant changes to the database itself [85]. Theoretically, two similar strings will share multiple q-grams. Positional q-grams record the position of q-grams within the string [14]. Danish and Ahy in [85] proposed to generate q-grams along with various processing methods such as substrings, joins, and distance. Unfortunately, the gram sequence approach is only efficient for short string comparison and becomes complex, expensive and unfeasible for large strings [85].
## (s17) Hashing
Number of References: 4

(p17.0) Hashing methods convert attributes into a sequence of hash values which are compared for similarity matching between different sets of strings. Hashing methods require conversion of all the data to find the smallest hash value, which could be a costly approach. Set-of-sets [8] is a hashing based data matching technique which works reasonably well in smaller string matching scenarios. The set-of-sets technique proposed in [8] divides strings into 3-grams and assigns a hash value to each tri-gram. Once hash values are assigned and placed in a hash bag, only the lowest matching hash values are considered for matching. Unfortunately, this technique doesn't yield accurate results when dealing with variable length strings and uses traditional hashing which results in completely different hash values for even a small variation [79]. Furthermore, the Set-of-sets requires conversion of all the data prior to comparison in order to find the smallest hash value, which could be a costly approach. To overcome this disadvantage, the h-gram (hash gram) method was proposed in [79] to address the deficits of the set-of-sets technique, by extending the n-gram technique; utilizing scale based hashing; increasing matching probability; and by reducing the cost associated in storage of hash codes.
## (s20) Fuzzy Matrix
Number of References: 2

(p20.0) Fuzzy Matrix [32,60] places records in the form of matrices and apply fuzzy matching techniques to perform record matching. Commonly used by social scientists to analyse behavioural data, the fuzzy matrix technique is also applicable to many other data types. When considering a fuzzy set, a match is not directly identified as positive or negative. Instead, the match is considered on its degree level of agreement with the relevant data. As a result, a spectrum is created which identifies all levels of agreement or truth.
## (s23) Iterative pattern
Number of References: 2

(p23.0) Iterative pattern is the process of repeating a step multiple times (or making "passes") until a match is found based on similarity scores and blocking variables (variables set to be ignored for similarity comparison). The Iterative approach uses attribute similarity, while considering the similarity between currently linked objects. For example, the Iterative pattern method will consider a match of "John Doe" and "Jonathan Doe" as a higher probability if there is additional matching information between the two records (such as spouse's name and children's names). The first part of the process is to measure string distance, followed by a clustering process. Iterative pattern methods have proven to detect duplicates that would have likely been missed by other methods [54]. The gains are greater when the mean size of the group is larger, and smaller when the mean size is smaller. Disadvantages surface when distinctive cliques do not exist for the entities or if references for each group appear randomly. Additionally, there is also the disadvantage of cost, as the Iterative pattern method is computationally quite expensive [54].
## (s24) Tree pattern
Number of References: 3

(p24.0) Tree pattern is based on decision trees with ordered branches and leaves. The nodes are compared based on the extracted tree information. CART and C.5 are two widely-known decision tree methods which create trees through an extensive search of the available variables and splitting values [55]. A Tree pattern starts at the root node and recursively partitions the records into each node of the tree and creates a child to represent each partition. The process of splitting into partitions is determined by the values of some attributes, known as splitting attributes, which are chosen based on various criteria. The algorithm stops when there are no further splits to be made. Hierarchical verification through trees examines the parent once a matching leaf is identified. If no match is found within the parent, the process stops; otherwise the algorithm continues to examine the grandparent and further up the tree [37]. Suffix trees such as DAWG [37] build the tree structure over the suffixes of S, with each leaf representing one suffix and each internal node representing one unique substring of S. DAWG has additional feature of failure links added in for those letters which are not in the tree. Disadvantages of Tree pattern lies in lengthy and time consuming process with manual criteria often needed for splitting.
## (s25) Sequence pattern
Number of References: 4

(p25.0) Sequence pattern methods perform data linkage based on sequence alignment. This technique attempts to simulate a sequential alignment algorithm, such as the BLAST (Basic Local Alignment Search Tool) [12] technique used in Biology. The researchers compared the data linkage problem with the gene sequence alignment problem for pattern matching, with the main motivation to use already invented BLAST tools and techniques. The algorithm translates record string data into DNA sequences, while considering the relative importance of tokens in the string data [12].

(p25.1) Further research in the Sequence pattern area have exposed variations based on the type of translation used to translate strings into DNA Sequence (i.e. weighted, hybrid, and multi-bit BLASTed linkage) [12]. BLASTed linkage has advantages through the careful selection of one of its four variations, as each variation performs well on specific types of data. Unfortunately, sequence pattern tends to perform poorly on particular data strings, depending upon the error rate, importance weight, and number of common tokens [12].
## (s28) Clustering/Feature extraction
Number of References: 4

(p28.0) Clustering, also known as the Feature extraction method performs data linkage based on common matching criteria in clusters, so that objects in clusters are similar. Soft clustering [61], or probabilistic clustering, is a relaxed version of clustering which uses partial assignment of a cluster centre. The SWOOSH [62] algorithms apply ICAR properties (idempotence, commutativity, associativity, representativity) to the match and merge function. With these properties and several assumptions, researchers introduced the brute force algorithm (BFA), including the G, R and F SWOOSH algorithms [44]. SIMCLUST is another similarity based clustering algorithm which places each table in its own cluster as a starting point and then works its way through all of the tables by consecutively choosing two tables (clusters) with the highest level of similarities. [5] proposed iDisc system which creates database representations through a multi-process learning technique. Base clusters are used to uncover topical clusters which are then aggregated through meta-clustering. Clustering in general can get extremely complex (such as forming clusters using semantics) and needs to be handled carefully while discovering relationships between matching clusters.
## (s30) Training based
Number of References: 6

(p30.0) Training based technique is a manual approach where users are constantly involved in providing statistical data based on previous/future predictions. In [7], researchers presented a two-step training approach using automatically selected, high quality examples which are then used to train a support vector machine classifier. The approach proposed in [7] outperforms k-means clustering, as well as other unsupervised methods. The Hidden Markov training model, or HMM, standardises name and address data as an alternative method to rule-based matching. Through use of lexicon-based tokenization and probabilistic hidden Markov models, the approach attempts to cut down on the heavy computing investment required by rule programming [64]. Once trained, the HMM can determine which sequence of hidden states is most likely to have emitted the observed sequence of symbols. When this is identified, the hidden states can be associated with words from the original input string. This approach seems advantageous in that it cuts down on time costs when compared to rule-based systems. However, this approach remains a lengthy process, and has shown to run into significant problems in various areas. For instance, HMM confuses given, middle, and surnames, especially when applied to homogenous data. Furthermore, outcomes proved to be less accurate than those of rule-based systems [64]. DATAMOLD [65] is a training-based method which enhances HMM. The program is seeded with a set of training examples which allows the system to extract data matches. A common problem with training techniques is that it requires many examples to be effective; and the system will not perform without an adequate training set [55].
## (s31) Pruning/Filtering statistic
Number of References: 5

(p31.0) Pruning statistic performs data linkage by trimming similar records on a top down approach. In [16], the data cleaning process of "deduplication" involves detecting and eliminating duplicate records to reduce confusion in the matching process. For data which accepts a large number of duplicates, pruning, before data matching, simplifies the process and makes it more effective. A pruning technique proposed by Verykios [34] recommends pruning as on derived decision trees used for classification of matched or mismatched pairs. The pruning function reduces the size of the trees, improving accuracy and speed [34]. The pruning phase of CORDS [16] (which is further discussed in the statistical analysis section) prunes non-candidates on the basis of data type, properties, pairing rules, and workload; such tasks are done to reduce the search space and make the process faster for large datasets. Pruning techniques [37] are based on the idea that it is much faster to determine non-matching records than matching records, and therefore aim to eliminate all non-matching records which do not contain errors. However, the disadvantage of such techniques is that they are not suitable in identifying matches of any type, and must be combined with another matching technique.
## (s37) Data extraction
Number of References: 3

(p37.0) Data extraction primarily involves extracting semantic data. Data extraction can be performed manually or with an induction and automatic extraction [72]. In [73], researchers used data recognisers to perform data extraction on the semantics of data. The recogniser method is aimed at reducing alignment after extraction, speeding up the extraction process, reusing existing knowledge, and cutting down on manual structure creation. This approach is found to be effective for simple unified domains, but not for complicated, loosely unified domains. Another benefit of the data extraction technique is that, after the data is extracted, it can be handled as instances in a traditional database. However, it generally requires a carefully constructed extraction plan by an expert in that specific knowledge domain [74].
## (s40) Statistical analysis
Number of References: 3

(p40.0) Statistical analysis techniques examine statistical measurements for determining term and concept relationships. Jaccard Similarity Coefficient [38] is a widely used statistical measurement for comparing terms, which consider the extent of overlap between two vectors. The measurement is the size of the intersection, divided by the size of the union of the vector dimension sets. Considering the corpus, the Jaccard Similarity approach determines a match to be present if there is a high probability for both concepts to be present within the same section. For attribute matching, a match is determined if there is a large amount of overlap between values [38]. For example, CORDS [16] is a statistical matching tool, built upon B-HUNT, which locates statistical correlations and soft functional dependencies. CORDS searches for correlated column pairs through enumerating potentially correlating pairs and pruning unqualified pairs. A chisquared analysis is performed in order to locate numerical and categorical correlations.
