# A Review on Machine Learning Styles in Computer Vision-Techniques and Future Directions

CorpusID: 252558821 - [https://www.semanticscholar.org/paper/59694d8b594ac2bb0f91dd4a0d681133f976939a](https://www.semanticscholar.org/paper/59694d8b594ac2bb0f91dd4a0d681133f976939a)

Fields: Computer Science

## (s4) II. BACKGROUND STUDY A. EVOLUTION OF MACHINE LEARNING STYLES
Number of References: 3

(p4.0) In July 1970, an initial investigation was done to test whether a perceptual learning process learns the visual symbols & transfer procedure was used with deaf first-grade children. This experimental study's authors found evidence for distinctive feature learning [11]. Based on Vygotsky's theories, Sir James Britton and others in England developed Collaborative Learning(Co-Learning) in the 1970s as an active learning method. According to Britton, a student's learning comes from a community of learners composed of other students [12]. Then, A theoretical rationale elaborates upon the concepts of meta goals. Meta-learning was provided in April 1975. In 1979, Seltzer Donald S. explained how robots could learn from different methods. This author explained how sensory information is used for improved Robot learning. Then, in 1980, scientists presented an adaptive model for self-supervised learning that uses a single pattern training technique to recognize vowel sounds on a computer [13].
## (s15) a: SELF-SUPERVISED LEARNING
Number of References: 2

(p15.0) In some ways, self-supervised learning is a sort of unsupervised learning because it adheres to the condition that no labels are assigned. Self-supervised learning, on the other hand, instead of looking for high-level patterns for clustering, tries to tackle tasks typically addressed by supervised learning (e.g., image classification) without any labeling provided. Figure 13 displays the working of self-supervised learning from input data till the final output generation. Instead of recommending new self-supervised learning techniques, this learning aims to examine how current selfsupervised learning strategies might be applied to address domain adaption problems [53]. The primary task can learn a domain invariant feature representation thanks to the pretext  job connecting the source and destination domains. In the source domain, the primary job has labels; however, in the destination domain, there is no labeling requirement. In other words, we develop unsupervised domain adaptation through self-supervised learning. The forwarded data flow is represented by solid lines in the diagram, while the optional data flow is indicated by dotted lines [53]. Through multitask learning, the pretext and main task (such as object identification, classification, or semantic segmentation) are simultaneously learned. Advantages of Self-Supervised learning-â€¢ The frequency of labeling needed may be reduced with the use of self-supervised learning.
## (s18) c: ASSOCIATION RULE LEARNING
Number of References: 2

(p18.0) An unsupervised learning method called association rule learning looks at how one data item depends on another and then maps to make it more profitable. It seeks to uncover exciting connections or interactions between the dataset's variables [58]. A technique of machine learning based on rules called association rule learning can be used to find ''IF-THEN'' sentences or other meaningful correlations between variables in large datasets. One example is that if a consumer purchases a computer or laptop (one thing), they will also purchase anti-virus software (another item) simultaneously. IOT services, medical diagnosis, usage behavior analytics, web usage VOLUME 10, 2022 mining, cutting-edge phone applications, cybersecurity applications, and bioinformatics are a few examples of contemporary uses for association rules. The order of events within or across transactions is rarely considered by association rule learning, in contrast to sequence mining. Commonly, the ''support'' and ''confidence'' metrics are employed to evaluate the value of association rules [2].
