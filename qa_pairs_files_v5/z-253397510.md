# Pretraining in Deep Reinforcement Learning: A Survey

CorpusID: 253397510 - [https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096](https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096)

Fields: Computer Science

## (s8) CHALLENGES & FUTURE DIRECTIONS
Number of References: 4

(p8.0) This kind of approach has several deficiencies. One of the most important issues is how to distinguish epistemic and aleatoric uncertainty. Epistemic uncertainty refers to uncertainty caused by a lack of knowledge. Aleatoric uncertainty, in contrast, refers to the variability in the outcome due to inherently random effects. A concrete phenomenon in RL is the noisy TV problem (Mavor-Parker et al., 2022), which refers to the cases where the agent gets trapped by its curiosity in highly stochastic environments. To mitigate this issue, some work attempts to use intrinsic rewards proportional to a reduction in uncertainty (Houthooft et al., 2016;Pathak et al., 2019). However, tractable epistemic uncertainty estimation in high dimension remains challenging (Hüllermeier & Waegeman, 2021) due to its sensitivity to imperfect data.
## (s11) Data Coverage Maximization
Number of References: 4

(p11.0) Previously we have discussed how to obtain knowledge or skills, measured by the agent's own capability, from unsupervised interaction. Albeit indirectly related to the agent's ability, data diversity induced by online pretraining plays an essential role in deciding how well the agent obtains prior knowledge. In the field of supervised learning, recent advances have shown that diverse data can enhance out-of-distribution generalization (Hendrycks et al., 2020b) and robustness (Hendrycks et al., 2020a). Another supporting evidence is that most of the famed datasets are large and diverse (Deng et al., 2009;Wang et al., 2019). Motivated by the above considerations, it is desired to use data coverage maximization, usually measured by state visitation, as an objective to stimulate unsupervised learning.
## (s12) COUNT-BASED EXPLORATION
Number of References: 11

(p12.0) The first category of data coverage maximization is count-based exploration. Count-based exploration methods directly use visit counts to guide the agent towards underexplored states (Bellemare et al., 2016;Ostrovski et al., 2017). For tabular MDPs, Model-based Interval Estimation with Exploration Bonuses (Strehl & Littman, 2008) provably turn state-action N (s, a) counts into an exploration bonus reward:

(p12.1) Built on Equation 2, a series of work has studied how to tractably generalize count bonuses to high-dimensional state spaces (Bellemare et al., 2016;Ostrovski et al., 2017;Tang et al., 2017). To approximate these counts in high dimensions, Bellemare et al. (2016) introduce pseudo-counts derived from a density model. Specifically, the pseudo-count is defined as:

(p12.2) where ρ is a density model over state space S, ρ t (s) is the density assigned to s after training on a sequence of states s 1 , . . . , s t , and ρ t (s) is the density of s if ρ were to be trained on s one additional time. Based on similar ideas, it has been shown that a better density model (Ostrovski et al., 2017) or a hash function (Tang et al., 2017;Rashid et al., 2020) for computing state statistics can further improve performance. Besides, a self-supervised inverse dynamics model as discussed in Section 3.1 can also be used to bias the count-based bonuses towards what the agent can control (Badia et al., 2020).
## (s31) Representation Transfer
Number of References: 7

(p31.0) In the field of supervised learning, recent advances (Devlin et al., 2019;He et al., 2020;Chen et al., 2020) have demonstrated that good representations can be pretrained on large-scale unlabeled dataset, as evidenced by their impressive downstream performances. The most common practice is to freeze the weights of the pretrained feature encoder and train a randomly initialized task-specific network on top of that during adaptation. The success of this paradigm is essentially based on the promise that related tasks can usually be solved using similar representations.

(p31.1) For RL, it has been shown that directly reusing pretrained task-agnostic representations can significantly improve sample efficiency on downstream tasks. For instance, Schwarzer et al. (2021b) conduct experiments on the Atari 100K benchmark and find that frozen representations pretrained on exploratory offline data already form a basis of data-efficient RL. This success also extends to the cases where domain discrepancy exists between upstream and downstream tasks (Shah & Kumar, 2021;Parisi et al., 2022). However, the issue of negative transfer in the face of domain discrepancy might be exacerbated for RL due to its complexity (Shah & Kumar, 2021).
## (s32) Policy Transfer
Number of References: 4

(p32.0) A compelling alternative for task adaptation is to transfer learned behaviors. As discussed in previous sections, existing work has explored how to pretrain primitive skills that can be reused to face new tasks or a single exploratory policy that facilitates exploration at the beginning of task adaptation. The differences in pretrained behaviors result in different adaptation strategies. To achieve high rewards on the downstream task with skill-conditioned policy π (a|s, z), a straightforward strategy is to simply choose the skill z with the best outcome and further enhance it with finetuning. However, a single best-performing skill can not fulfill its potential. To better combine diverse skills for task solving, one can view them from the perspective of hierarchical RL (Barto & Mahadevan, 2003;Kulkarni et al., 2016). In hierarchical RL, the decision-making task is typically decomposed into a two-level hierarchy, where a meta-controller π(z | s) decides which low-level policy to use for task solving, depending on the current state. This hierarchical scheme is agnostic to how the low-level policies are learned. Therefore, it is sufficient to train a meta-controller on top of the discovered skills, which has been proven effective for few-shot adaptation (Hakhamaneshi et al., 2022) and zero-shot adaptation (Sharma et al., 2020).
