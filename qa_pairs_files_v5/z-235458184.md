# A Survey on Semi-Supervised Learning for Delayed Partially Labelled Data Streams

CorpusID: 235458184 - [https://www.semanticscholar.org/paper/ff0e0cbb8ff6da3c7077e1d6aad3adbf6d11315a](https://www.semanticscholar.org/paper/ff0e0cbb8ff6da3c7077e1d6aad3adbf6d11315a)

Fields: Computer Science

## (s10) Self-training
Number of References: 2

(p10.0) Self-training figures as another commonly used technique for semi-supervised learning. The idea is to let a classifier learn from its previous mistakes and try to reinforce itself. Self-training acts as a wrapper algorithm that takes any arbitrary classifier. Therefore, if we have an existing, fullysupervised learner that is complicated and hard to modify, self-training is an approach worth considering. Self-training has seen its application in natural language processing tasks such as word sense disambiguation [98] and sentiment analysis [62].
## (s23) Reference data streams
Number of References: 13

(p23.0) 5.4.1 The selection of data used for comparative analysis. The evaluation of individual stream mining methods under consideration should be made on a benchmark set of data streams. Similarly to other stream mining studies, we propose that evaluation performed with real data streams should be accompanied by evaluation performed with synthetic data streams including the streams for which predefined concept drift events, including the periods affected by gradual concept drift, can be defined. The evaluation of both synthetic and real data streams is a common practice in works proposing new stream mining methods [12,41,42,99,99].

(p23.1) By definition, the evaluation requires multiple partly labelled delayed data streams to be included. However, synthetic data streams are typically fully labelled and rely on immediately available labels. This includes synthetic data streams frequently used in the evaluation of stream mining methods such as Agrawal [45], Hyperplane [45], LED [45,59], and Random Tree [59]. As proposed in Le Nguyen et al. [59], labels from their instances can be removed with probability to provide a partly labelled data stream.

(p23.2) In their recent study, Le Nguyen et al. [59] proposed establishing a baseline to evaluate semisupervised learning methods in data streams. Importantly, this includes the extension of the MOA framework, which enables such evaluation. Even though this proposal does not consider the delayed labelling, but immediate labelling only, it can serve as a starting point for defining a baseline set of delayed data streams and developing software serving evaluation needs. Data streams for which no natural delay exists, including all the synthetic data streams listed above, can be converted to delayed ones by adding a fixed delay [45].
