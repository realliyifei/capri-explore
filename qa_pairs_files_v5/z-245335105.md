# A Comprehensive Analytical Survey on Unsupervised and Semi-Supervised Graph Representation Learning Methods

CorpusID: 245335105 - [https://www.semanticscholar.org/paper/b415ecb687941e1e9ef68e04a4a1c68c73483d51](https://www.semanticscholar.org/paper/b415ecb687941e1e9ef68e04a4a1c68c73483d51)

Fields: Mathematics, Computer Science

## (s5) Feature Engineering
Number of References: 3

(p5.0) Early methods on graph mining tasks were mostly based on supervised (hand-crafted) feature engineering [41,3,28]. Those hand-crafted features are designed with respect to some common intuitions that are supposed to infer some meaningful information from graphs. For example, ReFeX by Henderson et al. (2011) extracts a predetermined set of features such as the degree of a vertex (in/out-degree in case of directed graph and weights in case of a weighted graph), number of edges in the egonet (number of incoming/outgoing edges in case of a directed graph) and recursive features by summing up or averaging two types of previous features. Gallagher and Eliassi-Rad (2008), on the other hand, focuses on extracting features such as the average degree of egonet, the number of incident links, betweenness centrality, and clustering coefficient. These extracted features are then arranged in a vector format with their corresponding label and fed to a standard machine learning classifier for making predictions. The prediction tasks based on the extracted features are conducted in the following way: First feature vectors with corresponding labels are partitioned into two sets, e.g., T % of the samples are used for training and the rest of the datasets are used for testing. Then, logistic regression or any other standard machine learning method can be trained to learn the parameters of the model using the training dataset. Finally, the prediction task is performed using the test dataset based on the trained model. These types of methods do not always perform well on graph mining tasks because a predefined set of features is not always enough to capture the latent characteristics of the graph. Sometimes, they are found to be difficult to generalize across different types of graphs due to the highly irregular structure. Thus, more advanced methods have been evolved over time which is briefly discussed in the following sections.
## (s6) Matrix Factorization
Number of References: 5

(p6.0) Matrix Factorization is an effective technique to decompose a matrix into two lower-dimensional rectangular matrices [53]. This technique has been successfully applied to recommender systems [53], data compression [113], and spectral clustering [22]. It was first applied to graph factorization in large scale by Ahmed et al. (2013). Matrix A of dimension M × N can be decomposed into two lower-dimensional rectangular matrices B and C having dimensions M × R and R × N, respectively. Generally, R is much less than M and N. In the case of factorizing an adjacency matrix, we have M = N and C = B , where B is a transpose matrix of B. The loss function for this problem is defined by the following Equation:
## (s30) Clustering
Number of References: 3

(p30.0) The clustering of vertices is an important task in graph mining where common/similar vertices tend to form a cluster. High quality embedding can be helpful to detect a community in large scale social networks. A good graph clustering has a higher number of intra-cluster edges and a lower number of inter-cluster edges. Generally, the Louvain algorithm [9] is widely used to find clusters in a graph which focuses on maximizing modularity. However, we can not apply it on embedding as we do not have any structural information about the graph. Instead, we apply the k-means 22 algorithm which can effectively detect clusters in the embedding space of the graph. The common practice is to set a value for k in a range and find the clusters that show the highest modularity score [100]. We report modularity scores of Force2Vec, DeepWalk, VERSE, HARP and LINE across different datasets in Fig. 18 (a). We consider Louvain algorithm as the baseline method. It has the modularity scores of 0.81, 0.88, 0.73, and 0.49 for Cora, Citeseer, Pubmed and Flickr datasets, respectively. We observe that Force2Vec achieves higher modulartiy scores for Cora, Citeseer, and Pubmed datasets. DeepWalk achieves higher modularity scores than other methods for Flickr dataset and VERSE shows better modularity score for the Youtube dataset. Notably these results are very competitive to the baseline method. For large graphs, such as Flickr and Youtube, the modularity scores of Force2Vec, VERSE and DeepWalk are comparative. As the LINE from authors' repository shows poor performance as usual, we run another implementation 23 . This version of LINE shows better performance compared to the previous version. In particular, this version of LINE achieves the modularity scores of 0.61, 0.51, and 0.51 for the Cora, Citeseer, and Pubmed datasets, respectively. Note that we do not show the results of other methods such as struc2vec or HOPE due to their poor performance in graph clustering task using the k-means algorithm.
## (s34) Effect of Dimensions
Number of References: 4

(p34.0) Some previous studies have shown that the performance on the prediction task may vary if we choose different values for hyper-parameters [79,34,100,80]. For example, after reaching a certain value for dimensionality, the accuracy of prediction starts to drop when we increase it further. Most of the previous studies suggest using dimensional embedding. To summarize the results, we conduct experiments varying the dimensions of the output embedding for some shallow network-based methods. We set different parameters as described in Section 4.3 and take 20% of the dataset to train the logistic regression model while the rest of the samples in the dataset are used for the classification. We report the results of the F1-micro scores for the Pubmed dataset in Figs. 21 (a). We observe that Force2Vec, DeepWalk, and HARP perform better than other methods for various dimensional embedding. We also notice that, for lower dimensions, the F1-micro scores are not that much less compared to higher dimensions. In fact, the VERSE tool shows better performance for 16-dimensional embedding for the Pubmed dataset. RolX shows high sensitivity for different dimensions. It shows the lowest performance for 16-dimensional embedding. Then, with the increase of dimension, the F1-micro score also increases until 128-dimension. Then, it falls a little for 256-dimensional embedding. The LINE method shows similar sensitivity to the VERSE method though its F1-micro scores are lower than the VERSE.
## (s35) Effect of Various Negative Samples
Number of References: 6

(p35.0) Noise-contrastive estimation [36] is a popular technique used by most of the shallow graph embedding models [100,117,90]. Using this technique, a subset of vertices are randomly selected from a uniform distribution as negative samples which are used alongside positive samples (i.e., k-hop neighbors) to optimize the objective function. However, randomly generated negative samples may hurt the optimization function due to the selection of false negative samples and this become vital if the number of negative samples is very large. Armandpour et al. (2019) have made efforts to analyze this issue theoretically and proposed robust negative sampling techniques for graph embedding problem. NSCaching [119] is another interesting work that generates efficient negative samples for knowledge graphs. In Fig. 21 (b), we empirically show the effect on performance measures varying the number of negative samples. To conduct experiment, we choose Force2Vec, VERSE and LINE methods as they support random negative sampling approach. We take 25% of the vertices in the training set and report the F1-micro score for the rest of the testing dataset. We observe that performance score drops when we use more than 5 negative samples for the Pubmed dataset and continues to decrease for more negative samples. VERSE method is more sensitive to higher negative samples than others as its performance score significantly drops after using more than 15 negative samples for the Pubmed dataset. Note that the size of this dataset is relatively small as it has only around 19K vertices. For smaller graphs than Pubmed, the number of randomly selected negative samples will show more sensitivity than bigger graphs than the Pubmed dataset. The reason is that for bigger graphs, the probability of selecting false negative is lower compared to the smaller graphs. Thus, care must be taken to choose an effective number of negative samples rather than random selection. 
