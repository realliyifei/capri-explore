# Silent Speech Interfaces for Speech Restoration: A Review

CorpusID: 221507674 - [https://www.semanticscholar.org/paper/02b75480b6bb368346648e65bd9c722340c37571](https://www.semanticscholar.org/paper/02b75480b6bb368346648e65bd9c722340c37571)

Fields: Medicine, Computer Science, Engineering

## (s2) A. Aphasia
Number of References: 2

(p2.0) Aphasia is a disorder that affects the comprehension and formulation of language and is caused by damage to the areas of the brain involved in language [49]. People with aphasia have difficulties with understanding, speaking, reading or writing, but their intelligence is normally unaffected. For instance, aphasic patients struggle in retrieving the words they want to say, a condition known as anomia. The opposite mental process, i.e., the transformation of messages heard or read into an internal message, is also affected in aphasia. Aphasia affects not only spoken but also written communication (reading/writing) and visual language (e.g., sign languages) [49].
## (s7) C. Comparison of the two SSI approaches
Number of References: 4

(p7.0) Each SSI approach has its advantages and disadvantages. Silent speech-to-text has the advantage that speech might be more accurately predicted from the biosignals, thanks to the language and pronunciation lexicon models used in ASR systems. These models impose strong constraints during speech decoding and may help recover some speech features, such as voicing or manner of articulation, which are not well captured by current sensing techniques [20], [29], [102], [174]. However, the use of these models also means that this approach is unable to recognise words that were not considered during training, such as words in a foreign language. The direct speech synthesis approach, in contrast, is not limited to a specific vocabulary and is language-independent. A second limitation of the silent speech-to-text approach is that the paralinguistic features of speech (e.g., speaker identity or mood), which are important for human communication, are lost after ASR, but could be recovered by direct synthesis techniques. Yet another problem of silent speech-to-text is that, in practice, it is difficult to record enough silent speech data to train a large vocabulary ASR system 2 , while direct synthesis systems require less training material (usually just a few hours of training data) because modelling the biosignal-to-speech mapping is arguably easier than training a full-fledged speech recogniser.
## (s9) A. Brain activity
Number of References: 5

(p9.0) Obtaining biosignals at the origin of speech production has the advantage that a wider range of speech disorders and pathologies can thus be addressed. Brain activity sensing techniques can potentially assist not only persons with voice disorders but also those with dysarthria or apraxia, or even some cases of aphasia. On the other hand, the internal processes of the brain that are involved in speech production are imperfectly understood, and recording brain activity at a high spatiotemporal resolution is still problematic, at best. 1) Neuroanatomy of speech production: The neuroanatomy of language production and comprehension has been a topic of intense investigation for more than 130 years [185]. Historically, the brain's left superior temporal gyrus (STG) has been identified as an important area for these cognitive processes. Studies have shown that patients with lesions to this brain area present deficits in language production and comprehension [186], and that a complex cortical network extending through multiple areas of the brain is involved in these processes [187].

(p9.1) This cortical network has recently been modelled by a dualstream model consisting of a ventral and a dorsal stream [185]. The ventral stream, which involves structures in the superior (i.e., STG) and middle portions of the temporal lobe, is related to speech processing for comprehension, while the dorsal stream maps acoustic speech signals to the frontal lobe articulatory networks, which are responsible for speech production. This dorsal stream is strongly left-hemisphere dominant and involves structures in the posterior dorsal and the posterior frontal lobe, including Broca's area, or inferior frontal gyrus (IFG), which is critically involved in speech production [188].
## (s10) B. Muscle activity
Number of References: 2

(p10.0) As shown in Fig. 1, during speech production the muscles in the face and larynx are responsible for the movements that will eventually result in the production of the acoustic signal. As mentioned above, the brain controls the activation of these muscles by means of electrical signals transmitted through the motor neurons of the peripheral nervous system. These electrical signals cause muscles to contract and relax, thus producing the required articulatory movements and gestures. EMG measures the electrical potentials generated by depolarisation of the external membrane of the muscle fibres in response to the stimulation of the muscles by the motor neurons [212]. The EMG signal resulting from the application of this technique is complex and dependent on the anatomical and physiological properties of the muscles [213].
## (s14) A. Improved sensing techniques
Number of References: 9

(p14.0) Most of the sensing techniques described in Section IV have only been validated in laboratory settings under controlled scenarios. Hence, certain issues need to be addressed before final products can be made available to the general public. First, while many techniques are designed to allow some portability and to be generally non-invasive, some problems remain. The equipment is not discreet and/or comfortable enough to be used as a wearable in real-world practice [26], [278] and may be insufficiently robust against sensor misalignment [279], [280]. Second, the linguistic information captured by these devices is often limited. For example, sEMG has difficulty in capturing tongue motions, while EMA/PMA cannot accurately model the phones articulated at the back of the vocal tract due to practical problems that may arise in locating sensors in this area (such as the gag reflex and the danger of the user swallowing the sensors) [102], [174]. These problems might be overcome by combining different types of sensors, each of which is focused on a different region of the vocal tract, thus enabling a broader spectrum of linguistic information to be obtained. Yet another issue is that of how to capture and model supra-segmental features (i.e., prosodic features), which play a key role in oral communication. Prosody is mainly conditioned by the airflow and the vibration of the vocal folds, which in the case of laryngectomised patients is not possible to recover. As a result, most direct synthesis techniques generating a voice from sensed articulatory movements can, at best, recover a monotonous voice with limited pitch variations [101], [281], [282]. The use of complementary information capable of restoring prosodic features is thus an important area for future research.
## (s19) F. Evaluation in more realistic scenarios
Number of References: 3

(p19.0) The vast majority of SSIs thus far proposed have been validated using offline analyses with pre-recorded data. In these analyses, a pre-recorded data corpus is used both for system training and for evaluation. While the results of these offline analyses are useful for optimising various system parameters (such as system latency, output quality and system robustness), online analyses are needed in order to evaluate system performance in real-world scenarios. Online analyses assess the efficacy of the SSI while it is in active use, possibly while the user is receiving real-time audio feedback. Ideally, the system should be tested in real-life scenarios, over a prolonged period (i.e., longitudinal analysis) and with an adequate number of users presenting a diversity of speech impairments at different stages of evolution. Regarding the first point, most offline analyses reported to date have been based on a pre-recorded list of words, commands or phonetically-rich sentences. While this type of vocabulary-oriented evaluation can provide insights into SSI accuracy for decoding different phones, it does not reflect the fact that, in most cases, users will employ the SSI to establish a goal-oriented dialogue (e.g., ordering food in a restaurant or asking for help) [325], [326]. In these situations, other factors come into play, such as contextual information and visual clues (e.g., body language), which can help to resolve confusion in word meaning during the dialogue [183].
