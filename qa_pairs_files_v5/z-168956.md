# A Survey on Metric Learning for Feature Vectors and Structured Data

CorpusID: 168956 - [https://www.semanticscholar.org/paper/6f0cde3fcab0044f386b5b8a4244c371507bec15](https://www.semanticscholar.org/paper/6f0cde3fcab0044f386b5b8a4244c371507bec15)

Fields: Mathematics, Computer Science

## (s0) Introduction
Number of References: 2

(p0.0) The notion of pairwise metric-used throughout this survey as a generic term for distance, similarity or dissimilarity function-between data points plays an important role in many machine learning, pattern recognition and data mining techniques. 1 For instance, in classification, the k-Nearest Neighbor classifier (Cover and Hart, 1967) uses a metric to identify the nearest neighbors; many clustering algorithms, such as the prominent K-Means (Lloyd, 1982), rely on distance measurements between data points; in information retrieval, doc-uments are often ranked according to their relevance to a given query based on similarity scores. Clearly, the performance of these methods depends on the quality of the metric: as in the saying "birds of a feather flock together", we hope that it identifies as similar (resp. dissimilar) the pairs of instances that are indeed semantically close (resp. different). General-purpose metrics exist (e.g., the Euclidean distance and the cosine similarity for feature vectors or the Levenshtein distance for strings) but they often fail to capture the idiosyncrasies of the data of interest. Improved results are expected when the metric is designed specifically for the task at hand. Since manual tuning is difficult and tedious, a lot of effort has gone into metric learning, the research topic devoted to automatically learning metrics from data.
## (s17) RCA (Bar-Hillel et al.)
Number of References: 4

(p17.0) Relevant Component Analysis 20 (Shental et al., 2002;Bar-Hillel et al., 2003 makes use of positive pairs only and is based on subsets of the training examples called "chunklets". These are obtained from the set of positive pairs by applying a transitive closure: for instance, if (x 1 , x 2 ) ∈ S and (x 2 , x 3 ) ∈ S, then x 1 , x 2 and x 3 belong to the same chunklet. Points in a chunklet are believed to share the same label. Assuming a total of n points in k chunklets, the algorithm is very efficient since it simply amounts to computing the following matrix:

(p17.1) andm j is its mean. Thus, RCA essentially reduces the within-chunklet variability in an effort to identify features that are irrelevant to the task. The inverse ofĈ is used in a Mahalanobis distance. The authors have shown that (i) it is the optimal solution to an information-theoretic criterion involving a mutual information measure, and (ii) it is also the optimal solution to the optimization problem consisting in minimizing the within-class distances. An obvious limitation of RCA is that it cannot make use of the discriminative information brought by negative pairs, which explains why it is not very competitive in practice. RCA was later extended to handle negative pairs, at the cost of a more expensive algorithm (Hoi et al., 2006;Yeung and Chang, 2006).
## (s19) Online Approaches
Number of References: 2

(p19.0) In online learning (Littlestone, 1988), the algorithm receives training instances one at a time and updates at each step the current hypothesis. Although the performance of online algorithms is typically inferior to batch algorithms, they are very useful to tackle large-scale problems that batch methods fail to address due to time and space complexity issues. Online learning methods often come with regret bounds, stating that the accumulated loss suffered along the way is not much worse than that of the best hypothesis chosen in hindsight. 22 Shalev-Shwartz et al., 2004), for Pseudo-metric Online Learning Algorithm, is the first online Mahalanobis distance learning approach and learns the matrix M as well as a threshold b ≥ 1. At each step t, POLA receives a pair
## (s23) RDML (Jin et al.) RDML
Number of References: 3

(p23.0) is the projection to the PSD cone. The parameter λ implements a trade-off between satisfying the pairwise constraint and staying close to the previous matrix M t−1 . Using some linear algebra, the authors show that this update can be performed by solving a convex quadratic program instead of resorting to eigenvalue computation like POLA. RDML is evaluated on several benchmark datasets and is shown to perform comparably to LMNN and ITML. (Kunapuli and Shavlik, 2012), for Mirror Descent Metric Learning, is an attempt of proposing a general framework for online Mahalanobis distance learning. It is based on composite mirror descent (Duchi et al., 2010), which allows online optimization of many regularized problems. It can accommodate a large class of loss functions and regularizers for which efficient updates are derived, and the algorithm comes with a regret bound. Their study focuses on regularization with the nuclear norm (also called trace norm) introduced by Fazel et al. (2001) and defined as M * = i σ i , where the σ i 's are the singular values of M . 23 It is known to be the best convex relaxation of the rank of the matrix and thus nuclear norm regularization tends to induce low-rank matrices. In practice, MDML has performance comparable to LMNN and ITML, is fast and sometimes induces low-rank solutions, but surprisingly the algorithm was not evaluated on large-scale datasets.
## (s38) NNCA (Salakhutdinov & Hinton)
Number of References: 2

(p38.0) Nonlinear NCA (Salakhutdinov and Hinton, 2007) is another distance learning approach based on deep learning. NNCA first learns a nonlinear, low-dimensional representation of the data using a deep belief network (stacked Restricted Boltzmann Machines) that is pretrained layer-by-layer in an unsupervised way. In a second step, the parameters of the last layer are fine-tuned by optimizing the NCA objective (Section 3.2). Additional unlabeled data can be used as a regularizer by minimizing their reconstruction error. Although it suffers from the same limitations as LSMD due to its deep structure, NNCA is shown to perform well when enough data is available. For instance, on a digit recognition dataset, NNCA based on a 30-dimensional nonlinear representation significantly outperforms k-NN in the original pixel space as well as NCA based on a linear space of same dimension.  observe that learning a Mahalanobis distance with an existing algorithm and plugging it into a RBF kernel does not significantly improve SVM classification performance. They instead propose Support Vector Metric Learning (SVML), an algorithm that alternates between (i) learning the SVM model with respect to the current Mahalanobis distance and (ii) learning a Mahalanobis distance that minimizes a surrogate of the validation error of the current SVM model. Since the latter step is nonconvex in any event (due to the nonconvex loss function), the authors optimize the distance based on the decomposition L T L, thus there is no PSD constraint and the approach can be made low-rank. Frobenius regularization on L may be used to avoid overfitting. The optimization procedure is done using a gradient descent approach and is rather efficient although subject to local minima. Nevertheless, SVML significantly improves standard SVM results. Kedem et al. (2012) propose Gradient-Boosted LMNN, a nonlinear method consisting in generalizing the Euclidean distance with a nonlinear transformation φ as follows:
## (s41) Local Metric Learning
Number of References: 7

(p41.0) The methods studied so far learn a global (linear or nonlinear) metric. However, if the data is heterogeneous, a single metric may not well capture the complexity of the task and it might be beneficial to use multiple local metrics that vary across the space (e.g., one for each class or for each instance). 33 This can often be seen as approximating the geodesic distance defined by a metric tensor (see Ramanan and Baker, 2011, for a review on this matter). It is typically crucial that the local metrics be learned simultaneously in order to make them meaningfully comparable and also to alleviate overfitting. Local metric learning has been shown to significantly outperform global methods on some problems, but typically comes at the expense of higher time and memory requirements. Furthermore, they usually do not give rise to a consistent global metric, although some recent work partially addresses this issue (Zhan et al., 2009;Hauberg et al., 2012).  Saul, 2008Saul, , 2009) learns several Mahalanobis distances in different parts of the space. As a preprocessing step, training data is partitioned in C clusters. These can be obtained either in a supervised way (using class labels) or without supervision (e.g., using K-Means). Then, C metrics (one for each cluster) are learned in a coupled fashion in the form of a generalization of the LMNN's objective, where the distance to a target neighbor or an impostor 33. The work of Frome et al. (2007) is one of the first to propose to learn multiple local metrics. However, their approach is specific to computer vision so we chose not to review it here. 34. Source code available at: http://www.cse.wustl.edu/~kilian/code/code.html x is measured under the local metric associated with the cluster to which x belongs. In practice, M 2 -LMNN can yield significant improvements over standard LMNN (especially with supervised clustering), but this comes at the expense of a higher computational cost, and important overfitting (since each local metric can be overly specific to its region) unless a large validation set is used (Wang et al., 2012c).
## (s43) Metric Learning for Histogram Data
Number of References: 4

(p43.0) Histograms are feature vectors that lie on the probability simplex S d . This representation is very common in areas dealing with complex objects, such as natural language processing, computer vision or bioinformatics: each instance is represented as a bag of features, i.e., a vector containing the frequency of each feature in the object. Bags-of(-visual)-words (Salton et al., 1975;Li and Perona, 2005) are a common example of such data. We present here three metric learning methods designed specifically for histograms. Kedem et al. (2012) propose χ 2 -LMNN, which is based on a simple yet prominent histogram metric, the χ 2 distance (Hafner et al., 1995), defined as
