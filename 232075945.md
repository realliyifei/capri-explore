# A Survey on Stance Detection for Mis-and Disinformation Identification

CorpusID: 232075945
 
tags: #Computer_Science

URL: [https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106](https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

A Survey on Stance Detection for Mis-and Disinformation Identification


Momchil Hardalov momchil@checkstep.com 
Sofia University "St. Kliment Ohridski"
Bulgaria

Arnav Arora 
University of Copenhagen
Denmark

Preslav Nakov preslav.nakov@checkstep.com 
Qatar Computing Research Institute
HBKU
DohaQatar

Isabelle Augenstein isabelle@checkstep.com 
University of Copenhagen
Denmark

Checkstep Research 
A Survey on Stance Detection for Mis-and Disinformation Identification

Understanding attitudes expressed in texts, also known as stance detection, plays an important role in systems for detecting false information online, be it misinformation (unintentionally false) or disinformation (intentionally false information). Stance detection has been framed in different ways, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right. While there have been prior efforts to contrast stance detection with other related tasks such as argumentation mining and sentiment analysis, there is no existing survey on examining the relationship between stance detection and mis-and disinformation detection. Here, we aim to bridge this gap by reviewing and analysing existing work in this area, with mis-and disinformation in focus, and discussing lessons learnt and future challenges.

# Introduction

The past decade is characterized by a rapid growth in popularity of social media platforms such as Facebook, Twitter, Reddit, and more recently, Parler. This, in turn, has led to a flood of dubious content, especially during controversial events such as Brexit and the US presidential election. More recently, with the emergence of the COVID-19 pandemic, social media were at the center of the first global infodemic (Alam et al., 2021), thus raising yet another red flag and a reminder of the need for effective mis-and disinformation detection online.

In this survey, we examine the relationship between automatically detecting false information online -including fact-checking, and detecting fake news, rumors, and hoaxes -and the core underlying Natural Language Processing (NLP) task needed to achieve this, namely stance detection. Therein, we consider mis-and disinformation, which both refer to false information, though disinformation has an additional intention to harm.

Detecting and aggregating the expressed stances towards a piece of information can be a powerful tool for a variety of tasks including understanding ideological debates (Hasan and Ng, 2014), gathering different frames of a particular issue (Shurafa et al., 2020) or determining the leanings of media outlets (Stefanov et al., 2020). The task of stance detection has been studied from different angles, e.g., in political debates (Habernal et al., 2018), for fact-checking , or regarding new products (Somasundaran et al., 2009). Moreover, different types of text have been studied, including social media posts (Zubiaga et al., 2016b) and news articles (Pomerleau and Rao, 2017). Finally, stances expressed by different actors have been considered, such as politicians (Johnson et al., 2009), journalists (Hanselowski et al., 2019, and users on the web (Derczynski et al., 2017).

There are some recent surveys related to stance detection. Zubiaga et al. (2018a) discuss the role of stance in rumour verification, Aldayel and Magdy (2021) survey stance detection for social media, and Küçük and Can (2020) survey stance detection holistically, without a specific focus on veracity. There are also surveys on fact-checking Guo et al., 2022), which mention, though do not exhaustively survey, stance.

However, there is no existing overview of the role that different formulations of stance detection play in the detection of false content. In that respect, stance detection could be modelled as factchecking -to gather the stances of users or texts towards a claim or a headline (and support factchecking or studying misinformation) -, or as a component of a system that uses stance as part of its process of judging the veracity of an input claim. Here, we aim to bridge this gap by surveying the research on stance for mis-and disinformation detection, including task formulations, datasets, and methods, from which we draw conclusions and lessons, and we forecast future research trends.


## Dataset


## Source(s) Target

Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

2 What is Stance?

In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as "a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): "for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.


# Stance and Factuality

Here, we offer an overview of the settings for misand disinformation identification to which stance detection has been successfully applied. As shown in Figure 1, stance can be used (a) as a way to perform fact-checking, or more typically, (b) as a component of a fact-checking pipeline. Table 1 shows an overview of the key characteristics of the available datasets. We include the source of the data and the target 1 towards which the stance is expressed in the provided textual context.  We further show the type of evidence: Single is a single document/fact, Multiple is multiple pieces of textual evidence, often facts or documents, Thread is a (conversational) sequence of posts or a discussion. The final column is the type of the target Task. Finally, we present a dataset-agnostic summary of the terminology used for the different types of stance (see Figure 2), which we describe in a fourlevel taxonomy: (i) sources, i.e., where the dataset was collected from, (ii) inputs that represent the stance target (e.g., claim), and the accompanying context (e.g., news article), (iii) categorisationmeta-level characteristics of the input, and (iv) the textual object types for a particular stance scenario (e.g., topic, tweet, etc.). Appendix A discusses different stance scenarios with corresponding contexts and targets, with illustrations in Table 3.


## Fact-Checking as Stance Detection

As stance detection is the core task within factchecking, prior work has studied it in isolation, e.g., predicting the stance towards one or more documents. More precisely, the stance of the textual evidence(s) toward the target claim is considered as a veracity label, as illustrated in Figure 1a.

Fact-Checking with One Evidence Document Pomerleau and Rao (2017) organised the first Fake News Challenge (FNC-1) with the aim of automatically detecting fake news. The goal was to detect the relatedness of a news article's body w.r.t. a headline (possibly from another news article), based on the stance that the former takes regarding the latter. The possible categories were positive, negative, discuss, and unrelated. This was a standalone task, as it provides stance annotations only, omitting the actual "truth labels", with the motivation of assisting fact-checkers in gathering several distinct arguments pertaining to a particular claim.

Fact-Checking with Multiple Evidence Documents The FEVER (Thorne et al., , 2019 shared task was introduced in 2018, aiming to determine the veracity of a claim based on a set of statements from Wikipedia. Claims can be composite and can contain multiple (contradicting) statements, which requires multi-hop reasoning, and the claimevidence pairs are annotated as SUPPORTED, RE-FUTED, and NOT ENOUGH INFO. The latter category includes claims that are either too general or too specific, and cannot be supported or refuted by the available information in Wikipedia. This setup may help fact-checkers understand the decisions a model made in their assessment of the veracity of a claim, or assist human fact-checkers.

The second edition (2019) of FEVER evaluated the robustness of models to adversarial attacks, where the participants were asked to provide new examples to "break" existing models, then to propose "fixes" for the system against such attacks.

Note that FEVER slightly differs from typical stance detection, as it considers evidence supporting or refuting a claim, rather than the stance of an author towards a claim. An alternative way to look at this is in terms of argument reasoning, i.e., extracting and providing factual evidence for a claim.

FEVER also has a connection to Natural Language Inference, i.e., determining the relationship between two sentences. We view FEVER as requiring stance detection as it resembles FNC, which is commonly seen as a stance detection task.

Apart from FEVER, Hanselowski et al. (2019) presented a task constructed from manually factchecked claims on Snopes. For this task, a model had to predict the stance of evidence sentences in articles written by journalists towards claims. Unlike FEVER, this task does not require multihop reasoning.

Chen et al. (2020) studied the verification of claims using tabular data. The TabFact dataset was generated by human annotators who created positive and negative statements about Wikipedia tables. Two different forms of reasoning in a statement are required: (i) linguistic, i.e., semantic understanding, and (ii) symbolic, i.e., using the table structure.


## Stance as a (Mis-/Dis-)information Detection Component

Fully automated systems can assist in gauging the extent and studying the spread of false information online. This is in contrast to the previously discussed applications of stance detection -as a stand-alone system for detecting mis-and disinformation. Here, we review its potency to serve as a component in an automated pipeline. Figure 1b illustrates the setup, which can also include steps such as modelling the user or profiling the media outlet among others. We discuss in more detail media profiling and misconceptions in Appendix B.

Rumors Stance detection can be used for rumour detection and debunking, where the stance of the crowd, media, or other sources towards a claim are used to determine the veracity of a currently circulating story or report of uncertain or doubtful factuality. More formally, for a textual input and a rumour expressed as text, stance detection here is to determine the position of the text towards the rumour as a category label from the set {Support, Deny, Query, Comment}. Zubiaga et al. (2016b) define these categories as whether the author: supports (Support) or denies (Deny) the veracity of the rumour they are responding to, "asks for additional evidence in relation to the veracity of the rumour" (Query) or "makes their own comment without a clear contribution to assessing the veracity of the rumour" (Comment). This setup was widely explored for microblogs and social media. Qazvinian et al. (2011) started with five rumours and classified the user's stance as endorse, deny, unrelated, question, or neutral. While they were among the first to demonstrate the feasibility of this task formulation, the limited size of their study and the focus on assessing the stance of individual posts limited its real-world applicability. Zubiaga et al. (2016b) analysed how people spread rumours on social media based on conversational threads. They included rumour threads associated with nine newsworthy events, and users' stance before and after the rumours were confirmed or denied. Ferreira and Vlachos (2016) collected claims and news articles from rumour sites with annotations for stance and veracity by journalists as part of the Emergent project. The goal was to use the stance of a news article, summarised into a single sentence, towards a claim as one of the components to determine its veracity. A downside is the need to summarise, in contrast to FNC-1 (Pomerleau and Rao, 2017), where entire news articles were used.  Zubiaga et al. (2016b). Bošnjak and Karan (2019) studied stance detection and claim verification of comments for Croatian news articles.


## Multiple languages

In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using "born in/on". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as "Sarawak is a ...", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., "Andrea Pirlo is an American professional footballer." vs. "Andrea Pirlo is an Italian professional footballer who plays for an American club.", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., "Terry Crews played on the Los Angeles Chargers." (NotE-noughInfo) is classified as refuted, given the sentence "In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ...", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, "The heart beats at a resting rate close to 22 bpm." is not classified as refuted based on the evidence sentence "The heart beats at a resting rate close to 72 bpm.", and similarly for months.

Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.


## State of the Art


# Lessons Learned and Future Trends

Dataset Size A major limitation holding back the performance of machine learning for stance detection is the size of the existing stance datasets, the vast majority of which contain at most a few thousand examples. Contrasted with the related task of Natural Language Inference, where datasets such as SNLI (Bowman et al., 2015) of more than half a million samples have been collected, this is far from optimal. Moreover, the small dataset sizes are often accompanied with skewed class distribution with very few examples from the minority classes, including many of the datasets in this study (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021).

This can lead to a significant disparity for label performance (see Section 4). Several techniques have been proposed to mitigate this, such as sampling strategies (Nie et al., 2019), weighting classes (Veyseh et al., 2017), 3 crafting artificial examples from auxiliary tasks Hardalov et al., 2022), or training on multiple datasets (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.

Data Mixing A potential way of overcoming limitations in terms of dataset size and focus is to combine multiple datasets. Yet, as we previously discussed (see Section 3), task definitions and label inventories vary across stance datasets. Further, large-scale studies of approaches that leverage the relationships between label inventories, or the similarity between datasets are still largely lacking. One promising direction is the use of label embeddings (Augenstein et al., 2018), as they offer a convenient way to learn interactions between disjoint label sets that carry semantic relations. One such first study was recently presented by Hardalov et al. (2021), which explored different strategies for leveraging inter-dataset signals and label interactions in both in-(seen targets) and out-of-domain (unseen targets) settings. This could help to overcome challenges faced by models trained on smallsize datasets, and even for smaller minority classes.

Multilinguality Multi-linguality is important for several reasons: (i) the content may originate in various languages, (ii) the evidence or the stance may not be expressed in the same language, thus (iii) posing a challenge for fact-checkers, who might not be speakers of the language the claim was originally made in, and (iv) it adds more data that can be leveraged for modelling stance. Currently, only a handful of datasets for factuality and stance cover languages other than English (see Table 1), and they are small in size and do not offer a cross-lingual setup. Recently, Vamvas and Sennrich (2020) proposed such a setup for three languages for stance in debates, Schick andSchütze (2021) explored few-shot learning, andHardalov et al. (2022) extended that paradigm with sentiment and stance pre-training and evaluated on twelve languages from various domains. Since cultural norms and expressed linguistic phenomena play a crucial role in understanding the context of a claim (Sap et al., 2019), we do not argue for a completely language-agnostic framework. Yet, empirically, training in cross-lingual setups improves performance by leveraging better representations learned on a similar language or by acting as a regulariser.

Modelling the Context Modelling the context is a particularly important, yet challenging task. In many cases, there is a need to consider the background of the stance-taker as well as the characteristics of the targeted object. In particular, in the context of social media, one can provide information about the users such as their previous activity, other users they interact most with, the threads they participate in, or even their interests (Zubiaga et al., 2016b;Gorrell et al., 2019;Li et al., 2019b). The context of the stance expressed in news articles is related to the features of the media outlets, such as source of funding, previously known biases, or credibility (Baly et al., 2019;Darwish et al., 2020;Stefanov et al., 2020;Baly et al., 2020). When using contextual information about the object, factual information about the real world, and the time of posting are all important. Incorporating these into a stance detection pipeline, while challenging, paves the way towards a robust detection process.

Multimodal Content Spreading mis-and disinformation through multiple modalities is becoming increasingly popular. One such example are deepfakes, i.e., synthetically created images or videos, in which (usually) the face of one person is replaced with another person's face. Another example are information propagation techniques such as memetic warfare. Hence, it is increasingly important to combine different modalities to understand the full context stance is being expressed in. Some work in this area is on fake news detection for images (Nakamura et al., 2020), claim verification for images (Zlatkova et al., 2019), or searching for fact-checked information to alleviate the spread of fake news (Vo and Lee, 2020). There has been work on meme analysis for related tasks: detecting hateful (Kiela et al., 2020), harmful (Pramanick et al., 2021;Sharma et al., 2022a), and propagandistic memes (Dimitrov et al., 2021a,b); see also a recent survey of harmful memes (Sharma et al., 2022b). This line of research is especially relevant for mis-and disinformation tasks that depend on the wisdom of the crowd in social media as it adds additional information sources (Qazvinian et al., 2011;Zubiaga et al., 2016b;Derczynski et al., 2017;Hossain et al., 2020); see Section 5.


## Shades of Truth

The notion of shades of truth is important in mis-and disinformation detection. For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false. We believe that such shades could be applied to stance and used in a larger pipeline. In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).

Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance. As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance. Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.


## Explainability

The ability for a model to be able to explain its decisions is getting increasingly important, especially for mis-and disinformation detection, as one could argue that it is a crucial step towards adopting fully automated fact-checking. The FEVER 2.0 task formulation (Thorne et al., 2019) can be viewed as a step towards obtaining such explanations, e.g., there have been efforts to identify adversarial triggers that offer explanations for the vulnerabilities at the model level (Atanasova et al., 2020b). However, FEVER is artificially created and is limited to Wikipedia, which may not reflect real-world settings. To mitigate this, explanation by professional journalists can be found on fact-checking websites, and can be further combined with stance detection in an automated system. In a step in this direction, Atanasova et al. (2020a) generated natural language explanations for claims from PolitiFact 4 given gold evidence document summaries by journalists.

Moreover, partial explanations can be obtained automatically from the underlying models, e.g., from memory networks (Mohtarami et al., 2018), attention weights (Zhou et al., 2019;Liu et al., 2020b), or topic relations (Si et al., 2021). However, such approaches are limited as they can require gold snippets justifying the document's stance, attention weights can be misleading (Jain and Wallace, 2019), and topics might be noisy due to their unsupervised nature. Other existing systems (Popat et al., 2017(Popat et al., , 2018Nadeem et al., 2019) offer explanations to a more limited extent, highlighting span overlaps between the target text and the evidence documents. Overall, there is a need for holistic and realistic explanations of how a factchecking model arrived at its prediction.

Integration People question false information more and tend to confirm true information (Mendoza et al., 2010). Thus, stance can play a vital role in verifying dubious content. In Appendix C, we discuss existing systems and real-world applications of stance for mis-and disinformation identification in more detail. However, we argue that a tighter integration between stance and factchecking is needed. Stance can be expressed in different forms, e.g., tweets, news articles, user posts, sentences in Wikipedia, and Wiki tables, among others and can have different formulations as part of the fact-checking pipeline (see Section 3). All these can guide human fact-checkers through the process of fact-checking, and can point them to relevant evidence. Moreover, the wisdom of the crowd can be a powerful instrument in the fight against mis-and disinformation (Pennycook and Rand, 2019), but we should note that vocal minorities can derail public discourse (Scannell et al., 2021). Nevertheless, these risks can be mitigated by taking into account the credibility of the user or of the information source, which can be done automatically or with the help of human fact-checkers.


# Conclusion

We surveyed the current state-of-the-art in stance detection for mis-and disinformation detection. We explored applications of stance for detecting fake news, verifying rumours, identifying misconceptions, and fact-checking. We also discussed existing approaches used in different aspects of the aforementioned tasks, and we outlined several interesting phenomena, which we summarised as lessons learned and promising future trends. 


## References


## A Examples of Stance

As outlined in Section 3, there are different formulations in which the task of stance definition is materialised. In Table 3, we present some instances of these as exemplified by different stance detection datasets. The target with respect to which the stance is assessed can vary, e.g., a headline, a comment, a claim, a topic, etc., which in turn can differ in length and form. Moreover, the context where the stance is expressed can vary not only in its domain, e.g., News in (Ferreira and Vlachos, 2016) and Twitter in (Qazvinian et al., 2011), but also in its structure, as seen in the example of multiple evidence sentences in  and threaded comments in (Gorrell et al., 2019). In a more detailed view of Table 3, we see that each group of examples has its own important specifics that alter the task of stance detection for mis-and disinformation detection. Figure 3a shows an example from the News domain, where we have a headline and an entire article body, and the goal is to find how the two are related in terms of the body's stance(s) towards the headline. In this scenario, the models need to be able to handle very long documents, on one hand, and on the other to reason over multiple fragments of the input text, which might potentially express different stances. It is possible to simplify the task by extracting a summary of the news article beforehand, and evaluating only the stance of that summary, as shown in Figure 3d. However, obtaining such summaries is not a trivial task: (a) they can be extracted by a human annotator (e.g., a journalist), which is time-consuming and expensive, and can require a priori knowledge about the headline/topic of interest as the article might have more than one highlight or viewpoint, or (b) they can be automatically generated using text summarisation methods, but the result can be noisy.

Stance is often expressed in social media such as Twitter, Facebook, Reddit, etc. We illustrate two such scenarios in Figures 3b and 3e. In contrast to the usually long and well-written news documents, social media posts are mostly short and depend on additional context such as the previous posts in a conversational thread (Figure 3e), or external URLs and implicit topics (Figure 3b). Moreover, these texts also need normalisation, as users tend to use slurs, emojis, and other informal language.


## B Additional Formulations of Stance as a Component for Fact-Checking

Beyond the approaches that we outlined in Section 3.2, stance has also been used for detecting misconceptions and for profiling media sources as part of a fact-checking pipeline. Below, we describe some work that follows these formulations.

Misconceptions Hossain et al. (2020) focused on detecting misinformation related to COVID-19, based on known misconceptions listed in Wikipedia. They evaluated the veracity of a tweet depending on whether it agrees, disagrees, or has no stance with respect to a set of misconceptions. A related formulation of the task is detecting previously fact-checked claims (Shaar et al., 2020). This allows to assess the veracity of dubious content by evaluating the stance of a claim regarding already checked stories, known misconceptions, and facts.

Media Profiling Stance detection has also been used for media profiling. Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics. They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using the procedure proposed by Darwish et al. (2020). This is an important step towards understanding media biases. Tweet: Wow, that is fascinating! I hope you never mock our proud Scandi heritage again.

(b) Examples from Qazvinian et al. (2011) andDerczynski et al. (2017) Claim: The Rodney King riots took place in the most populous county in the USA. ɀiki Evidence 1: The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992. ɀiki Evidence 2: Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.

(c) Example from  Headline: Jess Smith of Chatham, Kent was the smiling sun baby in the Teletubbies TV show ǌ Summary 1: Canterbury Christ Church University student Jess Smith, from Chatham, starred as Teletubbies sun ǌ Summary 2: This College Student Claims She Was The Teletubbies Sun Baby (d) Example from Ferreira and Vlachos (2016) u1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News u2: @u1 not ISIS flags u3: @u1 sorry -how do you know its an ISIS flag? Can you actually confirm that? ɳ u4: @u3 no she cant cos its actually not u5: @u1 More on situation at Martin Place in Sydney, AU LINK u6: @u1 Have you actually confirmed its an ISIS flag or are you talking shit ɳ  Table 3: Illustrative examples for different stance detection scenarios included in our survey. We annotate the expressed stance with (support, for), (deny, against), ɳ (query), and (comment).

The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015;Popat et al., 2017Popat et al., , 2018. The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed. The trustworthiness of Web sources has also been studied from a data analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims.

More recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check, 5 and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information. In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.  (Nakov et al., 2021) for a recent survey on media profiling.

There is a well-known connection between factuality and bias. 6 For example, hyper-partisanship is often linked to low trustworthiness (Potthast et al., 2018), e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and also more trustworthy.

User Profiling In the case of social media and community fora, it is important to model the trustworthiness of the user. In particular, there has been research on finding opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a), sockpuppets (Maity et al., 2017Kumar et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017).

## Figure 1 :
1Two stance detection formulations.

## Figure 2 :
2Types of stance. The Target is the object of the stance expressed in the Context.


Dungs et al. (2018) continued this line of research, but focused on the effectiveness of stance for predicting rumour veracity. Hartmann et al. (2019) explored the flow of (dis-)information on Twitter after the MH17 Plane Crash. The two RumourEval (Derczynski et al., 2017; Gorrell et al., 2019) shared tasks on automated claim validation aimed to identify and handle rumours based on user reactions and ensuing conversations in social media, offering annotations for both stance and veracity. The two editions of Ru-mourEval were similar in spirit, with the second one providing more tweets and also additionally Reddit posts. RumourEval demonstrated the importance of modelling the context of a story instead of drawing conclusions based on a single post.


(2018b);Yoneda et al. (2018);Nie et al. (2019) used LSTM-based models for natural language inference, e.g., enhanced sequential inference model (ESIM Chen et al.(2017)).Nie et al. (2019) proposed a neural semantic matching network, which ranked first in the competition, achieving 64.2 FEVER score. They used page view frequency and WordNet features in addition to pre-trained contextualized embeddings(Peters et al., 2018).


experimented with cross-domain learning from 16 stance detection datasets. They proposed a novel architecture (MoLE) that applies domain adaptation at different stages of the modelling process (Luo et al., 2002): feature-level (Guo et al., 2018; Wright and Augenstein, 2020) and decision-level (Ganin and Lempitsky, 2015). They further integrated label embeddings (Augenstein et al., 2018), and eventually developed an end-toend unsupervised framework for predicting stance from a set of unseen target labels. Hardalov et al. (2022) explored PET


(e) Example fromGorrell et al. (2019)    


5 http://mediabiasfactcheck.com Baly et al. (2020) extended the information sources to include Facebook followers and speech signals from the news medium's channel on YouTube. Finally, Hounsel et al. (2020) proposed to use domain, certificate, and hosting information about the infrastructure of the website. See

## Table 2 :
2State-of-the-art results on the stance detection 
datasets. Note that some papers round their results to 
integers, and thus we put '?' for them.  *  Extracted from 
the FEVER leaderboard. 2 




Qian Liu, Bei Chen, Jiaqi Guo, Zeqi Lin, and Jianguang Lou. 2020a. TAPEX:Table pre-training via learning a neural SQL executor. In Proceedings of the 10th International Conference on Learning Representations, ICLR '22, Virtual. Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019. Multi-task deep neural networks for natural language understanding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, ACL '19, pages 4487-4496, Florence, Italy. Zhenghao Liu, Chenyan Xiong, Maosong Sun, and Zhiyuan Liu. 2020b. Fine-grained fact verification with kernel graph attention network. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL '20, pages 7342-7351, Online. Michal Lukasik, P. K. Srijith, Duy Vu, Kalina Bontcheva, Arkaitz Zubiaga, and Trevor Cohn. 2016. Hawkes processes for continuous time sequence classification: an application to rumour stance classification in Twitter. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, ACL '16, pages 393-398, Berlin, Germany. R.C. Luo, Chih-Chen Yih, and Kuo Lan Su. 2002. Multisensor fusion and integration: approaches, applications, and future research directions. IEEE Sensors Journal, 2(2):107-119. Jing Ma and Wei Gao. 2020. Debunking rumors on Twitter with tree transformer. In Proceedings of the 28th International Conference on Computational Linguistics, COLING '20, pages 5455-5466, Barcelona, Spain (Online). Suman Kalyan Maity, Aishik Chakraborty, Pawan Goyal, and Animesh Mukherjee. 2017. Detection of sockpuppets in social media. In Proceedings of the ACM Conference on Computer Supported Cooperative Work and Social Computing, CSCW '17, Portland, Oregon, USA. Christopher Malon. 2018. Team Papelo: Transformer networks at FEVER. In Proceedings of the First Workshop on Fact Extraction and VERification, FEVER '18, pages 109-113, Brussels, Belgium. Marcelo Mendoza, Barbara Poblete, and Carlos Castillo. 2010. Twitter under crisis: Can we trust what we RT? In Proceedings of the First Workshop on Social Media Analytics, SOMA '10, pages 71--79, Washington D.C., District of Columbia. Todor Mihaylov, Georgi Georgiev, and Preslav Nakov. 2015a. Finding opinion manipulation trolls in news community forums. In Proceedings of the Nineteenth Conference on Computational Natural Language Learning, pages 310-314, Beijing, China. Todor Mihaylov, Ivan Koychev, Georgi Georgiev, and Preslav Nakov. 2015b. Exposing paid opinion manipulation trolls. In Proceedings of the International Conference Recent Advances in Natural Language Processing, pages 443-450, Hissar, Bulgaria. Saif Mohammad, Svetlana Kiritchenko, Parinaz Sobhani, Xiaodan Zhu, and Colin Cherry. 2016. SemEval-2016 task 6: Detecting stance in tweets. In Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval '16, pages 31-41, San Diego, California, USA. Mitra Mohtarami, Ramy Baly, James Glass, Preslav Nakov, Lluís Màrquez, and Alessandro Moschitti. 2018. Automatic stance detection using end-to-end memory networks. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '18, pages 767-776, New Orleans, Louisiana, USA.Ahmet Aker, Leon Derczynski, and Kalina Bontcheva. 
2017. Simple open stance classification for rumour 
analysis. In Proceedings of the International Con-
ference Recent Advances in Natural Language Pro-
cessing, RANLP '17, pages 31-39, Varna, Bulgaria. 

Firoj Alam, Fahim Dalvi, Shaden Shaar, Nadir Dur-
rani, Hamdy Mubarak, Alex Nikolov, Giovanni 
Da San Martino, Ahmed Abdelali, Hassan Sajjad, 
Kareem Darwish, and Preslav Nakov. 2021. Fight-
ing the COVID-19 infodemic in social media: A 
holistic perspective and a call to arms. Proceedings 
of the International AAAI Conference on Web and 
Social Media, 15(1):913-922. 

Abeer Aldayel and Walid Magdy. 2021. 
Stance 
detection on social media: State of the art and 
trends. Information Processing & Management, 
58(4):102597. 

Tariq Alhindi, Amal Alabdulkarim, Ali Alshehri, 
Muhammad Abdul-Mageed, and Preslav Nakov. 
2021. AraStance: A multi-country and multi-
domain dataset of Arabic stance detection for fact 
checking. In Proceedings of the Fourth Workshop 
on NLP for Internet Freedom: Censorship, Disinfor-
mation, and Propaganda, NLP4IF '21, pages 57-65. 

Atanas Atanasov, Gianmarco De Francisci Morales, 
and Preslav Nakov. 2019. Predicting the role of 
political trolls in social media. In Proceedings 
of the 23rd Conference on Computational Natural 
Language Learning, CoNLL '19, pages 1023-1034, 
Hong Kong, China. 

Pepa Atanasova, Jakob Grue Simonsen, Christina Li-
oma, and Isabelle Augenstein. 2020a. Generating 
fact checking explanations. In Proceedings of the 
58th Annual Meeting of the Association for Compu-
tational Linguistics, ACL '20, pages 7352-7364. 

Pepa Atanasova, Jakob Grue Simonsen, Christina Li-
oma, and Isabelle Augenstein. 2022. Fact checking 
with insufficient evidence. Transactions of the Asso-
ciation for Computational Linguistics. 

Pepa Atanasova, Dustin Wright, and Isabelle Augen-
stein. 2020b. Generating label cohesive and well-
formed adversarial claims. In Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP '20, pages 3168-3177. 

Isabelle Augenstein, Tim Rocktäschel, Andreas Vla-
chos, and Kalina Bontcheva. 2016. Stance detec-
tion with bidirectional conditional encoding. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP '16, 
pages 876-885, Austin, Texas, USA. 

Isabelle Augenstein, Sebastian Ruder, and Anders 
Søgaard. 2018. Multi-task learning of pairwise 
sequence classification tasks over disparate label 
spaces. In Proceedings of the 2018 Conference of 
the North American Chapter of the Association for 
Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT '18, pages 1896-1906, New 
Orleans, Louisiana, USA. 

Ramy Baly, Georgi Karadzhov, Dimitar Alexandrov, 
James Glass, and Preslav Nakov. 2018a. Predict-
ing factuality of reporting and bias of news media 
sources. In Proceedings of the 2018 Conference on 
Empirical Methods in Natural Language Processing, 
pages 3528-3539, Brussels, Belgium. 

Ramy Baly, Georgi Karadzhov, Jisun An, Haewoon 
Kwak, Yoan Dinkov, Ahmed Ali, James Glass, and 
Preslav Nakov. 2020. What was written vs. who 
read it: News media profiling using text analysis and 
social media context. In Proceedings of the 58th An-
nual Meeting of the Association for Computational 
Linguistics, ACL '20, pages 3364-3374, Online. 

Ramy Baly, Georgi Karadzhov, Abdelrhman Saleh, 
James Glass, and Preslav Nakov. 2019. Multi-task 
ordinal regression for jointly predicting the trustwor-
thiness and the leading political ideology of news 
media. In Proceedings of the 2019 Conference of 
the North American Chapter of the Association for 
Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT '19, pages 2109-2116, Min-
neapolis, Minnesota, USA. 

Ramy Baly, Mitra Mohtarami, James Glass, Lluís 
Màrquez, Alessandro Moschitti, and Preslav Nakov. 
2018b. Integrating stance detection and fact check-
ing in a unified corpus. In Proceedings of the 2018 
Conference of the North American Chapter of the 
Association for Computational Linguistics: Human 
Language Technologies, NAACL-HLT '18, pages 
21-27, New Orleans, Louisiana, USA. 

Douglas Biber and Edward Finegan. 1988. Adver-
bial stance types in English. Discourse Processes, 
11(1):1-34. 

Mihaela Bošnjak and Mladen Karan. 2019. Data set for 
stance and sentiment analysis from user comments 
on Croatian news. In Proceedings of the 7th Work-
shop on Balto-Slavic Natural Language Processing, 
BSNLP '19, pages 50-55, Florence, Italy. 

Samuel R. Bowman, Gabor Angeli, Christopher Potts, 
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
ence. In Proceedings of the 2015 Conference on 
Empirical Methods in Natural Language Processing, 
EMNLP '15, pages 632-642, Lisbon, Portugal. 

Cheng Chen, Kui Wu, Venkatesh Srinivasan, and 
Xudong Zhang. 2013. Battling the internet water 
army: Detection of hidden paid posters. In Pro-
ceedings of the 2013 IEEE/ACM International Con-
ference on Advances in Social Networks Analysis 
and Mining, ASONAM '13, pages 116-120, Nia-
gara, Ontario, Canada. 

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui 
Jiang, and Diana Inkpen. 2017. Enhanced LSTM 
for natural language inference. In Proceedings of 
the 55th Annual Meeting of the Association for Com-
putational Linguistics, ACL '17, pages 1657-1668, 
Vancouver, Canada. 

Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai 
Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and 
William Yang Wang. 2020. Tabfact: A large-scale 
dataset for table-based fact verification. In 8th Inter-
national Conference on Learning Representations, 
ICLR '20, Addis Ababa, Ethiopia. 

Kareem Darwish, Dimitar Alexandrov, Preslav Nakov, 
and Yelena Mejova. 2017. Seminar users in the 
Arabic Twitter sphere. 
In Proceedings of the 
9th International Conference on Social Informatics, 
SocInfo '17, pages 91-108, Oxford, UK. 

Kareem Darwish, Peter Stefanov, Michaël Aupetit, and 
Preslav Nakov. 2020. Unsupervised user stance de-
tection on Twitter. In Proceedings of the Interna-
tional AAAI Conference on Web and Social Media, 
volume 14 of ICWSM '20, pages 141-152. 

Leon Derczynski, Kalina Bontcheva, Maria Liakata, 
Rob Procter, Geraldine Wong Sak Hoi, and Arkaitz 
Zubiaga. 2017. SemEval-2017 task 8: RumourEval: 
Determining rumour veracity and support for ru-
mours. In Proceedings of the 11th International 
Workshop on Semantic Evaluation, SemEval '17, 
pages 69-76, Vancouver, Canada. 

Dimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj 
Alam, Fabrizio Silvestri, Hamed Firooz, Preslav 
Nakov, and Giovanni Da San Martino. 2021a. De-
tecting propaganda techniques in memes. In Pro-
ceedings of the Joint Conference of the 59th Annual 
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference 
on Natural Language Processing, ACL-IJCNLP '21, 
pages 6603-6617. 

Dimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj 
Alam, Fabrizio Silvestri, Hamed Firooz, Preslav 
Nakov, and Giovanni Da San Martino. 2021b. 
SemEval-2021 task 6: Detection of persuasion tech-
niques in texts and images. In Proceedings of the 
15th International Workshop on Semantic Evalua-
tion, SemEval '21, pages 70-98, Online. 

Xin Luna Dong, Evgeniy Gabrilovich, Kevin Murphy, 
Van Dang, Wilko Horn, Camillo Lugaresi, Shaohua 
Sun, and Wei Zhang. 2015. Knowledge-based trust: 
Estimating the trustworthiness of web sources. Proc. 
VLDB Endow., 8(9):938-949. 

John W Du Bois. 2007. The stance triangle. Stanc-
etaking in discourse: Subjectivity, evaluation, inter-
action, 164(3):139-182. 

Sebastian Dungs, Ahmet Aker, Norbert Fuhr, and 
Kalina Bontcheva. 2018. Can rumour stance alone 
predict veracity? In Proceedings of the 27th In-
ternational Conference on Computational Linguis-
tics, COLING '18, pages 3360-3370, Santa Fe, New 
Mexico, USA. 

Wei Fang, Moin Nadeem, Mitra Mohtarami, and James 
Glass. 2019. Neural multi-task learning for stance 
prediction. In Proceedings of the Second Workshop 
on Fact Extraction and VERification, FEVER '19, 
pages 13-19, Hong Kong, China. 

William Ferreira and Andreas Vlachos. 2016. Emer-
gent: a novel data-set for stance classification. In 
Proceedings of the 2016 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, 
NAACL-HLT '16, pages 1163-1168, San Diego, 
California, USA. 

Yaroslav Ganin and Victor S. Lempitsky. 2015. Un-
supervised domain adaptation by backpropagation. 
In Proceedings of the 32nd International Confer-
ence on Machine Learning, volume 37 of ICML''15, 
pages 1180-1189, Lille, France. 

Bilal Ghanem, Paolo Rosso, and Francisco Rangel. 
2018. 
Stance detection in fake news a com-
bined feature representation. In Proceedings of the 
First Workshop on Fact Extraction and VERification, 
FEVER '18, pages 66-71, Brussels, Belgium. 

Genevieve Gorrell, Elena Kochkina, Maria Liakata, 
Ahmet Aker, Arkaitz Zubiaga, Kalina Bontcheva, 
and Leon Derczynski. 2019. SemEval-2019 task 7: 
RumourEval, determining rumour veracity and sup-
port for rumours. In Proceedings of the 13th In-
ternational Workshop on Semantic Evaluation, Se-
mEval '17, pages 845-854, Minneapolis, Minnesota, 
USA. 

Maike Guderlei and Matthias Aßenmacher. 2020. Eval-
uating unsupervised representation learning for de-
tecting stances of fake news. 
In Proceedings 
of the 28th International Conference on Computa-
tional Linguistics, COLING '20, pages 6339-6349, 
Barcelona, Spain (Online). 

Jiang Guo, Darsh Shah, and Regina Barzilay. 2018. 
Multi-source domain adaptation with mixture of ex-
perts. In Proceedings of the 2018 Conference on 
Empirical Methods in Natural Language Processing, 
EMNLP '18, pages 4694-4703, Brussels, Belgium. 

Zhijiang Guo, Michael Schlichtkrull, and Andreas Vla-
chos. 2022. A Survey on Automated Fact-Checking. 
Transactions of the Association for Computational 
Linguistics, 10:178-206. 

Ivan Habernal, Henning Wachsmuth, Iryna Gurevych, 
and Benno Stein. 2018. The argument reasoning 
comprehension task: Identification and reconstruc-
tion of implicit warrants. In Proceedings of the 2018 
Conference of the North American Chapter of the 
Association for Computational Linguistics: Human 
Language Technologies, NAACL-HLT '18, pages 
1930-1940, New Orleans, Louisiana, USA. 

Andreas Hanselowski, Avinesh PVS, Benjamin 
Schiller, Felix Caspelherr, Debanjan Chaudhuri, 
Christian M. Meyer, and Iryna Gurevych. 2018a. A 
retrospective analysis of the fake news challenge 
stance-detection task. In Proceedings of the 27th 
International Conference on Computational Lin-
guistics, COLING '18, pages 1859-1874, Santa Fe, 
New Mexico, USA. 

Andreas Hanselowski, Christian Stab, Claudia Schulz, 
Zile Li, and Iryna Gurevych. 2019. A richly anno-
tated corpus for different tasks in automated fact-
checking. In Proceedings of the 23rd Confer-
ence on Computational Natural Language Learning, 
CoNLL '19, pages 493-503, Hong Kong, China. 

Andreas Hanselowski, Hao Zhang, Zile Li, Daniil 
Sorokin, Benjamin Schiller, Claudia Schulz, and 
Iryna Gurevych. 2018b. 
UKP-Athene: Multi-
sentence textual entailment for claim verification. In 
Proceedings of the First Workshop on Fact Extrac-
tion and VERification, FEVER '18, pages 103-108, 
Brussels, Belgium. 

Momchil Hardalov, Arnav Arora, Preslav Nakov, and 
Isabelle Augenstein. 2021. Cross-domain label-
adaptive stance detection. In Proceedings of the 
2021 Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP '21, pages 9011-
9028, Online and Punta Cana, Dominican Republic. 

Momchil Hardalov, Arnav Arora, Preslav Nakov, and 
Isabelle Augenstein. 2022. Few-shot cross-lingual 
stance detection with sentiment-based pre-training. 
In Proceedings of the Thirty-Sixth AAAI Conference 
on Artificial Intelligence, Online. 

Mareike Hartmann, Yevgeniy Golovchenko, and 
Isabelle Augenstein. 2019. 
Mapping (dis-
)information flow about the MH17 plane crash. 
In Proceedings of the Second Workshop on Natural 
Language Processing for Internet Freedom: Censor-
ship, Disinformation, and Propaganda, NLP4IF '19, 
pages 45-55, Hong Kong, China. 

Kazi Saidul Hasan and Vincent Ng. 2014. Why are 
you taking this stance? Identifying and classifying 
reasons in ideological debates. In Proceedings of the 
2014 Conference on Empirical Methods in Natural 
Language Processing, EMNLP '14, pages 751-762, 
Doha, Qatar. 

Tamanna Hossain, Robert L. Logan IV, Arjuna Ugarte, 
Yoshitomo Matsubara, Sean Young, and Sameer 
Singh. 2020. COVIDLies: Detecting COVID-19 
misinformation on social media. In Proceedings of 
the 1st Workshop on NLP for COVID-19 (Part 2) at 
EMNLP 2020, NLP-COVID19 '20, Online. 

Austin Hounsel, Jordan Holland, Ben Kaiser, Kevin 
Borgolte, Nick Feamster, and Jonathan Mayer. 2020. 
Identifying disinformation websites using infrastruc-
ture features. In Proceedings of the 10th USENIX 
Workshop on Free and Open Communications on the 
Internet, FOCI '20. 

Sarthak Jain and Byron C. Wallace. 2019. Attention is 
not Explanation. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, NAACL-HLT '19, pages 3543-
3556, Minneapolis, Minnesota, USA. 

Jude Khouja. 2020. Stance prediction and claim veri-
fication: An Arabic perspective. In Proceedings of 
the Third Workshop on Fact Extraction and VERifi-
cation, FEVER '20, pages 8-17, Online. 

Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj 
Goswami, Amanpreet Singh, Pratik Ringshia, and 
Davide Testuggine. 2020. The hateful memes chal-
lenge: Detecting hate speech in multimodal memes. 
In Advances in Neural Information Processing Sys-
tems 33: Annual Conference on Neural Informa-
tion Processing Systems, volume 33 of NeurIPS '20, 
pages 2611-2624. 

Elena Kochkina, Maria Liakata, and Isabelle Augen-
stein. 2017. Turing at SemEval-2017 task 8: Se-
quential approach to rumour stance classification 
with branch-LSTM. In Proceedings of the 11th In-
ternational Workshop on Semantic Evaluation, Se-
mEval '17, pages 475-480, Vancouver, Canada. 

Elena Kochkina, Maria Liakata, and Arkaitz Zubi-
aga. 2018. All-in-one: Multi-task learning for ru-
mour verification. In Proceedings of the 27th In-
ternational Conference on Computational Linguis-
tics, COLING '18, pages 3402-3413, Santa Fe, New 
Mexico, USA. 

Dilek Küçük and Fazli Can. 2020. Stance detection: A 
survey. ACM Comput. Surv., 53(1). 

Srijan Kumar, Justin Cheng, Jure Leskovec, and V. S. 
Subrahmanian. 2017. An army of me: Sockpuppets 
in online discussion communities. In Proceedings 
of the 26th International Conference on World Wide 
Web, WWW '17, pages 857-866, Perth, Australia. 

Sumeet Kumar and Kathleen Carley. 2019. Tree 
LSTMs with convolution units to predict stance and 
rumor veracity in social media conversations. In 
Proceedings of the 57th Annual Meeting of the As-
sociation for Computational Linguistics, ACL '19, 
pages 5047-5058, Florence, Italy. 

Nayeon Lee, Yejin Bang, Andrea Madotto, and Pas-
cale Fung. 2021. Towards few-shot fact-checking 
via perplexity. In Proceedings of the Conference of 
the North American Chapter of the Association for 
Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT '21, pages 1971-1981. 

Nayeon Lee, Belinda Z. Li, Sinong Wang, Wen-tau 
Yih, Hao Ma, and Madian Khabsa. 2020. Language 
models as fact checkers? In Proceedings of the 
Third Workshop on Fact Extraction and VERifica-
tion, FEVER '20, pages 36-41, Online. 

Jiawen Li, Yudianto Sujana, and Hung-Yu Kao. 2020. 
Exploiting microblog conversation structures to de-
tect rumors. In Proceedings of the 28th Inter-
national Conference on Computational Linguistics, 
COLING '20, pages 5420-5429, Barcelona, Spai. 

Quanzhi Li, Qiong Zhang, and Luo Si. 2019a. even-
tAI at SemEval-2019 task 7: Rumor detection on so-
cial media by exploiting content, user credibility and 
propagation information. In Proceedings of the 13th 
International Workshop on Semantic Evaluation, Se-
mEval '19, pages 855-859, Minneapolis, Minnesota, 
USA. 

Quanzhi Li, Qiong Zhang, and Luo Si. 2019b. Rumor 
detection by exploiting user credibility information, 
attention and multi-task learning. In Proceedings of 
the 57th Annual Meeting of the Association for Com-
putational Linguistics, ACL '19, pages 1173-1179, 
Florence, Italy. 

Sizhen Li, Shuai Zhao, Bo Cheng, and Hao Yang. 2018. 
An end-to-end multi-task learning model for fact 
checking. In Proceedings of the First Workshop 
on Fact Extraction and VERification, FEVER '18, 
pages 138-144, Brussels, Belgium. 

Anders Edelbo Lillie, Emil Refsgaard Middelboe, and 
Leon Derczynski. 2019. Joint rumour stance and ve-
racity prediction. In Proceedings of the 22nd Nordic 
Conference on Computational Linguistics, NoDaL-
iDa '19, pages 208-221, Turku, Finland. 




Headline: Robert Plant Ripped up $800M Led Zeppelin Reunion Contract ǌ Body: ...Led Zeppelin's Robert Plant turned down £500 MILLION to reform supergroup.. Topic: Sarah Palin getting divorced? Tweet: OneRiot.com -Palin Denies First Dude Divorce Rumors http://url Topic: N/A (Implicit)(a) Example from Pomerleau and Rao (2017) 


The target can either be explicit, e.g., a topic such as Public Healthcare, or implicit, where only the context is present and the target is not directly available and is usually a topic(Derczynski et al., 2017; Gorrell et al., 2019), e.g., Germanwings, or 'Prince to play in Toronto'. When the target is implicit, the task becomes similar to sentiment analysis.
The result from dominiks can be found at https:// competitions.codalab.org/competitions/18814#results
Weighting is not trivial for some setups, e.g., threaded stance(Zubiaga et al., 2018b) 
http://www.politifact.com/
For illustrative purposes the text is trimmed to include only the relevant passage. Next, inFigure 3cwe highlight another interesting setup: claim verification using multiple pieces of evidence. Here, the reasoning is carried in multiple hops over a set of texts. In particular, there might not exists a single passage from a document/post that supports/refutes the claim directly. In that case, a large enough chain of evidence might be needed, which can cover enough contextual knowledge in order to allow the model (or a person) to assess the veracity of the input claim.Finally, the examples inFigure 3demonstrate that stance can be used for mis-and disinformation detection in different ways: (i) directly, as in the examples inFigures 3a and 3b, or (ii) as multiple viewpoints, which are later aggregated into a final decision, as inFigure 3c, 3d and 3e.We thoroughly discussed all of the aforementioned setups in Section 3, including the publicly available datasets that focus on stance in the context of mis-and disinformation identification.
http://www.poynter.org/fact-checking/media-literacy/ 2021/should-you-trust-media-bias-charts/
AcknowledgementsWe would like to thank the anonymous reviewers for their useful feedback. Isabelle Augenstein's research is partially funded by a DFF Sapere Aude research leader grant with grant number 0171-00034B. The work is also part of the Tanbih megaproject, which is developed at the Qatar Computing Research Institute, HBKU, and aims to limit the impact of "fake news," propaganda, and media bias by making users aware of what they are reading, thus promoting media literacy and critical thinking.C Systems and ApplicationsThe systems and applications below use stance detection as part of a pipeline for identifying misand disinformation, see Section 4 for more details about the methods.Wen et al. (2018)worked in a cross-lingual crossplatform rumour verification setup. They included multimodal content from fake and from real posts with images or videos shared on Twitter. They then collected supporting documents from two search engines, Google and Baidu, in English and Chinese, which they used for veracity evaluation. They trained their stance detection model on English data (FNC-1) using pre-trained multilingual sentence embeddings, and further added cross-platform features in their final neural model.Popat et al. (2018)proposed CredEye, 7 a system for automatic credibility assessment of textual claims. The system takes a claim as an input and analyses its credibility by considering relevant articles it retrieved from the Web, by combining the predicted stance of the articles regarding the claim with linguistic features to obtain a credibility score(Popat et al., 2017).Nguyen et al. (2018)designed a prototype factchecker Web tool.8Their system leverages a probabilistic graphical model to assess a claim's veracity taking into consideration the stance of multiple articles regarding this claim, the reputation of the news sources, and the annotators' reliability. In addition, it offers explanations to the fact-checkers based on the aforementioned features, which was shown to improve the overall user satisfaction and trust in the predictions.Zubiaga et al. (2018a)considered a four-step tracking process as a pipeline for rumour verificatioon: (i) rumour detection, i.e., given a stream of claims, determine whether they are worth verifying or they do contain no rumours, (ii) rumour tracking for finding relevant information about the rumour using social media posts, sentence descriptions, and keywords, (iii) stance classification to collect stances towards the rumour, and (iv) veracity classification to aggregate the information from the tracking component, the collected stances, and optionally other relevant information about the sources, metadata about the users, etc., to predict a truth value for the rumour. 7 https://gate.d5.mpi-inf.mpg.de/credeye/ 8 http://fcweb.pythonanywhere.com/Nadeem et al. (2019)developed FAKTA, a system for automatic end-to-end fact-checking of claims. It retrieves relevant articles from Wikipedia and as well as from selected media sources, which it then uses for verification. FAKTA uses a stance detection model, trained in a FEVER setting, to predict the stance and to obtain entailed spans. These predictions, combined with linguistic analysis, are used to provide both document-and sentence-level explanations and a factuality score.Nguyen et al. (2020)proposed the Factual News Graph (FANG) model, which models the social context for fake news detection. In particular, FANG uses the stance of user comments with respect to the target news article as an integral component of its model, together with temporality, useruser interactions, article-source interactions, as well a source reliability information.
Leveraging joint interactions for credibility analysis in news communities. Subhabrata Mukherjee, Gerhard Weikum, 10.1145/2806416.2806537Proceedings of the 24th ACM International Conference on Information and Knowledge Management, CIKM '15. the 24th ACM International Conference on Information and Knowledge Management, CIKM '15Melbourne, AustraliaSubhabrata Mukherjee and Gerhard Weikum. 2015. Leveraging joint interactions for credibility analysis in news communities. In Proceedings of the 24th ACM International Conference on Information and Knowledge Management, CIKM '15, pages 353- 362, Melbourne, Australia.

FAKTA: An automatic endto-end fact checking system. Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, James Glass, 10.18653/v1/N19-4014Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), NAACL-HLT '19. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), NAACL-HLT '19Minneapolis, Minnesota, USAMoin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, and James Glass. 2019. FAKTA: An automatic end- to-end fact checking system. In Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computational Linguistics (Demonstrations), NAACL-HLT '19, pages 78-83, Minneapolis, Minnesota, USA.

Fakeddit: A new multimodal benchmark dataset for fine-grained fake news detection. Kai Nakamura, Sharon Levy, William Yang Wang, Proceedings of the 12th Language Resources and Evaluation Conference, LREC '20. the 12th Language Resources and Evaluation Conference, LREC '20Marseille, FranceKai Nakamura, Sharon Levy, and William Yang Wang. 2020. Fakeddit: A new multimodal benchmark dataset for fine-grained fake news detection. In Pro- ceedings of the 12th Language Resources and Eval- uation Conference, LREC '20, pages 6149-6157, Marseille, France.

Overview of the CLEF-2018 CheckThat! lab on automatic identification and verification of political claims. Preslav Nakov, Alberto Barrón-Cedeño, Tamer Elsayed, Reem Suwaileh, Lluís Màrquez, Wajdi Zaghouani, Pepa Atanasova, Spas Kyuchukov, Giovanni Da San Martino, 10.1007/978-3-319-98932-7_32Experimental IR Meets Multilinguality, Multimodality, and Interaction, CLEF '18. Preslav Nakov, Alberto Barrón-Cedeño, Tamer El- sayed, Reem Suwaileh, Lluís Màrquez, Wajdi Za- ghouani, Pepa Atanasova, Spas Kyuchukov, and Giovanni Da San Martino. 2018. Overview of the CLEF-2018 CheckThat! lab on automatic identifica- tion and verification of political claims. In Experi- mental IR Meets Multilinguality, Multimodality, and Interaction, CLEF '18, pages 372-387.

Preslav Nakov, Taha Husrev, Sencar, arXiv/2103.12506Jisun An, and Haewoon Kwak. 2021. A survey on predicting the factuality and the bias of news media. Preslav Nakov, Husrev Taha Sencar, Jisun An, and Hae- woon Kwak. 2021. A survey on predicting the factu- ality and the bias of news media. arXiv/2103.12506.

An interpretable joint graphical model for fact-checking from crowds. An T Nguyen, Aditya Kharosekar, Matthew Lease, Byron C Wallace, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, AAAI '18. the Thirty-Second AAAI Conference on Artificial Intelligence, AAAI '18New Orleans, Louisiana, USAAn T. Nguyen, Aditya Kharosekar, Matthew Lease, and Byron C. Wallace. 2018. An interpretable joint graphical model for fact-checking from crowds. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, AAAI '18, pages 1511- 1518, New Orleans, Louisiana, USA.

FANG: leveraging social context for fake news detection using graph representation. Kazunari Van-Hoang Nguyen, Preslav Sugiyama, Min-Yen Nakov, Kan, 10.1145/3340531.3412046Proceedings of the ACM International Conference on Information and Knowledge Management, CIKM '20. the ACM International Conference on Information and Knowledge Management, CIKM '20IrelandVirtual EventVan-Hoang Nguyen, Kazunari Sugiyama, Preslav Nakov, and Min-Yen Kan. 2020. FANG: leveraging social context for fake news detection using graph representation. In Proceedings of the ACM Inter- national Conference on Information and Knowledge Management, CIKM '20, pages 1165-1174, Virtual Event, Ireland.

Combining fact extraction and verification with neural semantic matching networks. Yixin Nie, Haonan Chen, Mohit Bansal, 10.1609/aaai.v33i01.33016859Proceedings of the Thirty-Third Conference on Artificial Intelligence AAAI 2019, AAAI '19. the Thirty-Third Conference on Artificial Intelligence AAAI 2019, AAAI '19Honolulu, Hawaii, USAYixin Nie, Haonan Chen, and Mohit Bansal. 2019. Combining fact extraction and verification with neu- ral semantic matching networks. In Proceedings of the Thirty-Third Conference on Artificial Intel- ligence AAAI 2019, AAAI '19, pages 6859-6866, Honolulu, Hawaii, USA.

Multi-hop fact checking of political claims. Wojciech Ostrowski, Arnav Arora, Pepa Atanasova, Isabelle Augenstein, 10.24963/ijcai.2021/536Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence. the Thirtieth International Joint Conference on Artificial IntelligenceWojciech Ostrowski, Arnav Arora, Pepa Atanasova, and Isabelle Augenstein. 2021. Multi-hop fact checking of political claims. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, pages 3892-3898.

Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. Bo Pang, Lillian Lee, 10.3115/1219840.1219855Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL '05. the 43rd Annual Meeting of the Association for Computational Linguistics, ACL '05Ann Arbor, Michigan, USABo Pang and Lillian Lee. 2005. Seeing stars: Exploit- ing class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the Association for Compu- tational Linguistics, ACL '05, pages 115-124, Ann Arbor, Michigan, USA.

Fighting misinformation on social media using crowdsourced judgments of news source quality. Gordon Pennycook, David G Rand, 10.1073/pnas.1806781116Proceedings of the National Academy of Sciences. the National Academy of Sciences116Gordon Pennycook and David G. Rand. 2019. Fighting misinformation on social media using crowdsourced judgments of news source quality. Proceedings of the National Academy of Sciences, 116(7):2521- 2526.

Deep contextualized word representations. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, 10.18653/v1/N18-1202Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '18. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '18New Orleans, Louisiana, USAMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '18, pages 2227-2237, New Orleans, Louisiana, USA.

Fake news challenge stage 1 (FNC-I): Stance detection. Dean Pomerleau, Delip Rao, Dean Pomerleau and Delip Rao. 2017. Fake news challenge stage 1 (FNC-I): Stance detection. https://www.fakenewschallenge.org/.

Where the truth lies: Explaining the credibility of emerging claims on the Web and social media. Kashyap Popat, Subhabrata Mukherjee, Jannik Strötgen, Gerhard Weikum, 10.1145/3041021.3055133Proceedings of the 26th International Conference on World Wide Web Companion, WWW '17 Companion. the 26th International Conference on World Wide Web Companion, WWW '17 CompanionPerth, AustraliaKashyap Popat, Subhabrata Mukherjee, Jannik Ströt- gen, and Gerhard Weikum. 2017. Where the truth lies: Explaining the credibility of emerging claims on the Web and social media. In Proceed- ings of the 26th International Conference on World Wide Web Companion, WWW '17 Companion, page 1003-1012, Perth, Australia.

CredEye: A credibility lens for analyzing and explaining misinformation. Kashyap Popat, Subhabrata Mukherjee, Jannik Strötgen, Gerhard Weikum, 10.1145/3184558.3186967Companion Proceedings of The Web Conference 2018, WWW '18. Lyon, FranceKashyap Popat, Subhabrata Mukherjee, Jannik Ströt- gen, and Gerhard Weikum. 2018. CredEye: A credi- bility lens for analyzing and explaining misinforma- tion. In Companion Proceedings of The Web Confer- ence 2018, WWW '18, page 155-158, Lyon, France.

A stylometric inquiry into hyperpartisan and fake news. Martin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendorff, Benno Stein, 10.18653/v1/P18-1022Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL '18. the 56th Annual Meeting of the Association for Computational Linguistics, ACL '18Melbourne, AustraliaMartin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendorff, and Benno Stein. 2018. A stylo- metric inquiry into hyperpartisan and fake news. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics, ACL '18, pages 231-240, Melbourne, Australia.

Detecting harmful memes and their targets. Shraman Pramanick, Dimitar Dimitrov, Rituparna Mukherjee, Shivam Sharma, Md Shad Akhtar, Preslav Nakov, Tanmoy Chakraborty, 10.18653/v1/2021.findings-acl.246Findings of ACL-IJCNLP. Shraman Pramanick, Dimitar Dimitrov, Rituparna Mukherjee, Shivam Sharma, Md. Shad Akhtar, Preslav Nakov, and Tanmoy Chakraborty. 2021. De- tecting harmful memes and their targets. In Findings of ACL-IJCNLP, pages 2783-2796.

Rumor has it: Identifying misinformation in microblogs. Emily Vahed Qazvinian, Rosengren, R Dragomir, Qiaozhu Radev, Mei, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP '11. the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP '11Edinburgh, Scotland, UKVahed Qazvinian, Emily Rosengren, Dragomir R. Radev, and Qiaozhu Mei. 2011. Rumor has it: Iden- tifying misinformation in microblogs. In Proceed- ings of the 2011 Conference on Empirical Meth- ods in Natural Language Processing, EMNLP '11, pages 1589-1599, Edinburgh, Scotland, UK.

Truth of varying shades: Analyzing language in fake news and political fact-checking. Eunsol Hannah Rashkin, Jin Yea Choi, Svitlana Jang, Yejin Volkova, Choi, 10.18653/v1/D17-1317Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkHannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova, and Yejin Choi. 2017. Truth of varying shades: Analyzing language in fake news and politi- cal fact-checking. In Proceedings of the Conference on Empirical Methods in Natural Language Process- ing, pages 2931-2937, Copenhagen, Denmark.

A simple but tough-to-beat baseline for the Fake News Challenge stance detection task. Benjamin Riedel, Isabelle Augenstein, P Georgios, Sebastian Spithourakis, Riedel, arXiv:1707.03264arXiv preprintBenjamin Riedel, Isabelle Augenstein, Georgios P Sp- ithourakis, and Sebastian Riedel. 2017. A sim- ple but tough-to-beat baseline for the Fake News Challenge stance detection task. arXiv preprint arXiv:1707.03264.

SemEval-2017 task 4: Sentiment analysis in Twitter. Sara Rosenthal, Noura Farra, Preslav Nakov, 10.18653/v1/S17-2088Proceedings of the 11th International Workshop on Semantic Evaluation, SemEval '17. the 11th International Workshop on Semantic Evaluation, SemEval '17Vancouver, CanadaSara Rosenthal, Noura Farra, and Preslav Nakov. 2017. SemEval-2017 task 4: Sentiment analysis in Twitter. In Proceedings of the 11th International Workshop on Semantic Evaluation, SemEval '17, pages 502- 518, Vancouver, Canada.

BuzzFace: A news veracity dataset with Facebook user commentary and egos. Giovanni Santia, Jake Williams, Proceedings of the Twelfth international AAAI conference on web and social media. the Twelfth international AAAI conference on web and social mediaPalo Alto, California, USA12Giovanni Santia and Jake Williams. 2018. BuzzFace: A news veracity dataset with Facebook user com- mentary and egos. In Proceedings of the Twelfth in- ternational AAAI conference on web and social me- dia, volume 12 of ICWSM '18, pages 531-540, Palo Alto, California, USA.

The risk of racial bias in hate speech detection. Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, Noah A Smith, 10.18653/v1/P19-1163Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, ACL '19. the 57th Annual Meeting of the Association for Computational Linguistics, ACL '19Florence, ItalyMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A. Smith. 2019. The risk of racial bias in hate speech detection. In Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics, ACL '19, pages 1668-1678, Florence, Italy.

COVID-19 vaccine discourse on Twitter: A content analysis of persuasion techniques, sentiment and mis/disinformation. Denise Scannell, Linda Desens, Marie Guadagno, Yolande Tra, Emily Acker, Kate Sheridan, Margo Rosner, Jennifer Mathieu, Mike Fulk, 10.1080/10810730.2021.1955050Journal of Health Communication. 267Denise Scannell, Linda Desens, Marie Guadagno, Yolande Tra, Emily Acker, Kate Sheridan, Margo Rosner, Jennifer Mathieu, and Mike Fulk. 2021. COVID-19 vaccine discourse on Twitter: A content analysis of persuasion techniques, sentiment and mis/disinformation. Journal of Health Communica- tion, 26(7):443-459.

Exploiting cloze-questions for few-shot text classification and natural language inference. Timo Schick, Hinrich Schütze, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, ACL '21. the 16th Conference of the European Chapter of the Association for Computational Linguistics, ACL '21OnlineTimo Schick and Hinrich Schütze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the As- sociation for Computational Linguistics, ACL '21, pages 255-269, Online.

Stance detection benchmark: How robust is your stance detection? KI-Künstliche Intelligenz. Benjamin Schiller, Johannes Daxenberger, Iryna Gurevych, Benjamin Schiller, Johannes Daxenberger, and Iryna Gurevych. 2021. Stance detection benchmark: How robust is your stance detection? KI-Künstliche Intel- ligenz, pages 1-13.

. Shaden Shaar, Nikolay Babulkov, Giovanni , Shaden Shaar, Nikolay Babulkov, Giovanni

That is a known lie: Detecting previously fact-checked claims. Martino Da San, Preslav Nakov, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL '20. the 58th Annual Meeting of the Association for Computational Linguistics, ACL '20Da San Martino, and Preslav Nakov. 2020. That is a known lie: Detecting previously fact-checked claims. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL '20, pages 3607-3618.

DISARM: Detecting the victims targeted by harmful memes. Shivam Sharma, Shad Md, Preslav Akhtar, Tanmoy Nakov, Chakraborty, Findings of NAACL 2022. Seattle, Washington, USAShivam Sharma, Md Shad Akhtar, Preslav Nakov, and Tanmoy Chakraborty. 2022a. DISARM: Detecting the victims targeted by harmful memes. In Findings of NAACL 2022, Seattle, Washington, USA.

Chereen Shurafa, Kareem Darwish, and Wajdi Zaghouani. 2020. Political framing: US COVID19 blame game. Shivam Sharma, Firoj Alam, Md Shad Akhtar, Dimitar Dimitrov, Giovanni Da San, Hamed Martino, Alon Firooz, Fabrizio Halevy, Preslav Silvestri, Tanmoy Nakov, Chakraborty, Proceedings of the 31st International Joint Conference on Artificial Intelligence, IJCAI-ECAI '22. the 31st International Joint Conference on Artificial Intelligence, IJCAI-ECAI '22Vienna, AustriaSocial InformaticsShivam Sharma, Firoj Alam, Md. Shad Akhtar, Dimitar Dimitrov, Giovanni Da San Martino, Hamed Firooz, Alon Halevy, Fabrizio Silvestri, Preslav Nakov, and Tanmoy Chakraborty. 2022b. Detecting and under- standing harmful memes: A survey. In Proceedings of the 31st International Joint Conference on Artifi- cial Intelligence, IJCAI-ECAI '22, Vienna, Austria. Chereen Shurafa, Kareem Darwish, and Wajdi Za- ghouani. 2020. Political framing: US COVID19 blame game. In Social Informatics, pages 333-351.

Topic-aware evidence reasoning and stance-aware aggregation for fact verification. Jiasheng Si, Deyu Zhou, Tongzhe Li, Xingyu Shi, Yulan He, Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing, ACL-IJCNLP '21. the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing, ACL-IJCNLP '21Jiasheng Si, Deyu Zhou, Tongzhe Li, Xingyu Shi, and Yulan He. 2021. Topic-aware evidence reasoning and stance-aware aggregation for fact verification. In Proceedings of the Annual Meeting of the Asso- ciation for Computational Linguistics and the Inter- national Joint Conference on Natural Language Pro- cessing, ACL-IJCNLP '21, pages 1612-1622.

Transfer learning from transformers to fake news challenge stance detection (FNC-1) task. Valeriya Slovikovskaya, Giuseppe Attardi, Proceedings of the 12th Language Resources and Evaluation Conference, LREC '20. the 12th Language Resources and Evaluation Conference, LREC '20Marseille, FranceValeriya Slovikovskaya and Giuseppe Attardi. 2020. Transfer learning from transformers to fake news challenge stance detection (FNC-1) task. In Pro- ceedings of the 12th Language Resources and Eval- uation Conference, LREC '20, pages 1211-1218, Marseille, France.

Predicting the topical stance and political leaning of media using tweets. Peter Stefanov, Kareem Darwish, Atanas Atanasov, Preslav Nakov, 10.18653/v1/2020.acl-main.50Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL '20. the 58th Annual Meeting of the Association for Computational Linguistics, ACL '20OnlinePeter Stefanov, Kareem Darwish, Atanas Atanasov, and Preslav Nakov. 2020. Predicting the topical stance and political leaning of media using tweets. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL '20, pages 527-537, Online.

Automated fact checking: Task formulations, methods and future directions. James Thorne, Andreas Vlachos, Proceedings of the 27th International Conference on Computational Linguistics, COLING '18. the 27th International Conference on Computational Linguistics, COLING '18Santa Fe, New Mexico, USAJames Thorne and Andreas Vlachos. 2018. Automated fact checking: Task formulations, methods and fu- ture directions. In Proceedings of the 27th Inter- national Conference on Computational Linguistics, COLING '18, pages 3346-3359, Santa Fe, New Mexico, USA.

FEVER: a large-scale dataset for fact extraction and VERification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, 10.18653/v1/N18-1074Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '18. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '18New Orleans, Louisiana, USAJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '18, pages 809-819, New Orleans, Louisiana, USA.

Christos Christodoulopoulos, and Arpit Mittal. 2019. The FEVER2.0 shared task. James Thorne, Andreas Vlachos, Oana Cocarascu, 10.18653/v1/D19-6601Proceedings of the Second Workshop on Fact Extraction and VERification, FEVER '19. the Second Workshop on Fact Extraction and VERification, FEVER '19Hong Kong, ChinaJames Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos, and Arpit Mittal. 2019. The FEVER2.0 shared task. In Proceedings of the Second Workshop on Fact Extraction and VERifica- tion, FEVER '19, pages 1-6, Hong Kong, China.

Early detection of rumours on Twitter via stance transfer learning. Lin Tian, Xiuzhen Zhang, Yan Wang, Huan Liu, 10.1007/978-3-030-45439-5_38Advances in Information Retrieval, ECIR '20. ChamLin Tian, Xiuzhen Zhang, Yan Wang, and Huan Liu. 2020. Early detection of rumours on Twitter via stance transfer learning. In Advances in Information Retrieval, ECIR '20, pages 575-588, Cham.

X-Stance: A multilingual multi-target dataset for stance detection. Jannis Vamvas, Rico Sennrich, Proceedings of the 5th Swiss Text Analytics Conference (SwissText) & the 16th Conference on Natural Language Processing (KONVENS). the 5th Swiss Text Analytics Conference (SwissText) & the 16th Conference on Natural Language Processing (KONVENS)Zurich, SwitzerlandJannis Vamvas and Rico Sennrich. 2020. X-Stance: A multilingual multi-target dataset for stance detec- tion. In Proceedings of the 5th Swiss Text Analytics Conference (SwissText) & the 16th Conference on Natural Language Processing (KONVENS), Zurich, Switzerland.

A temporal attentional model for rumor stance classification. Ben Amir Pouran, Javid Veyseh, Dejing Ebrahimi, Daniel Dou, Lowd, 10.1145/3132847.3133116Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM '17. the 2017 ACM on Conference on Information and Knowledge Management, CIKM '17SingaporeAmir Pouran Ben Veyseh, Javid Ebrahimi, Dejing Dou, and Daniel Lowd. 2017. A temporal attentional model for rumor stance classification. In Proceed- ings of the 2017 ACM on Conference on Informa- tion and Knowledge Management, CIKM '17, pages 2335-2338, Singapore.

Where are the facts? Searching for fact-checked information to alleviate the spread of fake news. Nguyen Vo, Kyumin Lee, 10.18653/v1/2020.emnlp-main.621Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP '20. the Conference on Empirical Methods in Natural Language Processing, EMNLP '20OnlineNguyen Vo and Kyumin Lee. 2020. Where are the facts? Searching for fact-checked information to alleviate the spread of fake news. In Proceedings of the Conference on Empirical Methods in Natu- ral Language Processing, EMNLP '20, pages 7717- 7731, Online.

Liar, Liar Pants on Fire": A new benchmark dataset for fake news detection. William Yang, Wang , 10.18653/v1/P17-2067Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL '17. the 55th Annual Meeting of the Association for Computational Linguistics, ACL '17Vancouver, CanadaWilliam Yang Wang. 2017. "Liar, Liar Pants on Fire": A new benchmark dataset for fake news detection. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL '17, pages 422-426, Vancouver, Canada.

Robust reasoning over heterogeneous textual information for fact verification. Yongyue Wang, Chunhe Xia, Chengxiang Si, Beitong Yao, Tianbo Wang, 10.1109/ACCESS.2020.3019586IEEE Access. 8Yongyue Wang, Chunhe Xia, Chengxiang Si, Beitong Yao, and Tianbo Wang. 2020. Robust reasoning over heterogeneous textual information for fact verifica- tion. IEEE Access, 8:157140-157150.

Modeling conversation structure and temporal dynamics for jointly predicting rumor stance and veracity. Penghui Wei, Nan Xu, Wenji Mao, 10.18653/v1/D19-1485Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, China19Penghui Wei, Nan Xu, and Wenji Mao. 2019. Mod- eling conversation structure and temporal dynam- ics for jointly predicting rumor stance and veracity. In Proceedings of the 2019 Conference on Empiri- cal Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP '19, pages 4787-4798, Hong Kong, China.

Misinformation adoption or rejection in the era of COVID-19. Maxwell Weinzierl, Suellen Hopfer, Sanda M Harabagiu, Proceedings of the Fifteenth International AAAI Conference on Web and Social Media. the Fifteenth International AAAI Conference on Web and Social Media15VirtualMaxwell Weinzierl, Suellen Hopfer, and Sanda M. Harabagiu. 2021. Misinformation adoption or re- jection in the era of COVID-19. In Proceedings of the Fifteenth International AAAI Conference on Web and Social Media, volume 15 of ICWSM' 21, pages 787-795, Virtual.

Cross-lingual cross-platform rumor verification pivoting on multimedia content. Weiming Wen, Songwen Su, Zhou Yu, 10.18653/v1/D18-1385Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, Belgium18Weiming Wen, Songwen Su, and Zhou Yu. 2018. Cross-lingual cross-platform rumor verification piv- oting on multimedia content. In Proceedings of the 2018 Conference on Empirical Methods in Natu- ral Language Processing, EMNLP '18, pages 3487- 3496, Brussels, Belgium.

Transformer based multi-source domain adaptation. Dustin Wright, Isabelle Augenstein, 10.18653/v1/2020.emnlp-main.639Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP '20. the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP '20OnlineDustin Wright and Isabelle Augenstein. 2020. Trans- former based multi-source domain adaptation. In Proceedings of the 2020 Conference on Empir- ical Methods in Natural Language Processing, EMNLP '20, pages 7963-7974, Online.

BLCU_NLP at SemEval-2019 task 7: An inference chain-based GPT model for rumour evaluation. Ruoyao Yang, Wanying Xie, Chunhua Liu, Dong Yu, 10.18653/v1/S19-2191Proceedings of the 13th International Workshop on Semantic Evaluation, SemEval '19. the 13th International Workshop on Semantic Evaluation, SemEval '19Minneapolis, Minnesota, USARuoyao Yang, Wanying Xie, Chunhua Liu, and Dong Yu. 2019. BLCU_NLP at SemEval-2019 task 7: An inference chain-based GPT model for rumour eval- uation. In Proceedings of the 13th International Workshop on Semantic Evaluation, SemEval '19, pages 1090-1096, Minneapolis, Minnesota, USA.

Coreferential reasoning learning for language representation. Deming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu, Peng Li, Maosong Sun, Zhiyuan Liu, 10.18653/v1/2020.emnlp-main.582Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP '20. the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP '20OnlineDeming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu, Peng Li, Maosong Sun, and Zhiyuan Liu. 2020. Coref- erential reasoning learning for language representa- tion. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP '20, pages 7170-7186, Online.

UCL machine reading group: Four factor framework for fact finding (HexaF). Takuma Yoneda, Jeff Mitchell, Johannes Welbl, Pontus Stenetorp, Sebastian Riedel, 10.18653/v1/W18-5515Proceedings of the First Workshop on Fact Extraction and VERification, FEVER '18. the First Workshop on Fact Extraction and VERification, FEVER '18Brussels, BelgiumTakuma Yoneda, Jeff Mitchell, Johannes Welbl, Pon- tus Stenetorp, and Sebastian Riedel. 2018. UCL machine reading group: Four factor framework for fact finding (HexaF). In Proceedings of the First Workshop on Fact Extraction and VERification, FEVER '18, pages 97-102, Brussels, Belgium.

Coupled hierarchical transformer for stance-aware rumor verification in social media conversations. Jianfei Yu, Jing Jiang, Ling Min , Serena Khoo, Hai Leong Chieu, Rui Xia, 10.18653/v1/2020.emnlp-main.108Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP '20. the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP '20OnlineJianfei Yu, Jing Jiang, Ling Min Serena Khoo, Hai Leong Chieu, and Rui Xia. 2020. Coupled hi- erarchical transformer for stance-aware rumor verifi- cation in social media conversations. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP '20, pages 1392-1401, Online.

Transformer-XH: Multi-evidence reasoning with extra hop attention. Chen Zhao, Chenyan Xiong, Corby Rosset, Xia Song, Paul N Bennett, Saurabh Tiwary, Proceedings of the 8th International Conference on Learning Representations, ICLR '20. the 8th International Conference on Learning Representations, ICLR '20Addis Ababa, EthiopiaChen Zhao, Chenyan Xiong, Corby Rosset, Xia Song, Paul N. Bennett, and Saurabh Tiwary. 2020. Transformer-XH: Multi-evidence reasoning with ex- tra hop attention. In Proceedings of the 8th Inter- national Conference on Learning Representations, ICLR '20, Addis Ababa, Ethiopia.

Reasoning over semantic-level graph for fact checking. Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin, 10.18653/v1/2020.acl-main.549Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL '20. the 58th Annual Meeting of the Association for Computational Linguistics, ACL '20OnlineWanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. 2020. Reasoning over semantic-level graph for fact checking. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, ACL '20, pages 6170-6180, Online.

GEAR: Graph-based evidence aggregating and reasoning for fact verification. Jie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, Maosong Sun, 10.18653/v1/P19-1085Proceedings of the Annual Meeting of the Association for Computational Linguistics, ACL '19. the Annual Meeting of the Association for Computational Linguistics, ACL '19Florence, ItalyJie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2019. GEAR: Graph-based evidence aggregating and rea- soning for fact verification. In Proceedings of the Annual Meeting of the Association for Computa- tional Linguistics, ACL '19, pages 892-901, Flo- rence, Italy.

Fact-checking meets fauxtography: Verifying claims about images. Dimitrina Zlatkova, Preslav Nakov, Ivan Koychev, 10.18653/v1/D19-1216Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language ProcessingHong Kong, China19Dimitrina Zlatkova, Preslav Nakov, and Ivan Koychev. 2019. Fact-checking meets fauxtography: Verify- ing claims about images. In Proceedings of the Conference on Empirical Methods in Natural Lan- guage Processing and the International Joint Con- ference on Natural Language Processing, EMNLP- IJCNLP '19, pages 2099-2108, Hong Kong, China.

Detection and resolution of rumours in social media: A survey. Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva, Maria Liakata, Rob Procter, 10.1145/3161603ACM Comput. Surv. 512Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva, Maria Liakata, and Rob Procter. 2018a. Detection and resolution of rumours in social media: A survey. ACM Comput. Surv., 51(2).

Stance classification in rumours as a sequential task exploiting the tree structure of social media conversations. Arkaitz Zubiaga, Elena Kochkina, Maria Liakata, Rob Procter, Michal Lukasik, Proceedings of the 26th International Conference on Computational Linguistics, COLING '16. the 26th International Conference on Computational Linguistics, COLING '16Osaka, JapanArkaitz Zubiaga, Elena Kochkina, Maria Liakata, Rob Procter, and Michal Lukasik. 2016a. Stance classi- fication in rumours as a sequential task exploiting the tree structure of social media conversations. In Proceedings of the 26th International Conference on Computational Linguistics, COLING '16, pages 2438-2448, Osaka, Japan.

Discourseaware rumour stance classification in social media using sequential classifiers. Arkaitz Zubiaga, Elena Kochkina, Maria Liakata, Rob Procter, Michal Lukasik, Kalina Bontcheva, Trevor Cohn, Isabelle Augenstein, 10.1016/j.ipm.2017.11.009Information Processing & Management. 542Arkaitz Zubiaga, Elena Kochkina, Maria Liakata, Rob Procter, Michal Lukasik, Kalina Bontcheva, Trevor Cohn, and Isabelle Augenstein. 2018b. Discourse- aware rumour stance classification in social media using sequential classifiers. Information Processing & Management, 54(2):273-290.

Analysing how people orient to and spread rumours in social media by looking at conversational threads. Arkaitz Zubiaga, Maria Liakata, Rob Procter, 10.1371/journal.pone.0150989PLOS ONE. 113Geraldine Wong Sak Hoi, and Peter TolmieArkaitz Zubiaga, Maria Liakata, Rob Procter, Geral- dine Wong Sak Hoi, and Peter Tolmie. 2016b. Analysing how people orient to and spread rumours in social media by looking at conversational threads. PLOS ONE, 11(3):1-29.