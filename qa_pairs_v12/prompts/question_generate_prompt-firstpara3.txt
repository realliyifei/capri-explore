You are building a scientific question-answering dataset.
You will be given a content extracted from a paper.
You should construct the best question that would be answered by the information that summarizes by (some of) the content. 

# The requirements of the question:
1. Unambiguous: clearly framed to avoid follow-up questions for clarification.
2. Expert-level and natural: phrased as if asked by a domain expert conducting research
3. Answerable: should be answered by the information in the content 
4. Not overly broad: should not be so vague that it's unclear where to start
5. Standalone and not overly tailored: understandable by any expert without needing specific context or jargon anchored specifically in the given contents
6. Expect a long-form, comprehensive answer: not simply extractive or yes-no questions
7. Less than 20 words

# Some good questions that satisfy the requirements:
What are the specific functions or general strengths of different layers in BERT?
In recent studies of natural language generation systems, what measures of social bias have been used recently?
How do multilingual NLP models handle joint vocabularies during pretraining?
What are some recent important BERT variants that have been proposed and what distinguishes them?
When finetuning a large language model, what are some recent methods for preventing catastrophic forgetting?
How does BERT encode world knowledge and how do people use it in downstream application?
What are the differences between publicly available linguistic typology databases?
Can you summarize the approaches used to incorporate typological information in downstream NLP tasks?
Why do conversation models often produce responses that are inconsistent with previous turns?

Only return the question itself, without anything else.

# Given extracted content:
Paper Title: A Primer in BERTology: What we know about how BERT works
Section Title: BERT layers
Extracted first paragraph of the section that contain the answer to the question: The first layer of BERT receives as input a combina- tion of token, segment, and positional embeddings.
Question: What are the specific functions or general strengths of different layers in BERT?

# Given extracted content:
Paper Title: A Survey on Contextual Embeddings
Section Title: Cross-lingual Polyglot Pre-training for Contextual Embeddings
Extracted first paragraph of the section that contain the answer to the question: Cross-lingual polyglot pre-training aims to learn joint multi-lingual representations, enabling knowledge transfer from data-rich languages like English to data-scarce languages like Romanian. Based on whether joint training and a shared vocabulary are used, we divide previous work into three categories.
Question: How do multilingual NLP models handle joint vocabularies during pretraining?

# Given extracted content:
Paper Title: Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing
Section Title: Uses of Typological Information in NLP Models
Extracted first paragraph of the section that contain the answer to the question: The typological features developed as discussed in § 4 are of significant importance for NLP algorithms. Particularly, they are used in three main ways. First, they can be manually converted into rules for expert systems (§5.1); second, they can be integrated into algorithms as constraints that inject prior knowledge or tie together specific parameters across languages (§ 5.2); and, finally, they can guide data selection and synthesis (§ 5.3). All of these approaches are summarized in Table 3 and described in detail in the following sections, with a particular focus on the second approach.
Question: Can you summarize the approaches used to incorporate typological information in downstream NLP tasks?

# Given extracted content:
Paper Title: [PAPER_TITLE]
Section Title: [SECTION_TITLE]
Extracted first paragraph of the section that contain the answer to the question: [CONTENTS]
Question: 