title,section,section_title,paragraph,num_reference,segmented_answer,indexed_answer
A Primer in BERTology: What we know about how BERT works,s1,BERT embeddings,"['p1.0', 'p1.1']",4,"1. Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.
2. In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.
3. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
4. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.
5. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.
6. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.
7. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic)."
A Primer in BERTology: What we know about how BERT works,s3,Syntactic knowledge,"['p3.0', 'p3.1', 'p3.2']",5,"1. As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it.
2. Htut et al.(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root.
3. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation.
4. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2).
5. Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).
6. Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task.
7. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences.
8. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations.
9. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input.
10. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019).
11. This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","(p3.0) As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(p3.1) (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

(p3.2) Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge."
A Primer in BERTology: What we know about how BERT works,s7,Self-attention heads,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4', 'p7.5', 'p7.6', 'p7.7', 'p7.8', 'p7.9']",7,"1. Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types: attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018); attending to previous/next tokens,    (Kovaleva et al., 2019)
2. According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"".
3. However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 .
4. Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019).
5. This apparent redundancy must be related to the overparametrization issue (see section 7).
6. Attention to (Kovaleva et al., 2019)5 is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to (Kovaleva et al., 2019)4 and punctuation.
7. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as (Kovaleva et al., 2019)5 and (Kovaleva et al., 2019)4, and the model learns to rely on them.
8. They suggest also that the function of (Kovaleva et al., 2019)4 might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.
9. (Kovaleva et al., 2019)4 gets increased attention starting in layer 5, but its importance for prediction drops.
10. If this hypothesis is correct, attention probing studies that excluded the (Kovaleva et al., 2019)4 and (Kovaleva et al., 2019)5 tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.
11. Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.
12. Some BERT heads seem to specialize in certain types of syntactic relations.
13. Htut et al.(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions.
14. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline.
15. The evidence for nsubj, advmod, and amod has some variation between these two studies.
16. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context.
17. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).
18. present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora.
19. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.
20. Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,.
21. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks.
22. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998).
23. Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","(p7.0) Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

(p7.1) • attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

(p7.2) • attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

(p7.3) Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

(p7.4) [SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

(p7.5) Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

(p7.6) Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(p7.7) (2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

(p7.8) Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

(p7.9) Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks."
A Primer in BERTology: What we know about how BERT works,s8,BERT layers,"['p8.0', 'p8.1', 'p8.2', 'p8.3', 'p8.4']",8,"1. The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings.
2. It stands to reason that the lower layers have the most linear word order information.
3. report a decrease in the knowledge of linear word order around layer 4 in BERT-base.
4. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.
5. There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers.
6. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large).
7. Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.
8. The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5).
9. There is conflicting evidence about syntactic chunks.
10. Tenney et al. (6-9 for base-BERT, 14-19 for BERT-large)0 conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling.
11. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing.
12. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.
13. The final layers of BERT are the most taskspecific.
14. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable .
15. In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019).
16. At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance.
17. Tenney et al. (6-9 for base-BERT, 14-19 for BERT-large)0 suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers.
18. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (6-9 for base-BERT, 14-19 for BERT-large)1.
19. But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial.
20. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.
21. The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"".
22. However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.","(p8.0) The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

(p8.1) There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

(p8.2) The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

(p8.3) The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

(p8.4) The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers."
A Primer in BERTology: What we know about how BERT works,s10,Pre-training BERT,"['p10.0', 'p10.1', 'p10.2', 'p10.3', 'p10.4', 'p10.5', 'p10.6', 'p10.7', 'p10.8', 'p10.9']",12,"1. The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM).
2. Multiple studies have come up with alternative training objectives to improve on BERT.
3. Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting .
4. Wang et al. (2019a) Mikolov et al. (2013b).
5. Permutation language modeling. Yang et al. (MLM)6 replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order.
6. See also the n-gram word order reconstruction task (Wang et al., 2019a).
7. Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020); Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words; Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).
8. Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (MLM)0.
9. Clinchant et al. (MLM)6 propose replacing the MASK token with (MLM)2 token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.
10. Another obvious source of improvement is pretraining data.
11. Liu et al. (MLM)3 explore the benefits of increasing the corpus volume and longer training.
12. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (MLM)4, there are ongoing efforts to incorporate structured knowledge resources .
13. Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (MLM)5 and ERNIE .
14. Alternatively, SemBERT  integrates semantic role information with BERT representations.
15. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides.
16. Hao et al. (MLM)6 conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (MLM)7.
17. However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (MLM)8.","(p10.0) The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

(p10.1) • Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

(p10.2) • Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

(p10.3) • Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

(p10.4) • Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

(p10.5) • Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

(p10.6) • Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

(p10.7) • Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

(p10.8) Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

(p10.9) Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019)."
A Primer in BERTology: What we know about how BERT works,s11,Model architecture choices,"['p11.0', 'p11.1']",4,"1. To date, the most systematic study of BERT architecture was performed by .
2. They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others.
3. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable.
4. Larger hidden representation size was consistently better, but the gains varied by setting.
5. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance.
6. They also publish their recommendations for other model parameters.
7. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.
8. observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.
9. Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers.
10. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.","(p11.0) To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

(p11.1) Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks."
A Primer in BERTology: What we know about how BERT works,s12,Fine-tuning BERT,"['p12.0', 'p12.1', 'p12.2', 'p12.3', 'p12.4', 'p12.5', 'p12.6']",5,"1. Pre-training + fine-tuning workflow is a crucial part of BERT.
2. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand.
3. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns.
4. It is understandable why fine-tuning would increase the attention to  Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).
5. Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).
6. With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules.
7. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost.
8. Artetxe et al.(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.
9. An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).Initialization can have a dramatic effect on the training process (Petrov, 2010).
10. However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018).
11. Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation.
12. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.
13. 7 How big should BERT be?","(p12.0) Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

(p12.1) • Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

(p12.2) With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(p12.3) (2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

(p12.4) An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

(p12.5) Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

(p12.6) 7 How big should BERT be?"
A Primer in BERTology: What we know about how BERT works,s13,Overparametrization,"['p13.0', 'p13.1', 'p13.2', 'p13.3', 'p13.4']",10,"1. Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT.
2. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.
3. Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have.
4. Voita et al. (Strubell et al., 2019;Schwartz et al., 2019)3 showed that all but a few Transformer heads could be pruned without significant losses in performance.
5. For BERT, Clark et al. (Strubell et al., 2019;Schwartz et al., 2019)3 observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (Strubell et al., 2019;Schwartz et al., 2019)3 were able to reduce most layers to a single head.
6. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance.
7. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019).
8. Additionally, Tenney et al. (Strubell et al., 2019;Schwartz et al., 2019)0 examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (Strubell et al., 2019;Schwartz et al., 2019)1.
9. Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case.
10. In particular, the opposite was observed for subjectverb agreement (Strubell et al., 2019;Schwartz et al., 2019)2 and sentence subject detection .
11. Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers.
12. Clark et al.(Strubell et al., 2019;Schwartz et al., 2019)3 suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.","(p13.0) Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

(p13.1) Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

(p13.2) Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

(p13.3) Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(p13.4) (2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training."
A Primer in BERTology: What we know about how BERT works,s14,BERT compression,"['p14.0', 'p14.1', 'p14.2', 'p14.3', 'p14.4']",8,"1. Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss.
2. Such efforts to date are summarized in Table 1.
3. Two main approaches include knowledge distillation and quantization.
4. The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase).
5. This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).
6. The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019).
7. Note that this strategy often requires compatible hardware.
8. Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).","(p14.0) Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

(p14.1) Two main approaches include knowledge distillation and quantization.

(p14.2) The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

(p14.3) The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

(p14.4) Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020)."
A Primer in BERTology: What we know about how BERT works,s15,Multilingual BERT,"['p15.0', 'p15.1', 'p15.2', 'p15.3', 'p15.4', 'p15.5', 'p15.6', 'p15.7']",29,"1. Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary).
2. Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing.
3. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019).
4. The model seems to naturally learn high-quality cross-lingual word alignments (110K wordpiece vocabulary)9, with caveats for open-class parts of speech (Cao et al., 2019).
5. Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (Wu and Dredze, 2019;Pires et al., 2019)8 note that this task could be solvable by simple lexical matches.
6. Pires et al. (Wu and Dredze, 2019;Pires et al., 2019)8 conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.
7. mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (110K wordpiece vocabulary)0, and adding the IDs in pre-training was not beneficial .
8. It is also aware of at least some typological language features (110K wordpiece vocabulary)1, and transfer between structurally similar languages works better Pires et al., 2019).
9. Singh et al. (Wu and Dredze, 2019;Pires et al., 2019)8 argue that if typological features structure its representation space, it could not be considered as interlingua.
10. However, Artetxe et al. (Wu and Dredze, 2019;Pires et al., 2019)8 show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.
11. At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (110K wordpiece vocabulary)4, and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (Wu and Dredze, 2019;Pires et al., 2019)8.
12. Pires et al. (Wu and Dredze, 2019;Pires et al., 2019)8 and Wu and Dredze (Wu and Dredze, 2019;Pires et al., 2019)8 hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages.
13. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary.
14. Artetxe et al. (Wu and Dredze, 2019;Pires et al., 2019)8 also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (110K wordpiece vocabulary)9 vocabulary, without any shared word-pieces.
15. To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge.
16. However, as Tenney et al. (Wu and Dredze, 2019;Pires et al., 2019)0 aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"".
17. There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis .
18. A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.
19. Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (Wu and Dredze, 2019;Pires et al., 2019)1 would not be sufficient (Wu and Dredze, 2019;Pires et al., 2019)2.
20. Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Wu and Dredze, 2019;Pires et al., 2019)3.
21. Head and layer ablation studies (Wu and Dredze, 2019;Pires et al., 2019)4 inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Wu and Dredze, 2019;Pires et al., 2019)5, the absence of heads that would perform parsing ""in general"" (Wu and Dredze, 2019;Pires et al., 2019)6.
22. Ablations are also problematic if the same information was duplicated elsewhere in the network.
23. To mitigate that, Michel et al. (Wu and Dredze, 2019;Pires et al., 2019)8 prune heads in the order set by a proxy importance score, and Voita et al. (Wu and Dredze, 2019;Pires et al., 2019)8
24. fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.
25. Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Wu and Dredze, 2019;Pires et al., 2019)9.
26. However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Rönnqvist et al., 2019)0.
27. Also, visualization is typically limited to qualitative analysis (Rönnqvist et al., 2019)1, and should not be interpreted as definitive evidence.","(p15.0) Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

(p15.1) mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

(p15.2) mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

(p15.3) At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

(p15.4) To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

(p15.5) Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

(p15.6) Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

(p15.7) Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence."
A Survey of Data Augmentation Approaches for NLP,s1,Background,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5']",6,"1. What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.
2. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020).
3. DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training.
4. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.
5. What are the goals and trade-offs?
6. Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).
7. As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance.
8. Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).
9. Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.
10. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.
11. Further, the distribution of augmented data should neither be too similar nor too different from the original.
12. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.
13. Effective DA approaches should aim for a balance.
14. Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.
15. Interpretation of DA Dao et al. (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020)0 note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.
16. Overall, there indeed appears to be a lack of research on why exactly DA works.
17. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles.
18. We discuss this challenge more in §6, and highlight some of the existing work below.
19. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2).
20. Rajput et al. (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020)0 show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods.
21. Dao et al. (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020)0 think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization.
22. Chen et al. (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020)1 show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","(p1.0) What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

(p1.1) What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

(p1.2) Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

(p1.3) Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

(p1.4) Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

(p1.5) Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant."
A Survey of Data Augmentation Approaches for NLP,s3,Rule-Based Techniques,"['p3.0', 'p3.1']",4,"1. Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model components.
2. Feature space DA approaches generate augmented examples in the model's feature space rather than input data.
3. Many few-shot learning approaches (Hariharan and Girshick, 2017;Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes (see §4.4).
4. Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold.
5. Wei and Zou (2019) propose EASY DATA AUG-MENTATION (EDA), a set of token-level random perturbation operations including random insertion, deletion, and swap.
6. They show improved performance on many text classification tasks.
7. UDA (Xie et al., 2020) show how supervised DA methods can be exploited for unsupervised data through consistency training on (x, DA(x)) pairs.
8. For paraphrase identification, Chen et al. (2020b) construct a signed graph over the data, with individual sentences as nodes and pair labels as signed edges.
9. They use balance theory and transitivity to infer augmented sentence pairs from this graph.
10. Motivated by image cropping and rotation, Şahin and
11. Steedman (2018) propose dependency tree morphing.
12. For dependency-annotated sentences, children of the same parent are swapped (à la rotation) or some deleted (see §4.4)0, as seen in Figure 2.
13. This is most beneficial for language families with rich case marking systems (see §4.4)1.","(p3.0) Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model components. Feature space DA approaches generate augmented examples in the model's feature space rather than input data. Many few-shot learning approaches (Hariharan and Girshick, 2017;Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes (see §4.4). Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold. Wei and Zou (2019) propose EASY DATA AUG-MENTATION (EDA), a set of token-level random perturbation operations including random insertion, deletion, and swap. They show improved performance on many text classification tasks. UDA (Xie et al., 2020) show how supervised DA methods can be exploited for unsupervised data through consistency training on (x, DA(x)) pairs.

(p3.1) For paraphrase identification, Chen et al. (2020b) construct a signed graph over the data, with individual sentences as nodes and pair labels as signed edges. They use balance theory and transitivity to infer augmented sentence pairs from this graph. Motivated by image cropping and rotation, Şahin and Steedman (2018) propose dependency tree morphing. For dependency-annotated sentences, children of the same parent are swapped (à la rotation) or some deleted (à la cropping), as seen in Figure 2. This is most beneficial for language families with rich case marking systems (e.g. Baltic and Slavic)."
A Survey of Data Augmentation Approaches for NLP,s4,Example Interpolation Techniques,"['p4.0', 'p4.1', 'p4.2']",5,"1. Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples.
2. This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA).
3. Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).
4. Another class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements.
5. Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Şahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism.
6. Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020).
7. For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes.
8. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see ""Multimodal challenges"" in §6).
9. A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs.
10. This has been overcome by mixing embeddings or higher hidden layers (MSDA)0.
11. Later variants propose speech-tailored mixing schemes (MSDA)1 and interpolation with adversarial examples (MSDA)2, among others.
12. SEQ2MIXUP (MSDA)3 generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (MSDA)4 prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(MSDA)5.
13. The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (MSDA)6.","(p4.0) Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples. This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA). Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).

(p4.1) Another class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements. Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Şahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism. Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020). For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see ""Multimodal challenges"" in §6).

(p4.2) A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs. This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c). Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others. SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α). The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a)."
"Language (Technology) is Power: A Critical Survey of ""Bias"" in NLP",s7,Language and social hierarchies,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4', 'p7.5']",9,"1. Turning frst to (R1), we argue that work analyzing ""bias"" in NLP systems will paint a much fuller picture if it engages with the relevant literature outside of NLP that explores the relationships between language and social hierarchies.
2. Many disciplines, including sociolinguistics, linguistic anthropology, sociology, and social psychology, study how language takes on social meaning and the role that language plays in maintaining social hierarchies.
3. For example, language is the means through which social groups are labeled and one way that beliefs about social groups are transmitted (e.g., Maass, 1999;Beukeboom and Burgers, 2019).
4. Group labels can serve as the basis of stereotypes and thus reinforce social inequalities: ""[T]he label content functions to identify a given category of people, and thereby conveys category boundaries and a position in a hierarchical taxonomy"" (Beukeboom and Burgers, 2019).
5. Similarly, ""controlling images,"" such as stereotypes of Black women, which are linguistically and visually transmitted through literature, news media, television, and so forth, provide ""ideological justifcation"" for their continued oppression (Collins, 2000, Chapter 4).
6. As a result, many groups have sought to bring about social changes through changes in language, disrupting patterns of oppression and marginalization via so-called ""gender-fair"" language (Sczesny et al., 2016;Menegatti and Rubini, 2017), language that is more inclusive to people with disabilities (ADA, 2018), and language that is less dehumanizing (e.g., abandoning the use of the term ""illegal"" in everyday discourse on immigration in the U.S. (Rosa, 2019)).
7. The fact that group labels are so contested is evidence of how deeply intertwined language and social hierarchies are.
8. Taking ""gender-fair"" language as an example, the hope is that reducing asymmetries in language about women and men will reduce asymmetries in their social standing.
9. Meanwhile, struggles over language use often arise from dominant social groups' desire to ""control both material and symbolic resources""-i.e., ""the right to decide what words will mean and to control those meanings""-as was the case in some white speakers' insistence on using offensive place names against the objections of Indigenous speakers (Hill, 2008, Chapter 3).
10. Sociolinguists and linguistic anthropologists have also examined language attitudes and language ideologies, or people's metalinguistic beliefs about language: Which language varieties or practices are taken as standard, ordinary, or unmarked?
11. Which are considered correct, prestigious, or appropriate for public use, and which are considered incorrect, uneducated, or offensive (e.g., Campbell-Kibler, 2009;Preston, 2009;Loudermilk, 2015;Lanehart and Malik, 2018)?
12. Which are rendered invisible (e.g., Maass, 1999;Beukeboom and Burgers, 2019)0?
13. 3 Language ideologies play a vital role in reinforcing and justifying social hierarchies because beliefs about language varieties or practices often translate into beliefs about their speakers (e.g., Maass, 1999;Beukeboom and Burgers, 2019)1.
14. For example, in the U.S., the portrayal of non-white speakers' language varieties and practices as linguistically defcient helped to justify violent European colonialism, and today continues to justify enduring racial hierarchies by maintaining views of non-white speakers as lacking the language ""required for complex thinking processes and successful engagement in the global economy"" (e.g., Maass, 1999;Beukeboom and Burgers, 2019)2.
15. Recognizing the role that language plays in maintaining social hierarchies is critical to the future of work analyzing ""bias"" in NLP systems.
16. First, it helps to explain why representational harms are harmful in their own right.
17. Second, the complexity of the relationships between language and social hierarchies illustrates why studying ""bias"" in NLP systems is so challenging, suggesting that researchers and practitioners will need to move beyond existing algorithmic fairness techniques.
18. We argue that work must be grounded in the relevant literature outside of NLP that examines the relationships between language and social hierarchies; without this grounding, researchers and practitioners risk measuring or mitigating only what is convenient to measure or mitigate, rather than what is most normatively concerning.
19. More specifcally, we recommend that work analyzing ""bias"" in NLP systems be reoriented around the following question: How are social hierarchies, language ideologies, and NLP systems coproduced?
20. This question mirrors Benjamin's (e.g., Maass, 1999;Beukeboom and Burgers, 2019)3 call to examine how ""race and technology are coproduced""-i.e., how racial hierarchies, and the ideologies and discourses that maintain them, create and are re-created by technology.
21. We recommend that researchers and practitioners similarly ask how existing social hierarchies and language ideologies drive the development and deployment of NLP systems, and how these systems therefore reproduce these hierarchies and ideologies.
22. As a starting point for reorienting work analyzing ""bias"" in NLP systems around this question, we provide the following concrete research questions:.  (e.g., Maass, 1999;Beukeboom and Burgers, 2019)4?
23. Are any non-quantitative evaluations performed? .
24. How do NLP systems reproduce or transform language ideologies?
25. Which language varieties or practices come to be deemed good or bad?
26. Might ""good"" language simply mean language that is easily handled by existing NLP systems?
27. For example, linguistic phenomena arising from many language practices (e.g., Maass, 1999;Beukeboom and Burgers, 2019)5 are described as ""noisy text"" and often viewed as a target for ""normalization.""
28. How do the language ideologies that are reproduced by NLP systems maintain social hierarchies? .
29. Which representational harms are being measured or mitigated?
30. Are these the most normatively concerning harms, or merely those that are well handled by existing algorithmic fairness techniques?
31. Are there other representational harms that might be analyzed?","(p7.0) Turning frst to (R1), we argue that work analyzing ""bias"" in NLP systems will paint a much fuller picture if it engages with the relevant literature outside of NLP that explores the relationships between language and social hierarchies. Many disciplines, including sociolinguistics, linguistic anthropology, sociology, and social psychology, study how language takes on social meaning and the role that language plays in maintaining social hierarchies. For example, language is the means through which social groups are labeled and one way that beliefs about social groups are transmitted (e.g., Maass, 1999;Beukeboom and Burgers, 2019). Group labels can serve as the basis of stereotypes and thus reinforce social inequalities: ""[T]he label content functions to identify a given category of people, and thereby conveys category boundaries and a position in a hierarchical taxonomy"" (Beukeboom and Burgers, 2019). Similarly, ""controlling images,"" such as stereotypes of Black women, which are linguistically and visually transmitted through literature, news media, television, and so forth, provide ""ideological justifcation"" for their continued oppression (Collins, 2000, Chapter 4).

(p7.1) As a result, many groups have sought to bring about social changes through changes in language, disrupting patterns of oppression and marginalization via so-called ""gender-fair"" language (Sczesny et al., 2016;Menegatti and Rubini, 2017), language that is more inclusive to people with disabilities (ADA, 2018), and language that is less dehumanizing (e.g., abandoning the use of the term ""illegal"" in everyday discourse on immigration in the U.S. (Rosa, 2019)). The fact that group labels are so contested is evidence of how deeply intertwined language and social hierarchies are. Taking ""gender-fair"" language as an example, the hope is that reducing asymmetries in language about women and men will reduce asymmetries in their social standing. Meanwhile, struggles over language use often arise from dominant social groups' desire to ""control both material and symbolic resources""-i.e., ""the right to decide what words will mean and to control those meanings""-as was the case in some white speakers' insistence on using offensive place names against the objections of Indigenous speakers (Hill, 2008, Chapter 3).

(p7.2) Sociolinguists and linguistic anthropologists have also examined language attitudes and language ideologies, or people's metalinguistic beliefs about language: Which language varieties or practices are taken as standard, ordinary, or unmarked? Which are considered correct, prestigious, or appropriate for public use, and which are considered incorrect, uneducated, or offensive (e.g., Campbell-Kibler, 2009;Preston, 2009;Loudermilk, 2015;Lanehart and Malik, 2018)? Which are rendered invisible (Roche, 2019)? 3 Language ideologies play a vital role in reinforcing and justifying social hierarchies because beliefs about language varieties or practices often translate into beliefs about their speakers (e.g. Alim et al., 2016;Rosa and Flores, 2017;Craft et al., 2020). For example, in the U.S., the portrayal of non-white speakers' language varieties and practices as linguistically defcient helped to justify violent European colonialism, and today continues to justify enduring racial hierarchies by maintaining views of non-white speakers as lacking the language ""required for complex thinking processes and successful engagement in the global economy"" (Rosa and Flores, 2017).

(p7.3) Recognizing the role that language plays in maintaining social hierarchies is critical to the future of work analyzing ""bias"" in NLP systems. First, it helps to explain why representational harms are harmful in their own right. Second, the complexity of the relationships between language and social hierarchies illustrates why studying ""bias"" in NLP systems is so challenging, suggesting that researchers and practitioners will need to move beyond existing algorithmic fairness techniques. We argue that work must be grounded in the relevant literature outside of NLP that examines the relationships between language and social hierarchies; without this grounding, researchers and practitioners risk measuring or mitigating only what is convenient to measure or mitigate, rather than what is most normatively concerning.

(p7.4) More specifcally, we recommend that work analyzing ""bias"" in NLP systems be reoriented around the following question: How are social hierarchies, language ideologies, and NLP systems coproduced? This question mirrors Benjamin's (2020) call to examine how ""race and technology are coproduced""-i.e., how racial hierarchies, and the ideologies and discourses that maintain them, create and are re-created by technology. We recommend that researchers and practitioners similarly ask how existing social hierarchies and language ideologies drive the development and deployment of NLP systems, and how these systems therefore reproduce these hierarchies and ideologies. As a starting point for reorienting work analyzing ""bias"" in NLP systems around this question, we provide the following concrete research questions:

(p7.5) .  (Olteanu et al., 2017)? Are any non-quantitative evaluations performed? . How do NLP systems reproduce or transform language ideologies? Which language varieties or practices come to be deemed good or bad? Might ""good"" language simply mean language that is easily handled by existing NLP systems? For example, linguistic phenomena arising from many language practices (Eisenstein, 2013) are described as ""noisy text"" and often viewed as a target for ""normalization."" How do the language ideologies that are reproduced by NLP systems maintain social hierarchies? . Which representational harms are being measured or mitigated? Are these the most normatively concerning harms, or merely those that are well handled by existing algorithmic fairness techniques? Are there other representational harms that might be analyzed?"
"Language (Technology) is Power: A Critical Survey of ""Bias"" in NLP",s9,Language use in practice,"['p9.0', 'p9.1', 'p9.2']",7,"1. Finally, we turn to (R3). Our perspective, which rests on a greater recognition of the relationships between language and social hierarchies, suggests several directions for examining language use in practice.
2. Here, we focus on two. First, because language is necessarily situated, and because different social groups have different lived experiences due to their different social positions (Hanna et al., 2020)-particularly groups at the intersections of multiple axes of oppression-we recommend that researchers and practitioners center work analyzing ""bias"" in NLP systems around the lived experiences of members of communities affected by these systems.
3. Second, we recommend that the power relations between technologists and such communities be interrogated and reimagined.
4. Researchers have pointed out that algorithmic fairness techniques, by proposing incremental technical mitigations-e.g., collecting new datasets or training better models-maintain these power relations by (a) assuming that automated systems should continue to exist, rather than asking whether they should be built at all, and ( There are many disciplines for researchers and practitioners to draw on when pursuing these directions. For example, in human-computer interaction, Hamidi et al. (2018) study transgender people's experiences with automated gender recognition systems in order to uncover how these systems reproduce structures of transgender exclusion by redefning what it means to perform gender ""normally.""
5. Value-sensitive design provides a framework for accounting for the values of different stakeholders in the design of technology (e.g., Friedman et al., 2006;Friedman and Hendry, 2019;Le Dantec et al., 2009;Yoo et al., 2019), while participatory design seeks to involve stakeholders in the design process itself (Sanders, 2002;Muller, 2007;Simonsen and Robertson, 2013;DiSalvo et al., 2013).
6. Participatory action research in education (Kemmis, 2006) and in language documentation and reclamation (Junker, 2018) is also relevant.
7. In particular, work on language reclamation to support decolonization and tribal sovereignty (Leonard, 2012) and work in sociolinguistics focus-ing on developing co-equal research relationships with community members and supporting linguistic justice efforts (e.g., Bucholtz et al., 2014Bucholtz et al., , 2016Bucholtz et al., , 2019 provide examples of more emancipatory relationships with communities. Finally, several workshops and events have begun to explore how to empower stakeholders in the development and deployment of technology (Vaccaro et al., 2019;Givens and Morris, 2020;Sassaman et al., 2020) 4 and how to help researchers and practitioners consider when not to build systems at all (Hanna et al., 2020)0.
8. As a starting point for engaging with communities affected by NLP systems, we therefore provide the following concrete research questions:.
9. How do communities become aware of NLP systems?
10. Do they resist them, and if so, how? .
11. What additional costs are borne by communities for whom NLP systems do not work well? .
12. Do NLP systems shift power toward oppressive institutions (Hanna et al., 2020)1, surveillance, or censorship), or away from such institutions? .
13. Who is involved in the development and deployment of NLP systems?
14. How do decision-making processes maintain power relations between technologists and communities affected by NLP systems?
15. Can these processes be changed to reimagine these relations?","(p9.0) Finally, we turn to (R3). Our perspective, which rests on a greater recognition of the relationships between language and social hierarchies, suggests several directions for examining language use in practice. Here, we focus on two. First, because language is necessarily situated, and because different social groups have different lived experiences due to their different social positions (Hanna et al., 2020)-particularly groups at the intersections of multiple axes of oppression-we recommend that researchers and practitioners center work analyzing ""bias"" in NLP systems around the lived experiences of members of communities affected by these systems. Second, we recommend that the power relations between technologists and such communities be interrogated and reimagined. Researchers have pointed out that algorithmic fairness techniques, by proposing incremental technical mitigations-e.g., collecting new datasets or training better models-maintain these power relations by (a) assuming that automated systems should continue to exist, rather than asking whether they should be built at all, and ( There are many disciplines for researchers and practitioners to draw on when pursuing these directions. For example, in human-computer interaction, Hamidi et al. (2018) study transgender people's experiences with automated gender recognition systems in order to uncover how these systems reproduce structures of transgender exclusion by redefning what it means to perform gender ""normally."" Value-sensitive design provides a framework for accounting for the values of different stakeholders in the design of technology (e.g., Friedman et al., 2006;Friedman and Hendry, 2019;Le Dantec et al., 2009;Yoo et al., 2019), while participatory design seeks to involve stakeholders in the design process itself (Sanders, 2002;Muller, 2007;Simonsen and Robertson, 2013;DiSalvo et al., 2013). Participatory action research in education (Kemmis, 2006) and in language documentation and reclamation (Junker, 2018) is also relevant. In particular, work on language reclamation to support decolonization and tribal sovereignty (Leonard, 2012) and work in sociolinguistics focus-ing on developing co-equal research relationships with community members and supporting linguistic justice efforts (e.g., Bucholtz et al., 2014Bucholtz et al., , 2016Bucholtz et al., , 2019 provide examples of more emancipatory relationships with communities. Finally, several workshops and events have begun to explore how to empower stakeholders in the development and deployment of technology (Vaccaro et al., 2019;Givens and Morris, 2020;Sassaman et al., 2020) 4 and how to help researchers and practitioners consider when not to build systems at all (Barocas et al., 2020).

(p9.1) As a starting point for engaging with communities affected by NLP systems, we therefore provide the following concrete research questions:

(p9.2) . How do communities become aware of NLP systems? Do they resist them, and if so, how? . What additional costs are borne by communities for whom NLP systems do not work well? . Do NLP systems shift power toward oppressive institutions (e.g., by enabling predictions that communities do not want made, linguistically based unfair allocation of resources or opportunities (Rosa and Flores, 2017), surveillance, or censorship), or away from such institutions? . Who is involved in the development and deployment of NLP systems? How do decision-making processes maintain power relations between technologists and communities affected by NLP systems? Can these processes be changed to reimagine these relations?"
"Language (Technology) is Power: A Critical Survey of ""Bias"" in NLP",s10,Case study,"['p10.0', 'p10.1', 'p10.2', 'p10.3', 'p10.4', 'p10.5', 'p10.6', 'p10.7']",8,"1. To illustrate our recommendations, we present a case study covering work on African-American English (AAE). 5 Work analyzing ""bias"" in the context of AAE has shown that part-of-speech taggers, language identifcation systems, and dependency parsers all work less well on text containing features associated with AAE than on text without these features (Jørgensen et al., , 2016, and that toxicity detection systems score tweets containing features associated with AAE as more offensive than tweets without them .
2. These papers have been critical for highlighting AAE as a language variety for which existing NLP systems may not work, illustrating their limitations.
3. However, they do not conceptualize ""racial bias"" in the same way.
4. The frst four of these papers simply focus on system performance differences between text containing features associated with AAE and text without these features.
5. In contrast, the last two papers also focus on such system performance differences, but motivate this focus with the following additional reasoning: If tweets containing features associated with AAE are scored as more offensive than tweets without these features, then this might (a) yield negative perceptions of AAE; (b) result in disproportionate removal of tweets containing these features, impeding participation in online platforms and reducing the space available online in which speakers can use AAE freely; and (c) cause AAE speakers to incur additional costs if they have to change their language practices to avoid negative perceptions or tweet removal.
6. More importantly, none of these papers engage with the literature on AAE, racial hierarchies in the U.S., and raciolinguistic ideologies.
7. By failing to engage with this literature-thereby treating AAE simply as one of many non-Penn Treebank varieties of English or perhaps as another challenging domain-work analyzing ""bias"" in NLP systems in the context of AAE fails to situate these systems in the world.
8. Who are the speakers of AAE? How are they viewed?
9. We argue that AAE as a language variety cannot be separated from its speakersprimarily Black people in the U.S., who experience systemic anti-Black racism-and the language ideologies that reinforce and justify racial hierarchies.
10. Even after decades of sociolinguistic efforts to legitimize AAE, it continues to be viewed as ""bad"" English and its speakers continue to be viewed as linguistically inadequate-a view called the defcit perspective (Alim et al., 2016;Rosa and Flores, 2017).
11. This perspective persists despite demonstrations that AAE is rule-bound and grammatical (Mufwene et al., 1998;Green, 2002), in addition to ample evidence of its speakers' linguistic adroitness (e.g., Alim, 2004;Rickford and King, 2016).
12. This perspective belongs to a broader set of raciolinguistic ideologies (a)5, which also produce allocational harms; speakers of AAE are frequently penalized for not adhering to dominant language practices, including in the education system (Alim, 2004;Terry et al., 2010), when seeking housing (Baugh, 2018), and in the judicial system, where their testimony is misunderstood or, worse yet, disbelieved (a)0.
13. These raciolinguistic ideologies position racialized communities as needing linguistic intervention, such as language education programs, in which these and other harms can be reduced if communities accommodate to dominant language practices (a)5.
14. In the technology industry, speakers of AAE are often not considered consumers who matter.
15. For example, Benjamin (a)2 recounts an Apple employee who worked on speech recognition for Siri: ""As they worked on different English dialects -Australian, Singaporean, and Indian English -(a)3 asked his boss: 'What about African American English?'
16. To this his boss responded: 'Well, Apple products are for the premium market.
17. ""'The reality, of course, is that speakers of AAE tend not to represent the ""premium market"" precisely because of institutions and policies that help to maintain racial hierarchies by systematically denying them the opportunities to develop wealth that are available to white Americans (a)4an exclusion that is reproduced in technology by countless decisions like the one described above.
18. Engaging with the literature outlined above situates the system behaviors that are described as ""bias,"" providing a foundation for normative reasoning.
19. Researchers and practitioners should be concerned about ""racial bias"" in toxicity detection systems
20. not only because performance differences impair system performance, but because they reproduce longstanding injustices of stigmatization and disenfranchisement for speakers of AAE.
21. In re-stigmatizing AAE, they reproduce language ideologies in which AAE is viewed as ungrammatical, uneducated, and offensive.
22. These ideologies, in turn, enable linguistic discrimination and justify enduring racial hierarchies (a)5.
23. Our perspective, which understands racial hierarchies and raciolinguistic ideologies as structural conditions that govern the development and deployment of technology, implies that techniques for measuring or mitigating ""bias"" in NLP systems will necessarily be incomplete unless they interrogate and dismantle these structural conditions, including the power relations between technologists and racialized communities.
24. We emphasize that engaging with the literature on AAE, racial hierarchies in the U.S., and raciolinguistic ideologies can generate new lines of engagement.
25. These lines include work on the ways that the decisions made during the development and deployment of NLP systems produce stigmatization and disenfranchisement, and work on AAE use in practice, such as the ways that speakers of AAE interact with NLP systems that were not designed for them.
26. This literature can also help researchers and practitioners address the allocational harms that may be produced by NLP systems, and ensure that even well-intentioned NLP systems do not position racialized communities as needing linguistic intervention or accommodation to dominant language practices.
27. Finally, researchers and practitioners wishing to design better systems can also draw on a growing body of work on anti-racist language pedagogy that challenges the defcit perspective of AAE and other racialized language practices (a)6, as well as the work that we described in section 4.3 on reimagining the power relations between technologists and communities affected by technology.","(p10.0) To illustrate our recommendations, we present a case study covering work on African-American English (AAE). 5 Work analyzing ""bias"" in the context of AAE has shown that part-of-speech taggers, language identifcation systems, and dependency parsers all work less well on text containing features associated with AAE than on text without these features (Jørgensen et al., , 2016, and that toxicity detection systems score tweets containing features associated with AAE as more offensive than tweets without them .

(p10.1) These papers have been critical for highlighting AAE as a language variety for which existing NLP systems may not work, illustrating their limitations. However, they do not conceptualize ""racial bias"" in the same way. The frst four of these papers simply focus on system performance differences between text containing features associated with AAE and text without these features. In contrast, the last two papers also focus on such system performance differences, but motivate this focus with the following additional reasoning: If tweets containing features associated with AAE are scored as more offensive than tweets without these features, then this might (a) yield negative perceptions of AAE; (b) result in disproportionate removal of tweets containing these features, impeding participation in online platforms and reducing the space available online in which speakers can use AAE freely; and (c) cause AAE speakers to incur additional costs if they have to change their language practices to avoid negative perceptions or tweet removal.

(p10.2) More importantly, none of these papers engage with the literature on AAE, racial hierarchies in the U.S., and raciolinguistic ideologies. By failing to engage with this literature-thereby treating AAE simply as one of many non-Penn Treebank varieties of English or perhaps as another challenging domain-work analyzing ""bias"" in NLP systems in the context of AAE fails to situate these systems in the world. Who are the speakers of AAE? How are they viewed? We argue that AAE as a language variety cannot be separated from its speakersprimarily Black people in the U.S., who experience systemic anti-Black racism-and the language ideologies that reinforce and justify racial hierarchies.

(p10.3) Even after decades of sociolinguistic efforts to legitimize AAE, it continues to be viewed as ""bad"" English and its speakers continue to be viewed as linguistically inadequate-a view called the defcit perspective (Alim et al., 2016;Rosa and Flores, 2017). This perspective persists despite demonstrations that AAE is rule-bound and grammatical (Mufwene et al., 1998;Green, 2002), in addition to ample evidence of its speakers' linguistic adroitness (e.g., Alim, 2004;Rickford and King, 2016). This perspective belongs to a broader set of raciolinguistic ideologies (Rosa and Flores, 2017), which also produce allocational harms; speakers of AAE are frequently penalized for not adhering to dominant language practices, including in the education system (Alim, 2004;Terry et al., 2010), when seeking housing (Baugh, 2018), and in the judicial system, where their testimony is misunderstood or, worse yet, disbelieved (Rickford and King, 2016;Jones et al., 2019). These raciolinguistic ideologies position racialized communities as needing linguistic intervention, such as language education programs, in which these and other harms can be reduced if communities accommodate to dominant language practices (Rosa and Flores, 2017).

(p10.4) In the technology industry, speakers of AAE are often not considered consumers who matter. For example, Benjamin (2019) recounts an Apple employee who worked on speech recognition for Siri: ""As they worked on different English dialects -Australian, Singaporean, and Indian English -[the employee] asked his boss: 'What about African American English?' To this his boss responded: 'Well, Apple products are for the premium market.""'

(p10.5) The reality, of course, is that speakers of AAE tend not to represent the ""premium market"" precisely because of institutions and policies that help to maintain racial hierarchies by systematically denying them the opportunities to develop wealth that are available to white Americans (Rothstein, 2017)an exclusion that is reproduced in technology by countless decisions like the one described above.

(p10.6) Engaging with the literature outlined above situates the system behaviors that are described as ""bias,"" providing a foundation for normative reasoning. Researchers and practitioners should be concerned about ""racial bias"" in toxicity detection systems not only because performance differences impair system performance, but because they reproduce longstanding injustices of stigmatization and disenfranchisement for speakers of AAE. In re-stigmatizing AAE, they reproduce language ideologies in which AAE is viewed as ungrammatical, uneducated, and offensive. These ideologies, in turn, enable linguistic discrimination and justify enduring racial hierarchies (Rosa and Flores, 2017). Our perspective, which understands racial hierarchies and raciolinguistic ideologies as structural conditions that govern the development and deployment of technology, implies that techniques for measuring or mitigating ""bias"" in NLP systems will necessarily be incomplete unless they interrogate and dismantle these structural conditions, including the power relations between technologists and racialized communities.

(p10.7) We emphasize that engaging with the literature on AAE, racial hierarchies in the U.S., and raciolinguistic ideologies can generate new lines of engagement. These lines include work on the ways that the decisions made during the development and deployment of NLP systems produce stigmatization and disenfranchisement, and work on AAE use in practice, such as the ways that speakers of AAE interact with NLP systems that were not designed for them. This literature can also help researchers and practitioners address the allocational harms that may be produced by NLP systems, and ensure that even well-intentioned NLP systems do not position racialized communities as needing linguistic intervention or accommodation to dominant language practices. Finally, researchers and practitioners wishing to design better systems can also draw on a growing body of work on anti-racist language pedagogy that challenges the defcit perspective of AAE and other racialized language practices (e.g. Flores and Chaparro, 2018; Baker-Bell, 2019; Martínez and Mejía, 2019), as well as the work that we described in section 4.3 on reimagining the power relations between technologists and communities affected by technology."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,s4,How Low is Low-Resource?,"['p4.0', 'p4.1', 'p4.2']",4,"1. On the dimension of task-specific labels, different thresholds are used to define low-resource.
2. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens.
3. Kann et al.(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens.
4. The threshold is also task-dependent and more complex tasks might also increase the resource requirements.
5. For text generation,  frame their work as low-resource with 350k labeled training instances.
6. Similar to the task, the resource requirements can also depend on the language.
7. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.
8. Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability.
9. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches.
10. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","(p4.0) On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(p4.1) (2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

(p4.2) Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,s6,Data Augmentation,"['p6.0', 'p6.1', 'p6.2', 'p6.3']",11,"1. New instances can be obtained based on existing ones by modifying the features with transformations that do not change the label.
2. In the computer vision community, this is a popular approach where, e.g., rotating an image is invariant to the classification of an image's content.
3. For text, on the token level, this can be done by replacing words with equivalents, such as synonyms (Wei and Zou, 2019), entities of the same type (Raiman and Miller, 2017;Dai and Adel, 2020) or words that share the same morphology (Gulordava et al., 2018;Vania et al., 2019).
4. Such replacements can also be guided by a language model that takes context into consideration (Fadaee et al., 2017;Kobayashi, 2018).
5. To go beyond the token level and add more diversity to the augmented sentences, data augmentation can also be performed on sentence parts.
6. Operations that (depending on the task) do not change the label include manipulation of parts of the dependency tree (Şahin and Steedman, 2018;Vania et al., 2019;Dehouck and Gómez-Rodríguez, 2020), simplification of sentences by removal of sentence parts (Şahin and Steedman, 2018) and inversion of the subject-object relation (Min et al., 2020).
7. For whole sentences, paraphrasing through backtranslation can be used.
8. This is a popular approach in machine translation where target sentences are back-translated into source sentences (Bojar and Tamchyna, 2011;Hoang et al., 2018).
9. An important aspect here is that errors in the source side/features do not seem to have a large negative effect on the generated target text the model needs to predict.
10. It is therefore also used in other text generation tasks like abstract summarization (Parida and Motlicek, 2019) and table-to-text generation (Raiman and Miller, 2017;Dai and Adel, 2020)0.
11. Back-translation has also been leveraged for text classification (Raiman and Miller, 2017;Dai and Adel, 2020)1.
12. This setting assumes, however, the availability of a translation system.
13. Instead, a language model can also be used for augmenting text classification datasets (Raiman and Miller, 2017;Dai and Adel, 2020)2.
14. It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label.
15. It then generates additional sentences that fit this label.
16. Ding et al. (Raiman and Miller, 2017;Dai and Adel, 2020)9 extend this idea for token level tasks.
17. Adversarial methods are often used to find weaknesses in machine learning models (Raiman and Miller, 2017;Dai and Adel, 2020)4.
18. They can, however, also be utilized to augment NLP datasets (Raiman and Miller, 2017;Dai and Adel, 2020)5.
19. Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (Raiman and Miller, 2017;Dai and Adel, 2020)6.
20. This approach is often applied on the level of vector representations.
21. For instance, Grundkiewicz et al. (Raiman and Miller, 2017;Dai and Adel, 2020)7 reverse the augmentation setting by applying transformations that flip the (Raiman and Miller, 2017;Dai and Adel, 2020)8 label.
22. In their case, they introduce errors in correct sentences to obtain new training data for a grammar correction task.
23. Open Issues: While data augmentation is ubiquitous in the computer vision community and while most of the above-presented approaches are taskindependent, it has not found such widespread use in natural language processing.
24. A reason might be that several of the approaches require an indepth understanding of the language.
25. There is not yet a unified framework that allows applying data augmentation across tasks and languages.
26. Recently, Longpre et al. (Raiman and Miller, 2017;Dai and Adel, 2020)9 hypothesised that data augmentation provides the same benefits as pretraining in transformer models.
27. However, we argue that data augmentation might be better suited to leverage the insights of linguistic or domain experts in low-resource settings when unlabeled data or hardware resources are limited.","(p6.0) New instances can be obtained based on existing ones by modifying the features with transformations that do not change the label. In the computer vision community, this is a popular approach where, e.g., rotating an image is invariant to the classification of an image's content. For text, on the token level, this can be done by replacing words with equivalents, such as synonyms (Wei and Zou, 2019), entities of the same type (Raiman and Miller, 2017;Dai and Adel, 2020) or words that share the same morphology (Gulordava et al., 2018;Vania et al., 2019). Such replacements can also be guided by a language model that takes context into consideration (Fadaee et al., 2017;Kobayashi, 2018).

(p6.1) To go beyond the token level and add more diversity to the augmented sentences, data augmentation can also be performed on sentence parts. Operations that (depending on the task) do not change the label include manipulation of parts of the dependency tree (Şahin and Steedman, 2018;Vania et al., 2019;Dehouck and Gómez-Rodríguez, 2020), simplification of sentences by removal of sentence parts (Şahin and Steedman, 2018) and inversion of the subject-object relation (Min et al., 2020). For whole sentences, paraphrasing through backtranslation can be used. This is a popular approach in machine translation where target sentences are back-translated into source sentences (Bojar and Tamchyna, 2011;Hoang et al., 2018). An important aspect here is that errors in the source side/features do not seem to have a large negative effect on the generated target text the model needs to predict. It is therefore also used in other text generation tasks like abstract summarization (Parida and Motlicek, 2019) and table-to-text generation (Ma et al., 2019). Back-translation has also been leveraged for text classification (Xie et al., 2020;Hegde and Patil, 2020). This setting assumes, however, the availability of a translation system. Instead, a language model can also be used for augmenting text classification datasets (Kumar et al., 2020;Anaby-Tavor et al., 2020). It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label. It then generates additional sentences that fit this label. Ding et al. (2020) extend this idea for token level tasks.

(p6.2) Adversarial methods are often used to find weaknesses in machine learning models (Jin et al., 2020;Garg and Ramakrishnan, 2020). They can, however, also be utilized to augment NLP datasets (Yasunaga et al., 2018;Morris et al., 2020). Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (according to a specific score). This approach is often applied on the level of vector representations. For instance, Grundkiewicz et al. (2019) reverse the augmentation setting by applying transformations that flip the (binary) label. In their case, they introduce errors in correct sentences to obtain new training data for a grammar correction task.

(p6.3) Open Issues: While data augmentation is ubiquitous in the computer vision community and while most of the above-presented approaches are taskindependent, it has not found such widespread use in natural language processing. A reason might be that several of the approaches require an indepth understanding of the language. There is not yet a unified framework that allows applying data augmentation across tasks and languages. Recently, Longpre et al. (2020) hypothesised that data augmentation provides the same benefits as pretraining in transformer models. However, we argue that data augmentation might be better suited to leverage the insights of linguistic or domain experts in low-resource settings when unlabeled data or hardware resources are limited."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,s7,Distant & Weak Supervision,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4']",16,"1. In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified.
2. The corresponding labels are obtained through a (semi-)automatic process from an external source of information.
3. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations.
4. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012).
5. It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a).
6. The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019).
7. This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.
8. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).
9. While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP.
10. Nevertheless, distant supervision has also been successfully em-
11. (NER)3 build a discourse-structure dataset using guidance from sentiment annotations.
12. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (NER)0 or from entity lists (NER)1.
13. For some classification tasks, the labels can be rephrased with simple rules into sentences.
14. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (NER)2.
15. An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.
16. Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited.
17. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules.
18. It is an open question whether a task needs to have specific properties to be suitable for this approach.
19. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.
20. Distant supervision methods heavily rely on auxiliary data.
21. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.
22. Kann et al. (NER)3 find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages.
23. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.
24. While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.
25. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme.
26. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches.
27. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (NER)4.","(p7.0) In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

(p7.1) While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

(p7.2) Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

(p7.3) Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

(p7.4) While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020)."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,s8,Cross-Lingual Annotation Projections,"['p8.0', 'p8.1']",9,"1. For cross-lingual projections, a task-specific classifier is trained in a high-resource language.
2. Using parallel corpora, the unlabeled low-resource data is then aligned to its equivalent in the highresource language where labels can be obtained using the aforementioned classifier.
3. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts (Yarowsky et al., 2001).
4. This approach can, therefore, be seen as a form of distant supervision specific for obtaining labeled data for lowresource languages.
5. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (Täckström et al., 2013;Wisniewski et al., 2014;Plank and Agić, 2018;Eskander et al., 2020).
6. Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014;Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agić and Vulić, 2019).
7. Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019;Zhang et al., 2019a;Fei et al., 2020;Amjad et al., 2020).
8. Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense and telicity that are easier to identify in a different language (Zhou et al., 2015;Marasović et al., 2016;Friedrich and Gateva, 2017).
9. Open issues: Cross-lingual projections set high requirements on the auxiliary data needing both labels in a high-resource language and means to project them into a low-resource language.
10. Especially the latter can be an issue as machine translation by itself might be problematic for a specific low-resource language.
11. A limitation of the parallel corpora is their domains like political proceedings or religious texts.
12. Mayhew et al. (2017), Fang and Cohn (2017) and Karamanolakis et al. (Yarowsky et al., 2001)0 propose systems with fewer requirements based on word translations, bilingual dictionaries and taskspecific seed words, respectively.","(p8.0) For cross-lingual projections, a task-specific classifier is trained in a high-resource language. Using parallel corpora, the unlabeled low-resource data is then aligned to its equivalent in the highresource language where labels can be obtained using the aforementioned classifier. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts (Yarowsky et al., 2001). This approach can, therefore, be seen as a form of distant supervision specific for obtaining labeled data for lowresource languages. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (Täckström et al., 2013;Wisniewski et al., 2014;Plank and Agić, 2018;Eskander et al., 2020). Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014;Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agić and Vulić, 2019). Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019;Zhang et al., 2019a;Fei et al., 2020;Amjad et al., 2020). Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense and telicity that are easier to identify in a different language (Zhou et al., 2015;Marasović et al., 2016;Friedrich and Gateva, 2017).

(p8.1) Open issues: Cross-lingual projections set high requirements on the auxiliary data needing both labels in a high-resource language and means to project them into a low-resource language. Especially the latter can be an issue as machine translation by itself might be problematic for a specific low-resource language. A limitation of the parallel corpora is their domains like political proceedings or religious texts. Mayhew et al. (2017), Fang and Cohn (2017) and Karamanolakis et al. (2020) propose systems with fewer requirements based on word translations, bilingual dictionaries and taskspecific seed words, respectively."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,s9,Learning with Noisy Labels,"['p9.0', 'p9.1', 'p9.2', 'p9.3']",12,"1. The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations.
2. These labels tend, however, to contain more errors.
3. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance.
4. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision.
5. We categorize these into two ideas: noise filtering and noise modeling.
6. Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled.
7. This often includes training a classifier to make the filtering decision.
8. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019).
9. Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).
10. The noise in the labels can also be modeled.
11. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b).
12. The classifier is no longer trained directly on the noisily-labeled data.
13. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution.
14. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels.
15. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing.
16. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances.
17. Rehbein and Ruppenhofer (2017), Lison et al. (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019)0 and Ren et al. (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019)0 leverage several sources of distant supervision and learn how to combine them.
18. In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method.
19. Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly.
20. Related approaches learn latent variables (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019)1, use constrained binary learning (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019)2 or construct a loss assuming that only unlabeled positive instances exist (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019)3.","(p9.0) The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations. These labels tend, however, to contain more errors. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision. We categorize these into two ideas: noise filtering and noise modeling.

(p9.1) Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled. This often includes training a classifier to make the filtering decision. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019). Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).

(p9.2) The noise in the labels can also be modeled. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances. Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.

(p9.3) In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method. Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly. Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019)."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,s12,Pre-Trained Language Representations,"['p12.0', 'p12.1']",4,"1. Feature vectors are the core input component of many neural network-based models for NLP tasks.
2. They are numerical representations of words or sentences, as neural architectures do not allow the processing of strings and characters as such.
3. Collobert et al. (2011) showed that training these models for the task of language-modeling on a large-scale corpus results in high-quality word representations, which can be reused for other downstream tasks as well.
4. Subword-based embeddings such as fastText n-gram embeddings (Bojanowski et al., 2017) and byte-pair-encoding embeddings (Heinzerling and Strube, 2018) addressed out-of-vocabulary issues by splitting words into multiple subwords, which in combination represent the original word.
5. showed that these embeddings leveraging subword information are beneficial for lowresource sequence labeling tasks, such as named entity recognition and typing, and outperform wordlevel embeddings.
6. Jungmaier et al. (Bojanowski et al., 2017)4 added smoothing to word2vec models to correct its bias towards rare words and achieved improvements in particular for low-resource settings.
7. In addition, pre-trained embeddings were published for more than 270 languages for both embedding methods.
8. This enabled the processing of texts in many languages, including multiple low-resource languages found in Wikipedia.
9. More recently, a trend emerged of pre-training large embedding models using a language model objective to create contextaware word representations by predicting the next word or sentence.
10. This includes pre-trained transformer models (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b).
11. These methods are particularly helpful for low-resource languages for which large amounts of unlabeled data are available, but task-specific labeled data is scarce (Cruz and Cheng, 2019).
12. Open Issues: While pre-trained language models achieve significant performance increases compared to standard word embeddings, it is still questionable if these methods are suited for real-world low-resource scenarios.
13. For example, all of these models require large hardware requirements, in particular, considering that the transformer model size keeps increasing to boost performance (Raffel et al., 2020).
14. Therefore, these large-scale methods might not be suited for low-resource scenarios where hardware is also low-resource.
15. Biljon et al. (Bojanowski et al., 2017)4 showed that low-to mediumdepth transformer sizes perform better than larger models for low-resource languages and Schick and Schütze (Bojanowski et al., 2017)4 managed to train models with three orders of magnitude fewer parameters that perform on-par with large-scale models like GPT-3 on few-shot task by reformulating the training task and using ensembling.
16. Melamud et al. (Bojanowski et al., 2017)1 showed that simple bag-of-words approaches are better when there are only a few dozen training instances or less for text classification, while more complex transformer models require more training data.
17. Bhattacharjee et al. (Bojanowski et al., 2017)4 found that crossview training (Bojanowski et al., 2017)3 leverages large amounts of unlabeled data better for task-specific applications in contrast to the general representations learned by BERT.
18. Moreover, data quality for low-resource, even for unlabeled data, might not be comparable to data from high-resource languages.
19. Alabi et al. (Bojanowski et al., 2017)4 found that word embeddings trained on larger amounts of unlabeled data from low-resource languages are not competitive to embeddings trained on smaller, but curated data sources.","(p12.0) Feature vectors are the core input component of many neural network-based models for NLP tasks. They are numerical representations of words or sentences, as neural architectures do not allow the processing of strings and characters as such. Collobert et al. (2011) showed that training these models for the task of language-modeling on a large-scale corpus results in high-quality word representations, which can be reused for other downstream tasks as well. Subword-based embeddings such as fastText n-gram embeddings (Bojanowski et al., 2017) and byte-pair-encoding embeddings (Heinzerling and Strube, 2018) addressed out-of-vocabulary issues by splitting words into multiple subwords, which in combination represent the original word.  showed that these embeddings leveraging subword information are beneficial for lowresource sequence labeling tasks, such as named entity recognition and typing, and outperform wordlevel embeddings. Jungmaier et al. (2020) added smoothing to word2vec models to correct its bias towards rare words and achieved improvements in particular for low-resource settings. In addition, pre-trained embeddings were published for more than 270 languages for both embedding methods. This enabled the processing of texts in many languages, including multiple low-resource languages found in Wikipedia. More recently, a trend emerged of pre-training large embedding models using a language model objective to create contextaware word representations by predicting the next word or sentence. This includes pre-trained transformer models (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b). These methods are particularly helpful for low-resource languages for which large amounts of unlabeled data are available, but task-specific labeled data is scarce (Cruz and Cheng, 2019).

(p12.1) Open Issues: While pre-trained language models achieve significant performance increases compared to standard word embeddings, it is still questionable if these methods are suited for real-world low-resource scenarios. For example, all of these models require large hardware requirements, in particular, considering that the transformer model size keeps increasing to boost performance (Raffel et al., 2020). Therefore, these large-scale methods might not be suited for low-resource scenarios where hardware is also low-resource. Biljon et al. (2020) showed that low-to mediumdepth transformer sizes perform better than larger models for low-resource languages and Schick and Schütze (2020) managed to train models with three orders of magnitude fewer parameters that perform on-par with large-scale models like GPT-3 on few-shot task by reformulating the training task and using ensembling. Melamud et al. (2019) showed that simple bag-of-words approaches are better when there are only a few dozen training instances or less for text classification, while more complex transformer models require more training data. Bhattacharjee et al. (2020) found that crossview training (Clark et al., 2018) leverages large amounts of unlabeled data better for task-specific applications in contrast to the general representations learned by BERT. Moreover, data quality for low-resource, even for unlabeled data, might not be comparable to data from high-resource languages. Alabi et al. (2020) found that word embeddings trained on larger amounts of unlabeled data from low-resource languages are not competitive to embeddings trained on smaller, but curated data sources."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,s14,Multilingual Language Models,"['p14.0', 'p14.1', 'p14.2', 'p14.3', 'p14.4']",4,"1. Analogously to low-resource domains, lowresource languages can also benefit from labeled resources available in other high-resource languages.
2. This usually requires the training of multilingual language representations by combining monolingual representations (Lange et al., 2020a) or training a single model for many languages, such as multilingual BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020) .
3. These models are trained using unlabeled, monolingual corpora from different languages and can be used in crossand multilingual settings, due to many languages seen during pre-training.
4. In cross-lingual zero-shot learning, no taskspecific labeled data is available in the low-resource target language.
5. Instead, labeled data from a high-resource language is leveraged.
6. A multilingual model can be trained on the target task in a high-resource language and afterwards, applied to the unseen target languages, such as for named entity recognition ( 2020) proposed adding a minimal amount of target-task and -language data (in the range of 10 to 100 labeled sentences) which resulted in a significant boost in performance for classification in low-resource languages.
7. The transfer between two languages can be improved by creating a common multilingual embedding space of multiple languages.
8. This is useful for standard word embeddings  as well as pre-trained language models.
9. For example, by aligning the languages inside a single multilin-
10. This alignment is typically done by computing a mapping between two different embedding spaces, such that the words in both embeddings share similar feature vectors after the mapping (Mikolov et al., 2013;Joulin et al., 2018).
11. This allows to use different embeddings inside the same model and helps when two languages do not share the same space inside a single model (Cao et al., 2020).
12. For example, Zhang et al. (2019b) used bilingual representations by creating cross-lingual word embeddings using a small set of parallel sentences between the highresource language English and three low-resource African languages, Swahili, Tagalog, and Somali, to improve document retrieval performance for the African languages.
13. Open Issues: While these multilingual models are a tremendous step towards enabling NLP in many languages, possible claims that these are universal language models do not hold.
14. For example, mBERT covers 104 and XLM-R 100 languages, which is a third of all languages in Wikipedia as outlined earlier.
15. Further, Wu and Dredze (2020) showed that, in particular, low-resource languages are not well-represented in mBERT.
16. Figure 2 shows which language families with at least 1 million speakers are covered by mBERT and XLM-RoBERTa 2 .
17. In particular, African and American languages are not well-represented within the transformer models, even though millions of people speak these languages.
18. This can be problematic, as languages from more distant language families are less suited for transfer learning, as Lauscher et al.(2020) showed.","(p14.0) Analogously to low-resource domains, lowresource languages can also benefit from labeled resources available in other high-resource languages. This usually requires the training of multilingual language representations by combining monolingual representations (Lange et al., 2020a) or training a single model for many languages, such as multilingual BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020) . These models are trained using unlabeled, monolingual corpora from different languages and can be used in crossand multilingual settings, due to many languages seen during pre-training.

(p14.1) In cross-lingual zero-shot learning, no taskspecific labeled data is available in the low-resource target language. Instead, labeled data from a high-resource language is leveraged. A multilingual model can be trained on the target task in a high-resource language and afterwards, applied to the unseen target languages, such as for named entity recognition ( 2020) proposed adding a minimal amount of target-task and -language data (in the range of 10 to 100 labeled sentences) which resulted in a significant boost in performance for classification in low-resource languages.

(p14.2) The transfer between two languages can be improved by creating a common multilingual embedding space of multiple languages. This is useful for standard word embeddings  as well as pre-trained language models. For example, by aligning the languages inside a single multilin- This alignment is typically done by computing a mapping between two different embedding spaces, such that the words in both embeddings share similar feature vectors after the mapping (Mikolov et al., 2013;Joulin et al., 2018). This allows to use different embeddings inside the same model and helps when two languages do not share the same space inside a single model (Cao et al., 2020). For example, Zhang et al. (2019b) used bilingual representations by creating cross-lingual word embeddings using a small set of parallel sentences between the highresource language English and three low-resource African languages, Swahili, Tagalog, and Somali, to improve document retrieval performance for the African languages.

(p14.3) Open Issues: While these multilingual models are a tremendous step towards enabling NLP in many languages, possible claims that these are universal language models do not hold. For example, mBERT covers 104 and XLM-R 100 languages, which is a third of all languages in Wikipedia as outlined earlier. Further, Wu and Dredze (2020) showed that, in particular, low-resource languages are not well-represented in mBERT. Figure 2 shows which language families with at least 1 million speakers are covered by mBERT and XLM-RoBERTa 2 . In particular, African and American languages are not well-represented within the transformer models, even though millions of people speak these languages. This can be problematic, as languages from more distant language families are less suited for transfer learning, as Lauscher et al.

(p14.4) (2020) showed."
