corpusid,title,domain,url,section,section_title,paragraph,para_listed_answer,answer,indexed_answer,filtered_refids,filtered_refids_qualified,num_reference,segmented_answer,question,itemized_question,QA_pair,question_firstpara,itemized_question_firstpara,QA_pair_firstpara,question_firstsenteachpara,itemized_question_firstsenteachpara,QA_pair_firstsenteachpara,question_firstpara3,itemized_question_firstpara3,QA_pair_firstpara3,question_firstsenteachpara3,itemized_question_firstsenteachpara3,QA_pair_firstsenteachpara3
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s1,BERT embeddings,"['p1.0', 'p1.1']","[""Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence."", ""In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).""]","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","[['b22', 'b24'], ['b21', 'b65']]","[['b22', 'b24'], ['b21', 'b65']]",4,"1. Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.
2. In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.
3. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
4. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.
5. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.
6. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.
7. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).",How do BERT's contextualized embeddings differ from conventional static embeddings in representing word senses?,1. How do BERT's contextualized embeddings differ from conventional static embeddings in representing word senses?,"Questions:

1. How do BERT's contextualized embeddings differ from conventional static embeddings in representing word senses?

Answer:

(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).",How do BERT's contextualized embeddings differ from conventional static embeddings?,1. How do BERT's contextualized embeddings differ from conventional static embeddings?,"Questions:

1. How do BERT's contextualized embeddings differ from conventional static embeddings?

Answer:

(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).",How do BERT's contextual embeddings differ from conventional static embeddings?,1. How do BERT's contextual embeddings differ from conventional static embeddings?,"Questions:

1. How do BERT's contextual embeddings differ from conventional static embeddings?

Answer:

(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).",How does BERT's approach to embeddings differ from conventional static embeddings?,1. How does BERT's approach to embeddings differ from conventional static embeddings?,"Questions:

1. How does BERT's approach to embeddings differ from conventional static embeddings?

Answer:

(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).",How does BERT generate contextualized token representations?,1. How does BERT generate contextualized token representations?,"Questions:

1. How does BERT generate contextualized token representations?

Answer:

(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic)."
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s3,Syntactic knowledge,"['p3.0', 'p3.1', 'p3.2']","['As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.', '(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).', 'Regarding syntactic competence of BERT\'s MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT\'s encoding of syntactic structure does not indicate that it actually relies on that knowledge.']","As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","(p3.0) As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(p3.1) (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

(p3.2) Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","[[], ['b5', 'b18'], [None, 'b64', 'b59']]","[[], ['b5', 'b18'], [None, 'b64', 'b59']]",5,"1. As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it.
2. Htut et al.(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root.
3. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation.
4. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2).
5. Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).
6. Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task.
7. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences.
8. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations.
9. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input.
10. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019).
11. This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.",How does BERT represent and utilize syntactic information in its model architecture and tasks?,1. How does BERT represent and utilize syntactic information in its model architecture and tasks?,"Questions:

1. How does BERT represent and utilize syntactic information in its model architecture and tasks?

Answer:

(p3.0) As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(p3.1) (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

(p3.2) Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.",How is syntactic information represented in BERT's self-attention mechanism?,1. How is syntactic information represented in BERT's self-attention mechanism?,"Questions:

1. How is syntactic information represented in BERT's self-attention mechanism?

Answer:

(p3.0) As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(p3.1) (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

(p3.2) Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.",How does BERT represent syntactic information and demonstrate syntactic competence?,1. How does BERT represent syntactic information and demonstrate syntactic competence?,"Questions:

1. How does BERT represent syntactic information and demonstrate syntactic competence?

Answer:

(p3.0) As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(p3.1) (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

(p3.2) Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.",How is syntactic information represented in BERT models?,1. How is syntactic information represented in BERT models?,"Questions:

1. How is syntactic information represented in BERT models?

Answer:

(p3.0) As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(p3.1) (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

(p3.2) Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.",How is syntactic information represented and utilized in BERT's architecture?,1. How is syntactic information represented and utilized in BERT's architecture?,"Questions:

1. How is syntactic information represented and utilized in BERT's architecture?

Answer:

(p3.0) As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(p3.1) (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

(p3.2) Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge."
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s7,Self-attention heads,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4', 'p7.5', 'p7.6', 'p7.7', 'p7.8', 'p7.9']","['Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:', '• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);', '• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).', 'Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.', '[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.', 'Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.', 'Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.', ""(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis."", ""Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data."", ""Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.""]","Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","(p7.0) Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

(p7.1) • attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

(p7.2) • attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

(p7.3) Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

(p7.4) [SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

(p7.5) Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

(p7.6) Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(p7.7) (2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

(p7.8) Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

(p7.9) Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","[[], [], [None, 'b9'], [None], [], [], [], [None, 'b58'], [], [None, 'b9']]","[[], [], [None, 'b9'], [None], [], [], [], [None, 'b58'], [], [None, 'b9']]",7,"1. Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types: attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018); attending to previous/next tokens,    (Kovaleva et al., 2019)
2. According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"".
3. However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 .
4. Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019).
5. This apparent redundancy must be related to the overparametrization issue (see section 7).
6. Attention to (Kovaleva et al., 2019)5 is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to (Kovaleva et al., 2019)4 and punctuation.
7. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as (Kovaleva et al., 2019)5 and (Kovaleva et al., 2019)4, and the model learns to rely on them.
8. They suggest also that the function of (Kovaleva et al., 2019)4 might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.
9. (Kovaleva et al., 2019)4 gets increased attention starting in layer 5, but its importance for prediction drops.
10. If this hypothesis is correct, attention probing studies that excluded the (Kovaleva et al., 2019)4 and (Kovaleva et al., 2019)5 tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.
11. Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.
12. Some BERT heads seem to specialize in certain types of syntactic relations.
13. Htut et al.(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions.
14. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline.
15. The evidence for nsubj, advmod, and amod has some variation between these two studies.
16. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context.
17. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).
18. present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora.
19. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.
20. Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,.
21. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks.
22. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998).
23. Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.",How do self-attention heads in BERT models contribute to understanding syntactic and semantic relationships?,1. How do self-attention heads in BERT models contribute to understanding syntactic and semantic relationships?,"Questions:

1. How do self-attention heads in BERT models contribute to understanding syntactic and semantic relationships?

Answer:

(p7.0) Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

(p7.1) • attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

(p7.2) • attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

(p7.3) Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

(p7.4) [SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

(p7.5) Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

(p7.6) Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(p7.7) (2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

(p7.8) Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

(p7.9) Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.",How do studies classify attention head types in Transformer models?,1. How do studies classify attention head types in Transformer models?,"Questions:

1. How do studies classify attention head types in Transformer models?

Answer:

(p7.0) Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

(p7.1) • attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

(p7.2) • attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

(p7.3) Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

(p7.4) [SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

(p7.5) Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

(p7.6) Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(p7.7) (2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

(p7.8) Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

(p7.9) Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.",How do BERT's self-attention heads function in relation to syntactic and sentence-level representations?,1. How do BERT's self-attention heads function in relation to syntactic and sentence-level representations?,"Questions:

1. How do BERT's self-attention heads function in relation to syntactic and sentence-level representations?

Answer:

(p7.0) Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

(p7.1) • attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

(p7.2) • attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

(p7.3) Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

(p7.4) [SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

(p7.5) Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

(p7.6) Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(p7.7) (2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

(p7.8) Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

(p7.9) Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.",What roles do self-attention heads play in understanding Transformer models?,1. What roles do self-attention heads play in understanding Transformer models?,"Questions:

1. What roles do self-attention heads play in understanding Transformer models?

Answer:

(p7.0) Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

(p7.1) • attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

(p7.2) • attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

(p7.3) Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

(p7.4) [SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

(p7.5) Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

(p7.6) Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(p7.7) (2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

(p7.8) Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

(p7.9) Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.",How do self-attention heads in BERT models function and contribute to understanding syntactic relations?,1. How do self-attention heads in BERT models function and contribute to understanding syntactic relations?,"Questions:

1. How do self-attention heads in BERT models function and contribute to understanding syntactic relations?

Answer:

(p7.0) Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

(p7.1) • attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

(p7.2) • attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

(p7.3) Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

(p7.4) [SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

(p7.5) Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

(p7.6) Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(p7.7) (2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

(p7.8) Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

(p7.9) Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks."
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s8,BERT layers,"['p8.0', 'p8.1', 'p8.2', 'p8.3', 'p8.4']","['The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.', 'There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.', 'The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.', ""The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT."", 'The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.']","The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.","(p8.0) The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

(p8.1) There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

(p8.2) The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

(p8.3) The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

(p8.4) The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.","[[], ['b5'], ['b57', 'b50', 'b5'], [None, 'b9', 'b50'], ['b5']]","[[], ['b5'], ['b57', 'b50', 'b5'], [None, 'b9', 'b50'], ['b5']]",8,"1. The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings.
2. It stands to reason that the lower layers have the most linear word order information.
3. report a decrease in the knowledge of linear word order around layer 4 in BERT-base.
4. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.
5. There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers.
6. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large).
7. Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.
8. The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5).
9. There is conflicting evidence about syntactic chunks.
10. Tenney et al. (6-9 for base-BERT, 14-19 for BERT-large)0 conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling.
11. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing.
12. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.
13. The final layers of BERT are the most taskspecific.
14. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable .
15. In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019).
16. At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance.
17. Tenney et al. (6-9 for base-BERT, 14-19 for BERT-large)0 suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers.
18. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (6-9 for base-BERT, 14-19 for BERT-large)1.
19. But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial.
20. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.
21. The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"".
22. However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.",How does the distribution of linguistic information vary across different layers in BERT models?,1. How does the distribution of linguistic information vary across different layers in BERT models?,"Questions:

1. How does the distribution of linguistic information vary across different layers in BERT models?

Answer:

(p8.0) The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

(p8.1) There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

(p8.2) The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

(p8.3) The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

(p8.4) The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.",How does the knowledge of linear word order and hierarchical sentence structure vary across BERT layers?,1. How does the knowledge of linear word order and hierarchical sentence structure vary across BERT layers?,"Questions:

1. How does the knowledge of linear word order and hierarchical sentence structure vary across BERT layers?

Answer:

(p8.0) The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

(p8.1) There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

(p8.2) The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

(p8.3) The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

(p8.4) The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.",How is syntactic information represented across different layers in BERT according to recent studies?,1. How is syntactic information represented across different layers in BERT according to recent studies?,"Questions:

1. How is syntactic information represented across different layers in BERT according to recent studies?

Answer:

(p8.0) The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

(p8.1) There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

(p8.2) The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

(p8.3) The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

(p8.4) The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.",How does the knowledge of linear word order and hierarchical sentence structure vary across BERT layers?,1. How does the knowledge of linear word order and hierarchical sentence structure vary across BERT layers?,"Questions:

1. How does the knowledge of linear word order and hierarchical sentence structure vary across BERT layers?

Answer:

(p8.0) The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

(p8.1) There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

(p8.2) The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

(p8.3) The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

(p8.4) The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.",How does the distribution of linguistic features vary across BERT's layers?,1. How does the distribution of linguistic features vary across BERT's layers?,"Questions:

1. How does the distribution of linguistic features vary across BERT's layers?

Answer:

(p8.0) The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

(p8.1) There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

(p8.2) The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

(p8.3) The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

(p8.4) The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers."
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s10,Pre-training BERT,"['p10.0', 'p10.1', 'p10.2', 'p10.3', 'p10.4', 'p10.5', 'p10.6', 'p10.7', 'p10.8', 'p10.9']","['The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.', '• Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).', '• Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).', ""• Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);"", '• Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;', '• Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).', '• Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).', '• Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.', 'Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .', 'Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).']","The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

• Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

• Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

• Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

• Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

• Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

• Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

• Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).","(p10.0) The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

(p10.1) • Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

(p10.2) • Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

(p10.3) • Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

(p10.4) • Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

(p10.5) • Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

(p10.6) • Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

(p10.7) • Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

(p10.8) Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

(p10.9) Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).","[[], ['b8', 'b16', 'b23', None, 'b62'], ['b62', 'b75'], ['b8'], [], ['b47'], [], [], ['b17'], ['b9', 'b31']]","[[], ['b8', 'b16', 'b23', None, 'b62'], ['b62', 'b75'], ['b8'], [], ['b47'], [], [], ['b17'], ['b9', 'b31']]",12,"1. The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM).
2. Multiple studies have come up with alternative training objectives to improve on BERT.
3. Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting .
4. Wang et al. (2019a) Mikolov et al. (2013b).
5. Permutation language modeling. Yang et al. (MLM)6 replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order.
6. See also the n-gram word order reconstruction task (Wang et al., 2019a).
7. Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020); Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words; Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).
8. Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (MLM)0.
9. Clinchant et al. (MLM)6 propose replacing the MASK token with (MLM)2 token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.
10. Another obvious source of improvement is pretraining data.
11. Liu et al. (MLM)3 explore the benefits of increasing the corpus volume and longer training.
12. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (MLM)4, there are ongoing efforts to incorporate structured knowledge resources .
13. Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (MLM)5 and ERNIE .
14. Alternatively, SemBERT  integrates semantic role information with BERT representations.
15. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides.
16. Hao et al. (MLM)6 conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (MLM)7.
17. However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (MLM)8.","What alternative training objectives have been proposed to improve BERT's performance, and how do they differ from the original model?","1. What alternative training objectives have been proposed to improve BERT's performance, and how do they differ from the original model?","Questions:

1. What alternative training objectives have been proposed to improve BERT's performance, and how do they differ from the original model?

Answer:

(p10.0) The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

(p10.1) • Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

(p10.2) • Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

(p10.3) • Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

(p10.4) • Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

(p10.5) • Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

(p10.6) • Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

(p10.7) • Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

(p10.8) Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

(p10.9) Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).",What alternative training objectives have been proposed to improve upon the original BERT's pre-training tasks?,1. What alternative training objectives have been proposed to improve upon the original BERT's pre-training tasks?,"Questions:

1. What alternative training objectives have been proposed to improve upon the original BERT's pre-training tasks?

Answer:

(p10.0) The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

(p10.1) • Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

(p10.2) • Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

(p10.3) • Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

(p10.4) • Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

(p10.5) • Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

(p10.6) • Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

(p10.7) • Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

(p10.8) Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

(p10.9) Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).",What are the innovations in pre-training methods that enhance BERT's performance on various tasks?,1. What are the innovations in pre-training methods that enhance BERT's performance on various tasks?,"Questions:

1. What are the innovations in pre-training methods that enhance BERT's performance on various tasks?

Answer:

(p10.0) The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

(p10.1) • Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

(p10.2) • Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

(p10.3) • Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

(p10.4) • Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

(p10.5) • Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

(p10.6) • Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

(p10.7) • Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

(p10.8) Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

(p10.9) Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).",What alternative training objectives have been proposed to improve on the original BERT model's pre-training tasks?,1. What alternative training objectives have been proposed to improve on the original BERT model's pre-training tasks?,"Questions:

1. What alternative training objectives have been proposed to improve on the original BERT model's pre-training tasks?

Answer:

(p10.0) The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

(p10.1) • Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

(p10.2) • Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

(p10.3) • Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

(p10.4) • Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

(p10.5) • Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

(p10.6) • Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

(p10.7) • Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

(p10.8) Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

(p10.9) Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).",What are some recent modifications to BERT's pre-training tasks and objectives?,1. What are some recent modifications to BERT's pre-training tasks and objectives?,"Questions:

1. What are some recent modifications to BERT's pre-training tasks and objectives?

Answer:

(p10.0) The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

(p10.1) • Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

(p10.2) • Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

(p10.3) • Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

(p10.4) • Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

(p10.5) • Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

(p10.6) • Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

(p10.7) • Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

(p10.8) Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

(p10.9) Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019)."
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s11,Model architecture choices,"['p11.0', 'p11.1']","[""To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks."", 'Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.']","To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.","(p11.0) To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

(p11.1) Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.","[['b20', 'b58', 'b75', 'b17'], []]","[['b20', 'b58', 'b75', 'b17'], []]",4,"1. To date, the most systematic study of BERT architecture was performed by .
2. They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others.
3. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable.
4. Larger hidden representation size was consistently better, but the gains varied by setting.
5. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance.
6. They also publish their recommendations for other model parameters.
7. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.
8. observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.
9. Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers.
10. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.",What have studies revealed about the impact of architectural choices on BERT's performance and training efficiency?,1. What have studies revealed about the impact of architectural choices on BERT's performance and training efficiency?,"Questions:

1. What have studies revealed about the impact of architectural choices on BERT's performance and training efficiency?

Answer:

(p11.0) To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

(p11.1) Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.",What findings have been made regarding the impact of BERT's architecture choices on its performance?,1. What findings have been made regarding the impact of BERT's architecture choices on its performance?,"Questions:

1. What findings have been made regarding the impact of BERT's architecture choices on its performance?

Answer:

(p11.0) To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

(p11.1) Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.",How does the recursive training approach in BERT architecture improve model efficiency?,1. How does the recursive training approach in BERT architecture improve model efficiency?,"Questions:

1. How does the recursive training approach in BERT architecture improve model efficiency?

Answer:

(p11.0) To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

(p11.1) Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.",What findings have systematic studies on BERT architecture revealed about the impact of model parameters on performance?,1. What findings have systematic studies on BERT architecture revealed about the impact of model parameters on performance?,"Questions:

1. What findings have systematic studies on BERT architecture revealed about the impact of model parameters on performance?

Answer:

(p11.0) To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

(p11.1) Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.",What insights have been gained from systematic studies on BERT's architecture?,1. What insights have been gained from systematic studies on BERT's architecture?,"Questions:

1. What insights have been gained from systematic studies on BERT's architecture?

Answer:

(p11.0) To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

(p11.1) Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks."
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s12,Fine-tuning BERT,"['p12.0', 'p12.1', 'p12.2', 'p12.3', 'p12.4', 'p12.5', 'p12.6']","['Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).', '• Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).', 'With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.', '(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.', 'An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).', 'Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.', '7 How big should BERT be?']","Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

• Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

7 How big should BERT be?","(p12.0) Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

(p12.1) • Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

(p12.2) With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(p12.3) (2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

(p12.4) An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

(p12.5) Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

(p12.6) 7 How big should BERT be?","[[None, 'b9'], ['b83'], [], [], ['b26'], ['b28'], []]","[[None, 'b9'], ['b83'], [], [], ['b26'], ['b28'], []]",5,"1. Pre-training + fine-tuning workflow is a crucial part of BERT.
2. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand.
3. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns.
4. It is understandable why fine-tuning would increase the attention to  Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).
5. Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).
6. With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules.
7. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost.
8. Artetxe et al.(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.
9. An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).Initialization can have a dramatic effect on the training process (Petrov, 2010).
10. However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018).
11. Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation.
12. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.
13. 7 How big should BERT be?",What are the key strategies and challenges in fine-tuning BERT for various NLP tasks?,1. What are the key strategies and challenges in fine-tuning BERT for various NLP tasks?,"Questions:

1. What are the key strategies and challenges in fine-tuning BERT for various NLP tasks?

Answer:

(p12.0) Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

(p12.1) • Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

(p12.2) With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(p12.3) (2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

(p12.4) An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

(p12.5) Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

(p12.6) 7 How big should BERT be?",How does fine-tuning affect BERT's attention to linguistic patterns versus [SEP] tokens?,1. How does fine-tuning affect BERT's attention to linguistic patterns versus [SEP] tokens?,"Questions:

1. How does fine-tuning affect BERT's attention to linguistic patterns versus [SEP] tokens?

Answer:

(p12.0) Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

(p12.1) • Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

(p12.2) With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(p12.3) (2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

(p12.4) An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

(p12.5) Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

(p12.6) 7 How big should BERT be?",What are the key strategies and findings related to fine-tuning BERT for improved performance and efficiency?,1. What are the key strategies and findings related to fine-tuning BERT for improved performance and efficiency?,"Questions:

1. What are the key strategies and findings related to fine-tuning BERT for improved performance and efficiency?

Answer:

(p12.0) Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

(p12.1) • Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

(p12.2) With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(p12.3) (2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

(p12.4) An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

(p12.5) Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

(p12.6) 7 How big should BERT be?",What are the observed effects of fine-tuning BERT on its attention patterns?,1. What are the observed effects of fine-tuning BERT on its attention patterns?,"Questions:

1. What are the observed effects of fine-tuning BERT on its attention patterns?

Answer:

(p12.0) Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

(p12.1) • Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

(p12.2) With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(p12.3) (2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

(p12.4) An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

(p12.5) Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

(p12.6) 7 How big should BERT be?",What are some recent methods for preventing catastrophic forgetting when finetuning BERT?,1. What are some recent methods for preventing catastrophic forgetting when finetuning BERT?,"Questions:

1. What are some recent methods for preventing catastrophic forgetting when finetuning BERT?

Answer:

(p12.0) Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

(p12.1) • Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

(p12.2) With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(p12.3) (2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

(p12.4) An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

(p12.5) Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

(p12.6) 7 How big should BERT be?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s13,Overparametrization,"['p13.0', 'p13.1', 'p13.2', 'p13.3', 'p13.4']","['Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.', 'Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).', 'Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .', 'Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.', '(2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.']","Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.","(p13.0) Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

(p13.1) Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

(p13.2) Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

(p13.3) Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(p13.4) (2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.","[['b70', 'b43', 'b38'], ['b58', 'b46', 'b50', None, 'b9', 'b20', 'b37'], [], [], []]","[['b70', 'b43', 'b38'], ['b58', 'b46', 'b50', None, 'b9', 'b20', 'b37'], [], [], []]",10,"1. Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT.
2. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.
3. Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have.
4. Voita et al. (Strubell et al., 2019;Schwartz et al., 2019)3 showed that all but a few Transformer heads could be pruned without significant losses in performance.
5. For BERT, Clark et al. (Strubell et al., 2019;Schwartz et al., 2019)3 observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (Strubell et al., 2019;Schwartz et al., 2019)3 were able to reduce most layers to a single head.
6. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance.
7. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019).
8. Additionally, Tenney et al. (Strubell et al., 2019;Schwartz et al., 2019)0 examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (Strubell et al., 2019;Schwartz et al., 2019)1.
9. Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case.
10. In particular, the opposite was observed for subjectverb agreement (Strubell et al., 2019;Schwartz et al., 2019)2 and sentence subject detection .
11. Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers.
12. Clark et al.(Strubell et al., 2019;Schwartz et al., 2019)3 suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.",What are the implications of overparametrization in transformer-based models like BERT on computational efficiency and model performance?,1. What are the implications of overparametrization in transformer-based models like BERT on computational efficiency and model performance?,"Questions:

1. What are the implications of overparametrization in transformer-based models like BERT on computational efficiency and model performance?

Answer:

(p13.0) Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

(p13.1) Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

(p13.2) Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

(p13.3) Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(p13.4) (2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.",What are the concerns associated with the increasing size of transformer-based models like T5 compared to BERT?,1. What are the concerns associated with the increasing size of transformer-based models like T5 compared to BERT?,"Questions:

1. What are the concerns associated with the increasing size of transformer-based models like T5 compared to BERT?

Answer:

(p13.0) Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

(p13.1) Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

(p13.2) Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

(p13.3) Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(p13.4) (2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.",What are the implications of overparametrization in transformer-based models like BERT?,1. What are the implications of overparametrization in transformer-based models like BERT?,"Questions:

1. What are the implications of overparametrization in transformer-based models like BERT?

Answer:

(p13.0) Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

(p13.1) Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

(p13.2) Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

(p13.3) Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(p13.4) (2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.",What are the implications of overparametrization in transformer-based models?,1. What are the implications of overparametrization in transformer-based models?,"Questions:

1. What are the implications of overparametrization in transformer-based models?

Answer:

(p13.0) Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

(p13.1) Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

(p13.2) Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

(p13.3) Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(p13.4) (2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.",What are the implications of overparametrization in transformer-based models?,1. What are the implications of overparametrization in transformer-based models?,"Questions:

1. What are the implications of overparametrization in transformer-based models?

Answer:

(p13.0) Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

(p13.1) Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

(p13.2) Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

(p13.3) Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(p13.4) (2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training."
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s14,BERT compression,"['p14.0', 'p14.1', 'p14.2', 'p14.3', 'p14.4']","['Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.', 'Two main approaches include knowledge distillation and quantization.', 'The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).', ""The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware."", ""Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).""]","Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

Two main approaches include knowledge distillation and quantization.

The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).","(p14.0) Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

(p14.1) Two main approaches include knowledge distillation and quantization.

(p14.2) The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

(p14.3) The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

(p14.4) Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).","[[], [], ['b54', None, 'b46', 'b37'], ['b40', 'b76'], ['b72', 'b11']]","[[], [], ['b54', None, 'b46', 'b37'], ['b40', 'b76'], ['b72', 'b11']]",8,"1. Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss.
2. Such efforts to date are summarized in Table 1.
3. Two main approaches include knowledge distillation and quantization.
4. The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase).
5. This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).
6. The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019).
7. Note that this strategy often requires compatible hardware.
8. Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).",What are the main approaches to BERT compression and how do they function?,1. What are the main approaches to BERT compression and how do they function?,"Questions:

1. What are the main approaches to BERT compression and how do they function?

Answer:

(p14.0) Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

(p14.1) Two main approaches include knowledge distillation and quantization.

(p14.2) The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

(p14.3) The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

(p14.4) Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).",What are the methods and outcomes of compressing BERT models according to recent studies?,1. What are the methods and outcomes of compressing BERT models according to recent studies?,"Questions:

1. What are the methods and outcomes of compressing BERT models according to recent studies?

Answer:

(p14.0) Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

(p14.1) Two main approaches include knowledge distillation and quantization.

(p14.2) The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

(p14.3) The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

(p14.4) Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).",What are the main approaches to compressing BERT and how do they function?,1. What are the main approaches to compressing BERT and how do they function?,"Questions:

1. What are the main approaches to compressing BERT and how do they function?

Answer:

(p14.0) Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

(p14.1) Two main approaches include knowledge distillation and quantization.

(p14.2) The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

(p14.3) The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

(p14.4) Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).",What are the methods and outcomes of compressing BERT models?,1. What are the methods and outcomes of compressing BERT models?,"Questions:

1. What are the methods and outcomes of compressing BERT models?

Answer:

(p14.0) Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

(p14.1) Two main approaches include knowledge distillation and quantization.

(p14.2) The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

(p14.3) The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

(p14.4) Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).",What are the main methods for compressing BERT models with minimal loss in accuracy?,1. What are the main methods for compressing BERT models with minimal loss in accuracy?,"Questions:

1. What are the main methods for compressing BERT models with minimal loss in accuracy?

Answer:

(p14.0) Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

(p14.1) Two main approaches include knowledge distillation and quantization.

(p14.2) The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

(p14.3) The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

(p14.4) Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020)."
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s15,Multilingual BERT,"['p15.0', 'p15.1', 'p15.2', 'p15.3', 'p15.4', 'p15.5', 'p15.6', 'p15.7']","['Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).', 'mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.', 'mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.', 'At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.', 'To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.', 'Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).', 'Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.', 'Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.']","Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.","(p15.0) Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

(p15.1) mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

(p15.2) mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

(p15.3) At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

(p15.4) To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

(p15.5) Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

(p15.6) Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

(p15.7) Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.","[['b36', 'b68', 'b64', 'b13', 'b30'], ['b13', 'b30'], ['b30', 'b13', 'b68', 'b42'], ['b68', 'b13', 'b36', 'b30'], ['b50'], ['b64', 'b2'], ['b58', 'b50', None, 'b9', 'b20', 'b2'], ['b39', 'b66', 'b4', None, 'b56']]","[['b36', 'b68', 'b64', 'b13', 'b30'], ['b13', 'b30'], ['b30', 'b13', 'b68', 'b42'], ['b68', 'b13', 'b36', 'b30'], ['b50'], ['b64', 'b2'], ['b58', 'b50', None, 'b9', 'b20', 'b2'], ['b39', 'b66', 'b4', None, 'b56']]",29,"1. Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary).
2. Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing.
3. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019).
4. The model seems to naturally learn high-quality cross-lingual word alignments (110K wordpiece vocabulary)9, with caveats for open-class parts of speech (Cao et al., 2019).
5. Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (Wu and Dredze, 2019;Pires et al., 2019)8 note that this task could be solvable by simple lexical matches.
6. Pires et al. (Wu and Dredze, 2019;Pires et al., 2019)8 conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.
7. mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (110K wordpiece vocabulary)0, and adding the IDs in pre-training was not beneficial .
8. It is also aware of at least some typological language features (110K wordpiece vocabulary)1, and transfer between structurally similar languages works better Pires et al., 2019).
9. Singh et al. (Wu and Dredze, 2019;Pires et al., 2019)8 argue that if typological features structure its representation space, it could not be considered as interlingua.
10. However, Artetxe et al. (Wu and Dredze, 2019;Pires et al., 2019)8 show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.
11. At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (110K wordpiece vocabulary)4, and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (Wu and Dredze, 2019;Pires et al., 2019)8.
12. Pires et al. (Wu and Dredze, 2019;Pires et al., 2019)8 and Wu and Dredze (Wu and Dredze, 2019;Pires et al., 2019)8 hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages.
13. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary.
14. Artetxe et al. (Wu and Dredze, 2019;Pires et al., 2019)8 also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (110K wordpiece vocabulary)9 vocabulary, without any shared word-pieces.
15. To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge.
16. However, as Tenney et al. (Wu and Dredze, 2019;Pires et al., 2019)0 aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"".
17. There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis .
18. A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.
19. Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (Wu and Dredze, 2019;Pires et al., 2019)1 would not be sufficient (Wu and Dredze, 2019;Pires et al., 2019)2.
20. Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Wu and Dredze, 2019;Pires et al., 2019)3.
21. Head and layer ablation studies (Wu and Dredze, 2019;Pires et al., 2019)4 inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Wu and Dredze, 2019;Pires et al., 2019)5, the absence of heads that would perform parsing ""in general"" (Wu and Dredze, 2019;Pires et al., 2019)6.
22. Ablations are also problematic if the same information was duplicated elsewhere in the network.
23. To mitigate that, Michel et al. (Wu and Dredze, 2019;Pires et al., 2019)8 prune heads in the order set by a proxy importance score, and Voita et al. (Wu and Dredze, 2019;Pires et al., 2019)8
24. fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.
25. Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Wu and Dredze, 2019;Pires et al., 2019)9.
26. However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Rönnqvist et al., 2019)0.
27. Also, visualization is typically limited to qualitative analysis (Rönnqvist et al., 2019)1, and should not be interpreted as definitive evidence.",How does mBERT achieve cross-lingual transfer and what are its limitations?,1. How does mBERT achieve cross-lingual transfer and what are its limitations?,"Questions:

1. How does mBERT achieve cross-lingual transfer and what are its limitations?

Answer:

(p15.0) Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

(p15.1) mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

(p15.2) mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

(p15.3) At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

(p15.4) To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

(p15.5) Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

(p15.6) Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

(p15.7) Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.",How does Multilingual BERT achieve cross-lingual transfer and what are its limitations?,1. How does Multilingual BERT achieve cross-lingual transfer and what are its limitations?,"Questions:

1. How does Multilingual BERT achieve cross-lingual transfer and what are its limitations?

Answer:

(p15.0) Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

(p15.1) mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

(p15.2) mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

(p15.3) At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

(p15.4) To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

(p15.5) Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

(p15.6) Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

(p15.7) Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.",How does Multilingual BERT (mBERT) handle language identification and syntactic properties across different languages?,1. How does Multilingual BERT (mBERT) handle language identification and syntactic properties across different languages?,"Questions:

1. How does Multilingual BERT (mBERT) handle language identification and syntactic properties across different languages?

Answer:

(p15.0) Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

(p15.1) mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

(p15.2) mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

(p15.3) At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

(p15.4) To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

(p15.5) Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

(p15.6) Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

(p15.7) Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.",What are the capabilities and limitations of Multilingual BERT in cross-lingual transfer tasks?,1. What are the capabilities and limitations of Multilingual BERT in cross-lingual transfer tasks?,"Questions:

1. What are the capabilities and limitations of Multilingual BERT in cross-lingual transfer tasks?

Answer:

(p15.0) Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

(p15.1) mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

(p15.2) mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

(p15.3) At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

(p15.4) To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

(p15.5) Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

(p15.6) Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

(p15.7) Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.",What improvements have been proposed for enhancing multilingual BERT's performance?,1. What improvements have been proposed for enhancing multilingual BERT's performance?,"Questions:

1. What improvements have been proposed for enhancing multilingual BERT's performance?

Answer:

(p15.0) Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

(p15.1) mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

(p15.2) mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

(p15.3) At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

(p15.4) To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

(p15.5) Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

(p15.6) Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

(p15.7) Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence."
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s7,Linear Classifiers,"['p7.0', 'p7.1', 'p7.2', 'p7.3']","[""The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept."", ""Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue."", 'Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.', 'Limitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.']","The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.

Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.

Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.

Limitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.","(p7.0) The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.

(p7.1) Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.

(p7.2) Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.

(p7.3) Limitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.","[['b21', 'b38'], ['b55', 'b15'], [], []]","[['b21', 'b38'], ['b55', 'b15'], [], []]",4,"1. The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed.
2. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept.
3. The regularization of the classifier directly effects the weights and therefore the ranking of neurons.
4. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero.
5. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features.
6. This translates to discovering group neurons that are jointly responsible for a concept.
7. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.
8. Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018).
9. Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization.
10. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.
11. suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.
12. Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution.
13. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons.
14. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria.
15. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.
16. Limitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.",How do different regularization techniques in neuron-level probes influence the interpretation of deep NLP models?,1. How do different regularization techniques in neuron-level probes influence the interpretation of deep NLP models?,"Questions:

1. How do different regularization techniques in neuron-level probes influence the interpretation of deep NLP models?

Answer:

(p7.0) The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.

(p7.1) Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.

(p7.2) Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.

(p7.3) Limitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.",How does the choice of regularization affect neuron importance scoring in deep NLP model interpretation?,1. How does the choice of regularization affect neuron importance scoring in deep NLP model interpretation?,"Questions:

1. How does the choice of regularization affect neuron importance scoring in deep NLP model interpretation?

Answer:

(p7.0) The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.

(p7.1) Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.

(p7.2) Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.

(p7.3) Limitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.",What are the limitations and assumptions underlying the use of linear and Gaussian classifiers in neuron-level interpretation of deep NLP models?,1. What are the limitations and assumptions underlying the use of linear and Gaussian classifiers in neuron-level interpretation of deep NLP models?,"Questions:

1. What are the limitations and assumptions underlying the use of linear and Gaussian classifiers in neuron-level interpretation of deep NLP models?

Answer:

(p7.0) The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.

(p7.1) Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.

(p7.2) Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.

(p7.3) Limitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.",How do linear classifiers interpret neuron importance in deep NLP models?,1. How do linear classifiers interpret neuron importance in deep NLP models?,"Questions:

1. How do linear classifiers interpret neuron importance in deep NLP models?

Answer:

(p7.0) The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.

(p7.1) Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.

(p7.2) Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.

(p7.3) Limitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.",What are the limitations of using linear classifiers for neuron-level interpretation in deep NLP models?,1. What are the limitations of using linear classifiers for neuron-level interpretation in deep NLP models?,"Questions:

1. What are the limitations of using linear classifiers for neuron-level interpretation in deep NLP models?

Answer:

(p7.0) The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.

(p7.1) Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.

(p7.2) Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.

(p7.3) Limitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts."
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s8,Causation-based methods,"['p8.0', 'p8.1', 'p8.2', 'p8.3', 'p8.4']","[""The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction."", ""Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons."", 'Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.', 'Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.', 'Limitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.']","The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction.

Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.

Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.

Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.

Limitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.","(p8.0) The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction.

(p8.1) Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.

(p8.2) Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.

(p8.3) Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.

(p8.4) Limitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.","[[], ['b21', 'b22'], [None, 'b9', 'b56'], ['b1', 'b46', None, 'b49', 'b26'], [None]]","[[], ['b21', 'b22'], [None, 'b9', 'b56'], ['b1', 'b46', None, 'b49', 'b26'], [None]]",11,"1. The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts.
2. They do not inherently reflect their importance towards the model's performance.
3. Causation-based methods identify neurons with respect to model's prediction.
4. Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value.
5. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance.
6. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised).
7. The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a).
8. The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019).
9. Here, the output class serves as the concept against which we want to find the salient neurons.
10. Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007).
11. Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation .
12. Nevertheless all these approaches are approximations and may incur search errors.
13. Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018).
14. Dai et al. (supervised)2 used an attribution-based method to identify salient neurons with respect to a relational fact.
15. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (supervised)0 to identify top neu-rons that express a relational fact.
16. The work of Dai et al. (supervised)2 shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.
17. Limitation The attribution-based methods highlight salient neurons with respect to a prediction.
18. What concepts these salient neurons have learned is unknown.
19. Dai et al. (supervised)2 worked around this by limiting their study to model classes where each class serves as a concept.
20. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.",How do causation-based methods contribute to the interpretation of neuron importance in deep NLP models?,1. How do causation-based methods contribute to the interpretation of neuron importance in deep NLP models?,"Questions:

1. How do causation-based methods contribute to the interpretation of neuron importance in deep NLP models?

Answer:

(p8.0) The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction.

(p8.1) Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.

(p8.2) Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.

(p8.3) Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.

(p8.4) Limitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.",How do causation-based methods differ from other neuron analysis techniques in deep NLP models?,1. How do causation-based methods differ from other neuron analysis techniques in deep NLP models?,"Questions:

1. How do causation-based methods differ from other neuron analysis techniques in deep NLP models?

Answer:

(p8.0) The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction.

(p8.1) Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.

(p8.2) Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.

(p8.3) Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.

(p8.4) Limitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.",What are the principles and limitations of causation-based methods in neuron-level interpretation of deep NLP models?,1. What are the principles and limitations of causation-based methods in neuron-level interpretation of deep NLP models?,"Questions:

1. What are the principles and limitations of causation-based methods in neuron-level interpretation of deep NLP models?

Answer:

(p8.0) The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction.

(p8.1) Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.

(p8.2) Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.

(p8.3) Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.

(p8.4) Limitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.",How do causation-based methods contribute to understanding neuron importance in deep NLP models?,1. How do causation-based methods contribute to understanding neuron importance in deep NLP models?,"Questions:

1. How do causation-based methods contribute to understanding neuron importance in deep NLP models?

Answer:

(p8.0) The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction.

(p8.1) Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.

(p8.2) Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.

(p8.3) Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.

(p8.4) Limitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.",What are the causation-based methods for interpreting neuron functions in deep NLP models?,1. What are the causation-based methods for interpreting neuron functions in deep NLP models?,"Questions:

1. What are the causation-based methods for interpreting neuron functions in deep NLP models?

Answer:

(p8.0) The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction.

(p8.1) Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.

(p8.2) Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.

(p8.3) Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.

(p8.4) Limitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept."
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s9,Miscellaneous Methods,"['p9.0', 'p9.1', 'p9.2', 'p9.3', 'p9.4', 'p9.5', 'p9.6', 'p9.7', 'p9.8']","['In this section, we cover a diverse set of methods that do not fit in the above defined categories.', ""Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons."", 'Limitation Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation. A thorough evaluation is necessary to know its true potential and efficacy in NLP.', 'Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.', 'Limitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into. Moreover, the scope of the method is limited to local interpretation.', 'Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.', 'Limitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.', 'Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it. The search involves identifying neurons that behave similarly across the models. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.', 'Limitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons. They can nevertheless be useful in tandem with the other interpretation methods. For example Dalvi et al. (2019) intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models.']","In this section, we cover a diverse set of methods that do not fit in the above defined categories.

Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.

Limitation Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation. A thorough evaluation is necessary to know its true potential and efficacy in NLP.

Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.

Limitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into. Moreover, the scope of the method is limited to local interpretation.

Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.

Limitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.

Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it. The search involves identifying neurons that behave similarly across the models. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.

Limitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons. They can nevertheless be useful in tandem with the other interpretation methods. For example Dalvi et al. (2019) intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models.","(p9.0) In this section, we cover a diverse set of methods that do not fit in the above defined categories.

(p9.1) Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.

(p9.2) Limitation Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation. A thorough evaluation is necessary to know its true potential and efficacy in NLP.

(p9.3) Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.

(p9.4) Limitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into. Moreover, the scope of the method is limited to local interpretation.

(p9.5) Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.

(p9.6) Limitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.

(p9.7) Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it. The search involves identifying neurons that behave similarly across the models. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.

(p9.8) Limitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons. They can nevertheless be useful in tandem with the other interpretation methods. For example Dalvi et al. (2019) intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models.","[[], ['b19', 'b6', 'b37'], [], ['b35'], [], ['b29', 'b28'], [], ['b39'], [None]]","[[], ['b19', 'b6', 'b37'], [], ['b35'], [], ['b29', 'b28'], [], ['b39'], [None]]",8,"1. In this section, we cover a diverse set of methods that do not fit in the above defined categories.
2. Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations.
3. It is possible that a neuron represents a diverse concept which is not featured in the corpus.
4. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations.
5. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role.
6. Corpus generation has been widely explored in Computer Vision.
7. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron.
8. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs.
9. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.Limitation
10. Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation.
11. A thorough evaluation is necessary to know its true potential and efficacy in NLP.
12. Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function.
13. Given a model, the activations of an input sentence form a matrix.
14. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept.
15. MF is a local interpretation method.
16. It is commonly used in analyzing vision models (Olah et al., 2018).
17. We could not find any research using MF on the NLP models.
18. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.
19. Limitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons.
20. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into.
21. Moreover, the scope of the method is limited to local interpretation.
22. Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion.
23. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster.
24. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.
25. aimed at identifying redundant neurons in the network.
26. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them.
27. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.
28. Limitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically.
29. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.
30. Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it.
31. The search involves identifying neurons that behave similarly across the models.
32. Bau et al. (2018)1 used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models.
33. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model.
34. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (2018)0 to capture information that may be distributed in fewer dimensions than the whole representation.
35. Limitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons.
36. They can nevertheless be useful in tandem with the other interpretation methods.
37. For example Dalvi et al. (2018)1 intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models.",What are the limitations of neuron-level interpretation methods in deep NLP models?,1. What are the limitations of neuron-level interpretation methods in deep NLP models?,"Questions:

1. What are the limitations of neuron-level interpretation methods in deep NLP models?

Answer:

(p9.0) In this section, we cover a diverse set of methods that do not fit in the above defined categories.

(p9.1) Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.

(p9.2) Limitation Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation. A thorough evaluation is necessary to know its true potential and efficacy in NLP.

(p9.3) Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.

(p9.4) Limitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into. Moreover, the scope of the method is limited to local interpretation.

(p9.5) Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.

(p9.6) Limitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.

(p9.7) Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it. The search involves identifying neurons that behave similarly across the models. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.

(p9.8) Limitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons. They can nevertheless be useful in tandem with the other interpretation methods. For example Dalvi et al. (2019) intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models.",What are the diverse methods for neuron-level interpretation of deep NLP models not covered by standard categories?,1. What are the diverse methods for neuron-level interpretation of deep NLP models not covered by standard categories?,"Questions:

1. What are the diverse methods for neuron-level interpretation of deep NLP models not covered by standard categories?

Answer:

(p9.0) In this section, we cover a diverse set of methods that do not fit in the above defined categories.

(p9.1) Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.

(p9.2) Limitation Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation. A thorough evaluation is necessary to know its true potential and efficacy in NLP.

(p9.3) Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.

(p9.4) Limitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into. Moreover, the scope of the method is limited to local interpretation.

(p9.5) Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.

(p9.6) Limitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.

(p9.7) Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it. The search involves identifying neurons that behave similarly across the models. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.

(p9.8) Limitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons. They can nevertheless be useful in tandem with the other interpretation methods. For example Dalvi et al. (2019) intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models.",What are the miscellaneous methods for neuron-level interpretation in deep NLP models and their limitations?,1. What are the miscellaneous methods for neuron-level interpretation in deep NLP models and their limitations?,"Questions:

1. What are the miscellaneous methods for neuron-level interpretation in deep NLP models and their limitations?

Answer:

(p9.0) In this section, we cover a diverse set of methods that do not fit in the above defined categories.

(p9.1) Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.

(p9.2) Limitation Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation. A thorough evaluation is necessary to know its true potential and efficacy in NLP.

(p9.3) Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.

(p9.4) Limitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into. Moreover, the scope of the method is limited to local interpretation.

(p9.5) Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.

(p9.6) Limitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.

(p9.7) Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it. The search involves identifying neurons that behave similarly across the models. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.

(p9.8) Limitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons. They can nevertheless be useful in tandem with the other interpretation methods. For example Dalvi et al. (2019) intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models.",What are the diverse methods for neuron-level interpretation of deep NLP models?,1. What are the diverse methods for neuron-level interpretation of deep NLP models?,"Questions:

1. What are the diverse methods for neuron-level interpretation of deep NLP models?

Answer:

(p9.0) In this section, we cover a diverse set of methods that do not fit in the above defined categories.

(p9.1) Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.

(p9.2) Limitation Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation. A thorough evaluation is necessary to know its true potential and efficacy in NLP.

(p9.3) Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.

(p9.4) Limitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into. Moreover, the scope of the method is limited to local interpretation.

(p9.5) Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.

(p9.6) Limitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.

(p9.7) Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it. The search involves identifying neurons that behave similarly across the models. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.

(p9.8) Limitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons. They can nevertheless be useful in tandem with the other interpretation methods. For example Dalvi et al. (2019) intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models.",What are the limitations of neuron-level interpretation methods in deep NLP models?,1. What are the limitations of neuron-level interpretation methods in deep NLP models?,"Questions:

1. What are the limitations of neuron-level interpretation methods in deep NLP models?

Answer:

(p9.0) In this section, we cover a diverse set of methods that do not fit in the above defined categories.

(p9.1) Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.

(p9.2) Limitation Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation. A thorough evaluation is necessary to know its true potential and efficacy in NLP.

(p9.3) Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.

(p9.4) Limitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into. Moreover, the scope of the method is limited to local interpretation.

(p9.5) Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.

(p9.6) Limitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.

(p9.7) Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it. The search involves identifying neurons that behave similarly across the models. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.

(p9.8) Limitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons. They can nevertheless be useful in tandem with the other interpretation methods. For example Dalvi et al. (2019) intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models."
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s18,Lexical Concepts,"['p18.0', 'p18.1']","['Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.', 'Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.']","Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.

Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.","(p18.0) Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.

(p18.1) Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.","[[], ['b22', 'b34', 'b20', 'b19', 'b37']]","[[], ['b22', 'b34', 'b20', 'b19', 'b37']]",5,"1. Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.
2. Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end.
3. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment.
4. For example ""I like this movie a lot"" or ""the movie is incredibly good"".
5. Similarly they discovered neurons that captured ""negation"".
6. Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained.
7. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task.
8. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"".
9. Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc.
10. They also found neurons that learn phrasal concepts.
11. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation.
12. They provided finer interpretation of the neurons by generating synthetic instances.
13. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.",How do deep NLP models use neurons to capture and interpret lexical concepts?,1. How do deep NLP models use neurons to capture and interpret lexical concepts?,"Questions:

1. How do deep NLP models use neurons to capture and interpret lexical concepts?

Answer:

(p18.0) Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.

(p18.1) Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.",How have researchers identified neurons that capture lexical concepts in deep NLP models?,1. How have researchers identified neurons that capture lexical concepts in deep NLP models?,"Questions:

1. How have researchers identified neurons that capture lexical concepts in deep NLP models?

Answer:

(p18.0) Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.

(p18.1) Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.",How do visualization and concept search methods contribute to neuron-level interpretation of lexical concepts in deep NLP models?,1. How do visualization and concept search methods contribute to neuron-level interpretation of lexical concepts in deep NLP models?,"Questions:

1. How do visualization and concept search methods contribute to neuron-level interpretation of lexical concepts in deep NLP models?

Answer:

(p18.0) Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.

(p18.1) Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.",How do researchers identify neurons that capture lexical concepts in deep NLP models?,1. How do researchers identify neurons that capture lexical concepts in deep NLP models?,"Questions:

1. How do researchers identify neurons that capture lexical concepts in deep NLP models?

Answer:

(p18.0) Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.

(p18.1) Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.",How do visualization techniques help identify lexical concept neurons in deep NLP models?,1. How do visualization techniques help identify lexical concept neurons in deep NLP models?,"Questions:

1. How do visualization techniques help identify lexical concept neurons in deep NLP models?

Answer:

(p18.0) Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.

(p18.1) Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron."
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s20,Linguistic Concepts,"['p20.0', 'p20.1', 'p20.2', 'p20.3']","[""A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018). § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012). Below we discuss major findings along this line of work:"", 'Neurons specialize in core linguistic concepts Dalvi et al. (2019) in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.', 'Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.', 'Neurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.']","A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018). § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012). Below we discuss major findings along this line of work:

Neurons specialize in core linguistic concepts Dalvi et al. (2019) in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.

Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.

Neurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.","(p20.0) A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018). § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012). Below we discuss major findings along this line of work:

(p20.1) Neurons specialize in core linguistic concepts Dalvi et al. (2019) in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.

(p20.2) Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.

(p20.3) Neurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.","[['b27', 'b51', 'b18'], [None], ['b54', None], ['b21', 'b34', 'b42', 'b20']]","[['b27', 'b51', 'b18'], [None], ['b54', None], ['b21', 'b34', 'b42', 'b20']]",10,"1. A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018).
2. § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012).
3. Below we discuss major findings along this line of work:Neurons specialize in core linguistic concepts Dalvi et al. (Vauquois, 1968;Jones et al., 2012)2 in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept.
4. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.
5. Neurons exhibit monosemous and polysemous behavior.
6. Xin et al. (Vauquois, 1968;Jones et al., 2012)2 found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis.
7. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc.
8. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"".
9. concepts. Suau et al. (2020) discovered neurons that capture different senses of a word.
10. Similarly, Bau et al. (Vauquois, 1968;Jones et al., 2012)2 found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.
11. Neurons capture syntactic concepts and complex semantic concepts.
12. Lakretz et al. (Vauquois, 1968;Jones et al., 2012)2 discovered neurons that capture subject-verb agreement within LSTM gates.
13. Karpathy et al. (Vauquois, 1968;Jones et al., 2012)1 also found neurons that activate within quotes and brackets capturing long-range dependency.
14. Na et al. (Vauquois, 1968;Jones et al., 2012)2 aligned neurons with syntactic parses to show that neurons learn syntactic phrases.
15. Seyffarth et al. (Vauquois, 1968;Jones et al., 2012)3 analyzed complex semantic properties underlying a given sentence.",How do neurons in deep NLP models capture and differentiate between core linguistic concepts?,1. How do neurons in deep NLP models capture and differentiate between core linguistic concepts?,"Questions:

1. How do neurons in deep NLP models capture and differentiate between core linguistic concepts?

Answer:

(p20.0) A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018). § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012). Below we discuss major findings along this line of work:

(p20.1) Neurons specialize in core linguistic concepts Dalvi et al. (2019) in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.

(p20.2) Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.

(p20.3) Neurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.",How do deep NLP models capture core-linguistic concepts according to recent studies?,1. How do deep NLP models capture core-linguistic concepts according to recent studies?,"Questions:

1. How do deep NLP models capture core-linguistic concepts according to recent studies?

Answer:

(p20.0) A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018). § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012). Below we discuss major findings along this line of work:

(p20.1) Neurons specialize in core linguistic concepts Dalvi et al. (2019) in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.

(p20.2) Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.

(p20.3) Neurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.",How do neurons in deep NLP models capture and differentiate between core linguistic concepts?,1. How do neurons in deep NLP models capture and differentiate between core linguistic concepts?,"Questions:

1. How do neurons in deep NLP models capture and differentiate between core linguistic concepts?

Answer:

(p20.0) A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018). § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012). Below we discuss major findings along this line of work:

(p20.1) Neurons specialize in core linguistic concepts Dalvi et al. (2019) in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.

(p20.2) Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.

(p20.3) Neurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.",What are the key linguistic concepts captured by neurons in deep NLP models?,1. What are the key linguistic concepts captured by neurons in deep NLP models?,"Questions:

1. What are the key linguistic concepts captured by neurons in deep NLP models?

Answer:

(p20.0) A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018). § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012). Below we discuss major findings along this line of work:

(p20.1) Neurons specialize in core linguistic concepts Dalvi et al. (2019) in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.

(p20.2) Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.

(p20.3) Neurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.",How do neurons in deep NLP models capture and specialize in linguistic concepts?,1. How do neurons in deep NLP models capture and specialize in linguistic concepts?,"Questions:

1. How do neurons in deep NLP models capture and specialize in linguistic concepts?

Answer:

(p20.0) A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018). § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012). Below we discuss major findings along this line of work:

(p20.1) Neurons specialize in core linguistic concepts Dalvi et al. (2019) in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.

(p20.2) Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.

(p20.3) Neurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s1,Parallel Architectures,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5', 'p1.6', 'p1.7', 'p1.8']","['As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers. In this case, there is no dependency other than layer sharing among tasks. Therefore, there is no constraint on the order of training samples from each task. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks. Fig. 1 illustrates different forms of parallel architectures.', '2.1.1 Vanilla Tree-like Architectures. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches). A shallow trunk can be simply the word representation layer [108] while a deep trunk can be the entire model except output layers. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153]. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.', 'The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task. A solution is to equip the shared trunk with task-specific encoders [47,58,137]. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages. Another way is to make different groups of tasks share different parts of the trunk [37,79,89]. Moreover, this idea can be applied to the decoder. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.', '2.1.2 Parallel Feature Fusion. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task. As shown in Fig. 1b, such models can use a globally shared encoder represents the corresponding label (ℎ are shared latent representations). The green blocks represent shared parameters and the orange blocks are task-specific parameters. Red circles represent feature fusion mechanism .', 'to produce shared representations that can be used as additional features for each task-specific model [69]. The shared representations can also be used indirectly as the key for attention layers in each task-specific model [118].', 'However, different parts of the shared features are not equally important to each task. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | · | compute the absolute value in an element-wise manner. After that, S is concatenated with task-specific representations to form the input to the output layer.', 'Some models directly integrate features from different tasks. A straightforward solution is to compute a weighted average of feature representations. Here the weights can be computed separately according to the input [61] or by an attention mechanism as in [153] that learns task representations as query keys in attention modules. In addition to weighted average, we can aggregate features via more sophisticated mechanisms. For example, based on the cross-stitch network [81], [22] proposes a gated network where shared and task-specific features are combined by a gating mechanism. A similar gating mechanism is used in [56] to combine features from the primary and the auxiliary task. The SLUICE network [100] learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in [135] to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph . The update gate z determines how much information should be maintained from task and then it emits final outputh for task . Mathematically, the two gates can be formulated as', 'where (·) denotes the sigmoid function and tanh(·) denotes the hyperbolic tangent function. When considering all pairwise directions, the output for each task is given by the sum of each row in', 'Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task. Given tasks, the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .']","As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers. In this case, there is no dependency other than layer sharing among tasks. Therefore, there is no constraint on the order of training samples from each task. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks. Fig. 1 illustrates different forms of parallel architectures.

2.1.1 Vanilla Tree-like Architectures. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches). A shallow trunk can be simply the word representation layer [108] while a deep trunk can be the entire model except output layers. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153]. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.

The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task. A solution is to equip the shared trunk with task-specific encoders [47,58,137]. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages. Another way is to make different groups of tasks share different parts of the trunk [37,79,89]. Moreover, this idea can be applied to the decoder. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.

2.1.2 Parallel Feature Fusion. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task. As shown in Fig. 1b, such models can use a globally shared encoder represents the corresponding label (ℎ are shared latent representations). The green blocks represent shared parameters and the orange blocks are task-specific parameters. Red circles represent feature fusion mechanism .

to produce shared representations that can be used as additional features for each task-specific model [69]. The shared representations can also be used indirectly as the key for attention layers in each task-specific model [118].

However, different parts of the shared features are not equally important to each task. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | · | compute the absolute value in an element-wise manner. After that, S is concatenated with task-specific representations to form the input to the output layer.

Some models directly integrate features from different tasks. A straightforward solution is to compute a weighted average of feature representations. Here the weights can be computed separately according to the input [61] or by an attention mechanism as in [153] that learns task representations as query keys in attention modules. In addition to weighted average, we can aggregate features via more sophisticated mechanisms. For example, based on the cross-stitch network [81], [22] proposes a gated network where shared and task-specific features are combined by a gating mechanism. A similar gating mechanism is used in [56] to combine features from the primary and the auxiliary task. The SLUICE network [100] learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in [135] to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph . The update gate z determines how much information should be maintained from task and then it emits final outputh for task . Mathematically, the two gates can be formulated as

where (·) denotes the sigmoid function and tanh(·) denotes the hyperbolic tangent function. When considering all pairwise directions, the output for each task is given by the sum of each row in

Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task. Given tasks, the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .","(p1.0) As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers. In this case, there is no dependency other than layer sharing among tasks. Therefore, there is no constraint on the order of training samples from each task. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks. Fig. 1 illustrates different forms of parallel architectures.

(p1.1) 2.1.1 Vanilla Tree-like Architectures. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches). A shallow trunk can be simply the word representation layer [108] while a deep trunk can be the entire model except output layers. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153]. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.

(p1.2) The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task. A solution is to equip the shared trunk with task-specific encoders [47,58,137]. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages. Another way is to make different groups of tasks share different parts of the trunk [37,79,89]. Moreover, this idea can be applied to the decoder. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.

(p1.3) 2.1.2 Parallel Feature Fusion. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task. As shown in Fig. 1b, such models can use a globally shared encoder represents the corresponding label (ℎ are shared latent representations). The green blocks represent shared parameters and the orange blocks are task-specific parameters. Red circles represent feature fusion mechanism .

(p1.4) to produce shared representations that can be used as additional features for each task-specific model [69]. The shared representations can also be used indirectly as the key for attention layers in each task-specific model [118].

(p1.5) However, different parts of the shared features are not equally important to each task. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | · | compute the absolute value in an element-wise manner. After that, S is concatenated with task-specific representations to form the input to the output layer.

(p1.6) Some models directly integrate features from different tasks. A straightforward solution is to compute a weighted average of feature representations. Here the weights can be computed separately according to the input [61] or by an attention mechanism as in [153] that learns task representations as query keys in attention modules. In addition to weighted average, we can aggregate features via more sophisticated mechanisms. For example, based on the cross-stitch network [81], [22] proposes a gated network where shared and task-specific features are combined by a gating mechanism. A similar gating mechanism is used in [56] to combine features from the primary and the auxiliary task. The SLUICE network [100] learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in [135] to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph . The update gate z determines how much information should be maintained from task and then it emits final outputh for task . Mathematically, the two gates can be formulated as

(p1.7) where (·) denotes the sigmoid function and tanh(·) denotes the hyperbolic tangent function. When considering all pairwise directions, the output for each task is given by the sum of each row in

(p1.8) Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task. Given tasks, the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .","[[], ['b107', None, 'b6'], ['b136', 'b57', 'b46', 'b36', 'b78', 'b64', 'b88', 'b127'], [], ['b117', 'b68'], ['b132', 'b68', 'b38', 'b81', 'b147'], ['b21', 'b55', 'b60', 'b152', 'b80', 'b99', 'b134'], [], ['b135', 'b100', 'b143']]","[[], ['b107', None, 'b6'], ['b136', 'b57', 'b46', 'b36', 'b78', 'b64', 'b88', 'b127'], [], ['b117', 'b68'], ['b132', 'b68', 'b38', 'b81', 'b147'], ['b21', 'b55', 'b60', 'b152', 'b80', 'b99', 'b134'], [], ['b135', 'b100', 'b143']]",28,"1. As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers.
2. In this case, there is no dependency other than layer sharing among tasks.
3. Therefore, there is no constraint on the order of training samples from each task.
4. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks.
5. Fig. 1 illustrates different forms of parallel architectures.
6. 2.1.1 Vanilla Tree-like Architectures.
7. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches).
8. A shallow trunk can be simply the word representation layer [108] while a deep trunk can be the entire model except output layers.
9. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153].
10. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.
11. The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task.
12. A solution is to equip the shared trunk with task-specific encoders [47,58,137].
13. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages.
14. Another way is to make different groups of tasks share different parts of the trunk [37,79,89].
15. Moreover, this idea can be applied to the decoder.
16. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.
17. 2.1.2 Parallel Feature Fusion. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task.
18. As shown in Fig. 1b, such models can use a globally shared encoder represents the corresponding label (i.e., the trunk)0.
19. The green blocks represent shared parameters and the orange blocks are task-specific parameters.
20. Red circles represent feature fusion mechanism .to produce shared representations that can be used as additional features for each task-specific model (i.e., the trunk)3.
21. The shared representations can also be used indirectly as the key for attention layers in each task-specific model (i.e., the trunk)2.
22. However, different parts of the shared features are not equally important to each task.
23. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks.
24. For example, (i.e., the trunk)3 adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate.
25. Similarly, GIRN (i.e., the trunk)4 interleaves hidden states of LSTMs and (i.e., the trunk)5 extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks.
26. The sifted multi-task learning method (i.e., the trunk)6 filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism.
27. The gated shared feature G and attention result A are combined, as in (i.e., the trunk)7, to form the entire shared feature representation S = (i.e., the trunk)8 for each task, where ⊙ represents the element-wise multiplication and | ·
28. | compute the absolute value in an element-wise manner.
29. After that, S is concatenated with task-specific representations to form the input to the output layer.
30. Some models directly integrate features from different tasks.
31. A straightforward solution is to compute a weighted average of feature representations.
32. Here the weights can be computed separately according to the input (i.e., the trunk)9 or by an attention mechanism as in (i.e., the branches)0 that learns task representations as query keys in attention modules.
33. In addition to weighted average, we can aggregate features via more sophisticated mechanisms.
34. For example, based on the cross-stitch network (i.e., the branches)1, (i.e., the branches)2 proposes a gated network where shared and task-specific features are combined by a gating mechanism.
35. A similar gating mechanism is used in (i.e., the branches)3 to combine features from the primary and the auxiliary task.
36. The SLUICE network (i.e., the branches)4 learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks.
37. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in (i.e., the branches)5 to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates.
38. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph .
39. The update gate z determines how much information should be maintained from task and then it emits final outputh for task .
40. Mathematically, the two gates can be formulated aswhere (i.e., the branches)7 denotes the sigmoid function and tanh(i.e., the branches)7 denotes the hyperbolic tangent function.
41. When considering all pairwise directions, the output for each task is given by the sum of each row inTask routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task.
42. Given tasks , the routing network (i.e., the branches)8 splits RNN cells into several shared blocks with task-specific blocks (i.e., the branches)9
43. and then modulates the input to as well as output from each RNN block by a learned weight.
44. MCapsNet [108]0, which brings CapsNet [108]1 to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task.
45. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients [108]2 for capsule in the current layer and capsule in the next layer for task .",How do parallel architectures in multi-task learning models facilitate knowledge sharing among tasks?,1. How do parallel architectures in multi-task learning models facilitate knowledge sharing among tasks?,"Questions:

1. How do parallel architectures in multi-task learning models facilitate knowledge sharing among tasks?

Answer:

(p1.0) As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers. In this case, there is no dependency other than layer sharing among tasks. Therefore, there is no constraint on the order of training samples from each task. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks. Fig. 1 illustrates different forms of parallel architectures.

(p1.1) 2.1.1 Vanilla Tree-like Architectures. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches). A shallow trunk can be simply the word representation layer [108] while a deep trunk can be the entire model except output layers. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153]. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.

(p1.2) The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task. A solution is to equip the shared trunk with task-specific encoders [47,58,137]. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages. Another way is to make different groups of tasks share different parts of the trunk [37,79,89]. Moreover, this idea can be applied to the decoder. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.

(p1.3) 2.1.2 Parallel Feature Fusion. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task. As shown in Fig. 1b, such models can use a globally shared encoder represents the corresponding label (ℎ are shared latent representations). The green blocks represent shared parameters and the orange blocks are task-specific parameters. Red circles represent feature fusion mechanism .

(p1.4) to produce shared representations that can be used as additional features for each task-specific model [69]. The shared representations can also be used indirectly as the key for attention layers in each task-specific model [118].

(p1.5) However, different parts of the shared features are not equally important to each task. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | · | compute the absolute value in an element-wise manner. After that, S is concatenated with task-specific representations to form the input to the output layer.

(p1.6) Some models directly integrate features from different tasks. A straightforward solution is to compute a weighted average of feature representations. Here the weights can be computed separately according to the input [61] or by an attention mechanism as in [153] that learns task representations as query keys in attention modules. In addition to weighted average, we can aggregate features via more sophisticated mechanisms. For example, based on the cross-stitch network [81], [22] proposes a gated network where shared and task-specific features are combined by a gating mechanism. A similar gating mechanism is used in [56] to combine features from the primary and the auxiliary task. The SLUICE network [100] learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in [135] to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph . The update gate z determines how much information should be maintained from task and then it emits final outputh for task . Mathematically, the two gates can be formulated as

(p1.7) where (·) denotes the sigmoid function and tanh(·) denotes the hyperbolic tangent function. When considering all pairwise directions, the output for each task is given by the sum of each row in

(p1.8) Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task. Given tasks, the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .",How do parallel architectures in multi-task learning enable knowledge sharing among tasks?,1. How do parallel architectures in multi-task learning enable knowledge sharing among tasks?,"Questions:

1. How do parallel architectures in multi-task learning enable knowledge sharing among tasks?

Answer:

(p1.0) As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers. In this case, there is no dependency other than layer sharing among tasks. Therefore, there is no constraint on the order of training samples from each task. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks. Fig. 1 illustrates different forms of parallel architectures.

(p1.1) 2.1.1 Vanilla Tree-like Architectures. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches). A shallow trunk can be simply the word representation layer [108] while a deep trunk can be the entire model except output layers. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153]. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.

(p1.2) The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task. A solution is to equip the shared trunk with task-specific encoders [47,58,137]. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages. Another way is to make different groups of tasks share different parts of the trunk [37,79,89]. Moreover, this idea can be applied to the decoder. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.

(p1.3) 2.1.2 Parallel Feature Fusion. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task. As shown in Fig. 1b, such models can use a globally shared encoder represents the corresponding label (ℎ are shared latent representations). The green blocks represent shared parameters and the orange blocks are task-specific parameters. Red circles represent feature fusion mechanism .

(p1.4) to produce shared representations that can be used as additional features for each task-specific model [69]. The shared representations can also be used indirectly as the key for attention layers in each task-specific model [118].

(p1.5) However, different parts of the shared features are not equally important to each task. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | · | compute the absolute value in an element-wise manner. After that, S is concatenated with task-specific representations to form the input to the output layer.

(p1.6) Some models directly integrate features from different tasks. A straightforward solution is to compute a weighted average of feature representations. Here the weights can be computed separately according to the input [61] or by an attention mechanism as in [153] that learns task representations as query keys in attention modules. In addition to weighted average, we can aggregate features via more sophisticated mechanisms. For example, based on the cross-stitch network [81], [22] proposes a gated network where shared and task-specific features are combined by a gating mechanism. A similar gating mechanism is used in [56] to combine features from the primary and the auxiliary task. The SLUICE network [100] learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in [135] to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph . The update gate z determines how much information should be maintained from task and then it emits final outputh for task . Mathematically, the two gates can be formulated as

(p1.7) where (·) denotes the sigmoid function and tanh(·) denotes the hyperbolic tangent function. When considering all pairwise directions, the output for each task is given by the sum of each row in

(p1.8) Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task. Given tasks, the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .",How do parallel architectures in multi-task learning models manage and integrate shared and task-specific features?,1. How do parallel architectures in multi-task learning models manage and integrate shared and task-specific features?,"Questions:

1. How do parallel architectures in multi-task learning models manage and integrate shared and task-specific features?

Answer:

(p1.0) As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers. In this case, there is no dependency other than layer sharing among tasks. Therefore, there is no constraint on the order of training samples from each task. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks. Fig. 1 illustrates different forms of parallel architectures.

(p1.1) 2.1.1 Vanilla Tree-like Architectures. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches). A shallow trunk can be simply the word representation layer [108] while a deep trunk can be the entire model except output layers. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153]. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.

(p1.2) The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task. A solution is to equip the shared trunk with task-specific encoders [47,58,137]. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages. Another way is to make different groups of tasks share different parts of the trunk [37,79,89]. Moreover, this idea can be applied to the decoder. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.

(p1.3) 2.1.2 Parallel Feature Fusion. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task. As shown in Fig. 1b, such models can use a globally shared encoder represents the corresponding label (ℎ are shared latent representations). The green blocks represent shared parameters and the orange blocks are task-specific parameters. Red circles represent feature fusion mechanism .

(p1.4) to produce shared representations that can be used as additional features for each task-specific model [69]. The shared representations can also be used indirectly as the key for attention layers in each task-specific model [118].

(p1.5) However, different parts of the shared features are not equally important to each task. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | · | compute the absolute value in an element-wise manner. After that, S is concatenated with task-specific representations to form the input to the output layer.

(p1.6) Some models directly integrate features from different tasks. A straightforward solution is to compute a weighted average of feature representations. Here the weights can be computed separately according to the input [61] or by an attention mechanism as in [153] that learns task representations as query keys in attention modules. In addition to weighted average, we can aggregate features via more sophisticated mechanisms. For example, based on the cross-stitch network [81], [22] proposes a gated network where shared and task-specific features are combined by a gating mechanism. A similar gating mechanism is used in [56] to combine features from the primary and the auxiliary task. The SLUICE network [100] learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in [135] to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph . The update gate z determines how much information should be maintained from task and then it emits final outputh for task . Mathematically, the two gates can be formulated as

(p1.7) where (·) denotes the sigmoid function and tanh(·) denotes the hyperbolic tangent function. When considering all pairwise directions, the output for each task is given by the sum of each row in

(p1.8) Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task. Given tasks, the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .",How do parallel architectures in multi-task learning facilitate knowledge sharing among tasks?,1. How do parallel architectures in multi-task learning facilitate knowledge sharing among tasks?,"Questions:

1. How do parallel architectures in multi-task learning facilitate knowledge sharing among tasks?

Answer:

(p1.0) As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers. In this case, there is no dependency other than layer sharing among tasks. Therefore, there is no constraint on the order of training samples from each task. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks. Fig. 1 illustrates different forms of parallel architectures.

(p1.1) 2.1.1 Vanilla Tree-like Architectures. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches). A shallow trunk can be simply the word representation layer [108] while a deep trunk can be the entire model except output layers. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153]. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.

(p1.2) The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task. A solution is to equip the shared trunk with task-specific encoders [47,58,137]. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages. Another way is to make different groups of tasks share different parts of the trunk [37,79,89]. Moreover, this idea can be applied to the decoder. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.

(p1.3) 2.1.2 Parallel Feature Fusion. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task. As shown in Fig. 1b, such models can use a globally shared encoder represents the corresponding label (ℎ are shared latent representations). The green blocks represent shared parameters and the orange blocks are task-specific parameters. Red circles represent feature fusion mechanism .

(p1.4) to produce shared representations that can be used as additional features for each task-specific model [69]. The shared representations can also be used indirectly as the key for attention layers in each task-specific model [118].

(p1.5) However, different parts of the shared features are not equally important to each task. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | · | compute the absolute value in an element-wise manner. After that, S is concatenated with task-specific representations to form the input to the output layer.

(p1.6) Some models directly integrate features from different tasks. A straightforward solution is to compute a weighted average of feature representations. Here the weights can be computed separately according to the input [61] or by an attention mechanism as in [153] that learns task representations as query keys in attention modules. In addition to weighted average, we can aggregate features via more sophisticated mechanisms. For example, based on the cross-stitch network [81], [22] proposes a gated network where shared and task-specific features are combined by a gating mechanism. A similar gating mechanism is used in [56] to combine features from the primary and the auxiliary task. The SLUICE network [100] learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in [135] to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph . The update gate z determines how much information should be maintained from task and then it emits final outputh for task . Mathematically, the two gates can be formulated as

(p1.7) where (·) denotes the sigmoid function and tanh(·) denotes the hyperbolic tangent function. When considering all pairwise directions, the output for each task is given by the sum of each row in

(p1.8) Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task. Given tasks, the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .",What are the key characteristics and benefits of parallel architectures in multi-task learning?,1. What are the key characteristics and benefits of parallel architectures in multi-task learning?,"Questions:

1. What are the key characteristics and benefits of parallel architectures in multi-task learning?

Answer:

(p1.0) As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers. In this case, there is no dependency other than layer sharing among tasks. Therefore, there is no constraint on the order of training samples from each task. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks. Fig. 1 illustrates different forms of parallel architectures.

(p1.1) 2.1.1 Vanilla Tree-like Architectures. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches). A shallow trunk can be simply the word representation layer [108] while a deep trunk can be the entire model except output layers. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153]. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.

(p1.2) The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task. A solution is to equip the shared trunk with task-specific encoders [47,58,137]. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages. Another way is to make different groups of tasks share different parts of the trunk [37,79,89]. Moreover, this idea can be applied to the decoder. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.

(p1.3) 2.1.2 Parallel Feature Fusion. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task. As shown in Fig. 1b, such models can use a globally shared encoder represents the corresponding label (ℎ are shared latent representations). The green blocks represent shared parameters and the orange blocks are task-specific parameters. Red circles represent feature fusion mechanism .

(p1.4) to produce shared representations that can be used as additional features for each task-specific model [69]. The shared representations can also be used indirectly as the key for attention layers in each task-specific model [118].

(p1.5) However, different parts of the shared features are not equally important to each task. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | · | compute the absolute value in an element-wise manner. After that, S is concatenated with task-specific representations to form the input to the output layer.

(p1.6) Some models directly integrate features from different tasks. A straightforward solution is to compute a weighted average of feature representations. Here the weights can be computed separately according to the input [61] or by an attention mechanism as in [153] that learns task representations as query keys in attention modules. In addition to weighted average, we can aggregate features via more sophisticated mechanisms. For example, based on the cross-stitch network [81], [22] proposes a gated network where shared and task-specific features are combined by a gating mechanism. A similar gating mechanism is used in [56] to combine features from the primary and the auxiliary task. The SLUICE network [100] learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in [135] to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph . The update gate z determines how much information should be maintained from task and then it emits final outputh for task . Mathematically, the two gates can be formulated as

(p1.7) where (·) denotes the sigmoid function and tanh(·) denotes the hyperbolic tangent function. When considering all pairwise directions, the output for each task is given by the sum of each row in

(p1.8) Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task. Given tasks, the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task ."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s2,Supervision at Different,"['p2.0', 'p2.1']","['Feature Levels. Models using the parallel architecture handle multiple tasks in parallel. These tasks may concern features at different abstraction levels. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.', 'In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.']","Feature Levels. Models using the parallel architecture handle multiple tasks in parallel. These tasks may concern features at different abstraction levels. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.

In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.","(p2.0) Feature Levels. Models using the parallel architecture handle multiple tasks in parallel. These tasks may concern features at different abstraction levels. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.

(p2.1) In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.","[['b101', 'b95', 'b108', 'b10', 'b79', 'b32', 'b27', 'b92', 'b19', 'b56'], ['b61', 'b84', 'b96', 'b15', 'b153', 'b67']]","[['b101', 'b95', 'b108', 'b10', 'b79', 'b32', 'b27', 'b92', 'b19', 'b56'], ['b61', 'b84', 'b96', 'b15', 'b153', 'b67']]",16,"1. Feature Levels. Models using the parallel architecture handle multiple tasks in parallel.
2. These tasks may concern features at different abstraction levels.
3. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level.
4. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c.
5. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers.
6. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features.
7. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified.
8. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.
9. In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful.
10. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154].
11. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task.
12. [28,57]0 adds attention-level supervision to improve consistency of the two primary language generation tasks.
13. [28,57]1 minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.",How does multi-task learning in NLP utilize supervision at different feature levels for various tasks?,1. How does multi-task learning in NLP utilize supervision at different feature levels for various tasks?,"Questions:

1. How does multi-task learning in NLP utilize supervision at different feature levels for various tasks?

Answer:

(p2.0) Feature Levels. Models using the parallel architecture handle multiple tasks in parallel. These tasks may concern features at different abstraction levels. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.

(p2.1) In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.",How do multi-task learning models in NLP apply supervision at different feature levels for various tasks?,1. How do multi-task learning models in NLP apply supervision at different feature levels for various tasks?,"Questions:

1. How do multi-task learning models in NLP apply supervision at different feature levels for various tasks?

Answer:

(p2.0) Feature Levels. Models using the parallel architecture handle multiple tasks in parallel. These tasks may concern features at different abstraction levels. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.

(p2.1) In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.",How does multi-task learning improve primary task performance in NLP through the introduction of auxiliary tasks?,1. How does multi-task learning improve primary task performance in NLP through the introduction of auxiliary tasks?,"Questions:

1. How does multi-task learning improve primary task performance in NLP through the introduction of auxiliary tasks?

Answer:

(p2.0) Feature Levels. Models using the parallel architecture handle multiple tasks in parallel. These tasks may concern features at different abstraction levels. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.

(p2.1) In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.",How do multi-task learning models in NLP manage supervision for tasks at different feature levels?,1. How do multi-task learning models in NLP manage supervision for tasks at different feature levels?,"Questions:

1. How do multi-task learning models in NLP manage supervision for tasks at different feature levels?

Answer:

(p2.0) Feature Levels. Models using the parallel architecture handle multiple tasks in parallel. These tasks may concern features at different abstraction levels. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.

(p2.1) In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.",How does multi-task learning (MTL) enhance primary task performance in NLP?,1. How does multi-task learning (MTL) enhance primary task performance in NLP?,"Questions:

1. How does multi-task learning (MTL) enhance primary task performance in NLP?

Answer:

(p2.0) Feature Levels. Models using the parallel architecture handle multiple tasks in parallel. These tasks may concern features at different abstraction levels. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.

(p2.1) In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s3,Hierarchical Architectures,"['p3.0', 'p3.1']","['The hierarchical architecture considers hierarchical relationships among multiple tasks. The features and output of one task can be used by another task as an extra input or additional control signals. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures. Fig. 2 illustrates different hierarchical architectures.', '2.2.1 Hierarchical Feature Fusion. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum. [124] fuses topic features of different roles into the main model via a gating mechanism. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection.']","The hierarchical architecture considers hierarchical relationships among multiple tasks. The features and output of one task can be used by another task as an extra input or additional control signals. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures. Fig. 2 illustrates different hierarchical architectures.

2.2.1 Hierarchical Feature Fusion. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum. [124] fuses topic features of different roles into the main model via a gating mechanism. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection.","(p3.0) The hierarchical architecture considers hierarchical relationships among multiple tasks. The features and output of one task can be used by another task as an extra input or additional control signals. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures. Fig. 2 illustrates different hierarchical architectures.

(p3.1) 2.2.1 Hierarchical Feature Fusion. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum. [124] fuses topic features of different roles into the main model via a gating mechanism. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection.","[[], ['b67', 'b120', 'b23', 'b123']]","[[], ['b67', 'b120', 'b23', 'b123']]",4,"1. The hierarchical architecture considers hierarchical relationships among multiple tasks.
2. The features and output of one task can be used by another task as an extra input or additional control signals.
3. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures.
4. Fig. 2 illustrates different hierarchical architectures.
5. 2.2.1 Hierarchical Feature Fusion.
6. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features.
7. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism.
8. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task.
9. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum.
10. [124] fuses topic features of different roles into the main model via a gating mechanism.
11. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection.",How do hierarchical architectures in multi-task learning enhance task performance through feature fusion?,1. How do hierarchical architectures in multi-task learning enhance task performance through feature fusion?,"Questions:

1. How do hierarchical architectures in multi-task learning enhance task performance through feature fusion?

Answer:

(p3.0) The hierarchical architecture considers hierarchical relationships among multiple tasks. The features and output of one task can be used by another task as an extra input or additional control signals. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures. Fig. 2 illustrates different hierarchical architectures.

(p3.1) 2.2.1 Hierarchical Feature Fusion. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum. [124] fuses topic features of different roles into the main model via a gating mechanism. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection.",How do hierarchical architectures in multi-task learning utilize relationships among tasks?,1. How do hierarchical architectures in multi-task learning utilize relationships among tasks?,"Questions:

1. How do hierarchical architectures in multi-task learning utilize relationships among tasks?

Answer:

(p3.0) The hierarchical architecture considers hierarchical relationships among multiple tasks. The features and output of one task can be used by another task as an extra input or additional control signals. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures. Fig. 2 illustrates different hierarchical architectures.

(p3.1) 2.2.1 Hierarchical Feature Fusion. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum. [124] fuses topic features of different roles into the main model via a gating mechanism. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection.",How do hierarchical architectures in multi-task learning NLP models manage relationships among multiple tasks?,1. How do hierarchical architectures in multi-task learning NLP models manage relationships among multiple tasks?,"Questions:

1. How do hierarchical architectures in multi-task learning NLP models manage relationships among multiple tasks?

Answer:

(p3.0) The hierarchical architecture considers hierarchical relationships among multiple tasks. The features and output of one task can be used by another task as an extra input or additional control signals. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures. Fig. 2 illustrates different hierarchical architectures.

(p3.1) 2.2.1 Hierarchical Feature Fusion. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum. [124] fuses topic features of different roles into the main model via a gating mechanism. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection.",How do hierarchical architectures in multi-task learning benefit from task relationships?,1. How do hierarchical architectures in multi-task learning benefit from task relationships?,"Questions:

1. How do hierarchical architectures in multi-task learning benefit from task relationships?

Answer:

(p3.0) The hierarchical architecture considers hierarchical relationships among multiple tasks. The features and output of one task can be used by another task as an extra input or additional control signals. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures. Fig. 2 illustrates different hierarchical architectures.

(p3.1) 2.2.1 Hierarchical Feature Fusion. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum. [124] fuses topic features of different roles into the main model via a gating mechanism. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection.",How do hierarchical architectures in multi-task learning enhance NLP models?,1. How do hierarchical architectures in multi-task learning enhance NLP models?,"Questions:

1. How do hierarchical architectures in multi-task learning enhance NLP models?

Answer:

(p3.0) The hierarchical architecture considers hierarchical relationships among multiple tasks. The features and output of one task can be used by another task as an extra input or additional control signals. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures. Fig. 2 illustrates different hierarchical architectures.

(p3.1) 2.2.1 Hierarchical Feature Fusion. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum. [124] fuses topic features of different roles into the main model via a gating mechanism. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s4,Hierarchical,"['p4.0', 'p4.1', 'p4.2', 'p4.3', 'p4.4']","['Pipeline. Instead of aggregating features from different tasks as in feature fusion architectures, pipeline architectures treat the output of a task as an extra input of another task and form a hierarchical pipeline between tasks. The extra input can be used directly as input features or used indirectly as control signals to enhance the performance of other tasks. In this section, we refer to output as the final result from a task, including the final output distribution and hidden states before the last output layer. We further divide hierarchical pipeline architectures into hierarchical feature pipeline and hierarchical signal pipeline.', 'In hierarchical feature pipeline, the output of one task is used as extra features for another task. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks. For example, [14] feeds the output of a question-review pair recognition model to the question answering model. [44] feeds the output of aspect term extraction to aspect-term sentiment classification. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.', 'Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.', 'The direction of hierarchical pipelines is not necessarily always from low-level tasks to high-level tasks. For example, in [1], the outputs of word-level tasks are fed to the char-level primary task. [99] feeds the output of more general classification models to more specific classification models during training, and the more general classification results are used to optimize beam search of more specific models at test time.', 'In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50]. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction. The outputs of a task can also be used for post-processing. For instance, [145] uses the output of NER to help extract multi-token entities.']","Pipeline. Instead of aggregating features from different tasks as in feature fusion architectures, pipeline architectures treat the output of a task as an extra input of another task and form a hierarchical pipeline between tasks. The extra input can be used directly as input features or used indirectly as control signals to enhance the performance of other tasks. In this section, we refer to output as the final result from a task, including the final output distribution and hidden states before the last output layer. We further divide hierarchical pipeline architectures into hierarchical feature pipeline and hierarchical signal pipeline.

In hierarchical feature pipeline, the output of one task is used as extra features for another task. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks. For example, [14] feeds the output of a question-review pair recognition model to the question answering model. [44] feeds the output of aspect term extraction to aspect-term sentiment classification. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.

Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.

The direction of hierarchical pipelines is not necessarily always from low-level tasks to high-level tasks. For example, in [1], the outputs of word-level tasks are fed to the char-level primary task. [99] feeds the output of more general classification models to more specific classification models during training, and the more general classification results are used to optimize beam search of more specific models at test time.

In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50]. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction. The outputs of a task can also be used for post-processing. For instance, [145] uses the output of NER to help extract multi-token entities.","(p4.0) Pipeline. Instead of aggregating features from different tasks as in feature fusion architectures, pipeline architectures treat the output of a task as an extra input of another task and form a hierarchical pipeline between tasks. The extra input can be used directly as input features or used indirectly as control signals to enhance the performance of other tasks. In this section, we refer to output as the final result from a task, including the final output distribution and hidden states before the last output layer. We further divide hierarchical pipeline architectures into hierarchical feature pipeline and hierarchical signal pipeline.

(p4.1) In hierarchical feature pipeline, the output of one task is used as extra features for another task. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks. For example, [14] feeds the output of a question-review pair recognition model to the question answering model. [44] feeds the output of aspect term extraction to aspect-term sentiment classification. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.

(p4.2) Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.

(p4.3) The direction of hierarchical pipelines is not necessarily always from low-level tasks to high-level tasks. For example, in [1], the outputs of word-level tasks are fed to the char-level primary task. [99] feeds the output of more general classification models to more specific classification models during training, and the more general classification results are used to optimize beam search of more specific models at test time.

(p4.4) In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50]. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction. The outputs of a task can also be used for post-processing. For instance, [145] uses the output of NER to help extract multi-token entities.","[[], ['b138', 'b109', 'b13', 'b151', 'b43'], ['b111', 'b32', 'b29', 'b42', 'b106'], ['b0', 'b98'], ['b73', 'b105', 'b49', 'b144']]","[[], ['b138', 'b109', 'b13', 'b151', 'b43'], ['b111', 'b32', 'b29', 'b42', 'b106'], ['b0', 'b98'], ['b73', 'b105', 'b49', 'b144']]",16,"1. Pipeline. Instead of aggregating features from different tasks as in feature fusion architectures, pipeline architectures treat the output of a task as an extra input of another task and form a hierarchical pipeline between tasks.
2. The extra input can be used directly as input features or used indirectly as control signals to enhance the performance of other tasks.
3. In this section, we refer to output as the final result from a task, including the final output distribution and hidden states before the last output layer.
4. We further divide hierarchical pipeline architectures into hierarchical feature pipeline and hierarchical signal pipeline.
5. In hierarchical feature pipeline, the output of one task is used as extra features for another task.
6. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks.
7. For example, [14] feeds the output of a question-review pair recognition model to the question answering model.
8. [44] feeds the output of aspect term extraction to aspect-term sentiment classification.
9. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations.
10. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections.
11. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.
12. Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships.
13. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction.
14. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks.
15. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task.
16. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task.
17. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.
18. The direction of hierarchical pipelines is not necessarily always from low-level tasks to high-level tasks.
19. For example, in [44]0, the outputs of word-level tasks are fed to the char-level primary task.
20. [44]1 feeds the output of more general classification models to more specific classification models during training, and the more general classification results are used to optimize beam search of more specific models at test time.
21. In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks.
22. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [44]2.
23. For the hashtag segmentation task, [44]3 first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features.
24. In [44]4, the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction.
25. The outputs of a task can also be used for post-processing.
26. For instance, [44]5 uses the output of NER to help extract multi-token entities.",How do hierarchical pipeline architectures enhance multi-task learning in NLP?,1. How do hierarchical pipeline architectures enhance multi-task learning in NLP?,"Questions:

1. How do hierarchical pipeline architectures enhance multi-task learning in NLP?

Answer:

(p4.0) Pipeline. Instead of aggregating features from different tasks as in feature fusion architectures, pipeline architectures treat the output of a task as an extra input of another task and form a hierarchical pipeline between tasks. The extra input can be used directly as input features or used indirectly as control signals to enhance the performance of other tasks. In this section, we refer to output as the final result from a task, including the final output distribution and hidden states before the last output layer. We further divide hierarchical pipeline architectures into hierarchical feature pipeline and hierarchical signal pipeline.

(p4.1) In hierarchical feature pipeline, the output of one task is used as extra features for another task. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks. For example, [14] feeds the output of a question-review pair recognition model to the question answering model. [44] feeds the output of aspect term extraction to aspect-term sentiment classification. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.

(p4.2) Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.

(p4.3) The direction of hierarchical pipelines is not necessarily always from low-level tasks to high-level tasks. For example, in [1], the outputs of word-level tasks are fed to the char-level primary task. [99] feeds the output of more general classification models to more specific classification models during training, and the more general classification results are used to optimize beam search of more specific models at test time.

(p4.4) In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50]. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction. The outputs of a task can also be used for post-processing. For instance, [145] uses the output of NER to help extract multi-token entities.",How do hierarchical pipeline architectures in multi-task NLP enhance task performance?,1. How do hierarchical pipeline architectures in multi-task NLP enhance task performance?,"Questions:

1. How do hierarchical pipeline architectures in multi-task NLP enhance task performance?

Answer:

(p4.0) Pipeline. Instead of aggregating features from different tasks as in feature fusion architectures, pipeline architectures treat the output of a task as an extra input of another task and form a hierarchical pipeline between tasks. The extra input can be used directly as input features or used indirectly as control signals to enhance the performance of other tasks. In this section, we refer to output as the final result from a task, including the final output distribution and hidden states before the last output layer. We further divide hierarchical pipeline architectures into hierarchical feature pipeline and hierarchical signal pipeline.

(p4.1) In hierarchical feature pipeline, the output of one task is used as extra features for another task. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks. For example, [14] feeds the output of a question-review pair recognition model to the question answering model. [44] feeds the output of aspect term extraction to aspect-term sentiment classification. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.

(p4.2) Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.

(p4.3) The direction of hierarchical pipelines is not necessarily always from low-level tasks to high-level tasks. For example, in [1], the outputs of word-level tasks are fed to the char-level primary task. [99] feeds the output of more general classification models to more specific classification models during training, and the more general classification results are used to optimize beam search of more specific models at test time.

(p4.4) In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50]. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction. The outputs of a task can also be used for post-processing. For instance, [145] uses the output of NER to help extract multi-token entities.",How do hierarchical pipelines in multi-task learning enhance task performance in NLP?,1. How do hierarchical pipelines in multi-task learning enhance task performance in NLP?,"Questions:

1. How do hierarchical pipelines in multi-task learning enhance task performance in NLP?

Answer:

(p4.0) Pipeline. Instead of aggregating features from different tasks as in feature fusion architectures, pipeline architectures treat the output of a task as an extra input of another task and form a hierarchical pipeline between tasks. The extra input can be used directly as input features or used indirectly as control signals to enhance the performance of other tasks. In this section, we refer to output as the final result from a task, including the final output distribution and hidden states before the last output layer. We further divide hierarchical pipeline architectures into hierarchical feature pipeline and hierarchical signal pipeline.

(p4.1) In hierarchical feature pipeline, the output of one task is used as extra features for another task. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks. For example, [14] feeds the output of a question-review pair recognition model to the question answering model. [44] feeds the output of aspect term extraction to aspect-term sentiment classification. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.

(p4.2) Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.

(p4.3) The direction of hierarchical pipelines is not necessarily always from low-level tasks to high-level tasks. For example, in [1], the outputs of word-level tasks are fed to the char-level primary task. [99] feeds the output of more general classification models to more specific classification models during training, and the more general classification results are used to optimize beam search of more specific models at test time.

(p4.4) In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50]. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction. The outputs of a task can also be used for post-processing. For instance, [145] uses the output of NER to help extract multi-token entities.",How do hierarchical pipeline architectures enhance multi-task learning in NLP?,1. How do hierarchical pipeline architectures enhance multi-task learning in NLP?,"Questions:

1. How do hierarchical pipeline architectures enhance multi-task learning in NLP?

Answer:

(p4.0) Pipeline. Instead of aggregating features from different tasks as in feature fusion architectures, pipeline architectures treat the output of a task as an extra input of another task and form a hierarchical pipeline between tasks. The extra input can be used directly as input features or used indirectly as control signals to enhance the performance of other tasks. In this section, we refer to output as the final result from a task, including the final output distribution and hidden states before the last output layer. We further divide hierarchical pipeline architectures into hierarchical feature pipeline and hierarchical signal pipeline.

(p4.1) In hierarchical feature pipeline, the output of one task is used as extra features for another task. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks. For example, [14] feeds the output of a question-review pair recognition model to the question answering model. [44] feeds the output of aspect term extraction to aspect-term sentiment classification. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.

(p4.2) Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.

(p4.3) The direction of hierarchical pipelines is not necessarily always from low-level tasks to high-level tasks. For example, in [1], the outputs of word-level tasks are fed to the char-level primary task. [99] feeds the output of more general classification models to more specific classification models during training, and the more general classification results are used to optimize beam search of more specific models at test time.

(p4.4) In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50]. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction. The outputs of a task can also be used for post-processing. For instance, [145] uses the output of NER to help extract multi-token entities.",How do hierarchical pipelines enhance multi-task learning in NLP?,1. How do hierarchical pipelines enhance multi-task learning in NLP?,"Questions:

1. How do hierarchical pipelines enhance multi-task learning in NLP?

Answer:

(p4.0) Pipeline. Instead of aggregating features from different tasks as in feature fusion architectures, pipeline architectures treat the output of a task as an extra input of another task and form a hierarchical pipeline between tasks. The extra input can be used directly as input features or used indirectly as control signals to enhance the performance of other tasks. In this section, we refer to output as the final result from a task, including the final output distribution and hidden states before the last output layer. We further divide hierarchical pipeline architectures into hierarchical feature pipeline and hierarchical signal pipeline.

(p4.1) In hierarchical feature pipeline, the output of one task is used as extra features for another task. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks. For example, [14] feeds the output of a question-review pair recognition model to the question answering model. [44] feeds the output of aspect term extraction to aspect-term sentiment classification. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.

(p4.2) Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.

(p4.3) The direction of hierarchical pipelines is not necessarily always from low-level tasks to high-level tasks. For example, in [1], the outputs of word-level tasks are fed to the char-level primary task. [99] feeds the output of more general classification models to more specific classification models during training, and the more general classification results are used to optimize beam search of more specific models at test time.

(p4.4) In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50]. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction. The outputs of a task can also be used for post-processing. For instance, [145] uses the output of NER to help extract multi-token entities."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s6,Modular Architectures,"['p6.0', 'p6.1', 'p6.2', 'p6.3', 'p6.4']","['The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules. The shared modules learn shared features from multiple tasks. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios. On the other hand, task-specific modules learn features that are specific to a certain task. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.', 'The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did. [1] shares word and character embedding matrices and combines them differently for different tasks. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT). Modular designs are also widely used in multi-lingual tasks. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks. Shared embeddings can be used alongside task-specific embeddings [59,138] as well. In addition to word embeddings, [147] shares label embeddings between tasks. Researchers have also developed modular architectures at a finer granularity. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer. [92] creates encoder modules on different levels, including task level, task group level, and universal level.', 'When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task. While this approach usually attains good performance, it poses heavy computational and storage costs. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters (adapters). [113] adds task-specific Projected Attention Layers (PALs) in parallel with self-attention operatioins in a pre-trained BERT model. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed. In Multiple ADapters for Cross-lingual transfer (MAD-X) [94], the model is decomposed into PALs (a) Bert and PALs [113] Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch. Further, task adaptation modules can also be dynamically generated by a meta-network. Hypergrid transformer [117] scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors as', 'where L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix. Differently, Conditionally Adaptive MTL (CA-MTL) [95] implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as', ') is a diagonal block matrix consisting of learnable linear transformations over z . Therefore, M(z ) injects task-specific bias into the attention map in the self-attention mechanism. Similar adaptation operations are used in the input alignment and layer normalization as well. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters.']","The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules. The shared modules learn shared features from multiple tasks. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios. On the other hand, task-specific modules learn features that are specific to a certain task. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.

The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did. [1] shares word and character embedding matrices and combines them differently for different tasks. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT). Modular designs are also widely used in multi-lingual tasks. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks. Shared embeddings can be used alongside task-specific embeddings [59,138] as well. In addition to word embeddings, [147] shares label embeddings between tasks. Researchers have also developed modular architectures at a finer granularity. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer. [92] creates encoder modules on different levels, including task level, task group level, and universal level.

When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task. While this approach usually attains good performance, it poses heavy computational and storage costs. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters (adapters). [113] adds task-specific Projected Attention Layers (PALs) in parallel with self-attention operatioins in a pre-trained BERT model. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed. In Multiple ADapters for Cross-lingual transfer (MAD-X) [94], the model is decomposed into PALs (a) Bert and PALs [113] Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch. Further, task adaptation modules can also be dynamically generated by a meta-network. Hypergrid transformer [117] scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors as

where L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix. Differently, Conditionally Adaptive MTL (CA-MTL) [95] implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as

) is a diagonal block matrix consisting of learnable linear transformations over z . Therefore, M(z ) injects task-specific bias into the attention map in the self-attention mechanism. Similar adaptation operations are used in the input alignment and layer normalization as well. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters.","(p6.0) The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules. The shared modules learn shared features from multiple tasks. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios. On the other hand, task-specific modules learn features that are specific to a certain task. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.

(p6.1) The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did. [1] shares word and character embedding matrices and combines them differently for different tasks. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT). Modular designs are also widely used in multi-lingual tasks. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks. Shared embeddings can be used alongside task-specific embeddings [59,138] as well. In addition to word embeddings, [147] shares label embeddings between tasks. Researchers have also developed modular architectures at a finer granularity. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer. [92] creates encoder modules on different levels, including task level, task group level, and universal level.

(p6.2) When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task. While this approach usually attains good performance, it poses heavy computational and storage costs. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters (adapters). [113] adds task-specific Projected Attention Layers (PALs) in parallel with self-attention operatioins in a pre-trained BERT model. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed. In Multiple ADapters for Cross-lingual transfer (MAD-X) [94], the model is decomposed into PALs (a) Bert and PALs [113] Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch. Further, task adaptation modules can also be dynamically generated by a meta-network. Hypergrid transformer [117] scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors as

(p6.3) where L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix. Differently, Conditionally Adaptive MTL (CA-MTL) [95] implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as

(p6.4) ) is a diagonal block matrix consisting of learnable linear transformations over z . Therefore, M(z ) injects task-specific bias into the attention map in the self-attention mechanism. Similar adaptation operations are used in the input alignment and layer normalization as well. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters.","[[], ['b58', 'b146', 'b102', 'b91', 'b57', 'b0', 'b48', 'b23', 'b118', 'b155', 'b137'], ['b93', 'b116', 'b112'], ['b94'], []]","[[], ['b58', 'b146', 'b102', 'b91', 'b57', 'b0', 'b48', 'b23', 'b118', 'b155', 'b137'], ['b93', 'b116', 'b112'], ['b94'], []]",15,"1. The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules.
2. The shared modules learn shared features from multiple tasks.
3. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios.
4. On the other hand, task-specific modules learn features that are specific to a certain task.
5. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.
6. The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1.
7. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did.
8. [1] shares word and character embedding matrices and combines them differently for different tasks.
9. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT).
10. Modular designs are also widely used in multi-lingual tasks.
11. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks.
12. Shared embeddings can be used alongside task-specific embeddings [59,138] as well.
13. In addition to word embeddings, [147] shares label embeddings between tasks.
14. Researchers have also developed modular architectures at a finer granularity.
15. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation.
16. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer.
17. [92] creates encoder modules on different levels, including task level, task group level, and universal level.
18. When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task.
19. While this approach usually attains good performance, it poses heavy computational and storage costs.
20. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model.
21. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters [1]0.
22. [1]6 adds task-specific Projected Attention Layers [1]2 in parallel with self-attention operatioins in a pre-trained BERT model.
23. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed.
24. In Multiple ADapters for Cross-lingual transfer [1]3 [1]4, the model is decomposed into PALs [1]5 Bert and PALs [1]6 Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings.
25. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch.
26. Further, task adaptation modules can also be dynamically generated by a meta-network.
27. Hypergrid transformer [1]7 scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors aswhere L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix.
28. Differently, Conditionally Adaptive MTL [1]8 [1]9 implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as) is a diagonal block matrix consisting of learnable linear transformations over z .
29. Therefore, M[103]0 injects task-specific bias into the attention map in the self-attention mechanism.
30. Similar adaptation operations are used in the input alignment and layer normalization as well.
31. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters.",How do modular architectures in multi-task learning enhance model performance and efficiency?,1. How do modular architectures in multi-task learning enhance model performance and efficiency?,"Questions:

1. How do modular architectures in multi-task learning enhance model performance and efficiency?

Answer:

(p6.0) The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules. The shared modules learn shared features from multiple tasks. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios. On the other hand, task-specific modules learn features that are specific to a certain task. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.

(p6.1) The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did. [1] shares word and character embedding matrices and combines them differently for different tasks. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT). Modular designs are also widely used in multi-lingual tasks. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks. Shared embeddings can be used alongside task-specific embeddings [59,138] as well. In addition to word embeddings, [147] shares label embeddings between tasks. Researchers have also developed modular architectures at a finer granularity. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer. [92] creates encoder modules on different levels, including task level, task group level, and universal level.

(p6.2) When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task. While this approach usually attains good performance, it poses heavy computational and storage costs. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters (adapters). [113] adds task-specific Projected Attention Layers (PALs) in parallel with self-attention operatioins in a pre-trained BERT model. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed. In Multiple ADapters for Cross-lingual transfer (MAD-X) [94], the model is decomposed into PALs (a) Bert and PALs [113] Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch. Further, task adaptation modules can also be dynamically generated by a meta-network. Hypergrid transformer [117] scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors as

(p6.3) where L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix. Differently, Conditionally Adaptive MTL (CA-MTL) [95] implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as

(p6.4) ) is a diagonal block matrix consisting of learnable linear transformations over z . Therefore, M(z ) injects task-specific bias into the attention map in the self-attention mechanism. Similar adaptation operations are used in the input alignment and layer normalization as well. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters.",How do modular architectures in multi-task learning models enhance generalization and prevent overfitting?,1. How do modular architectures in multi-task learning models enhance generalization and prevent overfitting?,"Questions:

1. How do modular architectures in multi-task learning models enhance generalization and prevent overfitting?

Answer:

(p6.0) The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules. The shared modules learn shared features from multiple tasks. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios. On the other hand, task-specific modules learn features that are specific to a certain task. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.

(p6.1) The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did. [1] shares word and character embedding matrices and combines them differently for different tasks. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT). Modular designs are also widely used in multi-lingual tasks. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks. Shared embeddings can be used alongside task-specific embeddings [59,138] as well. In addition to word embeddings, [147] shares label embeddings between tasks. Researchers have also developed modular architectures at a finer granularity. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer. [92] creates encoder modules on different levels, including task level, task group level, and universal level.

(p6.2) When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task. While this approach usually attains good performance, it poses heavy computational and storage costs. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters (adapters). [113] adds task-specific Projected Attention Layers (PALs) in parallel with self-attention operatioins in a pre-trained BERT model. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed. In Multiple ADapters for Cross-lingual transfer (MAD-X) [94], the model is decomposed into PALs (a) Bert and PALs [113] Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch. Further, task adaptation modules can also be dynamically generated by a meta-network. Hypergrid transformer [117] scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors as

(p6.3) where L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix. Differently, Conditionally Adaptive MTL (CA-MTL) [95] implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as

(p6.4) ) is a diagonal block matrix consisting of learnable linear transformations over z . Therefore, M(z ) injects task-specific bias into the attention map in the self-attention mechanism. Similar adaptation operations are used in the input alignment and layer normalization as well. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters.",How do modular architectures in multi-task learning models function and what are their key components?,1. How do modular architectures in multi-task learning models function and what are their key components?,"Questions:

1. How do modular architectures in multi-task learning models function and what are their key components?

Answer:

(p6.0) The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules. The shared modules learn shared features from multiple tasks. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios. On the other hand, task-specific modules learn features that are specific to a certain task. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.

(p6.1) The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did. [1] shares word and character embedding matrices and combines them differently for different tasks. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT). Modular designs are also widely used in multi-lingual tasks. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks. Shared embeddings can be used alongside task-specific embeddings [59,138] as well. In addition to word embeddings, [147] shares label embeddings between tasks. Researchers have also developed modular architectures at a finer granularity. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer. [92] creates encoder modules on different levels, including task level, task group level, and universal level.

(p6.2) When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task. While this approach usually attains good performance, it poses heavy computational and storage costs. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters (adapters). [113] adds task-specific Projected Attention Layers (PALs) in parallel with self-attention operatioins in a pre-trained BERT model. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed. In Multiple ADapters for Cross-lingual transfer (MAD-X) [94], the model is decomposed into PALs (a) Bert and PALs [113] Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch. Further, task adaptation modules can also be dynamically generated by a meta-network. Hypergrid transformer [117] scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors as

(p6.3) where L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix. Differently, Conditionally Adaptive MTL (CA-MTL) [95] implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as

(p6.4) ) is a diagonal block matrix consisting of learnable linear transformations over z . Therefore, M(z ) injects task-specific bias into the attention map in the self-attention mechanism. Similar adaptation operations are used in the input alignment and layer normalization as well. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters.",How do modular architectures in multi-task learning models enhance generalization and prevent overfitting?,1. How do modular architectures in multi-task learning models enhance generalization and prevent overfitting?,"Questions:

1. How do modular architectures in multi-task learning models enhance generalization and prevent overfitting?

Answer:

(p6.0) The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules. The shared modules learn shared features from multiple tasks. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios. On the other hand, task-specific modules learn features that are specific to a certain task. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.

(p6.1) The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did. [1] shares word and character embedding matrices and combines them differently for different tasks. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT). Modular designs are also widely used in multi-lingual tasks. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks. Shared embeddings can be used alongside task-specific embeddings [59,138] as well. In addition to word embeddings, [147] shares label embeddings between tasks. Researchers have also developed modular architectures at a finer granularity. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer. [92] creates encoder modules on different levels, including task level, task group level, and universal level.

(p6.2) When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task. While this approach usually attains good performance, it poses heavy computational and storage costs. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters (adapters). [113] adds task-specific Projected Attention Layers (PALs) in parallel with self-attention operatioins in a pre-trained BERT model. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed. In Multiple ADapters for Cross-lingual transfer (MAD-X) [94], the model is decomposed into PALs (a) Bert and PALs [113] Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch. Further, task adaptation modules can also be dynamically generated by a meta-network. Hypergrid transformer [117] scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors as

(p6.3) where L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix. Differently, Conditionally Adaptive MTL (CA-MTL) [95] implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as

(p6.4) ) is a diagonal block matrix consisting of learnable linear transformations over z . Therefore, M(z ) injects task-specific bias into the attention map in the self-attention mechanism. Similar adaptation operations are used in the input alignment and layer normalization as well. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters.",What are the key principles behind modular architectures in multi-task learning models?,1. What are the key principles behind modular architectures in multi-task learning models?,"Questions:

1. What are the key principles behind modular architectures in multi-task learning models?

Answer:

(p6.0) The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules. The shared modules learn shared features from multiple tasks. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios. On the other hand, task-specific modules learn features that are specific to a certain task. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.

(p6.1) The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did. [1] shares word and character embedding matrices and combines them differently for different tasks. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT). Modular designs are also widely used in multi-lingual tasks. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks. Shared embeddings can be used alongside task-specific embeddings [59,138] as well. In addition to word embeddings, [147] shares label embeddings between tasks. Researchers have also developed modular architectures at a finer granularity. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer. [92] creates encoder modules on different levels, including task level, task group level, and universal level.

(p6.2) When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task. While this approach usually attains good performance, it poses heavy computational and storage costs. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters (adapters). [113] adds task-specific Projected Attention Layers (PALs) in parallel with self-attention operatioins in a pre-trained BERT model. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed. In Multiple ADapters for Cross-lingual transfer (MAD-X) [94], the model is decomposed into PALs (a) Bert and PALs [113] Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch. Further, task adaptation modules can also be dynamically generated by a meta-network. Hypergrid transformer [117] scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors as

(p6.3) where L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix. Differently, Conditionally Adaptive MTL (CA-MTL) [95] implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as

(p6.4) ) is a diagonal block matrix consisting of learnable linear transformations over z . Therefore, M(z ) injects task-specific bias into the attention map in the self-attention mechanism. Similar adaptation operations are used in the input alignment and layer normalization as well. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s7,Generative Adversarial Architectures,"['p7.0', 'p7.1']","['Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.', 'An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.']","Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.

An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.","(p7.0) Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.

(p7.1) An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.","[['b77', 'b69', 'b118', 'b126', 'b137'], ['b123', 'b97']]","[['b77', 'b69', 'b118', 'b126', 'b137'], ['b123', 'b97']]",7,"1. Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision.
2. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones.
3. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images.
4. A similar idea can be used in MTL for NLP tasks.
5. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model.
6. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.
7. An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized.
8. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents.
9. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach.
10. First, a discriminator that rates the quality of candidate answers is trained on labeled samples.
11. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.",How do generative adversarial architectures enhance multi-task learning in NLP?,1. How do generative adversarial architectures enhance multi-task learning in NLP?,"Questions:

1. How do generative adversarial architectures enhance multi-task learning in NLP?

Answer:

(p7.0) Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.

(p7.1) An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.",How do Generative Adversarial Networks enhance multi-task learning in NLP tasks?,1. How do Generative Adversarial Networks enhance multi-task learning in NLP tasks?,"Questions:

1. How do Generative Adversarial Networks enhance multi-task learning in NLP tasks?

Answer:

(p7.0) Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.

(p7.1) An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.","How do generative adversarial networks benefit multi-task learning in NLP, especially regarding unlabeled data?","1. How do generative adversarial networks benefit multi-task learning in NLP, especially regarding unlabeled data?","Questions:

1. How do generative adversarial networks benefit multi-task learning in NLP, especially regarding unlabeled data?

Answer:

(p7.0) Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.

(p7.1) An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.",How do GANs enhance feature generalization in multi-task NLP models?,1. How do GANs enhance feature generalization in multi-task NLP models?,"Questions:

1. How do GANs enhance feature generalization in multi-task NLP models?

Answer:

(p7.0) Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.

(p7.1) An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.",What are the benefits of using generative adversarial networks in NLP tasks?,1. What are the benefits of using generative adversarial networks in NLP tasks?,"Questions:

1. What are the benefits of using generative adversarial networks in NLP tasks?

Answer:

(p7.0) Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.

(p7.1) An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s9,Loss Construction,"['p9.0', 'p9.1', 'p9.2', 'p9.3', 'p9.4', 'p9.5', 'p9.6', 'p9.7', 'p9.8', 'p9.9', 'p9.10']","['The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation. Different tasks may use different types of loss functions. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined as', 'where L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.', 'In this case, an important question is how to assign a proper weight to each task. The simplest way is to set them equally [91,126,156], i.e., = 1 . As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155]. For example, to prevent large datasets from dominating training, [93] sets the weights as', 'where |D | denotes the size of the training dataset for task . The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks as', 'where measures the variance of the training loss for task . In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label. To ensure that a student model could receive enough supervision during knowledge distillation, BAM! [19] combines the supervised loss L with the distillation loss L as', 'where increases linearly from 0 to 1 in the training process. In [112], three tasks are jointly optimized, including the primary essay organization evaluation (OE) task and the auxiliary sentence function identification (SFI) and paragraph function identification (PFI) tasks. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 (i.e., = = 1) and the weight of the OE task is set as', 'where is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller. [85] guides the model to focus on easy tasks by setting weights as', 'where denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models. In [62], the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input. [14] penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features. To learn domain-invariant features, [137] minimizes a distance function (·) between a pair of learned representations from different tasks. Candidates of (·) include the KL divergence, maximum mean discrepancy (MMD), and central moment discrepancy (CMD). Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex. To address this issue, Tchebycheff loss [75] optimizes an MTL model by an ∞ objective, which is formulated as', 'where L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 . The Tchebycheff loss can be combined with adversarial MTL as in [70].', 'Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance. As a remedy, PCGrad [142] directly projects conflicting gradients. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g as', 'Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g . Notice that PCGrad is a special case of GradVac when = 0. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.']","The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation. Different tasks may use different types of loss functions. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined as

where L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.

In this case, an important question is how to assign a proper weight to each task. The simplest way is to set them equally [91,126,156], i.e., = 1 . As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155]. For example, to prevent large datasets from dominating training, [93] sets the weights as

where |D | denotes the size of the training dataset for task . The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks as

where measures the variance of the training loss for task . In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label. To ensure that a student model could receive enough supervision during knowledge distillation, BAM! [19] combines the supervised loss L with the distillation loss L as

where increases linearly from 0 to 1 in the training process. In [112], three tasks are jointly optimized, including the primary essay organization evaluation (OE) task and the auxiliary sentence function identification (SFI) and paragraph function identification (PFI) tasks. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 (i.e., = = 1) and the weight of the OE task is set as

where is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller. [85] guides the model to focus on easy tasks by setting weights as

where denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models. In [62], the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input. [14] penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features. To learn domain-invariant features, [137] minimizes a distance function (·) between a pair of learned representations from different tasks. Candidates of (·) include the KL divergence, maximum mean discrepancy (MMD), and central moment discrepancy (CMD). Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex. To address this issue, Tchebycheff loss [75] optimizes an MTL model by an ∞ objective, which is formulated as

where L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 . The Tchebycheff loss can be combined with adversarial MTL as in [70].

Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance. As a remedy, PCGrad [142] directly projects conflicting gradients. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g as

Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g . Notice that PCGrad is a special case of GradVac when = 0. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.","(p9.0) The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation. Different tasks may use different types of loss functions. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined as

(p9.1) where L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.

(p9.2) In this case, an important question is how to assign a proper weight to each task. The simplest way is to set them equally [91,126,156], i.e., = 1 . As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155]. For example, to prevent large datasets from dominating training, [93] sets the weights as

(p9.3) where |D | denotes the size of the training dataset for task . The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks as

(p9.4) where measures the variance of the training loss for task . In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label. To ensure that a student model could receive enough supervision during knowledge distillation, BAM! [19] combines the supervised loss L with the distillation loss L as

(p9.5) where increases linearly from 0 to 1 in the training process. In [112], three tasks are jointly optimized, including the primary essay organization evaluation (OE) task and the auxiliary sentence function identification (SFI) and paragraph function identification (PFI) tasks. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 (i.e., = = 1) and the weight of the OE task is set as

(p9.6) where is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller. [85] guides the model to focus on easy tasks by setting weights as

(p9.7) where denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models. In [62], the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input. [14] penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features. To learn domain-invariant features, [137] minimizes a distance function (·) between a pair of learned representations from different tasks. Candidates of (·) include the KL divergence, maximum mean discrepancy (MMD), and central moment discrepancy (CMD). Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex. To address this issue, Tchebycheff loss [75] optimizes an MTL model by an ∞ objective, which is formulated as

(p9.8) where L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 . The Tchebycheff loss can be combined with adversarial MTL as in [70].

(p9.9) Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance. As a remedy, PCGrad [142] directly projects conflicting gradients. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g as

(p9.10) Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g . Notice that PCGrad is a special case of GradVac when = 0. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.","[['b140'], ['b113'], [None, 'b92', 'b155', 'b90', 'b125'], ['b56', 'b16'], ['b63', 'b18'], ['b111'], ['b84'], ['b74', 'b136', 'b61', 'b13'], ['b69'], ['b141'], ['b0', 'b128']]","[['b140'], ['b113'], [None, 'b92', 'b155', 'b90', 'b125'], ['b56', 'b16'], ['b63', 'b18'], ['b111'], ['b84'], ['b74', 'b136', 'b61', 'b13'], ['b69'], ['b141'], ['b0', 'b128']]",21,"1. The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function.
2. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation.
3. Different tasks may use different types of loss functions.
4. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning.
5. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined aswhere L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights.
6. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.
7. In this case, an important question is how to assign a proper weight to each task.
8. The simplest way is to set them equally [91,126,156], i.e., = 1 .
9. As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155].
10. For example, to prevent large datasets from dominating training, [93] sets the weights aswhere |D | denotes the size of the training dataset for task .
11. The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages.
12. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks aswhere measures the variance of the training loss for task .
13. In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label.
14. To ensure that a student model could receive enough supervision during knowledge distillation, BAM!
15. [19] combines the supervised loss L with the distillation loss L aswhere increases linearly from 0 to 1 in the training process.
16. In [112], three tasks are jointly optimized, including the primary essay organization evaluation [114]0 task and the auxiliary sentence function identification [114]1 and paragraph function identification [114]2 tasks.
17. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 [114]3 and the weight of the OE task is set aswhere is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller.
18. [114]4 guides the model to focus on easy tasks by setting weights aswhere denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature.
19. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models.
20. In [114]5, the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input.
21. [114]6 penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features.
22. To learn domain-invariant features, [114]7 minimizes a distance function [114]9 between a pair of learned representations from different tasks.
23. Candidates of [114]9 include the KL divergence, maximum mean discrepancy [91,126,156]0, and central moment discrepancy [91,126,156]1.
24. Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores.
25. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously.
26. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex.
27. To address this issue, Tchebycheff loss [91,126,156]2 optimizes an MTL model by an ∞ objective, which is formulated aswhere L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 .
28. The Tchebycheff loss can be combined with adversarial MTL as in [91,126,156]3.
29. Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients.
30. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance.
31. As a remedy, PCGrad [91,126,156]4 directly projects conflicting gradients.
32. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g asBased on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [91,126,156]5, which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients.
33. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g .
34. Notice that PCGrad is a special case of GradVac when = 0.
35. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.",How do multi-task learning models optimize the combination and weighting of different task loss functions?,1. How do multi-task learning models optimize the combination and weighting of different task loss functions?,"Questions:

1. How do multi-task learning models optimize the combination and weighting of different task loss functions?

Answer:

(p9.0) The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation. Different tasks may use different types of loss functions. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined as

(p9.1) where L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.

(p9.2) In this case, an important question is how to assign a proper weight to each task. The simplest way is to set them equally [91,126,156], i.e., = 1 . As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155]. For example, to prevent large datasets from dominating training, [93] sets the weights as

(p9.3) where |D | denotes the size of the training dataset for task . The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks as

(p9.4) where measures the variance of the training loss for task . In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label. To ensure that a student model could receive enough supervision during knowledge distillation, BAM! [19] combines the supervised loss L with the distillation loss L as

(p9.5) where increases linearly from 0 to 1 in the training process. In [112], three tasks are jointly optimized, including the primary essay organization evaluation (OE) task and the auxiliary sentence function identification (SFI) and paragraph function identification (PFI) tasks. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 (i.e., = = 1) and the weight of the OE task is set as

(p9.6) where is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller. [85] guides the model to focus on easy tasks by setting weights as

(p9.7) where denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models. In [62], the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input. [14] penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features. To learn domain-invariant features, [137] minimizes a distance function (·) between a pair of learned representations from different tasks. Candidates of (·) include the KL divergence, maximum mean discrepancy (MMD), and central moment discrepancy (CMD). Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex. To address this issue, Tchebycheff loss [75] optimizes an MTL model by an ∞ objective, which is formulated as

(p9.8) where L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 . The Tchebycheff loss can be combined with adversarial MTL as in [70].

(p9.9) Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance. As a remedy, PCGrad [142] directly projects conflicting gradients. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g as

(p9.10) Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g . Notice that PCGrad is a special case of GradVac when = 0. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.",How is the global loss function for an MTL model constructed and optimized?,1. How is the global loss function for an MTL model constructed and optimized?,"Questions:

1. How is the global loss function for an MTL model constructed and optimized?

Answer:

(p9.0) The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation. Different tasks may use different types of loss functions. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined as

(p9.1) where L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.

(p9.2) In this case, an important question is how to assign a proper weight to each task. The simplest way is to set them equally [91,126,156], i.e., = 1 . As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155]. For example, to prevent large datasets from dominating training, [93] sets the weights as

(p9.3) where |D | denotes the size of the training dataset for task . The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks as

(p9.4) where measures the variance of the training loss for task . In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label. To ensure that a student model could receive enough supervision during knowledge distillation, BAM! [19] combines the supervised loss L with the distillation loss L as

(p9.5) where increases linearly from 0 to 1 in the training process. In [112], three tasks are jointly optimized, including the primary essay organization evaluation (OE) task and the auxiliary sentence function identification (SFI) and paragraph function identification (PFI) tasks. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 (i.e., = = 1) and the weight of the OE task is set as

(p9.6) where is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller. [85] guides the model to focus on easy tasks by setting weights as

(p9.7) where denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models. In [62], the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input. [14] penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features. To learn domain-invariant features, [137] minimizes a distance function (·) between a pair of learned representations from different tasks. Candidates of (·) include the KL divergence, maximum mean discrepancy (MMD), and central moment discrepancy (CMD). Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex. To address this issue, Tchebycheff loss [75] optimizes an MTL model by an ∞ objective, which is formulated as

(p9.8) where L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 . The Tchebycheff loss can be combined with adversarial MTL as in [70].

(p9.9) Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance. As a remedy, PCGrad [142] directly projects conflicting gradients. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g as

(p9.10) Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g . Notice that PCGrad is a special case of GradVac when = 0. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.",What are the strategies for combining and weighting loss functions in multi-task learning models?,1. What are the strategies for combining and weighting loss functions in multi-task learning models?,"Questions:

1. What are the strategies for combining and weighting loss functions in multi-task learning models?

Answer:

(p9.0) The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation. Different tasks may use different types of loss functions. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined as

(p9.1) where L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.

(p9.2) In this case, an important question is how to assign a proper weight to each task. The simplest way is to set them equally [91,126,156], i.e., = 1 . As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155]. For example, to prevent large datasets from dominating training, [93] sets the weights as

(p9.3) where |D | denotes the size of the training dataset for task . The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks as

(p9.4) where measures the variance of the training loss for task . In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label. To ensure that a student model could receive enough supervision during knowledge distillation, BAM! [19] combines the supervised loss L with the distillation loss L as

(p9.5) where increases linearly from 0 to 1 in the training process. In [112], three tasks are jointly optimized, including the primary essay organization evaluation (OE) task and the auxiliary sentence function identification (SFI) and paragraph function identification (PFI) tasks. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 (i.e., = = 1) and the weight of the OE task is set as

(p9.6) where is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller. [85] guides the model to focus on easy tasks by setting weights as

(p9.7) where denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models. In [62], the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input. [14] penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features. To learn domain-invariant features, [137] minimizes a distance function (·) between a pair of learned representations from different tasks. Candidates of (·) include the KL divergence, maximum mean discrepancy (MMD), and central moment discrepancy (CMD). Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex. To address this issue, Tchebycheff loss [75] optimizes an MTL model by an ∞ objective, which is formulated as

(p9.8) where L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 . The Tchebycheff loss can be combined with adversarial MTL as in [70].

(p9.9) Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance. As a remedy, PCGrad [142] directly projects conflicting gradients. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g as

(p9.10) Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g . Notice that PCGrad is a special case of GradVac when = 0. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.",How is the global loss function constructed in multi-task learning models for NLP?,1. How is the global loss function constructed in multi-task learning models for NLP?,"Questions:

1. How is the global loss function constructed in multi-task learning models for NLP?

Answer:

(p9.0) The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation. Different tasks may use different types of loss functions. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined as

(p9.1) where L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.

(p9.2) In this case, an important question is how to assign a proper weight to each task. The simplest way is to set them equally [91,126,156], i.e., = 1 . As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155]. For example, to prevent large datasets from dominating training, [93] sets the weights as

(p9.3) where |D | denotes the size of the training dataset for task . The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks as

(p9.4) where measures the variance of the training loss for task . In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label. To ensure that a student model could receive enough supervision during knowledge distillation, BAM! [19] combines the supervised loss L with the distillation loss L as

(p9.5) where increases linearly from 0 to 1 in the training process. In [112], three tasks are jointly optimized, including the primary essay organization evaluation (OE) task and the auxiliary sentence function identification (SFI) and paragraph function identification (PFI) tasks. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 (i.e., = = 1) and the weight of the OE task is set as

(p9.6) where is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller. [85] guides the model to focus on easy tasks by setting weights as

(p9.7) where denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models. In [62], the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input. [14] penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features. To learn domain-invariant features, [137] minimizes a distance function (·) between a pair of learned representations from different tasks. Candidates of (·) include the KL divergence, maximum mean discrepancy (MMD), and central moment discrepancy (CMD). Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex. To address this issue, Tchebycheff loss [75] optimizes an MTL model by an ∞ objective, which is formulated as

(p9.8) where L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 . The Tchebycheff loss can be combined with adversarial MTL as in [70].

(p9.9) Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance. As a remedy, PCGrad [142] directly projects conflicting gradients. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g as

(p9.10) Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g . Notice that PCGrad is a special case of GradVac when = 0. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.",How are loss functions combined and weighted in multi-task learning models?,1. How are loss functions combined and weighted in multi-task learning models?,"Questions:

1. How are loss functions combined and weighted in multi-task learning models?

Answer:

(p9.0) The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation. Different tasks may use different types of loss functions. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined as

(p9.1) where L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.

(p9.2) In this case, an important question is how to assign a proper weight to each task. The simplest way is to set them equally [91,126,156], i.e., = 1 . As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155]. For example, to prevent large datasets from dominating training, [93] sets the weights as

(p9.3) where |D | denotes the size of the training dataset for task . The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks as

(p9.4) where measures the variance of the training loss for task . In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label. To ensure that a student model could receive enough supervision during knowledge distillation, BAM! [19] combines the supervised loss L with the distillation loss L as

(p9.5) where increases linearly from 0 to 1 in the training process. In [112], three tasks are jointly optimized, including the primary essay organization evaluation (OE) task and the auxiliary sentence function identification (SFI) and paragraph function identification (PFI) tasks. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 (i.e., = = 1) and the weight of the OE task is set as

(p9.6) where is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller. [85] guides the model to focus on easy tasks by setting weights as

(p9.7) where denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models. In [62], the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input. [14] penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features. To learn domain-invariant features, [137] minimizes a distance function (·) between a pair of learned representations from different tasks. Candidates of (·) include the KL divergence, maximum mean discrepancy (MMD), and central moment discrepancy (CMD). Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex. To address this issue, Tchebycheff loss [75] optimizes an MTL model by an ∞ objective, which is formulated as

(p9.8) where L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 . The Tchebycheff loss can be combined with adversarial MTL as in [70].

(p9.9) Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance. As a remedy, PCGrad [142] directly projects conflicting gradients. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g as

(p9.10) Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g . Notice that PCGrad is a special case of GradVac when = 0. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s10,Data Sampling,"['p10.0', 'p10.1', 'p10.2', 'p10.3', 'p10.4', 'p10.5', 'p10.6']","['Machine learning models often suffer from imbalanced data distributions. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.', 'A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.', '(2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,', 'As a generalization of proportional sampling in Eq. (2), for task can take the following form as', 'where 1 is called the sampling temperature [128]. Similar to task loss weights, researchers have proposed various techniques to adjust . When < 1, the divergence of sampling probabilities is reduced. can be viewed as a hyperparameter to be set by users or be changed dynamically during training. For example, the annealed sampling method [113] adjusts as training proceeds. Given a total number of epochs, at epoch is set to', 'In this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference. [128] defines as', 'where 0 and denote initial and maximum values of , respectively. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period.']","Machine learning models often suffer from imbalanced data distributions. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.

A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.

(2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,

As a generalization of proportional sampling in Eq. (2), for task can take the following form as

where 1 is called the sampling temperature [128]. Similar to task loss weights, researchers have proposed various techniques to adjust . When < 1, the divergence of sampling probabilities is reduced. can be viewed as a hyperparameter to be set by users or be changed dynamically during training. For example, the annealed sampling method [113] adjusts as training proceeds. Given a total number of epochs, at epoch is set to

In this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference. [128] defines as

where 0 and denote initial and maximum values of , respectively. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period.","(p10.0) Machine learning models often suffer from imbalanced data distributions. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.

(p10.1) A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.

(p10.2) (2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,

(p10.3) As a generalization of proportional sampling in Eq. (2), for task can take the following form as

(p10.4) where 1 is called the sampling temperature [128]. Similar to task loss weights, researchers have proposed various techniques to adjust . When < 1, the divergence of sampling probabilities is reduced. can be viewed as a hyperparameter to be set by users or be changed dynamically during training. For example, the annealed sampling method [113] adjusts as training proceeds. Given a total number of epochs, at epoch is set to

(p10.5) In this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference. [128] defines as

(p10.6) where 0 and denote initial and maximum values of , respectively. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period.","[[], ['b101'], ['b147'], [], ['b112', 'b127'], ['b127'], []]","[[], ['b101'], ['b147'], [], ['b112', 'b127'], ['b127'], []]",5,"1. Machine learning models often suffer from imbalanced data distributions.
2. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved.
3. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets.
4. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.
5. A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.
6. (2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting.
7. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,As a generalization of proportional sampling in Eq. (2), for task can take the following form aswhere 1 is called the sampling temperature [128].
8. Similar to task loss weights, researchers have proposed various techniques to adjust .
9. When < 1, the divergence of sampling probabilities is reduced.
10. can be viewed as a hyperparameter to be set by users or be changed dynamically during training.
11. For example, the annealed sampling method [113] adjusts as training proceeds.
12. Given a total number of epochs, at epoch is set toIn this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference.
13. [128] defines aswhere 0 and denote initial and maximum values of , respectively.
14. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period.",How do multi-task learning models address data imbalance through various sampling techniques?,1. How do multi-task learning models address data imbalance through various sampling techniques?,"Questions:

1. How do multi-task learning models address data imbalance through various sampling techniques?

Answer:

(p10.0) Machine learning models often suffer from imbalanced data distributions. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.

(p10.1) A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.

(p10.2) (2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,

(p10.3) As a generalization of proportional sampling in Eq. (2), for task can take the following form as

(p10.4) where 1 is called the sampling temperature [128]. Similar to task loss weights, researchers have proposed various techniques to adjust . When < 1, the divergence of sampling probabilities is reduced. can be viewed as a hyperparameter to be set by users or be changed dynamically during training. For example, the annealed sampling method [113] adjusts as training proceeds. Given a total number of epochs, at epoch is set to

(p10.5) In this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference. [128] defines as

(p10.6) where 0 and denote initial and maximum values of , respectively. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period.",How do multi-task learning models address imbalanced data distributions during training?,1. How do multi-task learning models address imbalanced data distributions during training?,"Questions:

1. How do multi-task learning models address imbalanced data distributions during training?

Answer:

(p10.0) Machine learning models often suffer from imbalanced data distributions. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.

(p10.1) A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.

(p10.2) (2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,

(p10.3) As a generalization of proportional sampling in Eq. (2), for task can take the following form as

(p10.4) where 1 is called the sampling temperature [128]. Similar to task loss weights, researchers have proposed various techniques to adjust . When < 1, the divergence of sampling probabilities is reduced. can be viewed as a hyperparameter to be set by users or be changed dynamically during training. For example, the annealed sampling method [113] adjusts as training proceeds. Given a total number of epochs, at epoch is set to

(p10.5) In this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference. [128] defines as

(p10.6) where 0 and denote initial and maximum values of , respectively. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period.",What are the advantages and limitations of proportional sampling in multi-task learning?,1. What are the advantages and limitations of proportional sampling in multi-task learning?,"Questions:

1. What are the advantages and limitations of proportional sampling in multi-task learning?

Answer:

(p10.0) Machine learning models often suffer from imbalanced data distributions. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.

(p10.1) A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.

(p10.2) (2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,

(p10.3) As a generalization of proportional sampling in Eq. (2), for task can take the following form as

(p10.4) where 1 is called the sampling temperature [128]. Similar to task loss weights, researchers have proposed various techniques to adjust . When < 1, the divergence of sampling probabilities is reduced. can be viewed as a hyperparameter to be set by users or be changed dynamically during training. For example, the annealed sampling method [113] adjusts as training proceeds. Given a total number of epochs, at epoch is set to

(p10.5) In this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference. [128] defines as

(p10.6) where 0 and denote initial and maximum values of , respectively. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period.",What strategies are employed to address data imbalance in multi-task learning settings?,1. What strategies are employed to address data imbalance in multi-task learning settings?,"Questions:

1. What strategies are employed to address data imbalance in multi-task learning settings?

Answer:

(p10.0) Machine learning models often suffer from imbalanced data distributions. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.

(p10.1) A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.

(p10.2) (2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,

(p10.3) As a generalization of proportional sampling in Eq. (2), for task can take the following form as

(p10.4) where 1 is called the sampling temperature [128]. Similar to task loss weights, researchers have proposed various techniques to adjust . When < 1, the divergence of sampling probabilities is reduced. can be viewed as a hyperparameter to be set by users or be changed dynamically during training. For example, the annealed sampling method [113] adjusts as training proceeds. Given a total number of epochs, at epoch is set to

(p10.5) In this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference. [128] defines as

(p10.6) where 0 and denote initial and maximum values of , respectively. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period.",What are the strategies for addressing data imbalance in multi-task learning models?,1. What are the strategies for addressing data imbalance in multi-task learning models?,"Questions:

1. What are the strategies for addressing data imbalance in multi-task learning models?

Answer:

(p10.0) Machine learning models often suffer from imbalanced data distributions. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.

(p10.1) A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.

(p10.2) (2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,

(p10.3) As a generalization of proportional sampling in Eq. (2), for task can take the following form as

(p10.4) where 1 is called the sampling temperature [128]. Similar to task loss weights, researchers have proposed various techniques to adjust . When < 1, the divergence of sampling probabilities is reduced. can be viewed as a hyperparameter to be set by users or be changed dynamically during training. For example, the annealed sampling method [113] adjusts as training proceeds. Given a total number of epochs, at epoch is set to

(p10.5) In this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference. [128] defines as

(p10.6) where 0 and denote initial and maximum values of , respectively. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s11,Task Scheduling,"['p11.0', 'p11.1', 'p11.2', 'p11.3', 'p11.4', 'p11.5', 'p11.6']","['Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.', 'Alternatively, we can train an MTL model on different tasks at different steps. Similar to the data sampling in Section 3.2, we can assign a task sampling weight for task , which is also called mixing ratio, to control the frequency of data batches from task . The most common task scheduling technique is to shuffle between different tasks [5,20,30,33,38,44,51,71,73,79,80,89,93,99,102,108,109,114,118], either randomly or according to a pre-defined schedule. While random shuffling is widely adopted, introducing more heuristics into scheduling could help further improving the performance of MTL models. For example, according to the similarity between each task and the primary task in a multi-lingual multi-task scenario, [65] defines as', 'where or is set to 1 if the corresponding task or language is the same as the primary task and 0.1 otherwise. Instead of using a fixed mixing ratio designed by hand, some researchers explore using a dynamic mixing ratio during the training process. [40] schedules tasks by a state machine that switches between the two tasks and updates learning rate when validation loss rises. [37] develops a controller meta-network that dynamically schedules tasks based on multi-armed bandits. The controller has arms and optimizes a control policy for arm (task) at step based on an estimated action value , defined as', 'where denotes temperature, is decay rate, and is the observed reward at step that is defined as the negative validation loss of the primary task. Besides probabilistic approaches, task scheduling could also use heuristics based on certain performance metrics. By optimizing the Tchebycheff loss, [75] learns from the task which has the worst validation performance at each step. The CA-MTL model [95] introduces an uncertainty-based sampling strategy based on Shannon entropy. Specifically, given a batch size and tasks, a pool of × samples are first sampled. Then, the uncertainty measure U ( ) for a sample x from task is defined as', ""where denotes the Shannon entropy of the model's prediction on x,ˆis the model's maximum average entropy over samples from each task, and ′ denotes the entropy of a uniform distribution and is used to normalize the variance of the number of classes in each task. At last, samples with the highest uncertainty measures are used for training at the current step. Experiments show that this uncertainty-based sampling strategy could effectively avoid catastrophic forgetting when jointly learning multiple tasks and outperforms the aforementioned annealed sampling [113]."", 'In some cases, multiple tasks are learned sequentially. Such tasks usually form a clear dependency relationship or are of different difficulty levels. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1). Unicoder [49] trains its five pre-training objectives sequentially in each step. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.', 'For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data. [125] first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.']","Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.

Alternatively, we can train an MTL model on different tasks at different steps. Similar to the data sampling in Section 3.2, we can assign a task sampling weight for task , which is also called mixing ratio, to control the frequency of data batches from task . The most common task scheduling technique is to shuffle between different tasks [5,20,30,33,38,44,51,71,73,79,80,89,93,99,102,108,109,114,118], either randomly or according to a pre-defined schedule. While random shuffling is widely adopted, introducing more heuristics into scheduling could help further improving the performance of MTL models. For example, according to the similarity between each task and the primary task in a multi-lingual multi-task scenario, [65] defines as

where or is set to 1 if the corresponding task or language is the same as the primary task and 0.1 otherwise. Instead of using a fixed mixing ratio designed by hand, some researchers explore using a dynamic mixing ratio during the training process. [40] schedules tasks by a state machine that switches between the two tasks and updates learning rate when validation loss rises. [37] develops a controller meta-network that dynamically schedules tasks based on multi-armed bandits. The controller has arms and optimizes a control policy for arm (task) at step based on an estimated action value , defined as

where denotes temperature, is decay rate, and is the observed reward at step that is defined as the negative validation loss of the primary task. Besides probabilistic approaches, task scheduling could also use heuristics based on certain performance metrics. By optimizing the Tchebycheff loss, [75] learns from the task which has the worst validation performance at each step. The CA-MTL model [95] introduces an uncertainty-based sampling strategy based on Shannon entropy. Specifically, given a batch size and tasks, a pool of × samples are first sampled. Then, the uncertainty measure U ( ) for a sample x from task is defined as

where denotes the Shannon entropy of the model's prediction on x,ˆis the model's maximum average entropy over samples from each task, and ′ denotes the entropy of a uniform distribution and is used to normalize the variance of the number of classes in each task. At last, samples with the highest uncertainty measures are used for training at the current step. Experiments show that this uncertainty-based sampling strategy could effectively avoid catastrophic forgetting when jointly learning multiple tasks and outperforms the aforementioned annealed sampling [113].

In some cases, multiple tasks are learned sequentially. Such tasks usually form a clear dependency relationship or are of different difficulty levels. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1). Unicoder [49] trains its five pre-training objectives sequentially in each step. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.

For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data. [125] first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.","(p11.0) Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.

(p11.1) Alternatively, we can train an MTL model on different tasks at different steps. Similar to the data sampling in Section 3.2, we can assign a task sampling weight for task , which is also called mixing ratio, to control the frequency of data batches from task . The most common task scheduling technique is to shuffle between different tasks [5,20,30,33,38,44,51,71,73,79,80,89,93,99,102,108,109,114,118], either randomly or according to a pre-defined schedule. While random shuffling is widely adopted, introducing more heuristics into scheduling could help further improving the performance of MTL models. For example, according to the similarity between each task and the primary task in a multi-lingual multi-task scenario, [65] defines as

(p11.2) where or is set to 1 if the corresponding task or language is the same as the primary task and 0.1 otherwise. Instead of using a fixed mixing ratio designed by hand, some researchers explore using a dynamic mixing ratio during the training process. [40] schedules tasks by a state machine that switches between the two tasks and updates learning rate when validation loss rises. [37] develops a controller meta-network that dynamically schedules tasks based on multi-armed bandits. The controller has arms and optimizes a control policy for arm (task) at step based on an estimated action value , defined as

(p11.3) where denotes temperature, is decay rate, and is the observed reward at step that is defined as the negative validation loss of the primary task. Besides probabilistic approaches, task scheduling could also use heuristics based on certain performance metrics. By optimizing the Tchebycheff loss, [75] learns from the task which has the worst validation performance at each step. The CA-MTL model [95] introduces an uncertainty-based sampling strategy based on Shannon entropy. Specifically, given a batch size and tasks, a pool of × samples are first sampled. Then, the uncertainty measure U ( ) for a sample x from task is defined as

(p11.4) where denotes the Shannon entropy of the model's prediction on x,ˆis the model's maximum average entropy over samples from each task, and ′ denotes the entropy of a uniform distribution and is used to normalize the variance of the number of classes in each task. At last, samples with the highest uncertainty measures are used for training at the current step. Experiments show that this uncertainty-based sampling strategy could effectively avoid catastrophic forgetting when jointly learning multiple tasks and outperforms the aforementioned annealed sampling [113].

(p11.5) In some cases, multiple tasks are learned sequentially. Such tasks usually form a clear dependency relationship or are of different difficulty levels. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1). Unicoder [49] trains its five pre-training objectives sequentially in each step. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.

(p11.6) For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data. [125] first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.","[['b133', 'b142', 'b110', 'b2', 'b147'], ['b72', 'b92', 'b29', 'b70', 'b43', 'b37', 'b50', 'b32', 'b108', 'b113', 'b88', 'b117', 'b4', 'b64', 'b19', 'b101', 'b107', 'b98', 'b79', 'b78'], ['b36', 'b39'], ['b94', 'b74'], ['b112'], ['b84', 'b89', 'b17', 'b48', 'b93', 'b49', 'b42', 'b29'], ['b54', 'b124', 'b43', 'b13']]","[['b133', 'b142', 'b110', 'b2', 'b147'], ['b72', 'b92', 'b29', 'b70', 'b43', 'b37', 'b50', 'b32', 'b108', 'b113', 'b88', 'b117', 'b4', 'b64', 'b19', 'b101', 'b107', 'b98', 'b79', 'b78'], ['b36', 'b39'], ['b94', 'b74'], ['b112'], ['b84', 'b89', 'b17', 'b48', 'b93', 'b49', 'b42', 'b29'], ['b54', 'b124', 'b43', 'b13']]",42,"1. Task scheduling determines the order of tasks in which an MTL model is trained.
2. A naive way is to train all tasks together.
3. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions.
4. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together.
5. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step.
6. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.
7. Alternatively, we can train an MTL model on different tasks at different steps.
8. Similar to the data sampling in Section 3.2, we can assign a task sampling weight for task , which is also called mixing ratio, to control the frequency of data batches from task .
9. The most common task scheduling technique is to shuffle between different tasks [5,20,30,33,38,44,51,71,73,79,80,89,93,99,102,108,109,114,118], either randomly or according to a pre-defined schedule.
10. While random shuffling is widely adopted, introducing more heuristics into scheduling could help further improving the performance of MTL models.
11. For example, according to the similarity between each task and the primary task in a multi-lingual multi-task scenario, [65] defines aswhere or is set to 1 if the corresponding task or language is the same as the primary task and 0.1 otherwise.
12. Instead of using a fixed mixing ratio designed by hand, some researchers explore using a dynamic mixing ratio during the training process.
13. [40] schedules tasks by a state machine that switches between the two tasks and updates learning rate when validation loss rises.
14. [37] develops a controller meta-network that dynamically schedules tasks based on multi-armed bandits.
15. The controller has arms and optimizes a control policy for arm (task) at step based on an estimated action value , defined aswhere denotes temperature, is decay rate, and is the observed reward at step that is defined as the negative validation loss of the primary task.
16. Besides probabilistic approaches, task scheduling could also use heuristics based on certain performance metrics.
17. By optimizing the Tchebycheff loss, [143]0 learns from the task which has the worst validation performance at each step.
18. The CA-MTL model [143]1 introduces an uncertainty-based sampling strategy based on Shannon entropy.
19. Specifically, given a batch size and tasks, a pool of × samples are first sampled.
20. Then, the uncertainty measure U [143]2 for a sample x from task is defined aswhere denotes the Shannon entropy of the model's prediction on x,ˆis the model's maximum average entropy over samples from each task, and ′ denotes the entropy of a uniform distribution and is used to normalize the variance of the number of classes in each task.
21. At last, samples with the highest uncertainty measures are used for training at the current step.
22. Experiments show that this uncertainty-based sampling strategy could effectively avoid catastrophic forgetting when jointly learning multiple tasks and outperforms the aforementioned annealed sampling [143]3.
23. In some cases, multiple tasks are learned sequentially.
24. Such tasks usually form a clear dependency relationship or are of different difficulty levels.
25. For instance, [143]4 uses a baby step curriculum learning approach [143]5 and trains different tasks in the order of increasing difficulties.
26. Similarly, [143]6 trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches.
27. [143]7 focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. [143]8.
28. Unicoder [143]9 trains its five pre-training objectives sequentially in each step.
29. [134]0 applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain.
30. [134]1 first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters.
31. To stabilize the training process when alternating between tasks, successive regularization [134]2 is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.
32. For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks.
33. For example, [134]3 trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task.
34. [134]4 trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data.
35. [134]5 first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data.
36. Similarly, [134]6 first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.",What are the various task scheduling strategies used in multi-task learning models?,1. What are the various task scheduling strategies used in multi-task learning models?,"Questions:

1. What are the various task scheduling strategies used in multi-task learning models?

Answer:

(p11.0) Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.

(p11.1) Alternatively, we can train an MTL model on different tasks at different steps. Similar to the data sampling in Section 3.2, we can assign a task sampling weight for task , which is also called mixing ratio, to control the frequency of data batches from task . The most common task scheduling technique is to shuffle between different tasks [5,20,30,33,38,44,51,71,73,79,80,89,93,99,102,108,109,114,118], either randomly or according to a pre-defined schedule. While random shuffling is widely adopted, introducing more heuristics into scheduling could help further improving the performance of MTL models. For example, according to the similarity between each task and the primary task in a multi-lingual multi-task scenario, [65] defines as

(p11.2) where or is set to 1 if the corresponding task or language is the same as the primary task and 0.1 otherwise. Instead of using a fixed mixing ratio designed by hand, some researchers explore using a dynamic mixing ratio during the training process. [40] schedules tasks by a state machine that switches between the two tasks and updates learning rate when validation loss rises. [37] develops a controller meta-network that dynamically schedules tasks based on multi-armed bandits. The controller has arms and optimizes a control policy for arm (task) at step based on an estimated action value , defined as

(p11.3) where denotes temperature, is decay rate, and is the observed reward at step that is defined as the negative validation loss of the primary task. Besides probabilistic approaches, task scheduling could also use heuristics based on certain performance metrics. By optimizing the Tchebycheff loss, [75] learns from the task which has the worst validation performance at each step. The CA-MTL model [95] introduces an uncertainty-based sampling strategy based on Shannon entropy. Specifically, given a batch size and tasks, a pool of × samples are first sampled. Then, the uncertainty measure U ( ) for a sample x from task is defined as

(p11.4) where denotes the Shannon entropy of the model's prediction on x,ˆis the model's maximum average entropy over samples from each task, and ′ denotes the entropy of a uniform distribution and is used to normalize the variance of the number of classes in each task. At last, samples with the highest uncertainty measures are used for training at the current step. Experiments show that this uncertainty-based sampling strategy could effectively avoid catastrophic forgetting when jointly learning multiple tasks and outperforms the aforementioned annealed sampling [113].

(p11.5) In some cases, multiple tasks are learned sequentially. Such tasks usually form a clear dependency relationship or are of different difficulty levels. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1). Unicoder [49] trains its five pre-training objectives sequentially in each step. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.

(p11.6) For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data. [125] first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.",What are the various approaches to task scheduling in multi-task learning models?,1. What are the various approaches to task scheduling in multi-task learning models?,"Questions:

1. What are the various approaches to task scheduling in multi-task learning models?

Answer:

(p11.0) Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.

(p11.1) Alternatively, we can train an MTL model on different tasks at different steps. Similar to the data sampling in Section 3.2, we can assign a task sampling weight for task , which is also called mixing ratio, to control the frequency of data batches from task . The most common task scheduling technique is to shuffle between different tasks [5,20,30,33,38,44,51,71,73,79,80,89,93,99,102,108,109,114,118], either randomly or according to a pre-defined schedule. While random shuffling is widely adopted, introducing more heuristics into scheduling could help further improving the performance of MTL models. For example, according to the similarity between each task and the primary task in a multi-lingual multi-task scenario, [65] defines as

(p11.2) where or is set to 1 if the corresponding task or language is the same as the primary task and 0.1 otherwise. Instead of using a fixed mixing ratio designed by hand, some researchers explore using a dynamic mixing ratio during the training process. [40] schedules tasks by a state machine that switches between the two tasks and updates learning rate when validation loss rises. [37] develops a controller meta-network that dynamically schedules tasks based on multi-armed bandits. The controller has arms and optimizes a control policy for arm (task) at step based on an estimated action value , defined as

(p11.3) where denotes temperature, is decay rate, and is the observed reward at step that is defined as the negative validation loss of the primary task. Besides probabilistic approaches, task scheduling could also use heuristics based on certain performance metrics. By optimizing the Tchebycheff loss, [75] learns from the task which has the worst validation performance at each step. The CA-MTL model [95] introduces an uncertainty-based sampling strategy based on Shannon entropy. Specifically, given a batch size and tasks, a pool of × samples are first sampled. Then, the uncertainty measure U ( ) for a sample x from task is defined as

(p11.4) where denotes the Shannon entropy of the model's prediction on x,ˆis the model's maximum average entropy over samples from each task, and ′ denotes the entropy of a uniform distribution and is used to normalize the variance of the number of classes in each task. At last, samples with the highest uncertainty measures are used for training at the current step. Experiments show that this uncertainty-based sampling strategy could effectively avoid catastrophic forgetting when jointly learning multiple tasks and outperforms the aforementioned annealed sampling [113].

(p11.5) In some cases, multiple tasks are learned sequentially. Such tasks usually form a clear dependency relationship or are of different difficulty levels. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1). Unicoder [49] trains its five pre-training objectives sequentially in each step. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.

(p11.6) For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data. [125] first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.",What are the different methodologies for task scheduling in multi-task learning models?,1. What are the different methodologies for task scheduling in multi-task learning models?,"Questions:

1. What are the different methodologies for task scheduling in multi-task learning models?

Answer:

(p11.0) Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.

(p11.1) Alternatively, we can train an MTL model on different tasks at different steps. Similar to the data sampling in Section 3.2, we can assign a task sampling weight for task , which is also called mixing ratio, to control the frequency of data batches from task . The most common task scheduling technique is to shuffle between different tasks [5,20,30,33,38,44,51,71,73,79,80,89,93,99,102,108,109,114,118], either randomly or according to a pre-defined schedule. While random shuffling is widely adopted, introducing more heuristics into scheduling could help further improving the performance of MTL models. For example, according to the similarity between each task and the primary task in a multi-lingual multi-task scenario, [65] defines as

(p11.2) where or is set to 1 if the corresponding task or language is the same as the primary task and 0.1 otherwise. Instead of using a fixed mixing ratio designed by hand, some researchers explore using a dynamic mixing ratio during the training process. [40] schedules tasks by a state machine that switches between the two tasks and updates learning rate when validation loss rises. [37] develops a controller meta-network that dynamically schedules tasks based on multi-armed bandits. The controller has arms and optimizes a control policy for arm (task) at step based on an estimated action value , defined as

(p11.3) where denotes temperature, is decay rate, and is the observed reward at step that is defined as the negative validation loss of the primary task. Besides probabilistic approaches, task scheduling could also use heuristics based on certain performance metrics. By optimizing the Tchebycheff loss, [75] learns from the task which has the worst validation performance at each step. The CA-MTL model [95] introduces an uncertainty-based sampling strategy based on Shannon entropy. Specifically, given a batch size and tasks, a pool of × samples are first sampled. Then, the uncertainty measure U ( ) for a sample x from task is defined as

(p11.4) where denotes the Shannon entropy of the model's prediction on x,ˆis the model's maximum average entropy over samples from each task, and ′ denotes the entropy of a uniform distribution and is used to normalize the variance of the number of classes in each task. At last, samples with the highest uncertainty measures are used for training at the current step. Experiments show that this uncertainty-based sampling strategy could effectively avoid catastrophic forgetting when jointly learning multiple tasks and outperforms the aforementioned annealed sampling [113].

(p11.5) In some cases, multiple tasks are learned sequentially. Such tasks usually form a clear dependency relationship or are of different difficulty levels. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1). Unicoder [49] trains its five pre-training objectives sequentially in each step. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.

(p11.6) For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data. [125] first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.",What are the different approaches to task scheduling in multi-task learning models?,1. What are the different approaches to task scheduling in multi-task learning models?,"Questions:

1. What are the different approaches to task scheduling in multi-task learning models?

Answer:

(p11.0) Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.

(p11.1) Alternatively, we can train an MTL model on different tasks at different steps. Similar to the data sampling in Section 3.2, we can assign a task sampling weight for task , which is also called mixing ratio, to control the frequency of data batches from task . The most common task scheduling technique is to shuffle between different tasks [5,20,30,33,38,44,51,71,73,79,80,89,93,99,102,108,109,114,118], either randomly or according to a pre-defined schedule. While random shuffling is widely adopted, introducing more heuristics into scheduling could help further improving the performance of MTL models. For example, according to the similarity between each task and the primary task in a multi-lingual multi-task scenario, [65] defines as

(p11.2) where or is set to 1 if the corresponding task or language is the same as the primary task and 0.1 otherwise. Instead of using a fixed mixing ratio designed by hand, some researchers explore using a dynamic mixing ratio during the training process. [40] schedules tasks by a state machine that switches between the two tasks and updates learning rate when validation loss rises. [37] develops a controller meta-network that dynamically schedules tasks based on multi-armed bandits. The controller has arms and optimizes a control policy for arm (task) at step based on an estimated action value , defined as

(p11.3) where denotes temperature, is decay rate, and is the observed reward at step that is defined as the negative validation loss of the primary task. Besides probabilistic approaches, task scheduling could also use heuristics based on certain performance metrics. By optimizing the Tchebycheff loss, [75] learns from the task which has the worst validation performance at each step. The CA-MTL model [95] introduces an uncertainty-based sampling strategy based on Shannon entropy. Specifically, given a batch size and tasks, a pool of × samples are first sampled. Then, the uncertainty measure U ( ) for a sample x from task is defined as

(p11.4) where denotes the Shannon entropy of the model's prediction on x,ˆis the model's maximum average entropy over samples from each task, and ′ denotes the entropy of a uniform distribution and is used to normalize the variance of the number of classes in each task. At last, samples with the highest uncertainty measures are used for training at the current step. Experiments show that this uncertainty-based sampling strategy could effectively avoid catastrophic forgetting when jointly learning multiple tasks and outperforms the aforementioned annealed sampling [113].

(p11.5) In some cases, multiple tasks are learned sequentially. Such tasks usually form a clear dependency relationship or are of different difficulty levels. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1). Unicoder [49] trains its five pre-training objectives sequentially in each step. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.

(p11.6) For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data. [125] first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.",What strategies are employed in task scheduling for multi-task learning models?,1. What strategies are employed in task scheduling for multi-task learning models?,"Questions:

1. What strategies are employed in task scheduling for multi-task learning models?

Answer:

(p11.0) Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.

(p11.1) Alternatively, we can train an MTL model on different tasks at different steps. Similar to the data sampling in Section 3.2, we can assign a task sampling weight for task , which is also called mixing ratio, to control the frequency of data batches from task . The most common task scheduling technique is to shuffle between different tasks [5,20,30,33,38,44,51,71,73,79,80,89,93,99,102,108,109,114,118], either randomly or according to a pre-defined schedule. While random shuffling is widely adopted, introducing more heuristics into scheduling could help further improving the performance of MTL models. For example, according to the similarity between each task and the primary task in a multi-lingual multi-task scenario, [65] defines as

(p11.2) where or is set to 1 if the corresponding task or language is the same as the primary task and 0.1 otherwise. Instead of using a fixed mixing ratio designed by hand, some researchers explore using a dynamic mixing ratio during the training process. [40] schedules tasks by a state machine that switches between the two tasks and updates learning rate when validation loss rises. [37] develops a controller meta-network that dynamically schedules tasks based on multi-armed bandits. The controller has arms and optimizes a control policy for arm (task) at step based on an estimated action value , defined as

(p11.3) where denotes temperature, is decay rate, and is the observed reward at step that is defined as the negative validation loss of the primary task. Besides probabilistic approaches, task scheduling could also use heuristics based on certain performance metrics. By optimizing the Tchebycheff loss, [75] learns from the task which has the worst validation performance at each step. The CA-MTL model [95] introduces an uncertainty-based sampling strategy based on Shannon entropy. Specifically, given a batch size and tasks, a pool of × samples are first sampled. Then, the uncertainty measure U ( ) for a sample x from task is defined as

(p11.4) where denotes the Shannon entropy of the model's prediction on x,ˆis the model's maximum average entropy over samples from each task, and ′ denotes the entropy of a uniform distribution and is used to normalize the variance of the number of classes in each task. At last, samples with the highest uncertainty measures are used for training at the current step. Experiments show that this uncertainty-based sampling strategy could effectively avoid catastrophic forgetting when jointly learning multiple tasks and outperforms the aforementioned annealed sampling [113].

(p11.5) In some cases, multiple tasks are learned sequentially. Such tasks usually form a clear dependency relationship or are of different difficulty levels. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1). Unicoder [49] trains its five pre-training objectives sequentially in each step. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.

(p11.6) For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data. [125] first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s13,Auxiliary MTL,"['p13.0', 'p13.1']","['Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.', ""Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction. [50] trains an extractive summarization model together with an auxiliary document-level classification task. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks. and rebuttal pairs of scientific papers. [58] makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task. To handle the primary disfluency detection task, [125] pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task.""]","Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.

Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction. [50] trains an extractive summarization model together with an auxiliary document-level classification task. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks. and rebuttal pairs of scientific papers. [58] makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task. To handle the primary disfluency detection task, [125] pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task.","(p13.0) Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.

(p13.1) Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction. [50] trains an extractive summarization model together with an auxiliary document-level classification task. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks. and rebuttal pairs of scientific papers. [58] makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task. To handle the primary disfluency detection task, [125] pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task.","[[], ['b60', 'b133', 'b96', 'b136', 'b0', 'b57', 'b83', 'b49', 'b124', 'b129', 'b14', 'b2']]","[[], ['b60', 'b133', 'b96', 'b136', 'b0', 'b57', 'b83', 'b49', 'b124', 'b129', 'b14', 'b2']]",12,"1. Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning.
2. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks.
3. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.
4. Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data.
5. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging.
6. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction.
7. [50] trains an extractive summarization model together with an auxiliary document-level classification task.
8. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture.
9. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model.
10. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features.
11. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task.
12. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation.
13. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1.
14. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved.
15. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively.
16. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks.
17. and rebuttal pairs of scientific papers.
18. [3]0 makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task.
19. To handle the primary disfluency detection task, [3]1 pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task.",How does Auxiliary MTL enhance performance in various NLP tasks?,1. How does Auxiliary MTL enhance performance in various NLP tasks?,"Questions:

1. How does Auxiliary MTL enhance performance in various NLP tasks?

Answer:

(p13.0) Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.

(p13.1) Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction. [50] trains an extractive summarization model together with an auxiliary document-level classification task. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks. and rebuttal pairs of scientific papers. [58] makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task. To handle the primary disfluency detection task, [125] pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task.",How does Auxiliary MTL enhance primary task performance in NLP?,1. How does Auxiliary MTL enhance primary task performance in NLP?,"Questions:

1. How does Auxiliary MTL enhance primary task performance in NLP?

Answer:

(p13.0) Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.

(p13.1) Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction. [50] trains an extractive summarization model together with an auxiliary document-level classification task. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks. and rebuttal pairs of scientific papers. [58] makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task. To handle the primary disfluency detection task, [125] pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task.",How does Auxiliary MTL enhance performance in primary NLP tasks?,1. How does Auxiliary MTL enhance performance in primary NLP tasks?,"Questions:

1. How does Auxiliary MTL enhance performance in primary NLP tasks?

Answer:

(p13.0) Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.

(p13.1) Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction. [50] trains an extractive summarization model together with an auxiliary document-level classification task. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks. and rebuttal pairs of scientific papers. [58] makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task. To handle the primary disfluency detection task, [125] pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task.",How does Auxiliary MTL enhance primary task performance in NLP?,1. How does Auxiliary MTL enhance primary task performance in NLP?,"Questions:

1. How does Auxiliary MTL enhance primary task performance in NLP?

Answer:

(p13.0) Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.

(p13.1) Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction. [50] trains an extractive summarization model together with an auxiliary document-level classification task. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks. and rebuttal pairs of scientific papers. [58] makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task. To handle the primary disfluency detection task, [125] pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task.",How does Auxiliary MTL enhance primary task performance in NLP?,1. How does Auxiliary MTL enhance primary task performance in NLP?,"Questions:

1. How does Auxiliary MTL enhance primary task performance in NLP?

Answer:

(p13.0) Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.

(p13.1) Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction. [50] trains an extractive summarization model together with an auxiliary document-level classification task. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks. and rebuttal pairs of scientific papers. [58] makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task. To handle the primary disfluency detection task, [125] pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s14,Primary,"['p14.0', 'p14.1', 'p14.2', 'p14.3']","['Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.', '[62] models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks. Generative adversarial MTL architectures are used to improve classification tasks as well. Targeting pharmacovigilance mining, [138] treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features. Differently, [98] enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data. Sentiment classification models can be enhanced by POS tagging and gaze prediction [80], label distribution learning [149], unsupervised topic modeling [124], or domain adversarial training [127]. In [132], besides the shared base model, a separate model is built for each Microblog user as an auxiliary task. [96] estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task. [67] introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task. [139] improves a community question answering model with an auxiliary question category classification task. To counter data scarcity in the multi-choice question answering task, [51] proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.', 'For text generation tasks, MTL is brought in to improve the quality of the generated text. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155]. [105] implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing. [9] enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks. [99] views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.', 'Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.']","Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.

[62] models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks. Generative adversarial MTL architectures are used to improve classification tasks as well. Targeting pharmacovigilance mining, [138] treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features. Differently, [98] enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data. Sentiment classification models can be enhanced by POS tagging and gaze prediction [80], label distribution learning [149], unsupervised topic modeling [124], or domain adversarial training [127]. In [132], besides the shared base model, a separate model is built for each Microblog user as an auxiliary task. [96] estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task. [67] introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task. [139] improves a community question answering model with an auxiliary question category classification task. To counter data scarcity in the multi-choice question answering task, [51] proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.

For text generation tasks, MTL is brought in to improve the quality of the generated text. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155]. [105] implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing. [9] enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks. [99] views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.

Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.","(p14.0) Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.

(p14.1) [62] models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks. Generative adversarial MTL architectures are used to improve classification tasks as well. Targeting pharmacovigilance mining, [138] treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features. Differently, [98] enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data. Sentiment classification models can be enhanced by POS tagging and gaze prediction [80], label distribution learning [149], unsupervised topic modeling [124], or domain adversarial training [127]. In [132], besides the shared base model, a separate model is built for each Microblog user as an auxiliary task. [96] estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task. [67] introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task. [139] improves a community question answering model with an auxiliary question category classification task. To counter data scarcity in the multi-choice question answering task, [51] proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.

(p14.2) For text generation tasks, MTL is brought in to improve the quality of the generated text. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155]. [105] implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing. [9] enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks. [99] views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.

(p14.3) Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.","[['b54', 'b55', 'b58', 'b132', 'b111', 'b13', 'b32', 'b27', 'b73', 'b52', 'b70', 'b106'], ['b138', 'b95', 'b66', 'b123', 'b148', 'b50', 'b97', 'b79', 'b126', 'b131', 'b137'], ['b37', 'b8', 'b104', 'b102', 'b72', 'b98', 'b36', 'b143', 'b153', 'b154', 'b127', 'b26'], ['b155', 'b113', 'b125']]","[['b54', 'b55', 'b58', 'b132', 'b111', 'b13', 'b32', 'b27', 'b73', 'b52', 'b70', 'b106'], ['b138', 'b95', 'b66', 'b123', 'b148', 'b50', 'b97', 'b79', 'b126', 'b131', 'b137'], ['b37', 'b8', 'b104', 'b102', 'b72', 'b98', 'b36', 'b143', 'b153', 'b154', 'b127', 'b26'], ['b155', 'b113', 'b125']]",38,"1. Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification.
2. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks.
3. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction.
4. [59] enhances a rumor detection model with user credibility features.
5. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance.
6. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token.
7. In [107], text classification is boosted by learning the predominant sense of words.
8. [133] assists the fake news detection task by stance classification.
9. [14] jointly learns the answer identification task with an auxiliary question answering task.
10. To improve slot filling performance for online shopping assistants, [56]0 adds NER and segment tagging tasks as auxiliary tasks.
11. In [56]1, the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.
12. [56]2 models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks.
13. Generative adversarial MTL architectures are used to improve classification tasks as well.
14. Targeting pharmacovigilance mining, [56]3 treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features.
15. Differently, [56]4 enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data.
16. Sentiment classification models can be enhanced by POS tagging and gaze prediction [56]5, label distribution learning [56]6, unsupervised topic modeling [56]7, or domain adversarial training [56]8.
17. In [56]9, besides the shared base model, a separate model is built for each Microblog user as an auxiliary task.
18. [53]0 estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task.
19. [53]1 introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task.
20. [53]2 improves a community question answering model with an auxiliary question category classification task.
21. To counter data scarcity in the multi-choice question answering task, [53]3 proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.
22. For text generation tasks, MTL is brought in to improve the quality of the generated text.
23. It is observed in [53]4 that adding a target-side language modeling task on the decoder of a neural machine translation [53]5 model brings moderate but consistent performance gain.
24. [53]6 learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks.
25. Similarly, [53]7 learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks.
26. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [53]8 adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data.
27. Recently, [53]9 builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder.
28. For the sentence simplification task, [55]0 uses paraphrase generation and entailment generation as two auxiliary tasks.
29. [55]1 builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks.
30. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [55]2 or task-oriented dialogue generation [55]3.
31. [55]4 implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing.
32. [55]5 enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks.
33. [55]6 views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.
34. Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks.
35. For example, [55]7 learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks.
36. [55]8 trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation.
37. As existing pre-trained models impose huge storage cost for the deployment, PinText [55]9 learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.",How does auxiliary multi-task learning (MTL) enhance performance in various NLP tasks?,1. How does auxiliary multi-task learning (MTL) enhance performance in various NLP tasks?,"Questions:

1. How does auxiliary multi-task learning (MTL) enhance performance in various NLP tasks?

Answer:

(p14.0) Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.

(p14.1) [62] models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks. Generative adversarial MTL architectures are used to improve classification tasks as well. Targeting pharmacovigilance mining, [138] treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features. Differently, [98] enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data. Sentiment classification models can be enhanced by POS tagging and gaze prediction [80], label distribution learning [149], unsupervised topic modeling [124], or domain adversarial training [127]. In [132], besides the shared base model, a separate model is built for each Microblog user as an auxiliary task. [96] estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task. [67] introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task. [139] improves a community question answering model with an auxiliary question category classification task. To counter data scarcity in the multi-choice question answering task, [51] proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.

(p14.2) For text generation tasks, MTL is brought in to improve the quality of the generated text. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155]. [105] implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing. [9] enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks. [99] views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.

(p14.3) Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.",How does auxiliary multi-task learning (MTL) enhance performance in various NLP classification tasks?,1. How does auxiliary multi-task learning (MTL) enhance performance in various NLP classification tasks?,"Questions:

1. How does auxiliary multi-task learning (MTL) enhance performance in various NLP classification tasks?

Answer:

(p14.0) Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.

(p14.1) [62] models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks. Generative adversarial MTL architectures are used to improve classification tasks as well. Targeting pharmacovigilance mining, [138] treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features. Differently, [98] enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data. Sentiment classification models can be enhanced by POS tagging and gaze prediction [80], label distribution learning [149], unsupervised topic modeling [124], or domain adversarial training [127]. In [132], besides the shared base model, a separate model is built for each Microblog user as an auxiliary task. [96] estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task. [67] introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task. [139] improves a community question answering model with an auxiliary question category classification task. To counter data scarcity in the multi-choice question answering task, [51] proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.

(p14.2) For text generation tasks, MTL is brought in to improve the quality of the generated text. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155]. [105] implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing. [9] enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks. [99] views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.

(p14.3) Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.",How is multi-task learning applied in various natural language processing tasks?,1. How is multi-task learning applied in various natural language processing tasks?,"Questions:

1. How is multi-task learning applied in various natural language processing tasks?

Answer:

(p14.0) Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.

(p14.1) [62] models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks. Generative adversarial MTL architectures are used to improve classification tasks as well. Targeting pharmacovigilance mining, [138] treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features. Differently, [98] enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data. Sentiment classification models can be enhanced by POS tagging and gaze prediction [80], label distribution learning [149], unsupervised topic modeling [124], or domain adversarial training [127]. In [132], besides the shared base model, a separate model is built for each Microblog user as an auxiliary task. [96] estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task. [67] introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task. [139] improves a community question answering model with an auxiliary question category classification task. To counter data scarcity in the multi-choice question answering task, [51] proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.

(p14.2) For text generation tasks, MTL is brought in to improve the quality of the generated text. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155]. [105] implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing. [9] enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks. [99] views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.

(p14.3) Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.",How does auxiliary multi-task learning (MTL) enhance performance in NLP classification tasks?,1. How does auxiliary multi-task learning (MTL) enhance performance in NLP classification tasks?,"Questions:

1. How does auxiliary multi-task learning (MTL) enhance performance in NLP classification tasks?

Answer:

(p14.0) Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.

(p14.1) [62] models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks. Generative adversarial MTL architectures are used to improve classification tasks as well. Targeting pharmacovigilance mining, [138] treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features. Differently, [98] enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data. Sentiment classification models can be enhanced by POS tagging and gaze prediction [80], label distribution learning [149], unsupervised topic modeling [124], or domain adversarial training [127]. In [132], besides the shared base model, a separate model is built for each Microblog user as an auxiliary task. [96] estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task. [67] introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task. [139] improves a community question answering model with an auxiliary question category classification task. To counter data scarcity in the multi-choice question answering task, [51] proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.

(p14.2) For text generation tasks, MTL is brought in to improve the quality of the generated text. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155]. [105] implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing. [9] enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks. [99] views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.

(p14.3) Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.",How does multi-task learning enhance performance in NLP tasks?,1. How does multi-task learning enhance performance in NLP tasks?,"Questions:

1. How does multi-task learning enhance performance in NLP tasks?

Answer:

(p14.0) Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.

(p14.1) [62] models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks. Generative adversarial MTL architectures are used to improve classification tasks as well. Targeting pharmacovigilance mining, [138] treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features. Differently, [98] enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data. Sentiment classification models can be enhanced by POS tagging and gaze prediction [80], label distribution learning [149], unsupervised topic modeling [124], or domain adversarial training [127]. In [132], besides the shared base model, a separate model is built for each Microblog user as an auxiliary task. [96] estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task. [67] introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task. [139] improves a community question answering model with an auxiliary question category classification task. To counter data scarcity in the multi-choice question answering task, [51] proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.

(p14.2) For text generation tasks, MTL is brought in to improve the quality of the generated text. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155]. [105] implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing. [9] enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks. [99] views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.

(p14.3) Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s15,Joint MTL,"['p15.0', 'p15.1', 'p15.2']","['Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.', ""Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks. One major problem of such tasks is the lack of sufficient labeled data. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques. [152] develops a joint MTL model for the NER and entity name normalization tasks in the medical field. [68,146] use MTL to perform simile detection, which includes simile sentence classification and simile component extraction. To analyze Twitter demographic data, [121] jointly learns classification models for genders, ages, political orientations, and locations. The SLUICE network [100] is used to learn four different non-literal language detection tasks in English and German [26]. [86] jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French. For community question answering, [52] builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time. To analyze the argumentative structure of scientific publications, [57] optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism. Considering the connection between sentence emotions and the use of the metaphor, [22] jointly trains a metaphor identification model with an emotion detection model. To ensure the consistency between generated key phrases (short text) and headlines (long text), [85] trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism. An MTL model is proposed in [111] to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools. Table 2. A summary of joint MTL studies according to types of tasks involved. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively. A single checkmark could mean joint learning of multiple tasks of the same type."", 'Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118]. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [32], Predicate-Argument Structures (PAS) [76], and Prague Semantic Dependencies (PSD) [42]) jointly. In [47], a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation (AMR) [4], Semantic Dependency Parsing (SDP) [88], and Universal Dependencies (UD) [87], and it shows that joint training improves performance on the testing UCCA dataset. [71] jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT. [29] shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks. In [143], an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.']","Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.

Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks. One major problem of such tasks is the lack of sufficient labeled data. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques. [152] develops a joint MTL model for the NER and entity name normalization tasks in the medical field. [68,146] use MTL to perform simile detection, which includes simile sentence classification and simile component extraction. To analyze Twitter demographic data, [121] jointly learns classification models for genders, ages, political orientations, and locations. The SLUICE network [100] is used to learn four different non-literal language detection tasks in English and German [26]. [86] jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French. For community question answering, [52] builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time. To analyze the argumentative structure of scientific publications, [57] optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism. Considering the connection between sentence emotions and the use of the metaphor, [22] jointly trains a metaphor identification model with an emotion detection model. To ensure the consistency between generated key phrases (short text) and headlines (long text), [85] trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism. An MTL model is proposed in [111] to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools. Table 2. A summary of joint MTL studies according to types of tasks involved. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively. A single checkmark could mean joint learning of multiple tasks of the same type.

Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118]. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [32], Predicate-Argument Structures (PAS) [76], and Prague Semantic Dependencies (PSD) [42]) jointly. In [47], a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation (AMR) [4], Semantic Dependency Parsing (SDP) [88], and Universal Dependencies (UD) [87], and it shows that joint training improves performance on the testing UCCA dataset. [71] jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT. [29] shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks. In [143], an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.","(p15.0) Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.

(p15.1) Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks. One major problem of such tasks is the lack of sufficient labeled data. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques. [152] develops a joint MTL model for the NER and entity name normalization tasks in the medical field. [68,146] use MTL to perform simile detection, which includes simile sentence classification and simile component extraction. To analyze Twitter demographic data, [121] jointly learns classification models for genders, ages, political orientations, and locations. The SLUICE network [100] is used to learn four different non-literal language detection tasks in English and German [26]. [86] jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French. For community question answering, [52] builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time. To analyze the argumentative structure of scientific publications, [57] optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism. Considering the connection between sentence emotions and the use of the metaphor, [22] jointly trains a metaphor identification model with an emotion detection model. To ensure the consistency between generated key phrases (short text) and headlines (long text), [85] trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism. An MTL model is proposed in [111] to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools. Table 2. A summary of joint MTL studies according to types of tasks involved. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively. A single checkmark could mean joint learning of multiple tasks of the same type.

(p15.2) Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118]. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [32], Predicate-Argument Structures (PAS) [76], and Prague Semantic Dependencies (PSD) [42]) jointly. In [47], a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation (AMR) [4], Semantic Dependency Parsing (SDP) [88], and Universal Dependencies (UD) [87], and it shows that joint training improves performance on the testing UCCA dataset. [71] jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT. [29] shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks. In [143], an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.","[['b101', 'b39', 'b109', 'b150', 'b140', 'b7', 'b92', 'b42', 'b43', 'b144', 'b71', 'b147'], ['b21', 'b84', 'b56', 'b110', 'b120', 'b145', 'b151', 'b99', 'b67', 'b25', 'b85', 'b51'], ['b3', 'b46', 'b70', 'b90', 'b28', 'b59', 'b41', 'b5', 'b20', 'b31', 'b115', 'b117', 'b142', 'b4', 'b87', 'b130', 'b40', 'b75', 'b53', 'b82', 'b86']]","[['b101', 'b39', 'b109', 'b150', 'b140', 'b7', 'b92', 'b42', 'b43', 'b144', 'b71', 'b147'], ['b21', 'b84', 'b56', 'b110', 'b120', 'b145', 'b151', 'b99', 'b67', 'b25', 'b85', 'b51'], ['b3', 'b46', 'b70', 'b90', 'b28', 'b59', 'b41', 'b5', 'b20', 'b31', 'b115', 'b117', 'b142', 'b4', 'b87', 'b130', 'b40', 'b75', 'b53', 'b82', 'b86']]",45,"1. Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously.
2. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other.
3. Table 2 gives an overview of task combinations used in joint MTL models.
4. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other.
5. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively.
6. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model.
7. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks.
8. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification.
9. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews.
10. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures.
11. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks.
12. [40,141,145] learn entity extraction alongside relation extraction.
13. For sentiment analysis tasks, [110]0 jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture.
14. [110]1 learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis.
15. [110]2 builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.
16. Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks.
17. One major problem of such tasks is the lack of sufficient labeled data.
18. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing.
19. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques.
20. [110]3 develops a joint MTL model for the NER and entity name normalization tasks in the medical field.
21. [110]4 use MTL to perform simile detection, which includes simile sentence classification and simile component extraction.
22. To analyze Twitter demographic data, [110]5 jointly learns classification models for genders, ages, political orientations, and locations.
23. The SLUICE network [110]6 is used to learn four different non-literal language detection tasks in English and German [110]7.
24. [110]8 jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French.
25. For community question answering, [110]9 builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time.
26. To analyze the argumentative structure of scientific publications, [43]0 optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism.
27. Considering the connection between sentence emotions and the use of the metaphor, [43]1 jointly trains a metaphor identification model with an emotion detection model.
28. To ensure the consistency between generated key phrases [43]2 and headlines [43]3, [43]4 trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism.
29. An MTL model is proposed in [43]5 to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools.
30. Table 2. A summary of joint MTL studies according to types of tasks involved.
31. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively.
32. A single checkmark could mean joint learning of multiple tasks of the same type.
33. Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks.
34. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions.
35. Applications in multi-domain NLP tasks include sentiment classification [43]6, dialog state tracking [43]7, essay scoring [43]8, deceptive review detection [43]9, multi-genre emotion detection and classification [148]0, RST discourse parsing [148]1, historical spelling normalization [148]2, and document classification [148]3.
36. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces.
37. [148]4 model three different formalisms of semantic dependency parsing [148]5 [148]6, Predicate-Argument Structures [148]7 [148]8, and Prague Semantic Dependencies [148]9 [72]0) jointly.
38. In [72]1, a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation [72]2 [72]3, Semantic Dependency Parsing [72]4 [72]5, and Universal Dependencies [72]6 [72]7, and it shows that joint training improves performance on the testing UCCA dataset.
39. [72]8 jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT.
40. [72]9 shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks.
41. In [102]0, an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.",How does joint Multi-Task Learning (MTL) differ from auxiliary MTL in natural language processing?,1. How does joint Multi-Task Learning (MTL) differ from auxiliary MTL in natural language processing?,"Questions:

1. How does joint Multi-Task Learning (MTL) differ from auxiliary MTL in natural language processing?

Answer:

(p15.0) Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.

(p15.1) Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks. One major problem of such tasks is the lack of sufficient labeled data. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques. [152] develops a joint MTL model for the NER and entity name normalization tasks in the medical field. [68,146] use MTL to perform simile detection, which includes simile sentence classification and simile component extraction. To analyze Twitter demographic data, [121] jointly learns classification models for genders, ages, political orientations, and locations. The SLUICE network [100] is used to learn four different non-literal language detection tasks in English and German [26]. [86] jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French. For community question answering, [52] builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time. To analyze the argumentative structure of scientific publications, [57] optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism. Considering the connection between sentence emotions and the use of the metaphor, [22] jointly trains a metaphor identification model with an emotion detection model. To ensure the consistency between generated key phrases (short text) and headlines (long text), [85] trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism. An MTL model is proposed in [111] to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools. Table 2. A summary of joint MTL studies according to types of tasks involved. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively. A single checkmark could mean joint learning of multiple tasks of the same type.

(p15.2) Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118]. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [32], Predicate-Argument Structures (PAS) [76], and Prague Semantic Dependencies (PSD) [42]) jointly. In [47], a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation (AMR) [4], Semantic Dependency Parsing (SDP) [88], and Universal Dependencies (UD) [87], and it shows that joint training improves performance on the testing UCCA dataset. [71] jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT. [29] shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks. In [143], an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.",How does joint MTL improve performance across various NLP tasks compared to single-task learning?,1. How does joint MTL improve performance across various NLP tasks compared to single-task learning?,"Questions:

1. How does joint MTL improve performance across various NLP tasks compared to single-task learning?

Answer:

(p15.0) Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.

(p15.1) Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks. One major problem of such tasks is the lack of sufficient labeled data. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques. [152] develops a joint MTL model for the NER and entity name normalization tasks in the medical field. [68,146] use MTL to perform simile detection, which includes simile sentence classification and simile component extraction. To analyze Twitter demographic data, [121] jointly learns classification models for genders, ages, political orientations, and locations. The SLUICE network [100] is used to learn four different non-literal language detection tasks in English and German [26]. [86] jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French. For community question answering, [52] builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time. To analyze the argumentative structure of scientific publications, [57] optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism. Considering the connection between sentence emotions and the use of the metaphor, [22] jointly trains a metaphor identification model with an emotion detection model. To ensure the consistency between generated key phrases (short text) and headlines (long text), [85] trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism. An MTL model is proposed in [111] to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools. Table 2. A summary of joint MTL studies according to types of tasks involved. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively. A single checkmark could mean joint learning of multiple tasks of the same type.

(p15.2) Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118]. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [32], Predicate-Argument Structures (PAS) [76], and Prague Semantic Dependencies (PSD) [42]) jointly. In [47], a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation (AMR) [4], Semantic Dependency Parsing (SDP) [88], and Universal Dependencies (UD) [87], and it shows that joint training improves performance on the testing UCCA dataset. [71] jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT. [29] shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks. In [143], an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.",What distinguishes joint MTL from auxiliary MTL in natural language processing?,1. What distinguishes joint MTL from auxiliary MTL in natural language processing?,"Questions:

1. What distinguishes joint MTL from auxiliary MTL in natural language processing?

Answer:

(p15.0) Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.

(p15.1) Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks. One major problem of such tasks is the lack of sufficient labeled data. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques. [152] develops a joint MTL model for the NER and entity name normalization tasks in the medical field. [68,146] use MTL to perform simile detection, which includes simile sentence classification and simile component extraction. To analyze Twitter demographic data, [121] jointly learns classification models for genders, ages, political orientations, and locations. The SLUICE network [100] is used to learn four different non-literal language detection tasks in English and German [26]. [86] jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French. For community question answering, [52] builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time. To analyze the argumentative structure of scientific publications, [57] optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism. Considering the connection between sentence emotions and the use of the metaphor, [22] jointly trains a metaphor identification model with an emotion detection model. To ensure the consistency between generated key phrases (short text) and headlines (long text), [85] trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism. An MTL model is proposed in [111] to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools. Table 2. A summary of joint MTL studies according to types of tasks involved. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively. A single checkmark could mean joint learning of multiple tasks of the same type.

(p15.2) Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118]. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [32], Predicate-Argument Structures (PAS) [76], and Prague Semantic Dependencies (PSD) [42]) jointly. In [47], a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation (AMR) [4], Semantic Dependency Parsing (SDP) [88], and Universal Dependencies (UD) [87], and it shows that joint training improves performance on the testing UCCA dataset. [71] jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT. [29] shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks. In [143], an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.",What advantages does joint MTL offer over traditional single-task learning models in NLP?,1. What advantages does joint MTL offer over traditional single-task learning models in NLP?,"Questions:

1. What advantages does joint MTL offer over traditional single-task learning models in NLP?

Answer:

(p15.0) Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.

(p15.1) Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks. One major problem of such tasks is the lack of sufficient labeled data. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques. [152] develops a joint MTL model for the NER and entity name normalization tasks in the medical field. [68,146] use MTL to perform simile detection, which includes simile sentence classification and simile component extraction. To analyze Twitter demographic data, [121] jointly learns classification models for genders, ages, political orientations, and locations. The SLUICE network [100] is used to learn four different non-literal language detection tasks in English and German [26]. [86] jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French. For community question answering, [52] builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time. To analyze the argumentative structure of scientific publications, [57] optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism. Considering the connection between sentence emotions and the use of the metaphor, [22] jointly trains a metaphor identification model with an emotion detection model. To ensure the consistency between generated key phrases (short text) and headlines (long text), [85] trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism. An MTL model is proposed in [111] to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools. Table 2. A summary of joint MTL studies according to types of tasks involved. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively. A single checkmark could mean joint learning of multiple tasks of the same type.

(p15.2) Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118]. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [32], Predicate-Argument Structures (PAS) [76], and Prague Semantic Dependencies (PSD) [42]) jointly. In [47], a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation (AMR) [4], Semantic Dependency Parsing (SDP) [88], and Universal Dependencies (UD) [87], and it shows that joint training improves performance on the testing UCCA dataset. [71] jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT. [29] shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks. In [143], an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.",What are the advantages of joint multi-task learning in NLP?,1. What are the advantages of joint multi-task learning in NLP?,"Questions:

1. What are the advantages of joint multi-task learning in NLP?

Answer:

(p15.0) Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.

(p15.1) Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks. One major problem of such tasks is the lack of sufficient labeled data. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques. [152] develops a joint MTL model for the NER and entity name normalization tasks in the medical field. [68,146] use MTL to perform simile detection, which includes simile sentence classification and simile component extraction. To analyze Twitter demographic data, [121] jointly learns classification models for genders, ages, political orientations, and locations. The SLUICE network [100] is used to learn four different non-literal language detection tasks in English and German [26]. [86] jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French. For community question answering, [52] builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time. To analyze the argumentative structure of scientific publications, [57] optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism. Considering the connection between sentence emotions and the use of the metaphor, [22] jointly trains a metaphor identification model with an emotion detection model. To ensure the consistency between generated key phrases (short text) and headlines (long text), [85] trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism. An MTL model is proposed in [111] to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools. Table 2. A summary of joint MTL studies according to types of tasks involved. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively. A single checkmark could mean joint learning of multiple tasks of the same type.

(p15.2) Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118]. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [32], Predicate-Argument Structures (PAS) [76], and Prague Semantic Dependencies (PSD) [42]) jointly. In [47], a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation (AMR) [4], Semantic Dependency Parsing (SDP) [88], and Universal Dependencies (UD) [87], and it shows that joint training improves performance on the testing UCCA dataset. [71] jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT. [29] shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks. In [143], an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s16,Multi-lingual MTL,"['p16.0', 'p16.1', 'p16.2']","['Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.', 'Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.', 'Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.']","Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.

Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.

Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.","(p16.0) Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.

(p16.1) Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.

(p16.2) Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.","[['b126', 'b78', 'b77'], ['b85', 'b118'], ['b48', 'b107', 'b64']]","[['b126', 'b78', 'b77'], ['b85', 'b118'], ['b48', 'b107', 'b64']]",8,"1. Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections.
2. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models.
3. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English.
4. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively.
5. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.
6. Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages.
7. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese.
8. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process.
9. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts.
10. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English.
11. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.
12. Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations.
13. [108] learns multi-lingual representations from two tasks.
14. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples.
15. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification.
16. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training.
17. In [78]0, a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels.
18. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way.
19. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.",How does multi-task learning enhance multi-lingual NLP models?,1. How does multi-task learning enhance multi-lingual NLP models?,"Questions:

1. How does multi-task learning enhance multi-lingual NLP models?

Answer:

(p16.0) Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.

(p16.1) Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.

(p16.2) Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.",How does multi-task learning enhance multi-lingual machine learning models in NLP?,1. How does multi-task learning enhance multi-lingual machine learning models in NLP?,"Questions:

1. How does multi-task learning enhance multi-lingual machine learning models in NLP?

Answer:

(p16.0) Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.

(p16.1) Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.

(p16.2) Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.",What are the goals and methods of multi-lingual multi-task learning in NLP?,1. What are the goals and methods of multi-lingual multi-task learning in NLP?,"Questions:

1. What are the goals and methods of multi-lingual multi-task learning in NLP?

Answer:

(p16.0) Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.

(p16.1) Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.

(p16.2) Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.",How does multi-task learning benefit multi-lingual machine learning models in NLP?,1. How does multi-task learning benefit multi-lingual machine learning models in NLP?,"Questions:

1. How does multi-task learning benefit multi-lingual machine learning models in NLP?

Answer:

(p16.0) Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.

(p16.1) Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.

(p16.2) Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.",What are the primary goals of multi-lingual multi-task learning in NLP?,1. What are the primary goals of multi-lingual multi-task learning in NLP?,"Questions:

1. What are the primary goals of multi-lingual multi-task learning in NLP?

Answer:

(p16.0) Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.

(p16.1) Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.

(p16.2) Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s17,Multimodal MTL,"['p17.0', 'p17.1', 'p17.2', 'p17.3']","['As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.', '[89] builds a video captioning model with two auxiliary tasks. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.', 'Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.', 'In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.']","As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.

[89] builds a video captioning model with two auxiliary tasks. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.

Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.

In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.","(p17.0) As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.

(p17.1) [89] builds a video captioning model with two auxiliary tasks. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.

(p17.2) Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.

(p17.3) In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.","[['b15'], ['b11'], ['b79', 'b10'], ['b114']]","[['b15'], ['b11'], ['b79', 'b10'], ['b114']]",5,"1. As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community.
2. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks.
3. MTL is a natural choice for implicitly injecting multimodal features into a single model.
4. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language.
5. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.
6. [89] builds a video captioning model with two auxiliary tasks.
7. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability.
8. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism.
9. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.
10. Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding.
11. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance.
12. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.
13. In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment.
14. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels.
15. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning.
16. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.",How does multimodal multi-task learning enhance NLP models' performance across different tasks?,1. How does multimodal multi-task learning enhance NLP models' performance across different tasks?,"Questions:

1. How does multimodal multi-task learning enhance NLP models' performance across different tasks?

Answer:

(p17.0) As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.

(p17.1) [89] builds a video captioning model with two auxiliary tasks. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.

(p17.2) Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.

(p17.3) In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.",How does multimodal multi-task learning enhance performance in cross-modal NLP tasks?,1. How does multimodal multi-task learning enhance performance in cross-modal NLP tasks?,"Questions:

1. How does multimodal multi-task learning enhance performance in cross-modal NLP tasks?

Answer:

(p17.0) As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.

(p17.1) [89] builds a video captioning model with two auxiliary tasks. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.

(p17.2) Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.

(p17.3) In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.","How does multimodal multi-task learning enhance NLP systems, particularly in video captioning and interactive agents?","1. How does multimodal multi-task learning enhance NLP systems, particularly in video captioning and interactive agents?","Questions:

1. How does multimodal multi-task learning enhance NLP systems, particularly in video captioning and interactive agents?

Answer:

(p17.0) As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.

(p17.1) [89] builds a video captioning model with two auxiliary tasks. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.

(p17.2) Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.

(p17.3) In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.",How does multimodal multi-task learning enhance NLP systems?,1. How does multimodal multi-task learning enhance NLP systems?,"Questions:

1. How does multimodal multi-task learning enhance NLP systems?

Answer:

(p17.0) As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.

(p17.1) [89] builds a video captioning model with two auxiliary tasks. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.

(p17.2) Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.

(p17.3) In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.",How does multimodal multi-task learning enhance NLP systems?,1. How does multimodal multi-task learning enhance NLP systems?,"Questions:

1. How does multimodal multi-task learning enhance NLP systems?

Answer:

(p17.0) As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.

(p17.1) [89] builds a video captioning model with two auxiliary tasks. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.

(p17.2) Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.

(p17.3) In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s18,Task Relatedness in MTL,"['p18.0', 'p18.1']","['A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.', 'As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.']","A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.

As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.","(p18.0) A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.

(p18.1) As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.","[['b52', 'b70', 'b76', 'b103'], ['b21', 'b53', 'b48', 'b9', 'b90']]","[['b52', 'b70', 'b76', 'b103'], ['b21', 'b53', 'b48', 'b9', 'b90']]",9,"1. A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training.
2. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks.
3. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy.
4. This finding also holds for rumor verification [53].
5. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other.
6. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels.
7. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets.
8. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.
9. As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness.
10. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks.
11. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted.
12. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar.
13. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks.
14. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.",What criteria determine the effectiveness of task selection for multi-task learning in NLP?,1. What criteria determine the effectiveness of task selection for multi-task learning in NLP?,"Questions:

1. What criteria determine the effectiveness of task selection for multi-task learning in NLP?

Answer:

(p18.0) A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.

(p18.1) As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.",What criteria determine the effectiveness of task selection for multi-task learning in NLP?,1. What criteria determine the effectiveness of task selection for multi-task learning in NLP?,"Questions:

1. What criteria determine the effectiveness of task selection for multi-task learning in NLP?

Answer:

(p18.0) A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.

(p18.1) As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.",How does task relatedness impact multi-task learning performance in NLP?,1. How does task relatedness impact multi-task learning performance in NLP?,"Questions:

1. How does task relatedness impact multi-task learning performance in NLP?

Answer:

(p18.0) A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.

(p18.1) As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.",What criteria determine task relatedness for effective multi-task learning in NLP?,1. What criteria determine task relatedness for effective multi-task learning in NLP?,"Questions:

1. What criteria determine task relatedness for effective multi-task learning in NLP?

Answer:

(p18.0) A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.

(p18.1) As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.",How does task relatedness affect multi-task learning performance in NLP?,1. How does task relatedness affect multi-task learning performance in NLP?,"Questions:

1. How does task relatedness affect multi-task learning performance in NLP?

Answer:

(p18.0) A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.

(p18.1) As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s20,Data Source,"['p20.0', 'p20.1']","['Given tasks with corresponding datasets D = {X , Y }, = 1, . . . , , where X denotes the set of data instances in task and Y denotes the corresponding labels, we denote the entire dataset for the tasks by D = {X, Y}. We describe different forms of D in the following sections.', '5.1.1 Disjoint Datasets. In most multi-task learning literature, the training sets of different tasks have distinct label spaces, i.e. ∀ ≠ , Y ∩ Y = ∅. In this case, D = {D 1 , . . . , D } The most popular way to train MTL models on such tasks is to alternate between different tasks [5,20,27,30,40,43,69,71,73,79,89,96,109,136,153], either randomly or by a pre-defined order. Thus the model handles data from different datasets in turns as discussed in Section 3.']","Given tasks with corresponding datasets D = {X , Y }, = 1, . . . , , where X denotes the set of data instances in task and Y denotes the corresponding labels, we denote the entire dataset for the tasks by D = {X, Y}. We describe different forms of D in the following sections.

5.1.1 Disjoint Datasets. In most multi-task learning literature, the training sets of different tasks have distinct label spaces, i.e. ∀ ≠ , Y ∩ Y = ∅. In this case, D = {D 1 , . . . , D } The most popular way to train MTL models on such tasks is to alternate between different tasks [5,20,27,30,40,43,69,71,73,79,89,96,109,136,153], either randomly or by a pre-defined order. Thus the model handles data from different datasets in turns as discussed in Section 3.","(p20.0) Given tasks with corresponding datasets D = {X , Y }, = 1, . . . , , where X denotes the set of data instances in task and Y denotes the corresponding labels, we denote the entire dataset for the tasks by D = {X, Y}. We describe different forms of D in the following sections.

(p20.1) 5.1.1 Disjoint Datasets. In most multi-task learning literature, the training sets of different tasks have distinct label spaces, i.e. ∀ ≠ , Y ∩ Y = ∅. In this case, D = {D 1 , . . . , D } The most popular way to train MTL models on such tasks is to alternate between different tasks [5,20,27,30,40,43,69,71,73,79,89,96,109,136,153], either randomly or by a pre-defined order. Thus the model handles data from different datasets in turns as discussed in Section 3.","[[], ['b95', 'b39', 'b152', 'b72', 'b135', 'b108', 'b68', 'b4', 'b78', 'b29', 'b42', 'b19', 'b70', 'b88', 'b26']]","[[], ['b95', 'b39', 'b152', 'b72', 'b135', 'b108', 'b68', 'b4', 'b78', 'b29', 'b42', 'b19', 'b70', 'b88', 'b26']]",15,"1. Given tasks with corresponding datasets D = {X , Y }, = 1, . . . , , where X denotes the set of data instances in task and Y denotes the corresponding labels, we denote the entire dataset for the tasks by D = {X, Y}.
2. We describe different forms of D in the following sections.
3. 5.1.1 Disjoint Datasets. In most multi-task learning literature, the training sets of different tasks have distinct label spaces, i.e. ∀ ≠ , Y ∩ Y = ∅. In this case, D = {D 1 , . . . , D } The most popular way to train MTL models on such tasks is to alternate between different tasks [5,20,27,30,40,43,69,71,73,79,89,96,109,136,153], either randomly or by a pre-defined order.
4. Thus the model handles data from different datasets in turns as discussed in Section 3.",How do multi-task learning models handle training on disjoint datasets?,1. How do multi-task learning models handle training on disjoint datasets?,"Questions:

1. How do multi-task learning models handle training on disjoint datasets?

Answer:

(p20.0) Given tasks with corresponding datasets D = {X , Y }, = 1, . . . , , where X denotes the set of data instances in task and Y denotes the corresponding labels, we denote the entire dataset for the tasks by D = {X, Y}. We describe different forms of D in the following sections.

(p20.1) 5.1.1 Disjoint Datasets. In most multi-task learning literature, the training sets of different tasks have distinct label spaces, i.e. ∀ ≠ , Y ∩ Y = ∅. In this case, D = {D 1 , . . . , D } The most popular way to train MTL models on such tasks is to alternate between different tasks [5,20,27,30,40,43,69,71,73,79,89,96,109,136,153], either randomly or by a pre-defined order. Thus the model handles data from different datasets in turns as discussed in Section 3.",How is the dataset structured in multi-task learning for natural language processing?,1. How is the dataset structured in multi-task learning for natural language processing?,"Questions:

1. How is the dataset structured in multi-task learning for natural language processing?

Answer:

(p20.0) Given tasks with corresponding datasets D = {X , Y }, = 1, . . . , , where X denotes the set of data instances in task and Y denotes the corresponding labels, we denote the entire dataset for the tasks by D = {X, Y}. We describe different forms of D in the following sections.

(p20.1) 5.1.1 Disjoint Datasets. In most multi-task learning literature, the training sets of different tasks have distinct label spaces, i.e. ∀ ≠ , Y ∩ Y = ∅. In this case, D = {D 1 , . . . , D } The most popular way to train MTL models on such tasks is to alternate between different tasks [5,20,27,30,40,43,69,71,73,79,89,96,109,136,153], either randomly or by a pre-defined order. Thus the model handles data from different datasets in turns as discussed in Section 3.",How do multi-task learning models handle training on datasets with disjoint label spaces?,1. How do multi-task learning models handle training on datasets with disjoint label spaces?,"Questions:

1. How do multi-task learning models handle training on datasets with disjoint label spaces?

Answer:

(p20.0) Given tasks with corresponding datasets D = {X , Y }, = 1, . . . , , where X denotes the set of data instances in task and Y denotes the corresponding labels, we denote the entire dataset for the tasks by D = {X, Y}. We describe different forms of D in the following sections.

(p20.1) 5.1.1 Disjoint Datasets. In most multi-task learning literature, the training sets of different tasks have distinct label spaces, i.e. ∀ ≠ , Y ∩ Y = ∅. In this case, D = {D 1 , . . . , D } The most popular way to train MTL models on such tasks is to alternate between different tasks [5,20,27,30,40,43,69,71,73,79,89,96,109,136,153], either randomly or by a pre-defined order. Thus the model handles data from different datasets in turns as discussed in Section 3.",What are the different forms of datasets used in multi-task learning for NLP?,1. What are the different forms of datasets used in multi-task learning for NLP?,"Questions:

1. What are the different forms of datasets used in multi-task learning for NLP?

Answer:

(p20.0) Given tasks with corresponding datasets D = {X , Y }, = 1, . . . , , where X denotes the set of data instances in task and Y denotes the corresponding labels, we denote the entire dataset for the tasks by D = {X, Y}. We describe different forms of D in the following sections.

(p20.1) 5.1.1 Disjoint Datasets. In most multi-task learning literature, the training sets of different tasks have distinct label spaces, i.e. ∀ ≠ , Y ∩ Y = ∅. In this case, D = {D 1 , . . . , D } The most popular way to train MTL models on such tasks is to alternate between different tasks [5,20,27,30,40,43,69,71,73,79,89,96,109,136,153], either randomly or by a pre-defined order. Thus the model handles data from different datasets in turns as discussed in Section 3.",How is multi-task learning implemented with disjoint datasets in NLP?,1. How is multi-task learning implemented with disjoint datasets in NLP?,"Questions:

1. How is multi-task learning implemented with disjoint datasets in NLP?

Answer:

(p20.0) Given tasks with corresponding datasets D = {X , Y }, = 1, . . . , , where X denotes the set of data instances in task and Y denotes the corresponding labels, we denote the entire dataset for the tasks by D = {X, Y}. We describe different forms of D in the following sections.

(p20.1) 5.1.1 Disjoint Datasets. In most multi-task learning literature, the training sets of different tasks have distinct label spaces, i.e. ∀ ≠ , Y ∩ Y = ∅. In this case, D = {D 1 , . . . , D } The most popular way to train MTL models on such tasks is to alternate between different tasks [5,20,27,30,40,43,69,71,73,79,89,96,109,136,153], either randomly or by a pre-defined order. Thus the model handles data from different datasets in turns as discussed in Section 3."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s21,Multi-label Datasets.,"['p21.0', 'p21.1', 'p21.2']","['Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time. In this case,', 'Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.', 'The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa.']","Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time. In this case,

Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.

The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa.","(p21.0) Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time. In this case,

(p21.1) Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.

(p21.2) The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa.","[[], ['b53', 'b120', 'b90', 'b28'], ['b55', 'b61', 'b96', 'b48', 'b23', 'b32', 'b63', 'b124', 'b129', 'b85', 'b106', 'b30', 'b115', 'b125']]","[[], ['b53', 'b120', 'b90', 'b28'], ['b55', 'b61', 'b96', 'b48', 'b23', 'b32', 'b63', 'b124', 'b129', 'b85', 'b106', 'b30', 'b115', 'b125']]",18,"1. Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time.
2. In this case,Multi-label datasets can be created by giving extra manual annotations to existing data.
3. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input.
4. [121] labels Twitter posts with 4 demographic labels.
5. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.
6. The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset.
7. Extra labels can be obtained using pre-defined rules [62,97].
8. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels.
9. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation.
10. [116] uses hashtags to represent genres of tweet posts.
11. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database.
12. Unicoder [49] uses translated text from the source language to fine-tune on the target language.
13. [121]0 creates disfluent sentences by randomly repeating or inserting -grams.
14. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models.
15. [121]1 obtains dominant word sense labels from WordNet [121]2.
16. [121]3 applies entity linking for QA data over databases through an entity linker.
17. [121]4 assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method.
18. [121]5 uses the output of a meta-network as labels for unsupervised training data.
19. As a special case of multi-label dataset, mask orchestration [121]6 provides different parts of an instance to different tasks by applying different masks.
20. That is, labels for one task may become the input for another task and vice versa.",How are multi-label datasets constructed and utilized in multi-task learning for NLP?,1. How are multi-label datasets constructed and utilized in multi-task learning for NLP?,"Questions:

1. How are multi-label datasets constructed and utilized in multi-task learning for NLP?

Answer:

(p21.0) Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time. In this case,

(p21.1) Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.

(p21.2) The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa.",How do multi-label datasets facilitate simultaneous optimization of task-specific components in NLP?,1. How do multi-label datasets facilitate simultaneous optimization of task-specific components in NLP?,"Questions:

1. How do multi-label datasets facilitate simultaneous optimization of task-specific components in NLP?

Answer:

(p21.0) Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time. In this case,

(p21.1) Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.

(p21.2) The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa.",How are multi-label datasets constructed and utilized in multi-task learning for NLP?,1. How are multi-label datasets constructed and utilized in multi-task learning for NLP?,"Questions:

1. How are multi-label datasets constructed and utilized in multi-task learning for NLP?

Answer:

(p21.0) Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time. In this case,

(p21.1) Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.

(p21.2) The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa.",How do multi-label datasets facilitate simultaneous optimization of task-specific components in NLP?,1. How do multi-label datasets facilitate simultaneous optimization of task-specific components in NLP?,"Questions:

1. How do multi-label datasets facilitate simultaneous optimization of task-specific components in NLP?

Answer:

(p21.0) Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time. In this case,

(p21.1) Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.

(p21.2) The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa.",How are multi-label datasets constructed and utilized in multi-task learning?,1. How are multi-label datasets constructed and utilized in multi-task learning?,"Questions:

1. How are multi-label datasets constructed and utilized in multi-task learning?

Answer:

(p21.0) Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time. In this case,

(p21.1) Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.

(p21.2) The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s22,Multi-task Benchmark Datasets,"['p22.0', 'p22.1']","['As summarized in Table 3, we list a few public multi-task benchmark datasets for NLP tasks.', '• GLUE [123] is a benchmark dataset for evaluating natural language understanding (NLU) models. The main benchmark consists of 8 sentence and sentence-pair classification tasks as well as a regression task. The tasks cover a diverse range of genres, dataset sizes, and difficulties. Besides, a diagnostic dataset is provided to evaluate the ability of NLU models on capturing a pre-defined set of language phenomena. • SuperGLUE [122] is a generalization of GLUE. As the performance of state-of-the-art models has exceeded non-expert human baselines on GLUE, SuperGLUE contains a set of 8 more challenging NLU tasks along with comprehensive human baselines. Besides retaining the two hardest tasks in GLUE, 6 tasks are added with two new question formats: coreference resolution and question answering (QA).  [46] is a multi-task few-shot learning dataset for world knowledge and problem solving abilities of language processing models. This dataset covers 57 subjects including 19 in STEM, 13 in humanities, 12 in social sciences, and 13 in other subjects. This dataset is split into a few-shot development set that has 5 questions for each subject, a validation set for tuning hyper-parameters containing 1540 questions, and a test set with 14079 questions. • Xtreme [48] is a multi-task benchmark dataset for evaluating cross-lingual generalization capabilities of multi-lingual representations covering 9 tasks in 40 languages. The tasks include 2 classification tasks, 2 structure prediction tasks, 3 question answering tasks, and 2 sentence retrieval tasks. Out of the 40 languages involved, 19 languages appear in at least 3 datasets and the rest 21 languages appear in at least one dataset. • XGLUE [63] is a benchmark dataset that supports the development and evaluation of large cross-lingual pre-trained language models. The XGLUE dataset includes 11 downstream tasks, including 3 single-input understanding tasks, 6 pair-input understanding tasks, and 2 generation tasks. The pre-training corpus consists of a small corpus that includes a 101G multi-lingual corpus covering 100 languages and a 146G bilingual corpus covering 27 languages, and a large corpus with 2,500G multi-lingual data covering 89 languages. • LSParD [105] is a multi-task semantic parsing dataset with 3 tasks, including question type classification, entity mention detection, and question semantic parsing. Each logical form is associated with a question and multiple human annotated paraphrases. This dataset contains 51,164 questions in 9 categories, 3361 logical form patterns, and 23,144 entities. • ECSA [33] is a dataset for slot filling, named entity recognition, and segmentation to evaluate online shopping assistant systems in Chinese. The training part contains 24,892 pairs of input utterances and their corresponding slot labels, named entity labels, and segment labels. The testing part includes 2,723 such pairs with an Out-of-Vocabulary (OOV) rate of 85.3%, which is much higher than the ATIS dataset [45] whose OOV is smaller than 1%. • ABC [34], the Anti-reflexive Bias Challenge, is a multi-task benchmark dataset designed for evaluating gender assumptions in NLP models. ABC consists of 4 tasks, including language modeling, natural language inference (NLI), coreference resolution, and machine translation. A total of 4,560 samples are collected by a template-based method. The language modeling task is to predict the pronoun of a sentence. For NLI and coreference resolution, three variations of each sentence are used to construct entailment pairs. For machine translation, sentences with two variations of third-person pronouns in English are used as source sentences. • CompGuessWhat?! [115] is a dataset for grounded language learning with 65,700 collected dialogues. It is an instance of the Grounded Language Learning with Attributes (GROLLA) framework. The evaluation process includes three parts: goal-oriented evaluation (e.g., Visual QA and Visual NLI), object attribute prediction, and zero-shot evaluation. • SCIERC [72] is a multi-label dataset for identifying entities, relations, and cross-sentence coreference clusters from abstracts of research papers. SCIERC contains 500 scientific abstracts collected from proceedings in 12 conferences and workshops in artificial intelligence.']","As summarized in Table 3, we list a few public multi-task benchmark datasets for NLP tasks.

• GLUE [123] is a benchmark dataset for evaluating natural language understanding (NLU) models. The main benchmark consists of 8 sentence and sentence-pair classification tasks as well as a regression task. The tasks cover a diverse range of genres, dataset sizes, and difficulties. Besides, a diagnostic dataset is provided to evaluate the ability of NLU models on capturing a pre-defined set of language phenomena. • SuperGLUE [122] is a generalization of GLUE. As the performance of state-of-the-art models has exceeded non-expert human baselines on GLUE, SuperGLUE contains a set of 8 more challenging NLU tasks along with comprehensive human baselines. Besides retaining the two hardest tasks in GLUE, 6 tasks are added with two new question formats: coreference resolution and question answering (QA).  [46] is a multi-task few-shot learning dataset for world knowledge and problem solving abilities of language processing models. This dataset covers 57 subjects including 19 in STEM, 13 in humanities, 12 in social sciences, and 13 in other subjects. This dataset is split into a few-shot development set that has 5 questions for each subject, a validation set for tuning hyper-parameters containing 1540 questions, and a test set with 14079 questions. • Xtreme [48] is a multi-task benchmark dataset for evaluating cross-lingual generalization capabilities of multi-lingual representations covering 9 tasks in 40 languages. The tasks include 2 classification tasks, 2 structure prediction tasks, 3 question answering tasks, and 2 sentence retrieval tasks. Out of the 40 languages involved, 19 languages appear in at least 3 datasets and the rest 21 languages appear in at least one dataset. • XGLUE [63] is a benchmark dataset that supports the development and evaluation of large cross-lingual pre-trained language models. The XGLUE dataset includes 11 downstream tasks, including 3 single-input understanding tasks, 6 pair-input understanding tasks, and 2 generation tasks. The pre-training corpus consists of a small corpus that includes a 101G multi-lingual corpus covering 100 languages and a 146G bilingual corpus covering 27 languages, and a large corpus with 2,500G multi-lingual data covering 89 languages. • LSParD [105] is a multi-task semantic parsing dataset with 3 tasks, including question type classification, entity mention detection, and question semantic parsing. Each logical form is associated with a question and multiple human annotated paraphrases. This dataset contains 51,164 questions in 9 categories, 3361 logical form patterns, and 23,144 entities. • ECSA [33] is a dataset for slot filling, named entity recognition, and segmentation to evaluate online shopping assistant systems in Chinese. The training part contains 24,892 pairs of input utterances and their corresponding slot labels, named entity labels, and segment labels. The testing part includes 2,723 such pairs with an Out-of-Vocabulary (OOV) rate of 85.3%, which is much higher than the ATIS dataset [45] whose OOV is smaller than 1%. • ABC [34], the Anti-reflexive Bias Challenge, is a multi-task benchmark dataset designed for evaluating gender assumptions in NLP models. ABC consists of 4 tasks, including language modeling, natural language inference (NLI), coreference resolution, and machine translation. A total of 4,560 samples are collected by a template-based method. The language modeling task is to predict the pronoun of a sentence. For NLI and coreference resolution, three variations of each sentence are used to construct entailment pairs. For machine translation, sentences with two variations of third-person pronouns in English are used as source sentences. • CompGuessWhat?! [115] is a dataset for grounded language learning with 65,700 collected dialogues. It is an instance of the Grounded Language Learning with Attributes (GROLLA) framework. The evaluation process includes three parts: goal-oriented evaluation (e.g., Visual QA and Visual NLI), object attribute prediction, and zero-shot evaluation. • SCIERC [72] is a multi-label dataset for identifying entities, relations, and cross-sentence coreference clusters from abstracts of research papers. SCIERC contains 500 scientific abstracts collected from proceedings in 12 conferences and workshops in artificial intelligence.","(p22.0) As summarized in Table 3, we list a few public multi-task benchmark datasets for NLP tasks.

(p22.1) • GLUE [123] is a benchmark dataset for evaluating natural language understanding (NLU) models. The main benchmark consists of 8 sentence and sentence-pair classification tasks as well as a regression task. The tasks cover a diverse range of genres, dataset sizes, and difficulties. Besides, a diagnostic dataset is provided to evaluate the ability of NLU models on capturing a pre-defined set of language phenomena. • SuperGLUE [122] is a generalization of GLUE. As the performance of state-of-the-art models has exceeded non-expert human baselines on GLUE, SuperGLUE contains a set of 8 more challenging NLU tasks along with comprehensive human baselines. Besides retaining the two hardest tasks in GLUE, 6 tasks are added with two new question formats: coreference resolution and question answering (QA).  [46] is a multi-task few-shot learning dataset for world knowledge and problem solving abilities of language processing models. This dataset covers 57 subjects including 19 in STEM, 13 in humanities, 12 in social sciences, and 13 in other subjects. This dataset is split into a few-shot development set that has 5 questions for each subject, a validation set for tuning hyper-parameters containing 1540 questions, and a test set with 14079 questions. • Xtreme [48] is a multi-task benchmark dataset for evaluating cross-lingual generalization capabilities of multi-lingual representations covering 9 tasks in 40 languages. The tasks include 2 classification tasks, 2 structure prediction tasks, 3 question answering tasks, and 2 sentence retrieval tasks. Out of the 40 languages involved, 19 languages appear in at least 3 datasets and the rest 21 languages appear in at least one dataset. • XGLUE [63] is a benchmark dataset that supports the development and evaluation of large cross-lingual pre-trained language models. The XGLUE dataset includes 11 downstream tasks, including 3 single-input understanding tasks, 6 pair-input understanding tasks, and 2 generation tasks. The pre-training corpus consists of a small corpus that includes a 101G multi-lingual corpus covering 100 languages and a 146G bilingual corpus covering 27 languages, and a large corpus with 2,500G multi-lingual data covering 89 languages. • LSParD [105] is a multi-task semantic parsing dataset with 3 tasks, including question type classification, entity mention detection, and question semantic parsing. Each logical form is associated with a question and multiple human annotated paraphrases. This dataset contains 51,164 questions in 9 categories, 3361 logical form patterns, and 23,144 entities. • ECSA [33] is a dataset for slot filling, named entity recognition, and segmentation to evaluate online shopping assistant systems in Chinese. The training part contains 24,892 pairs of input utterances and their corresponding slot labels, named entity labels, and segment labels. The testing part includes 2,723 such pairs with an Out-of-Vocabulary (OOV) rate of 85.3%, which is much higher than the ATIS dataset [45] whose OOV is smaller than 1%. • ABC [34], the Anti-reflexive Bias Challenge, is a multi-task benchmark dataset designed for evaluating gender assumptions in NLP models. ABC consists of 4 tasks, including language modeling, natural language inference (NLI), coreference resolution, and machine translation. A total of 4,560 samples are collected by a template-based method. The language modeling task is to predict the pronoun of a sentence. For NLI and coreference resolution, three variations of each sentence are used to construct entailment pairs. For machine translation, sentences with two variations of third-person pronouns in English are used as source sentences. • CompGuessWhat?! [115] is a dataset for grounded language learning with 65,700 collected dialogues. It is an instance of the Grounded Language Learning with Attributes (GROLLA) framework. The evaluation process includes three parts: goal-oriented evaluation (e.g., Visual QA and Visual NLI), object attribute prediction, and zero-shot evaluation. • SCIERC [72] is a multi-label dataset for identifying entities, relations, and cross-sentence coreference clusters from abstracts of research papers. SCIERC contains 500 scientific abstracts collected from proceedings in 12 conferences and workshops in artificial intelligence.","[[], ['b32', 'b62', 'b121', 'b45', 'b44', 'b114', 'b122', 'b71', 'b33']]","[[], ['b32', 'b62', 'b121', 'b45', 'b44', 'b114', 'b122', 'b71', 'b33']]",9,"1. As summarized in Table 3, we list a few public multi-task benchmark datasets for NLP tasks.
2. GLUE [123] is a benchmark dataset for evaluating natural language understanding (NLU) models.
3. The main benchmark consists of 8 sentence and sentence-pair classification tasks as well as a regression task.
4. The tasks cover a diverse range of genres, dataset sizes, and difficulties.
5. Besides, a diagnostic dataset is provided to evaluate the ability of NLU models on capturing a pre-defined set of language phenomena.
6. SuperGLUE [122] is a generalization of GLUE.
7. As the performance of state-of-the-art models has exceeded non-expert human baselines on GLUE, SuperGLUE contains a set of 8 more challenging NLU tasks along with comprehensive human baselines.
8. Besides retaining the two hardest tasks in GLUE, 6 tasks are added with two new question formats: coreference resolution and question answering (QA).
9. [46] is a multi-task few-shot learning dataset for world knowledge and problem solving abilities of language processing models.
10. This dataset covers 57 subjects including 19 in STEM, 13 in humanities, 12 in social sciences, and 13 in other subjects.
11. This dataset is split into a few-shot development set that has 5 questions for each subject, a validation set for tuning hyper-parameters containing 1540 questions, and a test set with 14079 questions.
12. Xtreme [48] is a multi-task benchmark dataset for evaluating cross-lingual generalization capabilities of multi-lingual representations covering 9 tasks in 40 languages.
13. The tasks include 2 classification tasks, 2 structure prediction tasks, 3 question answering tasks, and 2 sentence retrieval tasks.
14. Out of the 40 languages involved, 19 languages appear in at least 3 datasets and the rest 21 languages appear in at least one dataset.
15. XGLUE [63] is a benchmark dataset that supports the development and evaluation of large cross-lingual pre-trained language models.
16. The XGLUE dataset includes 11 downstream tasks, including 3 single-input understanding tasks, 6 pair-input understanding tasks, and 2 generation tasks.
17. The pre-training corpus consists of a small corpus that includes a 101G multi-lingual corpus covering 100 languages and a 146G bilingual corpus covering 27 languages, and a large corpus with 2,500G multi-lingual data covering 89 languages.
18. LSParD [105] is a multi-task semantic parsing dataset with 3 tasks, including question type classification, entity mention detection, and question semantic parsing.
19. Each logical form is associated with a question and multiple human annotated paraphrases.
20. This dataset contains 51,164 questions in 9 categories, 3361 logical form patterns, and 23,144 entities.
21. ECSA [33] is a dataset for slot filling, named entity recognition, and segmentation to evaluate online shopping assistant systems in Chinese.
22. The training part contains 24,892 pairs of input utterances and their corresponding slot labels, named entity labels, and segment labels.
23. The testing part includes 2,723 such pairs with an Out-of-Vocabulary (OOV) rate of 85.3%, which is much higher than the ATIS dataset (NLU)0 whose OOV is smaller than 1%.
24. ABC (NLU)1, the Anti-reflexive Bias Challenge, is a multi-task benchmark dataset designed for evaluating gender assumptions in NLP models.
25. ABC consists of 4 tasks, including language modeling, natural language inference (NLU)2, coreference resolution, and machine translation.
26. A total of 4,560 samples are collected by a template-based method.
27. The language modeling task is to predict the pronoun of a sentence.
28. For NLI and coreference resolution, three variations of each sentence are used to construct entailment pairs.
29. For machine translation, sentences with two variations of third-person pronouns in English are used as source sentences.
30. CompGuessWhat?! (NLU)3 is a dataset for grounded language learning with 65,700 collected dialogues.
31. It is an instance of the Grounded Language Learning with Attributes (NLU)4 framework.
32. The evaluation process includes three parts: goal-oriented evaluation (NLU)5, object attribute prediction, and zero-shot evaluation.
33. SCIERC (NLU)6 is a multi-label dataset for identifying entities, relations, and cross-sentence coreference clusters from abstracts of research papers.
34. SCIERC contains 500 scientific abstracts collected from proceedings in 12 conferences and workshops in artificial intelligence.",What are the characteristics and purposes of various multi-task benchmark datasets in NLP?,1. What are the characteristics and purposes of various multi-task benchmark datasets in NLP?,"Questions:

1. What are the characteristics and purposes of various multi-task benchmark datasets in NLP?

Answer:

(p22.0) As summarized in Table 3, we list a few public multi-task benchmark datasets for NLP tasks.

(p22.1) • GLUE [123] is a benchmark dataset for evaluating natural language understanding (NLU) models. The main benchmark consists of 8 sentence and sentence-pair classification tasks as well as a regression task. The tasks cover a diverse range of genres, dataset sizes, and difficulties. Besides, a diagnostic dataset is provided to evaluate the ability of NLU models on capturing a pre-defined set of language phenomena. • SuperGLUE [122] is a generalization of GLUE. As the performance of state-of-the-art models has exceeded non-expert human baselines on GLUE, SuperGLUE contains a set of 8 more challenging NLU tasks along with comprehensive human baselines. Besides retaining the two hardest tasks in GLUE, 6 tasks are added with two new question formats: coreference resolution and question answering (QA).  [46] is a multi-task few-shot learning dataset for world knowledge and problem solving abilities of language processing models. This dataset covers 57 subjects including 19 in STEM, 13 in humanities, 12 in social sciences, and 13 in other subjects. This dataset is split into a few-shot development set that has 5 questions for each subject, a validation set for tuning hyper-parameters containing 1540 questions, and a test set with 14079 questions. • Xtreme [48] is a multi-task benchmark dataset for evaluating cross-lingual generalization capabilities of multi-lingual representations covering 9 tasks in 40 languages. The tasks include 2 classification tasks, 2 structure prediction tasks, 3 question answering tasks, and 2 sentence retrieval tasks. Out of the 40 languages involved, 19 languages appear in at least 3 datasets and the rest 21 languages appear in at least one dataset. • XGLUE [63] is a benchmark dataset that supports the development and evaluation of large cross-lingual pre-trained language models. The XGLUE dataset includes 11 downstream tasks, including 3 single-input understanding tasks, 6 pair-input understanding tasks, and 2 generation tasks. The pre-training corpus consists of a small corpus that includes a 101G multi-lingual corpus covering 100 languages and a 146G bilingual corpus covering 27 languages, and a large corpus with 2,500G multi-lingual data covering 89 languages. • LSParD [105] is a multi-task semantic parsing dataset with 3 tasks, including question type classification, entity mention detection, and question semantic parsing. Each logical form is associated with a question and multiple human annotated paraphrases. This dataset contains 51,164 questions in 9 categories, 3361 logical form patterns, and 23,144 entities. • ECSA [33] is a dataset for slot filling, named entity recognition, and segmentation to evaluate online shopping assistant systems in Chinese. The training part contains 24,892 pairs of input utterances and their corresponding slot labels, named entity labels, and segment labels. The testing part includes 2,723 such pairs with an Out-of-Vocabulary (OOV) rate of 85.3%, which is much higher than the ATIS dataset [45] whose OOV is smaller than 1%. • ABC [34], the Anti-reflexive Bias Challenge, is a multi-task benchmark dataset designed for evaluating gender assumptions in NLP models. ABC consists of 4 tasks, including language modeling, natural language inference (NLI), coreference resolution, and machine translation. A total of 4,560 samples are collected by a template-based method. The language modeling task is to predict the pronoun of a sentence. For NLI and coreference resolution, three variations of each sentence are used to construct entailment pairs. For machine translation, sentences with two variations of third-person pronouns in English are used as source sentences. • CompGuessWhat?! [115] is a dataset for grounded language learning with 65,700 collected dialogues. It is an instance of the Grounded Language Learning with Attributes (GROLLA) framework. The evaluation process includes three parts: goal-oriented evaluation (e.g., Visual QA and Visual NLI), object attribute prediction, and zero-shot evaluation. • SCIERC [72] is a multi-label dataset for identifying entities, relations, and cross-sentence coreference clusters from abstracts of research papers. SCIERC contains 500 scientific abstracts collected from proceedings in 12 conferences and workshops in artificial intelligence.",What are some public multi-task benchmark datasets for NLP tasks?,1. What are some public multi-task benchmark datasets for NLP tasks?,"Questions:

1. What are some public multi-task benchmark datasets for NLP tasks?

Answer:

(p22.0) As summarized in Table 3, we list a few public multi-task benchmark datasets for NLP tasks.

(p22.1) • GLUE [123] is a benchmark dataset for evaluating natural language understanding (NLU) models. The main benchmark consists of 8 sentence and sentence-pair classification tasks as well as a regression task. The tasks cover a diverse range of genres, dataset sizes, and difficulties. Besides, a diagnostic dataset is provided to evaluate the ability of NLU models on capturing a pre-defined set of language phenomena. • SuperGLUE [122] is a generalization of GLUE. As the performance of state-of-the-art models has exceeded non-expert human baselines on GLUE, SuperGLUE contains a set of 8 more challenging NLU tasks along with comprehensive human baselines. Besides retaining the two hardest tasks in GLUE, 6 tasks are added with two new question formats: coreference resolution and question answering (QA).  [46] is a multi-task few-shot learning dataset for world knowledge and problem solving abilities of language processing models. This dataset covers 57 subjects including 19 in STEM, 13 in humanities, 12 in social sciences, and 13 in other subjects. This dataset is split into a few-shot development set that has 5 questions for each subject, a validation set for tuning hyper-parameters containing 1540 questions, and a test set with 14079 questions. • Xtreme [48] is a multi-task benchmark dataset for evaluating cross-lingual generalization capabilities of multi-lingual representations covering 9 tasks in 40 languages. The tasks include 2 classification tasks, 2 structure prediction tasks, 3 question answering tasks, and 2 sentence retrieval tasks. Out of the 40 languages involved, 19 languages appear in at least 3 datasets and the rest 21 languages appear in at least one dataset. • XGLUE [63] is a benchmark dataset that supports the development and evaluation of large cross-lingual pre-trained language models. The XGLUE dataset includes 11 downstream tasks, including 3 single-input understanding tasks, 6 pair-input understanding tasks, and 2 generation tasks. The pre-training corpus consists of a small corpus that includes a 101G multi-lingual corpus covering 100 languages and a 146G bilingual corpus covering 27 languages, and a large corpus with 2,500G multi-lingual data covering 89 languages. • LSParD [105] is a multi-task semantic parsing dataset with 3 tasks, including question type classification, entity mention detection, and question semantic parsing. Each logical form is associated with a question and multiple human annotated paraphrases. This dataset contains 51,164 questions in 9 categories, 3361 logical form patterns, and 23,144 entities. • ECSA [33] is a dataset for slot filling, named entity recognition, and segmentation to evaluate online shopping assistant systems in Chinese. The training part contains 24,892 pairs of input utterances and their corresponding slot labels, named entity labels, and segment labels. The testing part includes 2,723 such pairs with an Out-of-Vocabulary (OOV) rate of 85.3%, which is much higher than the ATIS dataset [45] whose OOV is smaller than 1%. • ABC [34], the Anti-reflexive Bias Challenge, is a multi-task benchmark dataset designed for evaluating gender assumptions in NLP models. ABC consists of 4 tasks, including language modeling, natural language inference (NLI), coreference resolution, and machine translation. A total of 4,560 samples are collected by a template-based method. The language modeling task is to predict the pronoun of a sentence. For NLI and coreference resolution, three variations of each sentence are used to construct entailment pairs. For machine translation, sentences with two variations of third-person pronouns in English are used as source sentences. • CompGuessWhat?! [115] is a dataset for grounded language learning with 65,700 collected dialogues. It is an instance of the Grounded Language Learning with Attributes (GROLLA) framework. The evaluation process includes three parts: goal-oriented evaluation (e.g., Visual QA and Visual NLI), object attribute prediction, and zero-shot evaluation. • SCIERC [72] is a multi-label dataset for identifying entities, relations, and cross-sentence coreference clusters from abstracts of research papers. SCIERC contains 500 scientific abstracts collected from proceedings in 12 conferences and workshops in artificial intelligence.",What are some benchmark datasets for evaluating multi-task learning in NLP?,1. What are some benchmark datasets for evaluating multi-task learning in NLP?,"Questions:

1. What are some benchmark datasets for evaluating multi-task learning in NLP?

Answer:

(p22.0) As summarized in Table 3, we list a few public multi-task benchmark datasets for NLP tasks.

(p22.1) • GLUE [123] is a benchmark dataset for evaluating natural language understanding (NLU) models. The main benchmark consists of 8 sentence and sentence-pair classification tasks as well as a regression task. The tasks cover a diverse range of genres, dataset sizes, and difficulties. Besides, a diagnostic dataset is provided to evaluate the ability of NLU models on capturing a pre-defined set of language phenomena. • SuperGLUE [122] is a generalization of GLUE. As the performance of state-of-the-art models has exceeded non-expert human baselines on GLUE, SuperGLUE contains a set of 8 more challenging NLU tasks along with comprehensive human baselines. Besides retaining the two hardest tasks in GLUE, 6 tasks are added with two new question formats: coreference resolution and question answering (QA).  [46] is a multi-task few-shot learning dataset for world knowledge and problem solving abilities of language processing models. This dataset covers 57 subjects including 19 in STEM, 13 in humanities, 12 in social sciences, and 13 in other subjects. This dataset is split into a few-shot development set that has 5 questions for each subject, a validation set for tuning hyper-parameters containing 1540 questions, and a test set with 14079 questions. • Xtreme [48] is a multi-task benchmark dataset for evaluating cross-lingual generalization capabilities of multi-lingual representations covering 9 tasks in 40 languages. The tasks include 2 classification tasks, 2 structure prediction tasks, 3 question answering tasks, and 2 sentence retrieval tasks. Out of the 40 languages involved, 19 languages appear in at least 3 datasets and the rest 21 languages appear in at least one dataset. • XGLUE [63] is a benchmark dataset that supports the development and evaluation of large cross-lingual pre-trained language models. The XGLUE dataset includes 11 downstream tasks, including 3 single-input understanding tasks, 6 pair-input understanding tasks, and 2 generation tasks. The pre-training corpus consists of a small corpus that includes a 101G multi-lingual corpus covering 100 languages and a 146G bilingual corpus covering 27 languages, and a large corpus with 2,500G multi-lingual data covering 89 languages. • LSParD [105] is a multi-task semantic parsing dataset with 3 tasks, including question type classification, entity mention detection, and question semantic parsing. Each logical form is associated with a question and multiple human annotated paraphrases. This dataset contains 51,164 questions in 9 categories, 3361 logical form patterns, and 23,144 entities. • ECSA [33] is a dataset for slot filling, named entity recognition, and segmentation to evaluate online shopping assistant systems in Chinese. The training part contains 24,892 pairs of input utterances and their corresponding slot labels, named entity labels, and segment labels. The testing part includes 2,723 such pairs with an Out-of-Vocabulary (OOV) rate of 85.3%, which is much higher than the ATIS dataset [45] whose OOV is smaller than 1%. • ABC [34], the Anti-reflexive Bias Challenge, is a multi-task benchmark dataset designed for evaluating gender assumptions in NLP models. ABC consists of 4 tasks, including language modeling, natural language inference (NLI), coreference resolution, and machine translation. A total of 4,560 samples are collected by a template-based method. The language modeling task is to predict the pronoun of a sentence. For NLI and coreference resolution, three variations of each sentence are used to construct entailment pairs. For machine translation, sentences with two variations of third-person pronouns in English are used as source sentences. • CompGuessWhat?! [115] is a dataset for grounded language learning with 65,700 collected dialogues. It is an instance of the Grounded Language Learning with Attributes (GROLLA) framework. The evaluation process includes three parts: goal-oriented evaluation (e.g., Visual QA and Visual NLI), object attribute prediction, and zero-shot evaluation. • SCIERC [72] is a multi-label dataset for identifying entities, relations, and cross-sentence coreference clusters from abstracts of research papers. SCIERC contains 500 scientific abstracts collected from proceedings in 12 conferences and workshops in artificial intelligence.",What are some examples of public multi-task benchmark datasets for NLP tasks?,1. What are some examples of public multi-task benchmark datasets for NLP tasks?,"Questions:

1. What are some examples of public multi-task benchmark datasets for NLP tasks?

Answer:

(p22.0) As summarized in Table 3, we list a few public multi-task benchmark datasets for NLP tasks.

(p22.1) • GLUE [123] is a benchmark dataset for evaluating natural language understanding (NLU) models. The main benchmark consists of 8 sentence and sentence-pair classification tasks as well as a regression task. The tasks cover a diverse range of genres, dataset sizes, and difficulties. Besides, a diagnostic dataset is provided to evaluate the ability of NLU models on capturing a pre-defined set of language phenomena. • SuperGLUE [122] is a generalization of GLUE. As the performance of state-of-the-art models has exceeded non-expert human baselines on GLUE, SuperGLUE contains a set of 8 more challenging NLU tasks along with comprehensive human baselines. Besides retaining the two hardest tasks in GLUE, 6 tasks are added with two new question formats: coreference resolution and question answering (QA).  [46] is a multi-task few-shot learning dataset for world knowledge and problem solving abilities of language processing models. This dataset covers 57 subjects including 19 in STEM, 13 in humanities, 12 in social sciences, and 13 in other subjects. This dataset is split into a few-shot development set that has 5 questions for each subject, a validation set for tuning hyper-parameters containing 1540 questions, and a test set with 14079 questions. • Xtreme [48] is a multi-task benchmark dataset for evaluating cross-lingual generalization capabilities of multi-lingual representations covering 9 tasks in 40 languages. The tasks include 2 classification tasks, 2 structure prediction tasks, 3 question answering tasks, and 2 sentence retrieval tasks. Out of the 40 languages involved, 19 languages appear in at least 3 datasets and the rest 21 languages appear in at least one dataset. • XGLUE [63] is a benchmark dataset that supports the development and evaluation of large cross-lingual pre-trained language models. The XGLUE dataset includes 11 downstream tasks, including 3 single-input understanding tasks, 6 pair-input understanding tasks, and 2 generation tasks. The pre-training corpus consists of a small corpus that includes a 101G multi-lingual corpus covering 100 languages and a 146G bilingual corpus covering 27 languages, and a large corpus with 2,500G multi-lingual data covering 89 languages. • LSParD [105] is a multi-task semantic parsing dataset with 3 tasks, including question type classification, entity mention detection, and question semantic parsing. Each logical form is associated with a question and multiple human annotated paraphrases. This dataset contains 51,164 questions in 9 categories, 3361 logical form patterns, and 23,144 entities. • ECSA [33] is a dataset for slot filling, named entity recognition, and segmentation to evaluate online shopping assistant systems in Chinese. The training part contains 24,892 pairs of input utterances and their corresponding slot labels, named entity labels, and segment labels. The testing part includes 2,723 such pairs with an Out-of-Vocabulary (OOV) rate of 85.3%, which is much higher than the ATIS dataset [45] whose OOV is smaller than 1%. • ABC [34], the Anti-reflexive Bias Challenge, is a multi-task benchmark dataset designed for evaluating gender assumptions in NLP models. ABC consists of 4 tasks, including language modeling, natural language inference (NLI), coreference resolution, and machine translation. A total of 4,560 samples are collected by a template-based method. The language modeling task is to predict the pronoun of a sentence. For NLI and coreference resolution, three variations of each sentence are used to construct entailment pairs. For machine translation, sentences with two variations of third-person pronouns in English are used as source sentences. • CompGuessWhat?! [115] is a dataset for grounded language learning with 65,700 collected dialogues. It is an instance of the Grounded Language Learning with Attributes (GROLLA) framework. The evaluation process includes three parts: goal-oriented evaluation (e.g., Visual QA and Visual NLI), object attribute prediction, and zero-shot evaluation. • SCIERC [72] is a multi-label dataset for identifying entities, relations, and cross-sentence coreference clusters from abstracts of research papers. SCIERC contains 500 scientific abstracts collected from proceedings in 12 conferences and workshops in artificial intelligence.",What are some notable multi-task benchmark datasets in NLP and their purposes?,1. What are some notable multi-task benchmark datasets in NLP and their purposes?,"Questions:

1. What are some notable multi-task benchmark datasets in NLP and their purposes?

Answer:

(p22.0) As summarized in Table 3, we list a few public multi-task benchmark datasets for NLP tasks.

(p22.1) • GLUE [123] is a benchmark dataset for evaluating natural language understanding (NLU) models. The main benchmark consists of 8 sentence and sentence-pair classification tasks as well as a regression task. The tasks cover a diverse range of genres, dataset sizes, and difficulties. Besides, a diagnostic dataset is provided to evaluate the ability of NLU models on capturing a pre-defined set of language phenomena. • SuperGLUE [122] is a generalization of GLUE. As the performance of state-of-the-art models has exceeded non-expert human baselines on GLUE, SuperGLUE contains a set of 8 more challenging NLU tasks along with comprehensive human baselines. Besides retaining the two hardest tasks in GLUE, 6 tasks are added with two new question formats: coreference resolution and question answering (QA).  [46] is a multi-task few-shot learning dataset for world knowledge and problem solving abilities of language processing models. This dataset covers 57 subjects including 19 in STEM, 13 in humanities, 12 in social sciences, and 13 in other subjects. This dataset is split into a few-shot development set that has 5 questions for each subject, a validation set for tuning hyper-parameters containing 1540 questions, and a test set with 14079 questions. • Xtreme [48] is a multi-task benchmark dataset for evaluating cross-lingual generalization capabilities of multi-lingual representations covering 9 tasks in 40 languages. The tasks include 2 classification tasks, 2 structure prediction tasks, 3 question answering tasks, and 2 sentence retrieval tasks. Out of the 40 languages involved, 19 languages appear in at least 3 datasets and the rest 21 languages appear in at least one dataset. • XGLUE [63] is a benchmark dataset that supports the development and evaluation of large cross-lingual pre-trained language models. The XGLUE dataset includes 11 downstream tasks, including 3 single-input understanding tasks, 6 pair-input understanding tasks, and 2 generation tasks. The pre-training corpus consists of a small corpus that includes a 101G multi-lingual corpus covering 100 languages and a 146G bilingual corpus covering 27 languages, and a large corpus with 2,500G multi-lingual data covering 89 languages. • LSParD [105] is a multi-task semantic parsing dataset with 3 tasks, including question type classification, entity mention detection, and question semantic parsing. Each logical form is associated with a question and multiple human annotated paraphrases. This dataset contains 51,164 questions in 9 categories, 3361 logical form patterns, and 23,144 entities. • ECSA [33] is a dataset for slot filling, named entity recognition, and segmentation to evaluate online shopping assistant systems in Chinese. The training part contains 24,892 pairs of input utterances and their corresponding slot labels, named entity labels, and segment labels. The testing part includes 2,723 such pairs with an Out-of-Vocabulary (OOV) rate of 85.3%, which is much higher than the ATIS dataset [45] whose OOV is smaller than 1%. • ABC [34], the Anti-reflexive Bias Challenge, is a multi-task benchmark dataset designed for evaluating gender assumptions in NLP models. ABC consists of 4 tasks, including language modeling, natural language inference (NLI), coreference resolution, and machine translation. A total of 4,560 samples are collected by a template-based method. The language modeling task is to predict the pronoun of a sentence. For NLI and coreference resolution, three variations of each sentence are used to construct entailment pairs. For machine translation, sentences with two variations of third-person pronouns in English are used as source sentences. • CompGuessWhat?! [115] is a dataset for grounded language learning with 65,700 collected dialogues. It is an instance of the Grounded Language Learning with Attributes (GROLLA) framework. The evaluation process includes three parts: goal-oriented evaluation (e.g., Visual QA and Visual NLI), object attribute prediction, and zero-shot evaluation. • SCIERC [72] is a multi-label dataset for identifying entities, relations, and cross-sentence coreference clusters from abstracts of research papers. SCIERC contains 500 scientific abstracts collected from proceedings in 12 conferences and workshops in artificial intelligence."
