corpusid,title,domain,url,section,section_title,paragraph,para_listed_answer,answer,indexed_answer,filtered_refids,filtered_refids_qualified,num_reference,segmented_answer,question,itemized_question,question_summ,itemized_question_summ,QA pair,QA pair summ
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s1,BERT embeddings,"['p1.0', 'p1.1']","[""Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence."", ""In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).""]","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","[['b22', 'b24'], ['b21', 'b65']]","[['b22', 'b24'], ['b21', 'b65']]",4,"1. Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.
2. In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.
3. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
4. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.
5. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.
6. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.
7. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","How do BERT's contextualized embeddings differ from conventional static embeddings?
What evidence supports the distributional hypothesis in the context of BERT's embeddings?
How does the position of a word in a sentence affect its representation in BERT's model?
In what way do BERT embeddings become more context-specific across different layers?
What findings suggest that BERT embeddings tend to occupy a narrow cone in the vector space, and how does this change across layers?","1. How do BERT's contextualized embeddings differ from conventional static embeddings?
2. What evidence supports the distributional hypothesis in the context of BERT's embeddings?
3. How does the position of a word in a sentence affect its representation in BERT's model?
4. In what way do BERT embeddings become more context-specific across different layers?
5. What findings suggest that BERT embeddings tend to occupy a narrow cone in the vector space, and how does this change across layers?",None,,"Questions:

1. How do BERT's contextualized embeddings differ from conventional static embeddings?
2. What evidence supports the distributional hypothesis in the context of BERT's embeddings?
3. How does the position of a word in a sentence affect its representation in BERT's model?
4. In what way do BERT embeddings become more context-specific across different layers?
5. What findings suggest that BERT embeddings tend to occupy a narrow cone in the vector space, and how does this change across layers?

Answer:

(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","Questions:



Answer:

(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic)."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s3,Syntactic knowledge,"['p3.0', 'p3.1', 'p3.2']","['As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.', '(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).', 'Regarding syntactic competence of BERT\'s MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT\'s encoding of syntactic structure does not indicate that it actually relies on that knowledge.']","As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","(p3.0) As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(p3.1) (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

(p3.2) Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","[[], ['b5', 'b18'], ['b59', None, 'b64']]","[[], ['b5', 'b18'], ['b59', None, 'b64']]",5,"1. As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it.
2. Htut et al.(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root.
3. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation.
4. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2).
5. Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).
6. Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task.
7. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences.
8. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations.
9. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input.
10. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019).
11. This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","How does BERT encode syntactic information, and what limitations have been identified in extracting syntactic structures from its self-attention weights?
What methods have researchers used to approximate or recover syntactic structures from BERT, and how successful have these attempts been?
In what ways does BERT's Masked Language Model (MLM) demonstrate syntactic competence, particularly in relation to subject-predicate agreement?
How does BERT handle negative polarity items (NPIs), and what does this reveal about its understanding of syntax?
What evidence suggests that BERT's performance is not significantly impacted by nonsensical inputs or syntactic irregularities, and how does this challenge the notion that it understands syntactic structure?","1. How does BERT encode syntactic information, and what limitations have been identified in extracting syntactic structures from its self-attention weights?
2. What methods have researchers used to approximate or recover syntactic structures from BERT, and how successful have these attempts been?
3. In what ways does BERT's Masked Language Model (MLM) demonstrate syntactic competence, particularly in relation to subject-predicate agreement?
4. How does BERT handle negative polarity items (NPIs), and what does this reveal about its understanding of syntax?
5. What evidence suggests that BERT's performance is not significantly impacted by nonsensical inputs or syntactic irregularities, and how does this challenge the notion that it understands syntactic structure?",None,,"Questions:

1. How does BERT encode syntactic information, and what limitations have been identified in extracting syntactic structures from its self-attention weights?
2. What methods have researchers used to approximate or recover syntactic structures from BERT, and how successful have these attempts been?
3. In what ways does BERT's Masked Language Model (MLM) demonstrate syntactic competence, particularly in relation to subject-predicate agreement?
4. How does BERT handle negative polarity items (NPIs), and what does this reveal about its understanding of syntax?
5. What evidence suggests that BERT's performance is not significantly impacted by nonsensical inputs or syntactic irregularities, and how does this challenge the notion that it understands syntactic structure?

Answer:

(p3.0) As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(p3.1) (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

(p3.2) Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","Questions:



Answer:

(p3.0) As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(p3.1) (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

(p3.2) Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s7,Self-attention heads,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4', 'p7.5', 'p7.6', 'p7.7', 'p7.8', 'p7.9']","['Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:', '• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);', '• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).', 'Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.', '[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.', 'Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.', 'Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.', ""(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis."", ""Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data."", ""Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.""]","Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","(p7.0) Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

(p7.1) • attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

(p7.2) • attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

(p7.3) Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

(p7.4) [SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

(p7.5) Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

(p7.6) Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(p7.7) (2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

(p7.8) Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

(p7.9) Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","[[], [], [None, 'b9'], [None], [], [], [], ['b58', None], [], [None, 'b9']]","[[], [], [None, 'b9'], [None], [], [], [], ['b58', None], [], [None, 'b9']]",7,"1. Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types: attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018); attending to previous/next tokens,    (Kovaleva et al., 2019)
2. According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"".
3. However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 .
4. Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019).
5. This apparent redundancy must be related to the overparametrization issue (see section 7).
6. Attention to (Kovaleva et al., 2019)5 is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to (Kovaleva et al., 2019)4 and punctuation.
7. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as (Kovaleva et al., 2019)5 and (Kovaleva et al., 2019)4, and the model learns to rely on them.
8. They suggest also that the function of (Kovaleva et al., 2019)4 might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.
9. (Kovaleva et al., 2019)4 gets increased attention starting in layer 5, but its importance for prediction drops.
10. If this hypothesis is correct, attention probing studies that excluded the (Kovaleva et al., 2019)4 and (Kovaleva et al., 2019)5 tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.
11. Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.
12. Some BERT heads seem to specialize in certain types of syntactic relations.
13. Htut et al.(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions.
14. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline.
15. The evidence for nsubj, advmod, and amod has some variation between these two studies.
16. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context.
17. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).
18. present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora.
19. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.
20. Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,.
21. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks.
22. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998).
23. Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","What classifications of attention head types have been proposed in the literature for understanding Transformer models?
How is the attention to [SEP] and punctuation tokens interpreted in the context of BERT models?
What does the ""heterogeneous"" self-attention pattern reveal about the linguistic information encoded by BERT heads?
In what way do certain BERT heads specialize in syntactic relations, and what types of syntactic positions are most affected?
How do studies assess the impact of specialized attention heads on BERT's performance in natural language understanding tasks?","1. What classifications of attention head types have been proposed in the literature for understanding Transformer models?
2. How is the attention to [SEP] and punctuation tokens interpreted in the context of BERT models?
3. What does the ""heterogeneous"" self-attention pattern reveal about the linguistic information encoded by BERT heads?
4. In what way do certain BERT heads specialize in syntactic relations, and what types of syntactic positions are most affected?
5. How do studies assess the impact of specialized attention heads on BERT's performance in natural language understanding tasks?",None,,"Questions:

1. What classifications of attention head types have been proposed in the literature for understanding Transformer models?
2. How is the attention to [SEP] and punctuation tokens interpreted in the context of BERT models?
3. What does the ""heterogeneous"" self-attention pattern reveal about the linguistic information encoded by BERT heads?
4. In what way do certain BERT heads specialize in syntactic relations, and what types of syntactic positions are most affected?
5. How do studies assess the impact of specialized attention heads on BERT's performance in natural language understanding tasks?

Answer:

(p7.0) Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

(p7.1) • attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

(p7.2) • attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

(p7.3) Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

(p7.4) [SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

(p7.5) Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

(p7.6) Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(p7.7) (2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

(p7.8) Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

(p7.9) Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","Questions:



Answer:

(p7.0) Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

(p7.1) • attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

(p7.2) • attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

(p7.3) Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

(p7.4) [SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

(p7.5) Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

(p7.6) Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(p7.7) (2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

(p7.8) Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

(p7.9) Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s8,BERT layers,"['p8.0', 'p8.1', 'p8.2', 'p8.3', 'p8.4']","['The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.', 'There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.', 'The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.', ""The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT."", 'The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.']","The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.","(p8.0) The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

(p8.1) There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

(p8.2) The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

(p8.3) The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

(p8.4) The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.","[[], ['b5'], ['b5', 'b57', 'b50'], [None, 'b50', 'b9'], ['b5']]","[[], ['b5'], ['b5', 'b57', 'b50'], [None, 'b50', 'b9'], ['b5']]",8,"1. The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings.
2. It stands to reason that the lower layers have the most linear word order information.
3. report a decrease in the knowledge of linear word order around layer 4 in BERT-base.
4. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.
5. There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers.
6. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large).
7. Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.
8. The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5).
9. There is conflicting evidence about syntactic chunks.
10. Tenney et al. (6-9 for base-BERT, 14-19 for BERT-large)0 conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling.
11. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing.
12. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.
13. The final layers of BERT are the most taskspecific.
14. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable .
15. In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019).
16. At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance.
17. Tenney et al. (6-9 for base-BERT, 14-19 for BERT-large)0 suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers.
18. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (6-9 for base-BERT, 14-19 for BERT-large)1.
19. But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial.
20. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.
21. The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"".
22. However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.","How does the representation of linear word order change across different layers in BERT, particularly from the first layer to layer 4?
What evidence supports the concentration of syntactic information in the middle layers of BERT, and how do these findings compare across different studies?
In what way do the final layers of BERT differ in function from the earlier layers, especially in terms of task specificity?
How does the distribution of semantic knowledge across BERT layers compare between BERT-base and BERT-large models?
What are the contrasting views on the localization of surface, syntactic, and semantic features within BERT layers, and how do these views relate to the performance on SentEval semantic tasks?","1. How does the representation of linear word order change across different layers in BERT, particularly from the first layer to layer 4?
2. What evidence supports the concentration of syntactic information in the middle layers of BERT, and how do these findings compare across different studies?
3. In what way do the final layers of BERT differ in function from the earlier layers, especially in terms of task specificity?
4. How does the distribution of semantic knowledge across BERT layers compare between BERT-base and BERT-large models?
5. What are the contrasting views on the localization of surface, syntactic, and semantic features within BERT layers, and how do these views relate to the performance on SentEval semantic tasks?","# How does the distribution of linguistic information across BERT layers impact model performance and understanding?
  - The initial layers of BERT primarily encode linear word order information, with a noted decrease in this knowledge around layer 4, shifting towards a greater understanding of hierarchical sentence structure.
  - Middle layers of BERT (layers 6-9 for BERT-base and 14-19 for BERT-large) are identified as the most syntactically informative, with studies showing success in reconstructing syntactic tree depth and achieving the best performance on syntactic probing tasks in these layers.
  - The final layers of BERT are highlighted as being the most task-specific, particularly tuned towards the Masked Language Model (MLM) task during pre-training, which suggests why these layers undergo the most significant changes during fine-tuning for specific tasks.

# What are the implications of the syntactic and semantic information distribution in BERT for NLP tasks?
  - There is a consensus that basic syntactic information appears earlier in BERT, while high-level semantic features are more prominent in the higher layers, suggesting a parallel to the order of components in traditional NLP pipelines.
  - Conflicting evidence exists regarding the utility of lower versus middle layers for tasks like POS tagging and chunking, with some studies finding middle layers more useful for parsing and others reporting the opposite.
  - The spread of semantic knowledge across the entire model, with semantics being more distributed than syntactic information, raises questions about the effectiveness of stacking more Transformer layers and the specific benefits it brings to understanding and processing language.

# How do findings on the layer-specific functionality of BERT contribute to our understanding of Transformer models?
  - The observation that middle layers of BERT are the most transferable across tasks supports the view that these layers capture a level of linguistic abstraction that is broadly useful for a variety of NLP tasks.
  - The fact that final layers are most task-specific, and that restoring lower layer weights of a fine-tuned BERT to their pre-trained values does not significantly impact performance, suggests a nuanced understanding of how different layers contribute to task-specific adaptations.
  - Disputes among researchers about the precise distribution and role of linguistic features across layers, especially regarding semantic features, highlight ongoing debates and areas for further investigation in the field of NLP and Transformer model research.","1. How does the distribution of linguistic information across BERT layers impact model performance and understanding?
2. What are the implications of the syntactic and semantic information distribution in BERT for NLP tasks?
3. How do findings on the layer-specific functionality of BERT contribute to our understanding of Transformer models?","Questions:

1. How does the representation of linear word order change across different layers in BERT, particularly from the first layer to layer 4?
2. What evidence supports the concentration of syntactic information in the middle layers of BERT, and how do these findings compare across different studies?
3. In what way do the final layers of BERT differ in function from the earlier layers, especially in terms of task specificity?
4. How does the distribution of semantic knowledge across BERT layers compare between BERT-base and BERT-large models?
5. What are the contrasting views on the localization of surface, syntactic, and semantic features within BERT layers, and how do these views relate to the performance on SentEval semantic tasks?

Answer:

(p8.0) The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

(p8.1) There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

(p8.2) The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

(p8.3) The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

(p8.4) The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.","Questions:

1. How does the distribution of linguistic information across BERT layers impact model performance and understanding?
2. What are the implications of the syntactic and semantic information distribution in BERT for NLP tasks?
3. How do findings on the layer-specific functionality of BERT contribute to our understanding of Transformer models?

Answer:

(p8.0) The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

(p8.1) There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

(p8.2) The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

(p8.3) The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

(p8.4) The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s10,Pre-training BERT,"['p10.0', 'p10.1', 'p10.2', 'p10.3', 'p10.4', 'p10.5', 'p10.6', 'p10.7', 'p10.8', 'p10.9']","['The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.', '• Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).', '• Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).', ""• Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);"", '• Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;', '• Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).', '• Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).', '• Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.', 'Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .', 'Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).']","The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

• Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

• Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

• Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

• Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

• Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

• Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

• Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).","(p10.0) The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

(p10.1) • Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

(p10.2) • Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

(p10.3) • Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

(p10.4) • Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

(p10.5) • Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

(p10.6) • Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

(p10.7) • Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

(p10.8) Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

(p10.9) Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).","[[], ['b62', 'b16', 'b23', 'b8', None], ['b62', 'b75'], ['b8'], [], ['b47'], [], [], ['b17'], ['b31', 'b9']]","[[], ['b62', 'b16', 'b23', 'b8', None], ['b62', 'b75'], ['b8'], [], ['b47'], [], [], ['b17'], ['b31', 'b9']]",12,"1. The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM).
2. Multiple studies have come up with alternative training objectives to improve on BERT.
3. Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting .
4. Wang et al. (2019a) Mikolov et al. (2013b).
5. Permutation language modeling. Yang et al. (MLM)6 replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order.
6. See also the n-gram word order reconstruction task (Wang et al., 2019a).
7. Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020); Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words; Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).
8. Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (MLM)0.
9. Clinchant et al. (MLM)6 propose replacing the MASK token with (MLM)2 token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.
10. Another obvious source of improvement is pretraining data.
11. Liu et al. (MLM)3 explore the benefits of increasing the corpus volume and longer training.
12. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (MLM)4, there are ongoing efforts to incorporate structured knowledge resources .
13. Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (MLM)5 and ERNIE .
14. Alternatively, SemBERT  integrates semantic role information with BERT representations.
15. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides.
16. Hao et al. (MLM)6 conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (MLM)7.
17. However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (MLM)8.","How does removing the Next Sentence Prediction (NSP) task affect BERT's performance on downstream tasks, particularly in cross-lingual settings?
What are the benefits of using permutation language modeling over the original masked language model (MLM) approach in BERT's pre-training?
How does the span boundary objective differ from traditional MLM in terms of predicting masked spans, and what advantages does it offer?
In what ways can incorporating structured knowledge resources or entity embeddings during BERT's pre-training phase enhance its understanding of structured knowledge?
How do pre-trained weights contribute to BERT's robustness against overfitting compared to a randomly initialized model during fine-tuning?","1. How does removing the Next Sentence Prediction (NSP) task affect BERT's performance on downstream tasks, particularly in cross-lingual settings?
2. What are the benefits of using permutation language modeling over the original masked language model (MLM) approach in BERT's pre-training?
3. How does the span boundary objective differ from traditional MLM in terms of predicting masked spans, and what advantages does it offer?
4. In what ways can incorporating structured knowledge resources or entity embeddings during BERT's pre-training phase enhance its understanding of structured knowledge?
5. How do pre-trained weights contribute to BERT's robustness against overfitting compared to a randomly initialized model during fine-tuning?",None,,"Questions:

1. How does removing the Next Sentence Prediction (NSP) task affect BERT's performance on downstream tasks, particularly in cross-lingual settings?
2. What are the benefits of using permutation language modeling over the original masked language model (MLM) approach in BERT's pre-training?
3. How does the span boundary objective differ from traditional MLM in terms of predicting masked spans, and what advantages does it offer?
4. In what ways can incorporating structured knowledge resources or entity embeddings during BERT's pre-training phase enhance its understanding of structured knowledge?
5. How do pre-trained weights contribute to BERT's robustness against overfitting compared to a randomly initialized model during fine-tuning?

Answer:

(p10.0) The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

(p10.1) • Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

(p10.2) • Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

(p10.3) • Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

(p10.4) • Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

(p10.5) • Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

(p10.6) • Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

(p10.7) • Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

(p10.8) Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

(p10.9) Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).","Questions:



Answer:

(p10.0) The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

(p10.1) • Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

(p10.2) • Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

(p10.3) • Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

(p10.4) • Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

(p10.5) • Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

(p10.6) • Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

(p10.7) • Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

(p10.8) Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

(p10.9) Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019)."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s11,Model architecture choices,"['p11.0', 'p11.1']","[""To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks."", 'Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.']","To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.","(p11.0) To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

(p11.1) Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.","[['b58', 'b17', 'b75', 'b20'], []]","[['b58', 'b17', 'b75', 'b20'], []]",4,"1. To date, the most systematic study of BERT architecture was performed by .
2. They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others.
3. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable.
4. Larger hidden representation size was consistently better, but the gains varied by setting.
5. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance.
6. They also publish their recommendations for other model parameters.
7. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.
8. observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.
9. Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers.
10. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.","How does the number of layers in BERT architecture influence its performance compared to the number of heads?
What are the implications of larger hidden representation sizes on BERT's performance?
How does large-batch training affect language model perplexity and downstream task performance in BERT models?
What effect does the normalization of embedding values of the [CLS] token have on BERT's training stability and performance on text classification tasks?
How can the training speed of BERT be improved without sacrificing accuracy on GLUE tasks through the use of a ""warm-start"" approach?","1. How does the number of layers in BERT architecture influence its performance compared to the number of heads?
2. What are the implications of larger hidden representation sizes on BERT's performance?
3. How does large-batch training affect language model perplexity and downstream task performance in BERT models?
4. What effect does the normalization of embedding values of the [CLS] token have on BERT's training stability and performance on text classification tasks?
5. How can the training speed of BERT be improved without sacrificing accuracy on GLUE tasks through the use of a ""warm-start"" approach?",None,,"Questions:

1. How does the number of layers in BERT architecture influence its performance compared to the number of heads?
2. What are the implications of larger hidden representation sizes on BERT's performance?
3. How does large-batch training affect language model perplexity and downstream task performance in BERT models?
4. What effect does the normalization of embedding values of the [CLS] token have on BERT's training stability and performance on text classification tasks?
5. How can the training speed of BERT be improved without sacrificing accuracy on GLUE tasks through the use of a ""warm-start"" approach?

Answer:

(p11.0) To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

(p11.1) Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.","Questions:



Answer:

(p11.0) To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

(p11.1) Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s12,Fine-tuning BERT,"['p12.0', 'p12.1', 'p12.2', 'p12.3', 'p12.4', 'p12.5', 'p12.6']","['Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).', '• Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).', 'With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.', '(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.', 'An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).', 'Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.', '7 How big should BERT be?']","Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

• Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

7 How big should BERT be?","(p12.0) Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

(p12.1) • Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

(p12.2) With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(p12.3) (2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

(p12.4) An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

(p12.5) Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

(p12.6) 7 How big should BERT be?","[[None, 'b9'], ['b83'], [], [], ['b26'], ['b28'], []]","[[None, 'b9'], ['b83'], [], [], ['b26'], ['b28'], []]",5,"1. Pre-training + fine-tuning workflow is a crucial part of BERT.
2. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand.
3. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns.
4. It is understandable why fine-tuning would increase the attention to  Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).
5. Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).
6. With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules.
7. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost.
8. Artetxe et al.(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.
9. An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).Initialization can have a dramatic effect on the training process (Petrov, 2010).
10. However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018).
11. Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation.
12. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.
13. 7 How big should BERT be?","How does the fine-tuning process in BERT models specifically alter the attention mechanism towards [SEP] tokens?
What advantages does the two-stage fine-tuning process offer in the context of BERT model training?
How do adversarial token perturbations enhance the robustness of BERT models?
What role do adapter modules play in reducing computational costs for BERT models while ensuring competitive performance?
How does weight initialization and training data order influence the variability in performance of BERT models during fine-tuning?","1. How does the fine-tuning process in BERT models specifically alter the attention mechanism towards [SEP] tokens?
2. What advantages does the two-stage fine-tuning process offer in the context of BERT model training?
3. How do adversarial token perturbations enhance the robustness of BERT models?
4. What role do adapter modules play in reducing computational costs for BERT models while ensuring competitive performance?
5. How does weight initialization and training data order influence the variability in performance of BERT models during fine-tuning?",None,,"Questions:

1. How does the fine-tuning process in BERT models specifically alter the attention mechanism towards [SEP] tokens?
2. What advantages does the two-stage fine-tuning process offer in the context of BERT model training?
3. How do adversarial token perturbations enhance the robustness of BERT models?
4. What role do adapter modules play in reducing computational costs for BERT models while ensuring competitive performance?
5. How does weight initialization and training data order influence the variability in performance of BERT models during fine-tuning?

Answer:

(p12.0) Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

(p12.1) • Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

(p12.2) With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(p12.3) (2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

(p12.4) An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

(p12.5) Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

(p12.6) 7 How big should BERT be?","Questions:



Answer:

(p12.0) Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

(p12.1) • Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

(p12.2) With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(p12.3) (2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

(p12.4) An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

(p12.5) Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

(p12.6) 7 How big should BERT be?"
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s13,Overparametrization,"['p13.0', 'p13.1', 'p13.2', 'p13.3', 'p13.4']","['Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.', 'Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).', 'Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .', 'Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.', '(2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.']","Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.","(p13.0) Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

(p13.1) Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

(p13.2) Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

(p13.3) Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(p13.4) (2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.","[['b38', 'b43', 'b70'], ['b46', 'b37', 'b9', None, 'b50', 'b58', 'b20'], [], [], []]","[['b38', 'b43', 'b70'], ['b46', 'b37', 'b9', None, 'b50', 'b58', 'b20'], [], [], []]",10,"1. Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT.
2. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.
3. Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have.
4. Voita et al. (Strubell et al., 2019;Schwartz et al., 2019)3 showed that all but a few Transformer heads could be pruned without significant losses in performance.
5. For BERT, Clark et al. (Strubell et al., 2019;Schwartz et al., 2019)3 observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (Strubell et al., 2019;Schwartz et al., 2019)3 were able to reduce most layers to a single head.
6. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance.
7. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019).
8. Additionally, Tenney et al. (Strubell et al., 2019;Schwartz et al., 2019)0 examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (Strubell et al., 2019;Schwartz et al., 2019)1.
9. Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case.
10. In particular, the opposite was observed for subjectverb agreement (Strubell et al., 2019;Schwartz et al., 2019)2 and sentence subject detection .
11. Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers.
12. Clark et al.(Strubell et al., 2019;Schwartz et al., 2019)3 suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.","What are the implications of overparametrization in transformer-based models like BERT on computational complexity and environmental sustainability?
How does the redundancy of transformer heads in BERT models affect their efficiency and performance?
In what ways can pruning transformer heads in models like BERT impact their performance on various NLP tasks?
What findings suggest that larger transformer models do not always outperform smaller versions in specific linguistic tasks?
What potential reasons have been proposed for the presence of redundant heads and layers in BERT models?","1. What are the implications of overparametrization in transformer-based models like BERT on computational complexity and environmental sustainability?
2. How does the redundancy of transformer heads in BERT models affect their efficiency and performance?
3. In what ways can pruning transformer heads in models like BERT impact their performance on various NLP tasks?
4. What findings suggest that larger transformer models do not always outperform smaller versions in specific linguistic tasks?
5. What potential reasons have been proposed for the presence of redundant heads and layers in BERT models?",None,,"Questions:

1. What are the implications of overparametrization in transformer-based models like BERT on computational complexity and environmental sustainability?
2. How does the redundancy of transformer heads in BERT models affect their efficiency and performance?
3. In what ways can pruning transformer heads in models like BERT impact their performance on various NLP tasks?
4. What findings suggest that larger transformer models do not always outperform smaller versions in specific linguistic tasks?
5. What potential reasons have been proposed for the presence of redundant heads and layers in BERT models?

Answer:

(p13.0) Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

(p13.1) Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

(p13.2) Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

(p13.3) Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(p13.4) (2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.","Questions:



Answer:

(p13.0) Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

(p13.1) Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

(p13.2) Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

(p13.3) Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(p13.4) (2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s14,BERT compression,"['p14.0', 'p14.1', 'p14.2', 'p14.3', 'p14.4']","['Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.', 'Two main approaches include knowledge distillation and quantization.', 'The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).', ""The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware."", ""Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).""]","Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

Two main approaches include knowledge distillation and quantization.

The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).","(p14.0) Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

(p14.1) Two main approaches include knowledge distillation and quantization.

(p14.2) The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

(p14.3) The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

(p14.4) Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).","[[], [], ['b54', 'b37', 'b46', None], ['b40', 'b76'], ['b11', 'b72']]","[[], [], ['b54', 'b37', 'b46', None], ['b40', 'b76'], ['b11', 'b72']]",8,"1. Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss.
2. Such efforts to date are summarized in Table 1.
3. Two main approaches include knowledge distillation and quantization.
4. The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase).
5. This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).
6. The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019).
7. Note that this strategy often requires compatible hardware.
8. Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).","What are the main approaches to compressing BERT models with minimal accuracy loss?
How does knowledge distillation work for compressing BERT models?
What role does quantization play in reducing BERT's memory footprint?
Can you explain the technique of decomposing BERT's embedding matrix for model compression?
What is progressive model replacing and how does it contribute to BERT compression?","1. What are the main approaches to compressing BERT models with minimal accuracy loss?
2. How does knowledge distillation work for compressing BERT models?
3. What role does quantization play in reducing BERT's memory footprint?
4. Can you explain the technique of decomposing BERT's embedding matrix for model compression?
5. What is progressive model replacing and how does it contribute to BERT compression?",None,,"Questions:

1. What are the main approaches to compressing BERT models with minimal accuracy loss?
2. How does knowledge distillation work for compressing BERT models?
3. What role does quantization play in reducing BERT's memory footprint?
4. Can you explain the technique of decomposing BERT's embedding matrix for model compression?
5. What is progressive model replacing and how does it contribute to BERT compression?

Answer:

(p14.0) Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

(p14.1) Two main approaches include knowledge distillation and quantization.

(p14.2) The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

(p14.3) The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

(p14.4) Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).","Questions:



Answer:

(p14.0) Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

(p14.1) Two main approaches include knowledge distillation and quantization.

(p14.2) The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

(p14.3) The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

(p14.4) Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020)."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s15,Multilingual BERT,"['p15.0', 'p15.1', 'p15.2', 'p15.3', 'p15.4', 'p15.5', 'p15.6', 'p15.7']","['Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).', 'mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.', 'mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.', 'At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.', 'To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.', 'Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).', 'Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.', 'Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.']","Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.","(p15.0) Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

(p15.1) mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

(p15.2) mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

(p15.3) At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

(p15.4) To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

(p15.5) Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

(p15.6) Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

(p15.7) Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.","[['b13', 'b36', 'b30', 'b64', 'b68'], ['b30', 'b13'], ['b68', 'b42', 'b30', 'b13'], ['b30', 'b68', 'b13', 'b36'], ['b50'], ['b64', 'b2'], ['b2', 'b9', None, 'b50', 'b58', 'b20'], ['b4', 'b39', 'b56', 'b66', None]]","[['b13', 'b36', 'b30', 'b64', 'b68'], ['b30', 'b13'], ['b68', 'b42', 'b30', 'b13'], ['b30', 'b68', 'b13', 'b36'], ['b50'], ['b64', 'b2'], ['b2', 'b9', None, 'b50', 'b58', 'b20'], ['b4', 'b39', 'b56', 'b66', None]]",29,"1. Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary).
2. Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing.
3. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019).
4. The model seems to naturally learn high-quality cross-lingual word alignments (110K wordpiece vocabulary)9, with caveats for open-class parts of speech (Cao et al., 2019).
5. Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (Wu and Dredze, 2019;Pires et al., 2019)8 note that this task could be solvable by simple lexical matches.
6. Pires et al. (Wu and Dredze, 2019;Pires et al., 2019)8 conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.
7. mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (110K wordpiece vocabulary)0, and adding the IDs in pre-training was not beneficial .
8. It is also aware of at least some typological language features (110K wordpiece vocabulary)1, and transfer between structurally similar languages works better Pires et al., 2019).
9. Singh et al. (Wu and Dredze, 2019;Pires et al., 2019)8 argue that if typological features structure its representation space, it could not be considered as interlingua.
10. However, Artetxe et al. (Wu and Dredze, 2019;Pires et al., 2019)8 show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.
11. At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (110K wordpiece vocabulary)4, and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (Wu and Dredze, 2019;Pires et al., 2019)8.
12. Pires et al. (Wu and Dredze, 2019;Pires et al., 2019)8 and Wu and Dredze (Wu and Dredze, 2019;Pires et al., 2019)8 hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages.
13. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary.
14. Artetxe et al. (Wu and Dredze, 2019;Pires et al., 2019)8 also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (110K wordpiece vocabulary)9 vocabulary, without any shared word-pieces.
15. To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge.
16. However, as Tenney et al. (Wu and Dredze, 2019;Pires et al., 2019)0 aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"".
17. There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis .
18. A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.
19. Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (Wu and Dredze, 2019;Pires et al., 2019)1 would not be sufficient (Wu and Dredze, 2019;Pires et al., 2019)2.
20. Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Wu and Dredze, 2019;Pires et al., 2019)3.
21. Head and layer ablation studies (Wu and Dredze, 2019;Pires et al., 2019)4 inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Wu and Dredze, 2019;Pires et al., 2019)5, the absence of heads that would perform parsing ""in general"" (Wu and Dredze, 2019;Pires et al., 2019)6.
22. Ablations are also problematic if the same information was duplicated elsewhere in the network.
23. To mitigate that, Michel et al. (Wu and Dredze, 2019;Pires et al., 2019)8 prune heads in the order set by a proxy importance score, and Voita et al. (Wu and Dredze, 2019;Pires et al., 2019)8
24. fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.
25. Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Wu and Dredze, 2019;Pires et al., 2019)9.
26. However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Rönnqvist et al., 2019)0.
27. Also, visualization is typically limited to qualitative analysis (Rönnqvist et al., 2019)1, and should not be interpreted as definitive evidence.","How does mBERT achieve cross-lingual transfer and what role do shared word-pieces play in its performance across languages?
What are the limitations of mBERT in language generation tasks compared to its success in zero-shot transfer tasks?
How does the inclusion of typological features and language identities in mBERT's training data influence its ability to transfer between structurally similar languages?
What proposals have been made to improve the interpretability and effectiveness of mBERT, particularly in relation to probing methods and head/layer ablation studies?
In what ways do attention visualizations contribute to our understanding of mBERT, and what are the ongoing debates regarding their utility in interpreting deep learning models?","1. How does mBERT achieve cross-lingual transfer and what role do shared word-pieces play in its performance across languages?
2. What are the limitations of mBERT in language generation tasks compared to its success in zero-shot transfer tasks?
3. How does the inclusion of typological features and language identities in mBERT's training data influence its ability to transfer between structurally similar languages?
4. What proposals have been made to improve the interpretability and effectiveness of mBERT, particularly in relation to probing methods and head/layer ablation studies?
5. In what ways do attention visualizations contribute to our understanding of mBERT, and what are the ongoing debates regarding their utility in interpreting deep learning models?",None,,"Questions:

1. How does mBERT achieve cross-lingual transfer and what role do shared word-pieces play in its performance across languages?
2. What are the limitations of mBERT in language generation tasks compared to its success in zero-shot transfer tasks?
3. How does the inclusion of typological features and language identities in mBERT's training data influence its ability to transfer between structurally similar languages?
4. What proposals have been made to improve the interpretability and effectiveness of mBERT, particularly in relation to probing methods and head/layer ablation studies?
5. In what ways do attention visualizations contribute to our understanding of mBERT, and what are the ongoing debates regarding their utility in interpreting deep learning models?

Answer:

(p15.0) Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

(p15.1) mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

(p15.2) mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

(p15.3) At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

(p15.4) To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

(p15.5) Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

(p15.6) Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

(p15.7) Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.","Questions:



Answer:

(p15.0) Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

(p15.1) mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

(p15.2) mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

(p15.3) At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

(p15.4) To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

(p15.5) Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

(p15.6) Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

(p15.7) Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s16,Directions for further research,"['p16.0', 'p16.1', 'p16.2', 'p16.3', 'p16.4']","['BERTology has clearly come a long way, but it is fair to say we still have more questions than answers about how BERT works. In this section, we list what we believe to be the most promising directions for further research, together with the starting points that we already have.', 'Benchmarks that require verbal reasoning. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020). As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.', 'Developing methods to ""teach"" reasoning. While the community had success extracting knowledge from large pre-trained models, they often fail if any reasoning needs to be performed on top of the facts they possess (see subsection 4.3). For instance,  propose a method to ""teach"" BERT quantification, conditionals, comparatives, and boolean coordination.', 'Learning what happens at inference time.', 'Most of the BERT analysis papers focused on different probes of the model, but we know much less about what knowledge actually gets used. At the moment, we know that the knowledge represented in BERT does not necessarily get used in downstream tasks (Kovaleva et al., 2019). As starting points for work in this direction, we also have other head ablation studies (Voita et al., 2019;Michel et al., 2019) and studies of how BERT behaves in reading comprehension task (van Aken et al., 2019; Arkhangelskaia and Dutta, 2019).']","BERTology has clearly come a long way, but it is fair to say we still have more questions than answers about how BERT works. In this section, we list what we believe to be the most promising directions for further research, together with the starting points that we already have.

Benchmarks that require verbal reasoning. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020). As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.

Developing methods to ""teach"" reasoning. While the community had success extracting knowledge from large pre-trained models, they often fail if any reasoning needs to be performed on top of the facts they possess (see subsection 4.3). For instance,  propose a method to ""teach"" BERT quantification, conditionals, comparatives, and boolean coordination.

Learning what happens at inference time.

Most of the BERT analysis papers focused on different probes of the model, but we know much less about what knowledge actually gets used. At the moment, we know that the knowledge represented in BERT does not necessarily get used in downstream tasks (Kovaleva et al., 2019). As starting points for work in this direction, we also have other head ablation studies (Voita et al., 2019;Michel et al., 2019) and studies of how BERT behaves in reading comprehension task (van Aken et al., 2019; Arkhangelskaia and Dutta, 2019).","(p16.0) BERTology has clearly come a long way, but it is fair to say we still have more questions than answers about how BERT works. In this section, we list what we believe to be the most promising directions for further research, together with the starting points that we already have.

(p16.1) Benchmarks that require verbal reasoning. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020). As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.

(p16.2) Developing methods to ""teach"" reasoning. While the community had success extracting knowledge from large pre-trained models, they often fail if any reasoning needs to be performed on top of the facts they possess (see subsection 4.3). For instance,  propose a method to ""teach"" BERT quantification, conditionals, comparatives, and boolean coordination.

(p16.3) Learning what happens at inference time.

(p16.4) Most of the BERT analysis papers focused on different probes of the model, but we know much less about what knowledge actually gets used. At the moment, we know that the knowledge represented in BERT does not necessarily get used in downstream tasks (Kovaleva et al., 2019). As starting points for work in this direction, we also have other head ablation studies (Voita et al., 2019;Michel et al., 2019) and studies of how BERT behaves in reading comprehension task (van Aken et al., 2019; Arkhangelskaia and Dutta, 2019).","[[], ['b41', 'b35', 'b77', 'b45'], [], [], ['b58', 'b20', 'b9']]","[[], ['b41', 'b35', 'b77', 'b45'], [], [], ['b58', 'b20', 'b9']]",7,"1. BERTology has clearly come a long way, but it is fair to say we still have more questions than answers about how BERT works.
2. In this section, we list what we believe to be the most promising directions for further research, together with the starting points that we already have.
3. Benchmarks that require verbal reasoning.
4. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems.
5. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020).
6. As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it.
7. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.
8. Developing methods to ""teach"" reasoning.
9. While the community had success extracting knowledge from large pre-trained models, they often fail if any reasoning needs to be performed on top of the facts they possess (see subsection 4.3).
10. For instance,  propose a method to ""teach"" BERT quantification, conditionals, comparatives, and boolean coordination.
11. Learning what happens at inference time.
12. Most of the BERT analysis papers focused on different probes of the model, but we know much less about what knowledge actually gets used.
13. At the moment, we know that the knowledge represented in BERT does not necessarily get used in downstream tasks (Kovaleva et al., 2019).
14. As starting points for work in this direction, we also have other head ablation studies (Voita et al., 2019;Michel et al., 2019) and studies of how BERT behaves in reading comprehension task (van Aken et al., 2019; Arkhangelskaia and Dutta, 2019).","How can the NLP community address the limitations of BERT's verbal reasoning abilities as highlighted by recent analysis papers?
What strategies have been proposed to ""teach"" BERT more complex reasoning skills such as quantification and boolean coordination?
What are the challenges in developing datasets that can better evaluate the verbal reasoning capabilities of models like BERT?
In what ways does the knowledge represented in BERT fail to be utilized in downstream tasks, according to recent studies?
How do head ablation studies and reading comprehension tasks contribute to our understanding of BERT's behavior at inference time?","1. How can the NLP community address the limitations of BERT's verbal reasoning abilities as highlighted by recent analysis papers?
2. What strategies have been proposed to ""teach"" BERT more complex reasoning skills such as quantification and boolean coordination?
3. What are the challenges in developing datasets that can better evaluate the verbal reasoning capabilities of models like BERT?
4. In what ways does the knowledge represented in BERT fail to be utilized in downstream tasks, according to recent studies?
5. How do head ablation studies and reading comprehension tasks contribute to our understanding of BERT's behavior at inference time?",None,,"Questions:

1. How can the NLP community address the limitations of BERT's verbal reasoning abilities as highlighted by recent analysis papers?
2. What strategies have been proposed to ""teach"" BERT more complex reasoning skills such as quantification and boolean coordination?
3. What are the challenges in developing datasets that can better evaluate the verbal reasoning capabilities of models like BERT?
4. In what ways does the knowledge represented in BERT fail to be utilized in downstream tasks, according to recent studies?
5. How do head ablation studies and reading comprehension tasks contribute to our understanding of BERT's behavior at inference time?

Answer:

(p16.0) BERTology has clearly come a long way, but it is fair to say we still have more questions than answers about how BERT works. In this section, we list what we believe to be the most promising directions for further research, together with the starting points that we already have.

(p16.1) Benchmarks that require verbal reasoning. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020). As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.

(p16.2) Developing methods to ""teach"" reasoning. While the community had success extracting knowledge from large pre-trained models, they often fail if any reasoning needs to be performed on top of the facts they possess (see subsection 4.3). For instance,  propose a method to ""teach"" BERT quantification, conditionals, comparatives, and boolean coordination.

(p16.3) Learning what happens at inference time.

(p16.4) Most of the BERT analysis papers focused on different probes of the model, but we know much less about what knowledge actually gets used. At the moment, we know that the knowledge represented in BERT does not necessarily get used in downstream tasks (Kovaleva et al., 2019). As starting points for work in this direction, we also have other head ablation studies (Voita et al., 2019;Michel et al., 2019) and studies of how BERT behaves in reading comprehension task (van Aken et al., 2019; Arkhangelskaia and Dutta, 2019).","Questions:



Answer:

(p16.0) BERTology has clearly come a long way, but it is fair to say we still have more questions than answers about how BERT works. In this section, we list what we believe to be the most promising directions for further research, together with the starting points that we already have.

(p16.1) Benchmarks that require verbal reasoning. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020). As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.

(p16.2) Developing methods to ""teach"" reasoning. While the community had success extracting knowledge from large pre-trained models, they often fail if any reasoning needs to be performed on top of the facts they possess (see subsection 4.3). For instance,  propose a method to ""teach"" BERT quantification, conditionals, comparatives, and boolean coordination.

(p16.3) Learning what happens at inference time.

(p16.4) Most of the BERT analysis papers focused on different probes of the model, but we know much less about what knowledge actually gets used. At the moment, we know that the knowledge represented in BERT does not necessarily get used in downstream tasks (Kovaleva et al., 2019). As starting points for work in this direction, we also have other head ablation studies (Voita et al., 2019;Michel et al., 2019) and studies of how BERT behaves in reading comprehension task (van Aken et al., 2019; Arkhangelskaia and Dutta, 2019)."
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s7,Linear Classifiers,"['p7.0', 'p7.1', 'p7.2', 'p7.3']","[""The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept."", ""Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue."", 'Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.', 'Limitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.']","The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.

Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.

Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.

Limitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.","(p7.0) The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.

(p7.1) Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.

(p7.2) Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.

(p7.3) Limitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.","[['b21', 'b38'], ['b15', 'b55'], [], []]","[['b21', 'b38'], ['b15', 'b55'], [], []]",4,"1. The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed.
2. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept.
3. The regularization of the classifier directly effects the weights and therefore the ranking of neurons.
4. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero.
5. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features.
6. This translates to discovering group neurons that are jointly responsible for a concept.
7. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.
8. Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018).
9. Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization.
10. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.
11. suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.
12. Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution.
13. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons.
14. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria.
15. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.
16. Limitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.","How does the choice of regularization technique in linear classifiers affect the interpretation of neuron importance for a given concept?
What are the implications of using L1 regularization versus L2 regularization in the context of neuron-level interpretation of deep NLP models?
How does ElasticNet regularization facilitate the understanding of neuron contributions to concept learning in deep NLP models?
What challenges arise from the variance patterns in neurons across different layers when interpreting deep NLP models, and how can z-normalization mitigate these issues?
What are the limitations of using a Gaussian classifier for neuron-level interpretation in deep NLP models, and how does the assumption of a Gaussian distribution impact the analysis?","1. How does the choice of regularization technique in linear classifiers affect the interpretation of neuron importance for a given concept?
2. What are the implications of using L1 regularization versus L2 regularization in the context of neuron-level interpretation of deep NLP models?
3. How does ElasticNet regularization facilitate the understanding of neuron contributions to concept learning in deep NLP models?
4. What challenges arise from the variance patterns in neurons across different layers when interpreting deep NLP models, and how can z-normalization mitigate these issues?
5. What are the limitations of using a Gaussian classifier for neuron-level interpretation in deep NLP models, and how does the assumption of a Gaussian distribution impact the analysis?",None,,"Questions:

1. How does the choice of regularization technique in linear classifiers affect the interpretation of neuron importance for a given concept?
2. What are the implications of using L1 regularization versus L2 regularization in the context of neuron-level interpretation of deep NLP models?
3. How does ElasticNet regularization facilitate the understanding of neuron contributions to concept learning in deep NLP models?
4. What challenges arise from the variance patterns in neurons across different layers when interpreting deep NLP models, and how can z-normalization mitigate these issues?
5. What are the limitations of using a Gaussian classifier for neuron-level interpretation in deep NLP models, and how does the assumption of a Gaussian distribution impact the analysis?

Answer:

(p7.0) The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.

(p7.1) Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.

(p7.2) Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.

(p7.3) Limitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.","Questions:



Answer:

(p7.0) The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.

(p7.1) Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.

(p7.2) Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.

(p7.3) Limitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts."
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s8,Causation-based methods,"['p8.0', 'p8.1', 'p8.2', 'p8.3', 'p8.4']","[""The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction."", ""Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons."", 'Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.', 'Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.', 'Limitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.']","The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction.

Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.

Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.

Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.

Limitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.","(p8.0) The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction.

(p8.1) Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.

(p8.2) Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.

(p8.3) Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.

(p8.4) Limitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.","[[], ['b21', 'b22'], [None, 'b56', 'b9'], ['b46', 'b1', None, 'b26', 'b49'], [None]]","[[], ['b21', 'b22'], [None, 'b56', 'b9'], ['b46', 'b1', None, 'b26', 'b49'], [None]]",11,"1. The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts.
2. They do not inherently reflect their importance towards the model's performance.
3. Causation-based methods identify neurons with respect to model's prediction.
4. Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value.
5. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance.
6. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised).
7. The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a).
8. The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019).
9. Here, the output class serves as the concept against which we want to find the salient neurons.
10. Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007).
11. Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation .
12. Nevertheless all these approaches are approximations and may incur search errors.
13. Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018).
14. Dai et al. (supervised)2 used an attribution-based method to identify salient neurons with respect to a relational fact.
15. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (supervised)0 to identify top neu-rons that express a relational fact.
16. The work of Dai et al. (supervised)2 shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.
17. Limitation The attribution-based methods highlight salient neurons with respect to a prediction.
18. What concepts these salient neurons have learned is unknown.
19. Dai et al. (supervised)2 worked around this by limiting their study to model classes where each class serves as a concept.
20. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.","What is the role of ablation in understanding the impact of individual neurons on a model's performance?
How do causation-based methods identify neurons that are crucial for a model's predictions?
What are the limitations of using ablation to identify groups of salient neurons and how have researchers attempted to overcome these challenges?
In what way do attribution-based methods contribute to the identification of salient neurons, and what limitations do they face?
How have researchers used integrated gradients to uncover neurons that encode specific relational facts within deep NLP models?","1. What is the role of ablation in understanding the impact of individual neurons on a model's performance?
2. How do causation-based methods identify neurons that are crucial for a model's predictions?
3. What are the limitations of using ablation to identify groups of salient neurons and how have researchers attempted to overcome these challenges?
4. In what way do attribution-based methods contribute to the identification of salient neurons, and what limitations do they face?
5. How have researchers used integrated gradients to uncover neurons that encode specific relational facts within deep NLP models?",None,,"Questions:

1. What is the role of ablation in understanding the impact of individual neurons on a model's performance?
2. How do causation-based methods identify neurons that are crucial for a model's predictions?
3. What are the limitations of using ablation to identify groups of salient neurons and how have researchers attempted to overcome these challenges?
4. In what way do attribution-based methods contribute to the identification of salient neurons, and what limitations do they face?
5. How have researchers used integrated gradients to uncover neurons that encode specific relational facts within deep NLP models?

Answer:

(p8.0) The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction.

(p8.1) Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.

(p8.2) Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.

(p8.3) Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.

(p8.4) Limitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.","Questions:



Answer:

(p8.0) The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction.

(p8.1) Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.

(p8.2) Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.

(p8.3) Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.

(p8.4) Limitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept."
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s9,Miscellaneous Methods,"['p9.0', 'p9.1', 'p9.2', 'p9.3', 'p9.4', 'p9.5', 'p9.6', 'p9.7', 'p9.8']","['In this section, we cover a diverse set of methods that do not fit in the above defined categories.', ""Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons."", 'Limitation Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation. A thorough evaluation is necessary to know its true potential and efficacy in NLP.', 'Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.', 'Limitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into. Moreover, the scope of the method is limited to local interpretation.', 'Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.', 'Limitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.', 'Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it. The search involves identifying neurons that behave similarly across the models. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.', 'Limitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons. They can nevertheless be useful in tandem with the other interpretation methods. For example Dalvi et al. (2019) intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models.']","In this section, we cover a diverse set of methods that do not fit in the above defined categories.

Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.

Limitation Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation. A thorough evaluation is necessary to know its true potential and efficacy in NLP.

Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.

Limitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into. Moreover, the scope of the method is limited to local interpretation.

Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.

Limitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.

Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it. The search involves identifying neurons that behave similarly across the models. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.

Limitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons. They can nevertheless be useful in tandem with the other interpretation methods. For example Dalvi et al. (2019) intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models.","(p9.0) In this section, we cover a diverse set of methods that do not fit in the above defined categories.

(p9.1) Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.

(p9.2) Limitation Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation. A thorough evaluation is necessary to know its true potential and efficacy in NLP.

(p9.3) Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.

(p9.4) Limitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into. Moreover, the scope of the method is limited to local interpretation.

(p9.5) Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.

(p9.6) Limitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.

(p9.7) Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it. The search involves identifying neurons that behave similarly across the models. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.

(p9.8) Limitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons. They can nevertheless be useful in tandem with the other interpretation methods. For example Dalvi et al. (2019) intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models.","[[], ['b37', 'b19', 'b6'], [], ['b35'], [], ['b28', 'b29'], [], ['b39'], [None]]","[[], ['b37', 'b19', 'b6'], [], ['b35'], [], ['b28', 'b29'], [], ['b39'], [None]]",8,"1. In this section, we cover a diverse set of methods that do not fit in the above defined categories.
2. Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations.
3. It is possible that a neuron represents a diverse concept which is not featured in the corpus.
4. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations.
5. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role.
6. Corpus generation has been widely explored in Computer Vision.
7. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron.
8. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs.
9. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.Limitation
10. Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation.
11. A thorough evaluation is necessary to know its true potential and efficacy in NLP.
12. Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function.
13. Given a model, the activations of an input sentence form a matrix.
14. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept.
15. MF is a local interpretation method.
16. It is commonly used in analyzing vision models (Olah et al., 2018).
17. We could not find any research using MF on the NLP models.
18. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.
19. Limitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons.
20. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into.
21. Moreover, the scope of the method is limited to local interpretation.
22. Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion.
23. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster.
24. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.
25. aimed at identifying redundant neurons in the network.
26. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them.
27. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.
28. Limitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically.
29. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.
30. Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it.
31. The search involves identifying neurons that behave similarly across the models.
32. Bau et al. (2018)1 used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models.
33. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model.
34. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (2018)0 to capture information that may be distributed in fewer dimensions than the whole representation.
35. Limitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons.
36. They can nevertheless be useful in tandem with the other interpretation methods.
37. For example Dalvi et al. (2018)1 intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models.","How does the Corpus Generation method facilitate neuron-level interpretation in NLP models?
What are the limitations of using Corpus Generation for interpreting neuron activations in NLP, and how do they affect its efficacy?
How does Matrix Factorization contribute to the local interpretation of neuron activations in NLP models?
In what ways do clustering methods aid in the analysis of neuron groups within neural networks?
What is the principle behind Multi-model Search in identifying salient information across different models, and what limitations does it face?","1. How does the Corpus Generation method facilitate neuron-level interpretation in NLP models?
2. What are the limitations of using Corpus Generation for interpreting neuron activations in NLP, and how do they affect its efficacy?
3. How does Matrix Factorization contribute to the local interpretation of neuron activations in NLP models?
4. In what ways do clustering methods aid in the analysis of neuron groups within neural networks?
5. What is the principle behind Multi-model Search in identifying salient information across different models, and what limitations does it face?",None,,"Questions:

1. How does the Corpus Generation method facilitate neuron-level interpretation in NLP models?
2. What are the limitations of using Corpus Generation for interpreting neuron activations in NLP, and how do they affect its efficacy?
3. How does Matrix Factorization contribute to the local interpretation of neuron activations in NLP models?
4. In what ways do clustering methods aid in the analysis of neuron groups within neural networks?
5. What is the principle behind Multi-model Search in identifying salient information across different models, and what limitations does it face?

Answer:

(p9.0) In this section, we cover a diverse set of methods that do not fit in the above defined categories.

(p9.1) Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.

(p9.2) Limitation Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation. A thorough evaluation is necessary to know its true potential and efficacy in NLP.

(p9.3) Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.

(p9.4) Limitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into. Moreover, the scope of the method is limited to local interpretation.

(p9.5) Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.

(p9.6) Limitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.

(p9.7) Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it. The search involves identifying neurons that behave similarly across the models. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.

(p9.8) Limitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons. They can nevertheless be useful in tandem with the other interpretation methods. For example Dalvi et al. (2019) intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models.","Questions:



Answer:

(p9.0) In this section, we cover a diverse set of methods that do not fit in the above defined categories.

(p9.1) Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.

(p9.2) Limitation Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation. A thorough evaluation is necessary to know its true potential and efficacy in NLP.

(p9.3) Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.

(p9.4) Limitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into. Moreover, the scope of the method is limited to local interpretation.

(p9.5) Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.

(p9.6) Limitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.

(p9.7) Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it. The search involves identifying neurons that behave similarly across the models. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.

(p9.8) Limitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons. They can nevertheless be useful in tandem with the other interpretation methods. For example Dalvi et al. (2019) intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models."
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s18,Lexical Concepts,"['p18.0', 'p18.1']","['Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.', 'Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.']","Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.

Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.","(p18.0) Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.

(p18.1) Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.","[[], ['b34', 'b22', 'b37', 'b19', 'b20']]","[[], ['b34', 'b22', 'b37', 'b19', 'b20']]",5,"1. Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.
2. Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end.
3. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment.
4. For example ""I like this movie a lot"" or ""the movie is incredibly good"".
5. Similarly they discovered neurons that captured ""negation"".
6. Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained.
7. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task.
8. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"".
9. Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc.
10. They also found neurons that learn phrasal concepts.
11. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation.
12. They provided finer interpretation of the neurons by generating synthetic instances.
13. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.","How do deep NLP models utilize neurons to identify the position of words within a sentence?
What is the significance of intensification neurons in the context of sentiment analysis?
How are neurons capable of capturing groups of related concepts in tasks involving multi-modal image captioning?
In what ways do neurons encode lexical concepts pertaining to legislative terminology?
How can the concept search method, when combined with Corpus Generation, provide a more detailed interpretation of neuron functions?","1. How do deep NLP models utilize neurons to identify the position of words within a sentence?
2. What is the significance of intensification neurons in the context of sentiment analysis?
3. How are neurons capable of capturing groups of related concepts in tasks involving multi-modal image captioning?
4. In what ways do neurons encode lexical concepts pertaining to legislative terminology?
5. How can the concept search method, when combined with Corpus Generation, provide a more detailed interpretation of neuron functions?",None,,"Questions:

1. How do deep NLP models utilize neurons to identify the position of words within a sentence?
2. What is the significance of intensification neurons in the context of sentiment analysis?
3. How are neurons capable of capturing groups of related concepts in tasks involving multi-modal image captioning?
4. In what ways do neurons encode lexical concepts pertaining to legislative terminology?
5. How can the concept search method, when combined with Corpus Generation, provide a more detailed interpretation of neuron functions?

Answer:

(p18.0) Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.

(p18.1) Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.","Questions:



Answer:

(p18.0) Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.

(p18.1) Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron."
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s20,Linguistic Concepts,"['p20.0', 'p20.1', 'p20.2', 'p20.3']","[""A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018). § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012). Below we discuss major findings along this line of work:"", 'Neurons specialize in core linguistic concepts Dalvi et al. (2019) in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.', 'Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.', 'Neurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.']","A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018). § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012). Below we discuss major findings along this line of work:

Neurons specialize in core linguistic concepts Dalvi et al. (2019) in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.

Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.

Neurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.","(p20.0) A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018). § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012). Below we discuss major findings along this line of work:

(p20.1) Neurons specialize in core linguistic concepts Dalvi et al. (2019) in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.

(p20.2) Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.

(p20.3) Neurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.","[['b18', 'b51', 'b27'], [None], ['b54', None], ['b21', 'b42', 'b20', 'b34']]","[['b18', 'b51', 'b27'], [None], ['b54', None], ['b21', 'b42', 'b20', 'b34']]",10,"1. A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018).
2. § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012).
3. Below we discuss major findings along this line of work:Neurons specialize in core linguistic concepts Dalvi et al. (Vauquois, 1968;Jones et al., 2012)2 in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept.
4. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.
5. Neurons exhibit monosemous and polysemous behavior.
6. Xin et al. (Vauquois, 1968;Jones et al., 2012)2 found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis.
7. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc.
8. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"".
9. concepts. Suau et al. (2020) discovered neurons that capture different senses of a word.
10. Similarly, Bau et al. (Vauquois, 1968;Jones et al., 2012)2 found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.
11. Neurons capture syntactic concepts and complex semantic concepts.
12. Lakretz et al. (Vauquois, 1968;Jones et al., 2012)2 discovered neurons that capture subject-verb agreement within LSTM gates.
13. Karpathy et al. (Vauquois, 1968;Jones et al., 2012)1 also found neurons that activate within quotes and brackets capturing long-range dependency.
14. Na et al. (Vauquois, 1968;Jones et al., 2012)2 aligned neurons with syntactic parses to show that neurons learn syntactic phrases.
15. Seyffarth et al. (Vauquois, 1968;Jones et al., 2012)3 analyzed complex semantic properties underlying a given sentence.","How do LSTM-based NMT models' neurons specialize in capturing different linguistic concepts?
What distinguishes the neuronal distribution for closed class concepts from that for open class concepts in deep NLP models?
How do neurons demonstrate monosemous versus polysemous behavior in the context of deep learning for NLP?
In what ways do neurons within deep NLP models capture syntactic and complex semantic concepts?
What function do switch neurons serve in the context of verb tense recognition in deep NLP models?","1. How do LSTM-based NMT models' neurons specialize in capturing different linguistic concepts?
2. What distinguishes the neuronal distribution for closed class concepts from that for open class concepts in deep NLP models?
3. How do neurons demonstrate monosemous versus polysemous behavior in the context of deep learning for NLP?
4. In what ways do neurons within deep NLP models capture syntactic and complex semantic concepts?
5. What function do switch neurons serve in the context of verb tense recognition in deep NLP models?","# How do neurons in deep NLP models specialize in capturing linguistic concepts?
  - Neurons specialize in core linguistic concepts such as nouns, verb forms, numbers, and articles, with the number of neurons responsible for a concept varying based on the nature of the concept. Closed class concepts are localized to fewer neurons, whereas open class concepts are distributed among a larger number of neurons.
  - Neurons exhibit monosemous and polysemous behavior, with some neurons exclusive to a single concept and others capturing several concepts. This includes the differentiation between closed class concepts, which do not evolve with language, and open class concepts, which grow as the language evolves.
  - Neurons capture syntactic concepts and complex semantic concepts, including subject-verb agreement, long-range dependencies within quotes and brackets, alignment with syntactic parses, and analysis of complex semantic properties underlying sentences.",1. How do neurons in deep NLP models specialize in capturing linguistic concepts?,"Questions:

1. How do LSTM-based NMT models' neurons specialize in capturing different linguistic concepts?
2. What distinguishes the neuronal distribution for closed class concepts from that for open class concepts in deep NLP models?
3. How do neurons demonstrate monosemous versus polysemous behavior in the context of deep learning for NLP?
4. In what ways do neurons within deep NLP models capture syntactic and complex semantic concepts?
5. What function do switch neurons serve in the context of verb tense recognition in deep NLP models?

Answer:

(p20.0) A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018). § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012). Below we discuss major findings along this line of work:

(p20.1) Neurons specialize in core linguistic concepts Dalvi et al. (2019) in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.

(p20.2) Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.

(p20.3) Neurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.","Questions:

1. How do neurons in deep NLP models specialize in capturing linguistic concepts?

Answer:

(p20.0) A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018). § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012). Below we discuss major findings along this line of work:

(p20.1) Neurons specialize in core linguistic concepts Dalvi et al. (2019) in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.

(p20.2) Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.

(p20.3) Neurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence."
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd,s32,Open Issues and Future Directions,"['p32.0', 'p32.1', 'p32.2', 'p32.3', 'p32.4', 'p32.5', 'p32.6']","['In the following section, we discuss several open issues and limitations related to methods, evaluation and datasets. Moreover, we provide potential future directions vital to the progress of neuron and model interpretation.', ""• DNNs are distributed in nature which encourages groups of neurons to work together to learn a concept. The current analysis methods, at large, ignore interaction between neurons while discovering neurons with respect to a concept. Trying all possible combination of neurons is a computationally intractable problem. A linear classifier using ElasticNet regularization (Dalvi et al., 2019) considers grouping of features during training -however, it's effectiveness in handling grouped neurons has not been empirically validated. Evolutionary algorithms || do not make any assumption of the underline distribution of the features and they have been effectively used for feature selection of multivariate fea-|| https://en.wikipedia.org/wiki/ Evolutionary_algorithm tures. Exploring them for neuron selection is a promising research direction to probe towards latent concepts in these models."", '• A large number of interpretation studies rely on human-defined linguistic concepts to probe a model. It is possible that the models do not strictly adhere to the humandefined concepts and learn novel concepts about the language. This results in an incorrect or incomplete analysis. Several researchers (Michael et al., 2020; made strides in this direction by analyzing hidden structures in the input representations in an unsupervised manner. They discovered existence of novel structures not captured in the human defined categories.  also proposed BERT ConceptNet, a manual annotation of the latent concepts in BERT. Introducing similar datasets across other models enables model-centric interpretation, and is a promising research direction.', ""• While a lot of work has been done on analyzing how knowledge is encoded within the learned representations, the question whether it is used by the model during prediction is a less explored area (Feder et al., 2021;Elazar et al., 2021). Ablation and knowledge attribution methods are two neuron interpretation methods that intrinsically use causal relation to select concept neurons. A few other studies evaluated the causal relation of the selected concept neurons via ablation or by clamping their activation values (Bau et al., 2019;Suau et al., 2020) and observed the change in model's prediction. However, most of the studies do not take into account the causal relation as part of the method or the evaluation of their method. The causal relation with respect to concept neurons is important to understand their importance to overall prediction and it leads way towards practical applications such as debiasing, model distillation and domain adaptation."", '• The work on neuron interpretation lacks standard evaluation benchmarks, and therefore studies conducted on identical models are not comparable. For example, there exists no gold annotation of neurons with respect to a certain dataset or a class. The curation of standard evaluation benchmarks is an essential step towards improving methods of interpretation of deep neural network models.', '• The neuron analysis methods vary in their theoretical foundations as well as the perspective they aim to capture with respect to a given concept. This results in a selection of neurons that may not strictly align across all methods. For example, Visualization, Neuron Search and Corpus Search discover neurons that are highly focused on a specific task (like ""less"" suffix or POS ""TO"" concepts), while Probing-based methods discover ranking of neurons that highlight grouping behavior within the neurons targeting broad concepts like POS ""Nouns"". Therefore, the choice of which neuron interpretation method to use is not straightforward and depends on various factors such as the nature of the concept to investigate, the availability of supervised data for the concept of interest etc. Apart from these high-level guiding principles, a thorough comparison of methods with respect to the nature of the concept of interest is needed to fully understand the strengths and weaknesses of each approach. Antverg and Belinkov (2022) is one such effort in this direction that compares three neuron interpretation methods.', ""• Neuron-level interpretation opens door for a number of applications useful for the successful deployment of DNN systems (Section 6). However, most of the research conducted in this direction is preliminary. For example, there are many open research questions in controlling system's behaviour using neurons such as: i) are all concepts manipulatable? ii) how to identify neurons that can be controlled to change the output? iii) is high distributiveness a hindrance for controlling model's behavior? iv) and whether disentangled (Bengio et al., 2012) and sparse models (Frankle and Carbin, 2019) may serve as a better alternate on this front? Addressing these questions will enable a more reliable control of the deep NLP models and entail numerous applications such as removing bias and adapting the system to novel domains.""]","In the following section, we discuss several open issues and limitations related to methods, evaluation and datasets. Moreover, we provide potential future directions vital to the progress of neuron and model interpretation.

• DNNs are distributed in nature which encourages groups of neurons to work together to learn a concept. The current analysis methods, at large, ignore interaction between neurons while discovering neurons with respect to a concept. Trying all possible combination of neurons is a computationally intractable problem. A linear classifier using ElasticNet regularization (Dalvi et al., 2019) considers grouping of features during training -however, it's effectiveness in handling grouped neurons has not been empirically validated. Evolutionary algorithms || do not make any assumption of the underline distribution of the features and they have been effectively used for feature selection of multivariate fea-|| https://en.wikipedia.org/wiki/ Evolutionary_algorithm tures. Exploring them for neuron selection is a promising research direction to probe towards latent concepts in these models.

• A large number of interpretation studies rely on human-defined linguistic concepts to probe a model. It is possible that the models do not strictly adhere to the humandefined concepts and learn novel concepts about the language. This results in an incorrect or incomplete analysis. Several researchers (Michael et al., 2020; made strides in this direction by analyzing hidden structures in the input representations in an unsupervised manner. They discovered existence of novel structures not captured in the human defined categories.  also proposed BERT ConceptNet, a manual annotation of the latent concepts in BERT. Introducing similar datasets across other models enables model-centric interpretation, and is a promising research direction.

• While a lot of work has been done on analyzing how knowledge is encoded within the learned representations, the question whether it is used by the model during prediction is a less explored area (Feder et al., 2021;Elazar et al., 2021). Ablation and knowledge attribution methods are two neuron interpretation methods that intrinsically use causal relation to select concept neurons. A few other studies evaluated the causal relation of the selected concept neurons via ablation or by clamping their activation values (Bau et al., 2019;Suau et al., 2020) and observed the change in model's prediction. However, most of the studies do not take into account the causal relation as part of the method or the evaluation of their method. The causal relation with respect to concept neurons is important to understand their importance to overall prediction and it leads way towards practical applications such as debiasing, model distillation and domain adaptation.

• The work on neuron interpretation lacks standard evaluation benchmarks, and therefore studies conducted on identical models are not comparable. For example, there exists no gold annotation of neurons with respect to a certain dataset or a class. The curation of standard evaluation benchmarks is an essential step towards improving methods of interpretation of deep neural network models.

• The neuron analysis methods vary in their theoretical foundations as well as the perspective they aim to capture with respect to a given concept. This results in a selection of neurons that may not strictly align across all methods. For example, Visualization, Neuron Search and Corpus Search discover neurons that are highly focused on a specific task (like ""less"" suffix or POS ""TO"" concepts), while Probing-based methods discover ranking of neurons that highlight grouping behavior within the neurons targeting broad concepts like POS ""Nouns"". Therefore, the choice of which neuron interpretation method to use is not straightforward and depends on various factors such as the nature of the concept to investigate, the availability of supervised data for the concept of interest etc. Apart from these high-level guiding principles, a thorough comparison of methods with respect to the nature of the concept of interest is needed to fully understand the strengths and weaknesses of each approach. Antverg and Belinkov (2022) is one such effort in this direction that compares three neuron interpretation methods.

• Neuron-level interpretation opens door for a number of applications useful for the successful deployment of DNN systems (Section 6). However, most of the research conducted in this direction is preliminary. For example, there are many open research questions in controlling system's behaviour using neurons such as: i) are all concepts manipulatable? ii) how to identify neurons that can be controlled to change the output? iii) is high distributiveness a hindrance for controlling model's behavior? iv) and whether disentangled (Bengio et al., 2012) and sparse models (Frankle and Carbin, 2019) may serve as a better alternate on this front? Addressing these questions will enable a more reliable control of the deep NLP models and entail numerous applications such as removing bias and adapting the system to novel domains.","(p32.0) In the following section, we discuss several open issues and limitations related to methods, evaluation and datasets. Moreover, we provide potential future directions vital to the progress of neuron and model interpretation.

(p32.1) • DNNs are distributed in nature which encourages groups of neurons to work together to learn a concept. The current analysis methods, at large, ignore interaction between neurons while discovering neurons with respect to a concept. Trying all possible combination of neurons is a computationally intractable problem. A linear classifier using ElasticNet regularization (Dalvi et al., 2019) considers grouping of features during training -however, it's effectiveness in handling grouped neurons has not been empirically validated. Evolutionary algorithms || do not make any assumption of the underline distribution of the features and they have been effectively used for feature selection of multivariate fea-|| https://en.wikipedia.org/wiki/ Evolutionary_algorithm tures. Exploring them for neuron selection is a promising research direction to probe towards latent concepts in these models.

(p32.2) • A large number of interpretation studies rely on human-defined linguistic concepts to probe a model. It is possible that the models do not strictly adhere to the humandefined concepts and learn novel concepts about the language. This results in an incorrect or incomplete analysis. Several researchers (Michael et al., 2020; made strides in this direction by analyzing hidden structures in the input representations in an unsupervised manner. They discovered existence of novel structures not captured in the human defined categories.  also proposed BERT ConceptNet, a manual annotation of the latent concepts in BERT. Introducing similar datasets across other models enables model-centric interpretation, and is a promising research direction.

(p32.3) • While a lot of work has been done on analyzing how knowledge is encoded within the learned representations, the question whether it is used by the model during prediction is a less explored area (Feder et al., 2021;Elazar et al., 2021). Ablation and knowledge attribution methods are two neuron interpretation methods that intrinsically use causal relation to select concept neurons. A few other studies evaluated the causal relation of the selected concept neurons via ablation or by clamping their activation values (Bau et al., 2019;Suau et al., 2020) and observed the change in model's prediction. However, most of the studies do not take into account the causal relation as part of the method or the evaluation of their method. The causal relation with respect to concept neurons is important to understand their importance to overall prediction and it leads way towards practical applications such as debiasing, model distillation and domain adaptation.

(p32.4) • The work on neuron interpretation lacks standard evaluation benchmarks, and therefore studies conducted on identical models are not comparable. For example, there exists no gold annotation of neurons with respect to a certain dataset or a class. The curation of standard evaluation benchmarks is an essential step towards improving methods of interpretation of deep neural network models.

(p32.5) • The neuron analysis methods vary in their theoretical foundations as well as the perspective they aim to capture with respect to a given concept. This results in a selection of neurons that may not strictly align across all methods. For example, Visualization, Neuron Search and Corpus Search discover neurons that are highly focused on a specific task (like ""less"" suffix or POS ""TO"" concepts), while Probing-based methods discover ranking of neurons that highlight grouping behavior within the neurons targeting broad concepts like POS ""Nouns"". Therefore, the choice of which neuron interpretation method to use is not straightforward and depends on various factors such as the nature of the concept to investigate, the availability of supervised data for the concept of interest etc. Apart from these high-level guiding principles, a thorough comparison of methods with respect to the nature of the concept of interest is needed to fully understand the strengths and weaknesses of each approach. Antverg and Belinkov (2022) is one such effort in this direction that compares three neuron interpretation methods.

(p32.6) • Neuron-level interpretation opens door for a number of applications useful for the successful deployment of DNN systems (Section 6). However, most of the research conducted in this direction is preliminary. For example, there are many open research questions in controlling system's behaviour using neurons such as: i) are all concepts manipulatable? ii) how to identify neurons that can be controlled to change the output? iii) is high distributiveness a hindrance for controlling model's behavior? iv) and whether disentangled (Bengio et al., 2012) and sparse models (Frankle and Carbin, 2019) may serve as a better alternate on this front? Addressing these questions will enable a more reliable control of the deep NLP models and entail numerous applications such as removing bias and adapting the system to novel domains.","[[], [], ['b30'], ['b5', 'b8', None], [], [], [None, 'b10']]","[[], [], ['b30'], ['b5', 'b8', None], [], [], [None, 'b10']]",6,"1. In the following section, we discuss several open issues and limitations related to methods, evaluation and datasets.
2. Moreover, we provide potential future directions vital to the progress of neuron and model interpretation.
3. DNNs are distributed in nature which encourages groups of neurons to work together to learn a concept.
4. The current analysis methods, at large, ignore interaction between neurons while discovering neurons with respect to a concept.
5. Trying all possible combination of neurons is a computationally intractable problem.
6. A linear classifier using ElasticNet regularization (Dalvi et al., 2019) considers grouping of features during training -however, it's effectiveness in handling grouped neurons has not been empirically validated.
7. Evolutionary algorithms || do not make any assumption of the underline distribution of the features and they have been effectively used for feature selection of multivariate
8. fea-|| https://en.wikipedia.org/wiki/ Evolutionary_algorithm tures.
9. Exploring them for neuron selection is a promising research direction to probe towards latent concepts in these models.
10. A large number of interpretation studies rely on human-defined linguistic concepts to probe a model.
11. It is possible that the models do not strictly adhere to the humandefined concepts and learn novel concepts about the language.
12. This results in an incorrect or incomplete analysis.
13. Several researchers (Michael et al., 2020; made strides in this direction by analyzing hidden structures in the input representations in an unsupervised manner.
14. They discovered existence of novel structures not captured in the human defined categories.
15. also proposed BERT ConceptNet, a manual annotation of the latent concepts in BERT.
16. Introducing similar datasets across other models enables model-centric interpretation, and is a promising research direction.
17. While a lot of work has been done on analyzing how knowledge is encoded within the learned representations, the question whether it is used by the model during prediction is a less explored area (Feder et al., 2021;Elazar et al., 2021).
18. Ablation and knowledge attribution methods are two neuron interpretation methods that intrinsically use causal relation to select concept neurons.
19. A few other studies evaluated the causal relation of the selected concept neurons via ablation or by clamping their activation values (Bau et al., 2019;Suau et al., 2020) and observed the change in model's prediction.
20. However, most of the studies do not take into account the causal relation as part of the method or the evaluation of their method.
21. The causal relation with respect to concept neurons is important to understand their importance to overall prediction and it leads way towards practical applications such as debiasing, model distillation and domain adaptation.
22. The work on neuron interpretation lacks standard evaluation benchmarks, and therefore studies conducted on identical models are not comparable.
23. For example, there exists no gold annotation of neurons with respect to a certain dataset or a class.
24. The curation of standard evaluation benchmarks is an essential step towards improving methods of interpretation of deep neural network models.
25. The neuron analysis methods vary in their theoretical foundations as well as the perspective they aim to capture with respect to a given concept.
26. This results in a selection of neurons that may not strictly align across all methods.
27. For example, Visualization, Neuron Search and Corpus Search discover neurons that are highly focused on a specific task (like ""less"" suffix or POS ""TO"" concepts), while Probing-based methods discover ranking of neurons that highlight grouping behavior within the neurons targeting broad concepts like POS ""Nouns"".
28. Therefore, the choice of which neuron interpretation method to use is not straightforward and depends on various factors such as the nature of the concept to investigate, the availability of supervised data for the concept of interest etc. Apart from these high-level guiding principles, a thorough comparison of methods with respect to the nature of the concept of interest is needed to fully understand the strengths and weaknesses of each approach.
29. Antverg and Belinkov (2022) is one such effort in this direction that compares three neuron interpretation methods.
30. Neuron-level interpretation opens door for a number of applications useful for the successful deployment of DNN systems (Section 6).
31. However, most of the research conducted in this direction is preliminary.
32. For example, there are many open research questions in controlling system's behaviour using neurons such as: i) are all concepts manipulatable?
33. ii) how to identify neurons that can be controlled to change the output?
34. iii) is high distributiveness a hindrance for controlling model's behavior?
35. iv) and whether disentangled (Bengio et al., 2012) and sparse models (Frankle and Carbin, 2019) may serve as a better alternate on this front?
36. Addressing these questions will enable a more reliable control of the deep NLP models and entail numerous applications such as removing bias and adapting the system to novel domains.","How can evolutionary algorithms be applied to neuron selection in DNNs to uncover latent concepts?
What are the implications of models learning novel language concepts not defined by human linguistic categories on interpretation studies?
How do ablation and knowledge attribution methods contribute to understanding the causal relationship between concept neurons and model predictions?
Why is the lack of standard evaluation benchmarks a significant challenge in neuron interpretation research?
What factors should be considered when choosing a neuron interpretation method for investigating specific linguistic concepts?","1. How can evolutionary algorithms be applied to neuron selection in DNNs to uncover latent concepts?
2. What are the implications of models learning novel language concepts not defined by human linguistic categories on interpretation studies?
3. How do ablation and knowledge attribution methods contribute to understanding the causal relationship between concept neurons and model predictions?
4. Why is the lack of standard evaluation benchmarks a significant challenge in neuron interpretation research?
5. What factors should be considered when choosing a neuron interpretation method for investigating specific linguistic concepts?",None,,"Questions:

1. How can evolutionary algorithms be applied to neuron selection in DNNs to uncover latent concepts?
2. What are the implications of models learning novel language concepts not defined by human linguistic categories on interpretation studies?
3. How do ablation and knowledge attribution methods contribute to understanding the causal relationship between concept neurons and model predictions?
4. Why is the lack of standard evaluation benchmarks a significant challenge in neuron interpretation research?
5. What factors should be considered when choosing a neuron interpretation method for investigating specific linguistic concepts?

Answer:

(p32.0) In the following section, we discuss several open issues and limitations related to methods, evaluation and datasets. Moreover, we provide potential future directions vital to the progress of neuron and model interpretation.

(p32.1) • DNNs are distributed in nature which encourages groups of neurons to work together to learn a concept. The current analysis methods, at large, ignore interaction between neurons while discovering neurons with respect to a concept. Trying all possible combination of neurons is a computationally intractable problem. A linear classifier using ElasticNet regularization (Dalvi et al., 2019) considers grouping of features during training -however, it's effectiveness in handling grouped neurons has not been empirically validated. Evolutionary algorithms || do not make any assumption of the underline distribution of the features and they have been effectively used for feature selection of multivariate fea-|| https://en.wikipedia.org/wiki/ Evolutionary_algorithm tures. Exploring them for neuron selection is a promising research direction to probe towards latent concepts in these models.

(p32.2) • A large number of interpretation studies rely on human-defined linguistic concepts to probe a model. It is possible that the models do not strictly adhere to the humandefined concepts and learn novel concepts about the language. This results in an incorrect or incomplete analysis. Several researchers (Michael et al., 2020; made strides in this direction by analyzing hidden structures in the input representations in an unsupervised manner. They discovered existence of novel structures not captured in the human defined categories.  also proposed BERT ConceptNet, a manual annotation of the latent concepts in BERT. Introducing similar datasets across other models enables model-centric interpretation, and is a promising research direction.

(p32.3) • While a lot of work has been done on analyzing how knowledge is encoded within the learned representations, the question whether it is used by the model during prediction is a less explored area (Feder et al., 2021;Elazar et al., 2021). Ablation and knowledge attribution methods are two neuron interpretation methods that intrinsically use causal relation to select concept neurons. A few other studies evaluated the causal relation of the selected concept neurons via ablation or by clamping their activation values (Bau et al., 2019;Suau et al., 2020) and observed the change in model's prediction. However, most of the studies do not take into account the causal relation as part of the method or the evaluation of their method. The causal relation with respect to concept neurons is important to understand their importance to overall prediction and it leads way towards practical applications such as debiasing, model distillation and domain adaptation.

(p32.4) • The work on neuron interpretation lacks standard evaluation benchmarks, and therefore studies conducted on identical models are not comparable. For example, there exists no gold annotation of neurons with respect to a certain dataset or a class. The curation of standard evaluation benchmarks is an essential step towards improving methods of interpretation of deep neural network models.

(p32.5) • The neuron analysis methods vary in their theoretical foundations as well as the perspective they aim to capture with respect to a given concept. This results in a selection of neurons that may not strictly align across all methods. For example, Visualization, Neuron Search and Corpus Search discover neurons that are highly focused on a specific task (like ""less"" suffix or POS ""TO"" concepts), while Probing-based methods discover ranking of neurons that highlight grouping behavior within the neurons targeting broad concepts like POS ""Nouns"". Therefore, the choice of which neuron interpretation method to use is not straightforward and depends on various factors such as the nature of the concept to investigate, the availability of supervised data for the concept of interest etc. Apart from these high-level guiding principles, a thorough comparison of methods with respect to the nature of the concept of interest is needed to fully understand the strengths and weaknesses of each approach. Antverg and Belinkov (2022) is one such effort in this direction that compares three neuron interpretation methods.

(p32.6) • Neuron-level interpretation opens door for a number of applications useful for the successful deployment of DNN systems (Section 6). However, most of the research conducted in this direction is preliminary. For example, there are many open research questions in controlling system's behaviour using neurons such as: i) are all concepts manipulatable? ii) how to identify neurons that can be controlled to change the output? iii) is high distributiveness a hindrance for controlling model's behavior? iv) and whether disentangled (Bengio et al., 2012) and sparse models (Frankle and Carbin, 2019) may serve as a better alternate on this front? Addressing these questions will enable a more reliable control of the deep NLP models and entail numerous applications such as removing bias and adapting the system to novel domains.","Questions:



Answer:

(p32.0) In the following section, we discuss several open issues and limitations related to methods, evaluation and datasets. Moreover, we provide potential future directions vital to the progress of neuron and model interpretation.

(p32.1) • DNNs are distributed in nature which encourages groups of neurons to work together to learn a concept. The current analysis methods, at large, ignore interaction between neurons while discovering neurons with respect to a concept. Trying all possible combination of neurons is a computationally intractable problem. A linear classifier using ElasticNet regularization (Dalvi et al., 2019) considers grouping of features during training -however, it's effectiveness in handling grouped neurons has not been empirically validated. Evolutionary algorithms || do not make any assumption of the underline distribution of the features and they have been effectively used for feature selection of multivariate fea-|| https://en.wikipedia.org/wiki/ Evolutionary_algorithm tures. Exploring them for neuron selection is a promising research direction to probe towards latent concepts in these models.

(p32.2) • A large number of interpretation studies rely on human-defined linguistic concepts to probe a model. It is possible that the models do not strictly adhere to the humandefined concepts and learn novel concepts about the language. This results in an incorrect or incomplete analysis. Several researchers (Michael et al., 2020; made strides in this direction by analyzing hidden structures in the input representations in an unsupervised manner. They discovered existence of novel structures not captured in the human defined categories.  also proposed BERT ConceptNet, a manual annotation of the latent concepts in BERT. Introducing similar datasets across other models enables model-centric interpretation, and is a promising research direction.

(p32.3) • While a lot of work has been done on analyzing how knowledge is encoded within the learned representations, the question whether it is used by the model during prediction is a less explored area (Feder et al., 2021;Elazar et al., 2021). Ablation and knowledge attribution methods are two neuron interpretation methods that intrinsically use causal relation to select concept neurons. A few other studies evaluated the causal relation of the selected concept neurons via ablation or by clamping their activation values (Bau et al., 2019;Suau et al., 2020) and observed the change in model's prediction. However, most of the studies do not take into account the causal relation as part of the method or the evaluation of their method. The causal relation with respect to concept neurons is important to understand their importance to overall prediction and it leads way towards practical applications such as debiasing, model distillation and domain adaptation.

(p32.4) • The work on neuron interpretation lacks standard evaluation benchmarks, and therefore studies conducted on identical models are not comparable. For example, there exists no gold annotation of neurons with respect to a certain dataset or a class. The curation of standard evaluation benchmarks is an essential step towards improving methods of interpretation of deep neural network models.

(p32.5) • The neuron analysis methods vary in their theoretical foundations as well as the perspective they aim to capture with respect to a given concept. This results in a selection of neurons that may not strictly align across all methods. For example, Visualization, Neuron Search and Corpus Search discover neurons that are highly focused on a specific task (like ""less"" suffix or POS ""TO"" concepts), while Probing-based methods discover ranking of neurons that highlight grouping behavior within the neurons targeting broad concepts like POS ""Nouns"". Therefore, the choice of which neuron interpretation method to use is not straightforward and depends on various factors such as the nature of the concept to investigate, the availability of supervised data for the concept of interest etc. Apart from these high-level guiding principles, a thorough comparison of methods with respect to the nature of the concept of interest is needed to fully understand the strengths and weaknesses of each approach. Antverg and Belinkov (2022) is one such effort in this direction that compares three neuron interpretation methods.

(p32.6) • Neuron-level interpretation opens door for a number of applications useful for the successful deployment of DNN systems (Section 6). However, most of the research conducted in this direction is preliminary. For example, there are many open research questions in controlling system's behaviour using neurons such as: i) are all concepts manipulatable? ii) how to identify neurons that can be controlled to change the output? iii) is high distributiveness a hindrance for controlling model's behavior? iv) and whether disentangled (Bengio et al., 2012) and sparse models (Frankle and Carbin, 2019) may serve as a better alternate on this front? Addressing these questions will enable a more reliable control of the deep NLP models and entail numerous applications such as removing bias and adapting the system to novel domains."
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s2,Characteristic LLMs,"['p2.0', 'p2.1']","['Encoder-Decoder or Encoder-only ELMo [80], BERT [28], RoBERTa [65], DistilBERT [90], BioBERT [57], XLM [54], Xlnet [119], ALBERT [55], ELECTRA [24], T5 [84], GLM [123], XLM-E [20], ST-MoE [133], AlexaTM [95] Training: Masked Language Models Model type: Discriminative (BERT-style)', 'Pretrain task: Predict masked words Decoder-only GPT-3 [16], OPT [126]. PaLM [22], BLOOM [92], MT-NLG [93], GLaM [32],Gopher [83], chinchilla [41], LaMDA [102], GPT-J [107], LLaMA [103], GPT-4 [76], BloombergGPT [117] Training Autoregressive Language Models Model type: Generative (GPT-style) We also briefly summarize the characteristics and the representative LLMs of each type in Table 1.']","Encoder-Decoder or Encoder-only ELMo [80], BERT [28], RoBERTa [65], DistilBERT [90], BioBERT [57], XLM [54], Xlnet [119], ALBERT [55], ELECTRA [24], T5 [84], GLM [123], XLM-E [20], ST-MoE [133], AlexaTM [95] Training: Masked Language Models Model type: Discriminative (BERT-style)

Pretrain task: Predict masked words Decoder-only GPT-3 [16], OPT [126]. PaLM [22], BLOOM [92], MT-NLG [93], GLaM [32],Gopher [83], chinchilla [41], LaMDA [102], GPT-J [107], LLaMA [103], GPT-4 [76], BloombergGPT [117] Training Autoregressive Language Models Model type: Generative (GPT-style) We also briefly summarize the characteristics and the representative LLMs of each type in Table 1.","(p2.0) Encoder-Decoder or Encoder-only ELMo [80], BERT [28], RoBERTa [65], DistilBERT [90], BioBERT [57], XLM [54], Xlnet [119], ALBERT [55], ELECTRA [24], T5 [84], GLM [123], XLM-E [20], ST-MoE [133], AlexaTM [95] Training: Masked Language Models Model type: Discriminative (BERT-style)

(p2.1) Pretrain task: Predict masked words Decoder-only GPT-3 [16], OPT [126]. PaLM [22], BLOOM [92], MT-NLG [93], GLaM [32],Gopher [83], chinchilla [41], LaMDA [102], GPT-J [107], LLaMA [103], GPT-4 [76], BloombergGPT [117] Training Autoregressive Language Models Model type: Generative (GPT-style) We also briefly summarize the characteristics and the representative LLMs of each type in Table 1.","[['b54', 'b25', 'b17', 'b87', 'b62', 'b77', 'b21', 'b116', 'b51', 'b130', 'b52', 'b92', 'b120', 'b81'], ['b89', 'b114', 'b38', 'b100', 'b73', 'b19', 'b13', 'b123', 'b80', 'b29', 'b90', 'b99', 'b104']]","[['b54', 'b25', 'b17', 'b87', 'b62', 'b77', 'b21', 'b116', 'b51', 'b130', 'b52', 'b92', 'b120', 'b81'], ['b89', 'b114', 'b38', 'b100', 'b73', 'b19', 'b13', 'b123', 'b80', 'b29', 'b90', 'b99', 'b104']]",27,"1. Encoder-Decoder or Encoder-only ELMo [80], BERT [28], RoBERTa [65], DistilBERT [90], BioBERT [57], XLM [54], Xlnet [119], ALBERT [55], ELECTRA [24], T5 [84], GLM [28]0, XLM-E [28]1, ST-MoE [28]2, AlexaTM [28]3 Training: Masked Language Models Model type: Discriminative [28]4Pretrain task: Predict masked words Decoder-only GPT-3 [28]5, OPT [28]6.
2. PaLM [28]7, BLOOM [28]8, MT-NLG [28]9, GLaM [65]0,Gopher [65]1, chinchilla [65]2, LaMDA [65]3, GPT-J [65]4, LLaMA [65]5, GPT-4 [65]6, BloombergGPT [65]7 Training Autoregressive Language Models Model type: Generative [65]8 We also briefly summarize the characteristics and the representative LLMs of each type in Table 1.","What distinguishes the training approach of Masked Language Models from Autoregressive Language Models in LLMs?
How do Encoder-Decoder and Encoder-only architectures differ in their approach to processing language in LLMs?
What are the key characteristics that define a discriminative model type, such as BERT-style LLMs?
In what ways do generative model types, exemplified by GPT-style LLMs, differ fundamentally from discriminative models in their design and function?
Can you explain the significance of pretraining tasks like predicting masked words in the context of enhancing LLM performance?","1. What distinguishes the training approach of Masked Language Models from Autoregressive Language Models in LLMs?
2. How do Encoder-Decoder and Encoder-only architectures differ in their approach to processing language in LLMs?
3. What are the key characteristics that define a discriminative model type, such as BERT-style LLMs?
4. In what ways do generative model types, exemplified by GPT-style LLMs, differ fundamentally from discriminative models in their design and function?
5. Can you explain the significance of pretraining tasks like predicting masked words in the context of enhancing LLM performance?",None,,"Questions:

1. What distinguishes the training approach of Masked Language Models from Autoregressive Language Models in LLMs?
2. How do Encoder-Decoder and Encoder-only architectures differ in their approach to processing language in LLMs?
3. What are the key characteristics that define a discriminative model type, such as BERT-style LLMs?
4. In what ways do generative model types, exemplified by GPT-style LLMs, differ fundamentally from discriminative models in their design and function?
5. Can you explain the significance of pretraining tasks like predicting masked words in the context of enhancing LLM performance?

Answer:

(p2.0) Encoder-Decoder or Encoder-only ELMo [80], BERT [28], RoBERTa [65], DistilBERT [90], BioBERT [57], XLM [54], Xlnet [119], ALBERT [55], ELECTRA [24], T5 [84], GLM [123], XLM-E [20], ST-MoE [133], AlexaTM [95] Training: Masked Language Models Model type: Discriminative (BERT-style)

(p2.1) Pretrain task: Predict masked words Decoder-only GPT-3 [16], OPT [126]. PaLM [22], BLOOM [92], MT-NLG [93], GLaM [32],Gopher [83], chinchilla [41], LaMDA [102], GPT-J [107], LLaMA [103], GPT-4 [76], BloombergGPT [117] Training Autoregressive Language Models Model type: Generative (GPT-style) We also briefly summarize the characteristics and the representative LLMs of each type in Table 1.","Questions:



Answer:

(p2.0) Encoder-Decoder or Encoder-only ELMo [80], BERT [28], RoBERTa [65], DistilBERT [90], BioBERT [57], XLM [54], Xlnet [119], ALBERT [55], ELECTRA [24], T5 [84], GLM [123], XLM-E [20], ST-MoE [133], AlexaTM [95] Training: Masked Language Models Model type: Discriminative (BERT-style)

(p2.1) Pretrain task: Predict masked words Decoder-only GPT-3 [16], OPT [126]. PaLM [22], BLOOM [92], MT-NLG [93], GLaM [32],Gopher [83], chinchilla [41], LaMDA [102], GPT-J [107], LLaMA [103], GPT-4 [76], BloombergGPT [117] Training Autoregressive Language Models Model type: Generative (GPT-style) We also briefly summarize the characteristics and the representative LLMs of each type in Table 1."
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s4,GPT-style Language Models: Decoder-only,"['p4.0', 'p4.1']","['Although language models are typically task-agnostic in architecture, these methods require fine-tuning on datasets of the specific downstream task. Researchers found that scaling up language models significantly improves the few-shot, even zero-shot performance [16]. The most successful models for better few-shot and zero-show performance are Autoregressive Language Models, which are trained by generating the next word in a sequence given the preceding words. These models have been widely used for downstream tasks such as text generation and question answering.', 'Examples of Autoregressive Language Models include GPT-3 [16], OPT [126], PaLM [22], and BLOOM [92]. The game changer, GPT-3, for the first time, demonstrated reasonable few-/zero-shot performance via prompting and in-context learning, thus showing the superiority of autoregressive language models. There are also models such as CodeX [2] that are optimized for specific tasks such as code generation, BloombergGPT [117] for the financial domain. The recent breakthrough is ChatGPT, which refines GPT-3 specifically for conversational tasks, resulting in more interactive, coherent, and context-aware conversational for various real-world applications.']","Although language models are typically task-agnostic in architecture, these methods require fine-tuning on datasets of the specific downstream task. Researchers found that scaling up language models significantly improves the few-shot, even zero-shot performance [16]. The most successful models for better few-shot and zero-show performance are Autoregressive Language Models, which are trained by generating the next word in a sequence given the preceding words. These models have been widely used for downstream tasks such as text generation and question answering.

Examples of Autoregressive Language Models include GPT-3 [16], OPT [126], PaLM [22], and BLOOM [92]. The game changer, GPT-3, for the first time, demonstrated reasonable few-/zero-shot performance via prompting and in-context learning, thus showing the superiority of autoregressive language models. There are also models such as CodeX [2] that are optimized for specific tasks such as code generation, BloombergGPT [117] for the financial domain. The recent breakthrough is ChatGPT, which refines GPT-3 specifically for conversational tasks, resulting in more interactive, coherent, and context-aware conversational for various real-world applications.","(p4.0) Although language models are typically task-agnostic in architecture, these methods require fine-tuning on datasets of the specific downstream task. Researchers found that scaling up language models significantly improves the few-shot, even zero-shot performance [16]. The most successful models for better few-shot and zero-show performance are Autoregressive Language Models, which are trained by generating the next word in a sequence given the preceding words. These models have been widely used for downstream tasks such as text generation and question answering.

(p4.1) Examples of Autoregressive Language Models include GPT-3 [16], OPT [126], PaLM [22], and BLOOM [92]. The game changer, GPT-3, for the first time, demonstrated reasonable few-/zero-shot performance via prompting and in-context learning, thus showing the superiority of autoregressive language models. There are also models such as CodeX [2] that are optimized for specific tasks such as code generation, BloombergGPT [117] for the financial domain. The recent breakthrough is ChatGPT, which refines GPT-3 specifically for conversational tasks, resulting in more interactive, coherent, and context-aware conversational for various real-world applications.","[['b13'], ['b89', 'b114', 'b19', 'b13', 'b123', None]]","[['b13'], ['b89', 'b114', 'b19', 'b13', 'b123', None]]",7,"1. Although language models are typically task-agnostic in architecture, these methods require fine-tuning on datasets of the specific downstream task.
2. Researchers found that scaling up language models significantly improves the few-shot, even zero-shot performance [16].
3. The most successful models for better few-shot and zero-show performance are Autoregressive Language Models, which are trained by generating the next word in a sequence given the preceding words.
4. These models have been widely used for downstream tasks such as text generation and question answering.
5. Examples of Autoregressive Language Models include GPT-3 [16], OPT [126], PaLM [22], and BLOOM [92].
6. The game changer, GPT-3, for the first time, demonstrated reasonable few-/zero-shot performance via prompting and in-context learning, thus showing the superiority of autoregressive language models.
7. There are also models such as CodeX [2] that are optimized for specific tasks such as code generation, BloombergGPT [117] for the financial domain.
8. The recent breakthrough is ChatGPT, which refines GPT-3 specifically for conversational tasks, resulting in more interactive, coherent, and context-aware conversational for various real-world applications.","How do autoregressive language models like GPT-3 improve few-shot and zero-shot learning capabilities?
What distinguishes autoregressive language models from other types of language models in terms of text generation and question answering tasks?
In what ways has GPT-3 demonstrated its superiority over previous models for few-/zero-shot performance?
How are specialized models like CodeX and BloombergGPT optimized for their respective tasks of code generation and financial analysis?
What advancements does ChatGPT bring to conversational AI compared to its predecessor, GPT-3?","1. How do autoregressive language models like GPT-3 improve few-shot and zero-shot learning capabilities?
2. What distinguishes autoregressive language models from other types of language models in terms of text generation and question answering tasks?
3. In what ways has GPT-3 demonstrated its superiority over previous models for few-/zero-shot performance?
4. How are specialized models like CodeX and BloombergGPT optimized for their respective tasks of code generation and financial analysis?
5. What advancements does ChatGPT bring to conversational AI compared to its predecessor, GPT-3?",None,,"Questions:

1. How do autoregressive language models like GPT-3 improve few-shot and zero-shot learning capabilities?
2. What distinguishes autoregressive language models from other types of language models in terms of text generation and question answering tasks?
3. In what ways has GPT-3 demonstrated its superiority over previous models for few-/zero-shot performance?
4. How are specialized models like CodeX and BloombergGPT optimized for their respective tasks of code generation and financial analysis?
5. What advancements does ChatGPT bring to conversational AI compared to its predecessor, GPT-3?

Answer:

(p4.0) Although language models are typically task-agnostic in architecture, these methods require fine-tuning on datasets of the specific downstream task. Researchers found that scaling up language models significantly improves the few-shot, even zero-shot performance [16]. The most successful models for better few-shot and zero-show performance are Autoregressive Language Models, which are trained by generating the next word in a sequence given the preceding words. These models have been widely used for downstream tasks such as text generation and question answering.

(p4.1) Examples of Autoregressive Language Models include GPT-3 [16], OPT [126], PaLM [22], and BLOOM [92]. The game changer, GPT-3, for the first time, demonstrated reasonable few-/zero-shot performance via prompting and in-context learning, thus showing the superiority of autoregressive language models. There are also models such as CodeX [2] that are optimized for specific tasks such as code generation, BloombergGPT [117] for the financial domain. The recent breakthrough is ChatGPT, which refines GPT-3 specifically for conversational tasks, resulting in more interactive, coherent, and context-aware conversational for various real-world applications.","Questions:



Answer:

(p4.0) Although language models are typically task-agnostic in architecture, these methods require fine-tuning on datasets of the specific downstream task. Researchers found that scaling up language models significantly improves the few-shot, even zero-shot performance [16]. The most successful models for better few-shot and zero-show performance are Autoregressive Language Models, which are trained by generating the next word in a sequence given the preceding words. These models have been widely used for downstream tasks such as text generation and question answering.

(p4.1) Examples of Autoregressive Language Models include GPT-3 [16], OPT [126], PaLM [22], and BLOOM [92]. The game changer, GPT-3, for the first time, demonstrated reasonable few-/zero-shot performance via prompting and in-context learning, thus showing the superiority of autoregressive language models. There are also models such as CodeX [2] that are optimized for specific tasks such as code generation, BloombergGPT [117] for the financial domain. The recent breakthrough is ChatGPT, which refines GPT-3 specifically for conversational tasks, resulting in more interactive, coherent, and context-aware conversational for various real-world applications."
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s7,Pretraining data,['p7.0'],"[""Pre-training data plays a pivotal role in the development of large language models. As the foundation of remarkable capabilities [5,47] of LLMs, the quality, quantitative, and diversity of pre-training data influence the performance of LLMs significantly [124]. The commonly used pretraining data consists of a myriad of text sources, including books, articles, and websites. The data is carefully curated to ensure a comprehensive representation of human knowledge, linguistic nuances, and cultural perspectives. The importance of pretraining data lies in its capacity to inform the language model with a rich understanding of word knowledge, grammar, syntax, and semantics, as well as the ability to recognize context and generate coherent responses. The diversity of pretraining data also plays a crucial role in shaping the model's performance, and the selection of LLMs highly depends on the components of the pretraining data. For example, PaLM [22] and BLOOM [92] excel in multilingual tasks and machine translation with an abundance of multilingual pretraining data. Moreover, PaLM's performance in Question Answering tasks is enhanced by incorporating a considerable amount of social media conversations and Books corpus [22]. Likewise, code execution and code completion capabilities of GPT-3.5 (code-davinci-002) are amplified by the integration of code data in its pretraining dataset. In brief, when selecting LLMs for downstream tasks, it is advisable to choose the model pre-trained on a similar field of data.""]","Pre-training data plays a pivotal role in the development of large language models. As the foundation of remarkable capabilities [5,47] of LLMs, the quality, quantitative, and diversity of pre-training data influence the performance of LLMs significantly [124]. The commonly used pretraining data consists of a myriad of text sources, including books, articles, and websites. The data is carefully curated to ensure a comprehensive representation of human knowledge, linguistic nuances, and cultural perspectives. The importance of pretraining data lies in its capacity to inform the language model with a rich understanding of word knowledge, grammar, syntax, and semantics, as well as the ability to recognize context and generate coherent responses. The diversity of pretraining data also plays a crucial role in shaping the model's performance, and the selection of LLMs highly depends on the components of the pretraining data. For example, PaLM [22] and BLOOM [92] excel in multilingual tasks and machine translation with an abundance of multilingual pretraining data. Moreover, PaLM's performance in Question Answering tasks is enhanced by incorporating a considerable amount of social media conversations and Books corpus [22]. Likewise, code execution and code completion capabilities of GPT-3.5 (code-davinci-002) are amplified by the integration of code data in its pretraining dataset. In brief, when selecting LLMs for downstream tasks, it is advisable to choose the model pre-trained on a similar field of data.","(p7.0) Pre-training data plays a pivotal role in the development of large language models. As the foundation of remarkable capabilities [5,47] of LLMs, the quality, quantitative, and diversity of pre-training data influence the performance of LLMs significantly [124]. The commonly used pretraining data consists of a myriad of text sources, including books, articles, and websites. The data is carefully curated to ensure a comprehensive representation of human knowledge, linguistic nuances, and cultural perspectives. The importance of pretraining data lies in its capacity to inform the language model with a rich understanding of word knowledge, grammar, syntax, and semantics, as well as the ability to recognize context and generate coherent responses. The diversity of pretraining data also plays a crucial role in shaping the model's performance, and the selection of LLMs highly depends on the components of the pretraining data. For example, PaLM [22] and BLOOM [92] excel in multilingual tasks and machine translation with an abundance of multilingual pretraining data. Moreover, PaLM's performance in Question Answering tasks is enhanced by incorporating a considerable amount of social media conversations and Books corpus [22]. Likewise, code execution and code completion capabilities of GPT-3.5 (code-davinci-002) are amplified by the integration of code data in its pretraining dataset. In brief, when selecting LLMs for downstream tasks, it is advisable to choose the model pre-trained on a similar field of data.","[['b2', 'b89', 'b19', 'b44', 'b121']]","[['b2', 'b89', 'b19', 'b44', 'b121']]",5,"1. Pre-training data plays a pivotal role in the development of large language models.
2. As the foundation of remarkable capabilities [5,47] of LLMs, the quality, quantitative, and diversity of pre-training data influence the performance of LLMs significantly [124].
3. The commonly used pretraining data consists of a myriad of text sources, including books, articles, and websites.
4. The data is carefully curated to ensure a comprehensive representation of human knowledge, linguistic nuances, and cultural perspectives.
5. The importance of pretraining data lies in its capacity to inform the language model with a rich understanding of word knowledge, grammar, syntax, and semantics, as well as the ability to recognize context and generate coherent responses.
6. The diversity of pretraining data also plays a crucial role in shaping the model's performance, and the selection of LLMs highly depends on the components of the pretraining data.
7. For example, PaLM [22] and BLOOM [92] excel in multilingual tasks and machine translation with an abundance of multilingual pretraining data.
8. Moreover, PaLM's performance in Question Answering tasks is enhanced by incorporating a considerable amount of social media conversations and Books corpus [22].
9. Likewise, code execution and code completion capabilities of GPT-3.5 (code-davinci-002) are amplified by the integration of code data in its pretraining dataset.
10. In brief, when selecting LLMs for downstream tasks, it is advisable to choose the model pre-trained on a similar field of data.","How does the diversity of pre-training data influence the performance of large language models (LLMs)?
What types of text sources are commonly used for pre-training large language models, and why?
In what ways does the quality and quantity of pre-training data affect the capabilities of LLMs?
How do specific pre-training datasets enhance the performance of LLMs in tasks like machine translation and code execution?
Why is it important to select an LLM pre-trained on data similar to the intended downstream task?","1. How does the diversity of pre-training data influence the performance of large language models (LLMs)?
2. What types of text sources are commonly used for pre-training large language models, and why?
3. In what ways does the quality and quantity of pre-training data affect the capabilities of LLMs?
4. How do specific pre-training datasets enhance the performance of LLMs in tasks like machine translation and code execution?
5. Why is it important to select an LLM pre-trained on data similar to the intended downstream task?",None,,"Questions:

1. How does the diversity of pre-training data influence the performance of large language models (LLMs)?
2. What types of text sources are commonly used for pre-training large language models, and why?
3. In what ways does the quality and quantity of pre-training data affect the capabilities of LLMs?
4. How do specific pre-training datasets enhance the performance of LLMs in tasks like machine translation and code execution?
5. Why is it important to select an LLM pre-trained on data similar to the intended downstream task?

Answer:

(p7.0) Pre-training data plays a pivotal role in the development of large language models. As the foundation of remarkable capabilities [5,47] of LLMs, the quality, quantitative, and diversity of pre-training data influence the performance of LLMs significantly [124]. The commonly used pretraining data consists of a myriad of text sources, including books, articles, and websites. The data is carefully curated to ensure a comprehensive representation of human knowledge, linguistic nuances, and cultural perspectives. The importance of pretraining data lies in its capacity to inform the language model with a rich understanding of word knowledge, grammar, syntax, and semantics, as well as the ability to recognize context and generate coherent responses. The diversity of pretraining data also plays a crucial role in shaping the model's performance, and the selection of LLMs highly depends on the components of the pretraining data. For example, PaLM [22] and BLOOM [92] excel in multilingual tasks and machine translation with an abundance of multilingual pretraining data. Moreover, PaLM's performance in Question Answering tasks is enhanced by incorporating a considerable amount of social media conversations and Books corpus [22]. Likewise, code execution and code completion capabilities of GPT-3.5 (code-davinci-002) are amplified by the integration of code data in its pretraining dataset. In brief, when selecting LLMs for downstream tasks, it is advisable to choose the model pre-trained on a similar field of data.","Questions:



Answer:

(p7.0) Pre-training data plays a pivotal role in the development of large language models. As the foundation of remarkable capabilities [5,47] of LLMs, the quality, quantitative, and diversity of pre-training data influence the performance of LLMs significantly [124]. The commonly used pretraining data consists of a myriad of text sources, including books, articles, and websites. The data is carefully curated to ensure a comprehensive representation of human knowledge, linguistic nuances, and cultural perspectives. The importance of pretraining data lies in its capacity to inform the language model with a rich understanding of word knowledge, grammar, syntax, and semantics, as well as the ability to recognize context and generate coherent responses. The diversity of pretraining data also plays a crucial role in shaping the model's performance, and the selection of LLMs highly depends on the components of the pretraining data. For example, PaLM [22] and BLOOM [92] excel in multilingual tasks and machine translation with an abundance of multilingual pretraining data. Moreover, PaLM's performance in Question Answering tasks is enhanced by incorporating a considerable amount of social media conversations and Books corpus [22]. Likewise, code execution and code completion capabilities of GPT-3.5 (code-davinci-002) are amplified by the integration of code data in its pretraining dataset. In brief, when selecting LLMs for downstream tasks, it is advisable to choose the model pre-trained on a similar field of data."
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s8,Finetuning data,"['p8.0', 'p8.1', 'p8.2', 'p8.3']","['When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant. In this section, we provide a succinct overview of the appropriate models to employ for each scenario. Zero annotated data: In scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach. LLMs have been shown to outperform previous zero-shot methods [120]. Additionally, the absence of a parameter update process ensures that catastrophic forgetting [49] is avoided since the language model parameters remain unaltered.', 'Few annotated data: In this case, the few-shot examples are directly incorporated in the input prompt of LLMs, which is named as in-context learning, and these examples can effectively guide LLMs to generalize to the task. As reported in [16], one-shot and few-shot performance make significant gains, even matching the performance of the SOTA', ""fine-tuned open-domain models. And LLMs' zero/few-shot ability can be improved further by scaling [16]. Alternatively, some few-shot learning methods are invented to enhance fine-tuned models, such as meta-learning [56] or transfer learning [88]. However, performance might be inferior compared to using LLMs due to fine-tuned models' smaller scale and overfitting. Abundant annotated data: With a substantial amount of annotated data for a particular task available, both fine-tuned models and LLMs can be considered. In most cases, fine-tuning the model can fit the data pretty well. Although, LLMs can be used to meet some constraints such as privacy [99].In this scenario, the choice between using a fine-tuned model or a LLM is task-specific and also depends on many factors, including desired performance, computational resources, and deployment constraints."", 'In a brief summary: LLMs are more versatile w.r.t. the data availability, while fine-tuned models can be considered with abundant annotated data.']","When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant. In this section, we provide a succinct overview of the appropriate models to employ for each scenario. Zero annotated data: In scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach. LLMs have been shown to outperform previous zero-shot methods [120]. Additionally, the absence of a parameter update process ensures that catastrophic forgetting [49] is avoided since the language model parameters remain unaltered.

Few annotated data: In this case, the few-shot examples are directly incorporated in the input prompt of LLMs, which is named as in-context learning, and these examples can effectively guide LLMs to generalize to the task. As reported in [16], one-shot and few-shot performance make significant gains, even matching the performance of the SOTA

fine-tuned open-domain models. And LLMs' zero/few-shot ability can be improved further by scaling [16]. Alternatively, some few-shot learning methods are invented to enhance fine-tuned models, such as meta-learning [56] or transfer learning [88]. However, performance might be inferior compared to using LLMs due to fine-tuned models' smaller scale and overfitting. Abundant annotated data: With a substantial amount of annotated data for a particular task available, both fine-tuned models and LLMs can be considered. In most cases, fine-tuning the model can fit the data pretty well. Although, LLMs can be used to meet some constraints such as privacy [99].In this scenario, the choice between using a fine-tuned model or a LLM is task-specific and also depends on many factors, including desired performance, computational resources, and deployment constraints.

In a brief summary: LLMs are more versatile w.r.t. the data availability, while fine-tuned models can be considered with abundant annotated data.","(p8.0) When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant. In this section, we provide a succinct overview of the appropriate models to employ for each scenario. Zero annotated data: In scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach. LLMs have been shown to outperform previous zero-shot methods [120]. Additionally, the absence of a parameter update process ensures that catastrophic forgetting [49] is avoided since the language model parameters remain unaltered.

(p8.1) Few annotated data: In this case, the few-shot examples are directly incorporated in the input prompt of LLMs, which is named as in-context learning, and these examples can effectively guide LLMs to generalize to the task. As reported in [16], one-shot and few-shot performance make significant gains, even matching the performance of the SOTA

(p8.2) fine-tuned open-domain models. And LLMs' zero/few-shot ability can be improved further by scaling [16]. Alternatively, some few-shot learning methods are invented to enhance fine-tuned models, such as meta-learning [56] or transfer learning [88]. However, performance might be inferior compared to using LLMs due to fine-tuned models' smaller scale and overfitting. Abundant annotated data: With a substantial amount of annotated data for a particular task available, both fine-tuned models and LLMs can be considered. In most cases, fine-tuning the model can fit the data pretty well. Although, LLMs can be used to meet some constraints such as privacy [99].In this scenario, the choice between using a fine-tuned model or a LLM is task-specific and also depends on many factors, including desired performance, computational resources, and deployment constraints.

(p8.3) In a brief summary: LLMs are more versatile w.r.t. the data availability, while fine-tuned models can be considered with abundant annotated data.","[['b46', 'b117'], ['b13'], ['b96', 'b85', 'b13', 'b53'], []]","[['b46', 'b117'], ['b13'], ['b96', 'b85', 'b13', 'b53'], []]",7,"1. When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant.
2. In this section, we provide a succinct overview of the appropriate models to employ for each scenario.
3. Zero annotated data: In scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach.
4. LLMs have been shown to outperform previous zero-shot methods [120].
5. Additionally, the absence of a parameter update process ensures that catastrophic forgetting [49] is avoided since the language model parameters remain unaltered.
6. Few annotated data: In this case, the few-shot examples are directly incorporated in the input prompt of LLMs, which is named as in-context learning, and these examples can effectively guide LLMs to generalize to the task.
7. As reported in [16], one-shot and few-shot performance make significant gains, even matching the performance of the SOTAfine-tuned open-domain models.
8. And LLMs' zero/few-shot ability can be improved further by scaling [16].
9. Alternatively, some few-shot learning methods are invented to enhance fine-tuned models, such as meta-learning [56] or transfer learning [88].
10. However, performance might be inferior compared to using LLMs due to fine-tuned models' smaller scale and overfitting.
11. Abundant annotated data: With a substantial amount of annotated data for a particular task available, both fine-tuned models and LLMs can be considered.
12. In most cases, fine-tuning the model can fit the data pretty well.
13. Although, LLMs can be used to meet some constraints such as privacy [99].In this scenario, the choice between using a fine-tuned model or a LLM is task-specific and also depends on many factors, including desired performance, computational resources, and deployment constraints.
14. In a brief summary: LLMs are more versatile w.r.t.
15. the data availability, while fine-tuned models can be considered with abundant annotated data.","What are the recommended approaches for employing LLMs when there is zero annotated data available for a task?
How does in-context learning with few annotated data examples influence the performance of LLMs compared to state-of-the-art fine-tuned open-domain models?
What are some few-shot learning methods that can enhance the performance of fine-tuned models, and how do they compare to LLMs in terms of scale and overfitting?
Under what circumstances should one consider using fine-tuned models over LLMs when there is an abundance of annotated data?
What factors determine the choice between using a fine-tuned model or a LLM in scenarios with abundant annotated data?","1. What are the recommended approaches for employing LLMs when there is zero annotated data available for a task?
2. How does in-context learning with few annotated data examples influence the performance of LLMs compared to state-of-the-art fine-tuned open-domain models?
3. What are some few-shot learning methods that can enhance the performance of fine-tuned models, and how do they compare to LLMs in terms of scale and overfitting?
4. Under what circumstances should one consider using fine-tuned models over LLMs when there is an abundance of annotated data?
5. What factors determine the choice between using a fine-tuned model or a LLM in scenarios with abundant annotated data?",None,,"Questions:

1. What are the recommended approaches for employing LLMs when there is zero annotated data available for a task?
2. How does in-context learning with few annotated data examples influence the performance of LLMs compared to state-of-the-art fine-tuned open-domain models?
3. What are some few-shot learning methods that can enhance the performance of fine-tuned models, and how do they compare to LLMs in terms of scale and overfitting?
4. Under what circumstances should one consider using fine-tuned models over LLMs when there is an abundance of annotated data?
5. What factors determine the choice between using a fine-tuned model or a LLM in scenarios with abundant annotated data?

Answer:

(p8.0) When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant. In this section, we provide a succinct overview of the appropriate models to employ for each scenario. Zero annotated data: In scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach. LLMs have been shown to outperform previous zero-shot methods [120]. Additionally, the absence of a parameter update process ensures that catastrophic forgetting [49] is avoided since the language model parameters remain unaltered.

(p8.1) Few annotated data: In this case, the few-shot examples are directly incorporated in the input prompt of LLMs, which is named as in-context learning, and these examples can effectively guide LLMs to generalize to the task. As reported in [16], one-shot and few-shot performance make significant gains, even matching the performance of the SOTA

(p8.2) fine-tuned open-domain models. And LLMs' zero/few-shot ability can be improved further by scaling [16]. Alternatively, some few-shot learning methods are invented to enhance fine-tuned models, such as meta-learning [56] or transfer learning [88]. However, performance might be inferior compared to using LLMs due to fine-tuned models' smaller scale and overfitting. Abundant annotated data: With a substantial amount of annotated data for a particular task available, both fine-tuned models and LLMs can be considered. In most cases, fine-tuning the model can fit the data pretty well. Although, LLMs can be used to meet some constraints such as privacy [99].In this scenario, the choice between using a fine-tuned model or a LLM is task-specific and also depends on many factors, including desired performance, computational resources, and deployment constraints.

(p8.3) In a brief summary: LLMs are more versatile w.r.t. the data availability, while fine-tuned models can be considered with abundant annotated data.","Questions:



Answer:

(p8.0) When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant. In this section, we provide a succinct overview of the appropriate models to employ for each scenario. Zero annotated data: In scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach. LLMs have been shown to outperform previous zero-shot methods [120]. Additionally, the absence of a parameter update process ensures that catastrophic forgetting [49] is avoided since the language model parameters remain unaltered.

(p8.1) Few annotated data: In this case, the few-shot examples are directly incorporated in the input prompt of LLMs, which is named as in-context learning, and these examples can effectively guide LLMs to generalize to the task. As reported in [16], one-shot and few-shot performance make significant gains, even matching the performance of the SOTA

(p8.2) fine-tuned open-domain models. And LLMs' zero/few-shot ability can be improved further by scaling [16]. Alternatively, some few-shot learning methods are invented to enhance fine-tuned models, such as meta-learning [56] or transfer learning [88]. However, performance might be inferior compared to using LLMs due to fine-tuned models' smaller scale and overfitting. Abundant annotated data: With a substantial amount of annotated data for a particular task available, both fine-tuned models and LLMs can be considered. In most cases, fine-tuning the model can fit the data pretty well. Although, LLMs can be used to meet some constraints such as privacy [99].In this scenario, the choice between using a fine-tuned model or a LLM is task-specific and also depends on many factors, including desired performance, computational resources, and deployment constraints.

(p8.3) In a brief summary: LLMs are more versatile w.r.t. the data availability, while fine-tuned models can be considered with abundant annotated data."
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s9,Test data/user data,"['p9.0', 'p9.1', 'p9.2']","[""When deploying LLMs for downstream tasks, we often face challenges stemming from distributional differences between the test/user data and that of the training data. These disparities may encompass domain shifts [132], out-of-distribution variations [31], or even adversarial examples [82]. Such challenges significantly hinder fine-tuned modes' effectiveness in real-world applications. They fit into a specific distribution and have a poor ability to generalize to OOD data."", ""However, LLMs perform quite well facing such scenarios because they do not have an explicit fitting process. Moreover, recent advancements have further enhanced the ability of language models in this regard. The Reinforcement Learning from Human Feedback (RLHF) method has notably enhanced LLMs' generalization capabilities [77]. For example,"", 'InstructGPT demonstrates proficiency in following various instructions for a wide range of tasks and occasionally complying with instructions in different languages, even though such instructions are scarce. Similarly, ChatGPT exhibits consistent advantages on most adversarial and out-of-distribution (OOD) classification and translation tasks [109]. Its superiority in understanding dialogue-related texts led to an impressive performance on the DDXPlus dataset [101], a medical diagnosis dataset designed for OOD evaluation.']","When deploying LLMs for downstream tasks, we often face challenges stemming from distributional differences between the test/user data and that of the training data. These disparities may encompass domain shifts [132], out-of-distribution variations [31], or even adversarial examples [82]. Such challenges significantly hinder fine-tuned modes' effectiveness in real-world applications. They fit into a specific distribution and have a poor ability to generalize to OOD data.

However, LLMs perform quite well facing such scenarios because they do not have an explicit fitting process. Moreover, recent advancements have further enhanced the ability of language models in this regard. The Reinforcement Learning from Human Feedback (RLHF) method has notably enhanced LLMs' generalization capabilities [77]. For example,

InstructGPT demonstrates proficiency in following various instructions for a wide range of tasks and occasionally complying with instructions in different languages, even though such instructions are scarce. Similarly, ChatGPT exhibits consistent advantages on most adversarial and out-of-distribution (OOD) classification and translation tasks [109]. Its superiority in understanding dialogue-related texts led to an impressive performance on the DDXPlus dataset [101], a medical diagnosis dataset designed for OOD evaluation.","(p9.0) When deploying LLMs for downstream tasks, we often face challenges stemming from distributional differences between the test/user data and that of the training data. These disparities may encompass domain shifts [132], out-of-distribution variations [31], or even adversarial examples [82]. Such challenges significantly hinder fine-tuned modes' effectiveness in real-world applications. They fit into a specific distribution and have a poor ability to generalize to OOD data.

(p9.1) However, LLMs perform quite well facing such scenarios because they do not have an explicit fitting process. Moreover, recent advancements have further enhanced the ability of language models in this regard. The Reinforcement Learning from Human Feedback (RLHF) method has notably enhanced LLMs' generalization capabilities [77]. For example,

(p9.2) InstructGPT demonstrates proficiency in following various instructions for a wide range of tasks and occasionally complying with instructions in different languages, even though such instructions are scarce. Similarly, ChatGPT exhibits consistent advantages on most adversarial and out-of-distribution (OOD) classification and translation tasks [109]. Its superiority in understanding dialogue-related texts led to an impressive performance on the DDXPlus dataset [101], a medical diagnosis dataset designed for OOD evaluation.","[['b28', 'b129', 'b79'], ['b74'], ['b98', 'b106']]","[['b28', 'b129', 'b79'], ['b74'], ['b98', 'b106']]",6,"1. When deploying LLMs for downstream tasks, we often face challenges stemming from distributional differences between the test/user data and that of the training data.
2. These disparities may encompass domain shifts [132], out-of-distribution variations [31], or even adversarial examples [82].
3. Such challenges significantly hinder fine-tuned modes' effectiveness in real-world applications.
4. They fit into a specific distribution and have a poor ability to generalize to OOD data.
5. However, LLMs perform quite well facing such scenarios because they do not have an explicit fitting process.
6. Moreover, recent advancements have further enhanced the ability of language models in this regard.
7. The Reinforcement Learning from Human Feedback (RLHF) method has notably enhanced LLMs' generalization capabilities [77].
8. For example,InstructGPT demonstrates proficiency in following various instructions for a wide range of tasks and occasionally complying with instructions in different languages, even though such instructions are scarce.
9. Similarly, ChatGPT exhibits consistent advantages on most adversarial and out-of-distribution (OOD) classification and translation tasks [109].
10. Its superiority in understanding dialogue-related texts led to an impressive performance on the DDXPlus dataset [101], a medical diagnosis dataset designed for OOD evaluation.","What are the main challenges LLMs face when applied to downstream tasks with test or user data?
How do domain shifts, out-of-distribution variations, and adversarial examples impact the effectiveness of fine-tuned LLMs in real-world applications?
In what ways have recent advancements, such as the Reinforcement Learning from Human Feedback (RLHF) method, improved the generalization capabilities of LLMs?
How does InstructGPT demonstrate its proficiency in handling a wide range of tasks and instructions in different languages?
What specific advantages does ChatGPT exhibit in adversarial and out-of-distribution classification and translation tasks, and how does it perform on the DDXPlus dataset?","1. What are the main challenges LLMs face when applied to downstream tasks with test or user data?
2. How do domain shifts, out-of-distribution variations, and adversarial examples impact the effectiveness of fine-tuned LLMs in real-world applications?
3. In what ways have recent advancements, such as the Reinforcement Learning from Human Feedback (RLHF) method, improved the generalization capabilities of LLMs?
4. How does InstructGPT demonstrate its proficiency in handling a wide range of tasks and instructions in different languages?
5. What specific advantages does ChatGPT exhibit in adversarial and out-of-distribution classification and translation tasks, and how does it perform on the DDXPlus dataset?",None,,"Questions:

1. What are the main challenges LLMs face when applied to downstream tasks with test or user data?
2. How do domain shifts, out-of-distribution variations, and adversarial examples impact the effectiveness of fine-tuned LLMs in real-world applications?
3. In what ways have recent advancements, such as the Reinforcement Learning from Human Feedback (RLHF) method, improved the generalization capabilities of LLMs?
4. How does InstructGPT demonstrate its proficiency in handling a wide range of tasks and instructions in different languages?
5. What specific advantages does ChatGPT exhibit in adversarial and out-of-distribution classification and translation tasks, and how does it perform on the DDXPlus dataset?

Answer:

(p9.0) When deploying LLMs for downstream tasks, we often face challenges stemming from distributional differences between the test/user data and that of the training data. These disparities may encompass domain shifts [132], out-of-distribution variations [31], or even adversarial examples [82]. Such challenges significantly hinder fine-tuned modes' effectiveness in real-world applications. They fit into a specific distribution and have a poor ability to generalize to OOD data.

(p9.1) However, LLMs perform quite well facing such scenarios because they do not have an explicit fitting process. Moreover, recent advancements have further enhanced the ability of language models in this regard. The Reinforcement Learning from Human Feedback (RLHF) method has notably enhanced LLMs' generalization capabilities [77]. For example,

(p9.2) InstructGPT demonstrates proficiency in following various instructions for a wide range of tasks and occasionally complying with instructions in different languages, even though such instructions are scarce. Similarly, ChatGPT exhibits consistent advantages on most adversarial and out-of-distribution (OOD) classification and translation tasks [109]. Its superiority in understanding dialogue-related texts led to an impressive performance on the DDXPlus dataset [101], a medical diagnosis dataset designed for OOD evaluation.","Questions:



Answer:

(p9.0) When deploying LLMs for downstream tasks, we often face challenges stemming from distributional differences between the test/user data and that of the training data. These disparities may encompass domain shifts [132], out-of-distribution variations [31], or even adversarial examples [82]. Such challenges significantly hinder fine-tuned modes' effectiveness in real-world applications. They fit into a specific distribution and have a poor ability to generalize to OOD data.

(p9.1) However, LLMs perform quite well facing such scenarios because they do not have an explicit fitting process. Moreover, recent advancements have further enhanced the ability of language models in this regard. The Reinforcement Learning from Human Feedback (RLHF) method has notably enhanced LLMs' generalization capabilities [77]. For example,

(p9.2) InstructGPT demonstrates proficiency in following various instructions for a wide range of tasks and occasionally complying with instructions in different languages, even though such instructions are scarce. Similarly, ChatGPT exhibits consistent advantages on most adversarial and out-of-distribution (OOD) classification and translation tasks [109]. Its superiority in understanding dialogue-related texts led to an impressive performance on the DDXPlus dataset [101], a medical diagnosis dataset designed for OOD evaluation."
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s13,No use case.,"['p13.0', 'p13.1', 'p13.2', 'p13.3', 'p13.4', 'p13.5', 'p13.6', 'p13.7', 'p13.8', 'p13.9', 'p13.10']","['In most natural language understanding tasks, such as tasks in GLUE [106] and SuperGLUE [105], fine-tuned models still have better performance, if such tasks come with rich well-annotated data and contain very few out-of-distribution examples on test sets. For different tasks and datasets, the gap between small fine-tuned models and LLMs varies.', 'In text classification, on most datasets, LLMs perform slightly worse than fine-tuned models. For sentiment analysis, such as on IMDB [69] and SST [94], fine-tuned models and LLMs perform equally well. For toxicity detection, which is another iconic text classification task, the gap is much larger. All LLMs cannot perform well on this task, and on', 'CivilComments [13] even the best one is only better than random guessing [59]. On the other hand, most popular fine-tuned models can obtain much better performance [33]. and the Perspective API 3 is still one of the best for detecting toxicity. This API is powered by a multilingual BERT-based model, which is tuned on publicly available toxicity data and several smaller single-language CNNs distilled from this model. This might be due to the fact that toxicity is defined by subtle nuances in linguistic expressions, and large language models are unable to accurately comprehend this task solely based on the provided input.', 'The trend of performance gaps is similar in some other tasks. For natural language inference (NLI) tasks, on most datasets, such as on RTE [106] and SNLI [14], fine-tuned models perform better than LLMs, while on some data such as CB [105], LLMs have obtained comparable performance with fine-tuned models [22]. For question answering (QA), on SQuADv2 [86], QuAC [21] and many other datasets, fine-tuned models have superior performance, while on CoQA [87], LLMs perform as well as fine-tuned models [22].', ""In information retrieval (IR) tasks, LLMs are not widely exploited yet. One major reason is that IR tasks are fundamentally different from others. There's no natural way to transform the thousands of candidate texts into a few/zero-shot form which is required by LLMs. The existing evaluation results on MS MARCO(regular/TREC) [73] show that methods based on fine-tuned models have better performance [59]. In this evaluation, the LLMs rank passages in an unorthodox way, which requires the LLMs to produce probabilities for passages one by one."", ""For some low-level intermediate tasks, which are not intended for regular users but rather for high level tasks, such as named entity recognition (NER) and dependency parsing, there's not enough result coming from LLMs, because the most current evaluation of LLMs focuses on practical tasks. According to available evaluation results, for the NER task, CoNLL03 [89] is still a challenge for LLMs [81], where the performance of fine-tuned models is around as twice as LLMs. These intermediate tasks may vanish soon because LLMs can take over high-level tasks without the help of those intermediate tasks (e.g. dependency parsing for coding tasks; NER for some text generation tasks)."", 'In brief, for most traditional NLU tasks, a fine-tuned model is a better choice in terms of the performance on benchmark datasets and the computational cost. The scale of LLMs is usually 10× or even 100× larger than fine-tuned models.', 'One possible cause for the inferior performance of LLMs on certain tasks can be the design of instructions/prompts.', 'Transforming input from tasks like IR and sentence labeling into a few/zero-short instruction form is non-trivial. There may be better ways to adapt language models to traditional NLP tasks in the future. On the other hand, the upper limit of capabilities of fine-tuned models is not reached, and some methods like FLAN-tuning [67] can further boost the performance on NLU tasks. Another interesting finding is that on NLU tasks, after fine-tuning, masked language models, like T5 [85], are better than most auto-regressive language models at the same scale, while some recent results imply that this gap can be bridged by scaling [22].', '4.1.2 Use case. However, there are still some NLU tasks suitable for LLMs.', ""One of the representative tasks is miscellaneous text classification [59]. In contrast to classic domain-specific text classification tasks such as sentiment analysis, miscellaneous text classification deals with a diverse range of topics and categories that may not have a clear or strong relationship with one another. It's closer to real-world cases and hard to be formatted for using fine-tuned models. Another is the Adversarial NLI (ANLI) [74]. It is a difficult dataset composed of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). LLMs have shown superior performance on ANLI, especially on the R3 and R2. Both examples demonstrate the exceptional ability of LLMs to generalize well on out-of-distribution and sparsely annotated data in traditional NLP tasks, surpassing that of fine-tuned models. We've discussed this in the section above 3.3.""]","In most natural language understanding tasks, such as tasks in GLUE [106] and SuperGLUE [105], fine-tuned models still have better performance, if such tasks come with rich well-annotated data and contain very few out-of-distribution examples on test sets. For different tasks and datasets, the gap between small fine-tuned models and LLMs varies.

In text classification, on most datasets, LLMs perform slightly worse than fine-tuned models. For sentiment analysis, such as on IMDB [69] and SST [94], fine-tuned models and LLMs perform equally well. For toxicity detection, which is another iconic text classification task, the gap is much larger. All LLMs cannot perform well on this task, and on

CivilComments [13] even the best one is only better than random guessing [59]. On the other hand, most popular fine-tuned models can obtain much better performance [33]. and the Perspective API 3 is still one of the best for detecting toxicity. This API is powered by a multilingual BERT-based model, which is tuned on publicly available toxicity data and several smaller single-language CNNs distilled from this model. This might be due to the fact that toxicity is defined by subtle nuances in linguistic expressions, and large language models are unable to accurately comprehend this task solely based on the provided input.

The trend of performance gaps is similar in some other tasks. For natural language inference (NLI) tasks, on most datasets, such as on RTE [106] and SNLI [14], fine-tuned models perform better than LLMs, while on some data such as CB [105], LLMs have obtained comparable performance with fine-tuned models [22]. For question answering (QA), on SQuADv2 [86], QuAC [21] and many other datasets, fine-tuned models have superior performance, while on CoQA [87], LLMs perform as well as fine-tuned models [22].

In information retrieval (IR) tasks, LLMs are not widely exploited yet. One major reason is that IR tasks are fundamentally different from others. There's no natural way to transform the thousands of candidate texts into a few/zero-shot form which is required by LLMs. The existing evaluation results on MS MARCO(regular/TREC) [73] show that methods based on fine-tuned models have better performance [59]. In this evaluation, the LLMs rank passages in an unorthodox way, which requires the LLMs to produce probabilities for passages one by one.

For some low-level intermediate tasks, which are not intended for regular users but rather for high level tasks, such as named entity recognition (NER) and dependency parsing, there's not enough result coming from LLMs, because the most current evaluation of LLMs focuses on practical tasks. According to available evaluation results, for the NER task, CoNLL03 [89] is still a challenge for LLMs [81], where the performance of fine-tuned models is around as twice as LLMs. These intermediate tasks may vanish soon because LLMs can take over high-level tasks without the help of those intermediate tasks (e.g. dependency parsing for coding tasks; NER for some text generation tasks).

In brief, for most traditional NLU tasks, a fine-tuned model is a better choice in terms of the performance on benchmark datasets and the computational cost. The scale of LLMs is usually 10× or even 100× larger than fine-tuned models.

One possible cause for the inferior performance of LLMs on certain tasks can be the design of instructions/prompts.

Transforming input from tasks like IR and sentence labeling into a few/zero-short instruction form is non-trivial. There may be better ways to adapt language models to traditional NLP tasks in the future. On the other hand, the upper limit of capabilities of fine-tuned models is not reached, and some methods like FLAN-tuning [67] can further boost the performance on NLU tasks. Another interesting finding is that on NLU tasks, after fine-tuning, masked language models, like T5 [85], are better than most auto-regressive language models at the same scale, while some recent results imply that this gap can be bridged by scaling [22].

4.1.2 Use case. However, there are still some NLU tasks suitable for LLMs.

One of the representative tasks is miscellaneous text classification [59]. In contrast to classic domain-specific text classification tasks such as sentiment analysis, miscellaneous text classification deals with a diverse range of topics and categories that may not have a clear or strong relationship with one another. It's closer to real-world cases and hard to be formatted for using fine-tuned models. Another is the Adversarial NLI (ANLI) [74]. It is a difficult dataset composed of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). LLMs have shown superior performance on ANLI, especially on the R3 and R2. Both examples demonstrate the exceptional ability of LLMs to generalize well on out-of-distribution and sparsely annotated data in traditional NLP tasks, surpassing that of fine-tuned models. We've discussed this in the section above 3.3.","(p13.0) In most natural language understanding tasks, such as tasks in GLUE [106] and SuperGLUE [105], fine-tuned models still have better performance, if such tasks come with rich well-annotated data and contain very few out-of-distribution examples on test sets. For different tasks and datasets, the gap between small fine-tuned models and LLMs varies.

(p13.1) In text classification, on most datasets, LLMs perform slightly worse than fine-tuned models. For sentiment analysis, such as on IMDB [69] and SST [94], fine-tuned models and LLMs perform equally well. For toxicity detection, which is another iconic text classification task, the gap is much larger. All LLMs cannot perform well on this task, and on

(p13.2) CivilComments [13] even the best one is only better than random guessing [59]. On the other hand, most popular fine-tuned models can obtain much better performance [33]. and the Perspective API 3 is still one of the best for detecting toxicity. This API is powered by a multilingual BERT-based model, which is tuned on publicly available toxicity data and several smaller single-language CNNs distilled from this model. This might be due to the fact that toxicity is defined by subtle nuances in linguistic expressions, and large language models are unable to accurately comprehend this task solely based on the provided input.

(p13.3) The trend of performance gaps is similar in some other tasks. For natural language inference (NLI) tasks, on most datasets, such as on RTE [106] and SNLI [14], fine-tuned models perform better than LLMs, while on some data such as CB [105], LLMs have obtained comparable performance with fine-tuned models [22]. For question answering (QA), on SQuADv2 [86], QuAC [21] and many other datasets, fine-tuned models have superior performance, while on CoQA [87], LLMs perform as well as fine-tuned models [22].

(p13.4) In information retrieval (IR) tasks, LLMs are not widely exploited yet. One major reason is that IR tasks are fundamentally different from others. There's no natural way to transform the thousands of candidate texts into a few/zero-shot form which is required by LLMs. The existing evaluation results on MS MARCO(regular/TREC) [73] show that methods based on fine-tuned models have better performance [59]. In this evaluation, the LLMs rank passages in an unorthodox way, which requires the LLMs to produce probabilities for passages one by one.

(p13.5) For some low-level intermediate tasks, which are not intended for regular users but rather for high level tasks, such as named entity recognition (NER) and dependency parsing, there's not enough result coming from LLMs, because the most current evaluation of LLMs focuses on practical tasks. According to available evaluation results, for the NER task, CoNLL03 [89] is still a challenge for LLMs [81], where the performance of fine-tuned models is around as twice as LLMs. These intermediate tasks may vanish soon because LLMs can take over high-level tasks without the help of those intermediate tasks (e.g. dependency parsing for coding tasks; NER for some text generation tasks).

(p13.6) In brief, for most traditional NLU tasks, a fine-tuned model is a better choice in terms of the performance on benchmark datasets and the computational cost. The scale of LLMs is usually 10× or even 100× larger than fine-tuned models.

(p13.7) One possible cause for the inferior performance of LLMs on certain tasks can be the design of instructions/prompts.

(p13.8) Transforming input from tasks like IR and sentence labeling into a few/zero-short instruction form is non-trivial. There may be better ways to adapt language models to traditional NLP tasks in the future. On the other hand, the upper limit of capabilities of fine-tuned models is not reached, and some methods like FLAN-tuning [67] can further boost the performance on NLU tasks. Another interesting finding is that on NLU tasks, after fine-tuning, masked language models, like T5 [85], are better than most auto-regressive language models at the same scale, while some recent results imply that this gap can be bridged by scaling [22].

(p13.9) 4.1.2 Use case. However, there are still some NLU tasks suitable for LLMs.

(p13.10) One of the representative tasks is miscellaneous text classification [59]. In contrast to classic domain-specific text classification tasks such as sentiment analysis, miscellaneous text classification deals with a diverse range of topics and categories that may not have a clear or strong relationship with one another. It's closer to real-world cases and hard to be formatted for using fine-tuned models. Another is the Adversarial NLI (ANLI) [74]. It is a difficult dataset composed of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). LLMs have shown superior performance on ANLI, especially on the R3 and R2. Both examples demonstrate the exceptional ability of LLMs to generalize well on out-of-distribution and sparsely annotated data in traditional NLP tasks, surpassing that of fine-tuned models. We've discussed this in the section above 3.3.","[['b103', 'b102'], ['b66', 'b91'], ['b30', 'b10', 'b56'], ['b84', 'b11', 'b18', 'b83', 'b19', 'b102', 'b103'], ['b56', 'b70'], ['b78', 'b86'], [], [], ['b82', 'b19', 'b64'], [], ['b71', 'b56']]","[['b103', 'b102'], ['b66', 'b91'], ['b30', 'b10', 'b56'], ['b84', 'b11', 'b18', 'b83', 'b19', 'b102', 'b103'], ['b56', 'b70'], ['b78', 'b86'], [], [], ['b82', 'b19', 'b64'], [], ['b71', 'b56']]",23,"1. In most natural language understanding tasks, such as tasks in GLUE [106] and SuperGLUE [105], fine-tuned models still have better performance, if such tasks come with rich well-annotated data and contain very few out-of-distribution examples on test sets.
2. For different tasks and datasets, the gap between small fine-tuned models and LLMs varies.
3. In text classification, on most datasets, LLMs perform slightly worse than fine-tuned models.
4. For sentiment analysis, such as on IMDB [69] and SST [94], fine-tuned models and LLMs perform equally well.
5. For toxicity detection, which is another iconic text classification task, the gap is much larger.
6. All LLMs cannot perform well on this task, and onCivilComments [13] even the best one is only better than random guessing [69]8.
7. On the other hand, most popular fine-tuned models can obtain much better performance [33].
8. and the Perspective API 3 is still one of the best for detecting toxicity.
9. This API is powered by a multilingual BERT-based model, which is tuned on publicly available toxicity data and several smaller single-language CNNs distilled from this model.
10. This might be due to the fact that toxicity is defined by subtle nuances in linguistic expressions, and large language models are unable to accurately comprehend this task solely based on the provided input.
11. The trend of performance gaps is similar in some other tasks.
12. For natural language inference (NLI) tasks, on most datasets, such as on RTE [106] and SNLI [14], fine-tuned models perform better than LLMs, while on some data such as CB [105], LLMs have obtained comparable performance with fine-tuned models [69]7.
13. For question answering (QA), on SQuADv2 [86], QuAC [21] and many other datasets, fine-tuned models have superior performance, while on CoQA [87], LLMs perform as well as fine-tuned models [69]7.
14. In information retrieval (IR) tasks, LLMs are not widely exploited yet.
15. One major reason is that IR tasks are fundamentally different from others.
16. There's no natural way to transform the thousands of candidate texts into a few/zero-shot form which is required by LLMs.
17. The existing evaluation results on MS MARCO(regular/TREC) [73] show that methods based on fine-tuned models have better performance [69]8.
18. In this evaluation, the LLMs rank passages in an unorthodox way, which requires the LLMs to produce probabilities for passages one by one.
19. For some low-level intermediate tasks, which are not intended for regular users but rather for high level tasks, such as named entity recognition [69]1 and dependency parsing, there's not enough result coming from LLMs, because the most current evaluation of LLMs focuses on practical tasks.
20. According to available evaluation results, for the NER task, CoNLL03 [69]2 is still a challenge for LLMs [69]3, where the performance of fine-tuned models is around as twice as LLMs.
21. These intermediate tasks may vanish soon because LLMs can take over high-level tasks without the help of those intermediate tasks [69]4.
22. In brief, for most traditional NLU tasks, a fine-tuned model is a better choice in terms of the performance on benchmark datasets and the computational cost.
23. The scale of LLMs is usually 10× or even 100× larger than fine-tuned models.
24. One possible cause for the inferior performance of LLMs on certain tasks can be the design of instructions/prompts.
25. Transforming input from tasks like IR and sentence labeling into a few/zero-short instruction form is non-trivial.
26. There may be better ways to adapt language models to traditional NLP tasks in the future.
27. On the other hand, the upper limit of capabilities of fine-tuned models is not reached, and some methods like FLAN-tuning [69]5 can further boost the performance on NLU tasks.
28. Another interesting finding is that on NLU tasks, after fine-tuning, masked language models, like T5 [69]6, are better than most auto-regressive language models at the same scale, while some recent results imply that this gap can be bridged by scaling [69]7.
29. 4.1.2 Use case. However, there are still some NLU tasks suitable for LLMs.
30. One of the representative tasks is miscellaneous text classification [69]8.
31. In contrast to classic domain-specific text classification tasks such as sentiment analysis, miscellaneous text classification deals with a diverse range of topics and categories that may not have a clear or strong relationship with one another.
32. It's closer to real-world cases and hard to be formatted for using fine-tuned models.
33. Another is the Adversarial NLI [69]9 [94]0.
34. It is a difficult dataset composed of adversarially mined natural language inference questions in three rounds [94]1.
35. LLMs have shown superior performance on ANLI, especially on the R3 and R2.
36. Both examples demonstrate the exceptional ability of LLMs to generalize well on out-of-distribution and sparsely annotated data in traditional NLP tasks, surpassing that of fine-tuned models.
37. We've discussed this in the section above 3.3.","How do fine-tuned models compare to LLMs in performance on GLUE and SuperGLUE benchmarks?
What factors contribute to the performance gap between LLMs and fine-tuned models in toxicity detection tasks?
In what NLU tasks have LLMs shown comparable or superior performance to fine-tuned models, according to recent evaluations?
Why are LLMs not widely exploited in information retrieval tasks, and what challenges do they face in this area?
How does the FLAN-tuning method impact the performance of fine-tuned models on NLU tasks compared to traditional fine-tuning approaches?","1. How do fine-tuned models compare to LLMs in performance on GLUE and SuperGLUE benchmarks?
2. What factors contribute to the performance gap between LLMs and fine-tuned models in toxicity detection tasks?
3. In what NLU tasks have LLMs shown comparable or superior performance to fine-tuned models, according to recent evaluations?
4. Why are LLMs not widely exploited in information retrieval tasks, and what challenges do they face in this area?
5. How does the FLAN-tuning method impact the performance of fine-tuned models on NLU tasks compared to traditional fine-tuning approaches?",None,,"Questions:

1. How do fine-tuned models compare to LLMs in performance on GLUE and SuperGLUE benchmarks?
2. What factors contribute to the performance gap between LLMs and fine-tuned models in toxicity detection tasks?
3. In what NLU tasks have LLMs shown comparable or superior performance to fine-tuned models, according to recent evaluations?
4. Why are LLMs not widely exploited in information retrieval tasks, and what challenges do they face in this area?
5. How does the FLAN-tuning method impact the performance of fine-tuned models on NLU tasks compared to traditional fine-tuning approaches?

Answer:

(p13.0) In most natural language understanding tasks, such as tasks in GLUE [106] and SuperGLUE [105], fine-tuned models still have better performance, if such tasks come with rich well-annotated data and contain very few out-of-distribution examples on test sets. For different tasks and datasets, the gap between small fine-tuned models and LLMs varies.

(p13.1) In text classification, on most datasets, LLMs perform slightly worse than fine-tuned models. For sentiment analysis, such as on IMDB [69] and SST [94], fine-tuned models and LLMs perform equally well. For toxicity detection, which is another iconic text classification task, the gap is much larger. All LLMs cannot perform well on this task, and on

(p13.2) CivilComments [13] even the best one is only better than random guessing [59]. On the other hand, most popular fine-tuned models can obtain much better performance [33]. and the Perspective API 3 is still one of the best for detecting toxicity. This API is powered by a multilingual BERT-based model, which is tuned on publicly available toxicity data and several smaller single-language CNNs distilled from this model. This might be due to the fact that toxicity is defined by subtle nuances in linguistic expressions, and large language models are unable to accurately comprehend this task solely based on the provided input.

(p13.3) The trend of performance gaps is similar in some other tasks. For natural language inference (NLI) tasks, on most datasets, such as on RTE [106] and SNLI [14], fine-tuned models perform better than LLMs, while on some data such as CB [105], LLMs have obtained comparable performance with fine-tuned models [22]. For question answering (QA), on SQuADv2 [86], QuAC [21] and many other datasets, fine-tuned models have superior performance, while on CoQA [87], LLMs perform as well as fine-tuned models [22].

(p13.4) In information retrieval (IR) tasks, LLMs are not widely exploited yet. One major reason is that IR tasks are fundamentally different from others. There's no natural way to transform the thousands of candidate texts into a few/zero-shot form which is required by LLMs. The existing evaluation results on MS MARCO(regular/TREC) [73] show that methods based on fine-tuned models have better performance [59]. In this evaluation, the LLMs rank passages in an unorthodox way, which requires the LLMs to produce probabilities for passages one by one.

(p13.5) For some low-level intermediate tasks, which are not intended for regular users but rather for high level tasks, such as named entity recognition (NER) and dependency parsing, there's not enough result coming from LLMs, because the most current evaluation of LLMs focuses on practical tasks. According to available evaluation results, for the NER task, CoNLL03 [89] is still a challenge for LLMs [81], where the performance of fine-tuned models is around as twice as LLMs. These intermediate tasks may vanish soon because LLMs can take over high-level tasks without the help of those intermediate tasks (e.g. dependency parsing for coding tasks; NER for some text generation tasks).

(p13.6) In brief, for most traditional NLU tasks, a fine-tuned model is a better choice in terms of the performance on benchmark datasets and the computational cost. The scale of LLMs is usually 10× or even 100× larger than fine-tuned models.

(p13.7) One possible cause for the inferior performance of LLMs on certain tasks can be the design of instructions/prompts.

(p13.8) Transforming input from tasks like IR and sentence labeling into a few/zero-short instruction form is non-trivial. There may be better ways to adapt language models to traditional NLP tasks in the future. On the other hand, the upper limit of capabilities of fine-tuned models is not reached, and some methods like FLAN-tuning [67] can further boost the performance on NLU tasks. Another interesting finding is that on NLU tasks, after fine-tuning, masked language models, like T5 [85], are better than most auto-regressive language models at the same scale, while some recent results imply that this gap can be bridged by scaling [22].

(p13.9) 4.1.2 Use case. However, there are still some NLU tasks suitable for LLMs.

(p13.10) One of the representative tasks is miscellaneous text classification [59]. In contrast to classic domain-specific text classification tasks such as sentiment analysis, miscellaneous text classification deals with a diverse range of topics and categories that may not have a clear or strong relationship with one another. It's closer to real-world cases and hard to be formatted for using fine-tuned models. Another is the Adversarial NLI (ANLI) [74]. It is a difficult dataset composed of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). LLMs have shown superior performance on ANLI, especially on the R3 and R2. Both examples demonstrate the exceptional ability of LLMs to generalize well on out-of-distribution and sparsely annotated data in traditional NLP tasks, surpassing that of fine-tuned models. We've discussed this in the section above 3.3.","Questions:



Answer:

(p13.0) In most natural language understanding tasks, such as tasks in GLUE [106] and SuperGLUE [105], fine-tuned models still have better performance, if such tasks come with rich well-annotated data and contain very few out-of-distribution examples on test sets. For different tasks and datasets, the gap between small fine-tuned models and LLMs varies.

(p13.1) In text classification, on most datasets, LLMs perform slightly worse than fine-tuned models. For sentiment analysis, such as on IMDB [69] and SST [94], fine-tuned models and LLMs perform equally well. For toxicity detection, which is another iconic text classification task, the gap is much larger. All LLMs cannot perform well on this task, and on

(p13.2) CivilComments [13] even the best one is only better than random guessing [59]. On the other hand, most popular fine-tuned models can obtain much better performance [33]. and the Perspective API 3 is still one of the best for detecting toxicity. This API is powered by a multilingual BERT-based model, which is tuned on publicly available toxicity data and several smaller single-language CNNs distilled from this model. This might be due to the fact that toxicity is defined by subtle nuances in linguistic expressions, and large language models are unable to accurately comprehend this task solely based on the provided input.

(p13.3) The trend of performance gaps is similar in some other tasks. For natural language inference (NLI) tasks, on most datasets, such as on RTE [106] and SNLI [14], fine-tuned models perform better than LLMs, while on some data such as CB [105], LLMs have obtained comparable performance with fine-tuned models [22]. For question answering (QA), on SQuADv2 [86], QuAC [21] and many other datasets, fine-tuned models have superior performance, while on CoQA [87], LLMs perform as well as fine-tuned models [22].

(p13.4) In information retrieval (IR) tasks, LLMs are not widely exploited yet. One major reason is that IR tasks are fundamentally different from others. There's no natural way to transform the thousands of candidate texts into a few/zero-shot form which is required by LLMs. The existing evaluation results on MS MARCO(regular/TREC) [73] show that methods based on fine-tuned models have better performance [59]. In this evaluation, the LLMs rank passages in an unorthodox way, which requires the LLMs to produce probabilities for passages one by one.

(p13.5) For some low-level intermediate tasks, which are not intended for regular users but rather for high level tasks, such as named entity recognition (NER) and dependency parsing, there's not enough result coming from LLMs, because the most current evaluation of LLMs focuses on practical tasks. According to available evaluation results, for the NER task, CoNLL03 [89] is still a challenge for LLMs [81], where the performance of fine-tuned models is around as twice as LLMs. These intermediate tasks may vanish soon because LLMs can take over high-level tasks without the help of those intermediate tasks (e.g. dependency parsing for coding tasks; NER for some text generation tasks).

(p13.6) In brief, for most traditional NLU tasks, a fine-tuned model is a better choice in terms of the performance on benchmark datasets and the computational cost. The scale of LLMs is usually 10× or even 100× larger than fine-tuned models.

(p13.7) One possible cause for the inferior performance of LLMs on certain tasks can be the design of instructions/prompts.

(p13.8) Transforming input from tasks like IR and sentence labeling into a few/zero-short instruction form is non-trivial. There may be better ways to adapt language models to traditional NLP tasks in the future. On the other hand, the upper limit of capabilities of fine-tuned models is not reached, and some methods like FLAN-tuning [67] can further boost the performance on NLU tasks. Another interesting finding is that on NLU tasks, after fine-tuning, masked language models, like T5 [85], are better than most auto-regressive language models at the same scale, while some recent results imply that this gap can be bridged by scaling [22].

(p13.9) 4.1.2 Use case. However, there are still some NLU tasks suitable for LLMs.

(p13.10) One of the representative tasks is miscellaneous text classification [59]. In contrast to classic domain-specific text classification tasks such as sentiment analysis, miscellaneous text classification deals with a diverse range of topics and categories that may not have a clear or strong relationship with one another. It's closer to real-world cases and hard to be formatted for using fine-tuned models. Another is the Adversarial NLI (ANLI) [74]. It is a difficult dataset composed of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). LLMs have shown superior performance on ANLI, especially on the R3 and R2. Both examples demonstrate the exceptional ability of LLMs to generalize well on out-of-distribution and sparsely annotated data in traditional NLP tasks, surpassing that of fine-tuned models. We've discussed this in the section above 3.3."
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s15,Remark 3,"['p15.0', 'p15.1', 'p15.2', 'p15.3', 'p15.4', 'p15.5', 'p15.6', 'p15.7']","['Due to their strong generation ability and creativity, LLMs show superiority at most generation tasks.', '4.2.1 Use case. Generation tasks require models to have a comprehensive understanding of the input contents or requirements and a certain level of creativity. This is what LLMs excel at.', 'For summarization tasks, although LLMs do not have an obvious advantage over fine-tuned models under traditional automatic evaluation metrics, such as ROUGE [60], human evaluation results indicate that humans tend to prefer the results generated by LLMs [38,127] compared to that of fine-tuned models. For example, on CNN/DailyMail [71] and XSUM [72], fine-tuned models like Brio [66] and Pegasus [125] have much better performance than any LLMs w.r.t.', ""ROUGE, but LLMs like OPT [126] perform far better in human evaluation considering all aspects including faithfulness, coherence, and relevance [127]. This demonstrates the superiority of LLMs in summarization tasks. On the other hand, it implies that current summarization benchmarks don't contain summaries with high quality or the automatic metrics are not proper for the evaluation of summarization."", ""In machine translation (MT), LLMs can perform competent translation, although the average performance is slightly worse than some commercial translation tools [45] considering some automatic metrics like BLEU [78]. LLMs are particularly good at translating some low-resource language texts to English texts, such as in the Romanian-English translation of WMT'16 [11], zero-shot or few-shot LLMs can perform better than SOTA fine-tuned model [22]. This is mainly due to the fact that English resources compose the main part of the pre-training data. BLOOM [92] is pre-trained on more multi-lingual data, leading to better translation quality in both rich-resource and low-resource translation."", 'Another interesting finding is that BLOOM achieves good translation quality among Romance languages, even for translation from Galician, which is not included in the pre-training data. One reasonable explanation is that texts from some languages in the same language group can help the LLMs learn more from the similarity. If more multi-lingual texts can be added to the pre-training data, the translation capability may be improved further.', 'Additionally, LLMs are highly skilled in open-ended generations. One example is that the news articles generated by LLMs are almost indistinguishable from real news articles by humans [16]. LLMs are remarkably adept at code synthesis as well. Either for text-code generation, such as HumanEval [18] and MBPP [7], or for code repairing, such as DeepFix [39], LLMs can perform pretty well. GPT-4 can even pass 25% problems in Leetcode, which are not trivial for most human coders [76]. With training on more code data, the coding capability of LLMs can be improved further [22].', 'While performing well on such tasks, the codes generated by LLMs should be tested carefully to figure out any subtle bugs, which is one of the main challenges for applying LLMs in code synthesis.']","Due to their strong generation ability and creativity, LLMs show superiority at most generation tasks.

4.2.1 Use case. Generation tasks require models to have a comprehensive understanding of the input contents or requirements and a certain level of creativity. This is what LLMs excel at.

For summarization tasks, although LLMs do not have an obvious advantage over fine-tuned models under traditional automatic evaluation metrics, such as ROUGE [60], human evaluation results indicate that humans tend to prefer the results generated by LLMs [38,127] compared to that of fine-tuned models. For example, on CNN/DailyMail [71] and XSUM [72], fine-tuned models like Brio [66] and Pegasus [125] have much better performance than any LLMs w.r.t.

ROUGE, but LLMs like OPT [126] perform far better in human evaluation considering all aspects including faithfulness, coherence, and relevance [127]. This demonstrates the superiority of LLMs in summarization tasks. On the other hand, it implies that current summarization benchmarks don't contain summaries with high quality or the automatic metrics are not proper for the evaluation of summarization.

In machine translation (MT), LLMs can perform competent translation, although the average performance is slightly worse than some commercial translation tools [45] considering some automatic metrics like BLEU [78]. LLMs are particularly good at translating some low-resource language texts to English texts, such as in the Romanian-English translation of WMT'16 [11], zero-shot or few-shot LLMs can perform better than SOTA fine-tuned model [22]. This is mainly due to the fact that English resources compose the main part of the pre-training data. BLOOM [92] is pre-trained on more multi-lingual data, leading to better translation quality in both rich-resource and low-resource translation.

Another interesting finding is that BLOOM achieves good translation quality among Romance languages, even for translation from Galician, which is not included in the pre-training data. One reasonable explanation is that texts from some languages in the same language group can help the LLMs learn more from the similarity. If more multi-lingual texts can be added to the pre-training data, the translation capability may be improved further.

Additionally, LLMs are highly skilled in open-ended generations. One example is that the news articles generated by LLMs are almost indistinguishable from real news articles by humans [16]. LLMs are remarkably adept at code synthesis as well. Either for text-code generation, such as HumanEval [18] and MBPP [7], or for code repairing, such as DeepFix [39], LLMs can perform pretty well. GPT-4 can even pass 25% problems in Leetcode, which are not trivial for most human coders [76]. With training on more code data, the coding capability of LLMs can be improved further [22].

While performing well on such tasks, the codes generated by LLMs should be tested carefully to figure out any subtle bugs, which is one of the main challenges for applying LLMs in code synthesis.","(p15.0) Due to their strong generation ability and creativity, LLMs show superiority at most generation tasks.

(p15.1) 4.2.1 Use case. Generation tasks require models to have a comprehensive understanding of the input contents or requirements and a certain level of creativity. This is what LLMs excel at.

(p15.2) For summarization tasks, although LLMs do not have an obvious advantage over fine-tuned models under traditional automatic evaluation metrics, such as ROUGE [60], human evaluation results indicate that humans tend to prefer the results generated by LLMs [38,127] compared to that of fine-tuned models. For example, on CNN/DailyMail [71] and XSUM [72], fine-tuned models like Brio [66] and Pegasus [125] have much better performance than any LLMs w.r.t.

(p15.3) ROUGE, but LLMs like OPT [126] perform far better in human evaluation considering all aspects including faithfulness, coherence, and relevance [127]. This demonstrates the superiority of LLMs in summarization tasks. On the other hand, it implies that current summarization benchmarks don't contain summaries with high quality or the automatic metrics are not proper for the evaluation of summarization.

(p15.4) In machine translation (MT), LLMs can perform competent translation, although the average performance is slightly worse than some commercial translation tools [45] considering some automatic metrics like BLEU [78]. LLMs are particularly good at translating some low-resource language texts to English texts, such as in the Romanian-English translation of WMT'16 [11], zero-shot or few-shot LLMs can perform better than SOTA fine-tuned model [22]. This is mainly due to the fact that English resources compose the main part of the pre-training data. BLOOM [92] is pre-trained on more multi-lingual data, leading to better translation quality in both rich-resource and low-resource translation.

(p15.5) Another interesting finding is that BLOOM achieves good translation quality among Romance languages, even for translation from Galician, which is not included in the pre-training data. One reasonable explanation is that texts from some languages in the same language group can help the LLMs learn more from the similarity. If more multi-lingual texts can be added to the pre-training data, the translation capability may be improved further.

(p15.6) Additionally, LLMs are highly skilled in open-ended generations. One example is that the news articles generated by LLMs are almost indistinguishable from real news articles by humans [16]. LLMs are remarkably adept at code synthesis as well. Either for text-code generation, such as HumanEval [18] and MBPP [7], or for code repairing, such as DeepFix [39], LLMs can perform pretty well. GPT-4 can even pass 25% problems in Leetcode, which are not trivial for most human coders [76]. With training on more code data, the coding capability of LLMs can be improved further [22].

(p15.7) While performing well on such tasks, the codes generated by LLMs should be tested carefully to figure out any subtle bugs, which is one of the main challenges for applying LLMs in code synthesis.","[[], [], ['b57', 'b124', 'b63', 'b68', 'b35', 'b69', 'b122'], ['b124', 'b123'], ['b89', 'b19', 'b42', 'b8', 'b75'], [], ['b4', 'b15', 'b73', 'b13', 'b19', 'b36'], []]","[[], [], ['b57', 'b124', 'b63', 'b68', 'b35', 'b69', 'b122'], ['b124', 'b123'], ['b89', 'b19', 'b42', 'b8', 'b75'], [], ['b4', 'b15', 'b73', 'b13', 'b19', 'b36'], []]",20,"1. Due to their strong generation ability and creativity, LLMs show superiority at most generation tasks.
2. 4.2.1 Use case. Generation tasks require models to have a comprehensive understanding of the input contents or requirements and a certain level of creativity.
3. This is what LLMs excel at. For summarization tasks, although LLMs do not have an obvious advantage over fine-tuned models under traditional automatic evaluation metrics, such as ROUGE [60], human evaluation results indicate that humans tend to prefer the results generated by LLMs [38,127] compared to that of fine-tuned models.
4. For example, on CNN/DailyMail [71] and XSUM [72], fine-tuned models like Brio [66] and Pegasus [125] have much better performance than any LLMs w.r.t.ROUGE, but LLMs like OPT [126] perform far better in human evaluation considering all aspects including faithfulness, coherence, and relevance [127].
5. This demonstrates the superiority of LLMs in summarization tasks.
6. On the other hand, it implies that current summarization benchmarks don't contain summaries with high quality or the automatic metrics are not proper for the evaluation of summarization.
7. In machine translation (MT), LLMs can perform competent translation, although the average performance is slightly worse than some commercial translation tools [45] considering some automatic metrics like BLEU [38,127]0.
8. LLMs are particularly good at translating some low-resource language texts to English texts, such as in the Romanian-English translation of WMT'16 [38,127]1, zero-shot or few-shot LLMs can perform better than SOTA fine-tuned model [38,127]9.
9. This is mainly due to the fact that English resources compose the main part of the pre-training data.
10. BLOOM [38,127]3 is pre-trained on more multi-lingual data, leading to better translation quality in both rich-resource and low-resource translation.
11. Another interesting finding is that BLOOM achieves good translation quality among Romance languages, even for translation from Galician, which is not included in the pre-training data.
12. One reasonable explanation is that texts from some languages in the same language group can help the LLMs learn more from the similarity.
13. If more multi-lingual texts can be added to the pre-training data, the translation capability may be improved further.
14. Additionally, LLMs are highly skilled in open-ended generations.
15. One example is that the news articles generated by LLMs are almost indistinguishable from real news articles by humans [38,127]4.
16. LLMs are remarkably adept at code synthesis as well.
17. Either for text-code generation, such as HumanEval [38,127]5 and MBPP [38,127]6, or for code repairing, such as DeepFix [38,127]7, LLMs can perform pretty well.
18. GPT-4 can even pass 25% problems in Leetcode, which are not trivial for most human coders [38,127]8.
19. With training on more code data, the coding capability of LLMs can be improved further [38,127]9.
20. While performing well on such tasks, the codes generated by LLMs should be tested carefully to figure out any subtle bugs, which is one of the main challenges for applying LLMs in code synthesis.","What are the advantages of LLMs in generating human-like news articles compared to traditional models?
How do LLMs perform in summarization tasks when evaluated by humans versus automatic evaluation metrics like ROUGE?
In what ways do LLMs show superiority in machine translation tasks, especially for low-resource languages?
What role does pre-training data play in enhancing the translation capabilities of LLMs for both rich-resource and low-resource languages?
What are the challenges associated with using LLMs for code synthesis, despite their demonstrated ability in text-code generation and code repairing tasks?","1. What are the advantages of LLMs in generating human-like news articles compared to traditional models?
2. How do LLMs perform in summarization tasks when evaluated by humans versus automatic evaluation metrics like ROUGE?
3. In what ways do LLMs show superiority in machine translation tasks, especially for low-resource languages?
4. What role does pre-training data play in enhancing the translation capabilities of LLMs for both rich-resource and low-resource languages?
5. What are the challenges associated with using LLMs for code synthesis, despite their demonstrated ability in text-code generation and code repairing tasks?","# How do LLMs perform in various generation tasks compared to fine-tuned models and what are the implications?
  - In summarization tasks, LLMs are preferred by humans over fine-tuned models despite lower scores in traditional metrics like ROUGE, indicating a gap between automatic evaluation metrics and human judgment.
  - For machine translation, especially in low-resource languages, LLMs can outperform state-of-the-art fine-tuned models, suggesting their potential in handling diverse linguistic data.
  - LLMs excel in open-ended generation tasks, such as producing news articles and code synthesis, demonstrating their creativity and understanding, though the generated codes require careful testing for subtle bugs.",1. How do LLMs perform in various generation tasks compared to fine-tuned models and what are the implications?,"Questions:

1. What are the advantages of LLMs in generating human-like news articles compared to traditional models?
2. How do LLMs perform in summarization tasks when evaluated by humans versus automatic evaluation metrics like ROUGE?
3. In what ways do LLMs show superiority in machine translation tasks, especially for low-resource languages?
4. What role does pre-training data play in enhancing the translation capabilities of LLMs for both rich-resource and low-resource languages?
5. What are the challenges associated with using LLMs for code synthesis, despite their demonstrated ability in text-code generation and code repairing tasks?

Answer:

(p15.0) Due to their strong generation ability and creativity, LLMs show superiority at most generation tasks.

(p15.1) 4.2.1 Use case. Generation tasks require models to have a comprehensive understanding of the input contents or requirements and a certain level of creativity. This is what LLMs excel at.

(p15.2) For summarization tasks, although LLMs do not have an obvious advantage over fine-tuned models under traditional automatic evaluation metrics, such as ROUGE [60], human evaluation results indicate that humans tend to prefer the results generated by LLMs [38,127] compared to that of fine-tuned models. For example, on CNN/DailyMail [71] and XSUM [72], fine-tuned models like Brio [66] and Pegasus [125] have much better performance than any LLMs w.r.t.

(p15.3) ROUGE, but LLMs like OPT [126] perform far better in human evaluation considering all aspects including faithfulness, coherence, and relevance [127]. This demonstrates the superiority of LLMs in summarization tasks. On the other hand, it implies that current summarization benchmarks don't contain summaries with high quality or the automatic metrics are not proper for the evaluation of summarization.

(p15.4) In machine translation (MT), LLMs can perform competent translation, although the average performance is slightly worse than some commercial translation tools [45] considering some automatic metrics like BLEU [78]. LLMs are particularly good at translating some low-resource language texts to English texts, such as in the Romanian-English translation of WMT'16 [11], zero-shot or few-shot LLMs can perform better than SOTA fine-tuned model [22]. This is mainly due to the fact that English resources compose the main part of the pre-training data. BLOOM [92] is pre-trained on more multi-lingual data, leading to better translation quality in both rich-resource and low-resource translation.

(p15.5) Another interesting finding is that BLOOM achieves good translation quality among Romance languages, even for translation from Galician, which is not included in the pre-training data. One reasonable explanation is that texts from some languages in the same language group can help the LLMs learn more from the similarity. If more multi-lingual texts can be added to the pre-training data, the translation capability may be improved further.

(p15.6) Additionally, LLMs are highly skilled in open-ended generations. One example is that the news articles generated by LLMs are almost indistinguishable from real news articles by humans [16]. LLMs are remarkably adept at code synthesis as well. Either for text-code generation, such as HumanEval [18] and MBPP [7], or for code repairing, such as DeepFix [39], LLMs can perform pretty well. GPT-4 can even pass 25% problems in Leetcode, which are not trivial for most human coders [76]. With training on more code data, the coding capability of LLMs can be improved further [22].

(p15.7) While performing well on such tasks, the codes generated by LLMs should be tested carefully to figure out any subtle bugs, which is one of the main challenges for applying LLMs in code synthesis.","Questions:

1. How do LLMs perform in various generation tasks compared to fine-tuned models and what are the implications?

Answer:

(p15.0) Due to their strong generation ability and creativity, LLMs show superiority at most generation tasks.

(p15.1) 4.2.1 Use case. Generation tasks require models to have a comprehensive understanding of the input contents or requirements and a certain level of creativity. This is what LLMs excel at.

(p15.2) For summarization tasks, although LLMs do not have an obvious advantage over fine-tuned models under traditional automatic evaluation metrics, such as ROUGE [60], human evaluation results indicate that humans tend to prefer the results generated by LLMs [38,127] compared to that of fine-tuned models. For example, on CNN/DailyMail [71] and XSUM [72], fine-tuned models like Brio [66] and Pegasus [125] have much better performance than any LLMs w.r.t.

(p15.3) ROUGE, but LLMs like OPT [126] perform far better in human evaluation considering all aspects including faithfulness, coherence, and relevance [127]. This demonstrates the superiority of LLMs in summarization tasks. On the other hand, it implies that current summarization benchmarks don't contain summaries with high quality or the automatic metrics are not proper for the evaluation of summarization.

(p15.4) In machine translation (MT), LLMs can perform competent translation, although the average performance is slightly worse than some commercial translation tools [45] considering some automatic metrics like BLEU [78]. LLMs are particularly good at translating some low-resource language texts to English texts, such as in the Romanian-English translation of WMT'16 [11], zero-shot or few-shot LLMs can perform better than SOTA fine-tuned model [22]. This is mainly due to the fact that English resources compose the main part of the pre-training data. BLOOM [92] is pre-trained on more multi-lingual data, leading to better translation quality in both rich-resource and low-resource translation.

(p15.5) Another interesting finding is that BLOOM achieves good translation quality among Romance languages, even for translation from Galician, which is not included in the pre-training data. One reasonable explanation is that texts from some languages in the same language group can help the LLMs learn more from the similarity. If more multi-lingual texts can be added to the pre-training data, the translation capability may be improved further.

(p15.6) Additionally, LLMs are highly skilled in open-ended generations. One example is that the news articles generated by LLMs are almost indistinguishable from real news articles by humans [16]. LLMs are remarkably adept at code synthesis as well. Either for text-code generation, such as HumanEval [18] and MBPP [7], or for code repairing, such as DeepFix [39], LLMs can perform pretty well. GPT-4 can even pass 25% problems in Leetcode, which are not trivial for most human coders [76]. With training on more code data, the coding capability of LLMs can be improved further [22].

(p15.7) While performing well on such tasks, the codes generated by LLMs should be tested carefully to figure out any subtle bugs, which is one of the main challenges for applying LLMs in code synthesis."
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s18,Remark 4,"['p18.0', 'p18.1', 'p18.2', 'p18.3', 'p18.4', 'p18.5', 'p18.6', 'p18.7']","['(1) LLMs excel at knowledge-intensive tasks due to their massive real-world knowledge.', '(2) LLMs struggle when the knowledge requirements do not match their learned knowledge, or when they face tasks that only require contextual knowledge, in which case fine-tuned models can work as well as LLMs.', '4.3.1 Use case. In general, with billions of training tokens and parameters, LLMs have much more real-world knowledge than fine-tuned models.', 'Closed-book question-answering tasks require the model to answer a given question about factual knowledge without any external information. It does require the memorization of real-world knowledge in the model. LLMs perform better on nearly all datasets, such as on NaturalQuestions [52], WebQuestions [9], and TriviaQA [46]. On TriviaQA, even zero-shot LLMs is still much better [22].', ""The massive multitask language understanding (MMLU) [40] is also highly knowledge-intensive. Some tasks only require the model to capture the self-contained knowledge in the contexts. The knowledge in the contexts from the input is enough for the model to make predictions. For these tasks, small fine-tuned models can work pretty well. One such task is machine reading comprehension (MRC). An MRC task provides several paragraphs and requires the model to predict the answer to questions based on these paragraphs. We've discussed MRC in the previous section because it's also a traditional NLU task."", ""Another scenario is that the knowledge within LLMs about real world is useless to the task, or even the required knowledge is counterfactual to the real world. As a result, the LLMs cannot work well on such tasks. In some cases, inconsistent knowledge may even make the LLMs worse than random guessing. For example, in Big-Bench, the Mnist ascii task requires the model to tell the digit represented by an ASCII art. The capability required by this task is nothing about real-world knowledge. Also, in the Inverse Scaling Phenomenon competition [70], the task redefine math redefines a common symbol and requires the model to choose between the original meaning and the meaning derived from the redefinition. What it requires contrasts to the LLMs' knowledge, thus LLMs even perform worse than random guessing."", 'As an alternative to real-world knowledge in LLMs, access to extra knowledge is allowed, and models can thus get enough knowledge for a task via retrieval augmentation. The basic idea of retrieval augmentation is to add an extra information retrieval step prior to making predictions, in which, some useful texts related to the task will be retrieved from a large corpus. Then, the model will make predictions based on both the input contexts and the retrieved texts.', 'With retrieved additional information, the closed-book task can become ""open-book"". In such a scenario, fine-tuned models are pretty good with much smaller sizes, because the required knowledge can be obtained by retrieving. For example, on NaturalQuestions [52], with extra corpus, retrieval augmented models [44,48] are much better than any other methods.']","(1) LLMs excel at knowledge-intensive tasks due to their massive real-world knowledge.

(2) LLMs struggle when the knowledge requirements do not match their learned knowledge, or when they face tasks that only require contextual knowledge, in which case fine-tuned models can work as well as LLMs.

4.3.1 Use case. In general, with billions of training tokens and parameters, LLMs have much more real-world knowledge than fine-tuned models.

Closed-book question-answering tasks require the model to answer a given question about factual knowledge without any external information. It does require the memorization of real-world knowledge in the model. LLMs perform better on nearly all datasets, such as on NaturalQuestions [52], WebQuestions [9], and TriviaQA [46]. On TriviaQA, even zero-shot LLMs is still much better [22].

The massive multitask language understanding (MMLU) [40] is also highly knowledge-intensive. Some tasks only require the model to capture the self-contained knowledge in the contexts. The knowledge in the contexts from the input is enough for the model to make predictions. For these tasks, small fine-tuned models can work pretty well. One such task is machine reading comprehension (MRC). An MRC task provides several paragraphs and requires the model to predict the answer to questions based on these paragraphs. We've discussed MRC in the previous section because it's also a traditional NLU task.

Another scenario is that the knowledge within LLMs about real world is useless to the task, or even the required knowledge is counterfactual to the real world. As a result, the LLMs cannot work well on such tasks. In some cases, inconsistent knowledge may even make the LLMs worse than random guessing. For example, in Big-Bench, the Mnist ascii task requires the model to tell the digit represented by an ASCII art. The capability required by this task is nothing about real-world knowledge. Also, in the Inverse Scaling Phenomenon competition [70], the task redefine math redefines a common symbol and requires the model to choose between the original meaning and the meaning derived from the redefinition. What it requires contrasts to the LLMs' knowledge, thus LLMs even perform worse than random guessing.

As an alternative to real-world knowledge in LLMs, access to extra knowledge is allowed, and models can thus get enough knowledge for a task via retrieval augmentation. The basic idea of retrieval augmentation is to add an extra information retrieval step prior to making predictions, in which, some useful texts related to the task will be retrieved from a large corpus. Then, the model will make predictions based on both the input contexts and the retrieved texts.

With retrieved additional information, the closed-book task can become ""open-book"". In such a scenario, fine-tuned models are pretty good with much smaller sizes, because the required knowledge can be obtained by retrieving. For example, on NaturalQuestions [52], with extra corpus, retrieval augmented models [44,48] are much better than any other methods.","(p18.0) (1) LLMs excel at knowledge-intensive tasks due to their massive real-world knowledge.

(p18.1) (2) LLMs struggle when the knowledge requirements do not match their learned knowledge, or when they face tasks that only require contextual knowledge, in which case fine-tuned models can work as well as LLMs.

(p18.2) 4.3.1 Use case. In general, with billions of training tokens and parameters, LLMs have much more real-world knowledge than fine-tuned models.

(p18.3) Closed-book question-answering tasks require the model to answer a given question about factual knowledge without any external information. It does require the memorization of real-world knowledge in the model. LLMs perform better on nearly all datasets, such as on NaturalQuestions [52], WebQuestions [9], and TriviaQA [46]. On TriviaQA, even zero-shot LLMs is still much better [22].

(p18.4) The massive multitask language understanding (MMLU) [40] is also highly knowledge-intensive. Some tasks only require the model to capture the self-contained knowledge in the contexts. The knowledge in the contexts from the input is enough for the model to make predictions. For these tasks, small fine-tuned models can work pretty well. One such task is machine reading comprehension (MRC). An MRC task provides several paragraphs and requires the model to predict the answer to questions based on these paragraphs. We've discussed MRC in the previous section because it's also a traditional NLU task.

(p18.5) Another scenario is that the knowledge within LLMs about real world is useless to the task, or even the required knowledge is counterfactual to the real world. As a result, the LLMs cannot work well on such tasks. In some cases, inconsistent knowledge may even make the LLMs worse than random guessing. For example, in Big-Bench, the Mnist ascii task requires the model to tell the digit represented by an ASCII art. The capability required by this task is nothing about real-world knowledge. Also, in the Inverse Scaling Phenomenon competition [70], the task redefine math redefines a common symbol and requires the model to choose between the original meaning and the meaning derived from the redefinition. What it requires contrasts to the LLMs' knowledge, thus LLMs even perform worse than random guessing.

(p18.6) As an alternative to real-world knowledge in LLMs, access to extra knowledge is allowed, and models can thus get enough knowledge for a task via retrieval augmentation. The basic idea of retrieval augmentation is to add an extra information retrieval step prior to making predictions, in which, some useful texts related to the task will be retrieved from a large corpus. Then, the model will make predictions based on both the input contexts and the retrieved texts.

(p18.7) With retrieved additional information, the closed-book task can become ""open-book"". In such a scenario, fine-tuned models are pretty good with much smaller sizes, because the required knowledge can be obtained by retrieving. For example, on NaturalQuestions [52], with extra corpus, retrieval augmented models [44,48] are much better than any other methods.","[[], [], [], ['b43', 'b6', 'b19', 'b49'], ['b37'], ['b67'], [], ['b45', 'b41', 'b49']]","[[], [], [], ['b43', 'b6', 'b19', 'b49'], ['b37'], ['b67'], [], ['b45', 'b41', 'b49']]",9,"1. (1) LLMs excel at knowledge-intensive tasks due to their massive real-world knowledge.
2. (2) LLMs struggle when the knowledge requirements do not match their learned knowledge, or when they face tasks that only require contextual knowledge, in which case fine-tuned models can work as well as LLMs.
3. 4.3.1 Use case. In general, with billions of training tokens and parameters, LLMs have much more real-world knowledge than fine-tuned models.Closed-book question-answering tasks require the model to answer a given question about factual knowledge without any external information.
4. It does require the memorization of real-world knowledge in the model.
5. LLMs perform better on nearly all datasets, such as on NaturalQuestions (2)0, WebQuestions [9], and TriviaQA [46].
6. On TriviaQA, even zero-shot LLMs is still much better [22].
7. The massive multitask language understanding (MMLU) [40] is also highly knowledge-intensive.
8. Some tasks only require the model to capture the self-contained knowledge in the contexts.
9. The knowledge in the contexts from the input is enough for the model to make predictions.
10. For these tasks, small fine-tuned models can work pretty well.
11. One such task is machine reading comprehension (MRC).
12. An MRC task provides several paragraphs and requires the model to predict the answer to questions based on these paragraphs.
13. We've discussed MRC in the previous section because it's also a traditional NLU task.
14. Another scenario is that the knowledge within LLMs about real world is useless to the task, or even the required knowledge is counterfactual to the real world.
15. As a result, the LLMs cannot work well on such tasks.
16. In some cases, inconsistent knowledge may even make the LLMs worse than random guessing.
17. For example, in Big-Bench, the Mnist ascii task requires the model to tell the digit represented by an ASCII art.
18. The capability required by this task is nothing about real-world knowledge.
19. Also, in the Inverse Scaling Phenomenon competition [70], the task redefine math redefines a common symbol and requires the model to choose between the original meaning and the meaning derived from the redefinition.
20. What it requires contrasts to the LLMs' knowledge, thus LLMs even perform worse than random guessing.
21. As an alternative to real-world knowledge in LLMs, access to extra knowledge is allowed, and models can thus get enough knowledge for a task via retrieval augmentation.
22. The basic idea of retrieval augmentation is to add an extra information retrieval step prior to making predictions, in which, some useful texts related to the task will be retrieved from a large corpus.
23. Then, the model will make predictions based on both the input contexts and the retrieved texts.
24. With retrieved additional information, the closed-book task can become ""open-book"".
25. In such a scenario, fine-tuned models are pretty good with much smaller sizes, because the required knowledge can be obtained by retrieving.
26. For example, on NaturalQuestions (2)0, with extra corpus, retrieval augmented models (2)1 are much better than any other methods.","What are the strengths of Large Language Models (LLMs) in performing knowledge-intensive tasks?
How do LLMs perform on closed-book question-answering tasks compared to fine-tuned models?
In what scenarios do LLMs struggle due to the mismatch or irrelevance of their pre-learned knowledge?
How does retrieval augmentation improve the performance of models on tasks requiring external knowledge?
What is the impact of the Inverse Scaling Phenomenon on the performance of LLMs in tasks with redefined common symbols?","1. What are the strengths of Large Language Models (LLMs) in performing knowledge-intensive tasks?
2. How do LLMs perform on closed-book question-answering tasks compared to fine-tuned models?
3. In what scenarios do LLMs struggle due to the mismatch or irrelevance of their pre-learned knowledge?
4. How does retrieval augmentation improve the performance of models on tasks requiring external knowledge?
5. What is the impact of the Inverse Scaling Phenomenon on the performance of LLMs in tasks with redefined common symbols?",None,,"Questions:

1. What are the strengths of Large Language Models (LLMs) in performing knowledge-intensive tasks?
2. How do LLMs perform on closed-book question-answering tasks compared to fine-tuned models?
3. In what scenarios do LLMs struggle due to the mismatch or irrelevance of their pre-learned knowledge?
4. How does retrieval augmentation improve the performance of models on tasks requiring external knowledge?
5. What is the impact of the Inverse Scaling Phenomenon on the performance of LLMs in tasks with redefined common symbols?

Answer:

(p18.0) (1) LLMs excel at knowledge-intensive tasks due to their massive real-world knowledge.

(p18.1) (2) LLMs struggle when the knowledge requirements do not match their learned knowledge, or when they face tasks that only require contextual knowledge, in which case fine-tuned models can work as well as LLMs.

(p18.2) 4.3.1 Use case. In general, with billions of training tokens and parameters, LLMs have much more real-world knowledge than fine-tuned models.

(p18.3) Closed-book question-answering tasks require the model to answer a given question about factual knowledge without any external information. It does require the memorization of real-world knowledge in the model. LLMs perform better on nearly all datasets, such as on NaturalQuestions [52], WebQuestions [9], and TriviaQA [46]. On TriviaQA, even zero-shot LLMs is still much better [22].

(p18.4) The massive multitask language understanding (MMLU) [40] is also highly knowledge-intensive. Some tasks only require the model to capture the self-contained knowledge in the contexts. The knowledge in the contexts from the input is enough for the model to make predictions. For these tasks, small fine-tuned models can work pretty well. One such task is machine reading comprehension (MRC). An MRC task provides several paragraphs and requires the model to predict the answer to questions based on these paragraphs. We've discussed MRC in the previous section because it's also a traditional NLU task.

(p18.5) Another scenario is that the knowledge within LLMs about real world is useless to the task, or even the required knowledge is counterfactual to the real world. As a result, the LLMs cannot work well on such tasks. In some cases, inconsistent knowledge may even make the LLMs worse than random guessing. For example, in Big-Bench, the Mnist ascii task requires the model to tell the digit represented by an ASCII art. The capability required by this task is nothing about real-world knowledge. Also, in the Inverse Scaling Phenomenon competition [70], the task redefine math redefines a common symbol and requires the model to choose between the original meaning and the meaning derived from the redefinition. What it requires contrasts to the LLMs' knowledge, thus LLMs even perform worse than random guessing.

(p18.6) As an alternative to real-world knowledge in LLMs, access to extra knowledge is allowed, and models can thus get enough knowledge for a task via retrieval augmentation. The basic idea of retrieval augmentation is to add an extra information retrieval step prior to making predictions, in which, some useful texts related to the task will be retrieved from a large corpus. Then, the model will make predictions based on both the input contexts and the retrieved texts.

(p18.7) With retrieved additional information, the closed-book task can become ""open-book"". In such a scenario, fine-tuned models are pretty good with much smaller sizes, because the required knowledge can be obtained by retrieving. For example, on NaturalQuestions [52], with extra corpus, retrieval augmented models [44,48] are much better than any other methods.","Questions:



Answer:

(p18.0) (1) LLMs excel at knowledge-intensive tasks due to their massive real-world knowledge.

(p18.1) (2) LLMs struggle when the knowledge requirements do not match their learned knowledge, or when they face tasks that only require contextual knowledge, in which case fine-tuned models can work as well as LLMs.

(p18.2) 4.3.1 Use case. In general, with billions of training tokens and parameters, LLMs have much more real-world knowledge than fine-tuned models.

(p18.3) Closed-book question-answering tasks require the model to answer a given question about factual knowledge without any external information. It does require the memorization of real-world knowledge in the model. LLMs perform better on nearly all datasets, such as on NaturalQuestions [52], WebQuestions [9], and TriviaQA [46]. On TriviaQA, even zero-shot LLMs is still much better [22].

(p18.4) The massive multitask language understanding (MMLU) [40] is also highly knowledge-intensive. Some tasks only require the model to capture the self-contained knowledge in the contexts. The knowledge in the contexts from the input is enough for the model to make predictions. For these tasks, small fine-tuned models can work pretty well. One such task is machine reading comprehension (MRC). An MRC task provides several paragraphs and requires the model to predict the answer to questions based on these paragraphs. We've discussed MRC in the previous section because it's also a traditional NLU task.

(p18.5) Another scenario is that the knowledge within LLMs about real world is useless to the task, or even the required knowledge is counterfactual to the real world. As a result, the LLMs cannot work well on such tasks. In some cases, inconsistent knowledge may even make the LLMs worse than random guessing. For example, in Big-Bench, the Mnist ascii task requires the model to tell the digit represented by an ASCII art. The capability required by this task is nothing about real-world knowledge. Also, in the Inverse Scaling Phenomenon competition [70], the task redefine math redefines a common symbol and requires the model to choose between the original meaning and the meaning derived from the redefinition. What it requires contrasts to the LLMs' knowledge, thus LLMs even perform worse than random guessing.

(p18.6) As an alternative to real-world knowledge in LLMs, access to extra knowledge is allowed, and models can thus get enough knowledge for a task via retrieval augmentation. The basic idea of retrieval augmentation is to add an extra information retrieval step prior to making predictions, in which, some useful texts related to the task will be retrieved from a large corpus. Then, the model will make predictions based on both the input contexts and the retrieved texts.

(p18.7) With retrieved additional information, the closed-book task can become ""open-book"". In such a scenario, fine-tuned models are pretty good with much smaller sizes, because the required knowledge can be obtained by retrieving. For example, on NaturalQuestions [52], with extra corpus, retrieval augmented models [44,48] are much better than any other methods."
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s21,Use Case with Reasoning.,"['p21.0', 'p21.1']","[""Reasoning, which involves making sense of information, drawing inferences, and making decisions, is one of the essential aspects of human intelligence. It is challenging for NLP. Many existing reasoning tasks can be classified into commonsense reasoning and arithmetic reasoning. Arithmetic reasoning/problem solving. The arithmetic reasoning capability of LLMs benefits greatly from the scaling of model size. For GPT-3, the ability of two-digit addition only becomes apparent when the number of parameters exceeds 13B [16]. Tasks to test arithmetic reasoning are trivial for humans and designed to challenge the capability of transferring natural language into mathematical symbols and multi-step inference. On GSM8k [26], SVAMP [79] and AQuA [61], LLMs, as generalists, have competitive performance with most methods which have task-specific designs. And GPT-4 overperforms any other methods [76], even some huge models particularly tuned for arithmetic problems [104]. Nevertheless, it should be noted that, without the intervention of external tools, LLMs may occasionally make mistakes in performing basic calculations, although chain-of-thought (CoT) prompting [115] can significantly improve LLMs' ability in calculations. Commonsense reasoning. Commonsense reasoning not only requires LLMs to remember factual knowledge but also requires LLMs to do several inference steps about the facts. Commonsense reasoning increases gradually with the growth of model size. Compared to fine-tuned models, LLMs keep the superiority on most datasets, such as StrategyQA [36] and ARC-C [25]. Especially on ARC-C, which contains difficult questions in science exams from grade 3 to grade 9,"", 'GPT-4 has been close to the performance of 100% (96.3%) [76].']","Reasoning, which involves making sense of information, drawing inferences, and making decisions, is one of the essential aspects of human intelligence. It is challenging for NLP. Many existing reasoning tasks can be classified into commonsense reasoning and arithmetic reasoning. Arithmetic reasoning/problem solving. The arithmetic reasoning capability of LLMs benefits greatly from the scaling of model size. For GPT-3, the ability of two-digit addition only becomes apparent when the number of parameters exceeds 13B [16]. Tasks to test arithmetic reasoning are trivial for humans and designed to challenge the capability of transferring natural language into mathematical symbols and multi-step inference. On GSM8k [26], SVAMP [79] and AQuA [61], LLMs, as generalists, have competitive performance with most methods which have task-specific designs. And GPT-4 overperforms any other methods [76], even some huge models particularly tuned for arithmetic problems [104]. Nevertheless, it should be noted that, without the intervention of external tools, LLMs may occasionally make mistakes in performing basic calculations, although chain-of-thought (CoT) prompting [115] can significantly improve LLMs' ability in calculations. Commonsense reasoning. Commonsense reasoning not only requires LLMs to remember factual knowledge but also requires LLMs to do several inference steps about the facts. Commonsense reasoning increases gradually with the growth of model size. Compared to fine-tuned models, LLMs keep the superiority on most datasets, such as StrategyQA [36] and ARC-C [25]. Especially on ARC-C, which contains difficult questions in science exams from grade 3 to grade 9,

GPT-4 has been close to the performance of 100% (96.3%) [76].","(p21.0) Reasoning, which involves making sense of information, drawing inferences, and making decisions, is one of the essential aspects of human intelligence. It is challenging for NLP. Many existing reasoning tasks can be classified into commonsense reasoning and arithmetic reasoning. Arithmetic reasoning/problem solving. The arithmetic reasoning capability of LLMs benefits greatly from the scaling of model size. For GPT-3, the ability of two-digit addition only becomes apparent when the number of parameters exceeds 13B [16]. Tasks to test arithmetic reasoning are trivial for humans and designed to challenge the capability of transferring natural language into mathematical symbols and multi-step inference. On GSM8k [26], SVAMP [79] and AQuA [61], LLMs, as generalists, have competitive performance with most methods which have task-specific designs. And GPT-4 overperforms any other methods [76], even some huge models particularly tuned for arithmetic problems [104]. Nevertheless, it should be noted that, without the intervention of external tools, LLMs may occasionally make mistakes in performing basic calculations, although chain-of-thought (CoT) prompting [115] can significantly improve LLMs' ability in calculations. Commonsense reasoning. Commonsense reasoning not only requires LLMs to remember factual knowledge but also requires LLMs to do several inference steps about the facts. Commonsense reasoning increases gradually with the growth of model size. Compared to fine-tuned models, LLMs keep the superiority on most datasets, such as StrategyQA [36] and ARC-C [25]. Especially on ARC-C, which contains difficult questions in science exams from grade 3 to grade 9,

(p21.1) GPT-4 has been close to the performance of 100% (96.3%) [76].","[['b101', 'b22', 'b73', 'b13', 'b23', 'b112', 'b58', 'b76', 'b33'], ['b73']]","[['b101', 'b22', 'b73', 'b13', 'b23', 'b112', 'b58', 'b76', 'b33'], ['b73']]",10,"1. Reasoning, which involves making sense of information, drawing inferences, and making decisions, is one of the essential aspects of human intelligence.
2. It is challenging for NLP. Many existing reasoning tasks can be classified into commonsense reasoning and arithmetic reasoning.
3. Arithmetic reasoning/problem solving.
4. The arithmetic reasoning capability of LLMs benefits greatly from the scaling of model size.
5. For GPT-3, the ability of two-digit addition only becomes apparent when the number of parameters exceeds 13B [16].
6. Tasks to test arithmetic reasoning are trivial for humans and designed to challenge the capability of transferring natural language into mathematical symbols and multi-step inference.
7. On GSM8k [26], SVAMP [79] and AQuA [61], LLMs, as generalists, have competitive performance with most methods which have task-specific designs.
8. And GPT-4 overperforms any other methods [26]1, even some huge models particularly tuned for arithmetic problems [104].
9. Nevertheless, it should be noted that, without the intervention of external tools, LLMs may occasionally make mistakes in performing basic calculations, although chain-of-thought (CoT) prompting [115] can significantly improve LLMs' ability in calculations.
10. Commonsense reasoning. Commonsense reasoning not only requires LLMs to remember factual knowledge but also requires LLMs to do several inference steps about the facts.
11. Commonsense reasoning increases gradually with the growth of model size.
12. Compared to fine-tuned models, LLMs keep the superiority on most datasets, such as StrategyQA [36] and ARC-C [25].
13. Especially on ARC-C, which contains difficult questions in science exams from grade 3 to grade 9,GPT-4 has been close to the performance of 100% [26]0 [26]1.","How does the arithmetic reasoning capability of LLMs like GPT-3 change with the scaling of model size?
What are the challenges LLMs face in performing basic calculations without external tools, and how does chain-of-thought prompting improve their performance?
In what ways does commonsense reasoning in LLMs depend on model size, and how does this affect their performance on datasets like StrategyQA and ARC-C?
How does GPT-4's performance on arithmetic reasoning and commonsense reasoning tasks compare to task-specific models and previous versions of GPT?
What role does the number of parameters play in the ability of LLMs to perform two-digit addition, and at what threshold does this ability become apparent?","1. How does the arithmetic reasoning capability of LLMs like GPT-3 change with the scaling of model size?
2. What are the challenges LLMs face in performing basic calculations without external tools, and how does chain-of-thought prompting improve their performance?
3. In what ways does commonsense reasoning in LLMs depend on model size, and how does this affect their performance on datasets like StrategyQA and ARC-C?
4. How does GPT-4's performance on arithmetic reasoning and commonsense reasoning tasks compare to task-specific models and previous versions of GPT?
5. What role does the number of parameters play in the ability of LLMs to perform two-digit addition, and at what threshold does this ability become apparent?","# How does the scaling of model size influence the reasoning capabilities of Large Language Models (LLMs) in arithmetic and commonsense reasoning tasks?
  - For arithmetic reasoning, the capability of LLMs like GPT-3 to perform tasks such as two-digit addition becomes significantly apparent only when the model size exceeds 13 billion parameters, indicating a direct correlation between model size and arithmetic reasoning performance.
  - In commonsense reasoning, the performance of LLMs also improves with the increase in model size, maintaining superiority over most fine-tuned models across various datasets, which suggests that larger models are better at inferring and making sense of factual knowledge.
  - The use of chain-of-thought (CoT) prompting has been shown to significantly enhance the calculation abilities of LLMs, suggesting that beyond scaling, specific prompting techniques can further improve reasoning performance.

# What are the implications of LLMs' performance on specialized reasoning tasks compared to task-specific models?
  - On arithmetic reasoning tasks such as those found in GSM8k, SVAMP, and AQuA, LLMs demonstrate competitive performance against most methods designed specifically for these tasks, with GPT-4 outperforming all other methods, indicating that LLMs can rival or exceed specialized models in arithmetic reasoning.
  - In commonsense reasoning, LLMs like GPT-4 achieve near-human performance levels on challenging datasets like ARC-C, which contains science exam questions for grades 3 to 9, showcasing the potential of LLMs to handle complex inference steps and factual knowledge recall without the need for task-specific tuning.
  - Despite their high performance, LLMs may still occasionally make mistakes in basic calculations without external tools, highlighting a limitation in their current reasoning capabilities and the need for further improvements or the integration of external computational tools.","1. How does the scaling of model size influence the reasoning capabilities of Large Language Models (LLMs) in arithmetic and commonsense reasoning tasks?
2. What are the implications of LLMs' performance on specialized reasoning tasks compared to task-specific models?","Questions:

1. How does the arithmetic reasoning capability of LLMs like GPT-3 change with the scaling of model size?
2. What are the challenges LLMs face in performing basic calculations without external tools, and how does chain-of-thought prompting improve their performance?
3. In what ways does commonsense reasoning in LLMs depend on model size, and how does this affect their performance on datasets like StrategyQA and ARC-C?
4. How does GPT-4's performance on arithmetic reasoning and commonsense reasoning tasks compare to task-specific models and previous versions of GPT?
5. What role does the number of parameters play in the ability of LLMs to perform two-digit addition, and at what threshold does this ability become apparent?

Answer:

(p21.0) Reasoning, which involves making sense of information, drawing inferences, and making decisions, is one of the essential aspects of human intelligence. It is challenging for NLP. Many existing reasoning tasks can be classified into commonsense reasoning and arithmetic reasoning. Arithmetic reasoning/problem solving. The arithmetic reasoning capability of LLMs benefits greatly from the scaling of model size. For GPT-3, the ability of two-digit addition only becomes apparent when the number of parameters exceeds 13B [16]. Tasks to test arithmetic reasoning are trivial for humans and designed to challenge the capability of transferring natural language into mathematical symbols and multi-step inference. On GSM8k [26], SVAMP [79] and AQuA [61], LLMs, as generalists, have competitive performance with most methods which have task-specific designs. And GPT-4 overperforms any other methods [76], even some huge models particularly tuned for arithmetic problems [104]. Nevertheless, it should be noted that, without the intervention of external tools, LLMs may occasionally make mistakes in performing basic calculations, although chain-of-thought (CoT) prompting [115] can significantly improve LLMs' ability in calculations. Commonsense reasoning. Commonsense reasoning not only requires LLMs to remember factual knowledge but also requires LLMs to do several inference steps about the facts. Commonsense reasoning increases gradually with the growth of model size. Compared to fine-tuned models, LLMs keep the superiority on most datasets, such as StrategyQA [36] and ARC-C [25]. Especially on ARC-C, which contains difficult questions in science exams from grade 3 to grade 9,

(p21.1) GPT-4 has been close to the performance of 100% (96.3%) [76].","Questions:

1. How does the scaling of model size influence the reasoning capabilities of Large Language Models (LLMs) in arithmetic and commonsense reasoning tasks?
2. What are the implications of LLMs' performance on specialized reasoning tasks compared to task-specific models?

Answer:

(p21.0) Reasoning, which involves making sense of information, drawing inferences, and making decisions, is one of the essential aspects of human intelligence. It is challenging for NLP. Many existing reasoning tasks can be classified into commonsense reasoning and arithmetic reasoning. Arithmetic reasoning/problem solving. The arithmetic reasoning capability of LLMs benefits greatly from the scaling of model size. For GPT-3, the ability of two-digit addition only becomes apparent when the number of parameters exceeds 13B [16]. Tasks to test arithmetic reasoning are trivial for humans and designed to challenge the capability of transferring natural language into mathematical symbols and multi-step inference. On GSM8k [26], SVAMP [79] and AQuA [61], LLMs, as generalists, have competitive performance with most methods which have task-specific designs. And GPT-4 overperforms any other methods [76], even some huge models particularly tuned for arithmetic problems [104]. Nevertheless, it should be noted that, without the intervention of external tools, LLMs may occasionally make mistakes in performing basic calculations, although chain-of-thought (CoT) prompting [115] can significantly improve LLMs' ability in calculations. Commonsense reasoning. Commonsense reasoning not only requires LLMs to remember factual knowledge but also requires LLMs to do several inference steps about the facts. Commonsense reasoning increases gradually with the growth of model size. Compared to fine-tuned models, LLMs keep the superiority on most datasets, such as StrategyQA [36] and ARC-C [25]. Especially on ARC-C, which contains difficult questions in science exams from grade 3 to grade 9,

(p21.1) GPT-4 has been close to the performance of 100% (96.3%) [76]."
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s22,Use Cases with Emergent Abilities.,"['p22.0', 'p22.1', 'p22.2']","['Scaling of models also endows the model with some unprecedented, fantastic abilities that go beyond the power-law rule. These abilities are called ""emergent ability"". As defined in [113], emergent abilities of LLMs are abilities that are not present in smaller-scale models but are present in large-scale models. This means such abilities cannot be predicted by extrapolating the performance improvements on smaller-scale models and the model suddenly gains good performance on some tasks once the scale exceeds a certain range. The emergent ability is typically unpredictable and surprising, leading to tasks that emerge randomly or unexpectedly. We examine concrete examples of the emergent abilities of LLMs and provide them as an important reference for deciding whether to leverage LLMs\' emergent abilities.', 'Handling word manipulation is a typical emergent ability. It refers to the ability to learn symbolic manipulations, such as the reversed words [16], in which the model is given a word spelled backwards, and must output the original word.', 'For example. GPT-3 [16] shows the emergent ability for word sorting, and word unscrambling tasks. PaLM [22] exhibits the emergent ability on ASCII word recognition 4 and hyperbaton 5 task. The logical abilities of language models tend to emerge as the model scales up, such as logical deduction, logical sequence, and logic grid puzzles. Additionally, other tasks, such as advanced coding (e.g., auto debugging, code line description), and concept understanding (e.g., novel concepts, simple Turing concepts), are also use cases with the emergent abilities of large language models.']","Scaling of models also endows the model with some unprecedented, fantastic abilities that go beyond the power-law rule. These abilities are called ""emergent ability"". As defined in [113], emergent abilities of LLMs are abilities that are not present in smaller-scale models but are present in large-scale models. This means such abilities cannot be predicted by extrapolating the performance improvements on smaller-scale models and the model suddenly gains good performance on some tasks once the scale exceeds a certain range. The emergent ability is typically unpredictable and surprising, leading to tasks that emerge randomly or unexpectedly. We examine concrete examples of the emergent abilities of LLMs and provide them as an important reference for deciding whether to leverage LLMs' emergent abilities.

Handling word manipulation is a typical emergent ability. It refers to the ability to learn symbolic manipulations, such as the reversed words [16], in which the model is given a word spelled backwards, and must output the original word.

For example. GPT-3 [16] shows the emergent ability for word sorting, and word unscrambling tasks. PaLM [22] exhibits the emergent ability on ASCII word recognition 4 and hyperbaton 5 task. The logical abilities of language models tend to emerge as the model scales up, such as logical deduction, logical sequence, and logic grid puzzles. Additionally, other tasks, such as advanced coding (e.g., auto debugging, code line description), and concept understanding (e.g., novel concepts, simple Turing concepts), are also use cases with the emergent abilities of large language models.","(p22.0) Scaling of models also endows the model with some unprecedented, fantastic abilities that go beyond the power-law rule. These abilities are called ""emergent ability"". As defined in [113], emergent abilities of LLMs are abilities that are not present in smaller-scale models but are present in large-scale models. This means such abilities cannot be predicted by extrapolating the performance improvements on smaller-scale models and the model suddenly gains good performance on some tasks once the scale exceeds a certain range. The emergent ability is typically unpredictable and surprising, leading to tasks that emerge randomly or unexpectedly. We examine concrete examples of the emergent abilities of LLMs and provide them as an important reference for deciding whether to leverage LLMs' emergent abilities.

(p22.1) Handling word manipulation is a typical emergent ability. It refers to the ability to learn symbolic manipulations, such as the reversed words [16], in which the model is given a word spelled backwards, and must output the original word.

(p22.2) For example. GPT-3 [16] shows the emergent ability for word sorting, and word unscrambling tasks. PaLM [22] exhibits the emergent ability on ASCII word recognition 4 and hyperbaton 5 task. The logical abilities of language models tend to emerge as the model scales up, such as logical deduction, logical sequence, and logic grid puzzles. Additionally, other tasks, such as advanced coding (e.g., auto debugging, code line description), and concept understanding (e.g., novel concepts, simple Turing concepts), are also use cases with the emergent abilities of large language models.","[['b110'], ['b13'], ['b19', 'b13']]","[['b110'], ['b13'], ['b19', 'b13']]",4,"1. Scaling of models also endows the model with some unprecedented, fantastic abilities that go beyond the power-law rule.
2. These abilities are called ""emergent ability"".
3. As defined in [113], emergent abilities of LLMs are abilities that are not present in smaller-scale models but are present in large-scale models.
4. This means such abilities cannot be predicted by extrapolating the performance improvements on smaller-scale models and the model suddenly gains good performance on some tasks once the scale exceeds a certain range.
5. The emergent ability is typically unpredictable and surprising, leading to tasks that emerge randomly or unexpectedly.
6. We examine concrete examples of the emergent abilities of LLMs and provide them as an important reference for deciding whether to leverage LLMs' emergent abilities.
7. Handling word manipulation is a typical emergent ability.
8. It refers to the ability to learn symbolic manipulations, such as the reversed words [16], in which the model is given a word spelled backwards, and must output the original word.
9. For example. GPT-3 [16] shows the emergent ability for word sorting, and word unscrambling tasks.
10. PaLM [22] exhibits the emergent ability on ASCII word recognition 4 and hyperbaton 5 task.
11. The logical abilities of language models tend to emerge as the model scales up, such as logical deduction, logical sequence, and logic grid puzzles.
12. Additionally, other tasks, such as advanced coding (e.g., auto debugging, code line description), and concept understanding (e.g., novel concepts, simple Turing concepts), are also use cases with the emergent abilities of large language models.","What defines an ""emergent ability"" in the context of large language models (LLMs), and how does it differ from abilities in smaller-scale models?
How does the scaling of models contribute to the development of emergent abilities in LLMs?
Can you provide examples of tasks that demonstrate the emergent abilities of GPT-3 and PaLM?
In what ways do logical abilities manifest as emergent abilities in large-scale language models?
What are some examples of advanced tasks that LLMs can perform due to their emergent abilities, such as in coding or concept understanding?","1. What defines an ""emergent ability"" in the context of large language models (LLMs), and how does it differ from abilities in smaller-scale models?
2. How does the scaling of models contribute to the development of emergent abilities in LLMs?
3. Can you provide examples of tasks that demonstrate the emergent abilities of GPT-3 and PaLM?
4. In what ways do logical abilities manifest as emergent abilities in large-scale language models?
5. What are some examples of advanced tasks that LLMs can perform due to their emergent abilities, such as in coding or concept understanding?",None,,"Questions:

1. What defines an ""emergent ability"" in the context of large language models (LLMs), and how does it differ from abilities in smaller-scale models?
2. How does the scaling of models contribute to the development of emergent abilities in LLMs?
3. Can you provide examples of tasks that demonstrate the emergent abilities of GPT-3 and PaLM?
4. In what ways do logical abilities manifest as emergent abilities in large-scale language models?
5. What are some examples of advanced tasks that LLMs can perform due to their emergent abilities, such as in coding or concept understanding?

Answer:

(p22.0) Scaling of models also endows the model with some unprecedented, fantastic abilities that go beyond the power-law rule. These abilities are called ""emergent ability"". As defined in [113], emergent abilities of LLMs are abilities that are not present in smaller-scale models but are present in large-scale models. This means such abilities cannot be predicted by extrapolating the performance improvements on smaller-scale models and the model suddenly gains good performance on some tasks once the scale exceeds a certain range. The emergent ability is typically unpredictable and surprising, leading to tasks that emerge randomly or unexpectedly. We examine concrete examples of the emergent abilities of LLMs and provide them as an important reference for deciding whether to leverage LLMs' emergent abilities.

(p22.1) Handling word manipulation is a typical emergent ability. It refers to the ability to learn symbolic manipulations, such as the reversed words [16], in which the model is given a word spelled backwards, and must output the original word.

(p22.2) For example. GPT-3 [16] shows the emergent ability for word sorting, and word unscrambling tasks. PaLM [22] exhibits the emergent ability on ASCII word recognition 4 and hyperbaton 5 task. The logical abilities of language models tend to emerge as the model scales up, such as logical deduction, logical sequence, and logic grid puzzles. Additionally, other tasks, such as advanced coding (e.g., auto debugging, code line description), and concept understanding (e.g., novel concepts, simple Turing concepts), are also use cases with the emergent abilities of large language models.","Questions:



Answer:

(p22.0) Scaling of models also endows the model with some unprecedented, fantastic abilities that go beyond the power-law rule. These abilities are called ""emergent ability"". As defined in [113], emergent abilities of LLMs are abilities that are not present in smaller-scale models but are present in large-scale models. This means such abilities cannot be predicted by extrapolating the performance improvements on smaller-scale models and the model suddenly gains good performance on some tasks once the scale exceeds a certain range. The emergent ability is typically unpredictable and surprising, leading to tasks that emerge randomly or unexpectedly. We examine concrete examples of the emergent abilities of LLMs and provide them as an important reference for deciding whether to leverage LLMs' emergent abilities.

(p22.1) Handling word manipulation is a typical emergent ability. It refers to the ability to learn symbolic manipulations, such as the reversed words [16], in which the model is given a word spelled backwards, and must output the original word.

(p22.2) For example. GPT-3 [16] shows the emergent ability for word sorting, and word unscrambling tasks. PaLM [22] exhibits the emergent ability on ASCII word recognition 4 and hyperbaton 5 task. The logical abilities of language models tend to emerge as the model scales up, such as logical deduction, logical sequence, and logic grid puzzles. Additionally, other tasks, such as advanced coding (e.g., auto debugging, code line description), and concept understanding (e.g., novel concepts, simple Turing concepts), are also use cases with the emergent abilities of large language models."
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s23,No-Use Cases and Understanding.,"['p23.0', 'p23.1', 'p23.2', 'p23.3']","['Although in most cases, as discussed above, larger models bring better performance, there are still many exceptions that should be considered when choosing the appropriate model.', 'On certain tasks, with the size of LLMs increasing, the performance begins to decrease, such as Redefine-math: tests whether language models are able to work with common symbols when they are redefined to mean something else; Intothe-unknown: requires the model to choose which piece of information would help answer a question; Memo-trap: asks an LM to write a phrase in a way that starts like a famous quote but ends differently 6 . This is also called Inverse Scaling Phenomenon. Another interesting phenomenon observed in the scaling of LLMs is called the U-shaped Phenomenon [114].', 'As the name implies, This phenomenon refers to that as LLM size increases, their performance on certain tasks initially improves but then starts to decline before eventually improving again, such as on: Hindsight-neglect: it tests whether language models are able to assess whether a bet was worth taking based on its expected value; NegationQA: this task takes an existing multiple-choice dataset and negates a part of each question to see if language models are sensitive to negation; Quote-repetition: it asks models to repeat back sentences given in the prompt, with few-shot examples to help it recognize the task. Hence the risk of diminishing performance should be noted and if the task is similar to those we just discussed, careful consideration should be given to whether or not to use huge LLMs.', ""Gaining a deeper understanding of emergent abilities, inverse scaling phenomenon and U-shape phenomenon in LLMs is essential for advancing research in this field. In a certain sense, the U-shape phenomenon suggests that small-scale models and huge-scale models make predictions with different internal mechanisms. From this perspective, the U-shape phenomenon can be seen as a transformation of the inverse-scaling phenomenon due to some emergent abilities from sufficiently large models [114]. GPT-4 [76] exhibits a reversal of the inverse scaling phenomenon in some cases, such as on a task called Hindsight Neglect. The explanation for these behaviors of LLMs during scaling is still an open problem. Several hypotheses have been proposed. For emergent abilities, one explanation is that there may be multiple key steps for a task and the LLM cannot handle this task until it's large enough to handle every step, and another explanation is focused on the granularity of evaluation metrics [113]. For inverse-scaling phenomenon and u-shape phenomenon, the explanations mainly focus on the model's over-reliance on information from its prior rather than the input prompts, valid but misleading few-shot examples, and distracting easier tasks within a hard task [114].""]","Although in most cases, as discussed above, larger models bring better performance, there are still many exceptions that should be considered when choosing the appropriate model.

On certain tasks, with the size of LLMs increasing, the performance begins to decrease, such as Redefine-math: tests whether language models are able to work with common symbols when they are redefined to mean something else; Intothe-unknown: requires the model to choose which piece of information would help answer a question; Memo-trap: asks an LM to write a phrase in a way that starts like a famous quote but ends differently 6 . This is also called Inverse Scaling Phenomenon. Another interesting phenomenon observed in the scaling of LLMs is called the U-shaped Phenomenon [114].

As the name implies, This phenomenon refers to that as LLM size increases, their performance on certain tasks initially improves but then starts to decline before eventually improving again, such as on: Hindsight-neglect: it tests whether language models are able to assess whether a bet was worth taking based on its expected value; NegationQA: this task takes an existing multiple-choice dataset and negates a part of each question to see if language models are sensitive to negation; Quote-repetition: it asks models to repeat back sentences given in the prompt, with few-shot examples to help it recognize the task. Hence the risk of diminishing performance should be noted and if the task is similar to those we just discussed, careful consideration should be given to whether or not to use huge LLMs.

Gaining a deeper understanding of emergent abilities, inverse scaling phenomenon and U-shape phenomenon in LLMs is essential for advancing research in this field. In a certain sense, the U-shape phenomenon suggests that small-scale models and huge-scale models make predictions with different internal mechanisms. From this perspective, the U-shape phenomenon can be seen as a transformation of the inverse-scaling phenomenon due to some emergent abilities from sufficiently large models [114]. GPT-4 [76] exhibits a reversal of the inverse scaling phenomenon in some cases, such as on a task called Hindsight Neglect. The explanation for these behaviors of LLMs during scaling is still an open problem. Several hypotheses have been proposed. For emergent abilities, one explanation is that there may be multiple key steps for a task and the LLM cannot handle this task until it's large enough to handle every step, and another explanation is focused on the granularity of evaluation metrics [113]. For inverse-scaling phenomenon and u-shape phenomenon, the explanations mainly focus on the model's over-reliance on information from its prior rather than the input prompts, valid but misleading few-shot examples, and distracting easier tasks within a hard task [114].","(p23.0) Although in most cases, as discussed above, larger models bring better performance, there are still many exceptions that should be considered when choosing the appropriate model.

(p23.1) On certain tasks, with the size of LLMs increasing, the performance begins to decrease, such as Redefine-math: tests whether language models are able to work with common symbols when they are redefined to mean something else; Intothe-unknown: requires the model to choose which piece of information would help answer a question; Memo-trap: asks an LM to write a phrase in a way that starts like a famous quote but ends differently 6 . This is also called Inverse Scaling Phenomenon. Another interesting phenomenon observed in the scaling of LLMs is called the U-shaped Phenomenon [114].

(p23.2) As the name implies, This phenomenon refers to that as LLM size increases, their performance on certain tasks initially improves but then starts to decline before eventually improving again, such as on: Hindsight-neglect: it tests whether language models are able to assess whether a bet was worth taking based on its expected value; NegationQA: this task takes an existing multiple-choice dataset and negates a part of each question to see if language models are sensitive to negation; Quote-repetition: it asks models to repeat back sentences given in the prompt, with few-shot examples to help it recognize the task. Hence the risk of diminishing performance should be noted and if the task is similar to those we just discussed, careful consideration should be given to whether or not to use huge LLMs.

(p23.3) Gaining a deeper understanding of emergent abilities, inverse scaling phenomenon and U-shape phenomenon in LLMs is essential for advancing research in this field. In a certain sense, the U-shape phenomenon suggests that small-scale models and huge-scale models make predictions with different internal mechanisms. From this perspective, the U-shape phenomenon can be seen as a transformation of the inverse-scaling phenomenon due to some emergent abilities from sufficiently large models [114]. GPT-4 [76] exhibits a reversal of the inverse scaling phenomenon in some cases, such as on a task called Hindsight Neglect. The explanation for these behaviors of LLMs during scaling is still an open problem. Several hypotheses have been proposed. For emergent abilities, one explanation is that there may be multiple key steps for a task and the LLM cannot handle this task until it's large enough to handle every step, and another explanation is focused on the granularity of evaluation metrics [113]. For inverse-scaling phenomenon and u-shape phenomenon, the explanations mainly focus on the model's over-reliance on information from its prior rather than the input prompts, valid but misleading few-shot examples, and distracting easier tasks within a hard task [114].","[[], ['b111', 'b3'], [], ['b111', 'b73', 'b110']]","[[], ['b111', 'b3'], [], ['b111', 'b73', 'b110']]",5,"1. Although in most cases, as discussed above, larger models bring better performance, there are still many exceptions that should be considered when choosing the appropriate model.
2. On certain tasks, with the size of LLMs increasing, the performance begins to decrease, such as Redefine-math: tests whether language models are able to work with common symbols when they are redefined to mean something else; Intothe-unknown: requires the model to choose which piece of information would help answer a question; Memo-trap: asks an LM to write a phrase in a way that starts like a famous quote but ends differently 6 .
3. This is also called Inverse Scaling Phenomenon.
4. Another interesting phenomenon observed in the scaling of LLMs is called the U-shaped Phenomenon [114].
5. As the name implies, This phenomenon refers to that as LLM size increases, their performance on certain tasks initially improves but then starts to decline before eventually improving again, such as on: Hindsight-neglect: it tests whether language models are able to assess whether a bet was worth taking based on its expected value; NegationQA: this task takes an existing multiple-choice dataset and negates a part of each question to see if language models are sensitive to negation; Quote-repetition: it asks models to repeat back sentences given in the prompt, with few-shot examples to help it recognize the task.
6. Hence the risk of diminishing performance should be noted and if the task is similar to those we just discussed, careful consideration should be given to whether or not to use huge LLMs.
7. Gaining a deeper understanding of emergent abilities, inverse scaling phenomenon and U-shape phenomenon in LLMs is essential for advancing research in this field.
8. In a certain sense, the U-shape phenomenon suggests that small-scale models and huge-scale models make predictions with different internal mechanisms.
9. From this perspective, the U-shape phenomenon can be seen as a transformation of the inverse-scaling phenomenon due to some emergent abilities from sufficiently large models [114].
10. GPT-4 [76] exhibits a reversal of the inverse scaling phenomenon in some cases, such as on a task called Hindsight Neglect.
11. The explanation for these behaviors of LLMs during scaling is still an open problem.
12. Several hypotheses have been proposed.
13. For emergent abilities, one explanation is that there may be multiple key steps for a task and the LLM cannot handle this task until it's large enough to handle every step, and another explanation is focused on the granularity of evaluation metrics [113].
14. For inverse-scaling phenomenon and u-shape phenomenon, the explanations mainly focus on the model's over-reliance on information from its prior rather than the input prompts, valid but misleading few-shot examples, and distracting easier tasks within a hard task [114].","What are the specific tasks that demonstrate the inverse scaling phenomenon in large language models (LLMs)?
How does the U-shaped phenomenon manifest in the performance of LLMs across different tasks?
Can you explain the potential reasons behind the emergent abilities observed in large-scale LLMs?
What hypotheses have been proposed to explain the inverse scaling and U-shaped phenomena in LLM scaling?
How does GPT-4 exhibit a reversal of the inverse scaling phenomenon, particularly in the context of the Hindsight Neglect task?","1. What are the specific tasks that demonstrate the inverse scaling phenomenon in large language models (LLMs)?
2. How does the U-shaped phenomenon manifest in the performance of LLMs across different tasks?
3. Can you explain the potential reasons behind the emergent abilities observed in large-scale LLMs?
4. What hypotheses have been proposed to explain the inverse scaling and U-shaped phenomena in LLM scaling?
5. How does GPT-4 exhibit a reversal of the inverse scaling phenomenon, particularly in the context of the Hindsight Neglect task?",None,,"Questions:

1. What are the specific tasks that demonstrate the inverse scaling phenomenon in large language models (LLMs)?
2. How does the U-shaped phenomenon manifest in the performance of LLMs across different tasks?
3. Can you explain the potential reasons behind the emergent abilities observed in large-scale LLMs?
4. What hypotheses have been proposed to explain the inverse scaling and U-shaped phenomena in LLM scaling?
5. How does GPT-4 exhibit a reversal of the inverse scaling phenomenon, particularly in the context of the Hindsight Neglect task?

Answer:

(p23.0) Although in most cases, as discussed above, larger models bring better performance, there are still many exceptions that should be considered when choosing the appropriate model.

(p23.1) On certain tasks, with the size of LLMs increasing, the performance begins to decrease, such as Redefine-math: tests whether language models are able to work with common symbols when they are redefined to mean something else; Intothe-unknown: requires the model to choose which piece of information would help answer a question; Memo-trap: asks an LM to write a phrase in a way that starts like a famous quote but ends differently 6 . This is also called Inverse Scaling Phenomenon. Another interesting phenomenon observed in the scaling of LLMs is called the U-shaped Phenomenon [114].

(p23.2) As the name implies, This phenomenon refers to that as LLM size increases, their performance on certain tasks initially improves but then starts to decline before eventually improving again, such as on: Hindsight-neglect: it tests whether language models are able to assess whether a bet was worth taking based on its expected value; NegationQA: this task takes an existing multiple-choice dataset and negates a part of each question to see if language models are sensitive to negation; Quote-repetition: it asks models to repeat back sentences given in the prompt, with few-shot examples to help it recognize the task. Hence the risk of diminishing performance should be noted and if the task is similar to those we just discussed, careful consideration should be given to whether or not to use huge LLMs.

(p23.3) Gaining a deeper understanding of emergent abilities, inverse scaling phenomenon and U-shape phenomenon in LLMs is essential for advancing research in this field. In a certain sense, the U-shape phenomenon suggests that small-scale models and huge-scale models make predictions with different internal mechanisms. From this perspective, the U-shape phenomenon can be seen as a transformation of the inverse-scaling phenomenon due to some emergent abilities from sufficiently large models [114]. GPT-4 [76] exhibits a reversal of the inverse scaling phenomenon in some cases, such as on a task called Hindsight Neglect. The explanation for these behaviors of LLMs during scaling is still an open problem. Several hypotheses have been proposed. For emergent abilities, one explanation is that there may be multiple key steps for a task and the LLM cannot handle this task until it's large enough to handle every step, and another explanation is focused on the granularity of evaluation metrics [113]. For inverse-scaling phenomenon and u-shape phenomenon, the explanations mainly focus on the model's over-reliance on information from its prior rather than the input prompts, valid but misleading few-shot examples, and distracting easier tasks within a hard task [114].","Questions:



Answer:

(p23.0) Although in most cases, as discussed above, larger models bring better performance, there are still many exceptions that should be considered when choosing the appropriate model.

(p23.1) On certain tasks, with the size of LLMs increasing, the performance begins to decrease, such as Redefine-math: tests whether language models are able to work with common symbols when they are redefined to mean something else; Intothe-unknown: requires the model to choose which piece of information would help answer a question; Memo-trap: asks an LM to write a phrase in a way that starts like a famous quote but ends differently 6 . This is also called Inverse Scaling Phenomenon. Another interesting phenomenon observed in the scaling of LLMs is called the U-shaped Phenomenon [114].

(p23.2) As the name implies, This phenomenon refers to that as LLM size increases, their performance on certain tasks initially improves but then starts to decline before eventually improving again, such as on: Hindsight-neglect: it tests whether language models are able to assess whether a bet was worth taking based on its expected value; NegationQA: this task takes an existing multiple-choice dataset and negates a part of each question to see if language models are sensitive to negation; Quote-repetition: it asks models to repeat back sentences given in the prompt, with few-shot examples to help it recognize the task. Hence the risk of diminishing performance should be noted and if the task is similar to those we just discussed, careful consideration should be given to whether or not to use huge LLMs.

(p23.3) Gaining a deeper understanding of emergent abilities, inverse scaling phenomenon and U-shape phenomenon in LLMs is essential for advancing research in this field. In a certain sense, the U-shape phenomenon suggests that small-scale models and huge-scale models make predictions with different internal mechanisms. From this perspective, the U-shape phenomenon can be seen as a transformation of the inverse-scaling phenomenon due to some emergent abilities from sufficiently large models [114]. GPT-4 [76] exhibits a reversal of the inverse scaling phenomenon in some cases, such as on a task called Hindsight Neglect. The explanation for these behaviors of LLMs during scaling is still an open problem. Several hypotheses have been proposed. For emergent abilities, one explanation is that there may be multiple key steps for a task and the LLM cannot handle this task until it's large enough to handle every step, and another explanation is focused on the granularity of evaluation metrics [113]. For inverse-scaling phenomenon and u-shape phenomenon, the explanations mainly focus on the model's over-reliance on information from its prior rather than the input prompts, valid but misleading few-shot examples, and distracting easier tasks within a hard task [114]."
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s26,No use case.,"['p26.0', 'p26.1']","['LLMs generally struggle with some tasks due to differences in objectives and training data.', ""Although LLMs have achieved remarkable success in various natural language processing tasks, their performance in regression tasks has been less impressive. For example, ChatGPT's performance on the GLUE STS-B dataset, which is a regression task evaluating sentence similarity, is inferior to a fine-tuned RoBERTa performance [130]. The Regression tasks typically involve predicting a continuous value rather than a discrete label, posing unique challenges for LLMs. One primary reason for their subpar performance is the inherent difference between the language modeling objective and the regression task objective. LLMs are designed to predict the next word in a sequence or generate coherent text, with their pre-training focused on capturing linguistic patterns and relationships. Consequently, their internal representations may not be well-suited for modeling continuous numerical outputs. Besides, LLMs have predominantly been trained on text data, focusing on capturing the intricacies of natural language processing. As a result, their performance on multimodal data, which involves handling multiple data types such as text, images, audio, video, actions, and robotics, remains largely unexplored. And fine-tuned multimodal models, like BEiT [110] and PaLI [19], still dominate many tasks such as visual question answering (VQA) and image captioning. Nonetheless, the recently introduced GPT-4 [76] has taken the step in multimodal fusion, but there is still a lack of detailed evaluation of its capabilities.""]","LLMs generally struggle with some tasks due to differences in objectives and training data.

Although LLMs have achieved remarkable success in various natural language processing tasks, their performance in regression tasks has been less impressive. For example, ChatGPT's performance on the GLUE STS-B dataset, which is a regression task evaluating sentence similarity, is inferior to a fine-tuned RoBERTa performance [130]. The Regression tasks typically involve predicting a continuous value rather than a discrete label, posing unique challenges for LLMs. One primary reason for their subpar performance is the inherent difference between the language modeling objective and the regression task objective. LLMs are designed to predict the next word in a sequence or generate coherent text, with their pre-training focused on capturing linguistic patterns and relationships. Consequently, their internal representations may not be well-suited for modeling continuous numerical outputs. Besides, LLMs have predominantly been trained on text data, focusing on capturing the intricacies of natural language processing. As a result, their performance on multimodal data, which involves handling multiple data types such as text, images, audio, video, actions, and robotics, remains largely unexplored. And fine-tuned multimodal models, like BEiT [110] and PaLI [19], still dominate many tasks such as visual question answering (VQA) and image captioning. Nonetheless, the recently introduced GPT-4 [76] has taken the step in multimodal fusion, but there is still a lack of detailed evaluation of its capabilities.","(p26.0) LLMs generally struggle with some tasks due to differences in objectives and training data.

(p26.1) Although LLMs have achieved remarkable success in various natural language processing tasks, their performance in regression tasks has been less impressive. For example, ChatGPT's performance on the GLUE STS-B dataset, which is a regression task evaluating sentence similarity, is inferior to a fine-tuned RoBERTa performance [130]. The Regression tasks typically involve predicting a continuous value rather than a discrete label, posing unique challenges for LLMs. One primary reason for their subpar performance is the inherent difference between the language modeling objective and the regression task objective. LLMs are designed to predict the next word in a sequence or generate coherent text, with their pre-training focused on capturing linguistic patterns and relationships. Consequently, their internal representations may not be well-suited for modeling continuous numerical outputs. Besides, LLMs have predominantly been trained on text data, focusing on capturing the intricacies of natural language processing. As a result, their performance on multimodal data, which involves handling multiple data types such as text, images, audio, video, actions, and robotics, remains largely unexplored. And fine-tuned multimodal models, like BEiT [110] and PaLI [19], still dominate many tasks such as visual question answering (VQA) and image captioning. Nonetheless, the recently introduced GPT-4 [76] has taken the step in multimodal fusion, but there is still a lack of detailed evaluation of its capabilities.","[[], ['b127', 'b73', 'b16', 'b107']]","[[], ['b127', 'b73', 'b16', 'b107']]",4,"1. LLMs generally struggle with some tasks due to differences in objectives and training data.
2. Although LLMs have achieved remarkable success in various natural language processing tasks, their performance in regression tasks has been less impressive.
3. For example, ChatGPT's performance on the GLUE STS-B dataset, which is a regression task evaluating sentence similarity, is inferior to a fine-tuned RoBERTa performance [130].
4. The Regression tasks typically involve predicting a continuous value rather than a discrete label, posing unique challenges for LLMs.
5. One primary reason for their subpar performance is the inherent difference between the language modeling objective and the regression task objective.
6. LLMs are designed to predict the next word in a sequence or generate coherent text, with their pre-training focused on capturing linguistic patterns and relationships.
7. Consequently, their internal representations may not be well-suited for modeling continuous numerical outputs.
8. Besides, LLMs have predominantly been trained on text data, focusing on capturing the intricacies of natural language processing.
9. As a result, their performance on multimodal data, which involves handling multiple data types such as text, images, audio, video, actions, and robotics, remains largely unexplored.
10. And fine-tuned multimodal models, like BEiT [110] and PaLI [19], still dominate many tasks such as visual question answering (VQA) and image captioning.
11. Nonetheless, the recently introduced GPT-4 [76] has taken the step in multimodal fusion, but there is still a lack of detailed evaluation of its capabilities.","Why do LLMs exhibit subpar performance in regression tasks compared to discrete label prediction tasks?
What are the inherent challenges LLMs face when tasked with regression, specifically in predicting continuous values?
How does the pre-training objective of LLMs impact their ability to model continuous numerical outputs effectively?
In what ways are LLMs limited when it comes to handling multimodal data involving text, images, audio, video, actions, and robotics?
What advancements have been made with GPT-4 in the area of multimodal fusion, and what remains unexplored in its capabilities?","1. Why do LLMs exhibit subpar performance in regression tasks compared to discrete label prediction tasks?
2. What are the inherent challenges LLMs face when tasked with regression, specifically in predicting continuous values?
3. How does the pre-training objective of LLMs impact their ability to model continuous numerical outputs effectively?
4. In what ways are LLMs limited when it comes to handling multimodal data involving text, images, audio, video, actions, and robotics?
5. What advancements have been made with GPT-4 in the area of multimodal fusion, and what remains unexplored in its capabilities?",None,,"Questions:

1. Why do LLMs exhibit subpar performance in regression tasks compared to discrete label prediction tasks?
2. What are the inherent challenges LLMs face when tasked with regression, specifically in predicting continuous values?
3. How does the pre-training objective of LLMs impact their ability to model continuous numerical outputs effectively?
4. In what ways are LLMs limited when it comes to handling multimodal data involving text, images, audio, video, actions, and robotics?
5. What advancements have been made with GPT-4 in the area of multimodal fusion, and what remains unexplored in its capabilities?

Answer:

(p26.0) LLMs generally struggle with some tasks due to differences in objectives and training data.

(p26.1) Although LLMs have achieved remarkable success in various natural language processing tasks, their performance in regression tasks has been less impressive. For example, ChatGPT's performance on the GLUE STS-B dataset, which is a regression task evaluating sentence similarity, is inferior to a fine-tuned RoBERTa performance [130]. The Regression tasks typically involve predicting a continuous value rather than a discrete label, posing unique challenges for LLMs. One primary reason for their subpar performance is the inherent difference between the language modeling objective and the regression task objective. LLMs are designed to predict the next word in a sequence or generate coherent text, with their pre-training focused on capturing linguistic patterns and relationships. Consequently, their internal representations may not be well-suited for modeling continuous numerical outputs. Besides, LLMs have predominantly been trained on text data, focusing on capturing the intricacies of natural language processing. As a result, their performance on multimodal data, which involves handling multiple data types such as text, images, audio, video, actions, and robotics, remains largely unexplored. And fine-tuned multimodal models, like BEiT [110] and PaLI [19], still dominate many tasks such as visual question answering (VQA) and image captioning. Nonetheless, the recently introduced GPT-4 [76] has taken the step in multimodal fusion, but there is still a lack of detailed evaluation of its capabilities.","Questions:



Answer:

(p26.0) LLMs generally struggle with some tasks due to differences in objectives and training data.

(p26.1) Although LLMs have achieved remarkable success in various natural language processing tasks, their performance in regression tasks has been less impressive. For example, ChatGPT's performance on the GLUE STS-B dataset, which is a regression task evaluating sentence similarity, is inferior to a fine-tuned RoBERTa performance [130]. The Regression tasks typically involve predicting a continuous value rather than a discrete label, posing unique challenges for LLMs. One primary reason for their subpar performance is the inherent difference between the language modeling objective and the regression task objective. LLMs are designed to predict the next word in a sequence or generate coherent text, with their pre-training focused on capturing linguistic patterns and relationships. Consequently, their internal representations may not be well-suited for modeling continuous numerical outputs. Besides, LLMs have predominantly been trained on text data, focusing on capturing the intricacies of natural language processing. As a result, their performance on multimodal data, which involves handling multiple data types such as text, images, audio, video, actions, and robotics, remains largely unexplored. And fine-tuned multimodal models, like BEiT [110] and PaLI [19], still dominate many tasks such as visual question answering (VQA) and image captioning. Nonetheless, the recently introduced GPT-4 [76] has taken the step in multimodal fusion, but there is still a lack of detailed evaluation of its capabilities."
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s27,Use case.,"['p27.0', 'p27.1', 'p27.2', 'p27.3', 'p27.4']","['LLMs are particularly suitable for certain tasks.', 'LLMs are very good at mimicking humans, acting as a chatbot, and performing various kinds of tasks. The LLMspowered ChatGPT 7 is surprising for its consistency, reliability, informativeness, and robustness during multiple utterances with humans. The human-feedback procedure plays an important role in acquiring such abilities LLMs can both act as a good annotator and data generator for data augmentation, such as in [27,29,99,121,122].', 'Some LLMs have been found as good as human annotators [37] in some tasks. And the collected texts from GPT-3.5 (text-davinci-003) have been used as human-like instruction-following demonstrations to train other language models [100].', 'LLMs can also be used for quality assessment on some NLG tasks, such as summarization and translation. On summarization tasks, GPT-4 as an evaluator achieves a higher correlation with humans than other methods with a large margin [64]. Some other evaluators based on LLMs [34,50,64,108] also show good human alignment in more NLG tasks, especially compared with traditional automatic metrics. But the LLM evaluator may have a bias towards the LLM-generated texts [64].', 'Also, as we discussed above, some abilities of LLMs bring bonuses in addition to performance improvement, such as interpretability. The CoT reasoning ability of LLMs can show how an LLM reaches the prediction, which is a good interpretation on the instance level, while it also improves the performance.']","LLMs are particularly suitable for certain tasks.

LLMs are very good at mimicking humans, acting as a chatbot, and performing various kinds of tasks. The LLMspowered ChatGPT 7 is surprising for its consistency, reliability, informativeness, and robustness during multiple utterances with humans. The human-feedback procedure plays an important role in acquiring such abilities LLMs can both act as a good annotator and data generator for data augmentation, such as in [27,29,99,121,122].

Some LLMs have been found as good as human annotators [37] in some tasks. And the collected texts from GPT-3.5 (text-davinci-003) have been used as human-like instruction-following demonstrations to train other language models [100].

LLMs can also be used for quality assessment on some NLG tasks, such as summarization and translation. On summarization tasks, GPT-4 as an evaluator achieves a higher correlation with humans than other methods with a large margin [64]. Some other evaluators based on LLMs [34,50,64,108] also show good human alignment in more NLG tasks, especially compared with traditional automatic metrics. But the LLM evaluator may have a bias towards the LLM-generated texts [64].

Also, as we discussed above, some abilities of LLMs bring bonuses in addition to performance improvement, such as interpretability. The CoT reasoning ability of LLMs can show how an LLM reaches the prediction, which is a good interpretation on the instance level, while it also improves the performance.","(p27.0) LLMs are particularly suitable for certain tasks.

(p27.1) LLMs are very good at mimicking humans, acting as a chatbot, and performing various kinds of tasks. The LLMspowered ChatGPT 7 is surprising for its consistency, reliability, informativeness, and robustness during multiple utterances with humans. The human-feedback procedure plays an important role in acquiring such abilities LLMs can both act as a good annotator and data generator for data augmentation, such as in [27,29,99,121,122].

(p27.2) Some LLMs have been found as good as human annotators [37] in some tasks. And the collected texts from GPT-3.5 (text-davinci-003) have been used as human-like instruction-following demonstrations to train other language models [100].

(p27.3) LLMs can also be used for quality assessment on some NLG tasks, such as summarization and translation. On summarization tasks, GPT-4 as an evaluator achieves a higher correlation with humans than other methods with a large margin [64]. Some other evaluators based on LLMs [34,50,64,108] also show good human alignment in more NLG tasks, especially compared with traditional automatic metrics. But the LLM evaluator may have a bias towards the LLM-generated texts [64].

(p27.4) Also, as we discussed above, some abilities of LLMs bring bonuses in addition to performance improvement, such as interpretability. The CoT reasoning ability of LLMs can show how an LLM reaches the prediction, which is a good interpretation on the instance level, while it also improves the performance.","[[], ['b119', 'b24', 'b96', 'b118', 'b26'], ['b97', 'b34'], ['b105', 'b61', 'b47', 'b31'], []]","[[], ['b119', 'b24', 'b96', 'b118', 'b26'], ['b97', 'b34'], ['b105', 'b61', 'b47', 'b31'], []]",11,"1. LLMs are particularly suitable for certain tasks.
2. LLMs are very good at mimicking humans, acting as a chatbot, and performing various kinds of tasks.
3. The LLMspowered ChatGPT 7 is surprising for its consistency, reliability, informativeness, and robustness during multiple utterances with humans.
4. The human-feedback procedure plays an important role in acquiring such abilities LLMs can both act as a good annotator and data generator for data augmentation, such as in [27,29,99,121,122].
5. Some LLMs have been found as good as human annotators [37] in some tasks.
6. And the collected texts from GPT-3.5 (text-davinci-003) have been used as human-like instruction-following demonstrations to train other language models [100].
7. LLMs can also be used for quality assessment on some NLG tasks, such as summarization and translation.
8. On summarization tasks, GPT-4 as an evaluator achieves a higher correlation with humans than other methods with a large margin [64].
9. Some other evaluators based on LLMs [34,50,64,108] also show good human alignment in more NLG tasks, especially compared with traditional automatic metrics.
10. But the LLM evaluator may have a bias towards the LLM-generated texts [64].
11. Also, as we discussed above, some abilities of LLMs bring bonuses in addition to performance improvement, such as interpretability.
12. The CoT reasoning ability of LLMs can show how an LLM reaches the prediction, which is a good interpretation on the instance level, while it also improves the performance.","How do LLMs like ChatGPT 7 demonstrate their capabilities in human-like interactions and data processing tasks?
What role does human feedback play in enhancing the abilities of LLMs for tasks such as annotation and data generation?
In what ways are LLMs utilized for quality assessment in natural language generation tasks, and how do they compare with traditional automatic metrics?
How does the CoT reasoning ability of LLMs contribute to both interpretability and performance improvement in predictions?
What are the limitations and potential biases of using LLMs as evaluators in NLG tasks, particularly in summarization and translation?","1. How do LLMs like ChatGPT 7 demonstrate their capabilities in human-like interactions and data processing tasks?
2. What role does human feedback play in enhancing the abilities of LLMs for tasks such as annotation and data generation?
3. In what ways are LLMs utilized for quality assessment in natural language generation tasks, and how do they compare with traditional automatic metrics?
4. How does the CoT reasoning ability of LLMs contribute to both interpretability and performance improvement in predictions?
5. What are the limitations and potential biases of using LLMs as evaluators in NLG tasks, particularly in summarization and translation?",None,,"Questions:

1. How do LLMs like ChatGPT 7 demonstrate their capabilities in human-like interactions and data processing tasks?
2. What role does human feedback play in enhancing the abilities of LLMs for tasks such as annotation and data generation?
3. In what ways are LLMs utilized for quality assessment in natural language generation tasks, and how do they compare with traditional automatic metrics?
4. How does the CoT reasoning ability of LLMs contribute to both interpretability and performance improvement in predictions?
5. What are the limitations and potential biases of using LLMs as evaluators in NLG tasks, particularly in summarization and translation?

Answer:

(p27.0) LLMs are particularly suitable for certain tasks.

(p27.1) LLMs are very good at mimicking humans, acting as a chatbot, and performing various kinds of tasks. The LLMspowered ChatGPT 7 is surprising for its consistency, reliability, informativeness, and robustness during multiple utterances with humans. The human-feedback procedure plays an important role in acquiring such abilities LLMs can both act as a good annotator and data generator for data augmentation, such as in [27,29,99,121,122].

(p27.2) Some LLMs have been found as good as human annotators [37] in some tasks. And the collected texts from GPT-3.5 (text-davinci-003) have been used as human-like instruction-following demonstrations to train other language models [100].

(p27.3) LLMs can also be used for quality assessment on some NLG tasks, such as summarization and translation. On summarization tasks, GPT-4 as an evaluator achieves a higher correlation with humans than other methods with a large margin [64]. Some other evaluators based on LLMs [34,50,64,108] also show good human alignment in more NLG tasks, especially compared with traditional automatic metrics. But the LLM evaluator may have a bias towards the LLM-generated texts [64].

(p27.4) Also, as we discussed above, some abilities of LLMs bring bonuses in addition to performance improvement, such as interpretability. The CoT reasoning ability of LLMs can show how an LLM reaches the prediction, which is a good interpretation on the instance level, while it also improves the performance.","Questions:



Answer:

(p27.0) LLMs are particularly suitable for certain tasks.

(p27.1) LLMs are very good at mimicking humans, acting as a chatbot, and performing various kinds of tasks. The LLMspowered ChatGPT 7 is surprising for its consistency, reliability, informativeness, and robustness during multiple utterances with humans. The human-feedback procedure plays an important role in acquiring such abilities LLMs can both act as a good annotator and data generator for data augmentation, such as in [27,29,99,121,122].

(p27.2) Some LLMs have been found as good as human annotators [37] in some tasks. And the collected texts from GPT-3.5 (text-davinci-003) have been used as human-like instruction-following demonstrations to train other language models [100].

(p27.3) LLMs can also be used for quality assessment on some NLG tasks, such as summarization and translation. On summarization tasks, GPT-4 as an evaluator achieves a higher correlation with humans than other methods with a large margin [64]. Some other evaluators based on LLMs [34,50,64,108] also show good human alignment in more NLG tasks, especially compared with traditional automatic metrics. But the LLM evaluator may have a bias towards the LLM-generated texts [64].

(p27.4) Also, as we discussed above, some abilities of LLMs bring bonuses in addition to performance improvement, such as interpretability. The CoT reasoning ability of LLMs can show how an LLM reaches the prediction, which is a good interpretation on the instance level, while it also improves the performance."
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s32,Efficiency,"['p32.0', 'p32.1', 'p32.2', 'p32.3', 'p32.4', 'p32.5']","['In real-world deployment, performance, cost, and latency are all important considerations, not just the performance of the models. While some parameter-efficient methods have been developed, practitioners must balance efficiency with effectiveness in the practice.', 'Cost. LLMs have grown increasingly larger in recent years, with models such as GPT-1, GPT-2, and GPT-3 featuring 117 million, 1.5 billion, and 175 billion parameters, respectively. The cost of training an LLM is heavily influenced by its size, with estimates suggesting that training the 11B parameter variant of T5 costs well over $1.3 million for a single run, while a single training run of GPT-3 175B requires $4.6 million [3]. The energy consumption for training large models is equally impressive. The total energy consumption for training a transformer model with 6B parameters to completion is estimated to be around 103.5 MWh [30]. Google reports that training PaLM consumed about 3.4 GWh in about two months [6]. Furthermore, the dataset size also scales rapidly with the size of the model, with GPT-3 175B trained on 499 billion tokens [16]. Another key metric that reflects the computing cost is Flops, with GPT-3 175B requiring 3.14 × 10 23', 'Flops, while a T5 11B model only requires 3.30 × 10 22 , which is 10 times less. In addition to these costs, hardware requirements are also substantial. OpenAI has collaborated with Microsoft on a supercomputer hosted in the Microsoft Azure cloud, consisting of 285k CPU cores and 10k high-end GPUs to support the training of large models. For users of the OpenAI API, pricing varies based on the model and usage, with options such as GPT-3.5-turbo charging $0.002 per 1k tokens for chat service. However, for users who require custom models, training costs $0.03 per 1k tokens, while usage costs $0.12 per 1k tokens [4]. Therefore, for users who cannot afford such a large cost, such as small startups, individual users, etc., a small, fine-tuned model is a better and more reasonable choice.', ""Latency. Latency is a crucial factor to consider in real-world applications of LLMs. Inference time is a commonly used metric to measure latency, which is highly dependent on the model size, architecture, and token size. For instance, the inference time for the GPT-J 6B model is 0.077s, 0.203s, and 0.707s when the max token size is set to 2, 8, and 32, respectively. Additionally, when the max token size is fixed at 32, the inference time for the InstructGPT model (davinci v2) is 1.969s. As LLMs are often too large to be run on a single user's machine, companies provide LLM services via APIs. The API latency can vary depending on the user's location, and the average latency of the OpenAI API service for a single request can range from a few hundred milliseconds to several seconds. In scenarios where high latency is not acceptable, large LLMs may not be appropriate. For example, scalability is critical in many information retrieval applications. To deploy information retrieval systems on the web, search engines require very efficient inference for systems to be useful. The idealized denoised inference time for the InstructGPT davinci v2 (175B*) model is 0.21s per request (i.e., a query-passage pair to be scored), which is too slow for web search engines."", 'Parameter-Efficient Tuning. In practice, we may tune the model on some specific datasets. Parameter-Efficient Tuning (PET) is an efficient technique to tune a small portation of model parameters (or extra parameters) while freezing most parameters of the pre-trained LLMs. The main goal of PEFT is to greatly decrease the computational and storage costs while keeping the performance of the original models. The common techniques for PET are LoRA [42], Prefix Tuning [58], P-Tuning [62,63]. As an illustration, the LoRA method maintains the weights of the pre-trained model and incorporates low-rank matrices into every layer of the Transformer architecture. This approach considerably minimizes the number of parameters that require training for subsequent tasks, thereby increasing overall efficiency.', 'Alpaca-LoRA 8 proposes integrating Low-Rank Adaptation (LoRA) into LLaMA-Alpaca, which enables runs LLaMA within hours on a single RTX 4090. All these PFT methods can be helpful either for fine-tuning a model to a specific task or tuning LLMs to meet special requirements like human alignment.']","In real-world deployment, performance, cost, and latency are all important considerations, not just the performance of the models. While some parameter-efficient methods have been developed, practitioners must balance efficiency with effectiveness in the practice.

Cost. LLMs have grown increasingly larger in recent years, with models such as GPT-1, GPT-2, and GPT-3 featuring 117 million, 1.5 billion, and 175 billion parameters, respectively. The cost of training an LLM is heavily influenced by its size, with estimates suggesting that training the 11B parameter variant of T5 costs well over $1.3 million for a single run, while a single training run of GPT-3 175B requires $4.6 million [3]. The energy consumption for training large models is equally impressive. The total energy consumption for training a transformer model with 6B parameters to completion is estimated to be around 103.5 MWh [30]. Google reports that training PaLM consumed about 3.4 GWh in about two months [6]. Furthermore, the dataset size also scales rapidly with the size of the model, with GPT-3 175B trained on 499 billion tokens [16]. Another key metric that reflects the computing cost is Flops, with GPT-3 175B requiring 3.14 × 10 23

Flops, while a T5 11B model only requires 3.30 × 10 22 , which is 10 times less. In addition to these costs, hardware requirements are also substantial. OpenAI has collaborated with Microsoft on a supercomputer hosted in the Microsoft Azure cloud, consisting of 285k CPU cores and 10k high-end GPUs to support the training of large models. For users of the OpenAI API, pricing varies based on the model and usage, with options such as GPT-3.5-turbo charging $0.002 per 1k tokens for chat service. However, for users who require custom models, training costs $0.03 per 1k tokens, while usage costs $0.12 per 1k tokens [4]. Therefore, for users who cannot afford such a large cost, such as small startups, individual users, etc., a small, fine-tuned model is a better and more reasonable choice.

Latency. Latency is a crucial factor to consider in real-world applications of LLMs. Inference time is a commonly used metric to measure latency, which is highly dependent on the model size, architecture, and token size. For instance, the inference time for the GPT-J 6B model is 0.077s, 0.203s, and 0.707s when the max token size is set to 2, 8, and 32, respectively. Additionally, when the max token size is fixed at 32, the inference time for the InstructGPT model (davinci v2) is 1.969s. As LLMs are often too large to be run on a single user's machine, companies provide LLM services via APIs. The API latency can vary depending on the user's location, and the average latency of the OpenAI API service for a single request can range from a few hundred milliseconds to several seconds. In scenarios where high latency is not acceptable, large LLMs may not be appropriate. For example, scalability is critical in many information retrieval applications. To deploy information retrieval systems on the web, search engines require very efficient inference for systems to be useful. The idealized denoised inference time for the InstructGPT davinci v2 (175B*) model is 0.21s per request (i.e., a query-passage pair to be scored), which is too slow for web search engines.

Parameter-Efficient Tuning. In practice, we may tune the model on some specific datasets. Parameter-Efficient Tuning (PET) is an efficient technique to tune a small portation of model parameters (or extra parameters) while freezing most parameters of the pre-trained LLMs. The main goal of PEFT is to greatly decrease the computational and storage costs while keeping the performance of the original models. The common techniques for PET are LoRA [42], Prefix Tuning [58], P-Tuning [62,63]. As an illustration, the LoRA method maintains the weights of the pre-trained model and incorporates low-rank matrices into every layer of the Transformer architecture. This approach considerably minimizes the number of parameters that require training for subsequent tasks, thereby increasing overall efficiency.

Alpaca-LoRA 8 proposes integrating Low-Rank Adaptation (LoRA) into LLaMA-Alpaca, which enables runs LLaMA within hours on a single RTX 4090. All these PFT methods can be helpful either for fine-tuning a model to a specific task or tuning LLMs to meet special requirements like human alignment.","(p32.0) In real-world deployment, performance, cost, and latency are all important considerations, not just the performance of the models. While some parameter-efficient methods have been developed, practitioners must balance efficiency with effectiveness in the practice.

(p32.1) Cost. LLMs have grown increasingly larger in recent years, with models such as GPT-1, GPT-2, and GPT-3 featuring 117 million, 1.5 billion, and 175 billion parameters, respectively. The cost of training an LLM is heavily influenced by its size, with estimates suggesting that training the 11B parameter variant of T5 costs well over $1.3 million for a single run, while a single training run of GPT-3 175B requires $4.6 million [3]. The energy consumption for training large models is equally impressive. The total energy consumption for training a transformer model with 6B parameters to completion is estimated to be around 103.5 MWh [30]. Google reports that training PaLM consumed about 3.4 GWh in about two months [6]. Furthermore, the dataset size also scales rapidly with the size of the model, with GPT-3 175B trained on 499 billion tokens [16]. Another key metric that reflects the computing cost is Flops, with GPT-3 175B requiring 3.14 × 10 23

(p32.2) Flops, while a T5 11B model only requires 3.30 × 10 22 , which is 10 times less. In addition to these costs, hardware requirements are also substantial. OpenAI has collaborated with Microsoft on a supercomputer hosted in the Microsoft Azure cloud, consisting of 285k CPU cores and 10k high-end GPUs to support the training of large models. For users of the OpenAI API, pricing varies based on the model and usage, with options such as GPT-3.5-turbo charging $0.002 per 1k tokens for chat service. However, for users who require custom models, training costs $0.03 per 1k tokens, while usage costs $0.12 per 1k tokens [4]. Therefore, for users who cannot afford such a large cost, such as small startups, individual users, etc., a small, fine-tuned model is a better and more reasonable choice.

(p32.3) Latency. Latency is a crucial factor to consider in real-world applications of LLMs. Inference time is a commonly used metric to measure latency, which is highly dependent on the model size, architecture, and token size. For instance, the inference time for the GPT-J 6B model is 0.077s, 0.203s, and 0.707s when the max token size is set to 2, 8, and 32, respectively. Additionally, when the max token size is fixed at 32, the inference time for the InstructGPT model (davinci v2) is 1.969s. As LLMs are often too large to be run on a single user's machine, companies provide LLM services via APIs. The API latency can vary depending on the user's location, and the average latency of the OpenAI API service for a single request can range from a few hundred milliseconds to several seconds. In scenarios where high latency is not acceptable, large LLMs may not be appropriate. For example, scalability is critical in many information retrieval applications. To deploy information retrieval systems on the web, search engines require very efficient inference for systems to be useful. The idealized denoised inference time for the InstructGPT davinci v2 (175B*) model is 0.21s per request (i.e., a query-passage pair to be scored), which is too slow for web search engines.

(p32.4) Parameter-Efficient Tuning. In practice, we may tune the model on some specific datasets. Parameter-Efficient Tuning (PET) is an efficient technique to tune a small portation of model parameters (or extra parameters) while freezing most parameters of the pre-trained LLMs. The main goal of PEFT is to greatly decrease the computational and storage costs while keeping the performance of the original models. The common techniques for PET are LoRA [42], Prefix Tuning [58], P-Tuning [62,63]. As an illustration, the LoRA method maintains the weights of the pre-trained model and incorporates low-rank matrices into every layer of the Transformer architecture. This approach considerably minimizes the number of parameters that require training for subsequent tasks, thereby increasing overall efficiency.

(p32.5) Alpaca-LoRA 8 proposes integrating Low-Rank Adaptation (LoRA) into LLaMA-Alpaca, which enables runs LLaMA within hours on a single RTX 4090. All these PFT methods can be helpful either for fine-tuning a model to a specific task or tuning LLMs to meet special requirements like human alignment.","[[], ['b3', 'b13', 'b1', 'b27'], [None], [], ['b60', 'b59', 'b39', 'b55'], []]","[[], ['b3', 'b13', 'b1', 'b27'], [None], [], ['b60', 'b59', 'b39', 'b55'], []]",9,"1. In real-world deployment, performance, cost, and latency are all important considerations, not just the performance of the models.
2. While some parameter-efficient methods have been developed, practitioners must balance efficiency with effectiveness in the practice.
3. Cost. LLMs have grown increasingly larger in recent years, with models such as GPT-1, GPT-2, and GPT-3 featuring 117 million, 1.5 billion, and 175 billion parameters, respectively.
4. The cost of training an LLM is heavily influenced by its size, with estimates suggesting that training the 11B parameter variant of T5 costs well over $1.3 million for a single run, while a single training run of GPT-3 175B requires $4.6 million [3].
5. The energy consumption for training large models is equally impressive.
6. The total energy consumption for training a transformer model with 6B parameters to completion is estimated to be around 103.5 MWh [30].
7. Google reports that training PaLM consumed about 3.4 GWh in about two months [6].
8. Furthermore, the dataset size also scales rapidly with the size of the model, with GPT-3 175B trained on 499 billion tokens [16].
9. Another key metric that reflects the computing cost is Flops, with GPT-3 175B requiring 3.14 × 10 23Flops, while a T5 11B model only requires 3.30 × 10 22 , which is 10 times less.
10. In addition to these costs, hardware requirements are also substantial.
11. OpenAI has collaborated with Microsoft on a supercomputer hosted in the Microsoft Azure cloud, consisting of 285k CPU cores and 10k high-end GPUs to support the training of large models.
12. For users of the OpenAI API, pricing varies based on the model and usage, with options such as GPT-3.5-turbo charging $0.002 per 1k tokens for chat service.
13. However, for users who require custom models, training costs $0.03 per 1k tokens, while usage costs $0.12 per 1k tokens [4].
14. Therefore, for users who cannot afford such a large cost, such as small startups, individual users, etc., a small, fine-tuned model is a better and more reasonable choice.
15. Latency. Latency is a crucial factor to consider in real-world applications of LLMs.
16. Inference time is a commonly used metric to measure latency, which is highly dependent on the model size, architecture, and token size.
17. For instance, the inference time for the GPT-J 6B model is 0.077s, 0.203s, and 0.707s when the max token size is set to 2, 8, and 32, respectively.
18. Additionally, when the max token size is fixed at 32, the inference time for the InstructGPT model (davinci v2) is 1.969s.
19. As LLMs are often too large to be run on a single user's machine, companies provide LLM services via APIs.
20. The API latency can vary depending on the user's location, and the average latency of the OpenAI API service for a single request can range from a few hundred milliseconds to several seconds.
21. In scenarios where high latency is not acceptable, large LLMs may not be appropriate.
22. For example, scalability is critical in many information retrieval applications.
23. To deploy information retrieval systems on the web, search engines require very efficient inference for systems to be useful.
24. The idealized denoised inference time for the InstructGPT davinci v2 (175B*) model is 0.21s per request (i.e., a query-passage pair to be scored), which is too slow for web search engines.
25. Parameter-Efficient Tuning. In practice, we may tune the model on some specific datasets.
26. Parameter-Efficient Tuning (PET) is an efficient technique to tune a small portation of model parameters (or extra parameters) while freezing most parameters of the pre-trained LLMs.
27. The main goal of PEFT is to greatly decrease the computational and storage costs while keeping the performance of the original models.
28. The common techniques for PET are LoRA [30]0, Prefix Tuning [30]1, P-Tuning [30]2.
29. As an illustration, the LoRA method maintains the weights of the pre-trained model and incorporates low-rank matrices into every layer of the Transformer architecture.
30. This approach considerably minimizes the number of parameters that require training for subsequent tasks, thereby increasing overall efficiency.
31. Alpaca-LoRA 8 proposes integrating Low-Rank Adaptation [30]3 into LLaMA-Alpaca, which enables runs LLaMA within hours on a single RTX 4090.
32. All these PFT methods can be helpful either for fine-tuning a model to a specific task or tuning LLMs to meet special requirements like human alignment.","How do parameter-efficient methods balance efficiency and effectiveness in deploying large language models (LLMs)?
What are the cost implications of training large LLMs like GPT-3 compared to smaller models, and how does this affect their accessibility to smaller entities?
How does latency impact the real-world application of LLMs, particularly in scenarios requiring high scalability like web search engines?
What are some common techniques for Parameter-Efficient Tuning (PET) in LLMs, and how do they contribute to computational and storage cost reduction?
How does the integration of Low-Rank Adaptation (LoRA) into LLMs, such as LLaMA-Alpaca, enhance model tuning efficiency on hardware like the RTX 4090?","1. How do parameter-efficient methods balance efficiency and effectiveness in deploying large language models (LLMs)?
2. What are the cost implications of training large LLMs like GPT-3 compared to smaller models, and how does this affect their accessibility to smaller entities?
3. How does latency impact the real-world application of LLMs, particularly in scenarios requiring high scalability like web search engines?
4. What are some common techniques for Parameter-Efficient Tuning (PET) in LLMs, and how do they contribute to computational and storage cost reduction?
5. How does the integration of Low-Rank Adaptation (LoRA) into LLMs, such as LLaMA-Alpaca, enhance model tuning efficiency on hardware like the RTX 4090?",None,,"Questions:

1. How do parameter-efficient methods balance efficiency and effectiveness in deploying large language models (LLMs)?
2. What are the cost implications of training large LLMs like GPT-3 compared to smaller models, and how does this affect their accessibility to smaller entities?
3. How does latency impact the real-world application of LLMs, particularly in scenarios requiring high scalability like web search engines?
4. What are some common techniques for Parameter-Efficient Tuning (PET) in LLMs, and how do they contribute to computational and storage cost reduction?
5. How does the integration of Low-Rank Adaptation (LoRA) into LLMs, such as LLaMA-Alpaca, enhance model tuning efficiency on hardware like the RTX 4090?

Answer:

(p32.0) In real-world deployment, performance, cost, and latency are all important considerations, not just the performance of the models. While some parameter-efficient methods have been developed, practitioners must balance efficiency with effectiveness in the practice.

(p32.1) Cost. LLMs have grown increasingly larger in recent years, with models such as GPT-1, GPT-2, and GPT-3 featuring 117 million, 1.5 billion, and 175 billion parameters, respectively. The cost of training an LLM is heavily influenced by its size, with estimates suggesting that training the 11B parameter variant of T5 costs well over $1.3 million for a single run, while a single training run of GPT-3 175B requires $4.6 million [3]. The energy consumption for training large models is equally impressive. The total energy consumption for training a transformer model with 6B parameters to completion is estimated to be around 103.5 MWh [30]. Google reports that training PaLM consumed about 3.4 GWh in about two months [6]. Furthermore, the dataset size also scales rapidly with the size of the model, with GPT-3 175B trained on 499 billion tokens [16]. Another key metric that reflects the computing cost is Flops, with GPT-3 175B requiring 3.14 × 10 23

(p32.2) Flops, while a T5 11B model only requires 3.30 × 10 22 , which is 10 times less. In addition to these costs, hardware requirements are also substantial. OpenAI has collaborated with Microsoft on a supercomputer hosted in the Microsoft Azure cloud, consisting of 285k CPU cores and 10k high-end GPUs to support the training of large models. For users of the OpenAI API, pricing varies based on the model and usage, with options such as GPT-3.5-turbo charging $0.002 per 1k tokens for chat service. However, for users who require custom models, training costs $0.03 per 1k tokens, while usage costs $0.12 per 1k tokens [4]. Therefore, for users who cannot afford such a large cost, such as small startups, individual users, etc., a small, fine-tuned model is a better and more reasonable choice.

(p32.3) Latency. Latency is a crucial factor to consider in real-world applications of LLMs. Inference time is a commonly used metric to measure latency, which is highly dependent on the model size, architecture, and token size. For instance, the inference time for the GPT-J 6B model is 0.077s, 0.203s, and 0.707s when the max token size is set to 2, 8, and 32, respectively. Additionally, when the max token size is fixed at 32, the inference time for the InstructGPT model (davinci v2) is 1.969s. As LLMs are often too large to be run on a single user's machine, companies provide LLM services via APIs. The API latency can vary depending on the user's location, and the average latency of the OpenAI API service for a single request can range from a few hundred milliseconds to several seconds. In scenarios where high latency is not acceptable, large LLMs may not be appropriate. For example, scalability is critical in many information retrieval applications. To deploy information retrieval systems on the web, search engines require very efficient inference for systems to be useful. The idealized denoised inference time for the InstructGPT davinci v2 (175B*) model is 0.21s per request (i.e., a query-passage pair to be scored), which is too slow for web search engines.

(p32.4) Parameter-Efficient Tuning. In practice, we may tune the model on some specific datasets. Parameter-Efficient Tuning (PET) is an efficient technique to tune a small portation of model parameters (or extra parameters) while freezing most parameters of the pre-trained LLMs. The main goal of PEFT is to greatly decrease the computational and storage costs while keeping the performance of the original models. The common techniques for PET are LoRA [42], Prefix Tuning [58], P-Tuning [62,63]. As an illustration, the LoRA method maintains the weights of the pre-trained model and incorporates low-rank matrices into every layer of the Transformer architecture. This approach considerably minimizes the number of parameters that require training for subsequent tasks, thereby increasing overall efficiency.

(p32.5) Alpaca-LoRA 8 proposes integrating Low-Rank Adaptation (LoRA) into LLaMA-Alpaca, which enables runs LLaMA within hours on a single RTX 4090. All these PFT methods can be helpful either for fine-tuning a model to a specific task or tuning LLMs to meet special requirements like human alignment.","Questions:



Answer:

(p32.0) In real-world deployment, performance, cost, and latency are all important considerations, not just the performance of the models. While some parameter-efficient methods have been developed, practitioners must balance efficiency with effectiveness in the practice.

(p32.1) Cost. LLMs have grown increasingly larger in recent years, with models such as GPT-1, GPT-2, and GPT-3 featuring 117 million, 1.5 billion, and 175 billion parameters, respectively. The cost of training an LLM is heavily influenced by its size, with estimates suggesting that training the 11B parameter variant of T5 costs well over $1.3 million for a single run, while a single training run of GPT-3 175B requires $4.6 million [3]. The energy consumption for training large models is equally impressive. The total energy consumption for training a transformer model with 6B parameters to completion is estimated to be around 103.5 MWh [30]. Google reports that training PaLM consumed about 3.4 GWh in about two months [6]. Furthermore, the dataset size also scales rapidly with the size of the model, with GPT-3 175B trained on 499 billion tokens [16]. Another key metric that reflects the computing cost is Flops, with GPT-3 175B requiring 3.14 × 10 23

(p32.2) Flops, while a T5 11B model only requires 3.30 × 10 22 , which is 10 times less. In addition to these costs, hardware requirements are also substantial. OpenAI has collaborated with Microsoft on a supercomputer hosted in the Microsoft Azure cloud, consisting of 285k CPU cores and 10k high-end GPUs to support the training of large models. For users of the OpenAI API, pricing varies based on the model and usage, with options such as GPT-3.5-turbo charging $0.002 per 1k tokens for chat service. However, for users who require custom models, training costs $0.03 per 1k tokens, while usage costs $0.12 per 1k tokens [4]. Therefore, for users who cannot afford such a large cost, such as small startups, individual users, etc., a small, fine-tuned model is a better and more reasonable choice.

(p32.3) Latency. Latency is a crucial factor to consider in real-world applications of LLMs. Inference time is a commonly used metric to measure latency, which is highly dependent on the model size, architecture, and token size. For instance, the inference time for the GPT-J 6B model is 0.077s, 0.203s, and 0.707s when the max token size is set to 2, 8, and 32, respectively. Additionally, when the max token size is fixed at 32, the inference time for the InstructGPT model (davinci v2) is 1.969s. As LLMs are often too large to be run on a single user's machine, companies provide LLM services via APIs. The API latency can vary depending on the user's location, and the average latency of the OpenAI API service for a single request can range from a few hundred milliseconds to several seconds. In scenarios where high latency is not acceptable, large LLMs may not be appropriate. For example, scalability is critical in many information retrieval applications. To deploy information retrieval systems on the web, search engines require very efficient inference for systems to be useful. The idealized denoised inference time for the InstructGPT davinci v2 (175B*) model is 0.21s per request (i.e., a query-passage pair to be scored), which is too slow for web search engines.

(p32.4) Parameter-Efficient Tuning. In practice, we may tune the model on some specific datasets. Parameter-Efficient Tuning (PET) is an efficient technique to tune a small portation of model parameters (or extra parameters) while freezing most parameters of the pre-trained LLMs. The main goal of PEFT is to greatly decrease the computational and storage costs while keeping the performance of the original models. The common techniques for PET are LoRA [42], Prefix Tuning [58], P-Tuning [62,63]. As an illustration, the LoRA method maintains the weights of the pre-trained model and incorporates low-rank matrices into every layer of the Transformer architecture. This approach considerably minimizes the number of parameters that require training for subsequent tasks, thereby increasing overall efficiency.

(p32.5) Alpaca-LoRA 8 proposes integrating Low-Rank Adaptation (LoRA) into LLaMA-Alpaca, which enables runs LLaMA within hours on a single RTX 4090. All these PFT methods can be helpful either for fine-tuning a model to a specific task or tuning LLMs to meet special requirements like human alignment."
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s33,Trustworthiness,"['p33.0', 'p33.1', 'p33.2', 'p33.3', 'p33.4']","['Given that LLMs are now involved in sensitive areas such as healthcare, finance, and law, it is crucial to ensure that they are trustworthy and capable of producing reliable output.', 'Robustness and Calibration. The accuracy and robustness of the LLMs are shown to have a very strong correlation [59].', 'The models that have high accuracy on the scenario also have good robustness. However, the robustness of the zero-shot becomes worse after being tuned on extra application-specific tasks data [116]. This may due to overfitting, which leads to poor generalizability due to the extremely high complexity of the model and the limited training samples from downstream tasks [43]. In a similar vein, it has been observed that fine-tuning a model can result in significant miscalibrations, owing to over-parameterization [51]. Therefore, fine-tuned models may not be an optimal choice when robustness and calibration are critical considerations. However, human alignment has been found as a potential solution for enhancing model robustness. InstructGPT davinci v2 (175B*) has been shown to outperform other models in terms of robustness. On the other hand, achieving optimal calibration of the model depends on the scenario and adaptation procedure employed.', 'Fairness and Bias. LLMs have been shown to exhibit disparate treatment and impact, perpetuating societal biases and potentially leading to discrimination [10,17]. To ensure fairness and equity for all users, it is crucial to address these issues in the development and deployment of NLP models. Disparities in performance between demographic groups can serve as an indicator of fairness problems. LLMs are particularly susceptible to fairness issues, as significant performance disparities have been observed across demographic categories such as dialect, religion, gender, and race [59]. However, research has shown that aligning models with human instructions can improve LLM performance regardless of their size, with the InstructGPTmodel (davinci v2) exhibiting smaller performance disparities than other LLMs [23].', ""Spurious Biases. The shortcut learning problem has been observed in various natural language understanding tasks under the pretraining and fine-tuning paradigm, where models heavily rely on spurious correlations between input and labels in the fine-tuning data for prediction [31,35,98]. For example, in reading comprehension tasks, fine-tuned models tend to focus on the lexical matching of words between the question and the original passage, neglecting the intended reading comprehension task itself [53]. In contrast, large language models are not directly trained on fine-tuned datasets, which makes it less likely for them to learn shortcut features present in the fine-tuned dataset, thereby enhancing the model's generalization capabilities. However, LLMs are not infallible and may exhibit some shortcut learning during in-context learning. For example, recent preliminary studies have begun investigating the robustness of prompt-based methods in large-scale language models [111,129]. One such study evaluates the few-shot learning performance of GPT-3 on text classification and information extraction tasks [129]. and reveal that the examined LLMs are susceptible to majority label bias and position bias, where they tend to predict answers based on the frequency or position of the answers in the training data. Moreover, these LLMs exhibit common token bias, favoring answers that are prevalent in their pre-training corpus. Recent studies show that this positional bias can be mitigated by selecting proper prompts [68]. In summary, while LLMs significantly reduce the shortcut learning problem prevalent in fine-tuned models, they still exhibit some shortcut learning issues and should be approached with caution when deploying them in downstream applications.""]","Given that LLMs are now involved in sensitive areas such as healthcare, finance, and law, it is crucial to ensure that they are trustworthy and capable of producing reliable output.

Robustness and Calibration. The accuracy and robustness of the LLMs are shown to have a very strong correlation [59].

The models that have high accuracy on the scenario also have good robustness. However, the robustness of the zero-shot becomes worse after being tuned on extra application-specific tasks data [116]. This may due to overfitting, which leads to poor generalizability due to the extremely high complexity of the model and the limited training samples from downstream tasks [43]. In a similar vein, it has been observed that fine-tuning a model can result in significant miscalibrations, owing to over-parameterization [51]. Therefore, fine-tuned models may not be an optimal choice when robustness and calibration are critical considerations. However, human alignment has been found as a potential solution for enhancing model robustness. InstructGPT davinci v2 (175B*) has been shown to outperform other models in terms of robustness. On the other hand, achieving optimal calibration of the model depends on the scenario and adaptation procedure employed.

Fairness and Bias. LLMs have been shown to exhibit disparate treatment and impact, perpetuating societal biases and potentially leading to discrimination [10,17]. To ensure fairness and equity for all users, it is crucial to address these issues in the development and deployment of NLP models. Disparities in performance between demographic groups can serve as an indicator of fairness problems. LLMs are particularly susceptible to fairness issues, as significant performance disparities have been observed across demographic categories such as dialect, religion, gender, and race [59]. However, research has shown that aligning models with human instructions can improve LLM performance regardless of their size, with the InstructGPTmodel (davinci v2) exhibiting smaller performance disparities than other LLMs [23].

Spurious Biases. The shortcut learning problem has been observed in various natural language understanding tasks under the pretraining and fine-tuning paradigm, where models heavily rely on spurious correlations between input and labels in the fine-tuning data for prediction [31,35,98]. For example, in reading comprehension tasks, fine-tuned models tend to focus on the lexical matching of words between the question and the original passage, neglecting the intended reading comprehension task itself [53]. In contrast, large language models are not directly trained on fine-tuned datasets, which makes it less likely for them to learn shortcut features present in the fine-tuned dataset, thereby enhancing the model's generalization capabilities. However, LLMs are not infallible and may exhibit some shortcut learning during in-context learning. For example, recent preliminary studies have begun investigating the robustness of prompt-based methods in large-scale language models [111,129]. One such study evaluates the few-shot learning performance of GPT-3 on text classification and information extraction tasks [129]. and reveal that the examined LLMs are susceptible to majority label bias and position bias, where they tend to predict answers based on the frequency or position of the answers in the training data. Moreover, these LLMs exhibit common token bias, favoring answers that are prevalent in their pre-training corpus. Recent studies show that this positional bias can be mitigated by selecting proper prompts [68]. In summary, while LLMs significantly reduce the shortcut learning problem prevalent in fine-tuned models, they still exhibit some shortcut learning issues and should be approached with caution when deploying them in downstream applications.","(p33.0) Given that LLMs are now involved in sensitive areas such as healthcare, finance, and law, it is crucial to ensure that they are trustworthy and capable of producing reliable output.

(p33.1) Robustness and Calibration. The accuracy and robustness of the LLMs are shown to have a very strong correlation [59].

(p33.2) The models that have high accuracy on the scenario also have good robustness. However, the robustness of the zero-shot becomes worse after being tuned on extra application-specific tasks data [116]. This may due to overfitting, which leads to poor generalizability due to the extremely high complexity of the model and the limited training samples from downstream tasks [43]. In a similar vein, it has been observed that fine-tuning a model can result in significant miscalibrations, owing to over-parameterization [51]. Therefore, fine-tuned models may not be an optimal choice when robustness and calibration are critical considerations. However, human alignment has been found as a potential solution for enhancing model robustness. InstructGPT davinci v2 (175B*) has been shown to outperform other models in terms of robustness. On the other hand, achieving optimal calibration of the model depends on the scenario and adaptation procedure employed.

(p33.3) Fairness and Bias. LLMs have been shown to exhibit disparate treatment and impact, perpetuating societal biases and potentially leading to discrimination [10,17]. To ensure fairness and equity for all users, it is crucial to address these issues in the development and deployment of NLP models. Disparities in performance between demographic groups can serve as an indicator of fairness problems. LLMs are particularly susceptible to fairness issues, as significant performance disparities have been observed across demographic categories such as dialect, religion, gender, and race [59]. However, research has shown that aligning models with human instructions can improve LLM performance regardless of their size, with the InstructGPTmodel (davinci v2) exhibiting smaller performance disparities than other LLMs [23].

(p33.4) Spurious Biases. The shortcut learning problem has been observed in various natural language understanding tasks under the pretraining and fine-tuning paradigm, where models heavily rely on spurious correlations between input and labels in the fine-tuning data for prediction [31,35,98]. For example, in reading comprehension tasks, fine-tuned models tend to focus on the lexical matching of words between the question and the original passage, neglecting the intended reading comprehension task itself [53]. In contrast, large language models are not directly trained on fine-tuned datasets, which makes it less likely for them to learn shortcut features present in the fine-tuned dataset, thereby enhancing the model's generalization capabilities. However, LLMs are not infallible and may exhibit some shortcut learning during in-context learning. For example, recent preliminary studies have begun investigating the robustness of prompt-based methods in large-scale language models [111,129]. One such study evaluates the few-shot learning performance of GPT-3 on text classification and information extraction tasks [129]. and reveal that the examined LLMs are susceptible to majority label bias and position bias, where they tend to predict answers based on the frequency or position of the answers in the training data. Moreover, these LLMs exhibit common token bias, favoring answers that are prevalent in their pre-training corpus. Recent studies show that this positional bias can be mitigated by selecting proper prompts [68]. In summary, while LLMs significantly reduce the shortcut learning problem prevalent in fine-tuned models, they still exhibit some shortcut learning issues and should be approached with caution when deploying them in downstream applications.","[[], ['b56'], ['b113', 'b40', 'b48'], ['b7', 'b14', 'b20', 'b56'], ['b108', 'b28', 'b65', 'b32', 'b95', 'b50', 'b126']]","[[], ['b56'], ['b113', 'b40', 'b48'], ['b7', 'b14', 'b20', 'b56'], ['b108', 'b28', 'b65', 'b32', 'b95', 'b50', 'b126']]",15,"1. Given that LLMs are now involved in sensitive areas such as healthcare, finance, and law, it is crucial to ensure that they are trustworthy and capable of producing reliable output.
2. Robustness and Calibration. The accuracy and robustness of the LLMs are shown to have a very strong correlation [59].
3. The models that have high accuracy on the scenario also have good robustness.
4. However, the robustness of the zero-shot becomes worse after being tuned on extra application-specific tasks data [116].
5. This may due to overfitting, which leads to poor generalizability due to the extremely high complexity of the model and the limited training samples from downstream tasks [43].
6. In a similar vein, it has been observed that fine-tuning a model can result in significant miscalibrations, owing to over-parameterization [51].
7. Therefore, fine-tuned models may not be an optimal choice when robustness and calibration are critical considerations.
8. However, human alignment has been found as a potential solution for enhancing model robustness.
9. InstructGPT davinci v2 (175B*) has been shown to outperform other models in terms of robustness.
10. On the other hand, achieving optimal calibration of the model depends on the scenario and adaptation procedure employed.
11. Fairness and Bias. LLMs have been shown to exhibit disparate treatment and impact, perpetuating societal biases and potentially leading to discrimination [10,17].
12. To ensure fairness and equity for all users, it is crucial to address these issues in the development and deployment of NLP models.
13. Disparities in performance between demographic groups can serve as an indicator of fairness problems.
14. LLMs are particularly susceptible to fairness issues, as significant performance disparities have been observed across demographic categories such as dialect, religion, gender, and race [59].
15. However, research has shown that aligning models with human instructions can improve LLM performance regardless of their size, with the InstructGPTmodel (davinci v2) exhibiting smaller performance disparities than other LLMs [23].Spurious Biases.
16. The shortcut learning problem has been observed in various natural language understanding tasks under the pretraining and fine-tuning paradigm, where models heavily rely on spurious correlations between input and labels in the fine-tuning data for prediction [31,35,98].
17. For example, in reading comprehension tasks, fine-tuned models tend to focus on the lexical matching of words between the question and the original passage, neglecting the intended reading comprehension task itself [116]0.
18. In contrast, large language models are not directly trained on fine-tuned datasets, which makes it less likely for them to learn shortcut features present in the fine-tuned dataset, thereby enhancing the model's generalization capabilities.
19. However, LLMs are not infallible and may exhibit some shortcut learning during in-context learning.
20. For example, recent preliminary studies have begun investigating the robustness of prompt-based methods in large-scale language models [116]1.
21. One such study evaluates the few-shot learning performance of GPT-3 on text classification and information extraction tasks [116]2.
22. and reveal that the examined LLMs are susceptible to majority label bias and position bias, where they tend to predict answers based on the frequency or position of the answers in the training data.
23. Moreover, these LLMs exhibit common token bias, favoring answers that are prevalent in their pre-training corpus.
24. Recent studies show that this positional bias can be mitigated by selecting proper prompts [116]3.
25. In summary, while LLMs significantly reduce the shortcut learning problem prevalent in fine-tuned models, they still exhibit some shortcut learning issues and should be approached with caution when deploying them in downstream applications.","How does the robustness of LLMs change when they are fine-tuned on application-specific tasks?
What are the implications of overfitting and over-parameterization on the calibration and generalizability of LLMs?
In what ways can human alignment enhance the robustness of LLMs, and which model has shown significant improvement in this area?
How do disparities in LLM performance across demographic groups indicate fairness issues, and what strategies have been shown to mitigate these disparities?
What are the challenges and potential solutions associated with shortcut learning in LLMs, especially in the context of prompt-based methods and few-shot learning?","1. How does the robustness of LLMs change when they are fine-tuned on application-specific tasks?
2. What are the implications of overfitting and over-parameterization on the calibration and generalizability of LLMs?
3. In what ways can human alignment enhance the robustness of LLMs, and which model has shown significant improvement in this area?
4. How do disparities in LLM performance across demographic groups indicate fairness issues, and what strategies have been shown to mitigate these disparities?
5. What are the challenges and potential solutions associated with shortcut learning in LLMs, especially in the context of prompt-based methods and few-shot learning?","# How can the trustworthiness of Large Language Models (LLMs) be ensured in sensitive areas?
  - **Robustness and Calibration**
    - The correlation between the accuracy of LLMs and their robustness is strong, indicating that models with high accuracy tend to be more robust.
    - Fine-tuning on application-specific tasks can degrade robustness due to overfitting and poor generalizability, highlighting the complexity of balancing model tuning with robustness.
    - Human alignment, such as with InstructGPT davinci v2, has been identified as a method to enhance model robustness, suggesting that human-centric approaches may mitigate some robustness issues.
  - **Fairness and Bias**
    - LLMs exhibit disparities in treatment and impact across demographic groups, raising concerns about societal biases and discrimination in model outputs.
    - Performance disparities across demographic categories indicate fairness issues, with some models showing smaller disparities, suggesting potential pathways to mitigate bias.
    - Aligning models with human instructions has been shown to improve performance uniformly, hinting at the importance of human-centric design in addressing fairness.
  - **Spurious Biases**
    - Shortcut learning in LLMs, where models rely on spurious correlations for predictions, poses challenges for model generalization and reliability.
    - While LLMs reduce the likelihood of learning shortcut features present in fine-tuned datasets, they are not immune to shortcut learning, especially in in-context learning scenarios.
    - Preliminary studies on prompt-based methods and few-shot learning reveal susceptibility to biases like majority label bias and position bias, but also suggest that proper prompt selection can mitigate some issues.",1. How can the trustworthiness of Large Language Models (LLMs) be ensured in sensitive areas?,"Questions:

1. How does the robustness of LLMs change when they are fine-tuned on application-specific tasks?
2. What are the implications of overfitting and over-parameterization on the calibration and generalizability of LLMs?
3. In what ways can human alignment enhance the robustness of LLMs, and which model has shown significant improvement in this area?
4. How do disparities in LLM performance across demographic groups indicate fairness issues, and what strategies have been shown to mitigate these disparities?
5. What are the challenges and potential solutions associated with shortcut learning in LLMs, especially in the context of prompt-based methods and few-shot learning?

Answer:

(p33.0) Given that LLMs are now involved in sensitive areas such as healthcare, finance, and law, it is crucial to ensure that they are trustworthy and capable of producing reliable output.

(p33.1) Robustness and Calibration. The accuracy and robustness of the LLMs are shown to have a very strong correlation [59].

(p33.2) The models that have high accuracy on the scenario also have good robustness. However, the robustness of the zero-shot becomes worse after being tuned on extra application-specific tasks data [116]. This may due to overfitting, which leads to poor generalizability due to the extremely high complexity of the model and the limited training samples from downstream tasks [43]. In a similar vein, it has been observed that fine-tuning a model can result in significant miscalibrations, owing to over-parameterization [51]. Therefore, fine-tuned models may not be an optimal choice when robustness and calibration are critical considerations. However, human alignment has been found as a potential solution for enhancing model robustness. InstructGPT davinci v2 (175B*) has been shown to outperform other models in terms of robustness. On the other hand, achieving optimal calibration of the model depends on the scenario and adaptation procedure employed.

(p33.3) Fairness and Bias. LLMs have been shown to exhibit disparate treatment and impact, perpetuating societal biases and potentially leading to discrimination [10,17]. To ensure fairness and equity for all users, it is crucial to address these issues in the development and deployment of NLP models. Disparities in performance between demographic groups can serve as an indicator of fairness problems. LLMs are particularly susceptible to fairness issues, as significant performance disparities have been observed across demographic categories such as dialect, religion, gender, and race [59]. However, research has shown that aligning models with human instructions can improve LLM performance regardless of their size, with the InstructGPTmodel (davinci v2) exhibiting smaller performance disparities than other LLMs [23].

(p33.4) Spurious Biases. The shortcut learning problem has been observed in various natural language understanding tasks under the pretraining and fine-tuning paradigm, where models heavily rely on spurious correlations between input and labels in the fine-tuning data for prediction [31,35,98]. For example, in reading comprehension tasks, fine-tuned models tend to focus on the lexical matching of words between the question and the original passage, neglecting the intended reading comprehension task itself [53]. In contrast, large language models are not directly trained on fine-tuned datasets, which makes it less likely for them to learn shortcut features present in the fine-tuned dataset, thereby enhancing the model's generalization capabilities. However, LLMs are not infallible and may exhibit some shortcut learning during in-context learning. For example, recent preliminary studies have begun investigating the robustness of prompt-based methods in large-scale language models [111,129]. One such study evaluates the few-shot learning performance of GPT-3 on text classification and information extraction tasks [129]. and reveal that the examined LLMs are susceptible to majority label bias and position bias, where they tend to predict answers based on the frequency or position of the answers in the training data. Moreover, these LLMs exhibit common token bias, favoring answers that are prevalent in their pre-training corpus. Recent studies show that this positional bias can be mitigated by selecting proper prompts [68]. In summary, while LLMs significantly reduce the shortcut learning problem prevalent in fine-tuned models, they still exhibit some shortcut learning issues and should be approached with caution when deploying them in downstream applications.","Questions:

1. How can the trustworthiness of Large Language Models (LLMs) be ensured in sensitive areas?

Answer:

(p33.0) Given that LLMs are now involved in sensitive areas such as healthcare, finance, and law, it is crucial to ensure that they are trustworthy and capable of producing reliable output.

(p33.1) Robustness and Calibration. The accuracy and robustness of the LLMs are shown to have a very strong correlation [59].

(p33.2) The models that have high accuracy on the scenario also have good robustness. However, the robustness of the zero-shot becomes worse after being tuned on extra application-specific tasks data [116]. This may due to overfitting, which leads to poor generalizability due to the extremely high complexity of the model and the limited training samples from downstream tasks [43]. In a similar vein, it has been observed that fine-tuning a model can result in significant miscalibrations, owing to over-parameterization [51]. Therefore, fine-tuned models may not be an optimal choice when robustness and calibration are critical considerations. However, human alignment has been found as a potential solution for enhancing model robustness. InstructGPT davinci v2 (175B*) has been shown to outperform other models in terms of robustness. On the other hand, achieving optimal calibration of the model depends on the scenario and adaptation procedure employed.

(p33.3) Fairness and Bias. LLMs have been shown to exhibit disparate treatment and impact, perpetuating societal biases and potentially leading to discrimination [10,17]. To ensure fairness and equity for all users, it is crucial to address these issues in the development and deployment of NLP models. Disparities in performance between demographic groups can serve as an indicator of fairness problems. LLMs are particularly susceptible to fairness issues, as significant performance disparities have been observed across demographic categories such as dialect, religion, gender, and race [59]. However, research has shown that aligning models with human instructions can improve LLM performance regardless of their size, with the InstructGPTmodel (davinci v2) exhibiting smaller performance disparities than other LLMs [23].

(p33.4) Spurious Biases. The shortcut learning problem has been observed in various natural language understanding tasks under the pretraining and fine-tuning paradigm, where models heavily rely on spurious correlations between input and labels in the fine-tuning data for prediction [31,35,98]. For example, in reading comprehension tasks, fine-tuned models tend to focus on the lexical matching of words between the question and the original passage, neglecting the intended reading comprehension task itself [53]. In contrast, large language models are not directly trained on fine-tuned datasets, which makes it less likely for them to learn shortcut features present in the fine-tuned dataset, thereby enhancing the model's generalization capabilities. However, LLMs are not infallible and may exhibit some shortcut learning during in-context learning. For example, recent preliminary studies have begun investigating the robustness of prompt-based methods in large-scale language models [111,129]. One such study evaluates the few-shot learning performance of GPT-3 on text classification and information extraction tasks [129]. and reveal that the examined LLMs are susceptible to majority label bias and position bias, where they tend to predict answers based on the frequency or position of the answers in the training data. Moreover, these LLMs exhibit common token bias, favoring answers that are prevalent in their pre-training corpus. Recent studies show that this positional bias can be mitigated by selecting proper prompts [68]. In summary, while LLMs significantly reduce the shortcut learning problem prevalent in fine-tuned models, they still exhibit some shortcut learning issues and should be approached with caution when deploying them in downstream applications."
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a,s34,Safety challenges,"['p34.0', 'p34.1', 'p34.2', 'p34.3', 'p34.4']","[""LLMs have demonstrated their extremely strong capabilities in many areas such as reasoning, knowledge retention, and coding. As they become more powerful and human-like, their potential to influence people's opinions and actions in significant ways grows. As a result, some new safety challenges to our society should be considered and have caught lots of attention in recent works [75,76]."", 'Hallucinations. The potential for LLMs to ""hallucinate,"" or generate nonsensical or untruthful content, can have significant negative impacts on the quality and reliability of information in various applications. As LLMs become increasingly convincing and believable, users may develop an overreliance on them and trust them to provide accurate information in areas with which they are somewhat familiar. This can be particularly dangerous if the model produces content that is entirely false or misleading, leading to incorrect decisions or actions taken based on that information.', 'Such outcomes can have serious consequences in many domains, such as healthcare, finance, or public policy, where the accuracy and reliability of information are critical. To mitigate these issues, reinforcement learning from human feedback (RLHF) is widely used [75,77] and LLMs themselves have been integrated into the loop [75].', 'Harmful content. Due to the high coherence, quality, and plausibility of texts generated by LLMs, harmful contents from LLMs can cause significant harm, including hate speech, discrimination, incitement to violence, false narratives, and even social engineering attack. The implementation of safeguards to detect and correct those contents can be mitigation [97]. These LLMs can also have dual-use potential by providing required illicit information, leading to risks such as the proliferation of weapons [75] and even terrorism attack planning. It is crucial to ensure using these LLMs responsibly, with safeguards in place to prevent harm. Also, in existing work, feedback from humans plays an important role in getting rid of harmful outputs.', 'Privacy. LLMs can face serious security issues. An example is the issue of user privacy. It is reported that Samsung employees were using ChatGPT to process their work when they inadvertently leaked top-secret data, including the source code proper of the new program, internal meeting minutes related to the hardware, etc. The Italian data protection agency declared that OpenAI, the developer of ChatGPT, illicitly gathered personal user data, leading Italy to become the first government to prohibit ChatGPT over privacy concerns [1].']","LLMs have demonstrated their extremely strong capabilities in many areas such as reasoning, knowledge retention, and coding. As they become more powerful and human-like, their potential to influence people's opinions and actions in significant ways grows. As a result, some new safety challenges to our society should be considered and have caught lots of attention in recent works [75,76].

Hallucinations. The potential for LLMs to ""hallucinate,"" or generate nonsensical or untruthful content, can have significant negative impacts on the quality and reliability of information in various applications. As LLMs become increasingly convincing and believable, users may develop an overreliance on them and trust them to provide accurate information in areas with which they are somewhat familiar. This can be particularly dangerous if the model produces content that is entirely false or misleading, leading to incorrect decisions or actions taken based on that information.

Such outcomes can have serious consequences in many domains, such as healthcare, finance, or public policy, where the accuracy and reliability of information are critical. To mitigate these issues, reinforcement learning from human feedback (RLHF) is widely used [75,77] and LLMs themselves have been integrated into the loop [75].

Harmful content. Due to the high coherence, quality, and plausibility of texts generated by LLMs, harmful contents from LLMs can cause significant harm, including hate speech, discrimination, incitement to violence, false narratives, and even social engineering attack. The implementation of safeguards to detect and correct those contents can be mitigation [97]. These LLMs can also have dual-use potential by providing required illicit information, leading to risks such as the proliferation of weapons [75] and even terrorism attack planning. It is crucial to ensure using these LLMs responsibly, with safeguards in place to prevent harm. Also, in existing work, feedback from humans plays an important role in getting rid of harmful outputs.

Privacy. LLMs can face serious security issues. An example is the issue of user privacy. It is reported that Samsung employees were using ChatGPT to process their work when they inadvertently leaked top-secret data, including the source code proper of the new program, internal meeting minutes related to the hardware, etc. The Italian data protection agency declared that OpenAI, the developer of ChatGPT, illicitly gathered personal user data, leading Italy to become the first government to prohibit ChatGPT over privacy concerns [1].","(p34.0) LLMs have demonstrated their extremely strong capabilities in many areas such as reasoning, knowledge retention, and coding. As they become more powerful and human-like, their potential to influence people's opinions and actions in significant ways grows. As a result, some new safety challenges to our society should be considered and have caught lots of attention in recent works [75,76].

(p34.1) Hallucinations. The potential for LLMs to ""hallucinate,"" or generate nonsensical or untruthful content, can have significant negative impacts on the quality and reliability of information in various applications. As LLMs become increasingly convincing and believable, users may develop an overreliance on them and trust them to provide accurate information in areas with which they are somewhat familiar. This can be particularly dangerous if the model produces content that is entirely false or misleading, leading to incorrect decisions or actions taken based on that information.

(p34.2) Such outcomes can have serious consequences in many domains, such as healthcare, finance, or public policy, where the accuracy and reliability of information are critical. To mitigate these issues, reinforcement learning from human feedback (RLHF) is widely used [75,77] and LLMs themselves have been integrated into the loop [75].

(p34.3) Harmful content. Due to the high coherence, quality, and plausibility of texts generated by LLMs, harmful contents from LLMs can cause significant harm, including hate speech, discrimination, incitement to violence, false narratives, and even social engineering attack. The implementation of safeguards to detect and correct those contents can be mitigation [97]. These LLMs can also have dual-use potential by providing required illicit information, leading to risks such as the proliferation of weapons [75] and even terrorism attack planning. It is crucial to ensure using these LLMs responsibly, with safeguards in place to prevent harm. Also, in existing work, feedback from humans plays an important role in getting rid of harmful outputs.

(p34.4) Privacy. LLMs can face serious security issues. An example is the issue of user privacy. It is reported that Samsung employees were using ChatGPT to process their work when they inadvertently leaked top-secret data, including the source code proper of the new program, internal meeting minutes related to the hardware, etc. The Italian data protection agency declared that OpenAI, the developer of ChatGPT, illicitly gathered personal user data, leading Italy to become the first government to prohibit ChatGPT over privacy concerns [1].","[['b73', 'b72'], [], ['b74', 'b72'], ['b72', 'b94'], ['b0']]","[['b73', 'b72'], [], ['b74', 'b72'], ['b72', 'b94'], ['b0']]",7,"1. LLMs have demonstrated their extremely strong capabilities in many areas such as reasoning, knowledge retention, and coding.
2. As they become more powerful and human-like, their potential to influence people's opinions and actions in significant ways grows.
3. As a result, some new safety challenges to our society should be considered and have caught lots of attention in recent works [75,76].Hallucinations.
4. The potential for LLMs to ""hallucinate,"" or generate nonsensical or untruthful content, can have significant negative impacts on the quality and reliability of information in various applications.
5. As LLMs become increasingly convincing and believable, users may develop an overreliance on them and trust them to provide accurate information in areas with which they are somewhat familiar.
6. This can be particularly dangerous if the model produces content that is entirely false or misleading, leading to incorrect decisions or actions taken based on that information.
7. Such outcomes can have serious consequences in many domains, such as healthcare, finance, or public policy, where the accuracy and reliability of information are critical.
8. To mitigate these issues, reinforcement learning from human feedback (RLHF) is widely used [75,77] and LLMs themselves have been integrated into the loop [75].
9. Harmful content. Due to the high coherence, quality, and plausibility of texts generated by LLMs, harmful contents from LLMs can cause significant harm, including hate speech, discrimination, incitement to violence, false narratives, and even social engineering attack.
10. The implementation of safeguards to detect and correct those contents can be mitigation [97].
11. These LLMs can also have dual-use potential by providing required illicit information, leading to risks such as the proliferation of weapons [75] and even terrorism attack planning.
12. It is crucial to ensure using these LLMs responsibly, with safeguards in place to prevent harm.
13. Also, in existing work, feedback from humans plays an important role in getting rid of harmful outputs.
14. Privacy. LLMs can face serious security issues.
15. An example is the issue of user privacy.
16. It is reported that Samsung employees were using ChatGPT to process their work when they inadvertently leaked top-secret data, including the source code proper of the new program, internal meeting minutes related to the hardware, etc.
17. The Italian data protection agency declared that OpenAI, the developer of ChatGPT, illicitly gathered personal user data, leading Italy to become the first government to prohibit ChatGPT over privacy concerns [1].","What are the potential negative impacts of LLMs ""hallucinating"" or generating false information on decision-making in critical domains?
How does reinforcement learning from human feedback (RLHF) contribute to mitigating the issue of hallucinations in LLMs?
In what ways can LLM-generated harmful content affect society, and what measures can be implemented to mitigate these risks?
How do privacy concerns arise with the use of LLMs, and what specific incident exemplifies these concerns?
What role does human feedback play in addressing the production of harmful content by LLMs?","1. What are the potential negative impacts of LLMs ""hallucinating"" or generating false information on decision-making in critical domains?
2. How does reinforcement learning from human feedback (RLHF) contribute to mitigating the issue of hallucinations in LLMs?
3. In what ways can LLM-generated harmful content affect society, and what measures can be implemented to mitigate these risks?
4. How do privacy concerns arise with the use of LLMs, and what specific incident exemplifies these concerns?
5. What role does human feedback play in addressing the production of harmful content by LLMs?",None,,"Questions:

1. What are the potential negative impacts of LLMs ""hallucinating"" or generating false information on decision-making in critical domains?
2. How does reinforcement learning from human feedback (RLHF) contribute to mitigating the issue of hallucinations in LLMs?
3. In what ways can LLM-generated harmful content affect society, and what measures can be implemented to mitigate these risks?
4. How do privacy concerns arise with the use of LLMs, and what specific incident exemplifies these concerns?
5. What role does human feedback play in addressing the production of harmful content by LLMs?

Answer:

(p34.0) LLMs have demonstrated their extremely strong capabilities in many areas such as reasoning, knowledge retention, and coding. As they become more powerful and human-like, their potential to influence people's opinions and actions in significant ways grows. As a result, some new safety challenges to our society should be considered and have caught lots of attention in recent works [75,76].

(p34.1) Hallucinations. The potential for LLMs to ""hallucinate,"" or generate nonsensical or untruthful content, can have significant negative impacts on the quality and reliability of information in various applications. As LLMs become increasingly convincing and believable, users may develop an overreliance on them and trust them to provide accurate information in areas with which they are somewhat familiar. This can be particularly dangerous if the model produces content that is entirely false or misleading, leading to incorrect decisions or actions taken based on that information.

(p34.2) Such outcomes can have serious consequences in many domains, such as healthcare, finance, or public policy, where the accuracy and reliability of information are critical. To mitigate these issues, reinforcement learning from human feedback (RLHF) is widely used [75,77] and LLMs themselves have been integrated into the loop [75].

(p34.3) Harmful content. Due to the high coherence, quality, and plausibility of texts generated by LLMs, harmful contents from LLMs can cause significant harm, including hate speech, discrimination, incitement to violence, false narratives, and even social engineering attack. The implementation of safeguards to detect and correct those contents can be mitigation [97]. These LLMs can also have dual-use potential by providing required illicit information, leading to risks such as the proliferation of weapons [75] and even terrorism attack planning. It is crucial to ensure using these LLMs responsibly, with safeguards in place to prevent harm. Also, in existing work, feedback from humans plays an important role in getting rid of harmful outputs.

(p34.4) Privacy. LLMs can face serious security issues. An example is the issue of user privacy. It is reported that Samsung employees were using ChatGPT to process their work when they inadvertently leaked top-secret data, including the source code proper of the new program, internal meeting minutes related to the hardware, etc. The Italian data protection agency declared that OpenAI, the developer of ChatGPT, illicitly gathered personal user data, leading Italy to become the first government to prohibit ChatGPT over privacy concerns [1].","Questions:



Answer:

(p34.0) LLMs have demonstrated their extremely strong capabilities in many areas such as reasoning, knowledge retention, and coding. As they become more powerful and human-like, their potential to influence people's opinions and actions in significant ways grows. As a result, some new safety challenges to our society should be considered and have caught lots of attention in recent works [75,76].

(p34.1) Hallucinations. The potential for LLMs to ""hallucinate,"" or generate nonsensical or untruthful content, can have significant negative impacts on the quality and reliability of information in various applications. As LLMs become increasingly convincing and believable, users may develop an overreliance on them and trust them to provide accurate information in areas with which they are somewhat familiar. This can be particularly dangerous if the model produces content that is entirely false or misleading, leading to incorrect decisions or actions taken based on that information.

(p34.2) Such outcomes can have serious consequences in many domains, such as healthcare, finance, or public policy, where the accuracy and reliability of information are critical. To mitigate these issues, reinforcement learning from human feedback (RLHF) is widely used [75,77] and LLMs themselves have been integrated into the loop [75].

(p34.3) Harmful content. Due to the high coherence, quality, and plausibility of texts generated by LLMs, harmful contents from LLMs can cause significant harm, including hate speech, discrimination, incitement to violence, false narratives, and even social engineering attack. The implementation of safeguards to detect and correct those contents can be mitigation [97]. These LLMs can also have dual-use potential by providing required illicit information, leading to risks such as the proliferation of weapons [75] and even terrorism attack planning. It is crucial to ensure using these LLMs responsibly, with safeguards in place to prevent harm. Also, in existing work, feedback from humans plays an important role in getting rid of harmful outputs.

(p34.4) Privacy. LLMs can face serious security issues. An example is the issue of user privacy. It is reported that Samsung employees were using ChatGPT to process their work when they inadvertently leaked top-secret data, including the source code proper of the new program, internal meeting minutes related to the hardware, etc. The Italian data protection agency declared that OpenAI, the developer of ChatGPT, illicitly gathered personal user data, leading Italy to become the first government to prohibit ChatGPT over privacy concerns [1]."
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s10,•,"['p10.0', 'p10.1']","['The NoOp module can be seen as a skip command. It is used to reduce computations when the controller decides no action is required. However, this system approaches question decomposition by having a decomposer model trained via human labels (Cao and Liu, 2021a). Figure 5: Architecture of Self-assembling MNM [18] ONUS: Perez et al. [27] proposed an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that map a complicate multi-hop question to some simpler single-hop sub-questions. Unlike other decomposition studies use a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions to decompose multi-hop question which require significant human effort, this model automatically learns to decompose different kinds of questions. The main idea has been shown in Figure 6. To decompose multi-hop question to simpler corpus , First some candidate sub-question from a simple corpus will be created by mining 10M possible sub-questions from Common Crawl with a classifier. It then trains a decomposition model on the mined data using Q and D with unsupervised sequence-to-sequence learning to map multi-hop questions to sub-question. With this idea, the model is able to train a large transformer model to generate decompositions and avoid heuristic/extractive decompositions.   RERC: Fu et al. [26] also handled multi-hop MRC with a focus on decomposing complicated questions into simpler ones. They proposed a three-stage Relation Extractor-Reader and Comparator (RERC) model. RERC is consist of 1) Relation Extractor to decompose the complex questions into simple sub-questions by automatically extracting the subject and key relations of the complex question, 2) Reader to find the answers to the sub-questions in turn by an advanced ALBERT model, and finally, 3) Comparator to perform the numerical comparison and summarizes all the answers to get the final answer ( Figure 9)', 'The main part of this model is Relation Extractor with two different structures: 1) classification-type (CRERC), where the evidence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; 2) span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor can automatically extract multiple corresponding spans from the question text as question relations. Figure 9: Architecture of the RERC models [26] The Decomposition technique was one of the first ideas for multi-hop MRC and as you can see in recent years (2021) still has attracted attention. The main disadvantage of this technique is that, instead of focusing on multi-hop reasoning as an important key of multi-hop MRC, it focuses on reducing the problem to a single-hop MRC. Thus, they actually do not go far beyond single-hop models.']","The NoOp module can be seen as a skip command. It is used to reduce computations when the controller decides no action is required. However, this system approaches question decomposition by having a decomposer model trained via human labels (Cao and Liu, 2021a). Figure 5: Architecture of Self-assembling MNM [18] ONUS: Perez et al. [27] proposed an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that map a complicate multi-hop question to some simpler single-hop sub-questions. Unlike other decomposition studies use a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions to decompose multi-hop question which require significant human effort, this model automatically learns to decompose different kinds of questions. The main idea has been shown in Figure 6. To decompose multi-hop question to simpler corpus , First some candidate sub-question from a simple corpus will be created by mining 10M possible sub-questions from Common Crawl with a classifier. It then trains a decomposition model on the mined data using Q and D with unsupervised sequence-to-sequence learning to map multi-hop questions to sub-question. With this idea, the model is able to train a large transformer model to generate decompositions and avoid heuristic/extractive decompositions.   RERC: Fu et al. [26] also handled multi-hop MRC with a focus on decomposing complicated questions into simpler ones. They proposed a three-stage Relation Extractor-Reader and Comparator (RERC) model. RERC is consist of 1) Relation Extractor to decompose the complex questions into simple sub-questions by automatically extracting the subject and key relations of the complex question, 2) Reader to find the answers to the sub-questions in turn by an advanced ALBERT model, and finally, 3) Comparator to perform the numerical comparison and summarizes all the answers to get the final answer ( Figure 9)

The main part of this model is Relation Extractor with two different structures: 1) classification-type (CRERC), where the evidence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; 2) span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor can automatically extract multiple corresponding spans from the question text as question relations. Figure 9: Architecture of the RERC models [26] The Decomposition technique was one of the first ideas for multi-hop MRC and as you can see in recent years (2021) still has attracted attention. The main disadvantage of this technique is that, instead of focusing on multi-hop reasoning as an important key of multi-hop MRC, it focuses on reducing the problem to a single-hop MRC. Thus, they actually do not go far beyond single-hop models.","(p10.0) The NoOp module can be seen as a skip command. It is used to reduce computations when the controller decides no action is required. However, this system approaches question decomposition by having a decomposer model trained via human labels (Cao and Liu, 2021a). Figure 5: Architecture of Self-assembling MNM [18] ONUS: Perez et al. [27] proposed an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that map a complicate multi-hop question to some simpler single-hop sub-questions. Unlike other decomposition studies use a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions to decompose multi-hop question which require significant human effort, this model automatically learns to decompose different kinds of questions. The main idea has been shown in Figure 6. To decompose multi-hop question to simpler corpus , First some candidate sub-question from a simple corpus will be created by mining 10M possible sub-questions from Common Crawl with a classifier. It then trains a decomposition model on the mined data using Q and D with unsupervised sequence-to-sequence learning to map multi-hop questions to sub-question. With this idea, the model is able to train a large transformer model to generate decompositions and avoid heuristic/extractive decompositions.   RERC: Fu et al. [26] also handled multi-hop MRC with a focus on decomposing complicated questions into simpler ones. They proposed a three-stage Relation Extractor-Reader and Comparator (RERC) model. RERC is consist of 1) Relation Extractor to decompose the complex questions into simple sub-questions by automatically extracting the subject and key relations of the complex question, 2) Reader to find the answers to the sub-questions in turn by an advanced ALBERT model, and finally, 3) Comparator to perform the numerical comparison and summarizes all the answers to get the final answer ( Figure 9)

(p10.1) The main part of this model is Relation Extractor with two different structures: 1) classification-type (CRERC), where the evidence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; 2) span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor can automatically extract multiple corresponding spans from the question text as question relations. Figure 9: Architecture of the RERC models [26] The Decomposition technique was one of the first ideas for multi-hop MRC and as you can see in recent years (2021) still has attracted attention. The main disadvantage of this technique is that, instead of focusing on multi-hop reasoning as an important key of multi-hop MRC, it focuses on reducing the problem to a single-hop MRC. Thus, they actually do not go far beyond single-hop models.","[['b17', 'b25', 'b26', 'b2'], ['b25']]","[['b17', 'b25', 'b26', 'b2'], ['b25']]",5,"1. The NoOp module can be seen as a skip command.
2. It is used to reduce computations when the controller decides no action is required.
3. However, this system approaches question decomposition by having a decomposer model trained via human labels (Cao and Liu, 2021a).
4. Figure 5: Architecture of Self-assembling MNM [18] ONUS: Perez et al. [27] proposed an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that map a complicate multi-hop question to some simpler single-hop sub-questions.
5. Unlike other decomposition studies use a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions to decompose multi-hop question which require significant human effort, this model automatically learns to decompose different kinds of questions.
6. The main idea has been shown in Figure 6.
7. To decompose multi-hop question to simpler corpus , First some candidate sub-question from a simple corpus will be created by mining 10M possible sub-questions from Common Crawl with a classifier.
8. It then trains a decomposition model on the mined data using Q and D with unsupervised sequence-to-sequence learning to map multi-hop questions to sub-question.
9. With this idea, the model is able to train a large transformer model to generate decompositions and avoid heuristic/extractive decompositions.
10. RERC: Fu et al. [26] also handled multi-hop MRC with a focus on decomposing complicated questions into simpler ones.
11. They proposed a three-stage Relation Extractor-Reader and Comparator (RERC) model.
12. RERC is consist of 1) Relation Extractor to decompose the complex questions into simple sub-questions by automatically extracting the subject and key relations of the complex question, 2) Reader to find the answers to the sub-questions in turn by an advanced ALBERT model, and finally, 3) Comparator to perform the numerical comparison and summarizes all the answers to get the final answer ( Figure 9)The main part of this model is Relation Extractor with two different structures: 1) classification-type (CRERC), where the evidence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; 2) span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor can automatically extract multiple corresponding spans from the question text as question relations.
13. Figure 9: Architecture of the RERC models [26]
14. The Decomposition technique was one of the first ideas for multi-hop MRC and as you can see in recent years [18]0 still has attracted attention.
15. The main disadvantage of this technique is that, instead of focusing on multi-hop reasoning as an important key of multi-hop MRC, it focuses on reducing the problem to a single-hop MRC.
16. Thus, they actually do not go far beyond single-hop models.","What is the function of the NoOp module in multi-hop machine reading comprehension systems?
How does the ONUS algorithm facilitate the decomposition of complex multi-hop questions into simpler single-hop sub-questions?
What are the primary functions of the Relation Extractor, Reader, and Comparator in the RERC model for multi-hop MRC?
In what ways do the classification-type and span-type structures within the RERC model's Relation Extractor differ?
What are the limitations of focusing primarily on question decomposition techniques in the context of multi-hop machine reading comprehension research?","1. What is the function of the NoOp module in multi-hop machine reading comprehension systems?
2. How does the ONUS algorithm facilitate the decomposition of complex multi-hop questions into simpler single-hop sub-questions?
3. What are the primary functions of the Relation Extractor, Reader, and Comparator in the RERC model for multi-hop MRC?
4. In what ways do the classification-type and span-type structures within the RERC model's Relation Extractor differ?
5. What are the limitations of focusing primarily on question decomposition techniques in the context of multi-hop machine reading comprehension research?",None,,"Questions:

1. What is the function of the NoOp module in multi-hop machine reading comprehension systems?
2. How does the ONUS algorithm facilitate the decomposition of complex multi-hop questions into simpler single-hop sub-questions?
3. What are the primary functions of the Relation Extractor, Reader, and Comparator in the RERC model for multi-hop MRC?
4. In what ways do the classification-type and span-type structures within the RERC model's Relation Extractor differ?
5. What are the limitations of focusing primarily on question decomposition techniques in the context of multi-hop machine reading comprehension research?

Answer:

(p10.0) The NoOp module can be seen as a skip command. It is used to reduce computations when the controller decides no action is required. However, this system approaches question decomposition by having a decomposer model trained via human labels (Cao and Liu, 2021a). Figure 5: Architecture of Self-assembling MNM [18] ONUS: Perez et al. [27] proposed an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that map a complicate multi-hop question to some simpler single-hop sub-questions. Unlike other decomposition studies use a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions to decompose multi-hop question which require significant human effort, this model automatically learns to decompose different kinds of questions. The main idea has been shown in Figure 6. To decompose multi-hop question to simpler corpus , First some candidate sub-question from a simple corpus will be created by mining 10M possible sub-questions from Common Crawl with a classifier. It then trains a decomposition model on the mined data using Q and D with unsupervised sequence-to-sequence learning to map multi-hop questions to sub-question. With this idea, the model is able to train a large transformer model to generate decompositions and avoid heuristic/extractive decompositions.   RERC: Fu et al. [26] also handled multi-hop MRC with a focus on decomposing complicated questions into simpler ones. They proposed a three-stage Relation Extractor-Reader and Comparator (RERC) model. RERC is consist of 1) Relation Extractor to decompose the complex questions into simple sub-questions by automatically extracting the subject and key relations of the complex question, 2) Reader to find the answers to the sub-questions in turn by an advanced ALBERT model, and finally, 3) Comparator to perform the numerical comparison and summarizes all the answers to get the final answer ( Figure 9)

(p10.1) The main part of this model is Relation Extractor with two different structures: 1) classification-type (CRERC), where the evidence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; 2) span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor can automatically extract multiple corresponding spans from the question text as question relations. Figure 9: Architecture of the RERC models [26] The Decomposition technique was one of the first ideas for multi-hop MRC and as you can see in recent years (2021) still has attracted attention. The main disadvantage of this technique is that, instead of focusing on multi-hop reasoning as an important key of multi-hop MRC, it focuses on reducing the problem to a single-hop MRC. Thus, they actually do not go far beyond single-hop models.","Questions:



Answer:

(p10.0) The NoOp module can be seen as a skip command. It is used to reduce computations when the controller decides no action is required. However, this system approaches question decomposition by having a decomposer model trained via human labels (Cao and Liu, 2021a). Figure 5: Architecture of Self-assembling MNM [18] ONUS: Perez et al. [27] proposed an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that map a complicate multi-hop question to some simpler single-hop sub-questions. Unlike other decomposition studies use a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions to decompose multi-hop question which require significant human effort, this model automatically learns to decompose different kinds of questions. The main idea has been shown in Figure 6. To decompose multi-hop question to simpler corpus , First some candidate sub-question from a simple corpus will be created by mining 10M possible sub-questions from Common Crawl with a classifier. It then trains a decomposition model on the mined data using Q and D with unsupervised sequence-to-sequence learning to map multi-hop questions to sub-question. With this idea, the model is able to train a large transformer model to generate decompositions and avoid heuristic/extractive decompositions.   RERC: Fu et al. [26] also handled multi-hop MRC with a focus on decomposing complicated questions into simpler ones. They proposed a three-stage Relation Extractor-Reader and Comparator (RERC) model. RERC is consist of 1) Relation Extractor to decompose the complex questions into simple sub-questions by automatically extracting the subject and key relations of the complex question, 2) Reader to find the answers to the sub-questions in turn by an advanced ALBERT model, and finally, 3) Comparator to perform the numerical comparison and summarizes all the answers to get the final answer ( Figure 9)

(p10.1) The main part of this model is Relation Extractor with two different structures: 1) classification-type (CRERC), where the evidence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; 2) span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor can automatically extract multiple corresponding spans from the question text as question relations. Figure 9: Architecture of the RERC models [26] The Decomposition technique was one of the first ideas for multi-hop MRC and as you can see in recent years (2021) still has attracted attention. The main disadvantage of this technique is that, instead of focusing on multi-hop reasoning as an important key of multi-hop MRC, it focuses on reducing the problem to a single-hop MRC. Thus, they actually do not go far beyond single-hop models."
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s13,Commonsense Algorithm consists of Commonsense Selection Representation to select useful relational knowledge paths and,"['p13.0', 'p13.1', 'p13.2', 'p13.3']","['Commonsense Model Incorporation to fill the gaps of reasoning between hops of inference using Necessary and Optional Information Cell (NOIC).  [29] QFE: Nishida et al. [30] proposed a model for explainable multi-hop QA named Query Focused Extractor (QFE) which is based on a summarization idea. They use the multi-task learning of the QA model for answer selection and QFE for evidence extraction.', 'QFE as the main part of this model adaptively determines the number of evidence sentences by considering the dependency among the evidence sentences and the coverage of the question. Unlike other approaches that extract each evidence sentence separately, QFE uses an RNN and attention mechanism to consider important information in the question and the relationships between sentences. This query-aware recurrent structure enables QFE to consider the dependency among the evidence sentences and cover the important information in the question sentence. In brief, the main goal of QFE is to summarize the context according to the question. Query-focused summarization is the task of summarizing the source document with regard to the given query. The multitask learning with QFE is general in the sense that it can be combined with any QA model. The overview of QFE is shown in Figure 11. is the current summarization vector, is the query vector considering the current summarization, is the extracted sentence, updates the RNN state. Figure 11: Overview of Query Focused Extractor at step t [30] TAP: Bhargav et al. [31] proposed a deep neural architecture, called TAP (Translucent Answer Prediction) cover of two main ideas: (1) Local Interaction: Each sentence should be understood in the context of its neighboring sentences and the question, (2) Global Interaction: A global (inter-passage) interaction among sentences must be identified and used for supporting facts. TAP is a hierarchical architecture that tries to capture the local and global interactions between the sentences and consists of two main parts: (Figure 12)', '• Local and Global Interaction eXtractor (LoGIX) with three layers: local layer to obtain intra-passage dependencies, Global Layer to obtain inter-passage dependencies, and Supporting Facts Prediction Layer to calculate the probability that a sentence is a supporting fact.', '• Answer Predictor (AP) to predict the final answer by reasoning over the supporting facts. It consists of four parts: Input Data Shaping to preprocess and concatenate the supporting facts, Context Encoding to encode the context using a pre-trained BERT model, Answer Type Predictor to classify the question into one of the three classes (yes, no and, span), and Answer Span Predictor to predict the final answer. Figure 12: Architecture of TAP [31] PH-Model: Cong et al. [32] focused on using the benefit of the hierarchical structure of the natural language text (documentpassage-sentence-word-character), while most existing studies ignore this information in the natural language context. Then they proposed a model for Chinese multi-hop MRC named (PH-Model), in which P stands for the Passage reranking framework, and H denotes the Hierarchical neural network. As you can see in Figure 13, PH-Model consists of multiple main parts: Bi_ONLSTM, that is an ordered neuron LSTM is used to obtain hierarchical information from a passage (Instead of traditional LSTM), Bidirectional Attention Flow is used to extract the hierarchical information form paragraphs to get the query-aware context and the context-aware query representation, Fused layer is used to merge all information and finally Pointer network to obtain the probability of the start and end positions of the answer ']","Commonsense Model Incorporation to fill the gaps of reasoning between hops of inference using Necessary and Optional Information Cell (NOIC).  [29] QFE: Nishida et al. [30] proposed a model for explainable multi-hop QA named Query Focused Extractor (QFE) which is based on a summarization idea. They use the multi-task learning of the QA model for answer selection and QFE for evidence extraction.

QFE as the main part of this model adaptively determines the number of evidence sentences by considering the dependency among the evidence sentences and the coverage of the question. Unlike other approaches that extract each evidence sentence separately, QFE uses an RNN and attention mechanism to consider important information in the question and the relationships between sentences. This query-aware recurrent structure enables QFE to consider the dependency among the evidence sentences and cover the important information in the question sentence. In brief, the main goal of QFE is to summarize the context according to the question. Query-focused summarization is the task of summarizing the source document with regard to the given query. The multitask learning with QFE is general in the sense that it can be combined with any QA model. The overview of QFE is shown in Figure 11. is the current summarization vector, is the query vector considering the current summarization, is the extracted sentence, updates the RNN state. Figure 11: Overview of Query Focused Extractor at step t [30] TAP: Bhargav et al. [31] proposed a deep neural architecture, called TAP (Translucent Answer Prediction) cover of two main ideas: (1) Local Interaction: Each sentence should be understood in the context of its neighboring sentences and the question, (2) Global Interaction: A global (inter-passage) interaction among sentences must be identified and used for supporting facts. TAP is a hierarchical architecture that tries to capture the local and global interactions between the sentences and consists of two main parts: (Figure 12)

• Local and Global Interaction eXtractor (LoGIX) with three layers: local layer to obtain intra-passage dependencies, Global Layer to obtain inter-passage dependencies, and Supporting Facts Prediction Layer to calculate the probability that a sentence is a supporting fact.

• Answer Predictor (AP) to predict the final answer by reasoning over the supporting facts. It consists of four parts: Input Data Shaping to preprocess and concatenate the supporting facts, Context Encoding to encode the context using a pre-trained BERT model, Answer Type Predictor to classify the question into one of the three classes (yes, no and, span), and Answer Span Predictor to predict the final answer. Figure 12: Architecture of TAP [31] PH-Model: Cong et al. [32] focused on using the benefit of the hierarchical structure of the natural language text (documentpassage-sentence-word-character), while most existing studies ignore this information in the natural language context. Then they proposed a model for Chinese multi-hop MRC named (PH-Model), in which P stands for the Passage reranking framework, and H denotes the Hierarchical neural network. As you can see in Figure 13, PH-Model consists of multiple main parts: Bi_ONLSTM, that is an ordered neuron LSTM is used to obtain hierarchical information from a passage (Instead of traditional LSTM), Bidirectional Attention Flow is used to extract the hierarchical information form paragraphs to get the query-aware context and the context-aware query representation, Fused layer is used to merge all information and finally Pointer network to obtain the probability of the start and end positions of the answer ","(p13.0) Commonsense Model Incorporation to fill the gaps of reasoning between hops of inference using Necessary and Optional Information Cell (NOIC).  [29] QFE: Nishida et al. [30] proposed a model for explainable multi-hop QA named Query Focused Extractor (QFE) which is based on a summarization idea. They use the multi-task learning of the QA model for answer selection and QFE for evidence extraction.

(p13.1) QFE as the main part of this model adaptively determines the number of evidence sentences by considering the dependency among the evidence sentences and the coverage of the question. Unlike other approaches that extract each evidence sentence separately, QFE uses an RNN and attention mechanism to consider important information in the question and the relationships between sentences. This query-aware recurrent structure enables QFE to consider the dependency among the evidence sentences and cover the important information in the question sentence. In brief, the main goal of QFE is to summarize the context according to the question. Query-focused summarization is the task of summarizing the source document with regard to the given query. The multitask learning with QFE is general in the sense that it can be combined with any QA model. The overview of QFE is shown in Figure 11. is the current summarization vector, is the query vector considering the current summarization, is the extracted sentence, updates the RNN state. Figure 11: Overview of Query Focused Extractor at step t [30] TAP: Bhargav et al. [31] proposed a deep neural architecture, called TAP (Translucent Answer Prediction) cover of two main ideas: (1) Local Interaction: Each sentence should be understood in the context of its neighboring sentences and the question, (2) Global Interaction: A global (inter-passage) interaction among sentences must be identified and used for supporting facts. TAP is a hierarchical architecture that tries to capture the local and global interactions between the sentences and consists of two main parts: (Figure 12)

(p13.2) • Local and Global Interaction eXtractor (LoGIX) with three layers: local layer to obtain intra-passage dependencies, Global Layer to obtain inter-passage dependencies, and Supporting Facts Prediction Layer to calculate the probability that a sentence is a supporting fact.

(p13.3) • Answer Predictor (AP) to predict the final answer by reasoning over the supporting facts. It consists of four parts: Input Data Shaping to preprocess and concatenate the supporting facts, Context Encoding to encode the context using a pre-trained BERT model, Answer Type Predictor to classify the question into one of the three classes (yes, no and, span), and Answer Span Predictor to predict the final answer. Figure 12: Architecture of TAP [31] PH-Model: Cong et al. [32] focused on using the benefit of the hierarchical structure of the natural language text (documentpassage-sentence-word-character), while most existing studies ignore this information in the natural language context. Then they proposed a model for Chinese multi-hop MRC named (PH-Model), in which P stands for the Passage reranking framework, and H denotes the Hierarchical neural network. As you can see in Figure 13, PH-Model consists of multiple main parts: Bi_ONLSTM, that is an ordered neuron LSTM is used to obtain hierarchical information from a passage (Instead of traditional LSTM), Bidirectional Attention Flow is used to extract the hierarchical information form paragraphs to get the query-aware context and the context-aware query representation, Fused layer is used to merge all information and finally Pointer network to obtain the probability of the start and end positions of the answer ","[['b28', 'b29'], ['b30', 'b1', 'b29'], [], ['b30', 'b31']]","[['b28', 'b29'], ['b30', 'b1', 'b29'], [], ['b30', 'b31']]",7,"1. Commonsense Model Incorporation to fill the gaps of reasoning between hops of inference using Necessary and Optional Information Cell (NOIC).
2. [29] QFE: Nishida et al. [30] proposed a model for explainable multi-hop QA named Query Focused Extractor (QFE) which is based on a summarization idea.
3. They use the multi-task learning of the QA model for answer selection and QFE for evidence extraction.QFE as the main part of this model adaptively determines the number of evidence sentences by considering the dependency among the evidence sentences and the coverage of the question.
4. Unlike other approaches that extract each evidence sentence separately, QFE uses an RNN and attention mechanism to consider important information in the question and the relationships between sentences.
5. This query-aware recurrent structure enables QFE to consider the dependency among the evidence sentences and cover the important information in the question sentence.
6. In brief, the main goal of QFE is to summarize the context according to the question.
7. Query-focused summarization is the task of summarizing the source document with regard to the given query.
8. The multitask learning with QFE is general in the sense that it can be combined with any QA model.
9. The overview of QFE is shown in Figure 11.
10. is the current summarization vector, is the query vector considering the current summarization, is the extracted sentence, updates the RNN state.
11. Figure 11: Overview of Query Focused Extractor at step t
12. [30] TAP: Bhargav et al. [29]4 proposed a deep neural architecture, called TAP (Translucent Answer Prediction) cover of two main ideas: (1) Local Interaction: Each sentence should be understood in the context of its neighboring sentences and the question, (2)
13. Global Interaction: A global (inter-passage) interaction among sentences must be identified and used for supporting facts.
14. TAP is a hierarchical architecture that tries to capture the local and global interactions between the sentences and consists of two main parts: [29]0 Local and Global Interaction eXtractor [29]1 with three layers: local layer to obtain intra-passage dependencies, Global Layer to obtain inter-passage dependencies, and Supporting Facts Prediction Layer to calculate the probability that a sentence is a supporting fact.
15. Answer Predictor [29]2 to predict the final answer by reasoning over the supporting facts.
16. It consists of four parts: Input Data Shaping to preprocess and concatenate the supporting facts, Context Encoding to encode the context using a pre-trained BERT model, Answer Type Predictor to classify the question into one of the three classes [29]3, and Answer Span Predictor to predict the final answer.
17. Figure 12: Architecture of TAP [29]4 PH-Model: Cong et al. [29]5 focused on using the benefit of the hierarchical structure of the natural language text [29]6, while most existing studies ignore this information in the natural language context.
18. Then they proposed a model for Chinese multi-hop MRC named [29]7, in which P stands for the Passage reranking framework, and H denotes the Hierarchical neural network.
19. As you can see in Figure 13, PH-Model consists of multiple main parts: Bi_ONLSTM, that is an ordered neuron LSTM is used to obtain hierarchical information from a passage [29]8, Bidirectional Attention Flow is used to extract the hierarchical information form paragraphs to get the query-aware context and the context-aware query representation, Fused layer is used to merge all information and finally Pointer network to obtain the probability of the start and end positions of the answer","How does the Query Focused Extractor (QFE) model utilize multi-task learning for enhancing evidence extraction in multi-hop QA?
What role does the RNN and attention mechanism play in the QFE model's process of evidence sentence selection?
How does the TAP model's Local and Global Interaction eXtractor (LoGIX) contribute to understanding sentence dependencies in multi-hop machine reading comprehension?
In what way does the Answer Predictor component of the TAP model utilize pre-trained BERT for context encoding?
What distinguishes the PH-Model's use of Bi_ONLSTM for obtaining hierarchical information in multi-hop machine reading comprehension from traditional LSTM approaches?","1. How does the Query Focused Extractor (QFE) model utilize multi-task learning for enhancing evidence extraction in multi-hop QA?
2. What role does the RNN and attention mechanism play in the QFE model's process of evidence sentence selection?
3. How does the TAP model's Local and Global Interaction eXtractor (LoGIX) contribute to understanding sentence dependencies in multi-hop machine reading comprehension?
4. In what way does the Answer Predictor component of the TAP model utilize pre-trained BERT for context encoding?
5. What distinguishes the PH-Model's use of Bi_ONLSTM for obtaining hierarchical information in multi-hop machine reading comprehension from traditional LSTM approaches?",None,,"Questions:

1. How does the Query Focused Extractor (QFE) model utilize multi-task learning for enhancing evidence extraction in multi-hop QA?
2. What role does the RNN and attention mechanism play in the QFE model's process of evidence sentence selection?
3. How does the TAP model's Local and Global Interaction eXtractor (LoGIX) contribute to understanding sentence dependencies in multi-hop machine reading comprehension?
4. In what way does the Answer Predictor component of the TAP model utilize pre-trained BERT for context encoding?
5. What distinguishes the PH-Model's use of Bi_ONLSTM for obtaining hierarchical information in multi-hop machine reading comprehension from traditional LSTM approaches?

Answer:

(p13.0) Commonsense Model Incorporation to fill the gaps of reasoning between hops of inference using Necessary and Optional Information Cell (NOIC).  [29] QFE: Nishida et al. [30] proposed a model for explainable multi-hop QA named Query Focused Extractor (QFE) which is based on a summarization idea. They use the multi-task learning of the QA model for answer selection and QFE for evidence extraction.

(p13.1) QFE as the main part of this model adaptively determines the number of evidence sentences by considering the dependency among the evidence sentences and the coverage of the question. Unlike other approaches that extract each evidence sentence separately, QFE uses an RNN and attention mechanism to consider important information in the question and the relationships between sentences. This query-aware recurrent structure enables QFE to consider the dependency among the evidence sentences and cover the important information in the question sentence. In brief, the main goal of QFE is to summarize the context according to the question. Query-focused summarization is the task of summarizing the source document with regard to the given query. The multitask learning with QFE is general in the sense that it can be combined with any QA model. The overview of QFE is shown in Figure 11. is the current summarization vector, is the query vector considering the current summarization, is the extracted sentence, updates the RNN state. Figure 11: Overview of Query Focused Extractor at step t [30] TAP: Bhargav et al. [31] proposed a deep neural architecture, called TAP (Translucent Answer Prediction) cover of two main ideas: (1) Local Interaction: Each sentence should be understood in the context of its neighboring sentences and the question, (2) Global Interaction: A global (inter-passage) interaction among sentences must be identified and used for supporting facts. TAP is a hierarchical architecture that tries to capture the local and global interactions between the sentences and consists of two main parts: (Figure 12)

(p13.2) • Local and Global Interaction eXtractor (LoGIX) with three layers: local layer to obtain intra-passage dependencies, Global Layer to obtain inter-passage dependencies, and Supporting Facts Prediction Layer to calculate the probability that a sentence is a supporting fact.

(p13.3) • Answer Predictor (AP) to predict the final answer by reasoning over the supporting facts. It consists of four parts: Input Data Shaping to preprocess and concatenate the supporting facts, Context Encoding to encode the context using a pre-trained BERT model, Answer Type Predictor to classify the question into one of the three classes (yes, no and, span), and Answer Span Predictor to predict the final answer. Figure 12: Architecture of TAP [31] PH-Model: Cong et al. [32] focused on using the benefit of the hierarchical structure of the natural language text (documentpassage-sentence-word-character), while most existing studies ignore this information in the natural language context. Then they proposed a model for Chinese multi-hop MRC named (PH-Model), in which P stands for the Passage reranking framework, and H denotes the Hierarchical neural network. As you can see in Figure 13, PH-Model consists of multiple main parts: Bi_ONLSTM, that is an ordered neuron LSTM is used to obtain hierarchical information from a passage (Instead of traditional LSTM), Bidirectional Attention Flow is used to extract the hierarchical information form paragraphs to get the query-aware context and the context-aware query representation, Fused layer is used to merge all information and finally Pointer network to obtain the probability of the start and end positions of the answer ","Questions:



Answer:

(p13.0) Commonsense Model Incorporation to fill the gaps of reasoning between hops of inference using Necessary and Optional Information Cell (NOIC).  [29] QFE: Nishida et al. [30] proposed a model for explainable multi-hop QA named Query Focused Extractor (QFE) which is based on a summarization idea. They use the multi-task learning of the QA model for answer selection and QFE for evidence extraction.

(p13.1) QFE as the main part of this model adaptively determines the number of evidence sentences by considering the dependency among the evidence sentences and the coverage of the question. Unlike other approaches that extract each evidence sentence separately, QFE uses an RNN and attention mechanism to consider important information in the question and the relationships between sentences. This query-aware recurrent structure enables QFE to consider the dependency among the evidence sentences and cover the important information in the question sentence. In brief, the main goal of QFE is to summarize the context according to the question. Query-focused summarization is the task of summarizing the source document with regard to the given query. The multitask learning with QFE is general in the sense that it can be combined with any QA model. The overview of QFE is shown in Figure 11. is the current summarization vector, is the query vector considering the current summarization, is the extracted sentence, updates the RNN state. Figure 11: Overview of Query Focused Extractor at step t [30] TAP: Bhargav et al. [31] proposed a deep neural architecture, called TAP (Translucent Answer Prediction) cover of two main ideas: (1) Local Interaction: Each sentence should be understood in the context of its neighboring sentences and the question, (2) Global Interaction: A global (inter-passage) interaction among sentences must be identified and used for supporting facts. TAP is a hierarchical architecture that tries to capture the local and global interactions between the sentences and consists of two main parts: (Figure 12)

(p13.2) • Local and Global Interaction eXtractor (LoGIX) with three layers: local layer to obtain intra-passage dependencies, Global Layer to obtain inter-passage dependencies, and Supporting Facts Prediction Layer to calculate the probability that a sentence is a supporting fact.

(p13.3) • Answer Predictor (AP) to predict the final answer by reasoning over the supporting facts. It consists of four parts: Input Data Shaping to preprocess and concatenate the supporting facts, Context Encoding to encode the context using a pre-trained BERT model, Answer Type Predictor to classify the question into one of the three classes (yes, no and, span), and Answer Span Predictor to predict the final answer. Figure 12: Architecture of TAP [31] PH-Model: Cong et al. [32] focused on using the benefit of the hierarchical structure of the natural language text (documentpassage-sentence-word-character), while most existing studies ignore this information in the natural language context. Then they proposed a model for Chinese multi-hop MRC named (PH-Model), in which P stands for the Passage reranking framework, and H denotes the Hierarchical neural network. As you can see in Figure 13, PH-Model consists of multiple main parts: Bi_ONLSTM, that is an ordered neuron LSTM is used to obtain hierarchical information from a passage (Instead of traditional LSTM), Bidirectional Attention Flow is used to extract the hierarchical information form paragraphs to get the query-aware context and the context-aware query representation, Fused layer is used to merge all information and finally Pointer network to obtain the probability of the start and end positions of the answer "
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s16,Rank-2 Path:,"['p16.0', 'p16.1', 'p16.2', 'p16.3', 'p16.4']","['(zoo lake, South Africa, gauteng) Passage1: ... Zoo Lake is a popular lake and public park in Johannesburg, South Africa. It is ... Passage2: ... aka The Reef, is a 56-kilometre -long north -facing scarp in the Gauteng Province of South Africa. It consists of a ...  [36] focused on the inherent sequential of the multi-hop MRC, which means the system must decide where to look next, based on the current state. Then they proposed a model named Deep Reinforcement Learning-based where the knowledge extraction phase is explicitly decoupled from the question answering phase. This model uses the current information in the knowledge chain to inform which information should be achieved in the next step. The model consists of two main components:1) a Linker to construct a sentence-level chain form the sentences of supporting documents that allow movement between documents, and 2) an Extractor which learns where to look based on the current question and already-visited sentences.', 'As Figure 16 shows, after embedding sentences into a matrix and constructing a graph, the policy is run to select the next sentence. In this regard, the convolution-based policy network is used to learn a traversal policy from current options, previously accepted sentences, and the current question. For example, in this figure, the policy has learned to select the correct next sentence (ct) from the previous sentence (ct-1). After this action, the answer sentence (o1) can be selected during the next step. SMR: Huo, Ge and Zhao [37] proposed a Sentence-level Multi-hop Reasoning approach named SMR while most existing approaches only use document-level or entity-level inferences. In this regard, an initial sentence is first found based on the main entity in the question, and that sentence is then used to find more relevant sentences to create multiple sentence-based reasoning chains as a memory network. Besides, some sentences are concatenated to prevent the mistakes of Co-Reference resolution methods and to reduce the number of hops. An example of such a concatenation is shown in Figure 17.', 'There are two phases in this model: 1) the Selecting phase to select the most relevant sentence to the network memory state as the initial sentence of the current hop, and 2) the Establishing phase to prepare to go to the next hop by updating the network memory state. Finally, the information of the reasoning chains is used to predict the final answer. Figure 17: Representation enrichment via concatenation of adjacent sentences [37] ChainEx: Chen, Lin and Durrett [38] proposed a sentence-based model that does not rely on gold annotated chains or supporting facts at the training and test phases, instead, pseudo-gold reasoning chains are derived using some heuristics based on named entity recognition and coreference resolution during the training time, and it learns to extract chains from raw texts at the test time. They first extract a reasoning chain over the text for a multi-hop reasoning task, and then apply a BERT-based QA system to find the answer by learning from these chains.', 'To construct the reasoning chain, each sentence is considered as a node in the chain, and there is an edge between two sentences if they have the same entity. Besides, there are edges between all sentences from the same paragraph. The model starts from the question and finds all possible reasoning chains. The chain extractor is a neural network with two main components: Sentence Encoding and Chain Prediction. In the sentence encoding component, the BERT-Para model provides a representation from each paragraph jointly with the question. In the chain prediction component, an LSTM-based pointer network is used to extract the reasoning chain. The output of the chain extractor is a variable-length sequence of sentences. Finally, a BERT-based QA system is applied to the extracted chains to find the final answer. Figure 18 show an example with two possible ""reasoning chains"". The first chain is most appropriate, while the second requires a less well-supported inferential leap. Figure 18: A multi-hop example in ChainEx [38] SCR: Huo and Zhao [35] proposed another sentence-level model, since using entity and document makes the chain too coarse and ambiguous or too subtle, limited, and less accurate. This study believes that using sentences as inference nodes is more reasonable than using documents or entities. To explain the problem, you can see an example in Figure 19, as it is clear, with using documents, although the path is more complete, there is a lot of unrelated information, which can produce redundancy. On the other hand, with using entities, although the path is concise it is too subtle and limited, and much information will be lost. However, if the path uses sentences as nodes, it is not limited anymore and with less information redundancy. Besides a sentence-based path can explain the reasoning process better.', 'Then they proposed a Sentence-based Circular Reasoning (SCR) approach, and it consists of three modules: Sentence Encoder (SE) to obtain the sentence representation, Path Generator (PG) to iteratively infer among sentences of multiple documents according to the question, and Path Evaluator (PE) to evaluate the obtained path and predicts an answer ( Figure 20). Unlike the decomposition technique, this technique focuses on multi-hop reasoning instead of simplifying the multi-hop MRC problem and has achieved great attention among studies. The main disadvantage of this technique is that they are expensive and time-consuming approaches, and require a large amount of data to train.']","(zoo lake, South Africa, gauteng) Passage1: ... Zoo Lake is a popular lake and public park in Johannesburg, South Africa. It is ... Passage2: ... aka The Reef, is a 56-kilometre -long north -facing scarp in the Gauteng Province of South Africa. It consists of a ...  [36] focused on the inherent sequential of the multi-hop MRC, which means the system must decide where to look next, based on the current state. Then they proposed a model named Deep Reinforcement Learning-based where the knowledge extraction phase is explicitly decoupled from the question answering phase. This model uses the current information in the knowledge chain to inform which information should be achieved in the next step. The model consists of two main components:1) a Linker to construct a sentence-level chain form the sentences of supporting documents that allow movement between documents, and 2) an Extractor which learns where to look based on the current question and already-visited sentences.

As Figure 16 shows, after embedding sentences into a matrix and constructing a graph, the policy is run to select the next sentence. In this regard, the convolution-based policy network is used to learn a traversal policy from current options, previously accepted sentences, and the current question. For example, in this figure, the policy has learned to select the correct next sentence (ct) from the previous sentence (ct-1). After this action, the answer sentence (o1) can be selected during the next step. SMR: Huo, Ge and Zhao [37] proposed a Sentence-level Multi-hop Reasoning approach named SMR while most existing approaches only use document-level or entity-level inferences. In this regard, an initial sentence is first found based on the main entity in the question, and that sentence is then used to find more relevant sentences to create multiple sentence-based reasoning chains as a memory network. Besides, some sentences are concatenated to prevent the mistakes of Co-Reference resolution methods and to reduce the number of hops. An example of such a concatenation is shown in Figure 17.

There are two phases in this model: 1) the Selecting phase to select the most relevant sentence to the network memory state as the initial sentence of the current hop, and 2) the Establishing phase to prepare to go to the next hop by updating the network memory state. Finally, the information of the reasoning chains is used to predict the final answer. Figure 17: Representation enrichment via concatenation of adjacent sentences [37] ChainEx: Chen, Lin and Durrett [38] proposed a sentence-based model that does not rely on gold annotated chains or supporting facts at the training and test phases, instead, pseudo-gold reasoning chains are derived using some heuristics based on named entity recognition and coreference resolution during the training time, and it learns to extract chains from raw texts at the test time. They first extract a reasoning chain over the text for a multi-hop reasoning task, and then apply a BERT-based QA system to find the answer by learning from these chains.

To construct the reasoning chain, each sentence is considered as a node in the chain, and there is an edge between two sentences if they have the same entity. Besides, there are edges between all sentences from the same paragraph. The model starts from the question and finds all possible reasoning chains. The chain extractor is a neural network with two main components: Sentence Encoding and Chain Prediction. In the sentence encoding component, the BERT-Para model provides a representation from each paragraph jointly with the question. In the chain prediction component, an LSTM-based pointer network is used to extract the reasoning chain. The output of the chain extractor is a variable-length sequence of sentences. Finally, a BERT-based QA system is applied to the extracted chains to find the final answer. Figure 18 show an example with two possible ""reasoning chains"". The first chain is most appropriate, while the second requires a less well-supported inferential leap. Figure 18: A multi-hop example in ChainEx [38] SCR: Huo and Zhao [35] proposed another sentence-level model, since using entity and document makes the chain too coarse and ambiguous or too subtle, limited, and less accurate. This study believes that using sentences as inference nodes is more reasonable than using documents or entities. To explain the problem, you can see an example in Figure 19, as it is clear, with using documents, although the path is more complete, there is a lot of unrelated information, which can produce redundancy. On the other hand, with using entities, although the path is concise it is too subtle and limited, and much information will be lost. However, if the path uses sentences as nodes, it is not limited anymore and with less information redundancy. Besides a sentence-based path can explain the reasoning process better.

Then they proposed a Sentence-based Circular Reasoning (SCR) approach, and it consists of three modules: Sentence Encoder (SE) to obtain the sentence representation, Path Generator (PG) to iteratively infer among sentences of multiple documents according to the question, and Path Evaluator (PE) to evaluate the obtained path and predicts an answer ( Figure 20). Unlike the decomposition technique, this technique focuses on multi-hop reasoning instead of simplifying the multi-hop MRC problem and has achieved great attention among studies. The main disadvantage of this technique is that they are expensive and time-consuming approaches, and require a large amount of data to train.","(p16.0) (zoo lake, South Africa, gauteng) Passage1: ... Zoo Lake is a popular lake and public park in Johannesburg, South Africa. It is ... Passage2: ... aka The Reef, is a 56-kilometre -long north -facing scarp in the Gauteng Province of South Africa. It consists of a ...  [36] focused on the inherent sequential of the multi-hop MRC, which means the system must decide where to look next, based on the current state. Then they proposed a model named Deep Reinforcement Learning-based where the knowledge extraction phase is explicitly decoupled from the question answering phase. This model uses the current information in the knowledge chain to inform which information should be achieved in the next step. The model consists of two main components:1) a Linker to construct a sentence-level chain form the sentences of supporting documents that allow movement between documents, and 2) an Extractor which learns where to look based on the current question and already-visited sentences.

(p16.1) As Figure 16 shows, after embedding sentences into a matrix and constructing a graph, the policy is run to select the next sentence. In this regard, the convolution-based policy network is used to learn a traversal policy from current options, previously accepted sentences, and the current question. For example, in this figure, the policy has learned to select the correct next sentence (ct) from the previous sentence (ct-1). After this action, the answer sentence (o1) can be selected during the next step. SMR: Huo, Ge and Zhao [37] proposed a Sentence-level Multi-hop Reasoning approach named SMR while most existing approaches only use document-level or entity-level inferences. In this regard, an initial sentence is first found based on the main entity in the question, and that sentence is then used to find more relevant sentences to create multiple sentence-based reasoning chains as a memory network. Besides, some sentences are concatenated to prevent the mistakes of Co-Reference resolution methods and to reduce the number of hops. An example of such a concatenation is shown in Figure 17.

(p16.2) There are two phases in this model: 1) the Selecting phase to select the most relevant sentence to the network memory state as the initial sentence of the current hop, and 2) the Establishing phase to prepare to go to the next hop by updating the network memory state. Finally, the information of the reasoning chains is used to predict the final answer. Figure 17: Representation enrichment via concatenation of adjacent sentences [37] ChainEx: Chen, Lin and Durrett [38] proposed a sentence-based model that does not rely on gold annotated chains or supporting facts at the training and test phases, instead, pseudo-gold reasoning chains are derived using some heuristics based on named entity recognition and coreference resolution during the training time, and it learns to extract chains from raw texts at the test time. They first extract a reasoning chain over the text for a multi-hop reasoning task, and then apply a BERT-based QA system to find the answer by learning from these chains.

(p16.3) To construct the reasoning chain, each sentence is considered as a node in the chain, and there is an edge between two sentences if they have the same entity. Besides, there are edges between all sentences from the same paragraph. The model starts from the question and finds all possible reasoning chains. The chain extractor is a neural network with two main components: Sentence Encoding and Chain Prediction. In the sentence encoding component, the BERT-Para model provides a representation from each paragraph jointly with the question. In the chain prediction component, an LSTM-based pointer network is used to extract the reasoning chain. The output of the chain extractor is a variable-length sequence of sentences. Finally, a BERT-based QA system is applied to the extracted chains to find the final answer. Figure 18 show an example with two possible ""reasoning chains"". The first chain is most appropriate, while the second requires a less well-supported inferential leap. Figure 18: A multi-hop example in ChainEx [38] SCR: Huo and Zhao [35] proposed another sentence-level model, since using entity and document makes the chain too coarse and ambiguous or too subtle, limited, and less accurate. This study believes that using sentences as inference nodes is more reasonable than using documents or entities. To explain the problem, you can see an example in Figure 19, as it is clear, with using documents, although the path is more complete, there is a lot of unrelated information, which can produce redundancy. On the other hand, with using entities, although the path is concise it is too subtle and limited, and much information will be lost. However, if the path uses sentences as nodes, it is not limited anymore and with less information redundancy. Besides a sentence-based path can explain the reasoning process better.

(p16.4) Then they proposed a Sentence-based Circular Reasoning (SCR) approach, and it consists of three modules: Sentence Encoder (SE) to obtain the sentence representation, Path Generator (PG) to iteratively infer among sentences of multiple documents according to the question, and Path Evaluator (PE) to evaluate the obtained path and predicts an answer ( Figure 20). Unlike the decomposition technique, this technique focuses on multi-hop reasoning instead of simplifying the multi-hop MRC problem and has achieved great attention among studies. The main disadvantage of this technique is that they are expensive and time-consuming approaches, and require a large amount of data to train.","[['b35'], ['b36'], ['b37', 'b36'], ['b37', 'b34'], []]","[['b35'], ['b36'], ['b37', 'b36'], ['b37', 'b34'], []]",6,"1. (zoo lake, South Africa, gauteng) Passage1: ...
2. Zoo Lake is a popular lake and public park in Johannesburg, South Africa.
3. It is ... Passage2: ... aka The Reef, is a 56-kilometre -long north -facing scarp in the Gauteng Province of South Africa.
4. It consists of a ...  [36] focused on the inherent sequential of the multi-hop MRC, which means the system must decide where to look next, based on the current state.
5. Then they proposed a model named Deep Reinforcement Learning-based where the knowledge extraction phase is explicitly decoupled from the question answering phase.
6. This model uses the current information in the knowledge chain to inform which information should be achieved in the next step.
7. The model consists of two main components:1) a Linker to construct a sentence-level chain form the sentences of supporting documents that allow movement between documents, and 2) an Extractor which learns where to look based on the current question and already-visited sentences.
8. As Figure 16 shows, after embedding sentences into a matrix and constructing a graph, the policy is run to select the next sentence.
9. In this regard, the convolution-based policy network is used to learn a traversal policy from current options, previously accepted sentences, and the current question.
10. For example, in this figure, the policy has learned to select the correct next sentence (ct) from the previous sentence (ct-1).
11. After this action, the answer sentence (o1) can be selected during the next step.
12. SMR: Huo, Ge and Zhao [37] proposed a Sentence-level Multi-hop Reasoning approach named SMR while most existing approaches only use document-level or entity-level inferences.
13. In this regard, an initial sentence is first found based on the main entity in the question, and that sentence is then used to find more relevant sentences to create multiple sentence-based reasoning chains as a memory network.
14. Besides, some sentences are concatenated to prevent the mistakes of Co-Reference resolution methods and to reduce the number of hops.
15. An example of such a concatenation is shown in Figure 17.
16. There are two phases in this model: 1) the Selecting phase to select the most relevant sentence to the network memory state as the initial sentence of the current hop, and 2) the Establishing phase to prepare to go to the next hop by updating the network memory state.
17. Finally, the information of the reasoning chains is used to predict the final answer.
18. Figure 17: Representation enrichment via concatenation of adjacent sentences [37] ChainEx: Chen, Lin and Durrett [38] proposed a sentence-based model that does not rely on gold annotated chains or supporting facts at the training and test phases, instead, pseudo-gold reasoning chains are derived using some heuristics based on named entity recognition and coreference resolution during the training time, and it learns to extract chains from raw texts at the test time.
19. They first extract a reasoning chain over the text for a multi-hop reasoning task, and then apply a BERT-based QA system to find the answer by learning from these chains.
20. To construct the reasoning chain, each sentence is considered as a node in the chain, and there is an edge between two sentences if they have the same entity.
21. Besides, there are edges between all sentences from the same paragraph.
22. The model starts from the question and finds all possible reasoning chains.
23. The chain extractor is a neural network with two main components: Sentence Encoding and Chain Prediction.
24. In the sentence encoding component, the BERT-Para model provides a representation from each paragraph jointly with the question.
25. In the chain prediction component, an LSTM-based pointer network is used to extract the reasoning chain.
26. The output of the chain extractor is a variable-length sequence of sentences.
27. Finally, a BERT-based QA system is applied to the extracted chains to find the final answer.
28. Figure 18 show an example with two possible ""reasoning chains"".
29. The first chain is most appropriate, while the second requires a less well-supported inferential leap.
30. Figure 18: A multi-hop example in ChainEx [38]
31. SCR: Huo and Zhao [35] proposed another sentence-level model, since using entity and document makes the chain too coarse and ambiguous or too subtle, limited, and less accurate.
32. This study believes that using sentences as inference nodes is more reasonable than using documents or entities.
33. To explain the problem, you can see an example in Figure 19, as it is clear, with using documents, although the path is more complete, there is a lot of unrelated information, which can produce redundancy.
34. On the other hand, with using entities, although the path is concise it is too subtle and limited, and much information will be lost.
35. However, if the path uses sentences as nodes, it is not limited anymore and with less information redundancy.
36. Besides a sentence-based path can explain the reasoning process better.
37. Then they proposed a Sentence-based Circular Reasoning [36]0 approach, and it consists of three modules: Sentence Encoder [36]1 to obtain the sentence representation, Path Generator [36]2 to iteratively infer among sentences of multiple documents according to the question, and Path Evaluator [36]3 to evaluate the obtained path and predicts an answer [36]4.
38. Unlike the decomposition technique, this technique focuses on multi-hop reasoning instead of simplifying the multi-hop MRC problem and has achieved great attention among studies.
39. The main disadvantage of this technique is that they are expensive and time-consuming approaches, and require a large amount of data to train.","How does the Deep Reinforcement Learning-based model in multi-hop MRC use current information to inform next-step knowledge extraction?
What advantages does the Sentence-level Multi-hop Reasoning (SMR) approach offer over document-level or entity-level inferences in multi-hop MRC tasks?
How does the ChainEx model derive pseudo-gold reasoning chains for multi-hop reasoning tasks without relying on gold annotated chains or supporting facts?
In what way does the Sentence-based Circular Reasoning (SCR) approach improve the reasoning process in multi-hop MRC by using sentences as inference nodes?
What are the main challenges associated with the Sentence-based Circular Reasoning (SCR) technique in multi-hop Machine Reading Comprehension?","1. How does the Deep Reinforcement Learning-based model in multi-hop MRC use current information to inform next-step knowledge extraction?
2. What advantages does the Sentence-level Multi-hop Reasoning (SMR) approach offer over document-level or entity-level inferences in multi-hop MRC tasks?
3. How does the ChainEx model derive pseudo-gold reasoning chains for multi-hop reasoning tasks without relying on gold annotated chains or supporting facts?
4. In what way does the Sentence-based Circular Reasoning (SCR) approach improve the reasoning process in multi-hop MRC by using sentences as inference nodes?
5. What are the main challenges associated with the Sentence-based Circular Reasoning (SCR) technique in multi-hop Machine Reading Comprehension?",None,,"Questions:

1. How does the Deep Reinforcement Learning-based model in multi-hop MRC use current information to inform next-step knowledge extraction?
2. What advantages does the Sentence-level Multi-hop Reasoning (SMR) approach offer over document-level or entity-level inferences in multi-hop MRC tasks?
3. How does the ChainEx model derive pseudo-gold reasoning chains for multi-hop reasoning tasks without relying on gold annotated chains or supporting facts?
4. In what way does the Sentence-based Circular Reasoning (SCR) approach improve the reasoning process in multi-hop MRC by using sentences as inference nodes?
5. What are the main challenges associated with the Sentence-based Circular Reasoning (SCR) technique in multi-hop Machine Reading Comprehension?

Answer:

(p16.0) (zoo lake, South Africa, gauteng) Passage1: ... Zoo Lake is a popular lake and public park in Johannesburg, South Africa. It is ... Passage2: ... aka The Reef, is a 56-kilometre -long north -facing scarp in the Gauteng Province of South Africa. It consists of a ...  [36] focused on the inherent sequential of the multi-hop MRC, which means the system must decide where to look next, based on the current state. Then they proposed a model named Deep Reinforcement Learning-based where the knowledge extraction phase is explicitly decoupled from the question answering phase. This model uses the current information in the knowledge chain to inform which information should be achieved in the next step. The model consists of two main components:1) a Linker to construct a sentence-level chain form the sentences of supporting documents that allow movement between documents, and 2) an Extractor which learns where to look based on the current question and already-visited sentences.

(p16.1) As Figure 16 shows, after embedding sentences into a matrix and constructing a graph, the policy is run to select the next sentence. In this regard, the convolution-based policy network is used to learn a traversal policy from current options, previously accepted sentences, and the current question. For example, in this figure, the policy has learned to select the correct next sentence (ct) from the previous sentence (ct-1). After this action, the answer sentence (o1) can be selected during the next step. SMR: Huo, Ge and Zhao [37] proposed a Sentence-level Multi-hop Reasoning approach named SMR while most existing approaches only use document-level or entity-level inferences. In this regard, an initial sentence is first found based on the main entity in the question, and that sentence is then used to find more relevant sentences to create multiple sentence-based reasoning chains as a memory network. Besides, some sentences are concatenated to prevent the mistakes of Co-Reference resolution methods and to reduce the number of hops. An example of such a concatenation is shown in Figure 17.

(p16.2) There are two phases in this model: 1) the Selecting phase to select the most relevant sentence to the network memory state as the initial sentence of the current hop, and 2) the Establishing phase to prepare to go to the next hop by updating the network memory state. Finally, the information of the reasoning chains is used to predict the final answer. Figure 17: Representation enrichment via concatenation of adjacent sentences [37] ChainEx: Chen, Lin and Durrett [38] proposed a sentence-based model that does not rely on gold annotated chains or supporting facts at the training and test phases, instead, pseudo-gold reasoning chains are derived using some heuristics based on named entity recognition and coreference resolution during the training time, and it learns to extract chains from raw texts at the test time. They first extract a reasoning chain over the text for a multi-hop reasoning task, and then apply a BERT-based QA system to find the answer by learning from these chains.

(p16.3) To construct the reasoning chain, each sentence is considered as a node in the chain, and there is an edge between two sentences if they have the same entity. Besides, there are edges between all sentences from the same paragraph. The model starts from the question and finds all possible reasoning chains. The chain extractor is a neural network with two main components: Sentence Encoding and Chain Prediction. In the sentence encoding component, the BERT-Para model provides a representation from each paragraph jointly with the question. In the chain prediction component, an LSTM-based pointer network is used to extract the reasoning chain. The output of the chain extractor is a variable-length sequence of sentences. Finally, a BERT-based QA system is applied to the extracted chains to find the final answer. Figure 18 show an example with two possible ""reasoning chains"". The first chain is most appropriate, while the second requires a less well-supported inferential leap. Figure 18: A multi-hop example in ChainEx [38] SCR: Huo and Zhao [35] proposed another sentence-level model, since using entity and document makes the chain too coarse and ambiguous or too subtle, limited, and less accurate. This study believes that using sentences as inference nodes is more reasonable than using documents or entities. To explain the problem, you can see an example in Figure 19, as it is clear, with using documents, although the path is more complete, there is a lot of unrelated information, which can produce redundancy. On the other hand, with using entities, although the path is concise it is too subtle and limited, and much information will be lost. However, if the path uses sentences as nodes, it is not limited anymore and with less information redundancy. Besides a sentence-based path can explain the reasoning process better.

(p16.4) Then they proposed a Sentence-based Circular Reasoning (SCR) approach, and it consists of three modules: Sentence Encoder (SE) to obtain the sentence representation, Path Generator (PG) to iteratively infer among sentences of multiple documents according to the question, and Path Evaluator (PE) to evaluate the obtained path and predicts an answer ( Figure 20). Unlike the decomposition technique, this technique focuses on multi-hop reasoning instead of simplifying the multi-hop MRC problem and has achieved great attention among studies. The main disadvantage of this technique is that they are expensive and time-consuming approaches, and require a large amount of data to train.","Questions:



Answer:

(p16.0) (zoo lake, South Africa, gauteng) Passage1: ... Zoo Lake is a popular lake and public park in Johannesburg, South Africa. It is ... Passage2: ... aka The Reef, is a 56-kilometre -long north -facing scarp in the Gauteng Province of South Africa. It consists of a ...  [36] focused on the inherent sequential of the multi-hop MRC, which means the system must decide where to look next, based on the current state. Then they proposed a model named Deep Reinforcement Learning-based where the knowledge extraction phase is explicitly decoupled from the question answering phase. This model uses the current information in the knowledge chain to inform which information should be achieved in the next step. The model consists of two main components:1) a Linker to construct a sentence-level chain form the sentences of supporting documents that allow movement between documents, and 2) an Extractor which learns where to look based on the current question and already-visited sentences.

(p16.1) As Figure 16 shows, after embedding sentences into a matrix and constructing a graph, the policy is run to select the next sentence. In this regard, the convolution-based policy network is used to learn a traversal policy from current options, previously accepted sentences, and the current question. For example, in this figure, the policy has learned to select the correct next sentence (ct) from the previous sentence (ct-1). After this action, the answer sentence (o1) can be selected during the next step. SMR: Huo, Ge and Zhao [37] proposed a Sentence-level Multi-hop Reasoning approach named SMR while most existing approaches only use document-level or entity-level inferences. In this regard, an initial sentence is first found based on the main entity in the question, and that sentence is then used to find more relevant sentences to create multiple sentence-based reasoning chains as a memory network. Besides, some sentences are concatenated to prevent the mistakes of Co-Reference resolution methods and to reduce the number of hops. An example of such a concatenation is shown in Figure 17.

(p16.2) There are two phases in this model: 1) the Selecting phase to select the most relevant sentence to the network memory state as the initial sentence of the current hop, and 2) the Establishing phase to prepare to go to the next hop by updating the network memory state. Finally, the information of the reasoning chains is used to predict the final answer. Figure 17: Representation enrichment via concatenation of adjacent sentences [37] ChainEx: Chen, Lin and Durrett [38] proposed a sentence-based model that does not rely on gold annotated chains or supporting facts at the training and test phases, instead, pseudo-gold reasoning chains are derived using some heuristics based on named entity recognition and coreference resolution during the training time, and it learns to extract chains from raw texts at the test time. They first extract a reasoning chain over the text for a multi-hop reasoning task, and then apply a BERT-based QA system to find the answer by learning from these chains.

(p16.3) To construct the reasoning chain, each sentence is considered as a node in the chain, and there is an edge between two sentences if they have the same entity. Besides, there are edges between all sentences from the same paragraph. The model starts from the question and finds all possible reasoning chains. The chain extractor is a neural network with two main components: Sentence Encoding and Chain Prediction. In the sentence encoding component, the BERT-Para model provides a representation from each paragraph jointly with the question. In the chain prediction component, an LSTM-based pointer network is used to extract the reasoning chain. The output of the chain extractor is a variable-length sequence of sentences. Finally, a BERT-based QA system is applied to the extracted chains to find the final answer. Figure 18 show an example with two possible ""reasoning chains"". The first chain is most appropriate, while the second requires a less well-supported inferential leap. Figure 18: A multi-hop example in ChainEx [38] SCR: Huo and Zhao [35] proposed another sentence-level model, since using entity and document makes the chain too coarse and ambiguous or too subtle, limited, and less accurate. This study believes that using sentences as inference nodes is more reasonable than using documents or entities. To explain the problem, you can see an example in Figure 19, as it is clear, with using documents, although the path is more complete, there is a lot of unrelated information, which can produce redundancy. On the other hand, with using entities, although the path is concise it is too subtle and limited, and much information will be lost. However, if the path uses sentences as nodes, it is not limited anymore and with less information redundancy. Besides a sentence-based path can explain the reasoning process better.

(p16.4) Then they proposed a Sentence-based Circular Reasoning (SCR) approach, and it consists of three modules: Sentence Encoder (SE) to obtain the sentence representation, Path Generator (PG) to iteratively infer among sentences of multiple documents according to the question, and Path Evaluator (PE) to evaluate the obtained path and predicts an answer ( Figure 20). Unlike the decomposition technique, this technique focuses on multi-hop reasoning instead of simplifying the multi-hop MRC problem and has achieved great attention among studies. The main disadvantage of this technique is that they are expensive and time-consuming approaches, and require a large amount of data to train."
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s20,•,"['p20.0', 'p20.1', 'p20.2', 'p20.3', 'p20.4', 'p20.5', 'p20.6', 'p20.7', 'p20.8']","['Passing information from tokens to entities by computing entity embeddings from tokens (Doc2Graph flow).', '• Propagating information over the entity graph.', '• Passing information from the entity graph to document tokens, as the final prediction is on tokens (Graph2Doc flow). However, this model still only implicitly combines knowledge from all passages, and are therefore unable to provide explicit reasoning paths [28]. BAG: Cao, Fang, and Tao [43] proposed a Bi-directional Attention Entity Graph Convolutional Network (BAG) with a focus on Relational Graph Convolutional Network (RGCN) to realize multi-hop reasoning by message propagating across different entity nodes in graphs and generating transformed representations of original ones. The R-GCN is employed to handle high-relational data characteristics and make use of different edge types. It uses graph neural networks to obtain the relationship between entities, or add a self-attention mechanism into the model, so as to obtain a gain in the result. They first construct an entity graph and apply graph convolutional networks to obtain a relation-aware representation of nodes. Then, the bidirectional attention mechanism is used to generate the representation of query-aware nodes. As it is shown in Figure 25, the model has five layers, Entity Graph Construction, Multi-level Features, GCN, Bi-directional Attention, and Output. However, as the number of inferences increases, the complexity of models will rise sharply due to the iteration of the message passing algorithm, resulting in low efficiency [35]. Figure 25: Architecture of BAG [43] CogQA: Ding et al. [44] to mimic the process of the human brain, proposed the CogQA framework that builds a cognitive graph inspired by dual-process theory. This theory approves that the human brain first retrieves relevant information implicitly and unconsciously, and finally an explicit and conscious reasoning process is applied to that relevant information. The cognitive graph structure in this framework can offer ordered and entity-level explainability and suits relational reasoning. Based on this theory, the proposed model has two components:', '• Implicit Extraction (System 1) in which the relevant information, like question-relevant entities, and candidates answer, are extracted from paragraphs. Then, the extracted relevant entities are used to construct a cognitive graph.', 'Such a graph can be seen as working memory.', '• Explicit Reasoning (System 2) in which the reasoning procedure is applied to the graph to guide System 1 to extract the next-hop entities. The main part of System 1 is BERT, and the main part of System 2 is GNN.', ""As it is shown in Figure 26, System 1 generates new hop and answer nodes based on clues[x; G] discovered by System 2, where G is the cognitive graph, and clues[x; G] comprises sentences in paragraphs of x's predecessor nodes from which x is extracted. Figure 26: Architecture of CogQA [44] DRNQA: Li et al. [45] proposed a Dynamic Reasoning Network (DRN) approach that, unlike other models that read the question only once, uses a query reshaping mechanism that considers the question several times, which as a result enhances the ability of the reasoning over the entity graph. There are two main parts:"", 'The entity graph construction component to construct the entity graph ( Figure 27). This graph is constructed from three levels: 1) Question-based level: there is an edge between two entities if their sentences have a common entity or phrase with the question, 2) Context-based level: there is an edge between two entities if they are from the same passages, and 3) Passage-based level: there is an edge between two entities if their sentences have at least one entity or phrase in common.', 'The dynamic reasoning network component ( Figure 28) to reason over the entity graph. The query reshaping mechanism ( Figure  29) causes the important parts of the question to be read frequently. In this mechanism, a weight is given to different parts of the question repeatedly. In this regard, a Graph Neural Network (GNN) and a Dynamic Graph Attention (GAT) are used in this component. As you can see in Figure 64. ']","Passing information from tokens to entities by computing entity embeddings from tokens (Doc2Graph flow).

• Propagating information over the entity graph.

• Passing information from the entity graph to document tokens, as the final prediction is on tokens (Graph2Doc flow). However, this model still only implicitly combines knowledge from all passages, and are therefore unable to provide explicit reasoning paths [28]. BAG: Cao, Fang, and Tao [43] proposed a Bi-directional Attention Entity Graph Convolutional Network (BAG) with a focus on Relational Graph Convolutional Network (RGCN) to realize multi-hop reasoning by message propagating across different entity nodes in graphs and generating transformed representations of original ones. The R-GCN is employed to handle high-relational data characteristics and make use of different edge types. It uses graph neural networks to obtain the relationship between entities, or add a self-attention mechanism into the model, so as to obtain a gain in the result. They first construct an entity graph and apply graph convolutional networks to obtain a relation-aware representation of nodes. Then, the bidirectional attention mechanism is used to generate the representation of query-aware nodes. As it is shown in Figure 25, the model has five layers, Entity Graph Construction, Multi-level Features, GCN, Bi-directional Attention, and Output. However, as the number of inferences increases, the complexity of models will rise sharply due to the iteration of the message passing algorithm, resulting in low efficiency [35]. Figure 25: Architecture of BAG [43] CogQA: Ding et al. [44] to mimic the process of the human brain, proposed the CogQA framework that builds a cognitive graph inspired by dual-process theory. This theory approves that the human brain first retrieves relevant information implicitly and unconsciously, and finally an explicit and conscious reasoning process is applied to that relevant information. The cognitive graph structure in this framework can offer ordered and entity-level explainability and suits relational reasoning. Based on this theory, the proposed model has two components:

• Implicit Extraction (System 1) in which the relevant information, like question-relevant entities, and candidates answer, are extracted from paragraphs. Then, the extracted relevant entities are used to construct a cognitive graph.

Such a graph can be seen as working memory.

• Explicit Reasoning (System 2) in which the reasoning procedure is applied to the graph to guide System 1 to extract the next-hop entities. The main part of System 1 is BERT, and the main part of System 2 is GNN.

As it is shown in Figure 26, System 1 generates new hop and answer nodes based on clues[x; G] discovered by System 2, where G is the cognitive graph, and clues[x; G] comprises sentences in paragraphs of x's predecessor nodes from which x is extracted. Figure 26: Architecture of CogQA [44] DRNQA: Li et al. [45] proposed a Dynamic Reasoning Network (DRN) approach that, unlike other models that read the question only once, uses a query reshaping mechanism that considers the question several times, which as a result enhances the ability of the reasoning over the entity graph. There are two main parts:

The entity graph construction component to construct the entity graph ( Figure 27). This graph is constructed from three levels: 1) Question-based level: there is an edge between two entities if their sentences have a common entity or phrase with the question, 2) Context-based level: there is an edge between two entities if they are from the same passages, and 3) Passage-based level: there is an edge between two entities if their sentences have at least one entity or phrase in common.

The dynamic reasoning network component ( Figure 28) to reason over the entity graph. The query reshaping mechanism ( Figure  29) causes the important parts of the question to be read frequently. In this mechanism, a weight is given to different parts of the question repeatedly. In this regard, a Graph Neural Network (GNN) and a Dynamic Graph Attention (GAT) are used in this component. As you can see in Figure 64. ","(p20.0) Passing information from tokens to entities by computing entity embeddings from tokens (Doc2Graph flow).

(p20.1) • Propagating information over the entity graph.

(p20.2) • Passing information from the entity graph to document tokens, as the final prediction is on tokens (Graph2Doc flow). However, this model still only implicitly combines knowledge from all passages, and are therefore unable to provide explicit reasoning paths [28]. BAG: Cao, Fang, and Tao [43] proposed a Bi-directional Attention Entity Graph Convolutional Network (BAG) with a focus on Relational Graph Convolutional Network (RGCN) to realize multi-hop reasoning by message propagating across different entity nodes in graphs and generating transformed representations of original ones. The R-GCN is employed to handle high-relational data characteristics and make use of different edge types. It uses graph neural networks to obtain the relationship between entities, or add a self-attention mechanism into the model, so as to obtain a gain in the result. They first construct an entity graph and apply graph convolutional networks to obtain a relation-aware representation of nodes. Then, the bidirectional attention mechanism is used to generate the representation of query-aware nodes. As it is shown in Figure 25, the model has five layers, Entity Graph Construction, Multi-level Features, GCN, Bi-directional Attention, and Output. However, as the number of inferences increases, the complexity of models will rise sharply due to the iteration of the message passing algorithm, resulting in low efficiency [35]. Figure 25: Architecture of BAG [43] CogQA: Ding et al. [44] to mimic the process of the human brain, proposed the CogQA framework that builds a cognitive graph inspired by dual-process theory. This theory approves that the human brain first retrieves relevant information implicitly and unconsciously, and finally an explicit and conscious reasoning process is applied to that relevant information. The cognitive graph structure in this framework can offer ordered and entity-level explainability and suits relational reasoning. Based on this theory, the proposed model has two components:

(p20.3) • Implicit Extraction (System 1) in which the relevant information, like question-relevant entities, and candidates answer, are extracted from paragraphs. Then, the extracted relevant entities are used to construct a cognitive graph.

(p20.4) Such a graph can be seen as working memory.

(p20.5) • Explicit Reasoning (System 2) in which the reasoning procedure is applied to the graph to guide System 1 to extract the next-hop entities. The main part of System 1 is BERT, and the main part of System 2 is GNN.

(p20.6) As it is shown in Figure 26, System 1 generates new hop and answer nodes based on clues[x; G] discovered by System 2, where G is the cognitive graph, and clues[x; G] comprises sentences in paragraphs of x's predecessor nodes from which x is extracted. Figure 26: Architecture of CogQA [44] DRNQA: Li et al. [45] proposed a Dynamic Reasoning Network (DRN) approach that, unlike other models that read the question only once, uses a query reshaping mechanism that considers the question several times, which as a result enhances the ability of the reasoning over the entity graph. There are two main parts:

(p20.7) The entity graph construction component to construct the entity graph ( Figure 27). This graph is constructed from three levels: 1) Question-based level: there is an edge between two entities if their sentences have a common entity or phrase with the question, 2) Context-based level: there is an edge between two entities if they are from the same passages, and 3) Passage-based level: there is an edge between two entities if their sentences have at least one entity or phrase in common.

(p20.8) The dynamic reasoning network component ( Figure 28) to reason over the entity graph. The query reshaping mechanism ( Figure  29) causes the important parts of the question to be read frequently. In this mechanism, a weight is given to different parts of the question repeatedly. In this regard, a Graph Neural Network (GNN) and a Dynamic Graph Attention (GAT) are used in this component. As you can see in Figure 64. ","[[], [], ['b42', 'b43', 'b34', 'b27'], [], [], [], ['b43', 'b44'], [], []]","[[], [], ['b42', 'b43', 'b34', 'b27'], [], [], [], ['b43', 'b44'], [], []]",6,"1. Passing information from tokens to entities by computing entity embeddings from tokens (Doc2Graph flow). Propagating information over the entity graph.
2. Passing information from the entity graph to document tokens, as the final prediction is on tokens (Graph2Doc flow).
3. However, this model still only implicitly combines knowledge from all passages, and are therefore unable to provide explicit reasoning paths [28].
4. BAG: Cao, Fang, and Tao [43] proposed a Bi-directional Attention Entity Graph Convolutional Network (BAG) with a focus on Relational Graph Convolutional Network (RGCN) to realize multi-hop reasoning by message propagating across different entity nodes in graphs and generating transformed representations of original ones.
5. The R-GCN is employed to handle high-relational data characteristics and make use of different edge types.
6. It uses graph neural networks to obtain the relationship between entities, or add a self-attention mechanism into the model, so as to obtain a gain in the result.
7. They first construct an entity graph and apply graph convolutional networks to obtain a relation-aware representation of nodes.
8. Then, the bidirectional attention mechanism is used to generate the representation of query-aware nodes.
9. As it is shown in Figure 25, the model has five layers, Entity Graph Construction, Multi-level Features, GCN, Bi-directional Attention, and Output.
10. However, as the number of inferences increases, the complexity of models will rise sharply due to the iteration of the message passing algorithm, resulting in low efficiency [35].
11. Figure 25: Architecture of BAG [43] CogQA: Ding et al. (Graph2Doc flow)3 to mimic the process of the human brain, proposed the CogQA framework that builds a cognitive graph inspired by dual-process theory.
12. This theory approves that the human brain first retrieves relevant information implicitly and unconsciously, and finally an explicit and conscious reasoning process is applied to that relevant information.
13. The cognitive graph structure in this framework can offer ordered and entity-level explainability and suits relational reasoning.
14. Based on this theory, the proposed model has two components: Implicit Extraction (System 1) in which the relevant information, like question-relevant entities, and candidates answer, are extracted from paragraphs.
15. Then, the extracted relevant entities are used to construct a cognitive graph.
16. Such a graph can be seen as working memory.
17. Explicit Reasoning (Graph2Doc flow)0 in which the reasoning procedure is applied to the graph to guide System 1 to extract the next-hop entities.
18. The main part of System 1 is BERT, and the main part of System 2 is GNN.
19. As it is shown in Figure 26, System 1 generates new hop and answer nodes based on clues(Graph2Doc flow)2 discovered by System 2, where G is the cognitive graph, and clues(Graph2Doc flow)2 comprises sentences in paragraphs of x's predecessor nodes from which x is extracted.
20. Figure 26: Architecture of CogQA (Graph2Doc flow)3 DRNQA: Li et al. (Graph2Doc flow)4 proposed a Dynamic Reasoning Network (Graph2Doc flow)5 approach that, unlike other models that read the question only once, uses a query reshaping mechanism that considers the question several times, which as a result enhances the ability of the reasoning over the entity graph.
21. There are two main parts:The entity graph construction component to construct the entity graph (Graph2Doc flow)6.
22. This graph is constructed from three levels: 1) Question-based level: there is an edge between two entities if their sentences have a common entity or phrase with the question, 2) Context-based level: there is an edge between two entities if they are from the same passages, and 3) Passage-based level: there is an edge between two entities if their sentences have at least one entity or phrase in common.
23. The dynamic reasoning network component (Graph2Doc flow)7 to reason over the entity graph.
24. The query reshaping mechanism (Graph2Doc flow)8 causes the important parts of the question to be read frequently.
25. In this mechanism, a weight is given to different parts of the question repeatedly.
26. In this regard, a Graph Neural Network (Graph2Doc flow)9 and a Dynamic Graph Attention [28]0 are used in this component.
27. As you can see in Figure 64.","How does the Bi-directional Attention Entity Graph Convolutional Network (BAG) utilize the Relational Graph Convolutional Network (RGCN) for multi-hop reasoning?
What role does the bidirectional attention mechanism play in the BAG model for generating query-aware node representations?
How does the CogQA framework implement dual-process theory in its approach to machine reading comprehension?
In the context of the Dynamic Reasoning Network (DRN) approach, how does the query reshaping mechanism enhance reasoning over the entity graph?
What are the three levels of entity graph construction in the DRNQA model, and how do they contribute to the model's understanding of the question context?","1. How does the Bi-directional Attention Entity Graph Convolutional Network (BAG) utilize the Relational Graph Convolutional Network (RGCN) for multi-hop reasoning?
2. What role does the bidirectional attention mechanism play in the BAG model for generating query-aware node representations?
3. How does the CogQA framework implement dual-process theory in its approach to machine reading comprehension?
4. In the context of the Dynamic Reasoning Network (DRN) approach, how does the query reshaping mechanism enhance reasoning over the entity graph?
5. What are the three levels of entity graph construction in the DRNQA model, and how do they contribute to the model's understanding of the question context?",None,,"Questions:

1. How does the Bi-directional Attention Entity Graph Convolutional Network (BAG) utilize the Relational Graph Convolutional Network (RGCN) for multi-hop reasoning?
2. What role does the bidirectional attention mechanism play in the BAG model for generating query-aware node representations?
3. How does the CogQA framework implement dual-process theory in its approach to machine reading comprehension?
4. In the context of the Dynamic Reasoning Network (DRN) approach, how does the query reshaping mechanism enhance reasoning over the entity graph?
5. What are the three levels of entity graph construction in the DRNQA model, and how do they contribute to the model's understanding of the question context?

Answer:

(p20.0) Passing information from tokens to entities by computing entity embeddings from tokens (Doc2Graph flow).

(p20.1) • Propagating information over the entity graph.

(p20.2) • Passing information from the entity graph to document tokens, as the final prediction is on tokens (Graph2Doc flow). However, this model still only implicitly combines knowledge from all passages, and are therefore unable to provide explicit reasoning paths [28]. BAG: Cao, Fang, and Tao [43] proposed a Bi-directional Attention Entity Graph Convolutional Network (BAG) with a focus on Relational Graph Convolutional Network (RGCN) to realize multi-hop reasoning by message propagating across different entity nodes in graphs and generating transformed representations of original ones. The R-GCN is employed to handle high-relational data characteristics and make use of different edge types. It uses graph neural networks to obtain the relationship between entities, or add a self-attention mechanism into the model, so as to obtain a gain in the result. They first construct an entity graph and apply graph convolutional networks to obtain a relation-aware representation of nodes. Then, the bidirectional attention mechanism is used to generate the representation of query-aware nodes. As it is shown in Figure 25, the model has five layers, Entity Graph Construction, Multi-level Features, GCN, Bi-directional Attention, and Output. However, as the number of inferences increases, the complexity of models will rise sharply due to the iteration of the message passing algorithm, resulting in low efficiency [35]. Figure 25: Architecture of BAG [43] CogQA: Ding et al. [44] to mimic the process of the human brain, proposed the CogQA framework that builds a cognitive graph inspired by dual-process theory. This theory approves that the human brain first retrieves relevant information implicitly and unconsciously, and finally an explicit and conscious reasoning process is applied to that relevant information. The cognitive graph structure in this framework can offer ordered and entity-level explainability and suits relational reasoning. Based on this theory, the proposed model has two components:

(p20.3) • Implicit Extraction (System 1) in which the relevant information, like question-relevant entities, and candidates answer, are extracted from paragraphs. Then, the extracted relevant entities are used to construct a cognitive graph.

(p20.4) Such a graph can be seen as working memory.

(p20.5) • Explicit Reasoning (System 2) in which the reasoning procedure is applied to the graph to guide System 1 to extract the next-hop entities. The main part of System 1 is BERT, and the main part of System 2 is GNN.

(p20.6) As it is shown in Figure 26, System 1 generates new hop and answer nodes based on clues[x; G] discovered by System 2, where G is the cognitive graph, and clues[x; G] comprises sentences in paragraphs of x's predecessor nodes from which x is extracted. Figure 26: Architecture of CogQA [44] DRNQA: Li et al. [45] proposed a Dynamic Reasoning Network (DRN) approach that, unlike other models that read the question only once, uses a query reshaping mechanism that considers the question several times, which as a result enhances the ability of the reasoning over the entity graph. There are two main parts:

(p20.7) The entity graph construction component to construct the entity graph ( Figure 27). This graph is constructed from three levels: 1) Question-based level: there is an edge between two entities if their sentences have a common entity or phrase with the question, 2) Context-based level: there is an edge between two entities if they are from the same passages, and 3) Passage-based level: there is an edge between two entities if their sentences have at least one entity or phrase in common.

(p20.8) The dynamic reasoning network component ( Figure 28) to reason over the entity graph. The query reshaping mechanism ( Figure  29) causes the important parts of the question to be read frequently. In this mechanism, a weight is given to different parts of the question repeatedly. In this regard, a Graph Neural Network (GNN) and a Dynamic Graph Attention (GAT) are used in this component. As you can see in Figure 64. ","Questions:



Answer:

(p20.0) Passing information from tokens to entities by computing entity embeddings from tokens (Doc2Graph flow).

(p20.1) • Propagating information over the entity graph.

(p20.2) • Passing information from the entity graph to document tokens, as the final prediction is on tokens (Graph2Doc flow). However, this model still only implicitly combines knowledge from all passages, and are therefore unable to provide explicit reasoning paths [28]. BAG: Cao, Fang, and Tao [43] proposed a Bi-directional Attention Entity Graph Convolutional Network (BAG) with a focus on Relational Graph Convolutional Network (RGCN) to realize multi-hop reasoning by message propagating across different entity nodes in graphs and generating transformed representations of original ones. The R-GCN is employed to handle high-relational data characteristics and make use of different edge types. It uses graph neural networks to obtain the relationship between entities, or add a self-attention mechanism into the model, so as to obtain a gain in the result. They first construct an entity graph and apply graph convolutional networks to obtain a relation-aware representation of nodes. Then, the bidirectional attention mechanism is used to generate the representation of query-aware nodes. As it is shown in Figure 25, the model has five layers, Entity Graph Construction, Multi-level Features, GCN, Bi-directional Attention, and Output. However, as the number of inferences increases, the complexity of models will rise sharply due to the iteration of the message passing algorithm, resulting in low efficiency [35]. Figure 25: Architecture of BAG [43] CogQA: Ding et al. [44] to mimic the process of the human brain, proposed the CogQA framework that builds a cognitive graph inspired by dual-process theory. This theory approves that the human brain first retrieves relevant information implicitly and unconsciously, and finally an explicit and conscious reasoning process is applied to that relevant information. The cognitive graph structure in this framework can offer ordered and entity-level explainability and suits relational reasoning. Based on this theory, the proposed model has two components:

(p20.3) • Implicit Extraction (System 1) in which the relevant information, like question-relevant entities, and candidates answer, are extracted from paragraphs. Then, the extracted relevant entities are used to construct a cognitive graph.

(p20.4) Such a graph can be seen as working memory.

(p20.5) • Explicit Reasoning (System 2) in which the reasoning procedure is applied to the graph to guide System 1 to extract the next-hop entities. The main part of System 1 is BERT, and the main part of System 2 is GNN.

(p20.6) As it is shown in Figure 26, System 1 generates new hop and answer nodes based on clues[x; G] discovered by System 2, where G is the cognitive graph, and clues[x; G] comprises sentences in paragraphs of x's predecessor nodes from which x is extracted. Figure 26: Architecture of CogQA [44] DRNQA: Li et al. [45] proposed a Dynamic Reasoning Network (DRN) approach that, unlike other models that read the question only once, uses a query reshaping mechanism that considers the question several times, which as a result enhances the ability of the reasoning over the entity graph. There are two main parts:

(p20.7) The entity graph construction component to construct the entity graph ( Figure 27). This graph is constructed from three levels: 1) Question-based level: there is an edge between two entities if their sentences have a common entity or phrase with the question, 2) Context-based level: there is an edge between two entities if they are from the same passages, and 3) Passage-based level: there is an edge between two entities if their sentences have at least one entity or phrase in common.

(p20.8) The dynamic reasoning network component ( Figure 28) to reason over the entity graph. The query reshaping mechanism ( Figure  29) causes the important parts of the question to be read frequently. In this mechanism, a weight is given to different parts of the question repeatedly. In this regard, a Graph Neural Network (GNN) and a Dynamic Graph Attention (GAT) are used in this component. As you can see in Figure 64. "
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s22,HDE:,"['p22.0', 'p22.1', 'p22.2', 'p22.3', 'p22.4', 'p22.5']","['Tu et al. [46] proposed a more complex graph named Heterogeneous Document-Entity (HDE) graph with different types of nodes and edges. This graph can cover different granularity levels of information in context and also enables rich information interaction among different types of nodes for accurate reasoning. The nodes in the HDE graph are candidates, documents, and entities. Besides, it has seven types of edges: 1) between a document node and a candidate node that appears in the same document.', '2) between a document node and its entity node. 3) between a candidate node and its entity node. 4) between two entity nodes from the same document. 5) between two entity nodes from different documents but they are mentions of the same candidate or query subject. 6) All candidate nodes connect with each other. 7) Entity nodes that do not meet previous conditions are connected as well. Figure 30 is an example of an HDE graph. In this figure, green nodes indicate documents, yellow nodes denote candidates, and blue nodes stand for entities. In addition, dash lines indicate type 1 edges, dash-dotted lines denote type 2 edges, square dot lines indicate type 3 edges, the red line denotes type 4 edge, the purple line indicates type 5 edge, and black lines indicate type 6 edges. The type 7 edge is not shown in this figure. As Figure 31 shows This model can be categorized into three parts: initializing HDE graph nodes with co-attention and self-attention-based context encoding, and reasoning over HDE graph with GNN-based message passing algorithms and score accumulation from updated HDE graph nodes representations.', 'However, [35] have shown that if the number of inferences increases, the complexity of models will rise sharply due to the iteration of cumbersome message passing algorithm, resulting in low efficiency.  [46] SAE: Tu et al. [47] proposed an interpretable system named Select, Answer, and Explain (SAE) with multi-hop reasoning graphs based on GNN with contextual sentence embeddings as nodes. This kind of sentence graph makes lead to an interpretable model because it can directly output supporting sentences with the answer prediction. The edges capture the global information presented within each document and also the cross-document reasoning path. Also, the contextual sentence embedding used in GNN is summarized over token representations based on a novel mixed attentive pooling mechanism. The attention weight is calculated from both answer span logits and self-attention output on token representations. This attention-based interaction enables the exploitation of complementary information between ""answer"" and ""explain"" tasks. The SAE system first filters unrelated documents, and selects gold documents using a document classifier trained with a novel pairwise learning-to-rank loss function.', 'The gold documents are then fed into a model to predict the answer and supporting sentences. The model is a multi-task learning process which means while the answer prediction is accomplished at the token level, the support sentence is predicted as a node classification task at the sentence level. As it is shown in Figure 32, the selection module consists of:', '•', 'The Multi-Head Self-Attention (MHSA) layer to capture interaction among documents']","Tu et al. [46] proposed a more complex graph named Heterogeneous Document-Entity (HDE) graph with different types of nodes and edges. This graph can cover different granularity levels of information in context and also enables rich information interaction among different types of nodes for accurate reasoning. The nodes in the HDE graph are candidates, documents, and entities. Besides, it has seven types of edges: 1) between a document node and a candidate node that appears in the same document.

2) between a document node and its entity node. 3) between a candidate node and its entity node. 4) between two entity nodes from the same document. 5) between two entity nodes from different documents but they are mentions of the same candidate or query subject. 6) All candidate nodes connect with each other. 7) Entity nodes that do not meet previous conditions are connected as well. Figure 30 is an example of an HDE graph. In this figure, green nodes indicate documents, yellow nodes denote candidates, and blue nodes stand for entities. In addition, dash lines indicate type 1 edges, dash-dotted lines denote type 2 edges, square dot lines indicate type 3 edges, the red line denotes type 4 edge, the purple line indicates type 5 edge, and black lines indicate type 6 edges. The type 7 edge is not shown in this figure. As Figure 31 shows This model can be categorized into three parts: initializing HDE graph nodes with co-attention and self-attention-based context encoding, and reasoning over HDE graph with GNN-based message passing algorithms and score accumulation from updated HDE graph nodes representations.

However, [35] have shown that if the number of inferences increases, the complexity of models will rise sharply due to the iteration of cumbersome message passing algorithm, resulting in low efficiency.  [46] SAE: Tu et al. [47] proposed an interpretable system named Select, Answer, and Explain (SAE) with multi-hop reasoning graphs based on GNN with contextual sentence embeddings as nodes. This kind of sentence graph makes lead to an interpretable model because it can directly output supporting sentences with the answer prediction. The edges capture the global information presented within each document and also the cross-document reasoning path. Also, the contextual sentence embedding used in GNN is summarized over token representations based on a novel mixed attentive pooling mechanism. The attention weight is calculated from both answer span logits and self-attention output on token representations. This attention-based interaction enables the exploitation of complementary information between ""answer"" and ""explain"" tasks. The SAE system first filters unrelated documents, and selects gold documents using a document classifier trained with a novel pairwise learning-to-rank loss function.

The gold documents are then fed into a model to predict the answer and supporting sentences. The model is a multi-task learning process which means while the answer prediction is accomplished at the token level, the support sentence is predicted as a node classification task at the sentence level. As it is shown in Figure 32, the selection module consists of:

•

The Multi-Head Self-Attention (MHSA) layer to capture interaction among documents","(p22.0) Tu et al. [46] proposed a more complex graph named Heterogeneous Document-Entity (HDE) graph with different types of nodes and edges. This graph can cover different granularity levels of information in context and also enables rich information interaction among different types of nodes for accurate reasoning. The nodes in the HDE graph are candidates, documents, and entities. Besides, it has seven types of edges: 1) between a document node and a candidate node that appears in the same document.

(p22.1) 2) between a document node and its entity node. 3) between a candidate node and its entity node. 4) between two entity nodes from the same document. 5) between two entity nodes from different documents but they are mentions of the same candidate or query subject. 6) All candidate nodes connect with each other. 7) Entity nodes that do not meet previous conditions are connected as well. Figure 30 is an example of an HDE graph. In this figure, green nodes indicate documents, yellow nodes denote candidates, and blue nodes stand for entities. In addition, dash lines indicate type 1 edges, dash-dotted lines denote type 2 edges, square dot lines indicate type 3 edges, the red line denotes type 4 edge, the purple line indicates type 5 edge, and black lines indicate type 6 edges. The type 7 edge is not shown in this figure. As Figure 31 shows This model can be categorized into three parts: initializing HDE graph nodes with co-attention and self-attention-based context encoding, and reasoning over HDE graph with GNN-based message passing algorithms and score accumulation from updated HDE graph nodes representations.

(p22.2) However, [35] have shown that if the number of inferences increases, the complexity of models will rise sharply due to the iteration of cumbersome message passing algorithm, resulting in low efficiency.  [46] SAE: Tu et al. [47] proposed an interpretable system named Select, Answer, and Explain (SAE) with multi-hop reasoning graphs based on GNN with contextual sentence embeddings as nodes. This kind of sentence graph makes lead to an interpretable model because it can directly output supporting sentences with the answer prediction. The edges capture the global information presented within each document and also the cross-document reasoning path. Also, the contextual sentence embedding used in GNN is summarized over token representations based on a novel mixed attentive pooling mechanism. The attention weight is calculated from both answer span logits and self-attention output on token representations. This attention-based interaction enables the exploitation of complementary information between ""answer"" and ""explain"" tasks. The SAE system first filters unrelated documents, and selects gold documents using a document classifier trained with a novel pairwise learning-to-rank loss function.

(p22.3) The gold documents are then fed into a model to predict the answer and supporting sentences. The model is a multi-task learning process which means while the answer prediction is accomplished at the token level, the support sentence is predicted as a node classification task at the sentence level. As it is shown in Figure 32, the selection module consists of:

(p22.4) •

(p22.5) The Multi-Head Self-Attention (MHSA) layer to capture interaction among documents","[['b45'], [], ['b46', 'b34', 'b45'], [], [], []]","[['b45'], [], ['b46', 'b34', 'b45'], [], [], []]",4,"1. Tu et al. [46] proposed a more complex graph named Heterogeneous Document-Entity (HDE) graph with different types of nodes and edges.
2. This graph can cover different granularity levels of information in context and also enables rich information interaction among different types of nodes for accurate reasoning.
3. The nodes in the HDE graph are candidates, documents, and entities.
4. Besides, it has seven types of edges: 1) between a document node and a candidate node that appears in the same document.2) between a document node and its entity node.
5. 3) between a candidate node and its entity node.
6. 4) between two entity nodes from the same document.
7. 5) between two entity nodes from different documents but they are mentions of the same candidate or query subject.
8. 6) All candidate nodes connect with each other.
9. 7) Entity nodes that do not meet previous conditions are connected as well.
10. Figure 30 is an example of an HDE graph.
11. In this figure, green nodes indicate documents, yellow nodes denote candidates, and blue nodes stand for entities.
12. In addition, dash lines indicate type 1 edges, dash-dotted lines denote type 2 edges, square dot lines indicate type 3 edges, the red line denotes type 4 edge, the purple line indicates type 5 edge, and black lines indicate type 6 edges.
13. The type 7 edge is not shown in this figure.
14. As Figure 31 shows This model can be categorized into three parts: initializing HDE graph nodes with co-attention and self-attention-based context encoding, and reasoning over HDE graph with GNN-based message passing algorithms and score accumulation from updated HDE graph nodes representations.
15. However, [35] have shown that if the number of inferences increases, the complexity of models will rise sharply due to the iteration of cumbersome message passing algorithm, resulting in low efficiency.
16. [46] SAE: Tu et al. [47] proposed an interpretable system named Select, Answer, and Explain (SAE) with multi-hop reasoning graphs based on GNN with contextual sentence embeddings as nodes.
17. This kind of sentence graph makes lead to an interpretable model because it can directly output supporting sentences with the answer prediction.
18. The edges capture the global information presented within each document and also the cross-document reasoning path.
19. Also, the contextual sentence embedding used in GNN is summarized over token representations based on a novel mixed attentive pooling mechanism.
20. The attention weight is calculated from both answer span logits and self-attention output on token representations.
21. This attention-based interaction enables the exploitation of complementary information between ""answer"" and ""explain"" tasks.
22. The SAE system first filters unrelated documents, and selects gold documents using a document classifier trained with a novel pairwise learning-to-rank
23. loss function. The gold documents are then fed into a model to predict the answer and supporting sentences.
24. The model is a multi-task learning process which means while the answer prediction is accomplished at the token level, the support sentence is predicted as a node classification task at the sentence level.
25. As it is shown in Figure 32, the selection module consists of:The Multi-Head Self-Attention (MHSA) layer to capture interaction among documents","1. How does the Heterogeneous Document-Entity (HDE) graph facilitate multi-hop machine reading comprehension through its structure of nodes and edges?
2. What are the specific roles of different types of edges in the HDE graph for enabling accurate reasoning in machine reading comprehension?
3. How does the complexity of models in multi-hop machine reading comprehension increase with the number of inferences, and what are the implications for efficiency?
4. In what ways does the Select, Answer, and Explain (SAE) system utilize GNN and contextual sentence embeddings to improve interpretability and accuracy in machine reading comprehension?
5. How does the Multi-Head Self-Attention (MHSA) layer contribute to the document selection process in the SAE system for multi-hop reasoning tasks?","1. 1. How does the Heterogeneous Document-Entity (HDE) graph facilitate multi-hop machine reading comprehension through its structure of nodes and edges?
2. 2. What are the specific roles of different types of edges in the HDE graph for enabling accurate reasoning in machine reading comprehension?
3. 3. How does the complexity of models in multi-hop machine reading comprehension increase with the number of inferences, and what are the implications for efficiency?
4. 4. In what ways does the Select, Answer, and Explain (SAE) system utilize GNN and contextual sentence embeddings to improve interpretability and accuracy in machine reading comprehension?
5. 5. How does the Multi-Head Self-Attention (MHSA) layer contribute to the document selection process in the SAE system for multi-hop reasoning tasks?",None,,"Questions:

1. 1. How does the Heterogeneous Document-Entity (HDE) graph facilitate multi-hop machine reading comprehension through its structure of nodes and edges?
2. 2. What are the specific roles of different types of edges in the HDE graph for enabling accurate reasoning in machine reading comprehension?
3. 3. How does the complexity of models in multi-hop machine reading comprehension increase with the number of inferences, and what are the implications for efficiency?
4. 4. In what ways does the Select, Answer, and Explain (SAE) system utilize GNN and contextual sentence embeddings to improve interpretability and accuracy in machine reading comprehension?
5. 5. How does the Multi-Head Self-Attention (MHSA) layer contribute to the document selection process in the SAE system for multi-hop reasoning tasks?

Answer:

(p22.0) Tu et al. [46] proposed a more complex graph named Heterogeneous Document-Entity (HDE) graph with different types of nodes and edges. This graph can cover different granularity levels of information in context and also enables rich information interaction among different types of nodes for accurate reasoning. The nodes in the HDE graph are candidates, documents, and entities. Besides, it has seven types of edges: 1) between a document node and a candidate node that appears in the same document.

(p22.1) 2) between a document node and its entity node. 3) between a candidate node and its entity node. 4) between two entity nodes from the same document. 5) between two entity nodes from different documents but they are mentions of the same candidate or query subject. 6) All candidate nodes connect with each other. 7) Entity nodes that do not meet previous conditions are connected as well. Figure 30 is an example of an HDE graph. In this figure, green nodes indicate documents, yellow nodes denote candidates, and blue nodes stand for entities. In addition, dash lines indicate type 1 edges, dash-dotted lines denote type 2 edges, square dot lines indicate type 3 edges, the red line denotes type 4 edge, the purple line indicates type 5 edge, and black lines indicate type 6 edges. The type 7 edge is not shown in this figure. As Figure 31 shows This model can be categorized into three parts: initializing HDE graph nodes with co-attention and self-attention-based context encoding, and reasoning over HDE graph with GNN-based message passing algorithms and score accumulation from updated HDE graph nodes representations.

(p22.2) However, [35] have shown that if the number of inferences increases, the complexity of models will rise sharply due to the iteration of cumbersome message passing algorithm, resulting in low efficiency.  [46] SAE: Tu et al. [47] proposed an interpretable system named Select, Answer, and Explain (SAE) with multi-hop reasoning graphs based on GNN with contextual sentence embeddings as nodes. This kind of sentence graph makes lead to an interpretable model because it can directly output supporting sentences with the answer prediction. The edges capture the global information presented within each document and also the cross-document reasoning path. Also, the contextual sentence embedding used in GNN is summarized over token representations based on a novel mixed attentive pooling mechanism. The attention weight is calculated from both answer span logits and self-attention output on token representations. This attention-based interaction enables the exploitation of complementary information between ""answer"" and ""explain"" tasks. The SAE system first filters unrelated documents, and selects gold documents using a document classifier trained with a novel pairwise learning-to-rank loss function.

(p22.3) The gold documents are then fed into a model to predict the answer and supporting sentences. The model is a multi-task learning process which means while the answer prediction is accomplished at the token level, the support sentence is predicted as a node classification task at the sentence level. As it is shown in Figure 32, the selection module consists of:

(p22.4) •

(p22.5) The Multi-Head Self-Attention (MHSA) layer to capture interaction among documents","Questions:



Answer:

(p22.0) Tu et al. [46] proposed a more complex graph named Heterogeneous Document-Entity (HDE) graph with different types of nodes and edges. This graph can cover different granularity levels of information in context and also enables rich information interaction among different types of nodes for accurate reasoning. The nodes in the HDE graph are candidates, documents, and entities. Besides, it has seven types of edges: 1) between a document node and a candidate node that appears in the same document.

(p22.1) 2) between a document node and its entity node. 3) between a candidate node and its entity node. 4) between two entity nodes from the same document. 5) between two entity nodes from different documents but they are mentions of the same candidate or query subject. 6) All candidate nodes connect with each other. 7) Entity nodes that do not meet previous conditions are connected as well. Figure 30 is an example of an HDE graph. In this figure, green nodes indicate documents, yellow nodes denote candidates, and blue nodes stand for entities. In addition, dash lines indicate type 1 edges, dash-dotted lines denote type 2 edges, square dot lines indicate type 3 edges, the red line denotes type 4 edge, the purple line indicates type 5 edge, and black lines indicate type 6 edges. The type 7 edge is not shown in this figure. As Figure 31 shows This model can be categorized into three parts: initializing HDE graph nodes with co-attention and self-attention-based context encoding, and reasoning over HDE graph with GNN-based message passing algorithms and score accumulation from updated HDE graph nodes representations.

(p22.2) However, [35] have shown that if the number of inferences increases, the complexity of models will rise sharply due to the iteration of cumbersome message passing algorithm, resulting in low efficiency.  [46] SAE: Tu et al. [47] proposed an interpretable system named Select, Answer, and Explain (SAE) with multi-hop reasoning graphs based on GNN with contextual sentence embeddings as nodes. This kind of sentence graph makes lead to an interpretable model because it can directly output supporting sentences with the answer prediction. The edges capture the global information presented within each document and also the cross-document reasoning path. Also, the contextual sentence embedding used in GNN is summarized over token representations based on a novel mixed attentive pooling mechanism. The attention weight is calculated from both answer span logits and self-attention output on token representations. This attention-based interaction enables the exploitation of complementary information between ""answer"" and ""explain"" tasks. The SAE system first filters unrelated documents, and selects gold documents using a document classifier trained with a novel pairwise learning-to-rank loss function.

(p22.3) The gold documents are then fed into a model to predict the answer and supporting sentences. The model is a multi-task learning process which means while the answer prediction is accomplished at the token level, the support sentence is predicted as a node classification task at the sentence level. As it is shown in Figure 32, the selection module consists of:

(p22.4) •

(p22.5) The Multi-Head Self-Attention (MHSA) layer to capture interaction among documents"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s23,•,"['p23.0', 'p23.1']","['The Pairwise Bi-Linear layer to find gold documents accurately. Figure 32: Architecture of the Selection module [47] HGN: Fang et al. [48] presented a method named Hierarchical Graph Network (HGN) which uses different levels of granularity to construct the graph. In this graph, there are four types of nodes: questions, paragraphs, sentences, and entities to cover the different structures of the problem input. Besides, there are seven types of bidirectional edges including edges between the question node and paragraph nodes, edges between the question node and its entity nodes, edges between a paragraph node and its sentence nodes, edges between sentence nodes and their linked paragraph nodes, edges between a sentence node and its entities nodes, edges between paragraph nodes, and edges between sentence nodes from the same paragraph. As you can see in Figure 33 after constructing the hierarchical Graph, a graph-attention-based message passing algorithm is used for reasoning over the graph and finally, the multi-task prediction module is used for paragraph selection, supporting facts prediction, entity prediction, and answer span extraction. However, it uses link information from Wikipedia pages, which makes the graph inflexible for general use [14]. Figure 33: Architecture of HGN [48] HGNN: Wang et al. [49] proposed a hierarchical graph neural network with a focus on compositional QA. In this study, the answers are derived from multiple but discontinuous segments in the documents. The nodes can be normal tokens, question tokens, sentence tokens, and special html image tokens. As you can see in figure (left), the input of the BERT sequence encoder is a question , two sentences ( 1 + 2 ) and a special image node ℎ 1 (some special tokens is used to indicate the question <SEP>, sentence <EOS> and special html image element <html>. The hierarchical graph neural networks blocks are shown in Figure 34 (middle). The attention-based Hierarchical Graph Neural Network (HGNN) uses three types of connections: 1) intra-sentence connection which means the connection of words within a sentence, 2) inter-sentence connection which means the connection of common tokens (e.g., sentence tokens or question tokens), and 3) global connection which means the connection between the question tokens and all the words in the document. The final prediction is made based on the sentence nodes and special html nodes. Finally, the connection mask matrix is used to connect the different tokens in the graph and predict the final answer. Figure 34: Architecture of HGNN [49] AMGN: Li et al. [39] proposed a model named Asynchronous Multi-grained Graph Network (AMGN) for multi-hop MRC has been proposed. Previous studies perform message passing synchronously at each step of the graph update and ignore that differentlevel relationships have different priorities and the reasoning has to be done in an ordered logic. First, a multi-grained graph is constructed using the entity and sentence to reflect the relation level of the information. Second, an algorithm for asynchronous message propagation according to the relationship levels (e.g., entity-entity! entity-sentence! sentence-sentence) to update the graph to mimic human multi-hop reading logic is proposed. Besides, a question reformulation mechanism (RNN-based) is proposed to iteratively update the latent question representation with sentence nodes. These sentence nodes are directly used for supporting fact prediction. As it has been shown in Figure 35, the whole model consists of four main components: Paragraph-selector for reducing search space, Encoder to encode the context and question, Construction & Reasoning for multi-grained graph construction and multi-step asynchronous node update, and Multi-task Prediction to predict the final answer. Figure 35: Architecture of AMGN [39] ClueReader: Feng et al. [13] Proposes a model with a heterogenous graph and also based on the grandmother cells concept [9] to imitate the process of human brain. This concept explains that to answer a question, we generally recall a mountain of related evidence whatever the form it is (such as a paragraph, a short sentence, or a phrase), and coordinate theirs inter relationships before we carry out the final results. However, most of the studies on multi-hop MRC cannot gather the semantic features in multi-angle representations, which causes incomplete conclusions. Inspired by the concept of the Grandmother Cells in cognitive neuroscience, a spatial graph attention framework named ClueReader has been proposed. This model is designed to assemble the semantic features in multi-angle representations and automatically concentrate or alleviate the information for reasoning. The name ""ClueReader"" is a metaphor for the pattern of the model: assume the subjects of queries are the start points of clues, the reasoning entities are bridge points, and the latent candidate entities are the grandmother cells. the model has to achieve candidate entities from the clues. As you can see in Figure 36 after constructing a graph from different kinds of nodes, a GAT layer performs the message passing to find the final answer. Figure 36: Architecture of ClueReader [13] IP-LQR: Tang et al. [50] proposed a model named the latent query reformulation method (IP-LQR) to consider the phrase as nodes to construct the graph. As you can see in the example in Figure 37 when ""Soviet statesman"" is used as the reasoning starting point, there are three viable subsequent entities to follow, namely, ""Mikhail Gorbachev"", ""Nikolai Viktorovich Podgorny""\' and ""Andrei Pavlovich Kirilenko"". It can be non-trivial to choose from the three candidates to ensure that the reasoning process will eventually lead to the right answer. In contrast, if ""former Soviet statesman"" has been considered as the starting point, it will be much easier to locate the right subsequent entity ""Mikhail Gorbachev"" to update the query.', ""Then they proposed the latent query reformulation method (IP-LQR), which incorporates phrases in the latent query reformulation to improve the cognitive ability of the system. They also design a semantic-augmented fusion method based on the phrase graph, which is then used to propagate the information (Figure 38). After encoding the context and question into the high vector space and acquiring the representations of phrases, sentences, and paragraphs via the mean-pooling layer. Then, a similarity evaluation strategy is designed to calculate the weights of edges in the graph, also the fusion layer is used as an information aggregation to latently update the original question's representation. Finally, a re-attention mechanism is used to help locate the gold answer based on the new representation.  Recently, graph-based models have become very popular and recognized as the main solution for multi-hop MRC because of the nature of modeling such a process into graph structure and the good results. But there is some drawback to this technique, the first one is the expensive computational process of the graph-based methods [52], and the second problem is that the graph-based models often can't cover all the inherent structure of documents and loss valuable structural information by modeling documents into graphs [12].""]","The Pairwise Bi-Linear layer to find gold documents accurately. Figure 32: Architecture of the Selection module [47] HGN: Fang et al. [48] presented a method named Hierarchical Graph Network (HGN) which uses different levels of granularity to construct the graph. In this graph, there are four types of nodes: questions, paragraphs, sentences, and entities to cover the different structures of the problem input. Besides, there are seven types of bidirectional edges including edges between the question node and paragraph nodes, edges between the question node and its entity nodes, edges between a paragraph node and its sentence nodes, edges between sentence nodes and their linked paragraph nodes, edges between a sentence node and its entities nodes, edges between paragraph nodes, and edges between sentence nodes from the same paragraph. As you can see in Figure 33 after constructing the hierarchical Graph, a graph-attention-based message passing algorithm is used for reasoning over the graph and finally, the multi-task prediction module is used for paragraph selection, supporting facts prediction, entity prediction, and answer span extraction. However, it uses link information from Wikipedia pages, which makes the graph inflexible for general use [14]. Figure 33: Architecture of HGN [48] HGNN: Wang et al. [49] proposed a hierarchical graph neural network with a focus on compositional QA. In this study, the answers are derived from multiple but discontinuous segments in the documents. The nodes can be normal tokens, question tokens, sentence tokens, and special html image tokens. As you can see in figure (left), the input of the BERT sequence encoder is a question , two sentences ( 1 + 2 ) and a special image node ℎ 1 (some special tokens is used to indicate the question <SEP>, sentence <EOS> and special html image element <html>. The hierarchical graph neural networks blocks are shown in Figure 34 (middle). The attention-based Hierarchical Graph Neural Network (HGNN) uses three types of connections: 1) intra-sentence connection which means the connection of words within a sentence, 2) inter-sentence connection which means the connection of common tokens (e.g., sentence tokens or question tokens), and 3) global connection which means the connection between the question tokens and all the words in the document. The final prediction is made based on the sentence nodes and special html nodes. Finally, the connection mask matrix is used to connect the different tokens in the graph and predict the final answer. Figure 34: Architecture of HGNN [49] AMGN: Li et al. [39] proposed a model named Asynchronous Multi-grained Graph Network (AMGN) for multi-hop MRC has been proposed. Previous studies perform message passing synchronously at each step of the graph update and ignore that differentlevel relationships have different priorities and the reasoning has to be done in an ordered logic. First, a multi-grained graph is constructed using the entity and sentence to reflect the relation level of the information. Second, an algorithm for asynchronous message propagation according to the relationship levels (e.g., entity-entity! entity-sentence! sentence-sentence) to update the graph to mimic human multi-hop reading logic is proposed. Besides, a question reformulation mechanism (RNN-based) is proposed to iteratively update the latent question representation with sentence nodes. These sentence nodes are directly used for supporting fact prediction. As it has been shown in Figure 35, the whole model consists of four main components: Paragraph-selector for reducing search space, Encoder to encode the context and question, Construction & Reasoning for multi-grained graph construction and multi-step asynchronous node update, and Multi-task Prediction to predict the final answer. Figure 35: Architecture of AMGN [39] ClueReader: Feng et al. [13] Proposes a model with a heterogenous graph and also based on the grandmother cells concept [9] to imitate the process of human brain. This concept explains that to answer a question, we generally recall a mountain of related evidence whatever the form it is (such as a paragraph, a short sentence, or a phrase), and coordinate theirs inter relationships before we carry out the final results. However, most of the studies on multi-hop MRC cannot gather the semantic features in multi-angle representations, which causes incomplete conclusions. Inspired by the concept of the Grandmother Cells in cognitive neuroscience, a spatial graph attention framework named ClueReader has been proposed. This model is designed to assemble the semantic features in multi-angle representations and automatically concentrate or alleviate the information for reasoning. The name ""ClueReader"" is a metaphor for the pattern of the model: assume the subjects of queries are the start points of clues, the reasoning entities are bridge points, and the latent candidate entities are the grandmother cells. the model has to achieve candidate entities from the clues. As you can see in Figure 36 after constructing a graph from different kinds of nodes, a GAT layer performs the message passing to find the final answer. Figure 36: Architecture of ClueReader [13] IP-LQR: Tang et al. [50] proposed a model named the latent query reformulation method (IP-LQR) to consider the phrase as nodes to construct the graph. As you can see in the example in Figure 37 when ""Soviet statesman"" is used as the reasoning starting point, there are three viable subsequent entities to follow, namely, ""Mikhail Gorbachev"", ""Nikolai Viktorovich Podgorny""' and ""Andrei Pavlovich Kirilenko"". It can be non-trivial to choose from the three candidates to ensure that the reasoning process will eventually lead to the right answer. In contrast, if ""former Soviet statesman"" has been considered as the starting point, it will be much easier to locate the right subsequent entity ""Mikhail Gorbachev"" to update the query.

Then they proposed the latent query reformulation method (IP-LQR), which incorporates phrases in the latent query reformulation to improve the cognitive ability of the system. They also design a semantic-augmented fusion method based on the phrase graph, which is then used to propagate the information (Figure 38). After encoding the context and question into the high vector space and acquiring the representations of phrases, sentences, and paragraphs via the mean-pooling layer. Then, a similarity evaluation strategy is designed to calculate the weights of edges in the graph, also the fusion layer is used as an information aggregation to latently update the original question's representation. Finally, a re-attention mechanism is used to help locate the gold answer based on the new representation.  Recently, graph-based models have become very popular and recognized as the main solution for multi-hop MRC because of the nature of modeling such a process into graph structure and the good results. But there is some drawback to this technique, the first one is the expensive computational process of the graph-based methods [52], and the second problem is that the graph-based models often can't cover all the inherent structure of documents and loss valuable structural information by modeling documents into graphs [12].","(p23.0) The Pairwise Bi-Linear layer to find gold documents accurately. Figure 32: Architecture of the Selection module [47] HGN: Fang et al. [48] presented a method named Hierarchical Graph Network (HGN) which uses different levels of granularity to construct the graph. In this graph, there are four types of nodes: questions, paragraphs, sentences, and entities to cover the different structures of the problem input. Besides, there are seven types of bidirectional edges including edges between the question node and paragraph nodes, edges between the question node and its entity nodes, edges between a paragraph node and its sentence nodes, edges between sentence nodes and their linked paragraph nodes, edges between a sentence node and its entities nodes, edges between paragraph nodes, and edges between sentence nodes from the same paragraph. As you can see in Figure 33 after constructing the hierarchical Graph, a graph-attention-based message passing algorithm is used for reasoning over the graph and finally, the multi-task prediction module is used for paragraph selection, supporting facts prediction, entity prediction, and answer span extraction. However, it uses link information from Wikipedia pages, which makes the graph inflexible for general use [14]. Figure 33: Architecture of HGN [48] HGNN: Wang et al. [49] proposed a hierarchical graph neural network with a focus on compositional QA. In this study, the answers are derived from multiple but discontinuous segments in the documents. The nodes can be normal tokens, question tokens, sentence tokens, and special html image tokens. As you can see in figure (left), the input of the BERT sequence encoder is a question , two sentences ( 1 + 2 ) and a special image node ℎ 1 (some special tokens is used to indicate the question <SEP>, sentence <EOS> and special html image element <html>. The hierarchical graph neural networks blocks are shown in Figure 34 (middle). The attention-based Hierarchical Graph Neural Network (HGNN) uses three types of connections: 1) intra-sentence connection which means the connection of words within a sentence, 2) inter-sentence connection which means the connection of common tokens (e.g., sentence tokens or question tokens), and 3) global connection which means the connection between the question tokens and all the words in the document. The final prediction is made based on the sentence nodes and special html nodes. Finally, the connection mask matrix is used to connect the different tokens in the graph and predict the final answer. Figure 34: Architecture of HGNN [49] AMGN: Li et al. [39] proposed a model named Asynchronous Multi-grained Graph Network (AMGN) for multi-hop MRC has been proposed. Previous studies perform message passing synchronously at each step of the graph update and ignore that differentlevel relationships have different priorities and the reasoning has to be done in an ordered logic. First, a multi-grained graph is constructed using the entity and sentence to reflect the relation level of the information. Second, an algorithm for asynchronous message propagation according to the relationship levels (e.g., entity-entity! entity-sentence! sentence-sentence) to update the graph to mimic human multi-hop reading logic is proposed. Besides, a question reformulation mechanism (RNN-based) is proposed to iteratively update the latent question representation with sentence nodes. These sentence nodes are directly used for supporting fact prediction. As it has been shown in Figure 35, the whole model consists of four main components: Paragraph-selector for reducing search space, Encoder to encode the context and question, Construction & Reasoning for multi-grained graph construction and multi-step asynchronous node update, and Multi-task Prediction to predict the final answer. Figure 35: Architecture of AMGN [39] ClueReader: Feng et al. [13] Proposes a model with a heterogenous graph and also based on the grandmother cells concept [9] to imitate the process of human brain. This concept explains that to answer a question, we generally recall a mountain of related evidence whatever the form it is (such as a paragraph, a short sentence, or a phrase), and coordinate theirs inter relationships before we carry out the final results. However, most of the studies on multi-hop MRC cannot gather the semantic features in multi-angle representations, which causes incomplete conclusions. Inspired by the concept of the Grandmother Cells in cognitive neuroscience, a spatial graph attention framework named ClueReader has been proposed. This model is designed to assemble the semantic features in multi-angle representations and automatically concentrate or alleviate the information for reasoning. The name ""ClueReader"" is a metaphor for the pattern of the model: assume the subjects of queries are the start points of clues, the reasoning entities are bridge points, and the latent candidate entities are the grandmother cells. the model has to achieve candidate entities from the clues. As you can see in Figure 36 after constructing a graph from different kinds of nodes, a GAT layer performs the message passing to find the final answer. Figure 36: Architecture of ClueReader [13] IP-LQR: Tang et al. [50] proposed a model named the latent query reformulation method (IP-LQR) to consider the phrase as nodes to construct the graph. As you can see in the example in Figure 37 when ""Soviet statesman"" is used as the reasoning starting point, there are three viable subsequent entities to follow, namely, ""Mikhail Gorbachev"", ""Nikolai Viktorovich Podgorny""' and ""Andrei Pavlovich Kirilenko"". It can be non-trivial to choose from the three candidates to ensure that the reasoning process will eventually lead to the right answer. In contrast, if ""former Soviet statesman"" has been considered as the starting point, it will be much easier to locate the right subsequent entity ""Mikhail Gorbachev"" to update the query.

(p23.1) Then they proposed the latent query reformulation method (IP-LQR), which incorporates phrases in the latent query reformulation to improve the cognitive ability of the system. They also design a semantic-augmented fusion method based on the phrase graph, which is then used to propagate the information (Figure 38). After encoding the context and question into the high vector space and acquiring the representations of phrases, sentences, and paragraphs via the mean-pooling layer. Then, a similarity evaluation strategy is designed to calculate the weights of edges in the graph, also the fusion layer is used as an information aggregation to latently update the original question's representation. Finally, a re-attention mechanism is used to help locate the gold answer based on the new representation.  Recently, graph-based models have become very popular and recognized as the main solution for multi-hop MRC because of the nature of modeling such a process into graph structure and the good results. But there is some drawback to this technique, the first one is the expensive computational process of the graph-based methods [52], and the second problem is that the graph-based models often can't cover all the inherent structure of documents and loss valuable structural information by modeling documents into graphs [12].","[['b46', 'b47', 'b48', 'b38', 'b13', 'b8', 'b12', 'b49'], ['b51', 'b11']]","[['b46', 'b47', 'b48', 'b38', 'b13', 'b8', 'b12', 'b49'], ['b51', 'b11']]",10,"1. The Pairwise Bi-Linear layer to find gold documents accurately.
2. Figure 32: Architecture of the Selection module [47] HGN: Fang et al. [48] presented a method named Hierarchical Graph Network (HGN) which uses different levels of granularity to construct the graph.
3. In this graph, there are four types of nodes: questions, paragraphs, sentences, and entities to cover the different structures of the problem input.
4. Besides, there are seven types of bidirectional edges including edges between the question node and paragraph nodes, edges between the question node and its entity nodes, edges between a paragraph node and its sentence nodes, edges between sentence nodes and their linked paragraph nodes, edges between a sentence node and its entities nodes, edges between paragraph nodes, and edges between sentence nodes from the same paragraph.
5. As you can see in Figure 33 after constructing the hierarchical Graph, a graph-attention-based message passing algorithm is used for reasoning over the graph and finally, the multi-task prediction module is used for paragraph selection, supporting facts prediction, entity prediction, and answer span extraction.
6. However, it uses link information from Wikipedia pages, which makes the graph inflexible for general use [14].
7. Figure 33: Architecture of HGN [48] HGNN: Wang et al. [49] proposed a hierarchical graph neural network with a focus on compositional QA.
8. In this study, the answers are derived from multiple but discontinuous segments in the documents.
9. The nodes can be normal tokens, question tokens, sentence tokens, and special html image tokens.
10. As you can see in figure (left), the input of the BERT sequence encoder is a question , two sentences ( 1 + 2 ) and a special image node ℎ 1 (some special tokens is used to indicate the question <SEP>, sentence <EOS> and special html image element <html>. The hierarchical graph neural networks blocks are shown in Figure 34 (middle).
11. The attention-based Hierarchical Graph Neural Network (HGNN) uses three types of connections: 1) intra-sentence connection which means the connection of words within a sentence, 2) inter-sentence connection which means the connection of common tokens (e.g., sentence tokens or question tokens), and 3) global connection which means the connection between the question tokens and all the words in the document.
12. The final prediction is made based on the sentence nodes and special html nodes.
13. Finally, the connection mask matrix is used to connect the different tokens in the graph and predict the final answer.
14. Figure 34: Architecture of HGNN [49] AMGN: Li et al. [39] proposed a model named Asynchronous Multi-grained Graph Network (AMGN) for multi-hop MRC has been proposed.
15. Previous studies perform message passing synchronously at each step of the graph update and ignore that differentlevel relationships have different priorities and the reasoning has to be done in an ordered logic.
16. First, a multi-grained graph is constructed using the entity and sentence to reflect the relation level of the information.
17. Second, an algorithm for asynchronous message propagation according to the relationship levels (e.g., entity-entity! entity-sentence! sentence-sentence) to update the graph to mimic human multi-hop reading logic is proposed.
18. Besides, a question reformulation mechanism (RNN-based) is proposed to iteratively update the latent question representation with sentence nodes.
19. These sentence nodes are directly used for supporting fact prediction.
20. As it has been shown in Figure 35, the whole model consists of four main components: Paragraph-selector for reducing search space, Encoder to encode the context and question, Construction & Reasoning for multi-grained graph construction and multi-step asynchronous node update, and Multi-task Prediction to predict the final answer.
21. Figure 35: Architecture of AMGN [39] ClueReader: Feng
22. et al. (HGN)0 Proposes a model with a heterogenous graph and also based on the grandmother cells concept [9] to imitate the process of human brain.
23. This concept explains that to answer a question, we generally recall a mountain of related evidence whatever the form it is (such as a paragraph, a short sentence, or a phrase), and coordinate theirs inter relationships before we carry out the final results.
24. However, most of the studies on multi-hop MRC cannot gather the semantic features in multi-angle representations, which causes incomplete conclusions.
25. Inspired by the concept of the Grandmother Cells in cognitive neuroscience, a spatial graph attention framework named ClueReader has been proposed.
26. This model is designed to assemble the semantic features in multi-angle representations and automatically concentrate or alleviate the information for reasoning.
27. The name ""ClueReader"" is a metaphor for the pattern of the model: assume the subjects of queries are the start points of clues, the reasoning entities are bridge points, and the latent candidate entities are the grandmother cells.
28. the model has to achieve candidate entities from the clues.
29. As you can see in Figure 36 after constructing a graph from different kinds of nodes, a GAT layer performs the message passing to find the final answer.
30. Figure 36: Architecture of ClueReader (HGN)0 IP-LQR: Tang et al. (HGN)1 proposed a model named the latent query reformulation method (HGN)3 to consider the phrase as nodes to construct the graph.
31. As you can see in the example in Figure 37 when ""Soviet statesman"" is used as the reasoning starting point, there are three viable subsequent entities to follow, namely, ""Mikhail Gorbachev"", ""Nikolai Viktorovich Podgorny""' and ""Andrei Pavlovich Kirilenko"".
32. It can be non-trivial to choose from the three candidates to ensure that the reasoning process will eventually lead to the right answer.
33. In contrast, if ""former Soviet statesman"" has been considered as the starting point, it will be much easier to locate the right subsequent entity ""Mikhail Gorbachev"" to update the query.
34. Then they proposed the latent query reformulation method (HGN)3, which incorporates phrases in the latent query reformulation to improve the cognitive ability of the system.
35. They also design a semantic-augmented fusion method based on the phrase graph, which is then used to propagate the information (HGN)4.
36. After encoding the context and question into the high vector space and acquiring the representations of phrases, sentences, and paragraphs via the mean-pooling layer.
37. Then, a similarity evaluation strategy is designed to calculate the weights of edges in the graph, also the fusion layer is used as an information aggregation to latently update the original question's representation.
38. Finally, a re-attention mechanism is used to help locate the gold answer based on the new representation.
39. Recently, graph-based models have become very popular and recognized as the main solution for multi-hop MRC because of the nature of modeling such a process into graph structure and the good results.
40. But there is some drawback to this technique, the first one is the expensive computational process of the graph-based methods (HGN)5, and the second problem is that the graph-based models often can't cover all the inherent structure of documents and loss valuable structural information by modeling documents into graphs (HGN)6.","How does the Hierarchical Graph Network (HGN) utilize different levels of granularity in its graph construction for machine reading comprehension?
What are the limitations of using Wikipedia link information in the Hierarchical Graph Network's graph construction?
How does the Asynchronous Multi-grained Graph Network (AMGN) model differ in its approach to message passing compared to previous studies in multi-hop MRC?
In what way does the ClueReader model incorporate the concept of grandmother cells to improve multi-hop machine reading comprehension?
What challenges are associated with the computational process and structural information loss in graph-based models for multi-hop machine reading comprehension?","1. How does the Hierarchical Graph Network (HGN) utilize different levels of granularity in its graph construction for machine reading comprehension?
2. What are the limitations of using Wikipedia link information in the Hierarchical Graph Network's graph construction?
3. How does the Asynchronous Multi-grained Graph Network (AMGN) model differ in its approach to message passing compared to previous studies in multi-hop MRC?
4. In what way does the ClueReader model incorporate the concept of grandmother cells to improve multi-hop machine reading comprehension?
5. What challenges are associated with the computational process and structural information loss in graph-based models for multi-hop machine reading comprehension?",None,,"Questions:

1. How does the Hierarchical Graph Network (HGN) utilize different levels of granularity in its graph construction for machine reading comprehension?
2. What are the limitations of using Wikipedia link information in the Hierarchical Graph Network's graph construction?
3. How does the Asynchronous Multi-grained Graph Network (AMGN) model differ in its approach to message passing compared to previous studies in multi-hop MRC?
4. In what way does the ClueReader model incorporate the concept of grandmother cells to improve multi-hop machine reading comprehension?
5. What challenges are associated with the computational process and structural information loss in graph-based models for multi-hop machine reading comprehension?

Answer:

(p23.0) The Pairwise Bi-Linear layer to find gold documents accurately. Figure 32: Architecture of the Selection module [47] HGN: Fang et al. [48] presented a method named Hierarchical Graph Network (HGN) which uses different levels of granularity to construct the graph. In this graph, there are four types of nodes: questions, paragraphs, sentences, and entities to cover the different structures of the problem input. Besides, there are seven types of bidirectional edges including edges between the question node and paragraph nodes, edges between the question node and its entity nodes, edges between a paragraph node and its sentence nodes, edges between sentence nodes and their linked paragraph nodes, edges between a sentence node and its entities nodes, edges between paragraph nodes, and edges between sentence nodes from the same paragraph. As you can see in Figure 33 after constructing the hierarchical Graph, a graph-attention-based message passing algorithm is used for reasoning over the graph and finally, the multi-task prediction module is used for paragraph selection, supporting facts prediction, entity prediction, and answer span extraction. However, it uses link information from Wikipedia pages, which makes the graph inflexible for general use [14]. Figure 33: Architecture of HGN [48] HGNN: Wang et al. [49] proposed a hierarchical graph neural network with a focus on compositional QA. In this study, the answers are derived from multiple but discontinuous segments in the documents. The nodes can be normal tokens, question tokens, sentence tokens, and special html image tokens. As you can see in figure (left), the input of the BERT sequence encoder is a question , two sentences ( 1 + 2 ) and a special image node ℎ 1 (some special tokens is used to indicate the question <SEP>, sentence <EOS> and special html image element <html>. The hierarchical graph neural networks blocks are shown in Figure 34 (middle). The attention-based Hierarchical Graph Neural Network (HGNN) uses three types of connections: 1) intra-sentence connection which means the connection of words within a sentence, 2) inter-sentence connection which means the connection of common tokens (e.g., sentence tokens or question tokens), and 3) global connection which means the connection between the question tokens and all the words in the document. The final prediction is made based on the sentence nodes and special html nodes. Finally, the connection mask matrix is used to connect the different tokens in the graph and predict the final answer. Figure 34: Architecture of HGNN [49] AMGN: Li et al. [39] proposed a model named Asynchronous Multi-grained Graph Network (AMGN) for multi-hop MRC has been proposed. Previous studies perform message passing synchronously at each step of the graph update and ignore that differentlevel relationships have different priorities and the reasoning has to be done in an ordered logic. First, a multi-grained graph is constructed using the entity and sentence to reflect the relation level of the information. Second, an algorithm for asynchronous message propagation according to the relationship levels (e.g., entity-entity! entity-sentence! sentence-sentence) to update the graph to mimic human multi-hop reading logic is proposed. Besides, a question reformulation mechanism (RNN-based) is proposed to iteratively update the latent question representation with sentence nodes. These sentence nodes are directly used for supporting fact prediction. As it has been shown in Figure 35, the whole model consists of four main components: Paragraph-selector for reducing search space, Encoder to encode the context and question, Construction & Reasoning for multi-grained graph construction and multi-step asynchronous node update, and Multi-task Prediction to predict the final answer. Figure 35: Architecture of AMGN [39] ClueReader: Feng et al. [13] Proposes a model with a heterogenous graph and also based on the grandmother cells concept [9] to imitate the process of human brain. This concept explains that to answer a question, we generally recall a mountain of related evidence whatever the form it is (such as a paragraph, a short sentence, or a phrase), and coordinate theirs inter relationships before we carry out the final results. However, most of the studies on multi-hop MRC cannot gather the semantic features in multi-angle representations, which causes incomplete conclusions. Inspired by the concept of the Grandmother Cells in cognitive neuroscience, a spatial graph attention framework named ClueReader has been proposed. This model is designed to assemble the semantic features in multi-angle representations and automatically concentrate or alleviate the information for reasoning. The name ""ClueReader"" is a metaphor for the pattern of the model: assume the subjects of queries are the start points of clues, the reasoning entities are bridge points, and the latent candidate entities are the grandmother cells. the model has to achieve candidate entities from the clues. As you can see in Figure 36 after constructing a graph from different kinds of nodes, a GAT layer performs the message passing to find the final answer. Figure 36: Architecture of ClueReader [13] IP-LQR: Tang et al. [50] proposed a model named the latent query reformulation method (IP-LQR) to consider the phrase as nodes to construct the graph. As you can see in the example in Figure 37 when ""Soviet statesman"" is used as the reasoning starting point, there are three viable subsequent entities to follow, namely, ""Mikhail Gorbachev"", ""Nikolai Viktorovich Podgorny""' and ""Andrei Pavlovich Kirilenko"". It can be non-trivial to choose from the three candidates to ensure that the reasoning process will eventually lead to the right answer. In contrast, if ""former Soviet statesman"" has been considered as the starting point, it will be much easier to locate the right subsequent entity ""Mikhail Gorbachev"" to update the query.

(p23.1) Then they proposed the latent query reformulation method (IP-LQR), which incorporates phrases in the latent query reformulation to improve the cognitive ability of the system. They also design a semantic-augmented fusion method based on the phrase graph, which is then used to propagate the information (Figure 38). After encoding the context and question into the high vector space and acquiring the representations of phrases, sentences, and paragraphs via the mean-pooling layer. Then, a similarity evaluation strategy is designed to calculate the weights of edges in the graph, also the fusion layer is used as an information aggregation to latently update the original question's representation. Finally, a re-attention mechanism is used to help locate the gold answer based on the new representation.  Recently, graph-based models have become very popular and recognized as the main solution for multi-hop MRC because of the nature of modeling such a process into graph structure and the good results. But there is some drawback to this technique, the first one is the expensive computational process of the graph-based methods [52], and the second problem is that the graph-based models often can't cover all the inherent structure of documents and loss valuable structural information by modeling documents into graphs [12].","Questions:



Answer:

(p23.0) The Pairwise Bi-Linear layer to find gold documents accurately. Figure 32: Architecture of the Selection module [47] HGN: Fang et al. [48] presented a method named Hierarchical Graph Network (HGN) which uses different levels of granularity to construct the graph. In this graph, there are four types of nodes: questions, paragraphs, sentences, and entities to cover the different structures of the problem input. Besides, there are seven types of bidirectional edges including edges between the question node and paragraph nodes, edges between the question node and its entity nodes, edges between a paragraph node and its sentence nodes, edges between sentence nodes and their linked paragraph nodes, edges between a sentence node and its entities nodes, edges between paragraph nodes, and edges between sentence nodes from the same paragraph. As you can see in Figure 33 after constructing the hierarchical Graph, a graph-attention-based message passing algorithm is used for reasoning over the graph and finally, the multi-task prediction module is used for paragraph selection, supporting facts prediction, entity prediction, and answer span extraction. However, it uses link information from Wikipedia pages, which makes the graph inflexible for general use [14]. Figure 33: Architecture of HGN [48] HGNN: Wang et al. [49] proposed a hierarchical graph neural network with a focus on compositional QA. In this study, the answers are derived from multiple but discontinuous segments in the documents. The nodes can be normal tokens, question tokens, sentence tokens, and special html image tokens. As you can see in figure (left), the input of the BERT sequence encoder is a question , two sentences ( 1 + 2 ) and a special image node ℎ 1 (some special tokens is used to indicate the question <SEP>, sentence <EOS> and special html image element <html>. The hierarchical graph neural networks blocks are shown in Figure 34 (middle). The attention-based Hierarchical Graph Neural Network (HGNN) uses three types of connections: 1) intra-sentence connection which means the connection of words within a sentence, 2) inter-sentence connection which means the connection of common tokens (e.g., sentence tokens or question tokens), and 3) global connection which means the connection between the question tokens and all the words in the document. The final prediction is made based on the sentence nodes and special html nodes. Finally, the connection mask matrix is used to connect the different tokens in the graph and predict the final answer. Figure 34: Architecture of HGNN [49] AMGN: Li et al. [39] proposed a model named Asynchronous Multi-grained Graph Network (AMGN) for multi-hop MRC has been proposed. Previous studies perform message passing synchronously at each step of the graph update and ignore that differentlevel relationships have different priorities and the reasoning has to be done in an ordered logic. First, a multi-grained graph is constructed using the entity and sentence to reflect the relation level of the information. Second, an algorithm for asynchronous message propagation according to the relationship levels (e.g., entity-entity! entity-sentence! sentence-sentence) to update the graph to mimic human multi-hop reading logic is proposed. Besides, a question reformulation mechanism (RNN-based) is proposed to iteratively update the latent question representation with sentence nodes. These sentence nodes are directly used for supporting fact prediction. As it has been shown in Figure 35, the whole model consists of four main components: Paragraph-selector for reducing search space, Encoder to encode the context and question, Construction & Reasoning for multi-grained graph construction and multi-step asynchronous node update, and Multi-task Prediction to predict the final answer. Figure 35: Architecture of AMGN [39] ClueReader: Feng et al. [13] Proposes a model with a heterogenous graph and also based on the grandmother cells concept [9] to imitate the process of human brain. This concept explains that to answer a question, we generally recall a mountain of related evidence whatever the form it is (such as a paragraph, a short sentence, or a phrase), and coordinate theirs inter relationships before we carry out the final results. However, most of the studies on multi-hop MRC cannot gather the semantic features in multi-angle representations, which causes incomplete conclusions. Inspired by the concept of the Grandmother Cells in cognitive neuroscience, a spatial graph attention framework named ClueReader has been proposed. This model is designed to assemble the semantic features in multi-angle representations and automatically concentrate or alleviate the information for reasoning. The name ""ClueReader"" is a metaphor for the pattern of the model: assume the subjects of queries are the start points of clues, the reasoning entities are bridge points, and the latent candidate entities are the grandmother cells. the model has to achieve candidate entities from the clues. As you can see in Figure 36 after constructing a graph from different kinds of nodes, a GAT layer performs the message passing to find the final answer. Figure 36: Architecture of ClueReader [13] IP-LQR: Tang et al. [50] proposed a model named the latent query reformulation method (IP-LQR) to consider the phrase as nodes to construct the graph. As you can see in the example in Figure 37 when ""Soviet statesman"" is used as the reasoning starting point, there are three viable subsequent entities to follow, namely, ""Mikhail Gorbachev"", ""Nikolai Viktorovich Podgorny""' and ""Andrei Pavlovich Kirilenko"". It can be non-trivial to choose from the three candidates to ensure that the reasoning process will eventually lead to the right answer. In contrast, if ""former Soviet statesman"" has been considered as the starting point, it will be much easier to locate the right subsequent entity ""Mikhail Gorbachev"" to update the query.

(p23.1) Then they proposed the latent query reformulation method (IP-LQR), which incorporates phrases in the latent query reformulation to improve the cognitive ability of the system. They also design a semantic-augmented fusion method based on the phrase graph, which is then used to propagate the information (Figure 38). After encoding the context and question into the high vector space and acquiring the representations of phrases, sentences, and paragraphs via the mean-pooling layer. Then, a similarity evaluation strategy is designed to calculate the weights of edges in the graph, also the fusion layer is used as an information aggregation to latently update the original question's representation. Finally, a re-attention mechanism is used to help locate the gold answer based on the new representation.  Recently, graph-based models have become very popular and recognized as the main solution for multi-hop MRC because of the nature of modeling such a process into graph structure and the good results. But there is some drawback to this technique, the first one is the expensive computational process of the graph-based methods [52], and the second problem is that the graph-based models often can't cover all the inherent structure of documents and loss valuable structural information by modeling documents into graphs [12]."
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s24,Graph-free technique,"['p24.0', 'p24.1', 'p24.2']","['While graph-based methods were being dominated in multi-hop MRC, a question arose that whether the use of a graph is necessary despite the mentioned problem? Several studies addressed this important question, which we named the Graph-free technique because the main aim of them is to prove that graph is not necessary for multi-hop MRC. They may use other techniques to prove this fact (decomposition, recurrent reasoning or any other new techniques). Because of the importance of this question, we will review these studies in this section with a focus on the main idea of them.', 'GF: Shao et al. [53] attempted to answer this question: How much does graph structure contribute to answer a multi-hop question. They reimplement a graph-based model-Dynamically Fused Graph Network [54]-and claimed that the graph structure can play an important role only if the pre-trained models are used in a feature-based manner, while if the pre-trained models are used in the fine-tuning approach, the graph structure may not be helpful. Then the adjacency matrix based on manually defined rules and the graph structure can be regarded as prior knowledge, which could be learned by self-attention or Transformers. They proved that when texts have been modeled as an entity graph, both graph-attention and self-attention can achieve comparable results, but when texts have been modeled as a sequence structure, only a 2-layer Transformer could achieve similar results as DFGN. As you can see in Figure 40 when the entity graph is fully connected, a graph-attention layer will degenerate into a selfattention layer. However, this study as the first attempt of a graph-free model suffers from huge performance gap compared to the state-art-of the graph based-models [14]. Figure 40: An example of the relation between the graph-attention network and the self-attention network [53] AMS: Yuntao et al. [52] proposed another non-graph model with a focus on the document filters step to denoise irrelevant documents, and proves that if this step has been done properly, even a single-hop model can be used for multi-hop. They investigate that promising performance of the filter from Hierarchical Graph Network (HGN) (Fang et al., 2020) and showed that for 2paragraph selection, both precision and recall can achieve around 95%, and for 4-paragraph selection, recall will be nearly 99%.', ""Then they proposed Answer Multi-hop questions by Single-hop QA (AMS) models and used a single-hop QA models based on the attention mechanism with the HGN's document filter. As you can see in Figure 41 after the Document denoise layer, they used an Attention-based single-hop layer. AMS could achieve a comparable result with stat-of-the-art graph-bade models but still couldn't improve them. Figure 41: Overall architecture of AMS [52] S2G: Wu, Zhang, and Zhao [14] investigated whether graph modeling is necessary for multi-hop. In this regard, they first proved that the retrieval stage is the most important module, while the existing studies focus on the reader module by graph modeling. This study presents a graph-free alternative named select-to-guide (S2G) to retrieve evidence paragraphs in a coarse-tofine manner, incorporated with two attention mechanisms, which shows conforming to the nature of multi-hop reasoning. For the paragraph retrieval module, this study introduced a cascaded paragraph retrieval module that retrieves the evidence paragraphs in an explicit coarse-to-fine manner, and in multi-task module there are one shared encoder module alongside with two interdependent modules with an attention mechanism ( Figure 42). However Concrete error analysis on S2G shows that there is still room for improvement on the multi-hop retriever module design. Figure 42: The multi-task module of S2G [14] As the last part of this section, Figure 43 has been prepared to summarize the techniques and models.""]","While graph-based methods were being dominated in multi-hop MRC, a question arose that whether the use of a graph is necessary despite the mentioned problem? Several studies addressed this important question, which we named the Graph-free technique because the main aim of them is to prove that graph is not necessary for multi-hop MRC. They may use other techniques to prove this fact (decomposition, recurrent reasoning or any other new techniques). Because of the importance of this question, we will review these studies in this section with a focus on the main idea of them.

GF: Shao et al. [53] attempted to answer this question: How much does graph structure contribute to answer a multi-hop question. They reimplement a graph-based model-Dynamically Fused Graph Network [54]-and claimed that the graph structure can play an important role only if the pre-trained models are used in a feature-based manner, while if the pre-trained models are used in the fine-tuning approach, the graph structure may not be helpful. Then the adjacency matrix based on manually defined rules and the graph structure can be regarded as prior knowledge, which could be learned by self-attention or Transformers. They proved that when texts have been modeled as an entity graph, both graph-attention and self-attention can achieve comparable results, but when texts have been modeled as a sequence structure, only a 2-layer Transformer could achieve similar results as DFGN. As you can see in Figure 40 when the entity graph is fully connected, a graph-attention layer will degenerate into a selfattention layer. However, this study as the first attempt of a graph-free model suffers from huge performance gap compared to the state-art-of the graph based-models [14]. Figure 40: An example of the relation between the graph-attention network and the self-attention network [53] AMS: Yuntao et al. [52] proposed another non-graph model with a focus on the document filters step to denoise irrelevant documents, and proves that if this step has been done properly, even a single-hop model can be used for multi-hop. They investigate that promising performance of the filter from Hierarchical Graph Network (HGN) (Fang et al., 2020) and showed that for 2paragraph selection, both precision and recall can achieve around 95%, and for 4-paragraph selection, recall will be nearly 99%.

Then they proposed Answer Multi-hop questions by Single-hop QA (AMS) models and used a single-hop QA models based on the attention mechanism with the HGN's document filter. As you can see in Figure 41 after the Document denoise layer, they used an Attention-based single-hop layer. AMS could achieve a comparable result with stat-of-the-art graph-bade models but still couldn't improve them. Figure 41: Overall architecture of AMS [52] S2G: Wu, Zhang, and Zhao [14] investigated whether graph modeling is necessary for multi-hop. In this regard, they first proved that the retrieval stage is the most important module, while the existing studies focus on the reader module by graph modeling. This study presents a graph-free alternative named select-to-guide (S2G) to retrieve evidence paragraphs in a coarse-tofine manner, incorporated with two attention mechanisms, which shows conforming to the nature of multi-hop reasoning. For the paragraph retrieval module, this study introduced a cascaded paragraph retrieval module that retrieves the evidence paragraphs in an explicit coarse-to-fine manner, and in multi-task module there are one shared encoder module alongside with two interdependent modules with an attention mechanism ( Figure 42). However Concrete error analysis on S2G shows that there is still room for improvement on the multi-hop retriever module design. Figure 42: The multi-task module of S2G [14] As the last part of this section, Figure 43 has been prepared to summarize the techniques and models.","(p24.0) While graph-based methods were being dominated in multi-hop MRC, a question arose that whether the use of a graph is necessary despite the mentioned problem? Several studies addressed this important question, which we named the Graph-free technique because the main aim of them is to prove that graph is not necessary for multi-hop MRC. They may use other techniques to prove this fact (decomposition, recurrent reasoning or any other new techniques). Because of the importance of this question, we will review these studies in this section with a focus on the main idea of them.

(p24.1) GF: Shao et al. [53] attempted to answer this question: How much does graph structure contribute to answer a multi-hop question. They reimplement a graph-based model-Dynamically Fused Graph Network [54]-and claimed that the graph structure can play an important role only if the pre-trained models are used in a feature-based manner, while if the pre-trained models are used in the fine-tuning approach, the graph structure may not be helpful. Then the adjacency matrix based on manually defined rules and the graph structure can be regarded as prior knowledge, which could be learned by self-attention or Transformers. They proved that when texts have been modeled as an entity graph, both graph-attention and self-attention can achieve comparable results, but when texts have been modeled as a sequence structure, only a 2-layer Transformer could achieve similar results as DFGN. As you can see in Figure 40 when the entity graph is fully connected, a graph-attention layer will degenerate into a selfattention layer. However, this study as the first attempt of a graph-free model suffers from huge performance gap compared to the state-art-of the graph based-models [14]. Figure 40: An example of the relation between the graph-attention network and the self-attention network [53] AMS: Yuntao et al. [52] proposed another non-graph model with a focus on the document filters step to denoise irrelevant documents, and proves that if this step has been done properly, even a single-hop model can be used for multi-hop. They investigate that promising performance of the filter from Hierarchical Graph Network (HGN) (Fang et al., 2020) and showed that for 2paragraph selection, both precision and recall can achieve around 95%, and for 4-paragraph selection, recall will be nearly 99%.

(p24.2) Then they proposed Answer Multi-hop questions by Single-hop QA (AMS) models and used a single-hop QA models based on the attention mechanism with the HGN's document filter. As you can see in Figure 41 after the Document denoise layer, they used an Attention-based single-hop layer. AMS could achieve a comparable result with stat-of-the-art graph-bade models but still couldn't improve them. Figure 41: Overall architecture of AMS [52] S2G: Wu, Zhang, and Zhao [14] investigated whether graph modeling is necessary for multi-hop. In this regard, they first proved that the retrieval stage is the most important module, while the existing studies focus on the reader module by graph modeling. This study presents a graph-free alternative named select-to-guide (S2G) to retrieve evidence paragraphs in a coarse-tofine manner, incorporated with two attention mechanisms, which shows conforming to the nature of multi-hop reasoning. For the paragraph retrieval module, this study introduced a cascaded paragraph retrieval module that retrieves the evidence paragraphs in an explicit coarse-to-fine manner, and in multi-task module there are one shared encoder module alongside with two interdependent modules with an attention mechanism ( Figure 42). However Concrete error analysis on S2G shows that there is still room for improvement on the multi-hop retriever module design. Figure 42: The multi-task module of S2G [14] As the last part of this section, Figure 43 has been prepared to summarize the techniques and models.","[[], ['b47', 'b13', 'b53', None, 'b51', 'b52'], ['b51', 'b13']]","[[], ['b47', 'b13', 'b53', None, 'b51', 'b52'], ['b51', 'b13']]",8,"1. While graph-based methods were being dominated in multi-hop MRC, a question arose that whether the use of a graph is necessary despite the mentioned problem?
2. Several studies addressed this important question, which we named the Graph-free technique because the main aim of them is to prove that graph is not necessary for multi-hop MRC.
3. They may use other techniques to prove this fact (decomposition, recurrent reasoning or any other new techniques).
4. Because of the importance of this question, we will review these studies in this section with a focus on the main idea of them.
5. GF: Shao et al. [53] attempted to answer this question: How much does graph structure contribute to answer a multi-hop question.
6. They reimplement a graph-based model-Dynamically Fused Graph Network [54]-and claimed that the graph structure can play an important role only if the pre-trained models are used in a feature-based manner, while if the pre-trained models are used in the fine-tuning approach, the graph structure may not be helpful.
7. Then the adjacency matrix based on manually defined rules and the graph structure can be regarded as prior knowledge, which could be learned by self-attention or Transformers.
8. They proved that when texts have been modeled as an entity graph, both graph-attention and self-attention can achieve comparable results, but when texts have been modeled as a sequence structure, only a 2-layer Transformer could achieve similar results as DFGN.
9. As you can see in Figure 40 when the entity graph is fully connected, a graph-attention layer will degenerate into a selfattention layer.
10. However, this study as the first attempt of a graph-free model suffers from huge performance gap compared to the state-art-of the graph based-models [14].
11. Figure 40: An example of the relation between the graph-attention network and the self-attention network [53] AMS: Yuntao et al. [52] proposed another non-graph model with a focus on the document filters step to denoise irrelevant documents, and proves that if this step has been done properly, even a single-hop model can be used for multi-hop.
12. They investigate that promising performance of the filter from Hierarchical Graph Network (HGN) (Fang et al., 2020) and showed that for 2paragraph selection, both precision and recall can achieve around 95%, and for 4-paragraph selection, recall will be nearly 99%.
13. Then they proposed Answer Multi-hop questions by Single-hop QA (AMS) models and used a single-hop QA models based on the attention mechanism with the HGN's document filter.
14. As you can see in Figure 41 after the Document denoise layer, they used an Attention-based single-hop layer.
15. AMS could achieve a comparable result with stat-of-the-art graph-bade models but still couldn't improve them.
16. Figure 41: Overall architecture of AMS [52] S2G: Wu, Zhang, and Zhao [14] investigated whether graph modeling is necessary for multi-hop.
17. In this regard, they first proved that the retrieval stage is the most important module, while the existing studies focus on the reader module by graph modeling.
18. This study presents a graph-free alternative named select-to-guide (S2G) to retrieve evidence paragraphs in a coarse-tofine manner, incorporated with two attention mechanisms, which shows conforming to the nature of multi-hop reasoning.
19. For the paragraph retrieval module, this study introduced a cascaded paragraph retrieval module that retrieves the evidence paragraphs in an explicit coarse-to-fine manner, and in multi-task module there are one shared encoder module alongside with two interdependent modules with an attention mechanism ( Figure 42).
20. However Concrete error analysis on S2G shows that there is still room for improvement on the multi-hop retriever module design.
21. Figure 42: The multi-task module of S2G [14] As the last part of this section, Figure 43 has been prepared to summarize the techniques and models.","How do graph-free techniques in multi-hop MRC compare to graph-based methods in terms of necessity and effectiveness?
What role does the graph structure play in the performance of multi-hop MRC models when using pre-trained models in a feature-based versus fine-tuning approach?
How does the document filter step in non-graph models impact the denoising of irrelevant documents for multi-hop question answering?
In what ways do attention mechanisms contribute to the performance of single-hop QA models in the context of multi-hop MRC?
What are the challenges and potential areas for improvement in the design of multi-hop retriever modules in graph-free MRC approaches?","1. How do graph-free techniques in multi-hop MRC compare to graph-based methods in terms of necessity and effectiveness?
2. What role does the graph structure play in the performance of multi-hop MRC models when using pre-trained models in a feature-based versus fine-tuning approach?
3. How does the document filter step in non-graph models impact the denoising of irrelevant documents for multi-hop question answering?
4. In what ways do attention mechanisms contribute to the performance of single-hop QA models in the context of multi-hop MRC?
5. What are the challenges and potential areas for improvement in the design of multi-hop retriever modules in graph-free MRC approaches?",None,,"Questions:

1. How do graph-free techniques in multi-hop MRC compare to graph-based methods in terms of necessity and effectiveness?
2. What role does the graph structure play in the performance of multi-hop MRC models when using pre-trained models in a feature-based versus fine-tuning approach?
3. How does the document filter step in non-graph models impact the denoising of irrelevant documents for multi-hop question answering?
4. In what ways do attention mechanisms contribute to the performance of single-hop QA models in the context of multi-hop MRC?
5. What are the challenges and potential areas for improvement in the design of multi-hop retriever modules in graph-free MRC approaches?

Answer:

(p24.0) While graph-based methods were being dominated in multi-hop MRC, a question arose that whether the use of a graph is necessary despite the mentioned problem? Several studies addressed this important question, which we named the Graph-free technique because the main aim of them is to prove that graph is not necessary for multi-hop MRC. They may use other techniques to prove this fact (decomposition, recurrent reasoning or any other new techniques). Because of the importance of this question, we will review these studies in this section with a focus on the main idea of them.

(p24.1) GF: Shao et al. [53] attempted to answer this question: How much does graph structure contribute to answer a multi-hop question. They reimplement a graph-based model-Dynamically Fused Graph Network [54]-and claimed that the graph structure can play an important role only if the pre-trained models are used in a feature-based manner, while if the pre-trained models are used in the fine-tuning approach, the graph structure may not be helpful. Then the adjacency matrix based on manually defined rules and the graph structure can be regarded as prior knowledge, which could be learned by self-attention or Transformers. They proved that when texts have been modeled as an entity graph, both graph-attention and self-attention can achieve comparable results, but when texts have been modeled as a sequence structure, only a 2-layer Transformer could achieve similar results as DFGN. As you can see in Figure 40 when the entity graph is fully connected, a graph-attention layer will degenerate into a selfattention layer. However, this study as the first attempt of a graph-free model suffers from huge performance gap compared to the state-art-of the graph based-models [14]. Figure 40: An example of the relation between the graph-attention network and the self-attention network [53] AMS: Yuntao et al. [52] proposed another non-graph model with a focus on the document filters step to denoise irrelevant documents, and proves that if this step has been done properly, even a single-hop model can be used for multi-hop. They investigate that promising performance of the filter from Hierarchical Graph Network (HGN) (Fang et al., 2020) and showed that for 2paragraph selection, both precision and recall can achieve around 95%, and for 4-paragraph selection, recall will be nearly 99%.

(p24.2) Then they proposed Answer Multi-hop questions by Single-hop QA (AMS) models and used a single-hop QA models based on the attention mechanism with the HGN's document filter. As you can see in Figure 41 after the Document denoise layer, they used an Attention-based single-hop layer. AMS could achieve a comparable result with stat-of-the-art graph-bade models but still couldn't improve them. Figure 41: Overall architecture of AMS [52] S2G: Wu, Zhang, and Zhao [14] investigated whether graph modeling is necessary for multi-hop. In this regard, they first proved that the retrieval stage is the most important module, while the existing studies focus on the reader module by graph modeling. This study presents a graph-free alternative named select-to-guide (S2G) to retrieve evidence paragraphs in a coarse-tofine manner, incorporated with two attention mechanisms, which shows conforming to the nature of multi-hop reasoning. For the paragraph retrieval module, this study introduced a cascaded paragraph retrieval module that retrieves the evidence paragraphs in an explicit coarse-to-fine manner, and in multi-task module there are one shared encoder module alongside with two interdependent modules with an attention mechanism ( Figure 42). However Concrete error analysis on S2G shows that there is still room for improvement on the multi-hop retriever module design. Figure 42: The multi-task module of S2G [14] As the last part of this section, Figure 43 has been prepared to summarize the techniques and models.","Questions:



Answer:

(p24.0) While graph-based methods were being dominated in multi-hop MRC, a question arose that whether the use of a graph is necessary despite the mentioned problem? Several studies addressed this important question, which we named the Graph-free technique because the main aim of them is to prove that graph is not necessary for multi-hop MRC. They may use other techniques to prove this fact (decomposition, recurrent reasoning or any other new techniques). Because of the importance of this question, we will review these studies in this section with a focus on the main idea of them.

(p24.1) GF: Shao et al. [53] attempted to answer this question: How much does graph structure contribute to answer a multi-hop question. They reimplement a graph-based model-Dynamically Fused Graph Network [54]-and claimed that the graph structure can play an important role only if the pre-trained models are used in a feature-based manner, while if the pre-trained models are used in the fine-tuning approach, the graph structure may not be helpful. Then the adjacency matrix based on manually defined rules and the graph structure can be regarded as prior knowledge, which could be learned by self-attention or Transformers. They proved that when texts have been modeled as an entity graph, both graph-attention and self-attention can achieve comparable results, but when texts have been modeled as a sequence structure, only a 2-layer Transformer could achieve similar results as DFGN. As you can see in Figure 40 when the entity graph is fully connected, a graph-attention layer will degenerate into a selfattention layer. However, this study as the first attempt of a graph-free model suffers from huge performance gap compared to the state-art-of the graph based-models [14]. Figure 40: An example of the relation between the graph-attention network and the self-attention network [53] AMS: Yuntao et al. [52] proposed another non-graph model with a focus on the document filters step to denoise irrelevant documents, and proves that if this step has been done properly, even a single-hop model can be used for multi-hop. They investigate that promising performance of the filter from Hierarchical Graph Network (HGN) (Fang et al., 2020) and showed that for 2paragraph selection, both precision and recall can achieve around 95%, and for 4-paragraph selection, recall will be nearly 99%.

(p24.2) Then they proposed Answer Multi-hop questions by Single-hop QA (AMS) models and used a single-hop QA models based on the attention mechanism with the HGN's document filter. As you can see in Figure 41 after the Document denoise layer, they used an Attention-based single-hop layer. AMS could achieve a comparable result with stat-of-the-art graph-bade models but still couldn't improve them. Figure 41: Overall architecture of AMS [52] S2G: Wu, Zhang, and Zhao [14] investigated whether graph modeling is necessary for multi-hop. In this regard, they first proved that the retrieval stage is the most important module, while the existing studies focus on the reader module by graph modeling. This study presents a graph-free alternative named select-to-guide (S2G) to retrieve evidence paragraphs in a coarse-tofine manner, incorporated with two attention mechanisms, which shows conforming to the nature of multi-hop reasoning. For the paragraph retrieval module, this study introduced a cascaded paragraph retrieval module that retrieves the evidence paragraphs in an explicit coarse-to-fine manner, and in multi-task module there are one shared encoder module alongside with two interdependent modules with an attention mechanism ( Figure 42). However Concrete error analysis on S2G shows that there is still room for improvement on the multi-hop retriever module design. Figure 42: The multi-task module of S2G [14] As the last part of this section, Figure 43 has been prepared to summarize the techniques and models."
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s25,Multi-hop MRC techniques,"['p25.0', 'p25.1']","['Decomposition Recurrent-reasoning Graph Path-based', 'Self-assembling MNM [18] ONUS [27] CGDe-FGIn [12] RERC [26] MHPGM [29] QFE [30] TAP [31] PH-Model [32] EPAr [33] PathNet [34] DRL-GRC [36] SMR [37] ChainEx [38] SCR [35] MHQA-GRN [40] DFGN [41] Entity-GCN [42] BAG [43] CogQA [44] DRNQA ']","Decomposition Recurrent-reasoning Graph Path-based

Self-assembling MNM [18] ONUS [27] CGDe-FGIn [12] RERC [26] MHPGM [29] QFE [30] TAP [31] PH-Model [32] EPAr [33] PathNet [34] DRL-GRC [36] SMR [37] ChainEx [38] SCR [35] MHQA-GRN [40] DFGN [41] Entity-GCN [42] BAG [43] CogQA [44] DRNQA ","(p25.0) Decomposition Recurrent-reasoning Graph Path-based

(p25.1) Self-assembling MNM [18] ONUS [27] CGDe-FGIn [12] RERC [26] MHPGM [29] QFE [30] TAP [31] PH-Model [32] EPAr [33] PathNet [34] DRL-GRC [36] SMR [37] ChainEx [38] SCR [35] MHQA-GRN [40] DFGN [41] Entity-GCN [42] BAG [43] CogQA [44] DRNQA ","[[], ['b34', 'b36', 'b32', 'b29', 'b25', 'b11', 'b40', 'b43', 'b26', 'b33', 'b17', 'b28', 'b37', 'b31', 'b39', 'b30', 'b35', 'b42', 'b41']]","[[], ['b34', 'b36', 'b32', 'b29', 'b25', 'b11', 'b40', 'b43', 'b26', 'b33', 'b17', 'b28', 'b37', 'b31', 'b39', 'b30', 'b35', 'b42', 'b41']]",19,"1. Decomposition Recurrent-reasoning Graph Path-basedSelf-assembling MNM [18] ONUS [27] CGDe-FGIn [12] RERC [26]
2. MHPGM [29] QFE [30] TAP [31] PH-Model [32] EPAr [33]
3. PathNet [34] DRL-GRC [27]0 SMR [27]1 ChainEx [27]2
4. SCR [27]3 MHQA-GRN [27]4 DFGN [27]5 Entity-GCN [27]6 BAG [27]7 CogQA [27]8 DRNQA","What are the key techniques used in multi-hop machine reading comprehension (MRC)?
How do graph-based approaches contribute to the advancement of multi-hop MRC?
What role does decomposition play in enhancing the performance of multi-hop MRC models?
Can you explain the significance of recurrent reasoning in the context of multi-hop MRC?
What are some examples of path-based methods in multi-hop machine reading comprehension, and how do they differ from other techniques?","1. What are the key techniques used in multi-hop machine reading comprehension (MRC)?
2. How do graph-based approaches contribute to the advancement of multi-hop MRC?
3. What role does decomposition play in enhancing the performance of multi-hop MRC models?
4. Can you explain the significance of recurrent reasoning in the context of multi-hop MRC?
5. What are some examples of path-based methods in multi-hop machine reading comprehension, and how do they differ from other techniques?",None,,"Questions:

1. What are the key techniques used in multi-hop machine reading comprehension (MRC)?
2. How do graph-based approaches contribute to the advancement of multi-hop MRC?
3. What role does decomposition play in enhancing the performance of multi-hop MRC models?
4. Can you explain the significance of recurrent reasoning in the context of multi-hop MRC?
5. What are some examples of path-based methods in multi-hop machine reading comprehension, and how do they differ from other techniques?

Answer:

(p25.0) Decomposition Recurrent-reasoning Graph Path-based

(p25.1) Self-assembling MNM [18] ONUS [27] CGDe-FGIn [12] RERC [26] MHPGM [29] QFE [30] TAP [31] PH-Model [32] EPAr [33] PathNet [34] DRL-GRC [36] SMR [37] ChainEx [38] SCR [35] MHQA-GRN [40] DFGN [41] Entity-GCN [42] BAG [43] CogQA [44] DRNQA ","Questions:



Answer:

(p25.0) Decomposition Recurrent-reasoning Graph Path-based

(p25.1) Self-assembling MNM [18] ONUS [27] CGDe-FGIn [12] RERC [26] MHPGM [29] QFE [30] TAP [31] PH-Model [32] EPAr [33] PathNet [34] DRL-GRC [36] SMR [37] ChainEx [38] SCR [35] MHQA-GRN [40] DFGN [41] Entity-GCN [42] BAG [43] CogQA [44] DRNQA "
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s29,HotpotQA,"['p29.0', 'p29.1']","['In this section, the performance of models on HotpotQA [11] will be investigated. Exact-match (EM) and F1 metrics have been used in this study to evaluate the model performance. EM is used to show whether the predicated answer and the ground-truth answer are exactly the same. F1 measures the average overlap between the predicted answer and the ground-truth answer. In addition, there are three sets of metrics: 1) Answer EM/F1 to evaluate the answer span prediction, 2) Support EM/F1 to evaluate the supporting facts prediction, and 3) Joint EM/F1 to combine the two previous ones.', 'To have an accurate comparison, the year of publications, the technique, along with the results, for Distracter setting and Fullwiki setting have been presented in this table 1 and 2. In the distractor setting, the best result is for AMGN [39] which is a graph-based model. Although the results are very diverse due to a large number of evaluation metrics, in general, graph-based methods have achieved relatively better results, and these good results were a motivation for great attention to the graph-based technique. But S2G [14] which is a graph-free model in 2021 has achieved a comparable result to the best graph-based model ( Table 1). A few models have published the result for the Fullwiki setting due to the difficulty of this setting, and among them, HGN [48], which is a graph-based model, has achieved the best result (Table 2).   ']","In this section, the performance of models on HotpotQA [11] will be investigated. Exact-match (EM) and F1 metrics have been used in this study to evaluate the model performance. EM is used to show whether the predicated answer and the ground-truth answer are exactly the same. F1 measures the average overlap between the predicted answer and the ground-truth answer. In addition, there are three sets of metrics: 1) Answer EM/F1 to evaluate the answer span prediction, 2) Support EM/F1 to evaluate the supporting facts prediction, and 3) Joint EM/F1 to combine the two previous ones.

To have an accurate comparison, the year of publications, the technique, along with the results, for Distracter setting and Fullwiki setting have been presented in this table 1 and 2. In the distractor setting, the best result is for AMGN [39] which is a graph-based model. Although the results are very diverse due to a large number of evaluation metrics, in general, graph-based methods have achieved relatively better results, and these good results were a motivation for great attention to the graph-based technique. But S2G [14] which is a graph-free model in 2021 has achieved a comparable result to the best graph-based model ( Table 1). A few models have published the result for the Fullwiki setting due to the difficulty of this setting, and among them, HGN [48], which is a graph-based model, has achieved the best result (Table 2).   ","(p29.0) In this section, the performance of models on HotpotQA [11] will be investigated. Exact-match (EM) and F1 metrics have been used in this study to evaluate the model performance. EM is used to show whether the predicated answer and the ground-truth answer are exactly the same. F1 measures the average overlap between the predicted answer and the ground-truth answer. In addition, there are three sets of metrics: 1) Answer EM/F1 to evaluate the answer span prediction, 2) Support EM/F1 to evaluate the supporting facts prediction, and 3) Joint EM/F1 to combine the two previous ones.

(p29.1) To have an accurate comparison, the year of publications, the technique, along with the results, for Distracter setting and Fullwiki setting have been presented in this table 1 and 2. In the distractor setting, the best result is for AMGN [39] which is a graph-based model. Although the results are very diverse due to a large number of evaluation metrics, in general, graph-based methods have achieved relatively better results, and these good results were a motivation for great attention to the graph-based technique. But S2G [14] which is a graph-free model in 2021 has achieved a comparable result to the best graph-based model ( Table 1). A few models have published the result for the Fullwiki setting due to the difficulty of this setting, and among them, HGN [48], which is a graph-based model, has achieved the best result (Table 2).   ","[['b10'], ['b38', 'b47', 'b13']]","[['b10'], ['b38', 'b47', 'b13']]",4,"1. In this section, the performance of models on HotpotQA [11] will be investigated.
2. Exact-match (EM) and F1 metrics have been used in this study to evaluate the model performance.
3. EM is used to show whether the predicated answer and the ground-truth answer are exactly the same.
4. F1 measures the average overlap between the predicted answer and the ground-truth answer.
5. In addition, there are three sets of metrics: 1) Answer EM/F1 to evaluate the answer span prediction, 2) Support EM/F1 to evaluate the supporting facts prediction, and 3) Joint EM/F1 to combine the two previous ones.
6. To have an accurate comparison, the year of publications, the technique, along with the results, for Distracter setting and Fullwiki setting have been presented in this table 1 and 2.
7. In the distractor setting, the best result is for AMGN [39] which is a graph-based model.
8. Although the results are very diverse due to a large number of evaluation metrics, in general, graph-based methods have achieved relatively better results, and these good results were a motivation for great attention to the graph-based technique.
9. But S2G [14] which is a graph-free model in 2021 has achieved a comparable result to the best graph-based model ( Table 1).
10. A few models have published the result for the Fullwiki setting due to the difficulty of this setting, and among them, HGN [48], which is a graph-based model, has achieved the best result (Table 2).","What metrics are used to evaluate model performance in HotpotQA?
How do Exact-match and F1 metrics differ in evaluating model performance in machine reading comprehension tasks?
What are the three sets of metrics used in HotpotQA to evaluate different aspects of model performance?
Why have graph-based methods generally achieved better results in the Distracter setting of HotpotQA?
What challenges are associated with the Fullwiki setting in HotpotQA that result in fewer models being evaluated in this context?","1. What metrics are used to evaluate model performance in HotpotQA?
2. How do Exact-match and F1 metrics differ in evaluating model performance in machine reading comprehension tasks?
3. What are the three sets of metrics used in HotpotQA to evaluate different aspects of model performance?
4. Why have graph-based methods generally achieved better results in the Distracter setting of HotpotQA?
5. What challenges are associated with the Fullwiki setting in HotpotQA that result in fewer models being evaluated in this context?",None,,"Questions:

1. What metrics are used to evaluate model performance in HotpotQA?
2. How do Exact-match and F1 metrics differ in evaluating model performance in machine reading comprehension tasks?
3. What are the three sets of metrics used in HotpotQA to evaluate different aspects of model performance?
4. Why have graph-based methods generally achieved better results in the Distracter setting of HotpotQA?
5. What challenges are associated with the Fullwiki setting in HotpotQA that result in fewer models being evaluated in this context?

Answer:

(p29.0) In this section, the performance of models on HotpotQA [11] will be investigated. Exact-match (EM) and F1 metrics have been used in this study to evaluate the model performance. EM is used to show whether the predicated answer and the ground-truth answer are exactly the same. F1 measures the average overlap between the predicted answer and the ground-truth answer. In addition, there are three sets of metrics: 1) Answer EM/F1 to evaluate the answer span prediction, 2) Support EM/F1 to evaluate the supporting facts prediction, and 3) Joint EM/F1 to combine the two previous ones.

(p29.1) To have an accurate comparison, the year of publications, the technique, along with the results, for Distracter setting and Fullwiki setting have been presented in this table 1 and 2. In the distractor setting, the best result is for AMGN [39] which is a graph-based model. Although the results are very diverse due to a large number of evaluation metrics, in general, graph-based methods have achieved relatively better results, and these good results were a motivation for great attention to the graph-based technique. But S2G [14] which is a graph-free model in 2021 has achieved a comparable result to the best graph-based model ( Table 1). A few models have published the result for the Fullwiki setting due to the difficulty of this setting, and among them, HGN [48], which is a graph-based model, has achieved the best result (Table 2).   ","Questions:



Answer:

(p29.0) In this section, the performance of models on HotpotQA [11] will be investigated. Exact-match (EM) and F1 metrics have been used in this study to evaluate the model performance. EM is used to show whether the predicated answer and the ground-truth answer are exactly the same. F1 measures the average overlap between the predicted answer and the ground-truth answer. In addition, there are three sets of metrics: 1) Answer EM/F1 to evaluate the answer span prediction, 2) Support EM/F1 to evaluate the supporting facts prediction, and 3) Joint EM/F1 to combine the two previous ones.

(p29.1) To have an accurate comparison, the year of publications, the technique, along with the results, for Distracter setting and Fullwiki setting have been presented in this table 1 and 2. In the distractor setting, the best result is for AMGN [39] which is a graph-based model. Although the results are very diverse due to a large number of evaluation metrics, in general, graph-based methods have achieved relatively better results, and these good results were a motivation for great attention to the graph-based technique. But S2G [14] which is a graph-free model in 2021 has achieved a comparable result to the best graph-based model ( Table 1). A few models have published the result for the Fullwiki setting due to the difficulty of this setting, and among them, HGN [48], which is a graph-based model, has achieved the best result (Table 2).   "
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8,s31,Model,['p31.0'],['Year Technique Accuracy (%) Test set Dev set MHQA-GRN [40] 2018 Graph-based 65.4 62.8 MHPGM [29] 2018 Reasoning-based 57.9 58.5 DRL-GRC [36] 2019 Neural ---65.12 BAG [43] 2019 Graph-based 69 66.5 Entity-GCN [42] 2019 Graph-based 71 '],Year Technique Accuracy (%) Test set Dev set MHQA-GRN [40] 2018 Graph-based 65.4 62.8 MHPGM [29] 2018 Reasoning-based 57.9 58.5 DRL-GRC [36] 2019 Neural ---65.12 BAG [43] 2019 Graph-based 69 66.5 Entity-GCN [42] 2019 Graph-based 71 ,(p31.0) Year Technique Accuracy (%) Test set Dev set MHQA-GRN [40] 2018 Graph-based 65.4 62.8 MHPGM [29] 2018 Reasoning-based 57.9 58.5 DRL-GRC [36] 2019 Neural ---65.12 BAG [43] 2019 Graph-based 69 66.5 Entity-GCN [42] 2019 Graph-based 71 ,"[['b28', 'b39', 'b42', 'b35', 'b41']]","[['b28', 'b39', 'b42', 'b35', 'b41']]",5,"1. Year Technique Accuracy (%) Test set Dev set MHQA-GRN [40] 2018 Graph-based 65.4 62.8 MHPGM [29] 2018 Reasoning-based 57.9 58.5 DRL-GRC [36] 2019 Neural ---65.12 BAG [43]
2. 2019 Graph-based 69 66.5 Entity-GCN [42] 2019 Graph-based 71","What are the key differences between graph-based and reasoning-based techniques in multi-hop machine reading comprehension as evidenced by their performance metrics?
How does the accuracy of neural-based approaches in multi-hop machine reading comprehension compare to graph-based methods according to recent studies?
What advancements in graph-based multi-hop machine reading comprehension techniques were observed between 2018 and 2019?
Can the development set performance be considered a reliable indicator of test set performance in multi-hop machine reading comprehension models?
What factors might contribute to the variance in accuracy between different multi-hop machine reading comprehension approaches as shown in the survey?","1. What are the key differences between graph-based and reasoning-based techniques in multi-hop machine reading comprehension as evidenced by their performance metrics?
2. How does the accuracy of neural-based approaches in multi-hop machine reading comprehension compare to graph-based methods according to recent studies?
3. What advancements in graph-based multi-hop machine reading comprehension techniques were observed between 2018 and 2019?
4. Can the development set performance be considered a reliable indicator of test set performance in multi-hop machine reading comprehension models?
5. What factors might contribute to the variance in accuracy between different multi-hop machine reading comprehension approaches as shown in the survey?",None,,"Questions:

1. What are the key differences between graph-based and reasoning-based techniques in multi-hop machine reading comprehension as evidenced by their performance metrics?
2. How does the accuracy of neural-based approaches in multi-hop machine reading comprehension compare to graph-based methods according to recent studies?
3. What advancements in graph-based multi-hop machine reading comprehension techniques were observed between 2018 and 2019?
4. Can the development set performance be considered a reliable indicator of test set performance in multi-hop machine reading comprehension models?
5. What factors might contribute to the variance in accuracy between different multi-hop machine reading comprehension approaches as shown in the survey?

Answer:

(p31.0) Year Technique Accuracy (%) Test set Dev set MHQA-GRN [40] 2018 Graph-based 65.4 62.8 MHPGM [29] 2018 Reasoning-based 57.9 58.5 DRL-GRC [36] 2019 Neural ---65.12 BAG [43] 2019 Graph-based 69 66.5 Entity-GCN [42] 2019 Graph-based 71 ","Questions:



Answer:

(p31.0) Year Technique Accuracy (%) Test set Dev set MHQA-GRN [40] 2018 Graph-based 65.4 62.8 MHPGM [29] 2018 Reasoning-based 57.9 58.5 DRL-GRC [36] 2019 Neural ---65.12 BAG [43] 2019 Graph-based 69 66.5 Entity-GCN [42] 2019 Graph-based 71 "
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s1,Parallel Architectures,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5', 'p1.6', 'p1.7', 'p1.8']","['As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers. In this case, there is no dependency other than layer sharing among tasks. Therefore, there is no constraint on the order of training samples from each task. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks. Fig. 1 illustrates different forms of parallel architectures.', '2.1.1 Vanilla Tree-like Architectures. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches). A shallow trunk can be simply the word representation layer [108] while a deep trunk can be the entire model except output layers. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153]. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.', 'The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task. A solution is to equip the shared trunk with task-specific encoders [47,58,137]. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages. Another way is to make different groups of tasks share different parts of the trunk [37,79,89]. Moreover, this idea can be applied to the decoder. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.', '2.1.2 Parallel Feature Fusion. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task. As shown in Fig. 1b, such models can use a globally shared encoder represents the corresponding label (ℎ are shared latent representations). The green blocks represent shared parameters and the orange blocks are task-specific parameters. Red circles represent feature fusion mechanism .', 'to produce shared representations that can be used as additional features for each task-specific model [69]. The shared representations can also be used indirectly as the key for attention layers in each task-specific model [118].', 'However, different parts of the shared features are not equally important to each task. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | · | compute the absolute value in an element-wise manner. After that, S is concatenated with task-specific representations to form the input to the output layer.', 'Some models directly integrate features from different tasks. A straightforward solution is to compute a weighted average of feature representations. Here the weights can be computed separately according to the input [61] or by an attention mechanism as in [153] that learns task representations as query keys in attention modules. In addition to weighted average, we can aggregate features via more sophisticated mechanisms. For example, based on the cross-stitch network [81], [22] proposes a gated network where shared and task-specific features are combined by a gating mechanism. A similar gating mechanism is used in [56] to combine features from the primary and the auxiliary task. The SLUICE network [100] learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in [135] to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph . The update gate z determines how much information should be maintained from task and then it emits final outputh for task . Mathematically, the two gates can be formulated as', 'where (·) denotes the sigmoid function and tanh(·) denotes the hyperbolic tangent function. When considering all pairwise directions, the output for each task is given by the sum of each row in', 'Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task. Given tasks, the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .']","As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers. In this case, there is no dependency other than layer sharing among tasks. Therefore, there is no constraint on the order of training samples from each task. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks. Fig. 1 illustrates different forms of parallel architectures.

2.1.1 Vanilla Tree-like Architectures. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches). A shallow trunk can be simply the word representation layer [108] while a deep trunk can be the entire model except output layers. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153]. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.

The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task. A solution is to equip the shared trunk with task-specific encoders [47,58,137]. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages. Another way is to make different groups of tasks share different parts of the trunk [37,79,89]. Moreover, this idea can be applied to the decoder. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.

2.1.2 Parallel Feature Fusion. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task. As shown in Fig. 1b, such models can use a globally shared encoder represents the corresponding label (ℎ are shared latent representations). The green blocks represent shared parameters and the orange blocks are task-specific parameters. Red circles represent feature fusion mechanism .

to produce shared representations that can be used as additional features for each task-specific model [69]. The shared representations can also be used indirectly as the key for attention layers in each task-specific model [118].

However, different parts of the shared features are not equally important to each task. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | · | compute the absolute value in an element-wise manner. After that, S is concatenated with task-specific representations to form the input to the output layer.

Some models directly integrate features from different tasks. A straightforward solution is to compute a weighted average of feature representations. Here the weights can be computed separately according to the input [61] or by an attention mechanism as in [153] that learns task representations as query keys in attention modules. In addition to weighted average, we can aggregate features via more sophisticated mechanisms. For example, based on the cross-stitch network [81], [22] proposes a gated network where shared and task-specific features are combined by a gating mechanism. A similar gating mechanism is used in [56] to combine features from the primary and the auxiliary task. The SLUICE network [100] learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in [135] to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph . The update gate z determines how much information should be maintained from task and then it emits final outputh for task . Mathematically, the two gates can be formulated as

where (·) denotes the sigmoid function and tanh(·) denotes the hyperbolic tangent function. When considering all pairwise directions, the output for each task is given by the sum of each row in

Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task. Given tasks, the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .","(p1.0) As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers. In this case, there is no dependency other than layer sharing among tasks. Therefore, there is no constraint on the order of training samples from each task. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks. Fig. 1 illustrates different forms of parallel architectures.

(p1.1) 2.1.1 Vanilla Tree-like Architectures. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches). A shallow trunk can be simply the word representation layer [108] while a deep trunk can be the entire model except output layers. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153]. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.

(p1.2) The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task. A solution is to equip the shared trunk with task-specific encoders [47,58,137]. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages. Another way is to make different groups of tasks share different parts of the trunk [37,79,89]. Moreover, this idea can be applied to the decoder. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.

(p1.3) 2.1.2 Parallel Feature Fusion. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task. As shown in Fig. 1b, such models can use a globally shared encoder represents the corresponding label (ℎ are shared latent representations). The green blocks represent shared parameters and the orange blocks are task-specific parameters. Red circles represent feature fusion mechanism .

(p1.4) to produce shared representations that can be used as additional features for each task-specific model [69]. The shared representations can also be used indirectly as the key for attention layers in each task-specific model [118].

(p1.5) However, different parts of the shared features are not equally important to each task. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | · | compute the absolute value in an element-wise manner. After that, S is concatenated with task-specific representations to form the input to the output layer.

(p1.6) Some models directly integrate features from different tasks. A straightforward solution is to compute a weighted average of feature representations. Here the weights can be computed separately according to the input [61] or by an attention mechanism as in [153] that learns task representations as query keys in attention modules. In addition to weighted average, we can aggregate features via more sophisticated mechanisms. For example, based on the cross-stitch network [81], [22] proposes a gated network where shared and task-specific features are combined by a gating mechanism. A similar gating mechanism is used in [56] to combine features from the primary and the auxiliary task. The SLUICE network [100] learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in [135] to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph . The update gate z determines how much information should be maintained from task and then it emits final outputh for task . Mathematically, the two gates can be formulated as

(p1.7) where (·) denotes the sigmoid function and tanh(·) denotes the hyperbolic tangent function. When considering all pairwise directions, the output for each task is given by the sum of each row in

(p1.8) Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task. Given tasks, the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .","[[], [None, 'b6', 'b107'], ['b78', 'b136', 'b46', 'b57', 'b36', 'b127', 'b88', 'b64'], [], ['b68', 'b117'], ['b147', 'b38', 'b132', 'b68', 'b81'], ['b134', 'b55', 'b152', 'b60', 'b21', 'b80', 'b99'], [], ['b100', 'b143', 'b135']]","[[], [None, 'b6', 'b107'], ['b78', 'b136', 'b46', 'b57', 'b36', 'b127', 'b88', 'b64'], [], ['b68', 'b117'], ['b147', 'b38', 'b132', 'b68', 'b81'], ['b134', 'b55', 'b152', 'b60', 'b21', 'b80', 'b99'], [], ['b100', 'b143', 'b135']]",28,"1. As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers.
2. In this case, there is no dependency other than layer sharing among tasks.
3. Therefore, there is no constraint on the order of training samples from each task.
4. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks.
5. Fig. 1 illustrates different forms of parallel architectures.
6. 2.1.1 Vanilla Tree-like Architectures.
7. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches).
8. A shallow trunk can be simply the word representation layer [108] while a deep trunk can be the entire model except output layers.
9. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153].
10. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.
11. The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task.
12. A solution is to equip the shared trunk with task-specific encoders [47,58,137].
13. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages.
14. Another way is to make different groups of tasks share different parts of the trunk [37,79,89].
15. Moreover, this idea can be applied to the decoder.
16. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.
17. 2.1.2 Parallel Feature Fusion. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task.
18. As shown in Fig. 1b, such models can use a globally shared encoder represents the corresponding label (i.e., the trunk)0.
19. The green blocks represent shared parameters and the orange blocks are task-specific parameters.
20. Red circles represent feature fusion mechanism .to produce shared representations that can be used as additional features for each task-specific model (i.e., the trunk)3.
21. The shared representations can also be used indirectly as the key for attention layers in each task-specific model (i.e., the trunk)2.
22. However, different parts of the shared features are not equally important to each task.
23. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks.
24. For example, (i.e., the trunk)3 adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate.
25. Similarly, GIRN (i.e., the trunk)4 interleaves hidden states of LSTMs and (i.e., the trunk)5 extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks.
26. The sifted multi-task learning method (i.e., the trunk)6 filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism.
27. The gated shared feature G and attention result A are combined, as in (i.e., the trunk)7, to form the entire shared feature representation S = (i.e., the trunk)8 for each task, where ⊙ represents the element-wise multiplication and | ·
28. | compute the absolute value in an element-wise manner.
29. After that, S is concatenated with task-specific representations to form the input to the output layer.
30. Some models directly integrate features from different tasks.
31. A straightforward solution is to compute a weighted average of feature representations.
32. Here the weights can be computed separately according to the input (i.e., the trunk)9 or by an attention mechanism as in (i.e., the branches)0 that learns task representations as query keys in attention modules.
33. In addition to weighted average, we can aggregate features via more sophisticated mechanisms.
34. For example, based on the cross-stitch network (i.e., the branches)1, (i.e., the branches)2 proposes a gated network where shared and task-specific features are combined by a gating mechanism.
35. A similar gating mechanism is used in (i.e., the branches)3 to combine features from the primary and the auxiliary task.
36. The SLUICE network (i.e., the branches)4 learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks.
37. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in (i.e., the branches)5 to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates.
38. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph .
39. The update gate z determines how much information should be maintained from task and then it emits final outputh for task .
40. Mathematically, the two gates can be formulated aswhere (i.e., the branches)7 denotes the sigmoid function and tanh(i.e., the branches)7 denotes the hyperbolic tangent function.
41. When considering all pairwise directions, the output for each task is given by the sum of each row inTask routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task.
42. Given tasks , the routing network (i.e., the branches)8 splits RNN cells into several shared blocks with task-specific blocks (i.e., the branches)9
43. and then modulates the input to as well as output from each RNN block by a learned weight.
44. MCapsNet [108]0, which brings CapsNet [108]1 to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task.
45. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients [108]2 for capsule in the current layer and capsule in the next layer for task .","What are the key characteristics of vanilla tree-like architectures in multi-task learning models?
How do multi-task learning models manage to share knowledge among tasks without explicit parameter sharing?
What mechanisms are employed by multi-task learning models to selectively utilize shared features and minimize inter-task interference?
How do parallel feature fusion techniques in multi-task learning models enhance task-specific model performance?
What role does task routing play in the context of multi-task learning, and how does it differ from dynamic routing in CapsNet?","1. What are the key characteristics of vanilla tree-like architectures in multi-task learning models?
2. How do multi-task learning models manage to share knowledge among tasks without explicit parameter sharing?
3. What mechanisms are employed by multi-task learning models to selectively utilize shared features and minimize inter-task interference?
4. How do parallel feature fusion techniques in multi-task learning models enhance task-specific model performance?
5. What role does task routing play in the context of multi-task learning, and how does it differ from dynamic routing in CapsNet?","# What are the key strategies and mechanisms for sharing and fusing features in Multi-Task Learning (MTL) models within parallel architectures?
  - Vanilla Tree-like Architectures: Utilizes a shared base feature extractor (trunk) with task-specific encoders and output layers (branches), aiming for hard sharing or multi-head architecture to force all tasks to share the same low-level feature representation. Enhancements include task-specific encoders for the shared trunk and different groups of tasks sharing different parts of the trunk or decoder.
  - Parallel Feature Fusion: Actively combines shared and task-specific features to form representations for each task through mechanisms like global memory modules in LSTMs, GIRN's interleaving hidden states, and gated shared feature fusion. This includes direct integration of features from different tasks using weighted averages or more sophisticated mechanisms like gated networks and the SLUICE network's task relatedness matrix.
  - Task Routing: Differentiates the paths samples take through the model based on their task, using techniques like splitting RNN cells into shared and task-specific blocks or employing task routing in MCapsNet to build different feature spaces for each task, thereby customizing the feature fusion process to the requirements of each task.",1. What are the key strategies and mechanisms for sharing and fusing features in Multi-Task Learning (MTL) models within parallel architectures?,"Questions:

1. What are the key characteristics of vanilla tree-like architectures in multi-task learning models?
2. How do multi-task learning models manage to share knowledge among tasks without explicit parameter sharing?
3. What mechanisms are employed by multi-task learning models to selectively utilize shared features and minimize inter-task interference?
4. How do parallel feature fusion techniques in multi-task learning models enhance task-specific model performance?
5. What role does task routing play in the context of multi-task learning, and how does it differ from dynamic routing in CapsNet?

Answer:

(p1.0) As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers. In this case, there is no dependency other than layer sharing among tasks. Therefore, there is no constraint on the order of training samples from each task. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks. Fig. 1 illustrates different forms of parallel architectures.

(p1.1) 2.1.1 Vanilla Tree-like Architectures. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches). A shallow trunk can be simply the word representation layer [108] while a deep trunk can be the entire model except output layers. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153]. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.

(p1.2) The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task. A solution is to equip the shared trunk with task-specific encoders [47,58,137]. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages. Another way is to make different groups of tasks share different parts of the trunk [37,79,89]. Moreover, this idea can be applied to the decoder. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.

(p1.3) 2.1.2 Parallel Feature Fusion. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task. As shown in Fig. 1b, such models can use a globally shared encoder represents the corresponding label (ℎ are shared latent representations). The green blocks represent shared parameters and the orange blocks are task-specific parameters. Red circles represent feature fusion mechanism .

(p1.4) to produce shared representations that can be used as additional features for each task-specific model [69]. The shared representations can also be used indirectly as the key for attention layers in each task-specific model [118].

(p1.5) However, different parts of the shared features are not equally important to each task. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | · | compute the absolute value in an element-wise manner. After that, S is concatenated with task-specific representations to form the input to the output layer.

(p1.6) Some models directly integrate features from different tasks. A straightforward solution is to compute a weighted average of feature representations. Here the weights can be computed separately according to the input [61] or by an attention mechanism as in [153] that learns task representations as query keys in attention modules. In addition to weighted average, we can aggregate features via more sophisticated mechanisms. For example, based on the cross-stitch network [81], [22] proposes a gated network where shared and task-specific features are combined by a gating mechanism. A similar gating mechanism is used in [56] to combine features from the primary and the auxiliary task. The SLUICE network [100] learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in [135] to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph . The update gate z determines how much information should be maintained from task and then it emits final outputh for task . Mathematically, the two gates can be formulated as

(p1.7) where (·) denotes the sigmoid function and tanh(·) denotes the hyperbolic tangent function. When considering all pairwise directions, the output for each task is given by the sum of each row in

(p1.8) Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task. Given tasks, the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .","Questions:

1. What are the key strategies and mechanisms for sharing and fusing features in Multi-Task Learning (MTL) models within parallel architectures?

Answer:

(p1.0) As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers. In this case, there is no dependency other than layer sharing among tasks. Therefore, there is no constraint on the order of training samples from each task. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks. Fig. 1 illustrates different forms of parallel architectures.

(p1.1) 2.1.1 Vanilla Tree-like Architectures. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches). A shallow trunk can be simply the word representation layer [108] while a deep trunk can be the entire model except output layers. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153]. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.

(p1.2) The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task. A solution is to equip the shared trunk with task-specific encoders [47,58,137]. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages. Another way is to make different groups of tasks share different parts of the trunk [37,79,89]. Moreover, this idea can be applied to the decoder. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.

(p1.3) 2.1.2 Parallel Feature Fusion. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task. As shown in Fig. 1b, such models can use a globally shared encoder represents the corresponding label (ℎ are shared latent representations). The green blocks represent shared parameters and the orange blocks are task-specific parameters. Red circles represent feature fusion mechanism .

(p1.4) to produce shared representations that can be used as additional features for each task-specific model [69]. The shared representations can also be used indirectly as the key for attention layers in each task-specific model [118].

(p1.5) However, different parts of the shared features are not equally important to each task. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | · | compute the absolute value in an element-wise manner. After that, S is concatenated with task-specific representations to form the input to the output layer.

(p1.6) Some models directly integrate features from different tasks. A straightforward solution is to compute a weighted average of feature representations. Here the weights can be computed separately according to the input [61] or by an attention mechanism as in [153] that learns task representations as query keys in attention modules. In addition to weighted average, we can aggregate features via more sophisticated mechanisms. For example, based on the cross-stitch network [81], [22] proposes a gated network where shared and task-specific features are combined by a gating mechanism. A similar gating mechanism is used in [56] to combine features from the primary and the auxiliary task. The SLUICE network [100] learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in [135] to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph . The update gate z determines how much information should be maintained from task and then it emits final outputh for task . Mathematically, the two gates can be formulated as

(p1.7) where (·) denotes the sigmoid function and tanh(·) denotes the hyperbolic tangent function. When considering all pairwise directions, the output for each task is given by the sum of each row in

(p1.8) Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task. Given tasks, the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task ."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s2,Supervision at Different,"['p2.0', 'p2.1']","['Feature Levels. Models using the parallel architecture handle multiple tasks in parallel. These tasks may concern features at different abstraction levels. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.', 'In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.']","Feature Levels. Models using the parallel architecture handle multiple tasks in parallel. These tasks may concern features at different abstraction levels. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.

In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.","(p2.0) Feature Levels. Models using the parallel architecture handle multiple tasks in parallel. These tasks may concern features at different abstraction levels. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.

(p2.1) In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.","[['b101', 'b10', 'b108', 'b19', 'b56', 'b32', 'b95', 'b27', 'b92', 'b79'], ['b84', 'b15', 'b67', 'b153', 'b96', 'b61']]","[['b101', 'b10', 'b108', 'b19', 'b56', 'b32', 'b95', 'b27', 'b92', 'b79'], ['b84', 'b15', 'b67', 'b153', 'b96', 'b61']]",16,"1. Feature Levels. Models using the parallel architecture handle multiple tasks in parallel.
2. These tasks may concern features at different abstraction levels.
3. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level.
4. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c.
5. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers.
6. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features.
7. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified.
8. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.
9. In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful.
10. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154].
11. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task.
12. [28,57]0 adds attention-level supervision to improve consistency of the two primary language generation tasks.
13. [28,57]1 minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.","How do parallel architecture models in multi-task learning handle tasks at different abstraction levels in NLP?
What is the rationale behind providing supervision signals at different depths in a multi-task learning model for NLP tasks?
How do auxiliary tasks at different levels contribute to the improvement of a primary task's performance in multi-task learning settings?
In what ways do skip connections enhance the performance of higher-level tasks in multi-task learning models?
How does attention-level supervision contribute to the consistency and performance of primary language generation tasks in multi-task learning?","1. How do parallel architecture models in multi-task learning handle tasks at different abstraction levels in NLP?
2. What is the rationale behind providing supervision signals at different depths in a multi-task learning model for NLP tasks?
3. How do auxiliary tasks at different levels contribute to the improvement of a primary task's performance in multi-task learning settings?
4. In what ways do skip connections enhance the performance of higher-level tasks in multi-task learning models?
5. How does attention-level supervision contribute to the consistency and performance of primary language generation tasks in multi-task learning?",None,,"Questions:

1. How do parallel architecture models in multi-task learning handle tasks at different abstraction levels in NLP?
2. What is the rationale behind providing supervision signals at different depths in a multi-task learning model for NLP tasks?
3. How do auxiliary tasks at different levels contribute to the improvement of a primary task's performance in multi-task learning settings?
4. In what ways do skip connections enhance the performance of higher-level tasks in multi-task learning models?
5. How does attention-level supervision contribute to the consistency and performance of primary language generation tasks in multi-task learning?

Answer:

(p2.0) Feature Levels. Models using the parallel architecture handle multiple tasks in parallel. These tasks may concern features at different abstraction levels. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.

(p2.1) In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.","Questions:



Answer:

(p2.0) Feature Levels. Models using the parallel architecture handle multiple tasks in parallel. These tasks may concern features at different abstraction levels. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.

(p2.1) In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s3,Hierarchical Architectures,"['p3.0', 'p3.1']","['The hierarchical architecture considers hierarchical relationships among multiple tasks. The features and output of one task can be used by another task as an extra input or additional control signals. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures. Fig. 2 illustrates different hierarchical architectures.', '2.2.1 Hierarchical Feature Fusion. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum. [124] fuses topic features of different roles into the main model via a gating mechanism. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection.']","The hierarchical architecture considers hierarchical relationships among multiple tasks. The features and output of one task can be used by another task as an extra input or additional control signals. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures. Fig. 2 illustrates different hierarchical architectures.

2.2.1 Hierarchical Feature Fusion. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum. [124] fuses topic features of different roles into the main model via a gating mechanism. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection.","(p3.0) The hierarchical architecture considers hierarchical relationships among multiple tasks. The features and output of one task can be used by another task as an extra input or additional control signals. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures. Fig. 2 illustrates different hierarchical architectures.

(p3.1) 2.2.1 Hierarchical Feature Fusion. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum. [124] fuses topic features of different roles into the main model via a gating mechanism. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection.","[[], ['b23', 'b67', 'b120', 'b123']]","[[], ['b23', 'b67', 'b120', 'b123']]",4,"1. The hierarchical architecture considers hierarchical relationships among multiple tasks.
2. The features and output of one task can be used by another task as an extra input or additional control signals.
3. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures.
4. Fig. 2 illustrates different hierarchical architectures.
5. 2.2.1 Hierarchical Feature Fusion.
6. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features.
7. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism.
8. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task.
9. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum.
10. [124] fuses topic features of different roles into the main model via a gating mechanism.
11. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection.","How does hierarchical architecture in multi-task learning utilize relationships among tasks?
What distinguishes hierarchical feature fusion from parallel feature fusion in multi-task learning?
How is attention mechanism applied in hierarchical feature fusion for Twitter demographic classification?
In what way do hierarchical architectures incorporate different types of features for knowledge base question answering?
What role do gating and inter-modal attention mechanisms play in enhancing multi-task learning models?","1. How does hierarchical architecture in multi-task learning utilize relationships among tasks?
2. What distinguishes hierarchical feature fusion from parallel feature fusion in multi-task learning?
3. How is attention mechanism applied in hierarchical feature fusion for Twitter demographic classification?
4. In what way do hierarchical architectures incorporate different types of features for knowledge base question answering?
5. What role do gating and inter-modal attention mechanisms play in enhancing multi-task learning models?",None,,"Questions:

1. How does hierarchical architecture in multi-task learning utilize relationships among tasks?
2. What distinguishes hierarchical feature fusion from parallel feature fusion in multi-task learning?
3. How is attention mechanism applied in hierarchical feature fusion for Twitter demographic classification?
4. In what way do hierarchical architectures incorporate different types of features for knowledge base question answering?
5. What role do gating and inter-modal attention mechanisms play in enhancing multi-task learning models?

Answer:

(p3.0) The hierarchical architecture considers hierarchical relationships among multiple tasks. The features and output of one task can be used by another task as an extra input or additional control signals. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures. Fig. 2 illustrates different hierarchical architectures.

(p3.1) 2.2.1 Hierarchical Feature Fusion. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum. [124] fuses topic features of different roles into the main model via a gating mechanism. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection.","Questions:



Answer:

(p3.0) The hierarchical architecture considers hierarchical relationships among multiple tasks. The features and output of one task can be used by another task as an extra input or additional control signals. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures. Fig. 2 illustrates different hierarchical architectures.

(p3.1) 2.2.1 Hierarchical Feature Fusion. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum. [124] fuses topic features of different roles into the main model via a gating mechanism. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s4,Hierarchical,"['p4.0', 'p4.1', 'p4.2', 'p4.3', 'p4.4']","['Pipeline. Instead of aggregating features from different tasks as in feature fusion architectures, pipeline architectures treat the output of a task as an extra input of another task and form a hierarchical pipeline between tasks. The extra input can be used directly as input features or used indirectly as control signals to enhance the performance of other tasks. In this section, we refer to output as the final result from a task, including the final output distribution and hidden states before the last output layer. We further divide hierarchical pipeline architectures into hierarchical feature pipeline and hierarchical signal pipeline.', 'In hierarchical feature pipeline, the output of one task is used as extra features for another task. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks. For example, [14] feeds the output of a question-review pair recognition model to the question answering model. [44] feeds the output of aspect term extraction to aspect-term sentiment classification. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.', 'Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.', 'The direction of hierarchical pipelines is not necessarily always from low-level tasks to high-level tasks. For example, in [1], the outputs of word-level tasks are fed to the char-level primary task. [99] feeds the output of more general classification models to more specific classification models during training, and the more general classification results are used to optimize beam search of more specific models at test time.', 'In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50]. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction. The outputs of a task can also be used for post-processing. For instance, [145] uses the output of NER to help extract multi-token entities.']","Pipeline. Instead of aggregating features from different tasks as in feature fusion architectures, pipeline architectures treat the output of a task as an extra input of another task and form a hierarchical pipeline between tasks. The extra input can be used directly as input features or used indirectly as control signals to enhance the performance of other tasks. In this section, we refer to output as the final result from a task, including the final output distribution and hidden states before the last output layer. We further divide hierarchical pipeline architectures into hierarchical feature pipeline and hierarchical signal pipeline.

In hierarchical feature pipeline, the output of one task is used as extra features for another task. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks. For example, [14] feeds the output of a question-review pair recognition model to the question answering model. [44] feeds the output of aspect term extraction to aspect-term sentiment classification. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.

Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.

The direction of hierarchical pipelines is not necessarily always from low-level tasks to high-level tasks. For example, in [1], the outputs of word-level tasks are fed to the char-level primary task. [99] feeds the output of more general classification models to more specific classification models during training, and the more general classification results are used to optimize beam search of more specific models at test time.

In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50]. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction. The outputs of a task can also be used for post-processing. For instance, [145] uses the output of NER to help extract multi-token entities.","(p4.0) Pipeline. Instead of aggregating features from different tasks as in feature fusion architectures, pipeline architectures treat the output of a task as an extra input of another task and form a hierarchical pipeline between tasks. The extra input can be used directly as input features or used indirectly as control signals to enhance the performance of other tasks. In this section, we refer to output as the final result from a task, including the final output distribution and hidden states before the last output layer. We further divide hierarchical pipeline architectures into hierarchical feature pipeline and hierarchical signal pipeline.

(p4.1) In hierarchical feature pipeline, the output of one task is used as extra features for another task. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks. For example, [14] feeds the output of a question-review pair recognition model to the question answering model. [44] feeds the output of aspect term extraction to aspect-term sentiment classification. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.

(p4.2) Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.

(p4.3) The direction of hierarchical pipelines is not necessarily always from low-level tasks to high-level tasks. For example, in [1], the outputs of word-level tasks are fed to the char-level primary task. [99] feeds the output of more general classification models to more specific classification models during training, and the more general classification results are used to optimize beam search of more specific models at test time.

(p4.4) In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50]. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction. The outputs of a task can also be used for post-processing. For instance, [145] uses the output of NER to help extract multi-token entities.","[[], ['b151', 'b138', 'b109', 'b13', 'b43'], ['b111', 'b42', 'b32', 'b29', 'b106'], ['b98', 'b0'], ['b144', 'b105', 'b73', 'b49']]","[[], ['b151', 'b138', 'b109', 'b13', 'b43'], ['b111', 'b42', 'b32', 'b29', 'b106'], ['b98', 'b0'], ['b144', 'b105', 'b73', 'b49']]",16,"1. Pipeline. Instead of aggregating features from different tasks as in feature fusion architectures, pipeline architectures treat the output of a task as an extra input of another task and form a hierarchical pipeline between tasks.
2. The extra input can be used directly as input features or used indirectly as control signals to enhance the performance of other tasks.
3. In this section, we refer to output as the final result from a task, including the final output distribution and hidden states before the last output layer.
4. We further divide hierarchical pipeline architectures into hierarchical feature pipeline and hierarchical signal pipeline.
5. In hierarchical feature pipeline, the output of one task is used as extra features for another task.
6. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks.
7. For example, [14] feeds the output of a question-review pair recognition model to the question answering model.
8. [44] feeds the output of aspect term extraction to aspect-term sentiment classification.
9. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations.
10. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections.
11. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.
12. Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships.
13. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction.
14. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks.
15. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task.
16. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task.
17. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.
18. The direction of hierarchical pipelines is not necessarily always from low-level tasks to high-level tasks.
19. For example, in [44]0, the outputs of word-level tasks are fed to the char-level primary task.
20. [44]1 feeds the output of more general classification models to more specific classification models during training, and the more general classification results are used to optimize beam search of more specific models at test time.
21. In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks.
22. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [44]2.
23. For the hashtag segmentation task, [44]3 first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features.
24. In [44]4, the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction.
25. The outputs of a task can also be used for post-processing.
26. For instance, [44]5 uses the output of NER to help extract multi-token entities.","How does the hierarchical feature pipeline architecture utilize the output of one task to enhance the performance of another in multi-task learning?
What are the benefits of using hierarchical signal pipeline over direct feature fusion in multi-task learning models?
In what ways do residual and cascade connections differ in their contribution to multi-task learning architectures?
How can the output of a high-level task be utilized in a lower-level task within a hierarchical pipeline architecture?
What role does the predicted probability of a task play in the hierarchical signal pipeline for enhancing document-level classification or entity disambiguation?","1. How does the hierarchical feature pipeline architecture utilize the output of one task to enhance the performance of another in multi-task learning?
2. What are the benefits of using hierarchical signal pipeline over direct feature fusion in multi-task learning models?
3. In what ways do residual and cascade connections differ in their contribution to multi-task learning architectures?
4. How can the output of a high-level task be utilized in a lower-level task within a hierarchical pipeline architecture?
5. What role does the predicted probability of a task play in the hierarchical signal pipeline for enhancing document-level classification or entity disambiguation?",None,,"Questions:

1. How does the hierarchical feature pipeline architecture utilize the output of one task to enhance the performance of another in multi-task learning?
2. What are the benefits of using hierarchical signal pipeline over direct feature fusion in multi-task learning models?
3. In what ways do residual and cascade connections differ in their contribution to multi-task learning architectures?
4. How can the output of a high-level task be utilized in a lower-level task within a hierarchical pipeline architecture?
5. What role does the predicted probability of a task play in the hierarchical signal pipeline for enhancing document-level classification or entity disambiguation?

Answer:

(p4.0) Pipeline. Instead of aggregating features from different tasks as in feature fusion architectures, pipeline architectures treat the output of a task as an extra input of another task and form a hierarchical pipeline between tasks. The extra input can be used directly as input features or used indirectly as control signals to enhance the performance of other tasks. In this section, we refer to output as the final result from a task, including the final output distribution and hidden states before the last output layer. We further divide hierarchical pipeline architectures into hierarchical feature pipeline and hierarchical signal pipeline.

(p4.1) In hierarchical feature pipeline, the output of one task is used as extra features for another task. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks. For example, [14] feeds the output of a question-review pair recognition model to the question answering model. [44] feeds the output of aspect term extraction to aspect-term sentiment classification. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.

(p4.2) Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.

(p4.3) The direction of hierarchical pipelines is not necessarily always from low-level tasks to high-level tasks. For example, in [1], the outputs of word-level tasks are fed to the char-level primary task. [99] feeds the output of more general classification models to more specific classification models during training, and the more general classification results are used to optimize beam search of more specific models at test time.

(p4.4) In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50]. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction. The outputs of a task can also be used for post-processing. For instance, [145] uses the output of NER to help extract multi-token entities.","Questions:



Answer:

(p4.0) Pipeline. Instead of aggregating features from different tasks as in feature fusion architectures, pipeline architectures treat the output of a task as an extra input of another task and form a hierarchical pipeline between tasks. The extra input can be used directly as input features or used indirectly as control signals to enhance the performance of other tasks. In this section, we refer to output as the final result from a task, including the final output distribution and hidden states before the last output layer. We further divide hierarchical pipeline architectures into hierarchical feature pipeline and hierarchical signal pipeline.

(p4.1) In hierarchical feature pipeline, the output of one task is used as extra features for another task. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks. For example, [14] feeds the output of a question-review pair recognition model to the question answering model. [44] feeds the output of aspect term extraction to aspect-term sentiment classification. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.

(p4.2) Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.

(p4.3) The direction of hierarchical pipelines is not necessarily always from low-level tasks to high-level tasks. For example, in [1], the outputs of word-level tasks are fed to the char-level primary task. [99] feeds the output of more general classification models to more specific classification models during training, and the more general classification results are used to optimize beam search of more specific models at test time.

(p4.4) In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50]. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction. The outputs of a task can also be used for post-processing. For instance, [145] uses the output of NER to help extract multi-token entities."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s6,Modular Architectures,"['p6.0', 'p6.1', 'p6.2', 'p6.3', 'p6.4']","['The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules. The shared modules learn shared features from multiple tasks. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios. On the other hand, task-specific modules learn features that are specific to a certain task. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.', 'The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did. [1] shares word and character embedding matrices and combines them differently for different tasks. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT). Modular designs are also widely used in multi-lingual tasks. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks. Shared embeddings can be used alongside task-specific embeddings [59,138] as well. In addition to word embeddings, [147] shares label embeddings between tasks. Researchers have also developed modular architectures at a finer granularity. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer. [92] creates encoder modules on different levels, including task level, task group level, and universal level.', 'When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task. While this approach usually attains good performance, it poses heavy computational and storage costs. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters (adapters). [113] adds task-specific Projected Attention Layers (PALs) in parallel with self-attention operatioins in a pre-trained BERT model. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed. In Multiple ADapters for Cross-lingual transfer (MAD-X) [94], the model is decomposed into PALs (a) Bert and PALs [113] Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch. Further, task adaptation modules can also be dynamically generated by a meta-network. Hypergrid transformer [117] scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors as', 'where L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix. Differently, Conditionally Adaptive MTL (CA-MTL) [95] implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as', ') is a diagonal block matrix consisting of learnable linear transformations over z . Therefore, M(z ) injects task-specific bias into the attention map in the self-attention mechanism. Similar adaptation operations are used in the input alignment and layer normalization as well. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters.']","The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules. The shared modules learn shared features from multiple tasks. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios. On the other hand, task-specific modules learn features that are specific to a certain task. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.

The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did. [1] shares word and character embedding matrices and combines them differently for different tasks. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT). Modular designs are also widely used in multi-lingual tasks. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks. Shared embeddings can be used alongside task-specific embeddings [59,138] as well. In addition to word embeddings, [147] shares label embeddings between tasks. Researchers have also developed modular architectures at a finer granularity. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer. [92] creates encoder modules on different levels, including task level, task group level, and universal level.

When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task. While this approach usually attains good performance, it poses heavy computational and storage costs. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters (adapters). [113] adds task-specific Projected Attention Layers (PALs) in parallel with self-attention operatioins in a pre-trained BERT model. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed. In Multiple ADapters for Cross-lingual transfer (MAD-X) [94], the model is decomposed into PALs (a) Bert and PALs [113] Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch. Further, task adaptation modules can also be dynamically generated by a meta-network. Hypergrid transformer [117] scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors as

where L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix. Differently, Conditionally Adaptive MTL (CA-MTL) [95] implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as

) is a diagonal block matrix consisting of learnable linear transformations over z . Therefore, M(z ) injects task-specific bias into the attention map in the self-attention mechanism. Similar adaptation operations are used in the input alignment and layer normalization as well. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters.","(p6.0) The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules. The shared modules learn shared features from multiple tasks. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios. On the other hand, task-specific modules learn features that are specific to a certain task. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.

(p6.1) The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did. [1] shares word and character embedding matrices and combines them differently for different tasks. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT). Modular designs are also widely used in multi-lingual tasks. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks. Shared embeddings can be used alongside task-specific embeddings [59,138] as well. In addition to word embeddings, [147] shares label embeddings between tasks. Researchers have also developed modular architectures at a finer granularity. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer. [92] creates encoder modules on different levels, including task level, task group level, and universal level.

(p6.2) When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task. While this approach usually attains good performance, it poses heavy computational and storage costs. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters (adapters). [113] adds task-specific Projected Attention Layers (PALs) in parallel with self-attention operatioins in a pre-trained BERT model. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed. In Multiple ADapters for Cross-lingual transfer (MAD-X) [94], the model is decomposed into PALs (a) Bert and PALs [113] Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch. Further, task adaptation modules can also be dynamically generated by a meta-network. Hypergrid transformer [117] scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors as

(p6.3) where L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix. Differently, Conditionally Adaptive MTL (CA-MTL) [95] implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as

(p6.4) ) is a diagonal block matrix consisting of learnable linear transformations over z . Therefore, M(z ) injects task-specific bias into the attention map in the self-attention mechanism. Similar adaptation operations are used in the input alignment and layer normalization as well. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters.","[[], ['b57', 'b48', 'b0', 'b102', 'b23', 'b155', 'b118', 'b58', 'b91', 'b146', 'b137'], ['b112', 'b93', 'b116'], ['b94'], []]","[[], ['b57', 'b48', 'b0', 'b102', 'b23', 'b155', 'b118', 'b58', 'b91', 'b146', 'b137'], ['b112', 'b93', 'b116'], ['b94'], []]",15,"1. The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules.
2. The shared modules learn shared features from multiple tasks.
3. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios.
4. On the other hand, task-specific modules learn features that are specific to a certain task.
5. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.
6. The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1.
7. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did.
8. [1] shares word and character embedding matrices and combines them differently for different tasks.
9. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT).
10. Modular designs are also widely used in multi-lingual tasks.
11. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks.
12. Shared embeddings can be used alongside task-specific embeddings [59,138] as well.
13. In addition to word embeddings, [147] shares label embeddings between tasks.
14. Researchers have also developed modular architectures at a finer granularity.
15. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation.
16. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer.
17. [92] creates encoder modules on different levels, including task level, task group level, and universal level.
18. When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task.
19. While this approach usually attains good performance, it poses heavy computational and storage costs.
20. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model.
21. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters [1]0.
22. [1]6 adds task-specific Projected Attention Layers [1]2 in parallel with self-attention operatioins in a pre-trained BERT model.
23. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed.
24. In Multiple ADapters for Cross-lingual transfer [1]3 [1]4, the model is decomposed into PALs [1]5 Bert and PALs [1]6 Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings.
25. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch.
26. Further, task adaptation modules can also be dynamically generated by a meta-network.
27. Hypergrid transformer [1]7 scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors aswhere L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix.
28. Differently, Conditionally Adaptive MTL [1]8 [1]9 implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as) is a diagonal block matrix consisting of learnable linear transformations over z .
29. Therefore, M[103]0 injects task-specific bias into the attention map in the self-attention mechanism.
30. Similar adaptation operations are used in the input alignment and layer normalization as well.
31. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters.","What are the benefits of using shared modules in multi-task learning (MTL) models, especially in low-resource scenarios?
How do task-specific modules in MTL models prevent overfitting compared to shared modules?
What are some examples of how modular architectures have been applied to multi-lingual tasks in natural language processing?
How do multi-task adapters, such as Projected Attention Layers (PALs), enhance the adaptability of pre-trained models to multiple tasks without significant increases in computational and storage costs?
In what ways do Hypergrid transformer and Conditionally Adaptive MTL (CA-MTL) models achieve task adaptation, and how do they compare in performance to single-task fine-tuned models on multi-task benchmark datasets?","1. What are the benefits of using shared modules in multi-task learning (MTL) models, especially in low-resource scenarios?
2. How do task-specific modules in MTL models prevent overfitting compared to shared modules?
3. What are some examples of how modular architectures have been applied to multi-lingual tasks in natural language processing?
4. How do multi-task adapters, such as Projected Attention Layers (PALs), enhance the adaptability of pre-trained models to multiple tasks without significant increases in computational and storage costs?
5. In what ways do Hypergrid transformer and Conditionally Adaptive MTL (CA-MTL) models achieve task adaptation, and how do they compare in performance to single-task fine-tuned models on multi-task benchmark datasets?",None,,"Questions:

1. What are the benefits of using shared modules in multi-task learning (MTL) models, especially in low-resource scenarios?
2. How do task-specific modules in MTL models prevent overfitting compared to shared modules?
3. What are some examples of how modular architectures have been applied to multi-lingual tasks in natural language processing?
4. How do multi-task adapters, such as Projected Attention Layers (PALs), enhance the adaptability of pre-trained models to multiple tasks without significant increases in computational and storage costs?
5. In what ways do Hypergrid transformer and Conditionally Adaptive MTL (CA-MTL) models achieve task adaptation, and how do they compare in performance to single-task fine-tuned models on multi-task benchmark datasets?

Answer:

(p6.0) The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules. The shared modules learn shared features from multiple tasks. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios. On the other hand, task-specific modules learn features that are specific to a certain task. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.

(p6.1) The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did. [1] shares word and character embedding matrices and combines them differently for different tasks. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT). Modular designs are also widely used in multi-lingual tasks. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks. Shared embeddings can be used alongside task-specific embeddings [59,138] as well. In addition to word embeddings, [147] shares label embeddings between tasks. Researchers have also developed modular architectures at a finer granularity. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer. [92] creates encoder modules on different levels, including task level, task group level, and universal level.

(p6.2) When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task. While this approach usually attains good performance, it poses heavy computational and storage costs. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters (adapters). [113] adds task-specific Projected Attention Layers (PALs) in parallel with self-attention operatioins in a pre-trained BERT model. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed. In Multiple ADapters for Cross-lingual transfer (MAD-X) [94], the model is decomposed into PALs (a) Bert and PALs [113] Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch. Further, task adaptation modules can also be dynamically generated by a meta-network. Hypergrid transformer [117] scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors as

(p6.3) where L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix. Differently, Conditionally Adaptive MTL (CA-MTL) [95] implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as

(p6.4) ) is a diagonal block matrix consisting of learnable linear transformations over z . Therefore, M(z ) injects task-specific bias into the attention map in the self-attention mechanism. Similar adaptation operations are used in the input alignment and layer normalization as well. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters.","Questions:



Answer:

(p6.0) The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules. The shared modules learn shared features from multiple tasks. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios. On the other hand, task-specific modules learn features that are specific to a certain task. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.

(p6.1) The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did. [1] shares word and character embedding matrices and combines them differently for different tasks. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT). Modular designs are also widely used in multi-lingual tasks. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks. Shared embeddings can be used alongside task-specific embeddings [59,138] as well. In addition to word embeddings, [147] shares label embeddings between tasks. Researchers have also developed modular architectures at a finer granularity. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer. [92] creates encoder modules on different levels, including task level, task group level, and universal level.

(p6.2) When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task. While this approach usually attains good performance, it poses heavy computational and storage costs. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters (adapters). [113] adds task-specific Projected Attention Layers (PALs) in parallel with self-attention operatioins in a pre-trained BERT model. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed. In Multiple ADapters for Cross-lingual transfer (MAD-X) [94], the model is decomposed into PALs (a) Bert and PALs [113] Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch. Further, task adaptation modules can also be dynamically generated by a meta-network. Hypergrid transformer [117] scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors as

(p6.3) where L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix. Differently, Conditionally Adaptive MTL (CA-MTL) [95] implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as

(p6.4) ) is a diagonal block matrix consisting of learnable linear transformations over z . Therefore, M(z ) injects task-specific bias into the attention map in the self-attention mechanism. Similar adaptation operations are used in the input alignment and layer normalization as well. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s7,Generative Adversarial Architectures,"['p7.0', 'p7.1']","['Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.', 'An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.']","Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.

An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.","(p7.0) Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.

(p7.1) An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.","[['b77', 'b118', 'b69', 'b126', 'b137'], ['b97', 'b123']]","[['b77', 'b118', 'b69', 'b126', 'b137'], ['b97', 'b123']]",7,"1. Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision.
2. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones.
3. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images.
4. A similar idea can be used in MTL for NLP tasks.
5. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model.
6. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.
7. An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized.
8. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents.
9. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach.
10. First, a discriminator that rates the quality of candidate answers is trained on labeled samples.
11. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.","How do Generative Adversarial Networks (GANs) contribute to the advancement of generative tasks in computer vision?
In what way does the application of GANs in Multi-Task Learning (MTL) for NLP tasks enhance model performance and robustness?
How does the introduction of a discriminator in MTL for NLP tasks encourage the production of task-invariant features?
What role does unlabeled data play in enhancing the capabilities of generative adversarial architectures in NLP?
How does a self-supervised approach using a discriminator improve the performance of extractive machine reading comprehension models?","1. How do Generative Adversarial Networks (GANs) contribute to the advancement of generative tasks in computer vision?
2. In what way does the application of GANs in Multi-Task Learning (MTL) for NLP tasks enhance model performance and robustness?
3. How does the introduction of a discriminator in MTL for NLP tasks encourage the production of task-invariant features?
4. What role does unlabeled data play in enhancing the capabilities of generative adversarial architectures in NLP?
5. How does a self-supervised approach using a discriminator improve the performance of extractive machine reading comprehension models?","# How do Generative Adversarial Networks (GANs) contribute to Multi-Task Learning (MTL) in Natural Language Processing (NLP)?
  - GANs train a discriminator to distinguish between generated and ground truth instances, which, when applied to NLP, helps in producing more generalized task-invariant features.
  - The adversarial objective in training models with GANs involves optimizing both the feature extractor and the discriminator, enhancing the model's performance and robustness.
  - GANs allow for the effective use of unlabeled data in NLP tasks, improving the quality of document representations and the performance of models like extractive machine reading comprehension through self-supervised adversarial training.",1. How do Generative Adversarial Networks (GANs) contribute to Multi-Task Learning (MTL) in Natural Language Processing (NLP)?,"Questions:

1. How do Generative Adversarial Networks (GANs) contribute to the advancement of generative tasks in computer vision?
2. In what way does the application of GANs in Multi-Task Learning (MTL) for NLP tasks enhance model performance and robustness?
3. How does the introduction of a discriminator in MTL for NLP tasks encourage the production of task-invariant features?
4. What role does unlabeled data play in enhancing the capabilities of generative adversarial architectures in NLP?
5. How does a self-supervised approach using a discriminator improve the performance of extractive machine reading comprehension models?

Answer:

(p7.0) Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.

(p7.1) An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.","Questions:

1. How do Generative Adversarial Networks (GANs) contribute to Multi-Task Learning (MTL) in Natural Language Processing (NLP)?

Answer:

(p7.0) Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.

(p7.1) An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s9,Loss Construction,"['p9.0', 'p9.1', 'p9.2', 'p9.3', 'p9.4', 'p9.5', 'p9.6', 'p9.7', 'p9.8', 'p9.9', 'p9.10']","['The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation. Different tasks may use different types of loss functions. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined as', 'where L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.', 'In this case, an important question is how to assign a proper weight to each task. The simplest way is to set them equally [91,126,156], i.e., = 1 . As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155]. For example, to prevent large datasets from dominating training, [93] sets the weights as', 'where |D | denotes the size of the training dataset for task . The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks as', 'where measures the variance of the training loss for task . In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label. To ensure that a student model could receive enough supervision during knowledge distillation, BAM! [19] combines the supervised loss L with the distillation loss L as', 'where increases linearly from 0 to 1 in the training process. In [112], three tasks are jointly optimized, including the primary essay organization evaluation (OE) task and the auxiliary sentence function identification (SFI) and paragraph function identification (PFI) tasks. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 (i.e., = = 1) and the weight of the OE task is set as', 'where is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller. [85] guides the model to focus on easy tasks by setting weights as', 'where denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models. In [62], the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input. [14] penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features. To learn domain-invariant features, [137] minimizes a distance function (·) between a pair of learned representations from different tasks. Candidates of (·) include the KL divergence, maximum mean discrepancy (MMD), and central moment discrepancy (CMD). Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex. To address this issue, Tchebycheff loss [75] optimizes an MTL model by an ∞ objective, which is formulated as', 'where L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 . The Tchebycheff loss can be combined with adversarial MTL as in [70].', 'Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance. As a remedy, PCGrad [142] directly projects conflicting gradients. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g as', 'Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g . Notice that PCGrad is a special case of GradVac when = 0. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.']","The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation. Different tasks may use different types of loss functions. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined as

where L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.

In this case, an important question is how to assign a proper weight to each task. The simplest way is to set them equally [91,126,156], i.e., = 1 . As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155]. For example, to prevent large datasets from dominating training, [93] sets the weights as

where |D | denotes the size of the training dataset for task . The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks as

where measures the variance of the training loss for task . In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label. To ensure that a student model could receive enough supervision during knowledge distillation, BAM! [19] combines the supervised loss L with the distillation loss L as

where increases linearly from 0 to 1 in the training process. In [112], three tasks are jointly optimized, including the primary essay organization evaluation (OE) task and the auxiliary sentence function identification (SFI) and paragraph function identification (PFI) tasks. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 (i.e., = = 1) and the weight of the OE task is set as

where is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller. [85] guides the model to focus on easy tasks by setting weights as

where denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models. In [62], the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input. [14] penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features. To learn domain-invariant features, [137] minimizes a distance function (·) between a pair of learned representations from different tasks. Candidates of (·) include the KL divergence, maximum mean discrepancy (MMD), and central moment discrepancy (CMD). Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex. To address this issue, Tchebycheff loss [75] optimizes an MTL model by an ∞ objective, which is formulated as

where L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 . The Tchebycheff loss can be combined with adversarial MTL as in [70].

Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance. As a remedy, PCGrad [142] directly projects conflicting gradients. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g as

Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g . Notice that PCGrad is a special case of GradVac when = 0. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.","(p9.0) The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation. Different tasks may use different types of loss functions. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined as

(p9.1) where L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.

(p9.2) In this case, an important question is how to assign a proper weight to each task. The simplest way is to set them equally [91,126,156], i.e., = 1 . As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155]. For example, to prevent large datasets from dominating training, [93] sets the weights as

(p9.3) where |D | denotes the size of the training dataset for task . The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks as

(p9.4) where measures the variance of the training loss for task . In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label. To ensure that a student model could receive enough supervision during knowledge distillation, BAM! [19] combines the supervised loss L with the distillation loss L as

(p9.5) where increases linearly from 0 to 1 in the training process. In [112], three tasks are jointly optimized, including the primary essay organization evaluation (OE) task and the auxiliary sentence function identification (SFI) and paragraph function identification (PFI) tasks. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 (i.e., = = 1) and the weight of the OE task is set as

(p9.6) where is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller. [85] guides the model to focus on easy tasks by setting weights as

(p9.7) where denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models. In [62], the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input. [14] penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features. To learn domain-invariant features, [137] minimizes a distance function (·) between a pair of learned representations from different tasks. Candidates of (·) include the KL divergence, maximum mean discrepancy (MMD), and central moment discrepancy (CMD). Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex. To address this issue, Tchebycheff loss [75] optimizes an MTL model by an ∞ objective, which is formulated as

(p9.8) where L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 . The Tchebycheff loss can be combined with adversarial MTL as in [70].

(p9.9) Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance. As a remedy, PCGrad [142] directly projects conflicting gradients. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g as

(p9.10) Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g . Notice that PCGrad is a special case of GradVac when = 0. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.","[['b140'], ['b113'], ['b155', 'b92', None, 'b90', 'b125'], ['b56', 'b16'], ['b63', 'b18'], ['b111'], ['b84'], ['b74', 'b136', 'b61', 'b13'], ['b69'], ['b141'], ['b128', 'b0']]","[['b140'], ['b113'], ['b155', 'b92', None, 'b90', 'b125'], ['b56', 'b16'], ['b63', 'b18'], ['b111'], ['b84'], ['b74', 'b136', 'b61', 'b13'], ['b69'], ['b141'], ['b128', 'b0']]",21,"1. The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function.
2. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation.
3. Different tasks may use different types of loss functions.
4. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning.
5. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined aswhere L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights.
6. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.
7. In this case, an important question is how to assign a proper weight to each task.
8. The simplest way is to set them equally [91,126,156], i.e., = 1 .
9. As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155].
10. For example, to prevent large datasets from dominating training, [93] sets the weights aswhere |D | denotes the size of the training dataset for task .
11. The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages.
12. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks aswhere measures the variance of the training loss for task .
13. In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label.
14. To ensure that a student model could receive enough supervision during knowledge distillation, BAM!
15. [19] combines the supervised loss L with the distillation loss L aswhere increases linearly from 0 to 1 in the training process.
16. In [112], three tasks are jointly optimized, including the primary essay organization evaluation [114]0 task and the auxiliary sentence function identification [114]1 and paragraph function identification [114]2 tasks.
17. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 [114]3 and the weight of the OE task is set aswhere is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller.
18. [114]4 guides the model to focus on easy tasks by setting weights aswhere denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature.
19. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models.
20. In [114]5, the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input.
21. [114]6 penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features.
22. To learn domain-invariant features, [114]7 minimizes a distance function [114]9 between a pair of learned representations from different tasks.
23. Candidates of [114]9 include the KL divergence, maximum mean discrepancy [91,126,156]0, and central moment discrepancy [91,126,156]1.
24. Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores.
25. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously.
26. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex.
27. To address this issue, Tchebycheff loss [91,126,156]2 optimizes an MTL model by an ∞ objective, which is formulated aswhere L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 .
28. The Tchebycheff loss can be combined with adversarial MTL as in [91,126,156]3.
29. Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients.
30. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance.
31. As a remedy, PCGrad [91,126,156]4 directly projects conflicting gradients.
32. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g asBased on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [91,126,156]5, which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients.
33. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g .
34. Notice that PCGrad is a special case of GradVac when = 0.
35. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.","What are the common strategies for combining loss functions in multi-task learning (MTL) models, and how do they impact model optimization?
How do different weighting strategies for loss functions in MTL models influence the training process and outcomes?
What role does the dynamic adjustment of task weights play in the training of MTL models, and what are some methods for implementing this adjustment?
How do techniques like PCGrad and GradVac address the issue of conflicting gradients in the training of MTL models, and what improvements do they offer?
What are the limitations of linearly combining loss functions in MTL and how does the Tchebycheff loss approach aim to overcome these limitations?","1. What are the common strategies for combining loss functions in multi-task learning (MTL) models, and how do they impact model optimization?
2. How do different weighting strategies for loss functions in MTL models influence the training process and outcomes?
3. What role does the dynamic adjustment of task weights play in the training of MTL models, and what are some methods for implementing this adjustment?
4. How do techniques like PCGrad and GradVac address the issue of conflicting gradients in the training of MTL models, and what improvements do they offer?
5. What are the limitations of linearly combining loss functions in MTL and how does the Tchebycheff loss approach aim to overcome these limitations?",None,,"Questions:

1. What are the common strategies for combining loss functions in multi-task learning (MTL) models, and how do they impact model optimization?
2. How do different weighting strategies for loss functions in MTL models influence the training process and outcomes?
3. What role does the dynamic adjustment of task weights play in the training of MTL models, and what are some methods for implementing this adjustment?
4. How do techniques like PCGrad and GradVac address the issue of conflicting gradients in the training of MTL models, and what improvements do they offer?
5. What are the limitations of linearly combining loss functions in MTL and how does the Tchebycheff loss approach aim to overcome these limitations?

Answer:

(p9.0) The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation. Different tasks may use different types of loss functions. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined as

(p9.1) where L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.

(p9.2) In this case, an important question is how to assign a proper weight to each task. The simplest way is to set them equally [91,126,156], i.e., = 1 . As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155]. For example, to prevent large datasets from dominating training, [93] sets the weights as

(p9.3) where |D | denotes the size of the training dataset for task . The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks as

(p9.4) where measures the variance of the training loss for task . In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label. To ensure that a student model could receive enough supervision during knowledge distillation, BAM! [19] combines the supervised loss L with the distillation loss L as

(p9.5) where increases linearly from 0 to 1 in the training process. In [112], three tasks are jointly optimized, including the primary essay organization evaluation (OE) task and the auxiliary sentence function identification (SFI) and paragraph function identification (PFI) tasks. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 (i.e., = = 1) and the weight of the OE task is set as

(p9.6) where is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller. [85] guides the model to focus on easy tasks by setting weights as

(p9.7) where denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models. In [62], the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input. [14] penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features. To learn domain-invariant features, [137] minimizes a distance function (·) between a pair of learned representations from different tasks. Candidates of (·) include the KL divergence, maximum mean discrepancy (MMD), and central moment discrepancy (CMD). Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex. To address this issue, Tchebycheff loss [75] optimizes an MTL model by an ∞ objective, which is formulated as

(p9.8) where L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 . The Tchebycheff loss can be combined with adversarial MTL as in [70].

(p9.9) Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance. As a remedy, PCGrad [142] directly projects conflicting gradients. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g as

(p9.10) Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g . Notice that PCGrad is a special case of GradVac when = 0. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.","Questions:



Answer:

(p9.0) The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation. Different tasks may use different types of loss functions. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined as

(p9.1) where L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.

(p9.2) In this case, an important question is how to assign a proper weight to each task. The simplest way is to set them equally [91,126,156], i.e., = 1 . As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155]. For example, to prevent large datasets from dominating training, [93] sets the weights as

(p9.3) where |D | denotes the size of the training dataset for task . The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks as

(p9.4) where measures the variance of the training loss for task . In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label. To ensure that a student model could receive enough supervision during knowledge distillation, BAM! [19] combines the supervised loss L with the distillation loss L as

(p9.5) where increases linearly from 0 to 1 in the training process. In [112], three tasks are jointly optimized, including the primary essay organization evaluation (OE) task and the auxiliary sentence function identification (SFI) and paragraph function identification (PFI) tasks. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 (i.e., = = 1) and the weight of the OE task is set as

(p9.6) where is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller. [85] guides the model to focus on easy tasks by setting weights as

(p9.7) where denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models. In [62], the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input. [14] penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features. To learn domain-invariant features, [137] minimizes a distance function (·) between a pair of learned representations from different tasks. Candidates of (·) include the KL divergence, maximum mean discrepancy (MMD), and central moment discrepancy (CMD). Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex. To address this issue, Tchebycheff loss [75] optimizes an MTL model by an ∞ objective, which is formulated as

(p9.8) where L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 . The Tchebycheff loss can be combined with adversarial MTL as in [70].

(p9.9) Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance. As a remedy, PCGrad [142] directly projects conflicting gradients. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g as

(p9.10) Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g . Notice that PCGrad is a special case of GradVac when = 0. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s10,Data Sampling,"['p10.0', 'p10.1', 'p10.2', 'p10.3', 'p10.4', 'p10.5', 'p10.6']","['Machine learning models often suffer from imbalanced data distributions. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.', 'A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.', '(2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,', 'As a generalization of proportional sampling in Eq. (2), for task can take the following form as', 'where 1 is called the sampling temperature [128]. Similar to task loss weights, researchers have proposed various techniques to adjust . When < 1, the divergence of sampling probabilities is reduced. can be viewed as a hyperparameter to be set by users or be changed dynamically during training. For example, the annealed sampling method [113] adjusts as training proceeds. Given a total number of epochs, at epoch is set to', 'In this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference. [128] defines as', 'where 0 and denote initial and maximum values of , respectively. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period.']","Machine learning models often suffer from imbalanced data distributions. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.

A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.

(2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,

As a generalization of proportional sampling in Eq. (2), for task can take the following form as

where 1 is called the sampling temperature [128]. Similar to task loss weights, researchers have proposed various techniques to adjust . When < 1, the divergence of sampling probabilities is reduced. can be viewed as a hyperparameter to be set by users or be changed dynamically during training. For example, the annealed sampling method [113] adjusts as training proceeds. Given a total number of epochs, at epoch is set to

In this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference. [128] defines as

where 0 and denote initial and maximum values of , respectively. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period.","(p10.0) Machine learning models often suffer from imbalanced data distributions. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.

(p10.1) A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.

(p10.2) (2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,

(p10.3) As a generalization of proportional sampling in Eq. (2), for task can take the following form as

(p10.4) where 1 is called the sampling temperature [128]. Similar to task loss weights, researchers have proposed various techniques to adjust . When < 1, the divergence of sampling probabilities is reduced. can be viewed as a hyperparameter to be set by users or be changed dynamically during training. For example, the annealed sampling method [113] adjusts as training proceeds. Given a total number of epochs, at epoch is set to

(p10.5) In this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference. [128] defines as

(p10.6) where 0 and denote initial and maximum values of , respectively. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period.","[[], ['b101'], ['b147'], [], ['b112', 'b127'], ['b127'], []]","[[], ['b101'], ['b147'], [], ['b112', 'b127'], ['b127'], []]",5,"1. Machine learning models often suffer from imbalanced data distributions.
2. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved.
3. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets.
4. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.
5. A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.
6. (2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting.
7. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,As a generalization of proportional sampling in Eq. (2), for task can take the following form aswhere 1 is called the sampling temperature [128].
8. Similar to task loss weights, researchers have proposed various techniques to adjust .
9. When < 1, the divergence of sampling probabilities is reduced.
10. can be viewed as a hyperparameter to be set by users or be changed dynamically during training.
11. For example, the annealed sampling method [113] adjusts as training proceeds.
12. Given a total number of epochs, at epoch is set toIn this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference.
13. [128] defines aswhere 0 and denote initial and maximum values of , respectively.
14. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period.","What are the challenges of handling data imbalance in multi-task learning (MTL) models?
How does proportional sampling work in the context of multi-task learning, and what are its limitations?
What is task-oriented sampling, and how does it differ from proportional sampling in addressing data imbalance in MTL?
How can the sampling temperature parameter influence the distribution of sampling probabilities in multi-task learning models?
What is the annealed sampling method, and how does it adjust the sampling temperature over the training process to reduce inter-task interference?","1. What are the challenges of handling data imbalance in multi-task learning (MTL) models?
2. How does proportional sampling work in the context of multi-task learning, and what are its limitations?
3. What is task-oriented sampling, and how does it differ from proportional sampling in addressing data imbalance in MTL?
4. How can the sampling temperature parameter influence the distribution of sampling probabilities in multi-task learning models?
5. What is the annealed sampling method, and how does it adjust the sampling temperature over the training process to reduce inter-task interference?",None,,"Questions:

1. What are the challenges of handling data imbalance in multi-task learning (MTL) models?
2. How does proportional sampling work in the context of multi-task learning, and what are its limitations?
3. What is task-oriented sampling, and how does it differ from proportional sampling in addressing data imbalance in MTL?
4. How can the sampling temperature parameter influence the distribution of sampling probabilities in multi-task learning models?
5. What is the annealed sampling method, and how does it adjust the sampling temperature over the training process to reduce inter-task interference?

Answer:

(p10.0) Machine learning models often suffer from imbalanced data distributions. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.

(p10.1) A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.

(p10.2) (2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,

(p10.3) As a generalization of proportional sampling in Eq. (2), for task can take the following form as

(p10.4) where 1 is called the sampling temperature [128]. Similar to task loss weights, researchers have proposed various techniques to adjust . When < 1, the divergence of sampling probabilities is reduced. can be viewed as a hyperparameter to be set by users or be changed dynamically during training. For example, the annealed sampling method [113] adjusts as training proceeds. Given a total number of epochs, at epoch is set to

(p10.5) In this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference. [128] defines as

(p10.6) where 0 and denote initial and maximum values of , respectively. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period.","Questions:



Answer:

(p10.0) Machine learning models often suffer from imbalanced data distributions. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.

(p10.1) A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.

(p10.2) (2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,

(p10.3) As a generalization of proportional sampling in Eq. (2), for task can take the following form as

(p10.4) where 1 is called the sampling temperature [128]. Similar to task loss weights, researchers have proposed various techniques to adjust . When < 1, the divergence of sampling probabilities is reduced. can be viewed as a hyperparameter to be set by users or be changed dynamically during training. For example, the annealed sampling method [113] adjusts as training proceeds. Given a total number of epochs, at epoch is set to

(p10.5) In this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference. [128] defines as

(p10.6) where 0 and denote initial and maximum values of , respectively. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s11,Task Scheduling,"['p11.0', 'p11.1', 'p11.2', 'p11.3', 'p11.4', 'p11.5', 'p11.6']","['Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.', 'Alternatively, we can train an MTL model on different tasks at different steps. Similar to the data sampling in Section 3.2, we can assign a task sampling weight for task , which is also called mixing ratio, to control the frequency of data batches from task . The most common task scheduling technique is to shuffle between different tasks [5,20,30,33,38,44,51,71,73,79,80,89,93,99,102,108,109,114,118], either randomly or according to a pre-defined schedule. While random shuffling is widely adopted, introducing more heuristics into scheduling could help further improving the performance of MTL models. For example, according to the similarity between each task and the primary task in a multi-lingual multi-task scenario, [65] defines as', 'where or is set to 1 if the corresponding task or language is the same as the primary task and 0.1 otherwise. Instead of using a fixed mixing ratio designed by hand, some researchers explore using a dynamic mixing ratio during the training process. [40] schedules tasks by a state machine that switches between the two tasks and updates learning rate when validation loss rises. [37] develops a controller meta-network that dynamically schedules tasks based on multi-armed bandits. The controller has arms and optimizes a control policy for arm (task) at step based on an estimated action value , defined as', 'where denotes temperature, is decay rate, and is the observed reward at step that is defined as the negative validation loss of the primary task. Besides probabilistic approaches, task scheduling could also use heuristics based on certain performance metrics. By optimizing the Tchebycheff loss, [75] learns from the task which has the worst validation performance at each step. The CA-MTL model [95] introduces an uncertainty-based sampling strategy based on Shannon entropy. Specifically, given a batch size and tasks, a pool of × samples are first sampled. Then, the uncertainty measure U ( ) for a sample x from task is defined as', ""where denotes the Shannon entropy of the model's prediction on x,ˆis the model's maximum average entropy over samples from each task, and ′ denotes the entropy of a uniform distribution and is used to normalize the variance of the number of classes in each task. At last, samples with the highest uncertainty measures are used for training at the current step. Experiments show that this uncertainty-based sampling strategy could effectively avoid catastrophic forgetting when jointly learning multiple tasks and outperforms the aforementioned annealed sampling [113]."", 'In some cases, multiple tasks are learned sequentially. Such tasks usually form a clear dependency relationship or are of different difficulty levels. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1). Unicoder [49] trains its five pre-training objectives sequentially in each step. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.', 'For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data. [125] first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.']","Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.

Alternatively, we can train an MTL model on different tasks at different steps. Similar to the data sampling in Section 3.2, we can assign a task sampling weight for task , which is also called mixing ratio, to control the frequency of data batches from task . The most common task scheduling technique is to shuffle between different tasks [5,20,30,33,38,44,51,71,73,79,80,89,93,99,102,108,109,114,118], either randomly or according to a pre-defined schedule. While random shuffling is widely adopted, introducing more heuristics into scheduling could help further improving the performance of MTL models. For example, according to the similarity between each task and the primary task in a multi-lingual multi-task scenario, [65] defines as

where or is set to 1 if the corresponding task or language is the same as the primary task and 0.1 otherwise. Instead of using a fixed mixing ratio designed by hand, some researchers explore using a dynamic mixing ratio during the training process. [40] schedules tasks by a state machine that switches between the two tasks and updates learning rate when validation loss rises. [37] develops a controller meta-network that dynamically schedules tasks based on multi-armed bandits. The controller has arms and optimizes a control policy for arm (task) at step based on an estimated action value , defined as

where denotes temperature, is decay rate, and is the observed reward at step that is defined as the negative validation loss of the primary task. Besides probabilistic approaches, task scheduling could also use heuristics based on certain performance metrics. By optimizing the Tchebycheff loss, [75] learns from the task which has the worst validation performance at each step. The CA-MTL model [95] introduces an uncertainty-based sampling strategy based on Shannon entropy. Specifically, given a batch size and tasks, a pool of × samples are first sampled. Then, the uncertainty measure U ( ) for a sample x from task is defined as

where denotes the Shannon entropy of the model's prediction on x,ˆis the model's maximum average entropy over samples from each task, and ′ denotes the entropy of a uniform distribution and is used to normalize the variance of the number of classes in each task. At last, samples with the highest uncertainty measures are used for training at the current step. Experiments show that this uncertainty-based sampling strategy could effectively avoid catastrophic forgetting when jointly learning multiple tasks and outperforms the aforementioned annealed sampling [113].

In some cases, multiple tasks are learned sequentially. Such tasks usually form a clear dependency relationship or are of different difficulty levels. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1). Unicoder [49] trains its five pre-training objectives sequentially in each step. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.

For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data. [125] first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.","(p11.0) Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.

(p11.1) Alternatively, we can train an MTL model on different tasks at different steps. Similar to the data sampling in Section 3.2, we can assign a task sampling weight for task , which is also called mixing ratio, to control the frequency of data batches from task . The most common task scheduling technique is to shuffle between different tasks [5,20,30,33,38,44,51,71,73,79,80,89,93,99,102,108,109,114,118], either randomly or according to a pre-defined schedule. While random shuffling is widely adopted, introducing more heuristics into scheduling could help further improving the performance of MTL models. For example, according to the similarity between each task and the primary task in a multi-lingual multi-task scenario, [65] defines as

(p11.2) where or is set to 1 if the corresponding task or language is the same as the primary task and 0.1 otherwise. Instead of using a fixed mixing ratio designed by hand, some researchers explore using a dynamic mixing ratio during the training process. [40] schedules tasks by a state machine that switches between the two tasks and updates learning rate when validation loss rises. [37] develops a controller meta-network that dynamically schedules tasks based on multi-armed bandits. The controller has arms and optimizes a control policy for arm (task) at step based on an estimated action value , defined as

(p11.3) where denotes temperature, is decay rate, and is the observed reward at step that is defined as the negative validation loss of the primary task. Besides probabilistic approaches, task scheduling could also use heuristics based on certain performance metrics. By optimizing the Tchebycheff loss, [75] learns from the task which has the worst validation performance at each step. The CA-MTL model [95] introduces an uncertainty-based sampling strategy based on Shannon entropy. Specifically, given a batch size and tasks, a pool of × samples are first sampled. Then, the uncertainty measure U ( ) for a sample x from task is defined as

(p11.4) where denotes the Shannon entropy of the model's prediction on x,ˆis the model's maximum average entropy over samples from each task, and ′ denotes the entropy of a uniform distribution and is used to normalize the variance of the number of classes in each task. At last, samples with the highest uncertainty measures are used for training at the current step. Experiments show that this uncertainty-based sampling strategy could effectively avoid catastrophic forgetting when jointly learning multiple tasks and outperforms the aforementioned annealed sampling [113].

(p11.5) In some cases, multiple tasks are learned sequentially. Such tasks usually form a clear dependency relationship or are of different difficulty levels. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1). Unicoder [49] trains its five pre-training objectives sequentially in each step. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.

(p11.6) For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data. [125] first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.","[['b133', 'b147', 'b110', 'b2', 'b142'], ['b32', 'b88', 'b98', 'b101', 'b108', 'b19', 'b107', 'b29', 'b92', 'b50', 'b113', 'b117', 'b43', 'b64', 'b4', 'b78', 'b72', 'b37', 'b79', 'b70'], ['b39', 'b36'], ['b74', 'b94'], ['b112'], ['b17', 'b84', 'b48', 'b89', 'b42', 'b93', 'b29', 'b49'], ['b54', 'b43', 'b124', 'b13']]","[['b133', 'b147', 'b110', 'b2', 'b142'], ['b32', 'b88', 'b98', 'b101', 'b108', 'b19', 'b107', 'b29', 'b92', 'b50', 'b113', 'b117', 'b43', 'b64', 'b4', 'b78', 'b72', 'b37', 'b79', 'b70'], ['b39', 'b36'], ['b74', 'b94'], ['b112'], ['b17', 'b84', 'b48', 'b89', 'b42', 'b93', 'b29', 'b49'], ['b54', 'b43', 'b124', 'b13']]",42,"1. Task scheduling determines the order of tasks in which an MTL model is trained.
2. A naive way is to train all tasks together.
3. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions.
4. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together.
5. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step.
6. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.
7. Alternatively, we can train an MTL model on different tasks at different steps.
8. Similar to the data sampling in Section 3.2, we can assign a task sampling weight for task , which is also called mixing ratio, to control the frequency of data batches from task .
9. The most common task scheduling technique is to shuffle between different tasks [5,20,30,33,38,44,51,71,73,79,80,89,93,99,102,108,109,114,118], either randomly or according to a pre-defined schedule.
10. While random shuffling is widely adopted, introducing more heuristics into scheduling could help further improving the performance of MTL models.
11. For example, according to the similarity between each task and the primary task in a multi-lingual multi-task scenario, [65] defines aswhere or is set to 1 if the corresponding task or language is the same as the primary task and 0.1 otherwise.
12. Instead of using a fixed mixing ratio designed by hand, some researchers explore using a dynamic mixing ratio during the training process.
13. [40] schedules tasks by a state machine that switches between the two tasks and updates learning rate when validation loss rises.
14. [37] develops a controller meta-network that dynamically schedules tasks based on multi-armed bandits.
15. The controller has arms and optimizes a control policy for arm (task) at step based on an estimated action value , defined aswhere denotes temperature, is decay rate, and is the observed reward at step that is defined as the negative validation loss of the primary task.
16. Besides probabilistic approaches, task scheduling could also use heuristics based on certain performance metrics.
17. By optimizing the Tchebycheff loss, [143]0 learns from the task which has the worst validation performance at each step.
18. The CA-MTL model [143]1 introduces an uncertainty-based sampling strategy based on Shannon entropy.
19. Specifically, given a batch size and tasks, a pool of × samples are first sampled.
20. Then, the uncertainty measure U [143]2 for a sample x from task is defined aswhere denotes the Shannon entropy of the model's prediction on x,ˆis the model's maximum average entropy over samples from each task, and ′ denotes the entropy of a uniform distribution and is used to normalize the variance of the number of classes in each task.
21. At last, samples with the highest uncertainty measures are used for training at the current step.
22. Experiments show that this uncertainty-based sampling strategy could effectively avoid catastrophic forgetting when jointly learning multiple tasks and outperforms the aforementioned annealed sampling [143]3.
23. In some cases, multiple tasks are learned sequentially.
24. Such tasks usually form a clear dependency relationship or are of different difficulty levels.
25. For instance, [143]4 uses a baby step curriculum learning approach [143]5 and trains different tasks in the order of increasing difficulties.
26. Similarly, [143]6 trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches.
27. [143]7 focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. [143]8.
28. Unicoder [143]9 trains its five pre-training objectives sequentially in each step.
29. [134]0 applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain.
30. [134]1 first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters.
31. To stabilize the training process when alternating between tasks, successive regularization [134]2 is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.
32. For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks.
33. For example, [134]3 trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task.
34. [134]4 trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data.
35. [134]5 first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data.
36. Similarly, [134]6 first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.","What are the advantages and disadvantages of training Multi-Task Learning (MTL) models with all tasks simultaneously versus sequentially?
How does task scheduling impact the performance of MTL models in natural language processing?
What role does the mixing ratio play in task scheduling for MTL models, and how can it be optimized?
How do dynamic task scheduling strategies, such as those based on state machines or controller meta-networks, compare to static scheduling methods in MTL?
What methodologies are employed to prevent catastrophic forgetting in MTL models when learning multiple tasks jointly?","1. What are the advantages and disadvantages of training Multi-Task Learning (MTL) models with all tasks simultaneously versus sequentially?
2. How does task scheduling impact the performance of MTL models in natural language processing?
3. What role does the mixing ratio play in task scheduling for MTL models, and how can it be optimized?
4. How do dynamic task scheduling strategies, such as those based on state machines or controller meta-networks, compare to static scheduling methods in MTL?
5. What methodologies are employed to prevent catastrophic forgetting in MTL models when learning multiple tasks jointly?",None,,"Questions:

1. What are the advantages and disadvantages of training Multi-Task Learning (MTL) models with all tasks simultaneously versus sequentially?
2. How does task scheduling impact the performance of MTL models in natural language processing?
3. What role does the mixing ratio play in task scheduling for MTL models, and how can it be optimized?
4. How do dynamic task scheduling strategies, such as those based on state machines or controller meta-networks, compare to static scheduling methods in MTL?
5. What methodologies are employed to prevent catastrophic forgetting in MTL models when learning multiple tasks jointly?

Answer:

(p11.0) Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.

(p11.1) Alternatively, we can train an MTL model on different tasks at different steps. Similar to the data sampling in Section 3.2, we can assign a task sampling weight for task , which is also called mixing ratio, to control the frequency of data batches from task . The most common task scheduling technique is to shuffle between different tasks [5,20,30,33,38,44,51,71,73,79,80,89,93,99,102,108,109,114,118], either randomly or according to a pre-defined schedule. While random shuffling is widely adopted, introducing more heuristics into scheduling could help further improving the performance of MTL models. For example, according to the similarity between each task and the primary task in a multi-lingual multi-task scenario, [65] defines as

(p11.2) where or is set to 1 if the corresponding task or language is the same as the primary task and 0.1 otherwise. Instead of using a fixed mixing ratio designed by hand, some researchers explore using a dynamic mixing ratio during the training process. [40] schedules tasks by a state machine that switches between the two tasks and updates learning rate when validation loss rises. [37] develops a controller meta-network that dynamically schedules tasks based on multi-armed bandits. The controller has arms and optimizes a control policy for arm (task) at step based on an estimated action value , defined as

(p11.3) where denotes temperature, is decay rate, and is the observed reward at step that is defined as the negative validation loss of the primary task. Besides probabilistic approaches, task scheduling could also use heuristics based on certain performance metrics. By optimizing the Tchebycheff loss, [75] learns from the task which has the worst validation performance at each step. The CA-MTL model [95] introduces an uncertainty-based sampling strategy based on Shannon entropy. Specifically, given a batch size and tasks, a pool of × samples are first sampled. Then, the uncertainty measure U ( ) for a sample x from task is defined as

(p11.4) where denotes the Shannon entropy of the model's prediction on x,ˆis the model's maximum average entropy over samples from each task, and ′ denotes the entropy of a uniform distribution and is used to normalize the variance of the number of classes in each task. At last, samples with the highest uncertainty measures are used for training at the current step. Experiments show that this uncertainty-based sampling strategy could effectively avoid catastrophic forgetting when jointly learning multiple tasks and outperforms the aforementioned annealed sampling [113].

(p11.5) In some cases, multiple tasks are learned sequentially. Such tasks usually form a clear dependency relationship or are of different difficulty levels. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1). Unicoder [49] trains its five pre-training objectives sequentially in each step. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.

(p11.6) For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data. [125] first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.","Questions:



Answer:

(p11.0) Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.

(p11.1) Alternatively, we can train an MTL model on different tasks at different steps. Similar to the data sampling in Section 3.2, we can assign a task sampling weight for task , which is also called mixing ratio, to control the frequency of data batches from task . The most common task scheduling technique is to shuffle between different tasks [5,20,30,33,38,44,51,71,73,79,80,89,93,99,102,108,109,114,118], either randomly or according to a pre-defined schedule. While random shuffling is widely adopted, introducing more heuristics into scheduling could help further improving the performance of MTL models. For example, according to the similarity between each task and the primary task in a multi-lingual multi-task scenario, [65] defines as

(p11.2) where or is set to 1 if the corresponding task or language is the same as the primary task and 0.1 otherwise. Instead of using a fixed mixing ratio designed by hand, some researchers explore using a dynamic mixing ratio during the training process. [40] schedules tasks by a state machine that switches between the two tasks and updates learning rate when validation loss rises. [37] develops a controller meta-network that dynamically schedules tasks based on multi-armed bandits. The controller has arms and optimizes a control policy for arm (task) at step based on an estimated action value , defined as

(p11.3) where denotes temperature, is decay rate, and is the observed reward at step that is defined as the negative validation loss of the primary task. Besides probabilistic approaches, task scheduling could also use heuristics based on certain performance metrics. By optimizing the Tchebycheff loss, [75] learns from the task which has the worst validation performance at each step. The CA-MTL model [95] introduces an uncertainty-based sampling strategy based on Shannon entropy. Specifically, given a batch size and tasks, a pool of × samples are first sampled. Then, the uncertainty measure U ( ) for a sample x from task is defined as

(p11.4) where denotes the Shannon entropy of the model's prediction on x,ˆis the model's maximum average entropy over samples from each task, and ′ denotes the entropy of a uniform distribution and is used to normalize the variance of the number of classes in each task. At last, samples with the highest uncertainty measures are used for training at the current step. Experiments show that this uncertainty-based sampling strategy could effectively avoid catastrophic forgetting when jointly learning multiple tasks and outperforms the aforementioned annealed sampling [113].

(p11.5) In some cases, multiple tasks are learned sequentially. Such tasks usually form a clear dependency relationship or are of different difficulty levels. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1). Unicoder [49] trains its five pre-training objectives sequentially in each step. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.

(p11.6) For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data. [125] first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s13,Auxiliary MTL,"['p13.0', 'p13.1']","['Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.', ""Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction. [50] trains an extractive summarization model together with an auxiliary document-level classification task. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks. and rebuttal pairs of scientific papers. [58] makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task. To handle the primary disfluency detection task, [125] pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task.""]","Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.

Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction. [50] trains an extractive summarization model together with an auxiliary document-level classification task. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks. and rebuttal pairs of scientific papers. [58] makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task. To handle the primary disfluency detection task, [125] pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task.","(p13.0) Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.

(p13.1) Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction. [50] trains an extractive summarization model together with an auxiliary document-level classification task. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks. and rebuttal pairs of scientific papers. [58] makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task. To handle the primary disfluency detection task, [125] pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task.","[[], ['b136', 'b133', 'b129', 'b57', 'b124', 'b2', 'b83', 'b0', 'b60', 'b96', 'b14', 'b49']]","[[], ['b136', 'b133', 'b129', 'b57', 'b124', 'b2', 'b83', 'b0', 'b60', 'b96', 'b14', 'b49']]",12,"1. Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning.
2. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks.
3. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.
4. Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data.
5. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging.
6. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction.
7. [50] trains an extractive summarization model together with an auxiliary document-level classification task.
8. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture.
9. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model.
10. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features.
11. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task.
12. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation.
13. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1.
14. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved.
15. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively.
16. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks.
17. and rebuttal pairs of scientific papers.
18. [3]0 makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task.
19. To handle the primary disfluency detection task, [3]1 pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task.","How does Auxiliary MTL contribute to the improvement of sequence tagging tasks in NLP?
What are the benefits of incorporating language modeling objectives into sequence labeling models?
How do auxiliary tasks enhance the performance of text generation models in natural language processing?
In what ways does multi-task learning (MTL) facilitate knowledge transfer from large open-domain corpora to data-scarce domains?
Can you describe how self-supervised tasks in Auxiliary MTL are used to improve disfluency detection in natural language processing?","1. How does Auxiliary MTL contribute to the improvement of sequence tagging tasks in NLP?
2. What are the benefits of incorporating language modeling objectives into sequence labeling models?
3. How do auxiliary tasks enhance the performance of text generation models in natural language processing?
4. In what ways does multi-task learning (MTL) facilitate knowledge transfer from large open-domain corpora to data-scarce domains?
5. Can you describe how self-supervised tasks in Auxiliary MTL are used to improve disfluency detection in natural language processing?",None,,"Questions:

1. How does Auxiliary MTL contribute to the improvement of sequence tagging tasks in NLP?
2. What are the benefits of incorporating language modeling objectives into sequence labeling models?
3. How do auxiliary tasks enhance the performance of text generation models in natural language processing?
4. In what ways does multi-task learning (MTL) facilitate knowledge transfer from large open-domain corpora to data-scarce domains?
5. Can you describe how self-supervised tasks in Auxiliary MTL are used to improve disfluency detection in natural language processing?

Answer:

(p13.0) Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.

(p13.1) Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction. [50] trains an extractive summarization model together with an auxiliary document-level classification task. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks. and rebuttal pairs of scientific papers. [58] makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task. To handle the primary disfluency detection task, [125] pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task.","Questions:



Answer:

(p13.0) Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.

(p13.1) Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction. [50] trains an extractive summarization model together with an auxiliary document-level classification task. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks. and rebuttal pairs of scientific papers. [58] makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task. To handle the primary disfluency detection task, [125] pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s14,Primary,"['p14.0', 'p14.1', 'p14.2', 'p14.3']","['Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.', '[62] models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks. Generative adversarial MTL architectures are used to improve classification tasks as well. Targeting pharmacovigilance mining, [138] treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features. Differently, [98] enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data. Sentiment classification models can be enhanced by POS tagging and gaze prediction [80], label distribution learning [149], unsupervised topic modeling [124], or domain adversarial training [127]. In [132], besides the shared base model, a separate model is built for each Microblog user as an auxiliary task. [96] estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task. [67] introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task. [139] improves a community question answering model with an auxiliary question category classification task. To counter data scarcity in the multi-choice question answering task, [51] proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.', 'For text generation tasks, MTL is brought in to improve the quality of the generated text. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155]. [105] implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing. [9] enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks. [99] views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.', 'Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.']","Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.

[62] models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks. Generative adversarial MTL architectures are used to improve classification tasks as well. Targeting pharmacovigilance mining, [138] treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features. Differently, [98] enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data. Sentiment classification models can be enhanced by POS tagging and gaze prediction [80], label distribution learning [149], unsupervised topic modeling [124], or domain adversarial training [127]. In [132], besides the shared base model, a separate model is built for each Microblog user as an auxiliary task. [96] estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task. [67] introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task. [139] improves a community question answering model with an auxiliary question category classification task. To counter data scarcity in the multi-choice question answering task, [51] proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.

For text generation tasks, MTL is brought in to improve the quality of the generated text. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155]. [105] implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing. [9] enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks. [99] views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.

Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.","(p14.0) Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.

(p14.1) [62] models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks. Generative adversarial MTL architectures are used to improve classification tasks as well. Targeting pharmacovigilance mining, [138] treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features. Differently, [98] enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data. Sentiment classification models can be enhanced by POS tagging and gaze prediction [80], label distribution learning [149], unsupervised topic modeling [124], or domain adversarial training [127]. In [132], besides the shared base model, a separate model is built for each Microblog user as an auxiliary task. [96] estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task. [67] introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task. [139] improves a community question answering model with an auxiliary question category classification task. To counter data scarcity in the multi-choice question answering task, [51] proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.

(p14.2) For text generation tasks, MTL is brought in to improve the quality of the generated text. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155]. [105] implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing. [9] enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks. [99] views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.

(p14.3) Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.","[['b54', 'b55', 'b111', 'b132', 'b73', 'b13', 'b32', 'b58', 'b27', 'b52', 'b106', 'b70'], ['b131', 'b138', 'b97', 'b123', 'b66', 'b95', 'b50', 'b126', 'b79', 'b148', 'b137'], ['b98', 'b72', 'b37', 'b102', 'b36', 'b127', 'b153', 'b8', 'b143', 'b26', 'b154', 'b104'], ['b113', 'b155', 'b125']]","[['b54', 'b55', 'b111', 'b132', 'b73', 'b13', 'b32', 'b58', 'b27', 'b52', 'b106', 'b70'], ['b131', 'b138', 'b97', 'b123', 'b66', 'b95', 'b50', 'b126', 'b79', 'b148', 'b137'], ['b98', 'b72', 'b37', 'b102', 'b36', 'b127', 'b153', 'b8', 'b143', 'b26', 'b154', 'b104'], ['b113', 'b155', 'b125']]",38,"1. Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification.
2. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks.
3. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction.
4. [59] enhances a rumor detection model with user credibility features.
5. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance.
6. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token.
7. In [107], text classification is boosted by learning the predominant sense of words.
8. [133] assists the fake news detection task by stance classification.
9. [14] jointly learns the answer identification task with an auxiliary question answering task.
10. To improve slot filling performance for online shopping assistants, [56]0 adds NER and segment tagging tasks as auxiliary tasks.
11. In [56]1, the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.
12. [56]2 models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks.
13. Generative adversarial MTL architectures are used to improve classification tasks as well.
14. Targeting pharmacovigilance mining, [56]3 treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features.
15. Differently, [56]4 enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data.
16. Sentiment classification models can be enhanced by POS tagging and gaze prediction [56]5, label distribution learning [56]6, unsupervised topic modeling [56]7, or domain adversarial training [56]8.
17. In [56]9, besides the shared base model, a separate model is built for each Microblog user as an auxiliary task.
18. [53]0 estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task.
19. [53]1 introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task.
20. [53]2 improves a community question answering model with an auxiliary question category classification task.
21. To counter data scarcity in the multi-choice question answering task, [53]3 proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.
22. For text generation tasks, MTL is brought in to improve the quality of the generated text.
23. It is observed in [53]4 that adding a target-side language modeling task on the decoder of a neural machine translation [53]5 model brings moderate but consistent performance gain.
24. [53]6 learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks.
25. Similarly, [53]7 learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks.
26. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [53]8 adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data.
27. Recently, [53]9 builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder.
28. For the sentence simplification task, [55]0 uses paraphrase generation and entailment generation as two auxiliary tasks.
29. [55]1 builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks.
30. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [55]2 or task-oriented dialogue generation [55]3.
31. [55]4 implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing.
32. [55]5 enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks.
33. [55]6 views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.
34. Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks.
35. For example, [55]7 learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks.
36. [55]8 trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation.
37. As existing pre-trained models impose huge storage cost for the deployment, PinText [55]9 learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.","How does auxiliary multi-task learning (MTL) contribute to the improvement of natural language processing tasks such as discourse relation classification and rumor detection?
What role does adversarial training play in enhancing feature extraction and generalization in multi-task learning models for classification tasks?
In what ways do auxiliary tasks like POS tagging, domain prediction, and sentiment classification enhance the performance of text generation models in multi-task learning frameworks?
How does multi-task learning address the challenge of data scarcity in tasks such as multi-choice question answering and semantic parsing?
What strategies are employed in multi-task learning to build general-purpose text representations for use in downstream tasks, and how do they mitigate the storage costs associated with pre-trained models?","1. How does auxiliary multi-task learning (MTL) contribute to the improvement of natural language processing tasks such as discourse relation classification and rumor detection?
2. What role does adversarial training play in enhancing feature extraction and generalization in multi-task learning models for classification tasks?
3. In what ways do auxiliary tasks like POS tagging, domain prediction, and sentiment classification enhance the performance of text generation models in multi-task learning frameworks?
4. How does multi-task learning address the challenge of data scarcity in tasks such as multi-choice question answering and semantic parsing?
5. What strategies are employed in multi-task learning to build general-purpose text representations for use in downstream tasks, and how do they mitigate the storage costs associated with pre-trained models?",None,,"Questions:

1. How does auxiliary multi-task learning (MTL) contribute to the improvement of natural language processing tasks such as discourse relation classification and rumor detection?
2. What role does adversarial training play in enhancing feature extraction and generalization in multi-task learning models for classification tasks?
3. In what ways do auxiliary tasks like POS tagging, domain prediction, and sentiment classification enhance the performance of text generation models in multi-task learning frameworks?
4. How does multi-task learning address the challenge of data scarcity in tasks such as multi-choice question answering and semantic parsing?
5. What strategies are employed in multi-task learning to build general-purpose text representations for use in downstream tasks, and how do they mitigate the storage costs associated with pre-trained models?

Answer:

(p14.0) Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.

(p14.1) [62] models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks. Generative adversarial MTL architectures are used to improve classification tasks as well. Targeting pharmacovigilance mining, [138] treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features. Differently, [98] enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data. Sentiment classification models can be enhanced by POS tagging and gaze prediction [80], label distribution learning [149], unsupervised topic modeling [124], or domain adversarial training [127]. In [132], besides the shared base model, a separate model is built for each Microblog user as an auxiliary task. [96] estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task. [67] introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task. [139] improves a community question answering model with an auxiliary question category classification task. To counter data scarcity in the multi-choice question answering task, [51] proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.

(p14.2) For text generation tasks, MTL is brought in to improve the quality of the generated text. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155]. [105] implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing. [9] enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks. [99] views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.

(p14.3) Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.","Questions:



Answer:

(p14.0) Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.

(p14.1) [62] models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks. Generative adversarial MTL architectures are used to improve classification tasks as well. Targeting pharmacovigilance mining, [138] treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features. Differently, [98] enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data. Sentiment classification models can be enhanced by POS tagging and gaze prediction [80], label distribution learning [149], unsupervised topic modeling [124], or domain adversarial training [127]. In [132], besides the shared base model, a separate model is built for each Microblog user as an auxiliary task. [96] estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task. [67] introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task. [139] improves a community question answering model with an auxiliary question category classification task. To counter data scarcity in the multi-choice question answering task, [51] proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.

(p14.2) For text generation tasks, MTL is brought in to improve the quality of the generated text. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155]. [105] implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing. [9] enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks. [99] views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.

(p14.3) Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s15,Joint MTL,"['p15.0', 'p15.1', 'p15.2']","['Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.', ""Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks. One major problem of such tasks is the lack of sufficient labeled data. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques. [152] develops a joint MTL model for the NER and entity name normalization tasks in the medical field. [68,146] use MTL to perform simile detection, which includes simile sentence classification and simile component extraction. To analyze Twitter demographic data, [121] jointly learns classification models for genders, ages, political orientations, and locations. The SLUICE network [100] is used to learn four different non-literal language detection tasks in English and German [26]. [86] jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French. For community question answering, [52] builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time. To analyze the argumentative structure of scientific publications, [57] optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism. Considering the connection between sentence emotions and the use of the metaphor, [22] jointly trains a metaphor identification model with an emotion detection model. To ensure the consistency between generated key phrases (short text) and headlines (long text), [85] trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism. An MTL model is proposed in [111] to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools. Table 2. A summary of joint MTL studies according to types of tasks involved. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively. A single checkmark could mean joint learning of multiple tasks of the same type."", 'Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118]. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [32], Predicate-Argument Structures (PAS) [76], and Prague Semantic Dependencies (PSD) [42]) jointly. In [47], a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation (AMR) [4], Semantic Dependency Parsing (SDP) [88], and Universal Dependencies (UD) [87], and it shows that joint training improves performance on the testing UCCA dataset. [71] jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT. [29] shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks. In [143], an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.']","Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.

Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks. One major problem of such tasks is the lack of sufficient labeled data. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques. [152] develops a joint MTL model for the NER and entity name normalization tasks in the medical field. [68,146] use MTL to perform simile detection, which includes simile sentence classification and simile component extraction. To analyze Twitter demographic data, [121] jointly learns classification models for genders, ages, political orientations, and locations. The SLUICE network [100] is used to learn four different non-literal language detection tasks in English and German [26]. [86] jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French. For community question answering, [52] builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time. To analyze the argumentative structure of scientific publications, [57] optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism. Considering the connection between sentence emotions and the use of the metaphor, [22] jointly trains a metaphor identification model with an emotion detection model. To ensure the consistency between generated key phrases (short text) and headlines (long text), [85] trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism. An MTL model is proposed in [111] to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools. Table 2. A summary of joint MTL studies according to types of tasks involved. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively. A single checkmark could mean joint learning of multiple tasks of the same type.

Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118]. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [32], Predicate-Argument Structures (PAS) [76], and Prague Semantic Dependencies (PSD) [42]) jointly. In [47], a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation (AMR) [4], Semantic Dependency Parsing (SDP) [88], and Universal Dependencies (UD) [87], and it shows that joint training improves performance on the testing UCCA dataset. [71] jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT. [29] shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks. In [143], an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.","(p15.0) Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.

(p15.1) Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks. One major problem of such tasks is the lack of sufficient labeled data. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques. [152] develops a joint MTL model for the NER and entity name normalization tasks in the medical field. [68,146] use MTL to perform simile detection, which includes simile sentence classification and simile component extraction. To analyze Twitter demographic data, [121] jointly learns classification models for genders, ages, political orientations, and locations. The SLUICE network [100] is used to learn four different non-literal language detection tasks in English and German [26]. [86] jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French. For community question answering, [52] builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time. To analyze the argumentative structure of scientific publications, [57] optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism. Considering the connection between sentence emotions and the use of the metaphor, [22] jointly trains a metaphor identification model with an emotion detection model. To ensure the consistency between generated key phrases (short text) and headlines (long text), [85] trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism. An MTL model is proposed in [111] to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools. Table 2. A summary of joint MTL studies according to types of tasks involved. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively. A single checkmark could mean joint learning of multiple tasks of the same type.

(p15.2) Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118]. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [32], Predicate-Argument Structures (PAS) [76], and Prague Semantic Dependencies (PSD) [42]) jointly. In [47], a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation (AMR) [4], Semantic Dependency Parsing (SDP) [88], and Universal Dependencies (UD) [87], and it shows that joint training improves performance on the testing UCCA dataset. [71] jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT. [29] shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks. In [143], an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.","[['b101', 'b147', 'b109', 'b140', 'b150', 'b39', 'b7', 'b42', 'b43', 'b144', 'b92', 'b71'], ['b99', 'b151', 'b25', 'b85', 'b84', 'b110', 'b67', 'b56', 'b21', 'b51', 'b145', 'b120'], ['b90', 'b142', 'b82', 'b46', 'b53', 'b3', 'b75', 'b20', 'b40', 'b117', 'b5', 'b86', 'b59', 'b4', 'b87', 'b28', 'b31', 'b115', 'b41', 'b130', 'b70']]","[['b101', 'b147', 'b109', 'b140', 'b150', 'b39', 'b7', 'b42', 'b43', 'b144', 'b92', 'b71'], ['b99', 'b151', 'b25', 'b85', 'b84', 'b110', 'b67', 'b56', 'b21', 'b51', 'b145', 'b120'], ['b90', 'b142', 'b82', 'b46', 'b53', 'b3', 'b75', 'b20', 'b40', 'b117', 'b5', 'b86', 'b59', 'b4', 'b87', 'b28', 'b31', 'b115', 'b41', 'b130', 'b70']]",45,"1. Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously.
2. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other.
3. Table 2 gives an overview of task combinations used in joint MTL models.
4. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other.
5. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively.
6. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model.
7. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks.
8. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification.
9. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews.
10. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures.
11. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks.
12. [40,141,145] learn entity extraction alongside relation extraction.
13. For sentiment analysis tasks, [110]0 jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture.
14. [110]1 learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis.
15. [110]2 builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.
16. Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks.
17. One major problem of such tasks is the lack of sufficient labeled data.
18. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing.
19. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques.
20. [110]3 develops a joint MTL model for the NER and entity name normalization tasks in the medical field.
21. [110]4 use MTL to perform simile detection, which includes simile sentence classification and simile component extraction.
22. To analyze Twitter demographic data, [110]5 jointly learns classification models for genders, ages, political orientations, and locations.
23. The SLUICE network [110]6 is used to learn four different non-literal language detection tasks in English and German [110]7.
24. [110]8 jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French.
25. For community question answering, [110]9 builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time.
26. To analyze the argumentative structure of scientific publications, [43]0 optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism.
27. Considering the connection between sentence emotions and the use of the metaphor, [43]1 jointly trains a metaphor identification model with an emotion detection model.
28. To ensure the consistency between generated key phrases [43]2 and headlines [43]3, [43]4 trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism.
29. An MTL model is proposed in [43]5 to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools.
30. Table 2. A summary of joint MTL studies according to types of tasks involved.
31. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively.
32. A single checkmark could mean joint learning of multiple tasks of the same type.
33. Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks.
34. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions.
35. Applications in multi-domain NLP tasks include sentiment classification [43]6, dialog state tracking [43]7, essay scoring [43]8, deceptive review detection [43]9, multi-genre emotion detection and classification [148]0, RST discourse parsing [148]1, historical spelling normalization [148]2, and document classification [148]3.
36. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces.
37. [148]4 model three different formalisms of semantic dependency parsing [148]5 [148]6, Predicate-Argument Structures [148]7 [148]8, and Prague Semantic Dependencies [148]9 [72]0) jointly.
38. In [72]1, a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation [72]2 [72]3, Semantic Dependency Parsing [72]4 [72]5, and Universal Dependencies [72]6 [72]7, and it shows that joint training improves performance on the testing UCCA dataset.
39. [72]8 jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT.
40. [72]9 shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks.
41. In [102]0, an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.","How does joint Multi-Task Learning (MTL) differ from auxiliary MTL in optimizing task performance?
What are the benefits of converting traditional pipeline models to joint MTL models in natural language processing tasks?
How does joint MTL utilize parallel feature fusion for learning multiple classification tasks?
In what ways does joint MTL address the challenge of insufficient labeled data in downstream tasks?
How does joint training in multi-domain or multi-formalism NLP tasks improve performance across different data distributions or label spaces?","1. How does joint Multi-Task Learning (MTL) differ from auxiliary MTL in optimizing task performance?
2. What are the benefits of converting traditional pipeline models to joint MTL models in natural language processing tasks?
3. How does joint MTL utilize parallel feature fusion for learning multiple classification tasks?
4. In what ways does joint MTL address the challenge of insufficient labeled data in downstream tasks?
5. How does joint training in multi-domain or multi-formalism NLP tasks improve performance across different data distributions or label spaces?",None,,"Questions:

1. How does joint Multi-Task Learning (MTL) differ from auxiliary MTL in optimizing task performance?
2. What are the benefits of converting traditional pipeline models to joint MTL models in natural language processing tasks?
3. How does joint MTL utilize parallel feature fusion for learning multiple classification tasks?
4. In what ways does joint MTL address the challenge of insufficient labeled data in downstream tasks?
5. How does joint training in multi-domain or multi-formalism NLP tasks improve performance across different data distributions or label spaces?

Answer:

(p15.0) Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.

(p15.1) Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks. One major problem of such tasks is the lack of sufficient labeled data. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques. [152] develops a joint MTL model for the NER and entity name normalization tasks in the medical field. [68,146] use MTL to perform simile detection, which includes simile sentence classification and simile component extraction. To analyze Twitter demographic data, [121] jointly learns classification models for genders, ages, political orientations, and locations. The SLUICE network [100] is used to learn four different non-literal language detection tasks in English and German [26]. [86] jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French. For community question answering, [52] builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time. To analyze the argumentative structure of scientific publications, [57] optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism. Considering the connection between sentence emotions and the use of the metaphor, [22] jointly trains a metaphor identification model with an emotion detection model. To ensure the consistency between generated key phrases (short text) and headlines (long text), [85] trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism. An MTL model is proposed in [111] to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools. Table 2. A summary of joint MTL studies according to types of tasks involved. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively. A single checkmark could mean joint learning of multiple tasks of the same type.

(p15.2) Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118]. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [32], Predicate-Argument Structures (PAS) [76], and Prague Semantic Dependencies (PSD) [42]) jointly. In [47], a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation (AMR) [4], Semantic Dependency Parsing (SDP) [88], and Universal Dependencies (UD) [87], and it shows that joint training improves performance on the testing UCCA dataset. [71] jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT. [29] shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks. In [143], an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.","Questions:



Answer:

(p15.0) Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.

(p15.1) Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks. One major problem of such tasks is the lack of sufficient labeled data. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques. [152] develops a joint MTL model for the NER and entity name normalization tasks in the medical field. [68,146] use MTL to perform simile detection, which includes simile sentence classification and simile component extraction. To analyze Twitter demographic data, [121] jointly learns classification models for genders, ages, political orientations, and locations. The SLUICE network [100] is used to learn four different non-literal language detection tasks in English and German [26]. [86] jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French. For community question answering, [52] builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time. To analyze the argumentative structure of scientific publications, [57] optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism. Considering the connection between sentence emotions and the use of the metaphor, [22] jointly trains a metaphor identification model with an emotion detection model. To ensure the consistency between generated key phrases (short text) and headlines (long text), [85] trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism. An MTL model is proposed in [111] to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools. Table 2. A summary of joint MTL studies according to types of tasks involved. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively. A single checkmark could mean joint learning of multiple tasks of the same type.

(p15.2) Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118]. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [32], Predicate-Argument Structures (PAS) [76], and Prague Semantic Dependencies (PSD) [42]) jointly. In [47], a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation (AMR) [4], Semantic Dependency Parsing (SDP) [88], and Universal Dependencies (UD) [87], and it shows that joint training improves performance on the testing UCCA dataset. [71] jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT. [29] shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks. In [143], an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s16,Multi-lingual MTL,"['p16.0', 'p16.1', 'p16.2']","['Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.', 'Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.', 'Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.']","Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.

Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.

Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.","(p16.0) Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.

(p16.1) Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.

(p16.2) Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.","[['b78', 'b126', 'b77'], ['b85', 'b118'], ['b64', 'b48', 'b107']]","[['b78', 'b126', 'b77'], ['b85', 'b118'], ['b64', 'b48', 'b107']]",8,"1. Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections.
2. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models.
3. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English.
4. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively.
5. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.
6. Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages.
7. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese.
8. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process.
9. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts.
10. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English.
11. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.
12. Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations.
13. [108] learns multi-lingual representations from two tasks.
14. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples.
15. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification.
16. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training.
17. In [78]0, a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels.
18. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way.
19. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.","How does leveraging data from multiple languages through Multi-Task Learning (MTL) benefit multi-lingual machine learning models?
What are the benefits of using adversarial training in multi-lingual machine learning models, as demonstrated in the partially shared multi-task model for language intent learning?
How does the proposed multi-lingual dialogue evaluation metric achieve a high correlation with human annotations and outperform monolingual counterparts?
In what ways does the formality-sensitive translation system from English to French leverage multi-task learning to improve translation quality?
How does the multi-lingual multi-task model facilitate low-resource knowledge transfer, and what role does domain adversarial method play in this process?","1. How does leveraging data from multiple languages through Multi-Task Learning (MTL) benefit multi-lingual machine learning models?
2. What are the benefits of using adversarial training in multi-lingual machine learning models, as demonstrated in the partially shared multi-task model for language intent learning?
3. How does the proposed multi-lingual dialogue evaluation metric achieve a high correlation with human annotations and outperform monolingual counterparts?
4. In what ways does the formality-sensitive translation system from English to French leverage multi-task learning to improve translation quality?
5. How does the multi-lingual multi-task model facilitate low-resource knowledge transfer, and what role does domain adversarial method play in this process?",None,,"Questions:

1. How does leveraging data from multiple languages through Multi-Task Learning (MTL) benefit multi-lingual machine learning models?
2. What are the benefits of using adversarial training in multi-lingual machine learning models, as demonstrated in the partially shared multi-task model for language intent learning?
3. How does the proposed multi-lingual dialogue evaluation metric achieve a high correlation with human annotations and outperform monolingual counterparts?
4. In what ways does the formality-sensitive translation system from English to French leverage multi-task learning to improve translation quality?
5. How does the multi-lingual multi-task model facilitate low-resource knowledge transfer, and what role does domain adversarial method play in this process?

Answer:

(p16.0) Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.

(p16.1) Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.

(p16.2) Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.","Questions:



Answer:

(p16.0) Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.

(p16.1) Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.

(p16.2) Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s17,Multimodal MTL,"['p17.0', 'p17.1', 'p17.2', 'p17.3']","['As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.', '[89] builds a video captioning model with two auxiliary tasks. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.', 'Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.', 'In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.']","As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.

[89] builds a video captioning model with two auxiliary tasks. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.

Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.

In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.","(p17.0) As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.

(p17.1) [89] builds a video captioning model with two auxiliary tasks. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.

(p17.2) Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.

(p17.3) In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.","[['b15'], ['b11'], ['b79', 'b10'], ['b114']]","[['b15'], ['b11'], ['b79', 'b10'], ['b114']]",5,"1. As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community.
2. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks.
3. MTL is a natural choice for implicitly injecting multimodal features into a single model.
4. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language.
5. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.
6. [89] builds a video captioning model with two auxiliary tasks.
7. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability.
8. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism.
9. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.
10. Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding.
11. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance.
12. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.
13. In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment.
14. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels.
15. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning.
16. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.","How does multimodal learning in NLP benefit from incorporating features from auditory, visual, and cognitive modalities?
What role does multi-task learning (MTL) play in enhancing the performance of speech translation models?
How do auxiliary tasks contribute to the improvement of video captioning models in multimodal learning?
In what ways does the incorporation of human cognitive processes, such as gaze behavior, impact sentiment classification models in NLP?
What are the key components of a multi-task benchmark for evaluating grounded language learning models, and how do they assess the quality and generalization ability of knowledge grounding?","1. How does multimodal learning in NLP benefit from incorporating features from auditory, visual, and cognitive modalities?
2. What role does multi-task learning (MTL) play in enhancing the performance of speech translation models?
3. How do auxiliary tasks contribute to the improvement of video captioning models in multimodal learning?
4. In what ways does the incorporation of human cognitive processes, such as gaze behavior, impact sentiment classification models in NLP?
5. What are the key components of a multi-task benchmark for evaluating grounded language learning models, and how do they assess the quality and generalization ability of knowledge grounding?",None,,"Questions:

1. How does multimodal learning in NLP benefit from incorporating features from auditory, visual, and cognitive modalities?
2. What role does multi-task learning (MTL) play in enhancing the performance of speech translation models?
3. How do auxiliary tasks contribute to the improvement of video captioning models in multimodal learning?
4. In what ways does the incorporation of human cognitive processes, such as gaze behavior, impact sentiment classification models in NLP?
5. What are the key components of a multi-task benchmark for evaluating grounded language learning models, and how do they assess the quality and generalization ability of knowledge grounding?

Answer:

(p17.0) As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.

(p17.1) [89] builds a video captioning model with two auxiliary tasks. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.

(p17.2) Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.

(p17.3) In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.","Questions:



Answer:

(p17.0) As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.

(p17.1) [89] builds a video captioning model with two auxiliary tasks. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.

(p17.2) Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.

(p17.3) In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s18,Task Relatedness in MTL,"['p18.0', 'p18.1']","['A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.', 'As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.']","A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.

As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.","(p18.0) A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.

(p18.1) As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.","[['b52', 'b103', 'b76', 'b70'], ['b48', 'b53', 'b9', 'b21', 'b90']]","[['b52', 'b103', 'b76', 'b70'], ['b48', 'b53', 'b9', 'b21', 'b90']]",9,"1. A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training.
2. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks.
3. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy.
4. This finding also holds for rumor verification [53].
5. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other.
6. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels.
7. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets.
8. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.
9. As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness.
10. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks.
11. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted.
12. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar.
13. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks.
14. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.","What criteria determine the suitability of tasks for joint training in multi-task learning within NLP?
How does the label distribution characteristics of auxiliary tasks impact the effectiveness of multi-task learning for semantic sequence labeling?
What is the proposed dataset similarity metric for estimating the likelihood of benefit from joint training in multi-task learning, and how does it work?
In what ways does the relatedness and complementarity between tasks influence the performance gains in multi-task learning?
How does multi-task learning facilitate knowledge transfer between different languages, and which language pairs have shown easier transferability?","1. What criteria determine the suitability of tasks for joint training in multi-task learning within NLP?
2. How does the label distribution characteristics of auxiliary tasks impact the effectiveness of multi-task learning for semantic sequence labeling?
3. What is the proposed dataset similarity metric for estimating the likelihood of benefit from joint training in multi-task learning, and how does it work?
4. In what ways does the relatedness and complementarity between tasks influence the performance gains in multi-task learning?
5. How does multi-task learning facilitate knowledge transfer between different languages, and which language pairs have shown easier transferability?",None,,"Questions:

1. What criteria determine the suitability of tasks for joint training in multi-task learning within NLP?
2. How does the label distribution characteristics of auxiliary tasks impact the effectiveness of multi-task learning for semantic sequence labeling?
3. What is the proposed dataset similarity metric for estimating the likelihood of benefit from joint training in multi-task learning, and how does it work?
4. In what ways does the relatedness and complementarity between tasks influence the performance gains in multi-task learning?
5. How does multi-task learning facilitate knowledge transfer between different languages, and which language pairs have shown easier transferability?

Answer:

(p18.0) A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.

(p18.1) As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.","Questions:



Answer:

(p18.0) A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.

(p18.1) As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s20,Data Source,"['p20.0', 'p20.1']","['Given tasks with corresponding datasets D = {X , Y }, = 1, . . . , , where X denotes the set of data instances in task and Y denotes the corresponding labels, we denote the entire dataset for the tasks by D = {X, Y}. We describe different forms of D in the following sections.', '5.1.1 Disjoint Datasets. In most multi-task learning literature, the training sets of different tasks have distinct label spaces, i.e. ∀ ≠ , Y ∩ Y = ∅. In this case, D = {D 1 , . . . , D } The most popular way to train MTL models on such tasks is to alternate between different tasks [5,20,27,30,40,43,69,71,73,79,89,96,109,136,153], either randomly or by a pre-defined order. Thus the model handles data from different datasets in turns as discussed in Section 3.']","Given tasks with corresponding datasets D = {X , Y }, = 1, . . . , , where X denotes the set of data instances in task and Y denotes the corresponding labels, we denote the entire dataset for the tasks by D = {X, Y}. We describe different forms of D in the following sections.

5.1.1 Disjoint Datasets. In most multi-task learning literature, the training sets of different tasks have distinct label spaces, i.e. ∀ ≠ , Y ∩ Y = ∅. In this case, D = {D 1 , . . . , D } The most popular way to train MTL models on such tasks is to alternate between different tasks [5,20,27,30,40,43,69,71,73,79,89,96,109,136,153], either randomly or by a pre-defined order. Thus the model handles data from different datasets in turns as discussed in Section 3.","(p20.0) Given tasks with corresponding datasets D = {X , Y }, = 1, . . . , , where X denotes the set of data instances in task and Y denotes the corresponding labels, we denote the entire dataset for the tasks by D = {X, Y}. We describe different forms of D in the following sections.

(p20.1) 5.1.1 Disjoint Datasets. In most multi-task learning literature, the training sets of different tasks have distinct label spaces, i.e. ∀ ≠ , Y ∩ Y = ∅. In this case, D = {D 1 , . . . , D } The most popular way to train MTL models on such tasks is to alternate between different tasks [5,20,27,30,40,43,69,71,73,79,89,96,109,136,153], either randomly or by a pre-defined order. Thus the model handles data from different datasets in turns as discussed in Section 3.","[[], ['b4', 'b78', 'b72', 'b152', 'b108', 'b19', 'b39', 'b42', 'b26', 'b88', 'b95', 'b29', 'b68', 'b135', 'b70']]","[[], ['b4', 'b78', 'b72', 'b152', 'b108', 'b19', 'b39', 'b42', 'b26', 'b88', 'b95', 'b29', 'b68', 'b135', 'b70']]",15,"1. Given tasks with corresponding datasets D = {X , Y }, = 1, . . . , , where X denotes the set of data instances in task and Y denotes the corresponding labels, we denote the entire dataset for the tasks by D = {X, Y}.
2. We describe different forms of D in the following sections.
3. 5.1.1 Disjoint Datasets. In most multi-task learning literature, the training sets of different tasks have distinct label spaces, i.e. ∀ ≠ , Y ∩ Y = ∅. In this case, D = {D 1 , . . . , D } The most popular way to train MTL models on such tasks is to alternate between different tasks [5,20,27,30,40,43,69,71,73,79,89,96,109,136,153], either randomly or by a pre-defined order.
4. Thus the model handles data from different datasets in turns as discussed in Section 3.","What is the definition of disjoint datasets in the context of multi-task learning?
How do multi-task learning models handle training on disjoint datasets?
What are the common strategies for alternating between tasks in multi-task learning models?
Can you explain the significance of distinct label spaces in multi-task learning datasets?
What does the notation D = {X, Y} represent in the context of multi-task learning datasets?","1. What is the definition of disjoint datasets in the context of multi-task learning?
2. How do multi-task learning models handle training on disjoint datasets?
3. What are the common strategies for alternating between tasks in multi-task learning models?
4. Can you explain the significance of distinct label spaces in multi-task learning datasets?
5. What does the notation D = {X, Y} represent in the context of multi-task learning datasets?",None,,"Questions:

1. What is the definition of disjoint datasets in the context of multi-task learning?
2. How do multi-task learning models handle training on disjoint datasets?
3. What are the common strategies for alternating between tasks in multi-task learning models?
4. Can you explain the significance of distinct label spaces in multi-task learning datasets?
5. What does the notation D = {X, Y} represent in the context of multi-task learning datasets?

Answer:

(p20.0) Given tasks with corresponding datasets D = {X , Y }, = 1, . . . , , where X denotes the set of data instances in task and Y denotes the corresponding labels, we denote the entire dataset for the tasks by D = {X, Y}. We describe different forms of D in the following sections.

(p20.1) 5.1.1 Disjoint Datasets. In most multi-task learning literature, the training sets of different tasks have distinct label spaces, i.e. ∀ ≠ , Y ∩ Y = ∅. In this case, D = {D 1 , . . . , D } The most popular way to train MTL models on such tasks is to alternate between different tasks [5,20,27,30,40,43,69,71,73,79,89,96,109,136,153], either randomly or by a pre-defined order. Thus the model handles data from different datasets in turns as discussed in Section 3.","Questions:



Answer:

(p20.0) Given tasks with corresponding datasets D = {X , Y }, = 1, . . . , , where X denotes the set of data instances in task and Y denotes the corresponding labels, we denote the entire dataset for the tasks by D = {X, Y}. We describe different forms of D in the following sections.

(p20.1) 5.1.1 Disjoint Datasets. In most multi-task learning literature, the training sets of different tasks have distinct label spaces, i.e. ∀ ≠ , Y ∩ Y = ∅. In this case, D = {D 1 , . . . , D } The most popular way to train MTL models on such tasks is to alternate between different tasks [5,20,27,30,40,43,69,71,73,79,89,96,109,136,153], either randomly or by a pre-defined order. Thus the model handles data from different datasets in turns as discussed in Section 3."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s21,Multi-label Datasets.,"['p21.0', 'p21.1', 'p21.2']","['Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time. In this case,', 'Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.', 'The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa.']","Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time. In this case,

Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.

The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa.","(p21.0) Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time. In this case,

(p21.1) Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.

(p21.2) The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa.","[[], ['b90', 'b28', 'b120', 'b53'], ['b85', 'b129', 'b48', 'b124', 'b55', 'b63', 'b23', 'b115', 'b96', 'b30', 'b32', 'b106', 'b61', 'b125']]","[[], ['b90', 'b28', 'b120', 'b53'], ['b85', 'b129', 'b48', 'b124', 'b55', 'b63', 'b23', 'b115', 'b96', 'b30', 'b32', 'b106', 'b61', 'b125']]",18,"1. Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time.
2. In this case,Multi-label datasets can be created by giving extra manual annotations to existing data.
3. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input.
4. [121] labels Twitter posts with 4 demographic labels.
5. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.
6. The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset.
7. Extra labels can be obtained using pre-defined rules [62,97].
8. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels.
9. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation.
10. [116] uses hashtags to represent genres of tweet posts.
11. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database.
12. Unicoder [49] uses translated text from the source language to fine-tune on the target language.
13. [121]0 creates disfluent sentences by randomly repeating or inserting -grams.
14. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models.
15. [121]1 obtains dominant word sense labels from WordNet [121]2.
16. [121]3 applies entity linking for QA data over databases through an entity linker.
17. [121]4 assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method.
18. [121]5 uses the output of a meta-network as labels for unsupervised training data.
19. As a special case of multi-label dataset, mask orchestration [121]6 provides different parts of an instance to different tasks by applying different masks.
20. That is, labels for one task may become the input for another task and vice versa.","How can multi-label datasets enhance the optimization of task-specific components in natural language processing tasks?
What are the methods for creating multi-label datasets in natural language processing research?
How does the use of pre-defined rules contribute to the automatic generation of multi-label datasets?
In what ways can external tools or previously trained models assist in the creation of self-supervised labels for multi-label datasets?
What is the role of mask orchestration in providing labels for multi-task learning in natural language processing, and how does it differ from traditional labeling methods?","1. How can multi-label datasets enhance the optimization of task-specific components in natural language processing tasks?
2. What are the methods for creating multi-label datasets in natural language processing research?
3. How does the use of pre-defined rules contribute to the automatic generation of multi-label datasets?
4. In what ways can external tools or previously trained models assist in the creation of self-supervised labels for multi-label datasets?
5. What is the role of mask orchestration in providing labels for multi-task learning in natural language processing, and how does it differ from traditional labeling methods?",None,,"Questions:

1. How can multi-label datasets enhance the optimization of task-specific components in natural language processing tasks?
2. What are the methods for creating multi-label datasets in natural language processing research?
3. How does the use of pre-defined rules contribute to the automatic generation of multi-label datasets?
4. In what ways can external tools or previously trained models assist in the creation of self-supervised labels for multi-label datasets?
5. What is the role of mask orchestration in providing labels for multi-task learning in natural language processing, and how does it differ from traditional labeling methods?

Answer:

(p21.0) Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time. In this case,

(p21.1) Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.

(p21.2) The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa.","Questions:



Answer:

(p21.0) Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time. In this case,

(p21.1) Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.

(p21.2) The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa."
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a,s22,Multi-task Benchmark Datasets,"['p22.0', 'p22.1']","['As summarized in Table 3, we list a few public multi-task benchmark datasets for NLP tasks.', '• GLUE [123] is a benchmark dataset for evaluating natural language understanding (NLU) models. The main benchmark consists of 8 sentence and sentence-pair classification tasks as well as a regression task. The tasks cover a diverse range of genres, dataset sizes, and difficulties. Besides, a diagnostic dataset is provided to evaluate the ability of NLU models on capturing a pre-defined set of language phenomena. • SuperGLUE [122] is a generalization of GLUE. As the performance of state-of-the-art models has exceeded non-expert human baselines on GLUE, SuperGLUE contains a set of 8 more challenging NLU tasks along with comprehensive human baselines. Besides retaining the two hardest tasks in GLUE, 6 tasks are added with two new question formats: coreference resolution and question answering (QA).  [46] is a multi-task few-shot learning dataset for world knowledge and problem solving abilities of language processing models. This dataset covers 57 subjects including 19 in STEM, 13 in humanities, 12 in social sciences, and 13 in other subjects. This dataset is split into a few-shot development set that has 5 questions for each subject, a validation set for tuning hyper-parameters containing 1540 questions, and a test set with 14079 questions. • Xtreme [48] is a multi-task benchmark dataset for evaluating cross-lingual generalization capabilities of multi-lingual representations covering 9 tasks in 40 languages. The tasks include 2 classification tasks, 2 structure prediction tasks, 3 question answering tasks, and 2 sentence retrieval tasks. Out of the 40 languages involved, 19 languages appear in at least 3 datasets and the rest 21 languages appear in at least one dataset. • XGLUE [63] is a benchmark dataset that supports the development and evaluation of large cross-lingual pre-trained language models. The XGLUE dataset includes 11 downstream tasks, including 3 single-input understanding tasks, 6 pair-input understanding tasks, and 2 generation tasks. The pre-training corpus consists of a small corpus that includes a 101G multi-lingual corpus covering 100 languages and a 146G bilingual corpus covering 27 languages, and a large corpus with 2,500G multi-lingual data covering 89 languages. • LSParD [105] is a multi-task semantic parsing dataset with 3 tasks, including question type classification, entity mention detection, and question semantic parsing. Each logical form is associated with a question and multiple human annotated paraphrases. This dataset contains 51,164 questions in 9 categories, 3361 logical form patterns, and 23,144 entities. • ECSA [33] is a dataset for slot filling, named entity recognition, and segmentation to evaluate online shopping assistant systems in Chinese. The training part contains 24,892 pairs of input utterances and their corresponding slot labels, named entity labels, and segment labels. The testing part includes 2,723 such pairs with an Out-of-Vocabulary (OOV) rate of 85.3%, which is much higher than the ATIS dataset [45] whose OOV is smaller than 1%. • ABC [34], the Anti-reflexive Bias Challenge, is a multi-task benchmark dataset designed for evaluating gender assumptions in NLP models. ABC consists of 4 tasks, including language modeling, natural language inference (NLI), coreference resolution, and machine translation. A total of 4,560 samples are collected by a template-based method. The language modeling task is to predict the pronoun of a sentence. For NLI and coreference resolution, three variations of each sentence are used to construct entailment pairs. For machine translation, sentences with two variations of third-person pronouns in English are used as source sentences. • CompGuessWhat?! [115] is a dataset for grounded language learning with 65,700 collected dialogues. It is an instance of the Grounded Language Learning with Attributes (GROLLA) framework. The evaluation process includes three parts: goal-oriented evaluation (e.g., Visual QA and Visual NLI), object attribute prediction, and zero-shot evaluation. • SCIERC [72] is a multi-label dataset for identifying entities, relations, and cross-sentence coreference clusters from abstracts of research papers. SCIERC contains 500 scientific abstracts collected from proceedings in 12 conferences and workshops in artificial intelligence.']","As summarized in Table 3, we list a few public multi-task benchmark datasets for NLP tasks.

• GLUE [123] is a benchmark dataset for evaluating natural language understanding (NLU) models. The main benchmark consists of 8 sentence and sentence-pair classification tasks as well as a regression task. The tasks cover a diverse range of genres, dataset sizes, and difficulties. Besides, a diagnostic dataset is provided to evaluate the ability of NLU models on capturing a pre-defined set of language phenomena. • SuperGLUE [122] is a generalization of GLUE. As the performance of state-of-the-art models has exceeded non-expert human baselines on GLUE, SuperGLUE contains a set of 8 more challenging NLU tasks along with comprehensive human baselines. Besides retaining the two hardest tasks in GLUE, 6 tasks are added with two new question formats: coreference resolution and question answering (QA).  [46] is a multi-task few-shot learning dataset for world knowledge and problem solving abilities of language processing models. This dataset covers 57 subjects including 19 in STEM, 13 in humanities, 12 in social sciences, and 13 in other subjects. This dataset is split into a few-shot development set that has 5 questions for each subject, a validation set for tuning hyper-parameters containing 1540 questions, and a test set with 14079 questions. • Xtreme [48] is a multi-task benchmark dataset for evaluating cross-lingual generalization capabilities of multi-lingual representations covering 9 tasks in 40 languages. The tasks include 2 classification tasks, 2 structure prediction tasks, 3 question answering tasks, and 2 sentence retrieval tasks. Out of the 40 languages involved, 19 languages appear in at least 3 datasets and the rest 21 languages appear in at least one dataset. • XGLUE [63] is a benchmark dataset that supports the development and evaluation of large cross-lingual pre-trained language models. The XGLUE dataset includes 11 downstream tasks, including 3 single-input understanding tasks, 6 pair-input understanding tasks, and 2 generation tasks. The pre-training corpus consists of a small corpus that includes a 101G multi-lingual corpus covering 100 languages and a 146G bilingual corpus covering 27 languages, and a large corpus with 2,500G multi-lingual data covering 89 languages. • LSParD [105] is a multi-task semantic parsing dataset with 3 tasks, including question type classification, entity mention detection, and question semantic parsing. Each logical form is associated with a question and multiple human annotated paraphrases. This dataset contains 51,164 questions in 9 categories, 3361 logical form patterns, and 23,144 entities. • ECSA [33] is a dataset for slot filling, named entity recognition, and segmentation to evaluate online shopping assistant systems in Chinese. The training part contains 24,892 pairs of input utterances and their corresponding slot labels, named entity labels, and segment labels. The testing part includes 2,723 such pairs with an Out-of-Vocabulary (OOV) rate of 85.3%, which is much higher than the ATIS dataset [45] whose OOV is smaller than 1%. • ABC [34], the Anti-reflexive Bias Challenge, is a multi-task benchmark dataset designed for evaluating gender assumptions in NLP models. ABC consists of 4 tasks, including language modeling, natural language inference (NLI), coreference resolution, and machine translation. A total of 4,560 samples are collected by a template-based method. The language modeling task is to predict the pronoun of a sentence. For NLI and coreference resolution, three variations of each sentence are used to construct entailment pairs. For machine translation, sentences with two variations of third-person pronouns in English are used as source sentences. • CompGuessWhat?! [115] is a dataset for grounded language learning with 65,700 collected dialogues. It is an instance of the Grounded Language Learning with Attributes (GROLLA) framework. The evaluation process includes three parts: goal-oriented evaluation (e.g., Visual QA and Visual NLI), object attribute prediction, and zero-shot evaluation. • SCIERC [72] is a multi-label dataset for identifying entities, relations, and cross-sentence coreference clusters from abstracts of research papers. SCIERC contains 500 scientific abstracts collected from proceedings in 12 conferences and workshops in artificial intelligence.","(p22.0) As summarized in Table 3, we list a few public multi-task benchmark datasets for NLP tasks.

(p22.1) • GLUE [123] is a benchmark dataset for evaluating natural language understanding (NLU) models. The main benchmark consists of 8 sentence and sentence-pair classification tasks as well as a regression task. The tasks cover a diverse range of genres, dataset sizes, and difficulties. Besides, a diagnostic dataset is provided to evaluate the ability of NLU models on capturing a pre-defined set of language phenomena. • SuperGLUE [122] is a generalization of GLUE. As the performance of state-of-the-art models has exceeded non-expert human baselines on GLUE, SuperGLUE contains a set of 8 more challenging NLU tasks along with comprehensive human baselines. Besides retaining the two hardest tasks in GLUE, 6 tasks are added with two new question formats: coreference resolution and question answering (QA).  [46] is a multi-task few-shot learning dataset for world knowledge and problem solving abilities of language processing models. This dataset covers 57 subjects including 19 in STEM, 13 in humanities, 12 in social sciences, and 13 in other subjects. This dataset is split into a few-shot development set that has 5 questions for each subject, a validation set for tuning hyper-parameters containing 1540 questions, and a test set with 14079 questions. • Xtreme [48] is a multi-task benchmark dataset for evaluating cross-lingual generalization capabilities of multi-lingual representations covering 9 tasks in 40 languages. The tasks include 2 classification tasks, 2 structure prediction tasks, 3 question answering tasks, and 2 sentence retrieval tasks. Out of the 40 languages involved, 19 languages appear in at least 3 datasets and the rest 21 languages appear in at least one dataset. • XGLUE [63] is a benchmark dataset that supports the development and evaluation of large cross-lingual pre-trained language models. The XGLUE dataset includes 11 downstream tasks, including 3 single-input understanding tasks, 6 pair-input understanding tasks, and 2 generation tasks. The pre-training corpus consists of a small corpus that includes a 101G multi-lingual corpus covering 100 languages and a 146G bilingual corpus covering 27 languages, and a large corpus with 2,500G multi-lingual data covering 89 languages. • LSParD [105] is a multi-task semantic parsing dataset with 3 tasks, including question type classification, entity mention detection, and question semantic parsing. Each logical form is associated with a question and multiple human annotated paraphrases. This dataset contains 51,164 questions in 9 categories, 3361 logical form patterns, and 23,144 entities. • ECSA [33] is a dataset for slot filling, named entity recognition, and segmentation to evaluate online shopping assistant systems in Chinese. The training part contains 24,892 pairs of input utterances and their corresponding slot labels, named entity labels, and segment labels. The testing part includes 2,723 such pairs with an Out-of-Vocabulary (OOV) rate of 85.3%, which is much higher than the ATIS dataset [45] whose OOV is smaller than 1%. • ABC [34], the Anti-reflexive Bias Challenge, is a multi-task benchmark dataset designed for evaluating gender assumptions in NLP models. ABC consists of 4 tasks, including language modeling, natural language inference (NLI), coreference resolution, and machine translation. A total of 4,560 samples are collected by a template-based method. The language modeling task is to predict the pronoun of a sentence. For NLI and coreference resolution, three variations of each sentence are used to construct entailment pairs. For machine translation, sentences with two variations of third-person pronouns in English are used as source sentences. • CompGuessWhat?! [115] is a dataset for grounded language learning with 65,700 collected dialogues. It is an instance of the Grounded Language Learning with Attributes (GROLLA) framework. The evaluation process includes three parts: goal-oriented evaluation (e.g., Visual QA and Visual NLI), object attribute prediction, and zero-shot evaluation. • SCIERC [72] is a multi-label dataset for identifying entities, relations, and cross-sentence coreference clusters from abstracts of research papers. SCIERC contains 500 scientific abstracts collected from proceedings in 12 conferences and workshops in artificial intelligence.","[[], ['b62', 'b114', 'b32', 'b44', 'b45', 'b33', 'b121', 'b71', 'b122']]","[[], ['b62', 'b114', 'b32', 'b44', 'b45', 'b33', 'b121', 'b71', 'b122']]",9,"1. As summarized in Table 3, we list a few public multi-task benchmark datasets for NLP tasks.
2. GLUE [123] is a benchmark dataset for evaluating natural language understanding (NLU) models.
3. The main benchmark consists of 8 sentence and sentence-pair classification tasks as well as a regression task.
4. The tasks cover a diverse range of genres, dataset sizes, and difficulties.
5. Besides, a diagnostic dataset is provided to evaluate the ability of NLU models on capturing a pre-defined set of language phenomena.
6. SuperGLUE [122] is a generalization of GLUE.
7. As the performance of state-of-the-art models has exceeded non-expert human baselines on GLUE, SuperGLUE contains a set of 8 more challenging NLU tasks along with comprehensive human baselines.
8. Besides retaining the two hardest tasks in GLUE, 6 tasks are added with two new question formats: coreference resolution and question answering (QA).
9. [46] is a multi-task few-shot learning dataset for world knowledge and problem solving abilities of language processing models.
10. This dataset covers 57 subjects including 19 in STEM, 13 in humanities, 12 in social sciences, and 13 in other subjects.
11. This dataset is split into a few-shot development set that has 5 questions for each subject, a validation set for tuning hyper-parameters containing 1540 questions, and a test set with 14079 questions.
12. Xtreme [48] is a multi-task benchmark dataset for evaluating cross-lingual generalization capabilities of multi-lingual representations covering 9 tasks in 40 languages.
13. The tasks include 2 classification tasks, 2 structure prediction tasks, 3 question answering tasks, and 2 sentence retrieval tasks.
14. Out of the 40 languages involved, 19 languages appear in at least 3 datasets and the rest 21 languages appear in at least one dataset.
15. XGLUE [63] is a benchmark dataset that supports the development and evaluation of large cross-lingual pre-trained language models.
16. The XGLUE dataset includes 11 downstream tasks, including 3 single-input understanding tasks, 6 pair-input understanding tasks, and 2 generation tasks.
17. The pre-training corpus consists of a small corpus that includes a 101G multi-lingual corpus covering 100 languages and a 146G bilingual corpus covering 27 languages, and a large corpus with 2,500G multi-lingual data covering 89 languages.
18. LSParD [105] is a multi-task semantic parsing dataset with 3 tasks, including question type classification, entity mention detection, and question semantic parsing.
19. Each logical form is associated with a question and multiple human annotated paraphrases.
20. This dataset contains 51,164 questions in 9 categories, 3361 logical form patterns, and 23,144 entities.
21. ECSA [33] is a dataset for slot filling, named entity recognition, and segmentation to evaluate online shopping assistant systems in Chinese.
22. The training part contains 24,892 pairs of input utterances and their corresponding slot labels, named entity labels, and segment labels.
23. The testing part includes 2,723 such pairs with an Out-of-Vocabulary (OOV) rate of 85.3%, which is much higher than the ATIS dataset (NLU)0 whose OOV is smaller than 1%.
24. ABC (NLU)1, the Anti-reflexive Bias Challenge, is a multi-task benchmark dataset designed for evaluating gender assumptions in NLP models.
25. ABC consists of 4 tasks, including language modeling, natural language inference (NLU)2, coreference resolution, and machine translation.
26. A total of 4,560 samples are collected by a template-based method.
27. The language modeling task is to predict the pronoun of a sentence.
28. For NLI and coreference resolution, three variations of each sentence are used to construct entailment pairs.
29. For machine translation, sentences with two variations of third-person pronouns in English are used as source sentences.
30. CompGuessWhat?! (NLU)3 is a dataset for grounded language learning with 65,700 collected dialogues.
31. It is an instance of the Grounded Language Learning with Attributes (NLU)4 framework.
32. The evaluation process includes three parts: goal-oriented evaluation (NLU)5, object attribute prediction, and zero-shot evaluation.
33. SCIERC (NLU)6 is a multi-label dataset for identifying entities, relations, and cross-sentence coreference clusters from abstracts of research papers.
34. SCIERC contains 500 scientific abstracts collected from proceedings in 12 conferences and workshops in artificial intelligence.","What are the main components and evaluation criteria of the GLUE benchmark dataset for natural language understanding models?
How does SuperGLUE extend the challenges of GLUE and what new task formats does it introduce for evaluating NLU models?
What distinguishes the multi-task few-shot learning dataset in terms of subject coverage and question format for assessing world knowledge in language processing models?
In what ways does the Xtreme benchmark dataset evaluate cross-lingual generalization capabilities of multi-lingual representations?
How does the ABC dataset address gender assumptions in NLP models and what tasks are included to evaluate these biases?","1. What are the main components and evaluation criteria of the GLUE benchmark dataset for natural language understanding models?
2. How does SuperGLUE extend the challenges of GLUE and what new task formats does it introduce for evaluating NLU models?
3. What distinguishes the multi-task few-shot learning dataset in terms of subject coverage and question format for assessing world knowledge in language processing models?
4. In what ways does the Xtreme benchmark dataset evaluate cross-lingual generalization capabilities of multi-lingual representations?
5. How does the ABC dataset address gender assumptions in NLP models and what tasks are included to evaluate these biases?",None,,"Questions:

1. What are the main components and evaluation criteria of the GLUE benchmark dataset for natural language understanding models?
2. How does SuperGLUE extend the challenges of GLUE and what new task formats does it introduce for evaluating NLU models?
3. What distinguishes the multi-task few-shot learning dataset in terms of subject coverage and question format for assessing world knowledge in language processing models?
4. In what ways does the Xtreme benchmark dataset evaluate cross-lingual generalization capabilities of multi-lingual representations?
5. How does the ABC dataset address gender assumptions in NLP models and what tasks are included to evaluate these biases?

Answer:

(p22.0) As summarized in Table 3, we list a few public multi-task benchmark datasets for NLP tasks.

(p22.1) • GLUE [123] is a benchmark dataset for evaluating natural language understanding (NLU) models. The main benchmark consists of 8 sentence and sentence-pair classification tasks as well as a regression task. The tasks cover a diverse range of genres, dataset sizes, and difficulties. Besides, a diagnostic dataset is provided to evaluate the ability of NLU models on capturing a pre-defined set of language phenomena. • SuperGLUE [122] is a generalization of GLUE. As the performance of state-of-the-art models has exceeded non-expert human baselines on GLUE, SuperGLUE contains a set of 8 more challenging NLU tasks along with comprehensive human baselines. Besides retaining the two hardest tasks in GLUE, 6 tasks are added with two new question formats: coreference resolution and question answering (QA).  [46] is a multi-task few-shot learning dataset for world knowledge and problem solving abilities of language processing models. This dataset covers 57 subjects including 19 in STEM, 13 in humanities, 12 in social sciences, and 13 in other subjects. This dataset is split into a few-shot development set that has 5 questions for each subject, a validation set for tuning hyper-parameters containing 1540 questions, and a test set with 14079 questions. • Xtreme [48] is a multi-task benchmark dataset for evaluating cross-lingual generalization capabilities of multi-lingual representations covering 9 tasks in 40 languages. The tasks include 2 classification tasks, 2 structure prediction tasks, 3 question answering tasks, and 2 sentence retrieval tasks. Out of the 40 languages involved, 19 languages appear in at least 3 datasets and the rest 21 languages appear in at least one dataset. • XGLUE [63] is a benchmark dataset that supports the development and evaluation of large cross-lingual pre-trained language models. The XGLUE dataset includes 11 downstream tasks, including 3 single-input understanding tasks, 6 pair-input understanding tasks, and 2 generation tasks. The pre-training corpus consists of a small corpus that includes a 101G multi-lingual corpus covering 100 languages and a 146G bilingual corpus covering 27 languages, and a large corpus with 2,500G multi-lingual data covering 89 languages. • LSParD [105] is a multi-task semantic parsing dataset with 3 tasks, including question type classification, entity mention detection, and question semantic parsing. Each logical form is associated with a question and multiple human annotated paraphrases. This dataset contains 51,164 questions in 9 categories, 3361 logical form patterns, and 23,144 entities. • ECSA [33] is a dataset for slot filling, named entity recognition, and segmentation to evaluate online shopping assistant systems in Chinese. The training part contains 24,892 pairs of input utterances and their corresponding slot labels, named entity labels, and segment labels. The testing part includes 2,723 such pairs with an Out-of-Vocabulary (OOV) rate of 85.3%, which is much higher than the ATIS dataset [45] whose OOV is smaller than 1%. • ABC [34], the Anti-reflexive Bias Challenge, is a multi-task benchmark dataset designed for evaluating gender assumptions in NLP models. ABC consists of 4 tasks, including language modeling, natural language inference (NLI), coreference resolution, and machine translation. A total of 4,560 samples are collected by a template-based method. The language modeling task is to predict the pronoun of a sentence. For NLI and coreference resolution, three variations of each sentence are used to construct entailment pairs. For machine translation, sentences with two variations of third-person pronouns in English are used as source sentences. • CompGuessWhat?! [115] is a dataset for grounded language learning with 65,700 collected dialogues. It is an instance of the Grounded Language Learning with Attributes (GROLLA) framework. The evaluation process includes three parts: goal-oriented evaluation (e.g., Visual QA and Visual NLI), object attribute prediction, and zero-shot evaluation. • SCIERC [72] is a multi-label dataset for identifying entities, relations, and cross-sentence coreference clusters from abstracts of research papers. SCIERC contains 500 scientific abstracts collected from proceedings in 12 conferences and workshops in artificial intelligence.","Questions:



Answer:

(p22.0) As summarized in Table 3, we list a few public multi-task benchmark datasets for NLP tasks.

(p22.1) • GLUE [123] is a benchmark dataset for evaluating natural language understanding (NLU) models. The main benchmark consists of 8 sentence and sentence-pair classification tasks as well as a regression task. The tasks cover a diverse range of genres, dataset sizes, and difficulties. Besides, a diagnostic dataset is provided to evaluate the ability of NLU models on capturing a pre-defined set of language phenomena. • SuperGLUE [122] is a generalization of GLUE. As the performance of state-of-the-art models has exceeded non-expert human baselines on GLUE, SuperGLUE contains a set of 8 more challenging NLU tasks along with comprehensive human baselines. Besides retaining the two hardest tasks in GLUE, 6 tasks are added with two new question formats: coreference resolution and question answering (QA).  [46] is a multi-task few-shot learning dataset for world knowledge and problem solving abilities of language processing models. This dataset covers 57 subjects including 19 in STEM, 13 in humanities, 12 in social sciences, and 13 in other subjects. This dataset is split into a few-shot development set that has 5 questions for each subject, a validation set for tuning hyper-parameters containing 1540 questions, and a test set with 14079 questions. • Xtreme [48] is a multi-task benchmark dataset for evaluating cross-lingual generalization capabilities of multi-lingual representations covering 9 tasks in 40 languages. The tasks include 2 classification tasks, 2 structure prediction tasks, 3 question answering tasks, and 2 sentence retrieval tasks. Out of the 40 languages involved, 19 languages appear in at least 3 datasets and the rest 21 languages appear in at least one dataset. • XGLUE [63] is a benchmark dataset that supports the development and evaluation of large cross-lingual pre-trained language models. The XGLUE dataset includes 11 downstream tasks, including 3 single-input understanding tasks, 6 pair-input understanding tasks, and 2 generation tasks. The pre-training corpus consists of a small corpus that includes a 101G multi-lingual corpus covering 100 languages and a 146G bilingual corpus covering 27 languages, and a large corpus with 2,500G multi-lingual data covering 89 languages. • LSParD [105] is a multi-task semantic parsing dataset with 3 tasks, including question type classification, entity mention detection, and question semantic parsing. Each logical form is associated with a question and multiple human annotated paraphrases. This dataset contains 51,164 questions in 9 categories, 3361 logical form patterns, and 23,144 entities. • ECSA [33] is a dataset for slot filling, named entity recognition, and segmentation to evaluate online shopping assistant systems in Chinese. The training part contains 24,892 pairs of input utterances and their corresponding slot labels, named entity labels, and segment labels. The testing part includes 2,723 such pairs with an Out-of-Vocabulary (OOV) rate of 85.3%, which is much higher than the ATIS dataset [45] whose OOV is smaller than 1%. • ABC [34], the Anti-reflexive Bias Challenge, is a multi-task benchmark dataset designed for evaluating gender assumptions in NLP models. ABC consists of 4 tasks, including language modeling, natural language inference (NLI), coreference resolution, and machine translation. A total of 4,560 samples are collected by a template-based method. The language modeling task is to predict the pronoun of a sentence. For NLI and coreference resolution, three variations of each sentence are used to construct entailment pairs. For machine translation, sentences with two variations of third-person pronouns in English are used as source sentences. • CompGuessWhat?! [115] is a dataset for grounded language learning with 65,700 collected dialogues. It is an instance of the Grounded Language Learning with Attributes (GROLLA) framework. The evaluation process includes three parts: goal-oriented evaluation (e.g., Visual QA and Visual NLI), object attribute prediction, and zero-shot evaluation. • SCIERC [72] is a multi-label dataset for identifying entities, relations, and cross-sentence coreference clusters from abstracts of research papers. SCIERC contains 500 scientific abstracts collected from proceedings in 12 conferences and workshops in artificial intelligence."
231603122,Persuasive Natural Language Generation -A Literature Review,"Business, Linguistics, Computer Science",https://www.semanticscholar.org/paper/e9ac35f68ef4290d74551bafdfb09a4ea0983940,s2,Method,"['p2.0', 'p2.1', 'p2.2', 'p2.3', 'p2.4', 'p2.5', 'p2.6', 'p2.7', 'p2.8', 'p2.9']","[""This paper's methodology follows a framework proposed by vom Brocke et al. (2009) which is based on a screening of the review literature itself and especially highlights the need for comprehensively documenting the process of literature search in such an article (Duerr et al. 2016). The framework is structured into the following five steps: (1) definition of review scope, (2) conceptualization of topic, (3) literature search, (4) literature analysis and synthesis, and (5) research agenda. Each of the steps will be briefly explained, when it will be addressed in the course of this work."", 'The first step is the definition of the review scope of this literature review. It is summarized in Figure 1 (categories applicable to this review on Persuasion and NLG research are highlighted) which is based on the taxonomy adapted by vom Brocke et al. (2009).  Brocke et al. 2009) This literature review focuses on outcomes of applied research in the domains of Persuasion and Natural Language Generation. The goal is to integrate findings with respect to five categories concluded from the business problem of persuading individuals through textual exchange (DeMers 2016). These categories were chosen as they address the psychological and technical aspects of persuasion and are a prerequisite to creating a persuasive NLG artificial intelligence. We selected this field because the artificial generation of persuasion through NLG has, as this literature review reveals, not been addressed in seminal articles following our structured research approach. Persuasion is already commonly studied in psychology (Quirk et al. 1985, Marwell & Schmitt 1967. Also, numerous studies in natural language processing focus on identifying and classifying persuasion in an automated way (Li et al. 2020, Rocklage et al. 2018, Iyer & Sycara 2019). This paper is organized along a conceptual structure. We did not take a particular perspective to provide a neutral representation of the results. As an audience of this review specialized scholars having an interest in the field of persuasion and the artificial generation of it were chosen. For coverage, our literature review can be categorized as representative as our research has been limited to certain journals, but does not consider the totality of the literature. The second step is conceptualization of the topic .', 'It addresses the point that an author of a review article must begin with a topic in need of review, a broad conception of what is known about the topic, and potential areas where new knowledge may be needed. In the following, we conceptualize persuasion, and embed it into a business context. Furthermore, we introduce related theories, and conclude a categorization for the successive literature review.', 'In persuasion, the persuader induces a particular kind of mental state in the persuadee , e.g., through threats, but unlike an expression of sentiment, persuasion intends a change in the mental state of the persuadee (Iyer & Sycara 2019). Contemporary psychology and communication science (Rocklage et al. 2018, Park et al. 2015, Hunter et al. 2019 require the persuader to be acting intentionally, that is, the persuasive act. In the context of NLG, we usually refer to messages generated or augmented by an artificial intelligence, if we use the term persuasive act.', ""In his seminal work ' On Rhetoric ', Aristotle introduced his well-known ontology for persuasive acts. Accordingly, persuasion depends on multiple facets: emotions ( pathos ), logical structure of the argument ( logos ), the context ( cairos ), and on the speaker ( ethos ) (Schiappa & Nordin 2013)."", 'Likewise, contemporary business literature conceptualizes persuasive acts through ""principles of persuasion"" (DeMers 2016). The author concludes that six interventions help at achieving persuasiveness. The first is being confident and remaining confident during the entirety of an appeal. Next, the introduction of logical argumentation fosters persuasiveness, since individuals are more inclined to be persuaded by logic. Additionally, making an appeal seem beneficial to the other party, by demonstrating the value of an appeal, choosing words carefully (i.e., selecting from a vocabulary that may be more persuasive), and using flattery (i.e., finding appropriate compliments) are recommended. Lastly, DeMers (2016) reveals that being patient and persistent (i.e., not to greatly alter one\'s approach) strengthens a persuader\'s persuasiveness. Next, we embed the presented ""principles of persuasion"" into related theories on persuasion (Cameron 2009).', ""Festinger's Theory of Cognitive Dissonance (1957) focuses on the relationships among cognitive elements, which include beliefs, opinions, attitudes, or knowledge (O'Keefe 1990). This relates most to Aristotle's cairos , and the need to create benevolence for the persuadee. Evaluating the 'business principles', this theory resonates best with what DeMers (2016) defines as 'making [the cognition] appealing to the other party'. However cognitions, and thus, a persuadee's perceived benevolence, can be dissonant, consonant, or irrelevant to each other. If a persuadee is presented with a sufficiently vast cognitive inconsistency, then they will perceive psychological discomfort, leading to an attempt to restore their cognitive balance through a reduction or elimination of the inconsistency (Stiff 1994, Harmon-Jones 2002. The magnitude of dissonance determines one's motivation to reduce it (Stiff 1994, Festinger 1957. Approaches towards reducing dissonance are: changing terms to make them more consonant, adding additional consonant percipience, or altering the magnitude of the percipience (Harmon-Jones 2002, Stiff 1994, O'Keefe 1990."", ""In 'principles of persuasion', DeMers (2016) contends that appropriate flattering and the usage of so-called high value words contribute to persuasive acts in business contexts (cf. Aristotle's ethos ). Accordingly, Language Expectancy Theory (LET) identifies written or spoken language as a rule-based system through which persuadees develop expectations and preferences towards ''appropriate'' linguistic usage of words in varying situations (Burgoon & Miller 1985). Such expectations are frequently consistent with sociological and cultural norms, while preferences tend to relate to societal standards and cultural values (Burgoon & Miller 1985, Burgoon et al. 2002. Positive expectations that facilitate a persuasive act are, for instance, if a persuader stylizes a behavior that is perceived as more preferred than expected by the persuadee. In contrast, negative ones are inhibiting persuasion, e.g., when the persuader makes use of language that is considered to be socially unacceptable (Burgoon & Miller 1985, Burgoon et al. 2002."", ""Next, the 'principles of persuasion' confer that an argument based on logic is persuasive (DeMers 2016). What Aristotle terms logos is consistent with the theory of probabilistic models. Probabilistic models (McGuire 1981, Wyer 1970) are based on the rules of formal logic and probability, and predict beliefs regarding the conclusion of reasoning. These predictions are based on mathematical probability, and as such this theory is consistent with what Aristotle defines as logos . An exemplary belief syllogism (McGuire 1981) is composed of two premises that lead to a logical conclusion. The theorists (McGuire 1981, Wyer 1970 explain that believing in the premises leads to the expectation that the identified conclusion will follow. However, rather than solely thinking in all-or-nothing scenarios, beliefs can be ascertained through subjective probabilities: one's judgment of the probability that each of the premises is true (McGuire 1981, Wyer 1970. Furthermore, if a message evokes a perceptual change in the truth of the premise, or additional premises are supplemented, following this theory, a change in perceiving the conclusion is expected."", ""Last, Balance Theory focuses on the triadic relationship involving two individuals (e.g., persuader and persuadee), the persuadee's attitude toward the persuader (Aristotle's pathos ), and their attitudes toward an attitudinal object (Heider 1958). The resulting triad can be balanced or unbalanced: This triad is in balance if all three relationships are positive, or one is positive and two are negative. If all three relationships are negative, or one is negative and two are positive, an unbalanced triad results, often motivating one to alter one of the three relationships (Heider 1958). Building on this theory, we aim to identify relational determinants that relate to improving the relationship between the persuader and the persuadee towards the attitudinal object (Heider 1958). In the business framework, we relate those determinants towards the pattern of 'trustworthiness', that represent the persuadee's attitude towards the persuader. In his persuasion attempt, the persuader wants the persuadee to have a positive attitude.""]","This paper's methodology follows a framework proposed by vom Brocke et al. (2009) which is based on a screening of the review literature itself and especially highlights the need for comprehensively documenting the process of literature search in such an article (Duerr et al. 2016). The framework is structured into the following five steps: (1) definition of review scope, (2) conceptualization of topic, (3) literature search, (4) literature analysis and synthesis, and (5) research agenda. Each of the steps will be briefly explained, when it will be addressed in the course of this work.

The first step is the definition of the review scope of this literature review. It is summarized in Figure 1 (categories applicable to this review on Persuasion and NLG research are highlighted) which is based on the taxonomy adapted by vom Brocke et al. (2009).  Brocke et al. 2009) This literature review focuses on outcomes of applied research in the domains of Persuasion and Natural Language Generation. The goal is to integrate findings with respect to five categories concluded from the business problem of persuading individuals through textual exchange (DeMers 2016). These categories were chosen as they address the psychological and technical aspects of persuasion and are a prerequisite to creating a persuasive NLG artificial intelligence. We selected this field because the artificial generation of persuasion through NLG has, as this literature review reveals, not been addressed in seminal articles following our structured research approach. Persuasion is already commonly studied in psychology (Quirk et al. 1985, Marwell & Schmitt 1967. Also, numerous studies in natural language processing focus on identifying and classifying persuasion in an automated way (Li et al. 2020, Rocklage et al. 2018, Iyer & Sycara 2019). This paper is organized along a conceptual structure. We did not take a particular perspective to provide a neutral representation of the results. As an audience of this review specialized scholars having an interest in the field of persuasion and the artificial generation of it were chosen. For coverage, our literature review can be categorized as representative as our research has been limited to certain journals, but does not consider the totality of the literature. The second step is conceptualization of the topic .

It addresses the point that an author of a review article must begin with a topic in need of review, a broad conception of what is known about the topic, and potential areas where new knowledge may be needed. In the following, we conceptualize persuasion, and embed it into a business context. Furthermore, we introduce related theories, and conclude a categorization for the successive literature review.

In persuasion, the persuader induces a particular kind of mental state in the persuadee , e.g., through threats, but unlike an expression of sentiment, persuasion intends a change in the mental state of the persuadee (Iyer & Sycara 2019). Contemporary psychology and communication science (Rocklage et al. 2018, Park et al. 2015, Hunter et al. 2019 require the persuader to be acting intentionally, that is, the persuasive act. In the context of NLG, we usually refer to messages generated or augmented by an artificial intelligence, if we use the term persuasive act.

In his seminal work ' On Rhetoric ', Aristotle introduced his well-known ontology for persuasive acts. Accordingly, persuasion depends on multiple facets: emotions ( pathos ), logical structure of the argument ( logos ), the context ( cairos ), and on the speaker ( ethos ) (Schiappa & Nordin 2013).

Likewise, contemporary business literature conceptualizes persuasive acts through ""principles of persuasion"" (DeMers 2016). The author concludes that six interventions help at achieving persuasiveness. The first is being confident and remaining confident during the entirety of an appeal. Next, the introduction of logical argumentation fosters persuasiveness, since individuals are more inclined to be persuaded by logic. Additionally, making an appeal seem beneficial to the other party, by demonstrating the value of an appeal, choosing words carefully (i.e., selecting from a vocabulary that may be more persuasive), and using flattery (i.e., finding appropriate compliments) are recommended. Lastly, DeMers (2016) reveals that being patient and persistent (i.e., not to greatly alter one's approach) strengthens a persuader's persuasiveness. Next, we embed the presented ""principles of persuasion"" into related theories on persuasion (Cameron 2009).

Festinger's Theory of Cognitive Dissonance (1957) focuses on the relationships among cognitive elements, which include beliefs, opinions, attitudes, or knowledge (O'Keefe 1990). This relates most to Aristotle's cairos , and the need to create benevolence for the persuadee. Evaluating the 'business principles', this theory resonates best with what DeMers (2016) defines as 'making [the cognition] appealing to the other party'. However cognitions, and thus, a persuadee's perceived benevolence, can be dissonant, consonant, or irrelevant to each other. If a persuadee is presented with a sufficiently vast cognitive inconsistency, then they will perceive psychological discomfort, leading to an attempt to restore their cognitive balance through a reduction or elimination of the inconsistency (Stiff 1994, Harmon-Jones 2002. The magnitude of dissonance determines one's motivation to reduce it (Stiff 1994, Festinger 1957. Approaches towards reducing dissonance are: changing terms to make them more consonant, adding additional consonant percipience, or altering the magnitude of the percipience (Harmon-Jones 2002, Stiff 1994, O'Keefe 1990.

In 'principles of persuasion', DeMers (2016) contends that appropriate flattering and the usage of so-called high value words contribute to persuasive acts in business contexts (cf. Aristotle's ethos ). Accordingly, Language Expectancy Theory (LET) identifies written or spoken language as a rule-based system through which persuadees develop expectations and preferences towards ''appropriate'' linguistic usage of words in varying situations (Burgoon & Miller 1985). Such expectations are frequently consistent with sociological and cultural norms, while preferences tend to relate to societal standards and cultural values (Burgoon & Miller 1985, Burgoon et al. 2002. Positive expectations that facilitate a persuasive act are, for instance, if a persuader stylizes a behavior that is perceived as more preferred than expected by the persuadee. In contrast, negative ones are inhibiting persuasion, e.g., when the persuader makes use of language that is considered to be socially unacceptable (Burgoon & Miller 1985, Burgoon et al. 2002.

Next, the 'principles of persuasion' confer that an argument based on logic is persuasive (DeMers 2016). What Aristotle terms logos is consistent with the theory of probabilistic models. Probabilistic models (McGuire 1981, Wyer 1970) are based on the rules of formal logic and probability, and predict beliefs regarding the conclusion of reasoning. These predictions are based on mathematical probability, and as such this theory is consistent with what Aristotle defines as logos . An exemplary belief syllogism (McGuire 1981) is composed of two premises that lead to a logical conclusion. The theorists (McGuire 1981, Wyer 1970 explain that believing in the premises leads to the expectation that the identified conclusion will follow. However, rather than solely thinking in all-or-nothing scenarios, beliefs can be ascertained through subjective probabilities: one's judgment of the probability that each of the premises is true (McGuire 1981, Wyer 1970. Furthermore, if a message evokes a perceptual change in the truth of the premise, or additional premises are supplemented, following this theory, a change in perceiving the conclusion is expected.

Last, Balance Theory focuses on the triadic relationship involving two individuals (e.g., persuader and persuadee), the persuadee's attitude toward the persuader (Aristotle's pathos ), and their attitudes toward an attitudinal object (Heider 1958). The resulting triad can be balanced or unbalanced: This triad is in balance if all three relationships are positive, or one is positive and two are negative. If all three relationships are negative, or one is negative and two are positive, an unbalanced triad results, often motivating one to alter one of the three relationships (Heider 1958). Building on this theory, we aim to identify relational determinants that relate to improving the relationship between the persuader and the persuadee towards the attitudinal object (Heider 1958). In the business framework, we relate those determinants towards the pattern of 'trustworthiness', that represent the persuadee's attitude towards the persuader. In his persuasion attempt, the persuader wants the persuadee to have a positive attitude.","(p2.0) This paper's methodology follows a framework proposed by vom Brocke et al. (2009) which is based on a screening of the review literature itself and especially highlights the need for comprehensively documenting the process of literature search in such an article (Duerr et al. 2016). The framework is structured into the following five steps: (1) definition of review scope, (2) conceptualization of topic, (3) literature search, (4) literature analysis and synthesis, and (5) research agenda. Each of the steps will be briefly explained, when it will be addressed in the course of this work.

(p2.1) The first step is the definition of the review scope of this literature review. It is summarized in Figure 1 (categories applicable to this review on Persuasion and NLG research are highlighted) which is based on the taxonomy adapted by vom Brocke et al. (2009).  Brocke et al. 2009) This literature review focuses on outcomes of applied research in the domains of Persuasion and Natural Language Generation. The goal is to integrate findings with respect to five categories concluded from the business problem of persuading individuals through textual exchange (DeMers 2016). These categories were chosen as they address the psychological and technical aspects of persuasion and are a prerequisite to creating a persuasive NLG artificial intelligence. We selected this field because the artificial generation of persuasion through NLG has, as this literature review reveals, not been addressed in seminal articles following our structured research approach. Persuasion is already commonly studied in psychology (Quirk et al. 1985, Marwell & Schmitt 1967. Also, numerous studies in natural language processing focus on identifying and classifying persuasion in an automated way (Li et al. 2020, Rocklage et al. 2018, Iyer & Sycara 2019). This paper is organized along a conceptual structure. We did not take a particular perspective to provide a neutral representation of the results. As an audience of this review specialized scholars having an interest in the field of persuasion and the artificial generation of it were chosen. For coverage, our literature review can be categorized as representative as our research has been limited to certain journals, but does not consider the totality of the literature. The second step is conceptualization of the topic .

(p2.2) It addresses the point that an author of a review article must begin with a topic in need of review, a broad conception of what is known about the topic, and potential areas where new knowledge may be needed. In the following, we conceptualize persuasion, and embed it into a business context. Furthermore, we introduce related theories, and conclude a categorization for the successive literature review.

(p2.3) In persuasion, the persuader induces a particular kind of mental state in the persuadee , e.g., through threats, but unlike an expression of sentiment, persuasion intends a change in the mental state of the persuadee (Iyer & Sycara 2019). Contemporary psychology and communication science (Rocklage et al. 2018, Park et al. 2015, Hunter et al. 2019 require the persuader to be acting intentionally, that is, the persuasive act. In the context of NLG, we usually refer to messages generated or augmented by an artificial intelligence, if we use the term persuasive act.

(p2.4) In his seminal work ' On Rhetoric ', Aristotle introduced his well-known ontology for persuasive acts. Accordingly, persuasion depends on multiple facets: emotions ( pathos ), logical structure of the argument ( logos ), the context ( cairos ), and on the speaker ( ethos ) (Schiappa & Nordin 2013).

(p2.5) Likewise, contemporary business literature conceptualizes persuasive acts through ""principles of persuasion"" (DeMers 2016). The author concludes that six interventions help at achieving persuasiveness. The first is being confident and remaining confident during the entirety of an appeal. Next, the introduction of logical argumentation fosters persuasiveness, since individuals are more inclined to be persuaded by logic. Additionally, making an appeal seem beneficial to the other party, by demonstrating the value of an appeal, choosing words carefully (i.e., selecting from a vocabulary that may be more persuasive), and using flattery (i.e., finding appropriate compliments) are recommended. Lastly, DeMers (2016) reveals that being patient and persistent (i.e., not to greatly alter one's approach) strengthens a persuader's persuasiveness. Next, we embed the presented ""principles of persuasion"" into related theories on persuasion (Cameron 2009).

(p2.6) Festinger's Theory of Cognitive Dissonance (1957) focuses on the relationships among cognitive elements, which include beliefs, opinions, attitudes, or knowledge (O'Keefe 1990). This relates most to Aristotle's cairos , and the need to create benevolence for the persuadee. Evaluating the 'business principles', this theory resonates best with what DeMers (2016) defines as 'making [the cognition] appealing to the other party'. However cognitions, and thus, a persuadee's perceived benevolence, can be dissonant, consonant, or irrelevant to each other. If a persuadee is presented with a sufficiently vast cognitive inconsistency, then they will perceive psychological discomfort, leading to an attempt to restore their cognitive balance through a reduction or elimination of the inconsistency (Stiff 1994, Harmon-Jones 2002. The magnitude of dissonance determines one's motivation to reduce it (Stiff 1994, Festinger 1957. Approaches towards reducing dissonance are: changing terms to make them more consonant, adding additional consonant percipience, or altering the magnitude of the percipience (Harmon-Jones 2002, Stiff 1994, O'Keefe 1990.

(p2.7) In 'principles of persuasion', DeMers (2016) contends that appropriate flattering and the usage of so-called high value words contribute to persuasive acts in business contexts (cf. Aristotle's ethos ). Accordingly, Language Expectancy Theory (LET) identifies written or spoken language as a rule-based system through which persuadees develop expectations and preferences towards ''appropriate'' linguistic usage of words in varying situations (Burgoon & Miller 1985). Such expectations are frequently consistent with sociological and cultural norms, while preferences tend to relate to societal standards and cultural values (Burgoon & Miller 1985, Burgoon et al. 2002. Positive expectations that facilitate a persuasive act are, for instance, if a persuader stylizes a behavior that is perceived as more preferred than expected by the persuadee. In contrast, negative ones are inhibiting persuasion, e.g., when the persuader makes use of language that is considered to be socially unacceptable (Burgoon & Miller 1985, Burgoon et al. 2002.

(p2.8) Next, the 'principles of persuasion' confer that an argument based on logic is persuasive (DeMers 2016). What Aristotle terms logos is consistent with the theory of probabilistic models. Probabilistic models (McGuire 1981, Wyer 1970) are based on the rules of formal logic and probability, and predict beliefs regarding the conclusion of reasoning. These predictions are based on mathematical probability, and as such this theory is consistent with what Aristotle defines as logos . An exemplary belief syllogism (McGuire 1981) is composed of two premises that lead to a logical conclusion. The theorists (McGuire 1981, Wyer 1970 explain that believing in the premises leads to the expectation that the identified conclusion will follow. However, rather than solely thinking in all-or-nothing scenarios, beliefs can be ascertained through subjective probabilities: one's judgment of the probability that each of the premises is true (McGuire 1981, Wyer 1970. Furthermore, if a message evokes a perceptual change in the truth of the premise, or additional premises are supplemented, following this theory, a change in perceiving the conclusion is expected.

(p2.9) Last, Balance Theory focuses on the triadic relationship involving two individuals (e.g., persuader and persuadee), the persuadee's attitude toward the persuader (Aristotle's pathos ), and their attitudes toward an attitudinal object (Heider 1958). The resulting triad can be balanced or unbalanced: This triad is in balance if all three relationships are positive, or one is positive and two are negative. If all three relationships are negative, or one is negative and two are positive, an unbalanced triad results, often motivating one to alter one of the three relationships (Heider 1958). Building on this theory, we aim to identify relational determinants that relate to improving the relationship between the persuader and the persuadee towards the attitudinal object (Heider 1958). In the business framework, we relate those determinants towards the pattern of 'trustworthiness', that represent the persuadee's attitude towards the persuader. In his persuasion attempt, the persuader wants the persuadee to have a positive attitude.","[['b75', 'b24'], ['b55', 'b66', 'b51', 'b43', 'b64', 'b75'], [], ['b66', 'b62', 'b43', 'b41'], ['b68'], ['b12'], [None, 'b34', 'b71', 'b60'], [None, 'b11'], ['b83', 'b56'], ['b35']]","[['b75', 'b24'], ['b55', 'b66', 'b51', 'b43', 'b64', 'b75'], [], ['b66', 'b62', 'b43', 'b41'], ['b68'], ['b12'], [None, 'b34', 'b71', 'b60'], [None, 'b11'], ['b83', 'b56'], ['b35']]",23,"1. This paper's methodology follows a framework proposed by vom Brocke et al. (2009) which is based on a screening of the review literature itself and especially highlights the need for comprehensively documenting the process of literature search in such an article (Duerr et al. 2016).
2. The framework is structured into the following five steps: (1) definition of review scope, (2) conceptualization of topic, (3) literature search, (4) literature analysis and synthesis, and (5) research agenda.
3. Each of the steps will be briefly explained, when it will be addressed in the course of this work.
4. The first step is the definition of the review scope of this literature review.
5. It is summarized in Figure 1 (categories applicable to this review on Persuasion and NLG research are highlighted) which is based on the taxonomy adapted by vom Brocke et al. (2009).
6. Brocke et al. 2009) This literature review focuses on outcomes of applied research in the domains of Persuasion and Natural Language Generation.
7. The goal is to integrate findings with respect to five categories concluded from the business problem of persuading individuals through textual exchange (2)1.
8. These categories were chosen as they address the psychological and technical aspects of persuasion and are a prerequisite to creating a persuasive NLG artificial intelligence.
9. We selected this field because the artificial generation of persuasion through NLG has, as this literature review reveals, not been addressed in seminal articles following our structured research approach.
10. Persuasion is already commonly studied in psychology (Duerr et al. 2016)0.
11. This paper is organized along a conceptual structure.
12. We did not take a particular perspective to provide a neutral representation of the results.
13. As an audience of this review specialized scholars having an interest in the field of persuasion and the artificial generation of it were chosen.
14. For coverage, our literature review can be categorized as representative as our research has been limited to certain journals, but does not consider the totality of the literature.
15. The second step is conceptualization of the topic .
16. It addresses the point that an author of a review article must begin with a topic in need of review, a broad conception of what is known about the topic, and potential areas where new knowledge may be needed.
17. In the following, we conceptualize persuasion, and embed it into a business context.
18. Furthermore, we introduce related theories, and conclude a categorization for the successive literature review.
19. In persuasion, the persuader induces a particular kind of mental state in the persuadee , e.g., through threats, but unlike an expression of sentiment, persuasion intends a change in the mental state of the persuadee (Duerr et al. 2016)1.
20. Contemporary psychology and communication science (Rocklage et al. 2018, Park et al. 2015, Hunter et al. 2019 require the persuader to be acting intentionally, that is, the persuasive act.
21. In the context of NLG, we usually refer to messages generated or augmented by an artificial intelligence, if we use the term persuasive act.
22. In his seminal work ' On Rhetoric ', Aristotle introduced his well-known ontology for persuasive acts.
23. Accordingly, persuasion depends on multiple facets: emotions (Duerr et al. 2016)2, logical structure of the argument (Duerr et al. 2016)3, the context (Duerr et al. 2016)4, and on the speaker (Duerr et al. 2016)5 (Duerr et al. 2016)6.
24. Likewise, contemporary business literature conceptualizes persuasive acts through ""principles of persuasion"" (2)1.
25. The author concludes that six interventions help at achieving persuasiveness.
26. The first is being confident and remaining confident during the entirety of an appeal.
27. Next, the introduction of logical argumentation fosters persuasiveness, since individuals are more inclined to be persuaded by logic.
28. Additionally, making an appeal seem beneficial to the other party, by demonstrating the value of an appeal, choosing words carefully (Duerr et al. 2016)8, and using flattery (Duerr et al. 2016)9 are recommended.
29. Lastly, DeMers (1)7 reveals that being patient and persistent (1)1 strengthens a persuader's persuasiveness.
30. Next, we embed the presented ""principles of persuasion"" into related theories on persuasion (1)2.
31. Festinger's Theory of Cognitive Dissonance (1)3 focuses on the relationships among cognitive elements, which include beliefs, opinions, attitudes, or knowledge (1)4.
32. This relates most to Aristotle's cairos , and the need to create benevolence for the persuadee.
33. Evaluating the 'business principles', this theory resonates best with what DeMers (1)7 defines as 'making (1)6 appealing to the other party'.
34. However cognitions, and thus, a persuadee's perceived benevolence, can be dissonant, consonant, or irrelevant to each other.
35. If a persuadee is presented with a sufficiently vast cognitive inconsistency, then they will perceive psychological discomfort, leading to an attempt to restore their cognitive balance through a reduction or elimination of the inconsistency (Stiff 1994, Harmon-Jones 2002.
36. The magnitude of dissonance determines one's motivation to reduce it (Stiff 1994, Festinger 1957.
37. Approaches towards reducing dissonance are: changing terms to make them more consonant, adding additional consonant percipience, or altering the magnitude of the percipience (Harmon-Jones 2002, Stiff 1994, O'Keefe 1990.
38. In 'principles of persuasion', DeMers (1)7 contends that appropriate flattering and the usage of so-called high value words contribute to persuasive acts in business contexts (1)8.
39. Accordingly, Language Expectancy Theory (1)9 identifies written or spoken language as a rule-based system through which persuadees develop expectations and preferences towards ''appropriate'' linguistic usage of words in varying situations (2)0.
40. Such expectations are frequently consistent with sociological and cultural norms, while preferences tend to relate to societal standards and cultural values (Burgoon & Miller 1985, Burgoon et al. 2002.
41. Positive expectations that facilitate a persuasive act are, for instance, if a persuader stylizes a behavior that is perceived as more preferred than expected by the persuadee.
42. In contrast, negative ones are inhibiting persuasion, e.g., when the persuader makes use of language that is considered to be socially unacceptable (Burgoon & Miller 1985, Burgoon et al. 2002.
43. Next, the 'principles of persuasion' confer that an argument based on logic is persuasive (2)1.
44. What Aristotle terms logos is consistent with the theory of probabilistic models.
45. Probabilistic models (2)2 are based on the rules of formal logic and probability, and predict beliefs regarding the conclusion of reasoning.
46. These predictions are based on mathematical probability, and as such this theory is consistent with what Aristotle defines as logos .
47. An exemplary belief syllogism (2)3 is composed of two premises that lead to a logical conclusion.
48. The theorists (McGuire 1981, Wyer 1970 explain that believing in the premises leads to the expectation that the identified conclusion will follow.
49. However, rather than solely thinking in all-or-nothing scenarios, beliefs can be ascertained through subjective probabilities: one's judgment of the probability that each of the premises is true (McGuire 1981, Wyer 1970.
50. Furthermore, if a message evokes a perceptual change in the truth of the premise, or additional premises are supplemented, following this theory, a change in perceiving the conclusion is expected.
51. Last, Balance Theory focuses on the triadic relationship involving two individuals (2)4, the persuadee's attitude toward the persuader (2)5, and their attitudes toward an attitudinal object (2)8.
52. The resulting triad can be balanced or unbalanced: This triad is in balance if all three relationships are positive, or one is positive and two are negative.
53. If all three relationships are negative, or one is negative and two are positive, an unbalanced triad results, often motivating one to alter one of the three relationships (2)8.
54. Building on this theory, we aim to identify relational determinants that relate to improving the relationship between the persuader and the persuadee towards the attitudinal object (2)8.
55. In the business framework, we relate those determinants towards the pattern of 'trustworthiness', that represent the persuadee's attitude towards the persuader.
56. In his persuasion attempt, the persuader wants the persuadee to have a positive attitude.","How does Aristotle's concept of persuasive acts contribute to the principles of persuasive natural language generation?
What implications does Festinger's Theory of Cognitive Dissonance have for crafting persuasive messages in a business context?
In what ways do probabilistic models inform the construction of logical arguments in persuasive NLG?
How does Language Expectancy Theory influence the choice of language in persuasive natural language generation?
What role does Balance Theory play in the dynamics of persuasion between the persuader and persuadee in NLG contexts?","1. How does Aristotle's concept of persuasive acts contribute to the principles of persuasive natural language generation?
2. What implications does Festinger's Theory of Cognitive Dissonance have for crafting persuasive messages in a business context?
3. In what ways do probabilistic models inform the construction of logical arguments in persuasive NLG?
4. How does Language Expectancy Theory influence the choice of language in persuasive natural language generation?
5. What role does Balance Theory play in the dynamics of persuasion between the persuader and persuadee in NLG contexts?",None,,"Questions:

1. How does Aristotle's concept of persuasive acts contribute to the principles of persuasive natural language generation?
2. What implications does Festinger's Theory of Cognitive Dissonance have for crafting persuasive messages in a business context?
3. In what ways do probabilistic models inform the construction of logical arguments in persuasive NLG?
4. How does Language Expectancy Theory influence the choice of language in persuasive natural language generation?
5. What role does Balance Theory play in the dynamics of persuasion between the persuader and persuadee in NLG contexts?

Answer:

(p2.0) This paper's methodology follows a framework proposed by vom Brocke et al. (2009) which is based on a screening of the review literature itself and especially highlights the need for comprehensively documenting the process of literature search in such an article (Duerr et al. 2016). The framework is structured into the following five steps: (1) definition of review scope, (2) conceptualization of topic, (3) literature search, (4) literature analysis and synthesis, and (5) research agenda. Each of the steps will be briefly explained, when it will be addressed in the course of this work.

(p2.1) The first step is the definition of the review scope of this literature review. It is summarized in Figure 1 (categories applicable to this review on Persuasion and NLG research are highlighted) which is based on the taxonomy adapted by vom Brocke et al. (2009).  Brocke et al. 2009) This literature review focuses on outcomes of applied research in the domains of Persuasion and Natural Language Generation. The goal is to integrate findings with respect to five categories concluded from the business problem of persuading individuals through textual exchange (DeMers 2016). These categories were chosen as they address the psychological and technical aspects of persuasion and are a prerequisite to creating a persuasive NLG artificial intelligence. We selected this field because the artificial generation of persuasion through NLG has, as this literature review reveals, not been addressed in seminal articles following our structured research approach. Persuasion is already commonly studied in psychology (Quirk et al. 1985, Marwell & Schmitt 1967. Also, numerous studies in natural language processing focus on identifying and classifying persuasion in an automated way (Li et al. 2020, Rocklage et al. 2018, Iyer & Sycara 2019). This paper is organized along a conceptual structure. We did not take a particular perspective to provide a neutral representation of the results. As an audience of this review specialized scholars having an interest in the field of persuasion and the artificial generation of it were chosen. For coverage, our literature review can be categorized as representative as our research has been limited to certain journals, but does not consider the totality of the literature. The second step is conceptualization of the topic .

(p2.2) It addresses the point that an author of a review article must begin with a topic in need of review, a broad conception of what is known about the topic, and potential areas where new knowledge may be needed. In the following, we conceptualize persuasion, and embed it into a business context. Furthermore, we introduce related theories, and conclude a categorization for the successive literature review.

(p2.3) In persuasion, the persuader induces a particular kind of mental state in the persuadee , e.g., through threats, but unlike an expression of sentiment, persuasion intends a change in the mental state of the persuadee (Iyer & Sycara 2019). Contemporary psychology and communication science (Rocklage et al. 2018, Park et al. 2015, Hunter et al. 2019 require the persuader to be acting intentionally, that is, the persuasive act. In the context of NLG, we usually refer to messages generated or augmented by an artificial intelligence, if we use the term persuasive act.

(p2.4) In his seminal work ' On Rhetoric ', Aristotle introduced his well-known ontology for persuasive acts. Accordingly, persuasion depends on multiple facets: emotions ( pathos ), logical structure of the argument ( logos ), the context ( cairos ), and on the speaker ( ethos ) (Schiappa & Nordin 2013).

(p2.5) Likewise, contemporary business literature conceptualizes persuasive acts through ""principles of persuasion"" (DeMers 2016). The author concludes that six interventions help at achieving persuasiveness. The first is being confident and remaining confident during the entirety of an appeal. Next, the introduction of logical argumentation fosters persuasiveness, since individuals are more inclined to be persuaded by logic. Additionally, making an appeal seem beneficial to the other party, by demonstrating the value of an appeal, choosing words carefully (i.e., selecting from a vocabulary that may be more persuasive), and using flattery (i.e., finding appropriate compliments) are recommended. Lastly, DeMers (2016) reveals that being patient and persistent (i.e., not to greatly alter one's approach) strengthens a persuader's persuasiveness. Next, we embed the presented ""principles of persuasion"" into related theories on persuasion (Cameron 2009).

(p2.6) Festinger's Theory of Cognitive Dissonance (1957) focuses on the relationships among cognitive elements, which include beliefs, opinions, attitudes, or knowledge (O'Keefe 1990). This relates most to Aristotle's cairos , and the need to create benevolence for the persuadee. Evaluating the 'business principles', this theory resonates best with what DeMers (2016) defines as 'making [the cognition] appealing to the other party'. However cognitions, and thus, a persuadee's perceived benevolence, can be dissonant, consonant, or irrelevant to each other. If a persuadee is presented with a sufficiently vast cognitive inconsistency, then they will perceive psychological discomfort, leading to an attempt to restore their cognitive balance through a reduction or elimination of the inconsistency (Stiff 1994, Harmon-Jones 2002. The magnitude of dissonance determines one's motivation to reduce it (Stiff 1994, Festinger 1957. Approaches towards reducing dissonance are: changing terms to make them more consonant, adding additional consonant percipience, or altering the magnitude of the percipience (Harmon-Jones 2002, Stiff 1994, O'Keefe 1990.

(p2.7) In 'principles of persuasion', DeMers (2016) contends that appropriate flattering and the usage of so-called high value words contribute to persuasive acts in business contexts (cf. Aristotle's ethos ). Accordingly, Language Expectancy Theory (LET) identifies written or spoken language as a rule-based system through which persuadees develop expectations and preferences towards ''appropriate'' linguistic usage of words in varying situations (Burgoon & Miller 1985). Such expectations are frequently consistent with sociological and cultural norms, while preferences tend to relate to societal standards and cultural values (Burgoon & Miller 1985, Burgoon et al. 2002. Positive expectations that facilitate a persuasive act are, for instance, if a persuader stylizes a behavior that is perceived as more preferred than expected by the persuadee. In contrast, negative ones are inhibiting persuasion, e.g., when the persuader makes use of language that is considered to be socially unacceptable (Burgoon & Miller 1985, Burgoon et al. 2002.

(p2.8) Next, the 'principles of persuasion' confer that an argument based on logic is persuasive (DeMers 2016). What Aristotle terms logos is consistent with the theory of probabilistic models. Probabilistic models (McGuire 1981, Wyer 1970) are based on the rules of formal logic and probability, and predict beliefs regarding the conclusion of reasoning. These predictions are based on mathematical probability, and as such this theory is consistent with what Aristotle defines as logos . An exemplary belief syllogism (McGuire 1981) is composed of two premises that lead to a logical conclusion. The theorists (McGuire 1981, Wyer 1970 explain that believing in the premises leads to the expectation that the identified conclusion will follow. However, rather than solely thinking in all-or-nothing scenarios, beliefs can be ascertained through subjective probabilities: one's judgment of the probability that each of the premises is true (McGuire 1981, Wyer 1970. Furthermore, if a message evokes a perceptual change in the truth of the premise, or additional premises are supplemented, following this theory, a change in perceiving the conclusion is expected.

(p2.9) Last, Balance Theory focuses on the triadic relationship involving two individuals (e.g., persuader and persuadee), the persuadee's attitude toward the persuader (Aristotle's pathos ), and their attitudes toward an attitudinal object (Heider 1958). The resulting triad can be balanced or unbalanced: This triad is in balance if all three relationships are positive, or one is positive and two are negative. If all three relationships are negative, or one is negative and two are positive, an unbalanced triad results, often motivating one to alter one of the three relationships (Heider 1958). Building on this theory, we aim to identify relational determinants that relate to improving the relationship between the persuader and the persuadee towards the attitudinal object (Heider 1958). In the business framework, we relate those determinants towards the pattern of 'trustworthiness', that represent the persuadee's attitude towards the persuader. In his persuasion attempt, the persuader wants the persuadee to have a positive attitude.","Questions:



Answer:

(p2.0) This paper's methodology follows a framework proposed by vom Brocke et al. (2009) which is based on a screening of the review literature itself and especially highlights the need for comprehensively documenting the process of literature search in such an article (Duerr et al. 2016). The framework is structured into the following five steps: (1) definition of review scope, (2) conceptualization of topic, (3) literature search, (4) literature analysis and synthesis, and (5) research agenda. Each of the steps will be briefly explained, when it will be addressed in the course of this work.

(p2.1) The first step is the definition of the review scope of this literature review. It is summarized in Figure 1 (categories applicable to this review on Persuasion and NLG research are highlighted) which is based on the taxonomy adapted by vom Brocke et al. (2009).  Brocke et al. 2009) This literature review focuses on outcomes of applied research in the domains of Persuasion and Natural Language Generation. The goal is to integrate findings with respect to five categories concluded from the business problem of persuading individuals through textual exchange (DeMers 2016). These categories were chosen as they address the psychological and technical aspects of persuasion and are a prerequisite to creating a persuasive NLG artificial intelligence. We selected this field because the artificial generation of persuasion through NLG has, as this literature review reveals, not been addressed in seminal articles following our structured research approach. Persuasion is already commonly studied in psychology (Quirk et al. 1985, Marwell & Schmitt 1967. Also, numerous studies in natural language processing focus on identifying and classifying persuasion in an automated way (Li et al. 2020, Rocklage et al. 2018, Iyer & Sycara 2019). This paper is organized along a conceptual structure. We did not take a particular perspective to provide a neutral representation of the results. As an audience of this review specialized scholars having an interest in the field of persuasion and the artificial generation of it were chosen. For coverage, our literature review can be categorized as representative as our research has been limited to certain journals, but does not consider the totality of the literature. The second step is conceptualization of the topic .

(p2.2) It addresses the point that an author of a review article must begin with a topic in need of review, a broad conception of what is known about the topic, and potential areas where new knowledge may be needed. In the following, we conceptualize persuasion, and embed it into a business context. Furthermore, we introduce related theories, and conclude a categorization for the successive literature review.

(p2.3) In persuasion, the persuader induces a particular kind of mental state in the persuadee , e.g., through threats, but unlike an expression of sentiment, persuasion intends a change in the mental state of the persuadee (Iyer & Sycara 2019). Contemporary psychology and communication science (Rocklage et al. 2018, Park et al. 2015, Hunter et al. 2019 require the persuader to be acting intentionally, that is, the persuasive act. In the context of NLG, we usually refer to messages generated or augmented by an artificial intelligence, if we use the term persuasive act.

(p2.4) In his seminal work ' On Rhetoric ', Aristotle introduced his well-known ontology for persuasive acts. Accordingly, persuasion depends on multiple facets: emotions ( pathos ), logical structure of the argument ( logos ), the context ( cairos ), and on the speaker ( ethos ) (Schiappa & Nordin 2013).

(p2.5) Likewise, contemporary business literature conceptualizes persuasive acts through ""principles of persuasion"" (DeMers 2016). The author concludes that six interventions help at achieving persuasiveness. The first is being confident and remaining confident during the entirety of an appeal. Next, the introduction of logical argumentation fosters persuasiveness, since individuals are more inclined to be persuaded by logic. Additionally, making an appeal seem beneficial to the other party, by demonstrating the value of an appeal, choosing words carefully (i.e., selecting from a vocabulary that may be more persuasive), and using flattery (i.e., finding appropriate compliments) are recommended. Lastly, DeMers (2016) reveals that being patient and persistent (i.e., not to greatly alter one's approach) strengthens a persuader's persuasiveness. Next, we embed the presented ""principles of persuasion"" into related theories on persuasion (Cameron 2009).

(p2.6) Festinger's Theory of Cognitive Dissonance (1957) focuses on the relationships among cognitive elements, which include beliefs, opinions, attitudes, or knowledge (O'Keefe 1990). This relates most to Aristotle's cairos , and the need to create benevolence for the persuadee. Evaluating the 'business principles', this theory resonates best with what DeMers (2016) defines as 'making [the cognition] appealing to the other party'. However cognitions, and thus, a persuadee's perceived benevolence, can be dissonant, consonant, or irrelevant to each other. If a persuadee is presented with a sufficiently vast cognitive inconsistency, then they will perceive psychological discomfort, leading to an attempt to restore their cognitive balance through a reduction or elimination of the inconsistency (Stiff 1994, Harmon-Jones 2002. The magnitude of dissonance determines one's motivation to reduce it (Stiff 1994, Festinger 1957. Approaches towards reducing dissonance are: changing terms to make them more consonant, adding additional consonant percipience, or altering the magnitude of the percipience (Harmon-Jones 2002, Stiff 1994, O'Keefe 1990.

(p2.7) In 'principles of persuasion', DeMers (2016) contends that appropriate flattering and the usage of so-called high value words contribute to persuasive acts in business contexts (cf. Aristotle's ethos ). Accordingly, Language Expectancy Theory (LET) identifies written or spoken language as a rule-based system through which persuadees develop expectations and preferences towards ''appropriate'' linguistic usage of words in varying situations (Burgoon & Miller 1985). Such expectations are frequently consistent with sociological and cultural norms, while preferences tend to relate to societal standards and cultural values (Burgoon & Miller 1985, Burgoon et al. 2002. Positive expectations that facilitate a persuasive act are, for instance, if a persuader stylizes a behavior that is perceived as more preferred than expected by the persuadee. In contrast, negative ones are inhibiting persuasion, e.g., when the persuader makes use of language that is considered to be socially unacceptable (Burgoon & Miller 1985, Burgoon et al. 2002.

(p2.8) Next, the 'principles of persuasion' confer that an argument based on logic is persuasive (DeMers 2016). What Aristotle terms logos is consistent with the theory of probabilistic models. Probabilistic models (McGuire 1981, Wyer 1970) are based on the rules of formal logic and probability, and predict beliefs regarding the conclusion of reasoning. These predictions are based on mathematical probability, and as such this theory is consistent with what Aristotle defines as logos . An exemplary belief syllogism (McGuire 1981) is composed of two premises that lead to a logical conclusion. The theorists (McGuire 1981, Wyer 1970 explain that believing in the premises leads to the expectation that the identified conclusion will follow. However, rather than solely thinking in all-or-nothing scenarios, beliefs can be ascertained through subjective probabilities: one's judgment of the probability that each of the premises is true (McGuire 1981, Wyer 1970. Furthermore, if a message evokes a perceptual change in the truth of the premise, or additional premises are supplemented, following this theory, a change in perceiving the conclusion is expected.

(p2.9) Last, Balance Theory focuses on the triadic relationship involving two individuals (e.g., persuader and persuadee), the persuadee's attitude toward the persuader (Aristotle's pathos ), and their attitudes toward an attitudinal object (Heider 1958). The resulting triad can be balanced or unbalanced: This triad is in balance if all three relationships are positive, or one is positive and two are negative. If all three relationships are negative, or one is negative and two are positive, an unbalanced triad results, often motivating one to alter one of the three relationships (Heider 1958). Building on this theory, we aim to identify relational determinants that relate to improving the relationship between the persuader and the persuadee towards the attitudinal object (Heider 1958). In the business framework, we relate those determinants towards the pattern of 'trustworthiness', that represent the persuadee's attitude towards the persuader. In his persuasion attempt, the persuader wants the persuadee to have a positive attitude."
231603122,Persuasive Natural Language Generation -A Literature Review,"Business, Linguistics, Computer Science",https://www.semanticscholar.org/paper/e9ac35f68ef4290d74551bafdfb09a4ea0983940,s5,Benevolence,"['p5.0', 'p5.1', 'p5.2', 'p5.3']","[""Determinants that aim at creating value for the persuadee are subsumed in this category (DeMers 2016, Voss & Raz 2016). In line with Cognitive Dissonance Theory (Festinger 1957) the identified eight determinants relate to altering a persuadee's perceived benevolence through dissonant or consonant measures (summarized in Table 3). An implementation in a persuasive NLG AI can be facilitated through identifying their absence or impact (Hunter et al 2019, Zarouali et al. 2020. The benevolence determinants are ordered alphabetically to not imply a specific order. The first column presents the determinants, the second column concisely defines each, and the third provides examples for all determinants that were identified. The last column states the corresponding citations. Linguistic Appropriacy"", ""This category subsumes fourteen determinants that facilitate an individual's stylome and aim at matching this with linguistic appropriacy. Such a stylome can be quantified and identified linguistically (Zarouali et al. 2020). Aforementioned Language Expectation Theory identifies written or spoken language as a rule-based system through which persuadees develop expectations (Burgoon & Miller 1985). The reason for profiling the stylome of an individual is to match these expectations (Park et al. 2015). Once implemented, a persuasive NLG AI can achieve congruence between a persuasive message and the persuadee, and thus generate persuasiveness. Table 2 introduces fourteen determinants of linguistic appropriacy in alphabetical order, provides a synopsis (i.e., brief summary, column two), an example for each determinant (column three), and the corresponding academic citation (in column four).  Quirk et al. 1985, Brewer 1980 Evidence Words Tendency to approve or disapprove something. Words such as: according to. Quirk et al. 1985 Familiarity Degree of familiarity of a word or how easily a word can be recognized by an adult. Word Frequency Indication of how often used words occur in a given language."", 'More uncommon words reflect that the writer possesses larger vocabulary. Baayen et al. 1995 Logical Argumentation', ""Previous academic works unveil that arguments with consistent logic in persuasive acts increase persuasiveness (Cialdini & Goldstein 2002, Walton et al. 2008, Block et al. 2019. In line with the theory of Probabilistic Models (McGuire 1981, Wyer 1970, it is assumed that conclusive statements lead to a persuadee's expectation that a conclusion will follow. Technical implementations of logical argumentation or logical meaning representations occur as first order logic or semantic argumentation graphs (Moens 2018, Block et al. 2019). The first column of Table 4 enumerates our fourteen logical argumentation determinants, while the second provides a synopsis. Column three provides an example, and column four the corresponding citation in which the factor was identified. As in previous tables, the determinants are merely sorted alphabetically. ""]","Determinants that aim at creating value for the persuadee are subsumed in this category (DeMers 2016, Voss & Raz 2016). In line with Cognitive Dissonance Theory (Festinger 1957) the identified eight determinants relate to altering a persuadee's perceived benevolence through dissonant or consonant measures (summarized in Table 3). An implementation in a persuasive NLG AI can be facilitated through identifying their absence or impact (Hunter et al 2019, Zarouali et al. 2020. The benevolence determinants are ordered alphabetically to not imply a specific order. The first column presents the determinants, the second column concisely defines each, and the third provides examples for all determinants that were identified. The last column states the corresponding citations. Linguistic Appropriacy

This category subsumes fourteen determinants that facilitate an individual's stylome and aim at matching this with linguistic appropriacy. Such a stylome can be quantified and identified linguistically (Zarouali et al. 2020). Aforementioned Language Expectation Theory identifies written or spoken language as a rule-based system through which persuadees develop expectations (Burgoon & Miller 1985). The reason for profiling the stylome of an individual is to match these expectations (Park et al. 2015). Once implemented, a persuasive NLG AI can achieve congruence between a persuasive message and the persuadee, and thus generate persuasiveness. Table 2 introduces fourteen determinants of linguistic appropriacy in alphabetical order, provides a synopsis (i.e., brief summary, column two), an example for each determinant (column three), and the corresponding academic citation (in column four).  Quirk et al. 1985, Brewer 1980 Evidence Words Tendency to approve or disapprove something. Words such as: according to. Quirk et al. 1985 Familiarity Degree of familiarity of a word or how easily a word can be recognized by an adult. Word Frequency Indication of how often used words occur in a given language.

More uncommon words reflect that the writer possesses larger vocabulary. Baayen et al. 1995 Logical Argumentation

Previous academic works unveil that arguments with consistent logic in persuasive acts increase persuasiveness (Cialdini & Goldstein 2002, Walton et al. 2008, Block et al. 2019. In line with the theory of Probabilistic Models (McGuire 1981, Wyer 1970, it is assumed that conclusive statements lead to a persuadee's expectation that a conclusion will follow. Technical implementations of logical argumentation or logical meaning representations occur as first order logic or semantic argumentation graphs (Moens 2018, Block et al. 2019). The first column of Table 4 enumerates our fourteen logical argumentation determinants, while the second provides a synopsis. Column three provides an example, and column four the corresponding citation in which the factor was identified. As in previous tables, the determinants are merely sorted alphabetically. ","(p5.0) Determinants that aim at creating value for the persuadee are subsumed in this category (DeMers 2016, Voss & Raz 2016). In line with Cognitive Dissonance Theory (Festinger 1957) the identified eight determinants relate to altering a persuadee's perceived benevolence through dissonant or consonant measures (summarized in Table 3). An implementation in a persuasive NLG AI can be facilitated through identifying their absence or impact (Hunter et al 2019, Zarouali et al. 2020. The benevolence determinants are ordered alphabetically to not imply a specific order. The first column presents the determinants, the second column concisely defines each, and the third provides examples for all determinants that were identified. The last column states the corresponding citations. Linguistic Appropriacy

(p5.1) This category subsumes fourteen determinants that facilitate an individual's stylome and aim at matching this with linguistic appropriacy. Such a stylome can be quantified and identified linguistically (Zarouali et al. 2020). Aforementioned Language Expectation Theory identifies written or spoken language as a rule-based system through which persuadees develop expectations (Burgoon & Miller 1985). The reason for profiling the stylome of an individual is to match these expectations (Park et al. 2015). Once implemented, a persuasive NLG AI can achieve congruence between a persuasive message and the persuadee, and thus generate persuasiveness. Table 2 introduces fourteen determinants of linguistic appropriacy in alphabetical order, provides a synopsis (i.e., brief summary, column two), an example for each determinant (column three), and the corresponding academic citation (in column four).  Quirk et al. 1985, Brewer 1980 Evidence Words Tendency to approve or disapprove something. Words such as: according to. Quirk et al. 1985 Familiarity Degree of familiarity of a word or how easily a word can be recognized by an adult. Word Frequency Indication of how often used words occur in a given language.

(p5.2) More uncommon words reflect that the writer possesses larger vocabulary. Baayen et al. 1995 Logical Argumentation

(p5.3) Previous academic works unveil that arguments with consistent logic in persuasive acts increase persuasiveness (Cialdini & Goldstein 2002, Walton et al. 2008, Block et al. 2019. In line with the theory of Probabilistic Models (McGuire 1981, Wyer 1970, it is assumed that conclusive statements lead to a persuadee's expectation that a conclusion will follow. Technical implementations of logical argumentation or logical meaning representations occur as first order logic or semantic argumentation graphs (Moens 2018, Block et al. 2019). The first column of Table 4 enumerates our fourteen logical argumentation determinants, while the second provides a synopsis. Column three provides an example, and column four the corresponding citation in which the factor was identified. As in previous tables, the determinants are merely sorted alphabetically. ","[['b86', 'b41', 'b76'], ['b62', 'b8', None, 'b86', 'b64'], ['b5'], ['b83', 'b77', 'b56', 'b7', None, 'b58']]","[['b86', 'b41', 'b76'], ['b62', 'b8', None, 'b86', 'b64'], ['b5'], ['b83', 'b77', 'b56', 'b7', None, 'b58']]",15,"1. Determinants that aim at creating value for the persuadee are subsumed in this category (DeMers 2016, Voss & Raz 2016).
2. In line with Cognitive Dissonance Theory (Festinger 1957)
3. the identified eight determinants relate to altering a persuadee's perceived benevolence through dissonant or consonant measures (summarized in Table 3).
4. An implementation in a persuasive NLG AI can be facilitated through identifying their absence or impact (Hunter et al 2019, Zarouali et al. 2020.
5. The benevolence determinants are ordered alphabetically to not imply a specific order.
6. The first column presents the determinants, the second column concisely defines each, and the third provides examples for all determinants that were identified.
7. The last column states the corresponding citations.
8. Linguistic AppropriacyThis category subsumes fourteen determinants that facilitate an individual's stylome and aim at matching this with linguistic appropriacy.
9. Such a stylome can be quantified and identified linguistically (Zarouali et al. 2020).
10. Aforementioned Language Expectation Theory identifies written or spoken language as a rule-based system through which persuadees develop expectations (Burgoon & Miller 1985).
11. The reason for profiling the stylome of an individual is to match these expectations (Park et al. 2015).
12. Once implemented, a persuasive NLG AI can achieve congruence between a persuasive message and the persuadee, and thus generate persuasiveness.
13. Table 2 introduces fourteen determinants of linguistic appropriacy in alphabetical order, provides a synopsis (i.e., brief summary, column two), an example for each determinant (column three), and the corresponding academic citation (in column four).
14. Quirk et al. 1985, Brewer 1980 Evidence Words Tendency to approve or disapprove something.
15. Words such as: according to. Quirk et al. 1985 Familiarity Degree of familiarity of a word or how easily a word can be recognized by an adult.
16. Word Frequency Indication of how often used words occur in a given language.
17. More uncommon words reflect that the writer possesses larger vocabulary.
18. Baayen et al. 1995 Logical ArgumentationPrevious academic works unveil that arguments with consistent logic in persuasive acts increase persuasiveness (Cialdini & Goldstein 2002, Walton et al. 2008, Block et al. 2019. In line with the theory of Probabilistic Models (McGuire 1981, Wyer 1970, it is assumed that conclusive statements lead to a persuadee's expectation that a conclusion will follow. Technical implementations of logical argumentation or logical meaning representations occur as first order logic or semantic argumentation graphs (Moens 2018, Block et al. 2019).
19. The first column of Table 4 enumerates our fourteen logical argumentation determinants, while the second provides a synopsis.
20. Column three provides an example, and column four the corresponding citation in which the factor was identified.
21. As in previous tables, the determinants are merely sorted alphabetically.","How does Cognitive Dissonance Theory apply to altering a persuadee's perceived benevolence in persuasive natural language generation?
What are the determinants of linguistic appropriacy in persuasive NLG AI, and how do they contribute to matching an individual's stylome?
How can the implementation of persuasive NLG AI achieve congruence between a persuasive message and the persuadee?
What role does logical argumentation play in increasing persuasiveness according to previous academic works?
How are logical argumentation determinants implemented in persuasive natural language generation AI to enhance persuasiveness?","1. How does Cognitive Dissonance Theory apply to altering a persuadee's perceived benevolence in persuasive natural language generation?
2. What are the determinants of linguistic appropriacy in persuasive NLG AI, and how do they contribute to matching an individual's stylome?
3. How can the implementation of persuasive NLG AI achieve congruence between a persuasive message and the persuadee?
4. What role does logical argumentation play in increasing persuasiveness according to previous academic works?
5. How are logical argumentation determinants implemented in persuasive natural language generation AI to enhance persuasiveness?",None,,"Questions:

1. How does Cognitive Dissonance Theory apply to altering a persuadee's perceived benevolence in persuasive natural language generation?
2. What are the determinants of linguistic appropriacy in persuasive NLG AI, and how do they contribute to matching an individual's stylome?
3. How can the implementation of persuasive NLG AI achieve congruence between a persuasive message and the persuadee?
4. What role does logical argumentation play in increasing persuasiveness according to previous academic works?
5. How are logical argumentation determinants implemented in persuasive natural language generation AI to enhance persuasiveness?

Answer:

(p5.0) Determinants that aim at creating value for the persuadee are subsumed in this category (DeMers 2016, Voss & Raz 2016). In line with Cognitive Dissonance Theory (Festinger 1957) the identified eight determinants relate to altering a persuadee's perceived benevolence through dissonant or consonant measures (summarized in Table 3). An implementation in a persuasive NLG AI can be facilitated through identifying their absence or impact (Hunter et al 2019, Zarouali et al. 2020. The benevolence determinants are ordered alphabetically to not imply a specific order. The first column presents the determinants, the second column concisely defines each, and the third provides examples for all determinants that were identified. The last column states the corresponding citations. Linguistic Appropriacy

(p5.1) This category subsumes fourteen determinants that facilitate an individual's stylome and aim at matching this with linguistic appropriacy. Such a stylome can be quantified and identified linguistically (Zarouali et al. 2020). Aforementioned Language Expectation Theory identifies written or spoken language as a rule-based system through which persuadees develop expectations (Burgoon & Miller 1985). The reason for profiling the stylome of an individual is to match these expectations (Park et al. 2015). Once implemented, a persuasive NLG AI can achieve congruence between a persuasive message and the persuadee, and thus generate persuasiveness. Table 2 introduces fourteen determinants of linguistic appropriacy in alphabetical order, provides a synopsis (i.e., brief summary, column two), an example for each determinant (column three), and the corresponding academic citation (in column four).  Quirk et al. 1985, Brewer 1980 Evidence Words Tendency to approve or disapprove something. Words such as: according to. Quirk et al. 1985 Familiarity Degree of familiarity of a word or how easily a word can be recognized by an adult. Word Frequency Indication of how often used words occur in a given language.

(p5.2) More uncommon words reflect that the writer possesses larger vocabulary. Baayen et al. 1995 Logical Argumentation

(p5.3) Previous academic works unveil that arguments with consistent logic in persuasive acts increase persuasiveness (Cialdini & Goldstein 2002, Walton et al. 2008, Block et al. 2019. In line with the theory of Probabilistic Models (McGuire 1981, Wyer 1970, it is assumed that conclusive statements lead to a persuadee's expectation that a conclusion will follow. Technical implementations of logical argumentation or logical meaning representations occur as first order logic or semantic argumentation graphs (Moens 2018, Block et al. 2019). The first column of Table 4 enumerates our fourteen logical argumentation determinants, while the second provides a synopsis. Column three provides an example, and column four the corresponding citation in which the factor was identified. As in previous tables, the determinants are merely sorted alphabetically. ","Questions:



Answer:

(p5.0) Determinants that aim at creating value for the persuadee are subsumed in this category (DeMers 2016, Voss & Raz 2016). In line with Cognitive Dissonance Theory (Festinger 1957) the identified eight determinants relate to altering a persuadee's perceived benevolence through dissonant or consonant measures (summarized in Table 3). An implementation in a persuasive NLG AI can be facilitated through identifying their absence or impact (Hunter et al 2019, Zarouali et al. 2020. The benevolence determinants are ordered alphabetically to not imply a specific order. The first column presents the determinants, the second column concisely defines each, and the third provides examples for all determinants that were identified. The last column states the corresponding citations. Linguistic Appropriacy

(p5.1) This category subsumes fourteen determinants that facilitate an individual's stylome and aim at matching this with linguistic appropriacy. Such a stylome can be quantified and identified linguistically (Zarouali et al. 2020). Aforementioned Language Expectation Theory identifies written or spoken language as a rule-based system through which persuadees develop expectations (Burgoon & Miller 1985). The reason for profiling the stylome of an individual is to match these expectations (Park et al. 2015). Once implemented, a persuasive NLG AI can achieve congruence between a persuasive message and the persuadee, and thus generate persuasiveness. Table 2 introduces fourteen determinants of linguistic appropriacy in alphabetical order, provides a synopsis (i.e., brief summary, column two), an example for each determinant (column three), and the corresponding academic citation (in column four).  Quirk et al. 1985, Brewer 1980 Evidence Words Tendency to approve or disapprove something. Words such as: according to. Quirk et al. 1985 Familiarity Degree of familiarity of a word or how easily a word can be recognized by an adult. Word Frequency Indication of how often used words occur in a given language.

(p5.2) More uncommon words reflect that the writer possesses larger vocabulary. Baayen et al. 1995 Logical Argumentation

(p5.3) Previous academic works unveil that arguments with consistent logic in persuasive acts increase persuasiveness (Cialdini & Goldstein 2002, Walton et al. 2008, Block et al. 2019. In line with the theory of Probabilistic Models (McGuire 1981, Wyer 1970, it is assumed that conclusive statements lead to a persuadee's expectation that a conclusion will follow. Technical implementations of logical argumentation or logical meaning representations occur as first order logic or semantic argumentation graphs (Moens 2018, Block et al. 2019). The first column of Table 4 enumerates our fourteen logical argumentation determinants, while the second provides a synopsis. Column three provides an example, and column four the corresponding citation in which the factor was identified. As in previous tables, the determinants are merely sorted alphabetically. "
231603122,Persuasive Natural Language Generation -A Literature Review,"Business, Linguistics, Computer Science",https://www.semanticscholar.org/paper/e9ac35f68ef4290d74551bafdfb09a4ea0983940,s6,Trustworthiness,['p6.0'],"[""Trust plays an important role in the persuader-persuadee relationship. If established, the persuadee's attitude toward the persuader -as identified in Balance Theory (Heider 1958) -helps a persuadee to reason about the reciprocative nature, honesty or reliability of the counterpart (Kim & Duhachek 2020). An implementation of trustworthiness can, amongst others, be realized through identifying a persuadee's psychological profile (e.g., extroverted individuals respond better to texts that have a positive valence, and are in that case more persuadable, Zarouali et al. 2020, Park et al. 2015 to influence the degree of persuasiveness of a persuasive act. This category collates fifteen determinants pertaining to the increase 8 Determinant""]","Trust plays an important role in the persuader-persuadee relationship. If established, the persuadee's attitude toward the persuader -as identified in Balance Theory (Heider 1958) -helps a persuadee to reason about the reciprocative nature, honesty or reliability of the counterpart (Kim & Duhachek 2020). An implementation of trustworthiness can, amongst others, be realized through identifying a persuadee's psychological profile (e.g., extroverted individuals respond better to texts that have a positive valence, and are in that case more persuadable, Zarouali et al. 2020, Park et al. 2015 to influence the degree of persuasiveness of a persuasive act. This category collates fifteen determinants pertaining to the increase 8 Determinant","(p6.0) Trust plays an important role in the persuader-persuadee relationship. If established, the persuadee's attitude toward the persuader -as identified in Balance Theory (Heider 1958) -helps a persuadee to reason about the reciprocative nature, honesty or reliability of the counterpart (Kim & Duhachek 2020). An implementation of trustworthiness can, amongst others, be realized through identifying a persuadee's psychological profile (e.g., extroverted individuals respond better to texts that have a positive valence, and are in that case more persuadable, Zarouali et al. 2020, Park et al. 2015 to influence the degree of persuasiveness of a persuasive act. This category collates fifteen determinants pertaining to the increase 8 Determinant","[['b62', 'b35', 'b86', 'b48']]","[['b62', 'b35', 'b86', 'b48']]",4,"1. Trust plays an important role in the persuader-persuadee relationship.
2. If established, the persuadee's attitude toward the persuader -as identified in Balance Theory (Heider 1958) -helps a persuadee to reason about the reciprocative nature, honesty or reliability of the counterpart (Kim & Duhachek 2020).
3. An implementation of trustworthiness can, amongst others, be realized through identifying a persuadee's psychological profile (e.g., extroverted individuals respond better to texts that have a positive valence, and are in that case more persuadable, Zarouali et al. 2020, Park et al. 2015 to influence the degree of persuasiveness of a persuasive act.
4. This category collates fifteen determinants pertaining to the increase 8 Determinant","How does Balance Theory explain the role of trust in the persuader-persuadee relationship?
What are the implications of a persuadee's psychological profile on the effectiveness of persuasive natural language generation?
How can trustworthiness be implemented in persuasive communication according to recent literature?
What specific psychological traits have been identified to influence the persuasiveness of texts?
Can you detail the determinants that increase the effectiveness of persuasive acts as identified in the literature review?","1. How does Balance Theory explain the role of trust in the persuader-persuadee relationship?
2. What are the implications of a persuadee's psychological profile on the effectiveness of persuasive natural language generation?
3. How can trustworthiness be implemented in persuasive communication according to recent literature?
4. What specific psychological traits have been identified to influence the persuasiveness of texts?
5. Can you detail the determinants that increase the effectiveness of persuasive acts as identified in the literature review?",None,,"Questions:

1. How does Balance Theory explain the role of trust in the persuader-persuadee relationship?
2. What are the implications of a persuadee's psychological profile on the effectiveness of persuasive natural language generation?
3. How can trustworthiness be implemented in persuasive communication according to recent literature?
4. What specific psychological traits have been identified to influence the persuasiveness of texts?
5. Can you detail the determinants that increase the effectiveness of persuasive acts as identified in the literature review?

Answer:

(p6.0) Trust plays an important role in the persuader-persuadee relationship. If established, the persuadee's attitude toward the persuader -as identified in Balance Theory (Heider 1958) -helps a persuadee to reason about the reciprocative nature, honesty or reliability of the counterpart (Kim & Duhachek 2020). An implementation of trustworthiness can, amongst others, be realized through identifying a persuadee's psychological profile (e.g., extroverted individuals respond better to texts that have a positive valence, and are in that case more persuadable, Zarouali et al. 2020, Park et al. 2015 to influence the degree of persuasiveness of a persuasive act. This category collates fifteen determinants pertaining to the increase 8 Determinant","Questions:



Answer:

(p6.0) Trust plays an important role in the persuader-persuadee relationship. If established, the persuadee's attitude toward the persuader -as identified in Balance Theory (Heider 1958) -helps a persuadee to reason about the reciprocative nature, honesty or reliability of the counterpart (Kim & Duhachek 2020). An implementation of trustworthiness can, amongst others, be realized through identifying a persuadee's psychological profile (e.g., extroverted individuals respond better to texts that have a positive valence, and are in that case more persuadable, Zarouali et al. 2020, Park et al. 2015 to influence the degree of persuasiveness of a persuasive act. This category collates fifteen determinants pertaining to the increase 8 Determinant"
231603122,Persuasive Natural Language Generation -A Literature Review,"Business, Linguistics, Computer Science",https://www.semanticscholar.org/paper/e9ac35f68ef4290d74551bafdfb09a4ea0983940,s10,Tools & Datasets,"['p10.0', 'p10.1', 'p10.2', 'p10.3', 'p10.4', 'p10.5']","['In the analyzed academic studies, we found that the authors use different datasets and tools to computationally process data for technical analyses of persuasion in NLP or NLG (e.g., in Guerini et al 2008a/b, Li et al. 2020, Iyer/Sycara 2019. Logically, the implementation of a persuasive NLG AI also depends on a variety of relevant tools and datasets which we identified and consolidated in Table 6. This table classifies our findings in types which are either tool or datasets (column one). We identified six tools and seventeen persuasion or message datasets. A software tool that is used in the context of persuasion and NLP, and datasets were chosen if they were used in the context of persuasion, textual exchange/debate and NLP. We further added a synopsis (column three) explaining every tool and Authority Appealing or making reference to higher authority or experts to persuade.', 'We called your mom Mariam and she says please put the gun down and come outside. Cialdini & Goldstein 2002, Catellani et al. 2020 Seeking Comprehension', 'Instead of prioritizing own arguments, it is wise to focus on understanding the persuadee.', ""What do you mean by that ? Fisher Uri 1981, Kouzehgar et al. 2015 Construal Learning involves the generalization and abstraction from one's repeated experiences which is a high-construal mental process."", 'A short-term investor as opposed to long-term investor may rely more on a financial artificial intelligence. Kim & Duhachek 2020, Abdallah et al. 2009 Emotionality The elicitation of positive or negative emotions to impose more weight on words.', 'Inclusion of words or expressions such as ""amazing"" or ""excellent"". Rocklage et al. 2018 Empathy This is a very good positive book that will make you very happy. Guerini et al. 2008b, Zarouali et al. 2020 dataset, providing a link (column four, if applicable) and the respective citation of the tool or dataset (column five). The tools and datasets are sorted alphabetically. ']","In the analyzed academic studies, we found that the authors use different datasets and tools to computationally process data for technical analyses of persuasion in NLP or NLG (e.g., in Guerini et al 2008a/b, Li et al. 2020, Iyer/Sycara 2019. Logically, the implementation of a persuasive NLG AI also depends on a variety of relevant tools and datasets which we identified and consolidated in Table 6. This table classifies our findings in types which are either tool or datasets (column one). We identified six tools and seventeen persuasion or message datasets. A software tool that is used in the context of persuasion and NLP, and datasets were chosen if they were used in the context of persuasion, textual exchange/debate and NLP. We further added a synopsis (column three) explaining every tool and Authority Appealing or making reference to higher authority or experts to persuade.

We called your mom Mariam and she says please put the gun down and come outside. Cialdini & Goldstein 2002, Catellani et al. 2020 Seeking Comprehension

Instead of prioritizing own arguments, it is wise to focus on understanding the persuadee.

What do you mean by that ? Fisher Uri 1981, Kouzehgar et al. 2015 Construal Learning involves the generalization and abstraction from one's repeated experiences which is a high-construal mental process.

A short-term investor as opposed to long-term investor may rely more on a financial artificial intelligence. Kim & Duhachek 2020, Abdallah et al. 2009 Emotionality The elicitation of positive or negative emotions to impose more weight on words.

Inclusion of words or expressions such as ""amazing"" or ""excellent"". Rocklage et al. 2018 Empathy This is a very good positive book that will make you very happy. Guerini et al. 2008b, Zarouali et al. 2020 dataset, providing a link (column four, if applicable) and the respective citation of the tool or dataset (column five). The tools and datasets are sorted alphabetically. ","(p10.0) In the analyzed academic studies, we found that the authors use different datasets and tools to computationally process data for technical analyses of persuasion in NLP or NLG (e.g., in Guerini et al 2008a/b, Li et al. 2020, Iyer/Sycara 2019. Logically, the implementation of a persuasive NLG AI also depends on a variety of relevant tools and datasets which we identified and consolidated in Table 6. This table classifies our findings in types which are either tool or datasets (column one). We identified six tools and seventeen persuasion or message datasets. A software tool that is used in the context of persuasion and NLP, and datasets were chosen if they were used in the context of persuasion, textual exchange/debate and NLP. We further added a synopsis (column three) explaining every tool and Authority Appealing or making reference to higher authority or experts to persuade.

(p10.1) We called your mom Mariam and she says please put the gun down and come outside. Cialdini & Goldstein 2002, Catellani et al. 2020 Seeking Comprehension

(p10.2) Instead of prioritizing own arguments, it is wise to focus on understanding the persuadee.

(p10.3) What do you mean by that ? Fisher Uri 1981, Kouzehgar et al. 2015 Construal Learning involves the generalization and abstraction from one's repeated experiences which is a high-construal mental process.

(p10.4) A short-term investor as opposed to long-term investor may rely more on a financial artificial intelligence. Kim & Duhachek 2020, Abdallah et al. 2009 Emotionality The elicitation of positive or negative emotions to impose more weight on words.

(p10.5) Inclusion of words or expressions such as ""amazing"" or ""excellent"". Rocklage et al. 2018 Empathy This is a very good positive book that will make you very happy. Guerini et al. 2008b, Zarouali et al. 2020 dataset, providing a link (column four, if applicable) and the respective citation of the tool or dataset (column five). The tools and datasets are sorted alphabetically. ","[['b30', None, 'b43'], [None, 'b14'], [], [None, 'b50'], ['b0', 'b48'], ['b66', 'b86', 'b31']]","[['b30', None, 'b43'], [None, 'b14'], [], [None, 'b50'], ['b0', 'b48'], ['b66', 'b86', 'b31']]",12,"1. In the analyzed academic studies, we found that the authors use different datasets and tools to computationally process data for technical analyses of persuasion in NLP or NLG (e.g., in Guerini et al 2008a/b, Li et al. 2020, Iyer/Sycara 2019. Logically, the implementation of a persuasive NLG AI also depends on a variety of relevant tools and datasets which we identified and consolidated in Table 6. This table classifies our findings in types which are either tool or datasets (column one).
2. We identified six tools and seventeen persuasion or message datasets.
3. A software tool that is used in the context of persuasion and NLP, and datasets were chosen if they were used in the context of persuasion, textual exchange/debate and NLP.
4. We further added a synopsis (column three) explaining every tool and Authority Appealing or making reference to higher authority or experts to persuade.
5. We called your mom Mariam and she says please put the gun down and come outside.
6. Cialdini & Goldstein 2002, Catellani et al. 2020
7. Seeking ComprehensionInstead of prioritizing own arguments, it is wise to focus on understanding the persuadee.
8. What do you mean by that ? Fisher Uri 1981, Kouzehgar et al. 2015 Construal Learning involves the generalization and abstraction from one's repeated experiences which is a high-construal mental process.
9. A short-term investor as opposed to long-term investor may rely more on a financial artificial intelligence.
10. Kim & Duhachek 2020, Abdallah et al. 2009
11. Emotionality The elicitation of positive or negative emotions to impose more weight on words.
12. Inclusion of words or expressions such as ""amazing"" or ""excellent"".
13. Rocklage et al. 2018 Empathy This is a very good positive book that will make you very happy.
14. Guerini et al. 2008b, Zarouali et al. 2020 dataset, providing a link (column four, if applicable) and the respective citation of the tool or dataset (column five).
15. The tools and datasets are sorted alphabetically.","What are the key datasets and tools used in computational analyses of persuasion within NLP and NLG fields?
How do the identified tools and datasets contribute to the technical analysis of persuasion in natural language processing?
What criteria were used to select the datasets and tools for persuasive natural language generation research?
Can you explain the role of emotional appeal in persuasive natural language generation according to the literature?
What is the significance of empathy in enhancing the persuasiveness of natural language generation, based on academic findings?","1. What are the key datasets and tools used in computational analyses of persuasion within NLP and NLG fields?
2. How do the identified tools and datasets contribute to the technical analysis of persuasion in natural language processing?
3. What criteria were used to select the datasets and tools for persuasive natural language generation research?
4. Can you explain the role of emotional appeal in persuasive natural language generation according to the literature?
5. What is the significance of empathy in enhancing the persuasiveness of natural language generation, based on academic findings?",None,,"Questions:

1. What are the key datasets and tools used in computational analyses of persuasion within NLP and NLG fields?
2. How do the identified tools and datasets contribute to the technical analysis of persuasion in natural language processing?
3. What criteria were used to select the datasets and tools for persuasive natural language generation research?
4. Can you explain the role of emotional appeal in persuasive natural language generation according to the literature?
5. What is the significance of empathy in enhancing the persuasiveness of natural language generation, based on academic findings?

Answer:

(p10.0) In the analyzed academic studies, we found that the authors use different datasets and tools to computationally process data for technical analyses of persuasion in NLP or NLG (e.g., in Guerini et al 2008a/b, Li et al. 2020, Iyer/Sycara 2019. Logically, the implementation of a persuasive NLG AI also depends on a variety of relevant tools and datasets which we identified and consolidated in Table 6. This table classifies our findings in types which are either tool or datasets (column one). We identified six tools and seventeen persuasion or message datasets. A software tool that is used in the context of persuasion and NLP, and datasets were chosen if they were used in the context of persuasion, textual exchange/debate and NLP. We further added a synopsis (column three) explaining every tool and Authority Appealing or making reference to higher authority or experts to persuade.

(p10.1) We called your mom Mariam and she says please put the gun down and come outside. Cialdini & Goldstein 2002, Catellani et al. 2020 Seeking Comprehension

(p10.2) Instead of prioritizing own arguments, it is wise to focus on understanding the persuadee.

(p10.3) What do you mean by that ? Fisher Uri 1981, Kouzehgar et al. 2015 Construal Learning involves the generalization and abstraction from one's repeated experiences which is a high-construal mental process.

(p10.4) A short-term investor as opposed to long-term investor may rely more on a financial artificial intelligence. Kim & Duhachek 2020, Abdallah et al. 2009 Emotionality The elicitation of positive or negative emotions to impose more weight on words.

(p10.5) Inclusion of words or expressions such as ""amazing"" or ""excellent"". Rocklage et al. 2018 Empathy This is a very good positive book that will make you very happy. Guerini et al. 2008b, Zarouali et al. 2020 dataset, providing a link (column four, if applicable) and the respective citation of the tool or dataset (column five). The tools and datasets are sorted alphabetically. ","Questions:



Answer:

(p10.0) In the analyzed academic studies, we found that the authors use different datasets and tools to computationally process data for technical analyses of persuasion in NLP or NLG (e.g., in Guerini et al 2008a/b, Li et al. 2020, Iyer/Sycara 2019. Logically, the implementation of a persuasive NLG AI also depends on a variety of relevant tools and datasets which we identified and consolidated in Table 6. This table classifies our findings in types which are either tool or datasets (column one). We identified six tools and seventeen persuasion or message datasets. A software tool that is used in the context of persuasion and NLP, and datasets were chosen if they were used in the context of persuasion, textual exchange/debate and NLP. We further added a synopsis (column three) explaining every tool and Authority Appealing or making reference to higher authority or experts to persuade.

(p10.1) We called your mom Mariam and she says please put the gun down and come outside. Cialdini & Goldstein 2002, Catellani et al. 2020 Seeking Comprehension

(p10.2) Instead of prioritizing own arguments, it is wise to focus on understanding the persuadee.

(p10.3) What do you mean by that ? Fisher Uri 1981, Kouzehgar et al. 2015 Construal Learning involves the generalization and abstraction from one's repeated experiences which is a high-construal mental process.

(p10.4) A short-term investor as opposed to long-term investor may rely more on a financial artificial intelligence. Kim & Duhachek 2020, Abdallah et al. 2009 Emotionality The elicitation of positive or negative emotions to impose more weight on words.

(p10.5) Inclusion of words or expressions such as ""amazing"" or ""excellent"". Rocklage et al. 2018 Empathy This is a very good positive book that will make you very happy. Guerini et al. 2008b, Zarouali et al. 2020 dataset, providing a link (column four, if applicable) and the respective citation of the tool or dataset (column five). The tools and datasets are sorted alphabetically. "
231603122,Persuasive Natural Language Generation -A Literature Review,"Business, Linguistics, Computer Science",https://www.semanticscholar.org/paper/e9ac35f68ef4290d74551bafdfb09a4ea0983940,s11,Discussion,"['p11.0', 'p11.1', 'p11.2', 'p11.3', 'p11.4', 'p11.5']","['This section displays the last step of the framework for literature reviewing (Vom Brocke et al. 2009): (5) Developing a research agenda.', ""For our proposed agenda for future research in the field of persuasive NLG (Figure 2), we conclude that an unambiguous and concise comprehension of how the forty-nine identified determinants influence the generation of persuasive artificially generated messages (i.e., the persuasive act) is needed. Furthermore, twenty-one tools & datasets were identified that allow one to train generative deep neural nets within the scope of persuasive NLG artificial intelligence. Next, we conclude our research proposals. Our framework encompasses four identified categories (based on the 'principles of persuasion' and embedded into academic theories, i.e. Cognitive Dissonance, Language Expectancy, Probabilistic Models, and Balance Theory) as prerequisites for persuasive NLG that have not been comprehensively considered in the journals considered for this review before. For evaluating our approach the research proposal (RP1): ' How should successful persuasiveness in Natural Language Generation be theorized? ' has to be answered first in order to use this instrument as a remainder of further identified research proposals. Consequently, this framework can be used to investigate different successful approaches to generate persuasive messages through a persuasive NLG artificial intelligence."", ""Future research should investigate the empirical implementation of benevolence for the persuadee . In such regard, a given example in the circumstance of hostage negotiation (Gilbert 2010) may be transferable to business situations (cf. Table 3, Outcome: I want you to put the gun down because I don't want to see you get hurt ). This example shows that the persuadee can expect benevolence as an outcome, if he acts in a certain way. Combining the identified determinants, a thorough linguistic analysis of appropriate language for the persuadee can be derived. As an example, meaningfulness is a crucial linguistic concept in persuasion (Atalay et al. 2019, Graesser et al. 2011; cf. Table 2), but lexical overlap even more directly influences persuasiveness, since it provides a consistency towards the persuadee's language expectancy (Heider 1958). A consistent argumentative logic implementable with determinants such as connectives, hedges, or logical operators, allows coherently concatenating a variety of arguments as well as the creation of an argumentative narrative. Logic can provide a blueprint for writing, or an approach to effectively organize a persuasive act (Habernal & Gurevych 2016). A high trust-level of a persuader would mean that s/he is likely to be chosen as an interaction partner (Axelrod 1984)."", ""To the research community we propose a framework with persuasive determinants that are particularly pronounced in a persuasive act, which is also contingent on environmental aspects. Investigating these mechanisms (Table 3 -6) would potentially provide insights regarding our research proposal (RP2): 'How should the determinants within the categories 'language appropriacy, trustworthiness, benevolence, and logical argumentation' be implemented and integrated to contribute to increased persuasiveness through persuasive Natural Language Generation?'"", ""A variety of different tools and datasets prepare the input for deep learning models that underpin artificial intelligence and their training for text generation. Such models are inherently complex, so it is crucial to experiment, carefully prepare different datasets, and use the identified tools strategically to make the persuadee act upon the persuasive act (Anand et al. 2011). In this light, we propose RP 3: 'Which tools & which datasets are most contributive for deep learning training to increase persuasiveness in persuasive Natural Language Generation?'"", 'Ultimately, the persuader will need to complement the persuasive measures that a persuasive NLG AI can suggest -due to possible deficiencies of crucial information that computational systems may inherently lack (e.g., aspects not explicitly outlined in textual data). Therefore, a persuasive NLG may be limited to serve as an assistant proposing suitable techniques or recommending alterations to linguistic measures in specific situations. Still, the persuader will be the one to edit and submit any artificially generated persuasive message to a persuadee, and is therefore fully responsible . Yet, such artificial intelligence can be used to help people to persuade them to do good things (like losing weight; Hunter et al. 2019).']","This section displays the last step of the framework for literature reviewing (Vom Brocke et al. 2009): (5) Developing a research agenda.

For our proposed agenda for future research in the field of persuasive NLG (Figure 2), we conclude that an unambiguous and concise comprehension of how the forty-nine identified determinants influence the generation of persuasive artificially generated messages (i.e., the persuasive act) is needed. Furthermore, twenty-one tools & datasets were identified that allow one to train generative deep neural nets within the scope of persuasive NLG artificial intelligence. Next, we conclude our research proposals. Our framework encompasses four identified categories (based on the 'principles of persuasion' and embedded into academic theories, i.e. Cognitive Dissonance, Language Expectancy, Probabilistic Models, and Balance Theory) as prerequisites for persuasive NLG that have not been comprehensively considered in the journals considered for this review before. For evaluating our approach the research proposal (RP1): ' How should successful persuasiveness in Natural Language Generation be theorized? ' has to be answered first in order to use this instrument as a remainder of further identified research proposals. Consequently, this framework can be used to investigate different successful approaches to generate persuasive messages through a persuasive NLG artificial intelligence.

Future research should investigate the empirical implementation of benevolence for the persuadee . In such regard, a given example in the circumstance of hostage negotiation (Gilbert 2010) may be transferable to business situations (cf. Table 3, Outcome: I want you to put the gun down because I don't want to see you get hurt ). This example shows that the persuadee can expect benevolence as an outcome, if he acts in a certain way. Combining the identified determinants, a thorough linguistic analysis of appropriate language for the persuadee can be derived. As an example, meaningfulness is a crucial linguistic concept in persuasion (Atalay et al. 2019, Graesser et al. 2011; cf. Table 2), but lexical overlap even more directly influences persuasiveness, since it provides a consistency towards the persuadee's language expectancy (Heider 1958). A consistent argumentative logic implementable with determinants such as connectives, hedges, or logical operators, allows coherently concatenating a variety of arguments as well as the creation of an argumentative narrative. Logic can provide a blueprint for writing, or an approach to effectively organize a persuasive act (Habernal & Gurevych 2016). A high trust-level of a persuader would mean that s/he is likely to be chosen as an interaction partner (Axelrod 1984).

To the research community we propose a framework with persuasive determinants that are particularly pronounced in a persuasive act, which is also contingent on environmental aspects. Investigating these mechanisms (Table 3 -6) would potentially provide insights regarding our research proposal (RP2): 'How should the determinants within the categories 'language appropriacy, trustworthiness, benevolence, and logical argumentation' be implemented and integrated to contribute to increased persuasiveness through persuasive Natural Language Generation?'

A variety of different tools and datasets prepare the input for deep learning models that underpin artificial intelligence and their training for text generation. Such models are inherently complex, so it is crucial to experiment, carefully prepare different datasets, and use the identified tools strategically to make the persuadee act upon the persuasive act (Anand et al. 2011). In this light, we propose RP 3: 'Which tools & which datasets are most contributive for deep learning training to increase persuasiveness in persuasive Natural Language Generation?'

Ultimately, the persuader will need to complement the persuasive measures that a persuasive NLG AI can suggest -due to possible deficiencies of crucial information that computational systems may inherently lack (e.g., aspects not explicitly outlined in textual data). Therefore, a persuasive NLG may be limited to serve as an assistant proposing suitable techniques or recommending alterations to linguistic measures in specific situations. Still, the persuader will be the one to edit and submit any artificially generated persuasive message to a persuadee, and is therefore fully responsible . Yet, such artificial intelligence can be used to help people to persuade them to do good things (like losing weight; Hunter et al. 2019).","(p11.0) This section displays the last step of the framework for literature reviewing (Vom Brocke et al. 2009): (5) Developing a research agenda.

(p11.1) For our proposed agenda for future research in the field of persuasive NLG (Figure 2), we conclude that an unambiguous and concise comprehension of how the forty-nine identified determinants influence the generation of persuasive artificially generated messages (i.e., the persuasive act) is needed. Furthermore, twenty-one tools & datasets were identified that allow one to train generative deep neural nets within the scope of persuasive NLG artificial intelligence. Next, we conclude our research proposals. Our framework encompasses four identified categories (based on the 'principles of persuasion' and embedded into academic theories, i.e. Cognitive Dissonance, Language Expectancy, Probabilistic Models, and Balance Theory) as prerequisites for persuasive NLG that have not been comprehensively considered in the journals considered for this review before. For evaluating our approach the research proposal (RP1): ' How should successful persuasiveness in Natural Language Generation be theorized? ' has to be answered first in order to use this instrument as a remainder of further identified research proposals. Consequently, this framework can be used to investigate different successful approaches to generate persuasive messages through a persuasive NLG artificial intelligence.

(p11.2) Future research should investigate the empirical implementation of benevolence for the persuadee . In such regard, a given example in the circumstance of hostage negotiation (Gilbert 2010) may be transferable to business situations (cf. Table 3, Outcome: I want you to put the gun down because I don't want to see you get hurt ). This example shows that the persuadee can expect benevolence as an outcome, if he acts in a certain way. Combining the identified determinants, a thorough linguistic analysis of appropriate language for the persuadee can be derived. As an example, meaningfulness is a crucial linguistic concept in persuasion (Atalay et al. 2019, Graesser et al. 2011; cf. Table 2), but lexical overlap even more directly influences persuasiveness, since it provides a consistency towards the persuadee's language expectancy (Heider 1958). A consistent argumentative logic implementable with determinants such as connectives, hedges, or logical operators, allows coherently concatenating a variety of arguments as well as the creation of an argumentative narrative. Logic can provide a blueprint for writing, or an approach to effectively organize a persuasive act (Habernal & Gurevych 2016). A high trust-level of a persuader would mean that s/he is likely to be chosen as an interaction partner (Axelrod 1984).

(p11.3) To the research community we propose a framework with persuasive determinants that are particularly pronounced in a persuasive act, which is also contingent on environmental aspects. Investigating these mechanisms (Table 3 -6) would potentially provide insights regarding our research proposal (RP2): 'How should the determinants within the categories 'language appropriacy, trustworthiness, benevolence, and logical argumentation' be implemented and integrated to contribute to increased persuasiveness through persuasive Natural Language Generation?'

(p11.4) A variety of different tools and datasets prepare the input for deep learning models that underpin artificial intelligence and their training for text generation. Such models are inherently complex, so it is crucial to experiment, carefully prepare different datasets, and use the identified tools strategically to make the persuadee act upon the persuasive act (Anand et al. 2011). In this light, we propose RP 3: 'Which tools & which datasets are most contributive for deep learning training to increase persuasiveness in persuasive Natural Language Generation?'

(p11.5) Ultimately, the persuader will need to complement the persuasive measures that a persuasive NLG AI can suggest -due to possible deficiencies of crucial information that computational systems may inherently lack (e.g., aspects not explicitly outlined in textual data). Therefore, a persuasive NLG may be limited to serve as an assistant proposing suitable techniques or recommending alterations to linguistic measures in specific situations. Still, the persuader will be the one to edit and submit any artificially generated persuasive message to a persuadee, and is therefore fully responsible . Yet, such artificial intelligence can be used to help people to persuade them to do good things (like losing weight; Hunter et al. 2019).","[[None], [], ['b28', 'b35', None, 'b3', 'b29', 'b33'], [], ['b63'], []]","[[None], [], ['b28', 'b35', None, 'b3', 'b29', 'b33'], [], ['b63'], []]",8,"1. This section displays the last step of the framework for literature reviewing (Vom Brocke et al. 2009): (5) Developing a research agenda.
2. For our proposed agenda for future research in the field of persuasive NLG (Figure 2), we conclude that an unambiguous and concise comprehension of how the forty-nine identified determinants influence the generation of persuasive artificially generated messages (i.e., the persuasive act) is needed.
3. Furthermore, twenty-one tools & datasets were identified that allow one to train generative deep neural nets within the scope of persuasive NLG artificial intelligence.
4. Next, we conclude our research proposals.
5. Our framework encompasses four identified categories (based on the 'principles of persuasion' and embedded into academic theories, i.e. Cognitive Dissonance, Language Expectancy, Probabilistic Models, and Balance Theory) as prerequisites for persuasive NLG that have not been comprehensively considered in the journals considered for this review before.
6. For evaluating our approach the research proposal (RP1): ' How should successful persuasiveness in Natural Language Generation be theorized? '
7. has to be answered first in order to use this instrument as a remainder of further identified research proposals.
8. Consequently, this framework can be used to investigate different successful approaches to generate persuasive messages through a persuasive NLG artificial intelligence.
9. Future research should investigate the empirical implementation of benevolence for the persuadee .
10. In such regard, a given example in the circumstance of hostage negotiation (Gilbert 2010) may be transferable to business situations (cf. Table 3, Outcome: I want you to put the gun down because I don't want to see you get hurt ).
11. This example shows that the persuadee can expect benevolence as an outcome, if he acts in a certain way.
12. Combining the identified determinants, a thorough linguistic analysis of appropriate language for the persuadee can be derived.
13. As an example, meaningfulness is a crucial linguistic concept in persuasion (Atalay et al. 2019, Graesser et al. 2011; cf. Table 2), but lexical overlap even more directly influences persuasiveness, since it provides a consistency towards the persuadee's language expectancy (Heider 1958).
14. A consistent argumentative logic implementable with determinants such as connectives, hedges, or logical operators, allows coherently concatenating a variety of arguments as well as the creation of an argumentative narrative.
15. Logic can provide a blueprint for writing, or an approach to effectively organize a persuasive act (5)0.
16. A high trust-level of a persuader would mean that s/he is likely to be chosen as an interaction partner (5)1.
17. To the research community we propose a framework with persuasive determinants that are particularly pronounced in a persuasive act, which is also contingent on environmental aspects.
18. Investigating these mechanisms (5)2 would potentially provide insights regarding our research proposal (5)3: 'How should the determinants within the categories 'language appropriacy, trustworthiness, benevolence, and logical argumentation' be implemented and integrated to contribute to increased persuasiveness through persuasive Natural Language Generation?'
19. A variety of different tools and datasets prepare the input for deep learning models that underpin artificial intelligence and their training for text generation.
20. Such models are inherently complex, so it is crucial to experiment, carefully prepare different datasets, and use the identified tools strategically to make the persuadee act upon the persuasive act (5)4.
21. In this light, we propose RP 3: 'Which tools & which datasets are most contributive for deep learning training to increase persuasiveness in persuasive Natural Language Generation?'Ultimately, the persuader will need to complement the persuasive measures that a persuasive NLG AI can suggest -due to possible deficiencies of crucial information that computational systems may inherently lack (5)5.
22. Therefore, a persuasive NLG may be limited to serve as an assistant proposing suitable techniques or recommending alterations to linguistic measures in specific situations.
23. Still, the persuader will be the one to edit and submit any artificially generated persuasive message to a persuadee, and is therefore fully responsible .
24. Yet, such artificial intelligence can be used to help people to persuade them to do good things (5)6.","What are the identified determinants that influence the generation of persuasive messages in Natural Language Generation (NLG)?
How can the principles of persuasion, such as Cognitive Dissonance and Language Expectancy, be integrated into persuasive NLG frameworks?
What role do tools and datasets play in training deep learning models for persuasive NLG, and which are considered most effective?
In what ways can persuasive NLG AI act as an assistant to persuaders, and what are its limitations?
How can linguistic concepts like meaningfulness and lexical overlap contribute to the effectiveness of persuasive messages in NLG?","1. What are the identified determinants that influence the generation of persuasive messages in Natural Language Generation (NLG)?
2. How can the principles of persuasion, such as Cognitive Dissonance and Language Expectancy, be integrated into persuasive NLG frameworks?
3. What role do tools and datasets play in training deep learning models for persuasive NLG, and which are considered most effective?
4. In what ways can persuasive NLG AI act as an assistant to persuaders, and what are its limitations?
5. How can linguistic concepts like meaningfulness and lexical overlap contribute to the effectiveness of persuasive messages in NLG?",None,,"Questions:

1. What are the identified determinants that influence the generation of persuasive messages in Natural Language Generation (NLG)?
2. How can the principles of persuasion, such as Cognitive Dissonance and Language Expectancy, be integrated into persuasive NLG frameworks?
3. What role do tools and datasets play in training deep learning models for persuasive NLG, and which are considered most effective?
4. In what ways can persuasive NLG AI act as an assistant to persuaders, and what are its limitations?
5. How can linguistic concepts like meaningfulness and lexical overlap contribute to the effectiveness of persuasive messages in NLG?

Answer:

(p11.0) This section displays the last step of the framework for literature reviewing (Vom Brocke et al. 2009): (5) Developing a research agenda.

(p11.1) For our proposed agenda for future research in the field of persuasive NLG (Figure 2), we conclude that an unambiguous and concise comprehension of how the forty-nine identified determinants influence the generation of persuasive artificially generated messages (i.e., the persuasive act) is needed. Furthermore, twenty-one tools & datasets were identified that allow one to train generative deep neural nets within the scope of persuasive NLG artificial intelligence. Next, we conclude our research proposals. Our framework encompasses four identified categories (based on the 'principles of persuasion' and embedded into academic theories, i.e. Cognitive Dissonance, Language Expectancy, Probabilistic Models, and Balance Theory) as prerequisites for persuasive NLG that have not been comprehensively considered in the journals considered for this review before. For evaluating our approach the research proposal (RP1): ' How should successful persuasiveness in Natural Language Generation be theorized? ' has to be answered first in order to use this instrument as a remainder of further identified research proposals. Consequently, this framework can be used to investigate different successful approaches to generate persuasive messages through a persuasive NLG artificial intelligence.

(p11.2) Future research should investigate the empirical implementation of benevolence for the persuadee . In such regard, a given example in the circumstance of hostage negotiation (Gilbert 2010) may be transferable to business situations (cf. Table 3, Outcome: I want you to put the gun down because I don't want to see you get hurt ). This example shows that the persuadee can expect benevolence as an outcome, if he acts in a certain way. Combining the identified determinants, a thorough linguistic analysis of appropriate language for the persuadee can be derived. As an example, meaningfulness is a crucial linguistic concept in persuasion (Atalay et al. 2019, Graesser et al. 2011; cf. Table 2), but lexical overlap even more directly influences persuasiveness, since it provides a consistency towards the persuadee's language expectancy (Heider 1958). A consistent argumentative logic implementable with determinants such as connectives, hedges, or logical operators, allows coherently concatenating a variety of arguments as well as the creation of an argumentative narrative. Logic can provide a blueprint for writing, or an approach to effectively organize a persuasive act (Habernal & Gurevych 2016). A high trust-level of a persuader would mean that s/he is likely to be chosen as an interaction partner (Axelrod 1984).

(p11.3) To the research community we propose a framework with persuasive determinants that are particularly pronounced in a persuasive act, which is also contingent on environmental aspects. Investigating these mechanisms (Table 3 -6) would potentially provide insights regarding our research proposal (RP2): 'How should the determinants within the categories 'language appropriacy, trustworthiness, benevolence, and logical argumentation' be implemented and integrated to contribute to increased persuasiveness through persuasive Natural Language Generation?'

(p11.4) A variety of different tools and datasets prepare the input for deep learning models that underpin artificial intelligence and their training for text generation. Such models are inherently complex, so it is crucial to experiment, carefully prepare different datasets, and use the identified tools strategically to make the persuadee act upon the persuasive act (Anand et al. 2011). In this light, we propose RP 3: 'Which tools & which datasets are most contributive for deep learning training to increase persuasiveness in persuasive Natural Language Generation?'

(p11.5) Ultimately, the persuader will need to complement the persuasive measures that a persuasive NLG AI can suggest -due to possible deficiencies of crucial information that computational systems may inherently lack (e.g., aspects not explicitly outlined in textual data). Therefore, a persuasive NLG may be limited to serve as an assistant proposing suitable techniques or recommending alterations to linguistic measures in specific situations. Still, the persuader will be the one to edit and submit any artificially generated persuasive message to a persuadee, and is therefore fully responsible . Yet, such artificial intelligence can be used to help people to persuade them to do good things (like losing weight; Hunter et al. 2019).","Questions:



Answer:

(p11.0) This section displays the last step of the framework for literature reviewing (Vom Brocke et al. 2009): (5) Developing a research agenda.

(p11.1) For our proposed agenda for future research in the field of persuasive NLG (Figure 2), we conclude that an unambiguous and concise comprehension of how the forty-nine identified determinants influence the generation of persuasive artificially generated messages (i.e., the persuasive act) is needed. Furthermore, twenty-one tools & datasets were identified that allow one to train generative deep neural nets within the scope of persuasive NLG artificial intelligence. Next, we conclude our research proposals. Our framework encompasses four identified categories (based on the 'principles of persuasion' and embedded into academic theories, i.e. Cognitive Dissonance, Language Expectancy, Probabilistic Models, and Balance Theory) as prerequisites for persuasive NLG that have not been comprehensively considered in the journals considered for this review before. For evaluating our approach the research proposal (RP1): ' How should successful persuasiveness in Natural Language Generation be theorized? ' has to be answered first in order to use this instrument as a remainder of further identified research proposals. Consequently, this framework can be used to investigate different successful approaches to generate persuasive messages through a persuasive NLG artificial intelligence.

(p11.2) Future research should investigate the empirical implementation of benevolence for the persuadee . In such regard, a given example in the circumstance of hostage negotiation (Gilbert 2010) may be transferable to business situations (cf. Table 3, Outcome: I want you to put the gun down because I don't want to see you get hurt ). This example shows that the persuadee can expect benevolence as an outcome, if he acts in a certain way. Combining the identified determinants, a thorough linguistic analysis of appropriate language for the persuadee can be derived. As an example, meaningfulness is a crucial linguistic concept in persuasion (Atalay et al. 2019, Graesser et al. 2011; cf. Table 2), but lexical overlap even more directly influences persuasiveness, since it provides a consistency towards the persuadee's language expectancy (Heider 1958). A consistent argumentative logic implementable with determinants such as connectives, hedges, or logical operators, allows coherently concatenating a variety of arguments as well as the creation of an argumentative narrative. Logic can provide a blueprint for writing, or an approach to effectively organize a persuasive act (Habernal & Gurevych 2016). A high trust-level of a persuader would mean that s/he is likely to be chosen as an interaction partner (Axelrod 1984).

(p11.3) To the research community we propose a framework with persuasive determinants that are particularly pronounced in a persuasive act, which is also contingent on environmental aspects. Investigating these mechanisms (Table 3 -6) would potentially provide insights regarding our research proposal (RP2): 'How should the determinants within the categories 'language appropriacy, trustworthiness, benevolence, and logical argumentation' be implemented and integrated to contribute to increased persuasiveness through persuasive Natural Language Generation?'

(p11.4) A variety of different tools and datasets prepare the input for deep learning models that underpin artificial intelligence and their training for text generation. Such models are inherently complex, so it is crucial to experiment, carefully prepare different datasets, and use the identified tools strategically to make the persuadee act upon the persuasive act (Anand et al. 2011). In this light, we propose RP 3: 'Which tools & which datasets are most contributive for deep learning training to increase persuasiveness in persuasive Natural Language Generation?'

(p11.5) Ultimately, the persuader will need to complement the persuasive measures that a persuasive NLG AI can suggest -due to possible deficiencies of crucial information that computational systems may inherently lack (e.g., aspects not explicitly outlined in textual data). Therefore, a persuasive NLG may be limited to serve as an assistant proposing suitable techniques or recommending alterations to linguistic measures in specific situations. Still, the persuader will be the one to edit and submit any artificially generated persuasive message to a persuadee, and is therefore fully responsible . Yet, such artificial intelligence can be used to help people to persuade them to do good things (like losing weight; Hunter et al. 2019)."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s1,Related Surveys,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5', 'p1.6', 'p1.7']","['We relate this paper to existing surveys of the two specific dimensions MML and Transformers. There exist a few MML surveys [1], [11], [12]. In particular, [1] proposed a structured, acknowledged taxonomy by five challenges, which we also adopt as part of our structure. Unlike [1], [11], and [12], which review general machine learning models, we instead focus on Transformer architectures and their self-attention mechanisms. Several surveys dedicated to Transformers have been recently introduced, with a range of emphases including general Transformers [48], efficient designs [49], visualization [50], computer vision tasks [51], [52], [53], [54], medical imaging [55], video tasks [56], and vision language pretraining [57]. While [51], [53], [54], [55] consider MML, their reviews are somewhat limited in the scope, taxonomy, and coverage. To our knowledge, only a few surveys on video-language pretraining (VLP) [57], [58], [59] are relevant to MML. However, VLP is only a subdomain of MML. In this survey, we focus solely on the intersection of multimodal learning and Transformers.', 'Features To our knowledge, this paper is the first comprehensive review of the state of Transformer based multimodal machine learning. The major features of this survey include (1) We highlight that Transformers have the advantage that they can work in a modality-agnostic way. Thus, they are compatible with various modalities (and combinations of modalities). To support this view, we, for the first time, offer an understanding of the intrinsic traits of Transformers in a multimodal context from a geometrically topological perspective. We suggest that self-attention be treated as a graph style modelling, which models the input sequence (both uni-modal and multimodal) as a fully-connected graph. Specifically, self-attention models the embedding of arbitrary tokens from an arbitrary modality as a graph node.', '(2) We discuss the key components of Transformers in a multimodal context as mathematically as possible.', '(3) Based on Transformers, cross-modal interactions (e.g., fusion, alignment) are essentially processed by self-attention and its variants. In this paper, we extract the mathematical essence and formulations of Transformer based MML practices, from the perspective of self-attention designs.', 'Contributions Having presented our review of the landscape of multimodal learning, Transformer ecosystem, and multimodal big data era in Section 2, we summarize our main contributions as the follows.', '(1) In Section 3, we present a systematic reviewing of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective.', '(2) We contribute a taxonomy for Transformer based MML from two complementary perspectives, i.e., application based and challenge based. In Section 4, we provide a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks. In Section 5, we summarize the common challenges and designs shared by the various multimodal Transformer models and applications.', '(3) In Section 6, we discuss current bottlenecks, existing problems, and potential research directions for Transformer based MML.']","We relate this paper to existing surveys of the two specific dimensions MML and Transformers. There exist a few MML surveys [1], [11], [12]. In particular, [1] proposed a structured, acknowledged taxonomy by five challenges, which we also adopt as part of our structure. Unlike [1], [11], and [12], which review general machine learning models, we instead focus on Transformer architectures and their self-attention mechanisms. Several surveys dedicated to Transformers have been recently introduced, with a range of emphases including general Transformers [48], efficient designs [49], visualization [50], computer vision tasks [51], [52], [53], [54], medical imaging [55], video tasks [56], and vision language pretraining [57]. While [51], [53], [54], [55] consider MML, their reviews are somewhat limited in the scope, taxonomy, and coverage. To our knowledge, only a few surveys on video-language pretraining (VLP) [57], [58], [59] are relevant to MML. However, VLP is only a subdomain of MML. In this survey, we focus solely on the intersection of multimodal learning and Transformers.

Features To our knowledge, this paper is the first comprehensive review of the state of Transformer based multimodal machine learning. The major features of this survey include (1) We highlight that Transformers have the advantage that they can work in a modality-agnostic way. Thus, they are compatible with various modalities (and combinations of modalities). To support this view, we, for the first time, offer an understanding of the intrinsic traits of Transformers in a multimodal context from a geometrically topological perspective. We suggest that self-attention be treated as a graph style modelling, which models the input sequence (both uni-modal and multimodal) as a fully-connected graph. Specifically, self-attention models the embedding of arbitrary tokens from an arbitrary modality as a graph node.

(2) We discuss the key components of Transformers in a multimodal context as mathematically as possible.

(3) Based on Transformers, cross-modal interactions (e.g., fusion, alignment) are essentially processed by self-attention and its variants. In this paper, we extract the mathematical essence and formulations of Transformer based MML practices, from the perspective of self-attention designs.

Contributions Having presented our review of the landscape of multimodal learning, Transformer ecosystem, and multimodal big data era in Section 2, we summarize our main contributions as the follows.

(1) In Section 3, we present a systematic reviewing of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective.

(2) We contribute a taxonomy for Transformer based MML from two complementary perspectives, i.e., application based and challenge based. In Section 4, we provide a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks. In Section 5, we summarize the common challenges and designs shared by the various multimodal Transformer models and applications.

(3) In Section 6, we discuss current bottlenecks, existing problems, and potential research directions for Transformer based MML.","(p1.0) We relate this paper to existing surveys of the two specific dimensions MML and Transformers. There exist a few MML surveys [1], [11], [12]. In particular, [1] proposed a structured, acknowledged taxonomy by five challenges, which we also adopt as part of our structure. Unlike [1], [11], and [12], which review general machine learning models, we instead focus on Transformer architectures and their self-attention mechanisms. Several surveys dedicated to Transformers have been recently introduced, with a range of emphases including general Transformers [48], efficient designs [49], visualization [50], computer vision tasks [51], [52], [53], [54], medical imaging [55], video tasks [56], and vision language pretraining [57]. While [51], [53], [54], [55] consider MML, their reviews are somewhat limited in the scope, taxonomy, and coverage. To our knowledge, only a few surveys on video-language pretraining (VLP) [57], [58], [59] are relevant to MML. However, VLP is only a subdomain of MML. In this survey, we focus solely on the intersection of multimodal learning and Transformers.

(p1.1) Features To our knowledge, this paper is the first comprehensive review of the state of Transformer based multimodal machine learning. The major features of this survey include (1) We highlight that Transformers have the advantage that they can work in a modality-agnostic way. Thus, they are compatible with various modalities (and combinations of modalities). To support this view, we, for the first time, offer an understanding of the intrinsic traits of Transformers in a multimodal context from a geometrically topological perspective. We suggest that self-attention be treated as a graph style modelling, which models the input sequence (both uni-modal and multimodal) as a fully-connected graph. Specifically, self-attention models the embedding of arbitrary tokens from an arbitrary modality as a graph node.

(p1.2) (2) We discuss the key components of Transformers in a multimodal context as mathematically as possible.

(p1.3) (3) Based on Transformers, cross-modal interactions (e.g., fusion, alignment) are essentially processed by self-attention and its variants. In this paper, we extract the mathematical essence and formulations of Transformer based MML practices, from the perspective of self-attention designs.

(p1.4) Contributions Having presented our review of the landscape of multimodal learning, Transformer ecosystem, and multimodal big data era in Section 2, we summarize our main contributions as the follows.

(p1.5) (1) In Section 3, we present a systematic reviewing of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective.

(p1.6) (2) We contribute a taxonomy for Transformer based MML from two complementary perspectives, i.e., application based and challenge based. In Section 4, we provide a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks. In Section 5, we summarize the common challenges and designs shared by the various multimodal Transformer models and applications.

(p1.7) (3) In Section 6, we discuss current bottlenecks, existing problems, and potential research directions for Transformer based MML.","[['b54', 'b47', 'b11', 'b48', 'b57', 'b55', 'b0', 'b56', 'b53', 'b51', 'b58', 'b50', 'b52', 'b10', 'b49'], [], [], [], [], [], [], []]","[['b54', 'b47', 'b11', 'b48', 'b57', 'b55', 'b0', 'b56', 'b53', 'b51', 'b58', 'b50', 'b52', 'b10', 'b49'], [], [], [], [], [], [], []]",15,"1. We relate this paper to existing surveys of the two specific dimensions MML and Transformers.
2. There exist a few MML surveys [1], [11], [12].
3. In particular, [1] proposed a structured, acknowledged taxonomy by five challenges, which we also adopt as part of our structure.
4. Unlike [1], [11], and [12], which review general machine learning models, we instead focus on Transformer architectures and their self-attention mechanisms.
5. Several surveys dedicated to Transformers have been recently introduced, with a range of emphases including general Transformers [48], efficient designs [49], visualization [50], computer vision tasks [51], [52], [53], [54], medical imaging [55], video tasks [56], and vision language pretraining [57].
6. While [51], [53], [54], [55] consider MML, their reviews are somewhat limited in the scope, taxonomy, and coverage.
7. To our knowledge, only a few surveys on video-language pretraining (VLP) [57], [58], [59] are relevant to MML.
8. However, VLP is only a subdomain of MML.
9. In this survey, we focus solely on the intersection of multimodal learning and Transformers.
10. Features To our knowledge, this paper is the first comprehensive review of the state of Transformer based multimodal machine learning.
11. The major features of this survey include (1) We highlight that Transformers have the advantage that they can work in a modality-agnostic way.
12. Thus, they are compatible with various modalities (and combinations of modalities).
13. To support this view, we, for the first time, offer an understanding of the intrinsic traits of Transformers in a multimodal context from a geometrically topological perspective.
14. We suggest that self-attention be treated as a graph style modelling, which models the input sequence (both uni-modal and multimodal) as a fully-connected graph.
15. Specifically, self-attention models the embedding of arbitrary tokens from an arbitrary modality as a graph node.
16. (2) We discuss the key components of Transformers in a multimodal context as mathematically as possible.
17. (3) Based on Transformers, cross-modal interactions (e.g., fusion, alignment) are essentially processed by self-attention and its variants.
18. In this paper, we extract the mathematical essence and formulations of Transformer based MML practices, from the perspective of self-attention designs.
19. Contributions Having presented our review of the landscape of multimodal learning, Transformer ecosystem, and multimodal big data era in Section 2, we summarize our main contributions as the follows.
20. (1) In Section 3, we present a systematic reviewing of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective.
21. (2) We contribute a taxonomy for Transformer based MML from two complementary perspectives, i.e., application based and challenge based.
22. In Section 4, we provide a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks.
23. In Section 5, we summarize the common challenges and designs shared by the various multimodal Transformer models and applications.
24. (3) In Section 6, we discuss current bottlenecks, existing problems, and potential research directions for Transformer based MML.","How does the survey differentiate its focus from existing MML surveys and general machine learning model reviews?
What unique perspective does this paper introduce regarding the intrinsic traits of Transformers in a multimodal context?
How are cross-modal interactions processed in Transformer-based multimodal learning according to the survey?
What systematic review does the paper present regarding Vanilla Transformer, Vision Transformer, and multimodal Transformers?
What are the identified current bottlenecks and potential research directions for Transformer-based multimodal learning as discussed in the survey?","1. How does the survey differentiate its focus from existing MML surveys and general machine learning model reviews?
2. What unique perspective does this paper introduce regarding the intrinsic traits of Transformers in a multimodal context?
3. How are cross-modal interactions processed in Transformer-based multimodal learning according to the survey?
4. What systematic review does the paper present regarding Vanilla Transformer, Vision Transformer, and multimodal Transformers?
5. What are the identified current bottlenecks and potential research directions for Transformer-based multimodal learning as discussed in the survey?",None,,"Questions:

1. How does the survey differentiate its focus from existing MML surveys and general machine learning model reviews?
2. What unique perspective does this paper introduce regarding the intrinsic traits of Transformers in a multimodal context?
3. How are cross-modal interactions processed in Transformer-based multimodal learning according to the survey?
4. What systematic review does the paper present regarding Vanilla Transformer, Vision Transformer, and multimodal Transformers?
5. What are the identified current bottlenecks and potential research directions for Transformer-based multimodal learning as discussed in the survey?

Answer:

(p1.0) We relate this paper to existing surveys of the two specific dimensions MML and Transformers. There exist a few MML surveys [1], [11], [12]. In particular, [1] proposed a structured, acknowledged taxonomy by five challenges, which we also adopt as part of our structure. Unlike [1], [11], and [12], which review general machine learning models, we instead focus on Transformer architectures and their self-attention mechanisms. Several surveys dedicated to Transformers have been recently introduced, with a range of emphases including general Transformers [48], efficient designs [49], visualization [50], computer vision tasks [51], [52], [53], [54], medical imaging [55], video tasks [56], and vision language pretraining [57]. While [51], [53], [54], [55] consider MML, their reviews are somewhat limited in the scope, taxonomy, and coverage. To our knowledge, only a few surveys on video-language pretraining (VLP) [57], [58], [59] are relevant to MML. However, VLP is only a subdomain of MML. In this survey, we focus solely on the intersection of multimodal learning and Transformers.

(p1.1) Features To our knowledge, this paper is the first comprehensive review of the state of Transformer based multimodal machine learning. The major features of this survey include (1) We highlight that Transformers have the advantage that they can work in a modality-agnostic way. Thus, they are compatible with various modalities (and combinations of modalities). To support this view, we, for the first time, offer an understanding of the intrinsic traits of Transformers in a multimodal context from a geometrically topological perspective. We suggest that self-attention be treated as a graph style modelling, which models the input sequence (both uni-modal and multimodal) as a fully-connected graph. Specifically, self-attention models the embedding of arbitrary tokens from an arbitrary modality as a graph node.

(p1.2) (2) We discuss the key components of Transformers in a multimodal context as mathematically as possible.

(p1.3) (3) Based on Transformers, cross-modal interactions (e.g., fusion, alignment) are essentially processed by self-attention and its variants. In this paper, we extract the mathematical essence and formulations of Transformer based MML practices, from the perspective of self-attention designs.

(p1.4) Contributions Having presented our review of the landscape of multimodal learning, Transformer ecosystem, and multimodal big data era in Section 2, we summarize our main contributions as the follows.

(p1.5) (1) In Section 3, we present a systematic reviewing of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective.

(p1.6) (2) We contribute a taxonomy for Transformer based MML from two complementary perspectives, i.e., application based and challenge based. In Section 4, we provide a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks. In Section 5, we summarize the common challenges and designs shared by the various multimodal Transformer models and applications.

(p1.7) (3) In Section 6, we discuss current bottlenecks, existing problems, and potential research directions for Transformer based MML.","Questions:



Answer:

(p1.0) We relate this paper to existing surveys of the two specific dimensions MML and Transformers. There exist a few MML surveys [1], [11], [12]. In particular, [1] proposed a structured, acknowledged taxonomy by five challenges, which we also adopt as part of our structure. Unlike [1], [11], and [12], which review general machine learning models, we instead focus on Transformer architectures and their self-attention mechanisms. Several surveys dedicated to Transformers have been recently introduced, with a range of emphases including general Transformers [48], efficient designs [49], visualization [50], computer vision tasks [51], [52], [53], [54], medical imaging [55], video tasks [56], and vision language pretraining [57]. While [51], [53], [54], [55] consider MML, their reviews are somewhat limited in the scope, taxonomy, and coverage. To our knowledge, only a few surveys on video-language pretraining (VLP) [57], [58], [59] are relevant to MML. However, VLP is only a subdomain of MML. In this survey, we focus solely on the intersection of multimodal learning and Transformers.

(p1.1) Features To our knowledge, this paper is the first comprehensive review of the state of Transformer based multimodal machine learning. The major features of this survey include (1) We highlight that Transformers have the advantage that they can work in a modality-agnostic way. Thus, they are compatible with various modalities (and combinations of modalities). To support this view, we, for the first time, offer an understanding of the intrinsic traits of Transformers in a multimodal context from a geometrically topological perspective. We suggest that self-attention be treated as a graph style modelling, which models the input sequence (both uni-modal and multimodal) as a fully-connected graph. Specifically, self-attention models the embedding of arbitrary tokens from an arbitrary modality as a graph node.

(p1.2) (2) We discuss the key components of Transformers in a multimodal context as mathematically as possible.

(p1.3) (3) Based on Transformers, cross-modal interactions (e.g., fusion, alignment) are essentially processed by self-attention and its variants. In this paper, we extract the mathematical essence and formulations of Transformer based MML practices, from the perspective of self-attention designs.

(p1.4) Contributions Having presented our review of the landscape of multimodal learning, Transformer ecosystem, and multimodal big data era in Section 2, we summarize our main contributions as the follows.

(p1.5) (1) In Section 3, we present a systematic reviewing of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective.

(p1.6) (2) We contribute a taxonomy for Transformer based MML from two complementary perspectives, i.e., application based and challenge based. In Section 4, we provide a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks. In Section 5, we summarize the common challenges and designs shared by the various multimodal Transformer models and applications.

(p1.7) (3) In Section 6, we discuss current bottlenecks, existing problems, and potential research directions for Transformer based MML."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s3,Multimodal Learning (MML),"['p3.0', 'p3.1', 'p3.2']","['MML [1], [60], [61] has been an important research area in recent decades; an early multimodal application -audiovisual speech recognition was studied in 1980s [62]. MML is key to human societies. The world we humans live in is a multimodal environment, thus both our observations and behaviours are multimodal [63]. For instance, an AI navigation robot needs multimodal sensors to perceive the real-world environment [64], [65], [66], e.g., camera, LiDAR, radar, ultrasonic, GNSS, HD Map, odometer. Furthermore, human behaviours, emotions, events, actions, and humour are multimodal, thus various human-centred MML tasks are widely studied, including multimodal emotion recognition [67], multimodal event representation [68], understanding multimodal humor [69], face-body-voice based video person-clustering [70], etc.', 'Thanks to the development of the internet and a wide variety of intelligent devices in recent years, increasing amounts of multimodal data are being transmitted over the internet, thus an increasing number of multimodal application scenarios are emerging. In modern life, we can see various multimodal applications, including commercial services (e.g., e-commerce/commodity retrieval [71], visionand-language navigation (VLN) [72], [73], [74], [75], [76]), communication (e.g., lip reading [77], sign language translation [28], [29]), human-computer interaction [78], healthcare AI [79], [80], surveillance AI [81], etc.', 'Moreover, in the era of Deep Learning, deep neural networks greatly promote the development of MML, and Transformers [2] are a highly competitive architecture family, bringing new challenges and opportunities to MML. In particular, the recent success of large language models and their multimodal derivatives [82], [83], [84], [85], [86] further demonstrates the potential of Transformers in multimodal foundation models.']","MML [1], [60], [61] has been an important research area in recent decades; an early multimodal application -audiovisual speech recognition was studied in 1980s [62]. MML is key to human societies. The world we humans live in is a multimodal environment, thus both our observations and behaviours are multimodal [63]. For instance, an AI navigation robot needs multimodal sensors to perceive the real-world environment [64], [65], [66], e.g., camera, LiDAR, radar, ultrasonic, GNSS, HD Map, odometer. Furthermore, human behaviours, emotions, events, actions, and humour are multimodal, thus various human-centred MML tasks are widely studied, including multimodal emotion recognition [67], multimodal event representation [68], understanding multimodal humor [69], face-body-voice based video person-clustering [70], etc.

Thanks to the development of the internet and a wide variety of intelligent devices in recent years, increasing amounts of multimodal data are being transmitted over the internet, thus an increasing number of multimodal application scenarios are emerging. In modern life, we can see various multimodal applications, including commercial services (e.g., e-commerce/commodity retrieval [71], visionand-language navigation (VLN) [72], [73], [74], [75], [76]), communication (e.g., lip reading [77], sign language translation [28], [29]), human-computer interaction [78], healthcare AI [79], [80], surveillance AI [81], etc.

Moreover, in the era of Deep Learning, deep neural networks greatly promote the development of MML, and Transformers [2] are a highly competitive architecture family, bringing new challenges and opportunities to MML. In particular, the recent success of large language models and their multimodal derivatives [82], [83], [84], [85], [86] further demonstrates the potential of Transformers in multimodal foundation models.","(p3.0) MML [1], [60], [61] has been an important research area in recent decades; an early multimodal application -audiovisual speech recognition was studied in 1980s [62]. MML is key to human societies. The world we humans live in is a multimodal environment, thus both our observations and behaviours are multimodal [63]. For instance, an AI navigation robot needs multimodal sensors to perceive the real-world environment [64], [65], [66], e.g., camera, LiDAR, radar, ultrasonic, GNSS, HD Map, odometer. Furthermore, human behaviours, emotions, events, actions, and humour are multimodal, thus various human-centred MML tasks are widely studied, including multimodal emotion recognition [67], multimodal event representation [68], understanding multimodal humor [69], face-body-voice based video person-clustering [70], etc.

(p3.1) Thanks to the development of the internet and a wide variety of intelligent devices in recent years, increasing amounts of multimodal data are being transmitted over the internet, thus an increasing number of multimodal application scenarios are emerging. In modern life, we can see various multimodal applications, including commercial services (e.g., e-commerce/commodity retrieval [71], visionand-language navigation (VLN) [72], [73], [74], [75], [76]), communication (e.g., lip reading [77], sign language translation [28], [29]), human-computer interaction [78], healthcare AI [79], [80], surveillance AI [81], etc.

(p3.2) Moreover, in the era of Deep Learning, deep neural networks greatly promote the development of MML, and Transformers [2] are a highly competitive architecture family, bringing new challenges and opportunities to MML. In particular, the recent success of large language models and their multimodal derivatives [82], [83], [84], [85], [86] further demonstrates the potential of Transformers in multimodal foundation models.","[['b62', 'b63', 'b69', 'b67', 'b0', 'b65', 'b60', 'b68', 'b66', 'b64', 'b59', 'b61'], ['b78', 'b72', 'b74', 'b28', 'b73', 'b77', 'b80', 'b27', 'b75', 'b79', 'b71', 'b76', 'b70'], ['b82', 'b84', 'b85', 'b1', 'b83', 'b81']]","[['b62', 'b63', 'b69', 'b67', 'b0', 'b65', 'b60', 'b68', 'b66', 'b64', 'b59', 'b61'], ['b78', 'b72', 'b74', 'b28', 'b73', 'b77', 'b80', 'b27', 'b75', 'b79', 'b71', 'b76', 'b70'], ['b82', 'b84', 'b85', 'b1', 'b83', 'b81']]",31,"1. MML [1], [60], [61] has been an important research area in recent decades; an early multimodal application -audiovisual speech recognition was studied in 1980s [62].
2. MML is key to human societies. The world we humans live in is a multimodal environment, thus both our observations and behaviours are multimodal [63].
3. For instance, an AI navigation robot needs multimodal sensors to perceive the real-world environment [64], [65], [66], e.g., camera, LiDAR, radar, ultrasonic, GNSS, HD Map, odometer.
4. Furthermore, human behaviours, emotions, events, actions, and humour are multimodal, thus various human-centred MML tasks are widely studied, including multimodal emotion recognition [67], multimodal event representation [68], understanding multimodal humor [60]0, face-body-voice based video person-clustering [60]1, etc.Thanks to the development of the internet and a wide variety of intelligent devices in recent years, increasing amounts of multimodal data are being transmitted over the internet, thus an increasing number of multimodal application scenarios are emerging.
5. In modern life, we can see various multimodal applications, including commercial services [60]2 [60]3, [60]4, [60]5, [60]6, [60]7), communication [60]8, human-computer interaction [60]9, healthcare AI [61]0, [61]1, surveillance AI [61]2, etc.Moreover, in the era of Deep Learning, deep neural networks greatly promote the development of MML, and Transformers [61]3 are a highly competitive architecture family, bringing new challenges and opportunities to MML.
6. In particular, the recent success of large language models and their multimodal derivatives [61]4, [61]5, [61]6, [61]7, [61]8 further demonstrates the potential of Transformers in multimodal foundation models.","How has the application of multimodal learning evolved since its early days, specifically in the context of audiovisual speech recognition?
What roles do multimodal sensors play in the functionality of AI navigation robots, and what are some examples of these sensors?
In what ways have human-centred MML tasks expanded to include various aspects of human behavior and interaction?
How have advancements in internet technology and intelligent devices influenced the growth of multimodal applications in recent years?
What impact have Transformers and large language models had on the development and potential of multimodal learning?","1. How has the application of multimodal learning evolved since its early days, specifically in the context of audiovisual speech recognition?
2. What roles do multimodal sensors play in the functionality of AI navigation robots, and what are some examples of these sensors?
3. In what ways have human-centred MML tasks expanded to include various aspects of human behavior and interaction?
4. How have advancements in internet technology and intelligent devices influenced the growth of multimodal applications in recent years?
5. What impact have Transformers and large language models had on the development and potential of multimodal learning?",None,,"Questions:

1. How has the application of multimodal learning evolved since its early days, specifically in the context of audiovisual speech recognition?
2. What roles do multimodal sensors play in the functionality of AI navigation robots, and what are some examples of these sensors?
3. In what ways have human-centred MML tasks expanded to include various aspects of human behavior and interaction?
4. How have advancements in internet technology and intelligent devices influenced the growth of multimodal applications in recent years?
5. What impact have Transformers and large language models had on the development and potential of multimodal learning?

Answer:

(p3.0) MML [1], [60], [61] has been an important research area in recent decades; an early multimodal application -audiovisual speech recognition was studied in 1980s [62]. MML is key to human societies. The world we humans live in is a multimodal environment, thus both our observations and behaviours are multimodal [63]. For instance, an AI navigation robot needs multimodal sensors to perceive the real-world environment [64], [65], [66], e.g., camera, LiDAR, radar, ultrasonic, GNSS, HD Map, odometer. Furthermore, human behaviours, emotions, events, actions, and humour are multimodal, thus various human-centred MML tasks are widely studied, including multimodal emotion recognition [67], multimodal event representation [68], understanding multimodal humor [69], face-body-voice based video person-clustering [70], etc.

(p3.1) Thanks to the development of the internet and a wide variety of intelligent devices in recent years, increasing amounts of multimodal data are being transmitted over the internet, thus an increasing number of multimodal application scenarios are emerging. In modern life, we can see various multimodal applications, including commercial services (e.g., e-commerce/commodity retrieval [71], visionand-language navigation (VLN) [72], [73], [74], [75], [76]), communication (e.g., lip reading [77], sign language translation [28], [29]), human-computer interaction [78], healthcare AI [79], [80], surveillance AI [81], etc.

(p3.2) Moreover, in the era of Deep Learning, deep neural networks greatly promote the development of MML, and Transformers [2] are a highly competitive architecture family, bringing new challenges and opportunities to MML. In particular, the recent success of large language models and their multimodal derivatives [82], [83], [84], [85], [86] further demonstrates the potential of Transformers in multimodal foundation models.","Questions:



Answer:

(p3.0) MML [1], [60], [61] has been an important research area in recent decades; an early multimodal application -audiovisual speech recognition was studied in 1980s [62]. MML is key to human societies. The world we humans live in is a multimodal environment, thus both our observations and behaviours are multimodal [63]. For instance, an AI navigation robot needs multimodal sensors to perceive the real-world environment [64], [65], [66], e.g., camera, LiDAR, radar, ultrasonic, GNSS, HD Map, odometer. Furthermore, human behaviours, emotions, events, actions, and humour are multimodal, thus various human-centred MML tasks are widely studied, including multimodal emotion recognition [67], multimodal event representation [68], understanding multimodal humor [69], face-body-voice based video person-clustering [70], etc.

(p3.1) Thanks to the development of the internet and a wide variety of intelligent devices in recent years, increasing amounts of multimodal data are being transmitted over the internet, thus an increasing number of multimodal application scenarios are emerging. In modern life, we can see various multimodal applications, including commercial services (e.g., e-commerce/commodity retrieval [71], visionand-language navigation (VLN) [72], [73], [74], [75], [76]), communication (e.g., lip reading [77], sign language translation [28], [29]), human-computer interaction [78], healthcare AI [79], [80], surveillance AI [81], etc.

(p3.2) Moreover, in the era of Deep Learning, deep neural networks greatly promote the development of MML, and Transformers [2] are a highly competitive architecture family, bringing new challenges and opportunities to MML. In particular, the recent success of large language models and their multimodal derivatives [82], [83], [84], [85], [86] further demonstrates the potential of Transformers in multimodal foundation models."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s4,Transformers: a Brief History and Milestones,"['p4.0', 'p4.1', 'p4.2', 'p4.3']","['Transformers are emerging as promising learners. Vanilla Transformer [2] benefits from a self-attention mechanism, and is a breakthrough model for sequence-specific representation learning that was originally proposed for NLP, achieving the state-of-the-art on various NLP tasks. Following the great success of Vanilla Transformer, a lot of derivative models have been proposed, e.g., BERT [4], BART [87], GPT [88], Longformer [43], Transformer-XL [89], XLNet [90].', 'Transformers currently stand at the dominant position in NLP domains, and this motivates researchers try to apply Transformers to other modalities, such as visual domains. In early attempts for visual domain, the general pipeline is ""CNN features + standard Transformer encoder"", and researchers achieved BERT-style pretraining, via preprocessing raw images by resizing to a low resolution and reshaping into a 1D sequence [91].', 'Vision Transformer (ViT) [5] is a seminal work that contributes an end-to-end solution by applying the encoder of Transformer to images. Both ViT and its variants have been widely applied to various computer vision tasks, including low-level tasks [92], recognition [93], detection [94], segmentation [95], etc, and also work well for both supervised [93] and self-supervised [96], [97], [98] visual learning. Moreover, some recently-released works provide further theoretical understanding for ViT, e.g., its internal representation robustness [99], the continuous behaviour of its latent representation propagation [100], [101]. Motivated by the great success of Transformer, VideoBERT [7] is a breakthrough work that is the first work to extend Transformer to the multimodal tasks. VideoBERT demonstrates the great potential of Transformer in multimodal context. Following VideoBERT, a lot of Transformer based multimodal pretraining models (e.g., ViLBERT [102], LXMERT [103], VisualBERT [104], VL-BERT [105], UNITER [106], CBT [107], Unicoder-VL [108], B2T2 [109], VLP [110], 12-in-1 [111], Oscar [112], Pixel-BERT [113], ActBERT [114], ImageBERT [115], HERO [116], UniVL [117]) have become research topics of increasing interest in the field of machine learning.', 'In 2021, CLIP [9] was proposed. It is a new milestone that uses multimodal pretraining to convert classification as a retrieval task that enables the pretrained models to tackle zero-shot recognition. Thus, CLIP is a successful practice that makes full use of large-scale multimodal pretraining to enable zero-shot learning. Recently, the idea of CLIP is further studied, e.g., CLIP pretrained model based zero-shot semantic segmentation [118], ALIGN [119], CLIP-TD [120], ALBEF [121], and CoCa [122].']","Transformers are emerging as promising learners. Vanilla Transformer [2] benefits from a self-attention mechanism, and is a breakthrough model for sequence-specific representation learning that was originally proposed for NLP, achieving the state-of-the-art on various NLP tasks. Following the great success of Vanilla Transformer, a lot of derivative models have been proposed, e.g., BERT [4], BART [87], GPT [88], Longformer [43], Transformer-XL [89], XLNet [90].

Transformers currently stand at the dominant position in NLP domains, and this motivates researchers try to apply Transformers to other modalities, such as visual domains. In early attempts for visual domain, the general pipeline is ""CNN features + standard Transformer encoder"", and researchers achieved BERT-style pretraining, via preprocessing raw images by resizing to a low resolution and reshaping into a 1D sequence [91].

Vision Transformer (ViT) [5] is a seminal work that contributes an end-to-end solution by applying the encoder of Transformer to images. Both ViT and its variants have been widely applied to various computer vision tasks, including low-level tasks [92], recognition [93], detection [94], segmentation [95], etc, and also work well for both supervised [93] and self-supervised [96], [97], [98] visual learning. Moreover, some recently-released works provide further theoretical understanding for ViT, e.g., its internal representation robustness [99], the continuous behaviour of its latent representation propagation [100], [101]. Motivated by the great success of Transformer, VideoBERT [7] is a breakthrough work that is the first work to extend Transformer to the multimodal tasks. VideoBERT demonstrates the great potential of Transformer in multimodal context. Following VideoBERT, a lot of Transformer based multimodal pretraining models (e.g., ViLBERT [102], LXMERT [103], VisualBERT [104], VL-BERT [105], UNITER [106], CBT [107], Unicoder-VL [108], B2T2 [109], VLP [110], 12-in-1 [111], Oscar [112], Pixel-BERT [113], ActBERT [114], ImageBERT [115], HERO [116], UniVL [117]) have become research topics of increasing interest in the field of machine learning.

In 2021, CLIP [9] was proposed. It is a new milestone that uses multimodal pretraining to convert classification as a retrieval task that enables the pretrained models to tackle zero-shot recognition. Thus, CLIP is a successful practice that makes full use of large-scale multimodal pretraining to enable zero-shot learning. Recently, the idea of CLIP is further studied, e.g., CLIP pretrained model based zero-shot semantic segmentation [118], ALIGN [119], CLIP-TD [120], ALBEF [121], and CoCa [122].","(p4.0) Transformers are emerging as promising learners. Vanilla Transformer [2] benefits from a self-attention mechanism, and is a breakthrough model for sequence-specific representation learning that was originally proposed for NLP, achieving the state-of-the-art on various NLP tasks. Following the great success of Vanilla Transformer, a lot of derivative models have been proposed, e.g., BERT [4], BART [87], GPT [88], Longformer [43], Transformer-XL [89], XLNet [90].

(p4.1) Transformers currently stand at the dominant position in NLP domains, and this motivates researchers try to apply Transformers to other modalities, such as visual domains. In early attempts for visual domain, the general pipeline is ""CNN features + standard Transformer encoder"", and researchers achieved BERT-style pretraining, via preprocessing raw images by resizing to a low resolution and reshaping into a 1D sequence [91].

(p4.2) Vision Transformer (ViT) [5] is a seminal work that contributes an end-to-end solution by applying the encoder of Transformer to images. Both ViT and its variants have been widely applied to various computer vision tasks, including low-level tasks [92], recognition [93], detection [94], segmentation [95], etc, and also work well for both supervised [93] and self-supervised [96], [97], [98] visual learning. Moreover, some recently-released works provide further theoretical understanding for ViT, e.g., its internal representation robustness [99], the continuous behaviour of its latent representation propagation [100], [101]. Motivated by the great success of Transformer, VideoBERT [7] is a breakthrough work that is the first work to extend Transformer to the multimodal tasks. VideoBERT demonstrates the great potential of Transformer in multimodal context. Following VideoBERT, a lot of Transformer based multimodal pretraining models (e.g., ViLBERT [102], LXMERT [103], VisualBERT [104], VL-BERT [105], UNITER [106], CBT [107], Unicoder-VL [108], B2T2 [109], VLP [110], 12-in-1 [111], Oscar [112], Pixel-BERT [113], ActBERT [114], ImageBERT [115], HERO [116], UniVL [117]) have become research topics of increasing interest in the field of machine learning.

(p4.3) In 2021, CLIP [9] was proposed. It is a new milestone that uses multimodal pretraining to convert classification as a retrieval task that enables the pretrained models to tackle zero-shot recognition. Thus, CLIP is a successful practice that makes full use of large-scale multimodal pretraining to enable zero-shot learning. Recently, the idea of CLIP is further studied, e.g., CLIP pretrained model based zero-shot semantic segmentation [118], ALIGN [119], CLIP-TD [120], ALBEF [121], and CoCa [122].","[['b1', 'b87', 'b89', 'b42', 'b86', 'b3', 'b88'], ['b90'], ['b110', 'b6', 'b114', 'b100', 'b91', 'b98', 'b101', 'b109', 'b108', 'b111', 'b97', 'b116', 'b105', 'b95', 'b107', 'b92', 'b103', 'b104', 'b113', 'b102', 'b112', 'b106', 'b99', 'b4', 'b115', 'b93', 'b96', 'b94'], ['b119', 'b117', 'b8', 'b118', 'b121', 'b120']]","[['b1', 'b87', 'b89', 'b42', 'b86', 'b3', 'b88'], ['b90'], ['b110', 'b6', 'b114', 'b100', 'b91', 'b98', 'b101', 'b109', 'b108', 'b111', 'b97', 'b116', 'b105', 'b95', 'b107', 'b92', 'b103', 'b104', 'b113', 'b102', 'b112', 'b106', 'b99', 'b4', 'b115', 'b93', 'b96', 'b94'], ['b119', 'b117', 'b8', 'b118', 'b121', 'b120']]",42,"1. Transformers are emerging as promising learners.
2. Vanilla Transformer [2] benefits from a self-attention mechanism, and is a breakthrough model for sequence-specific representation learning that was originally proposed for NLP, achieving the state-of-the-art on various NLP tasks.
3. Following the great success of Vanilla Transformer, a lot of derivative models have been proposed, e.g., BERT [4], BART [87], GPT [88], Longformer [43], Transformer-XL [89], XLNet [90].
4. Transformers currently stand at the dominant position in NLP domains, and this motivates researchers try to apply Transformers to other modalities, such as visual domains.
5. In early attempts for visual domain, the general pipeline is ""CNN features + standard Transformer encoder"", and researchers achieved BERT-style pretraining, via preprocessing raw images by resizing to a low resolution and reshaping into a 1D sequence [91].Vision Transformer (ViT) [5] is a seminal work that contributes an end-to-end solution by applying the encoder of Transformer to images.
6. Both ViT and its variants have been widely applied to various computer vision tasks, including low-level tasks [4]0, recognition [4]4, detection [4]2, segmentation [4]3, etc, and also work well for both supervised [4]4 and self-supervised [4]5, [4]6, [4]7 visual learning.
7. Moreover, some recently-released works provide further theoretical understanding for ViT, e.g., its internal representation robustness [4]8, the continuous behaviour of its latent representation propagation [4]9, [87]0.
8. Motivated by the great success of Transformer, VideoBERT [87]1 is a breakthrough work that is the first work to extend Transformer to the multimodal tasks.
9. VideoBERT demonstrates the great potential of Transformer in multimodal context.
10. Following VideoBERT, a lot of Transformer based multimodal pretraining models [87]2 have become research topics of increasing interest in the field of machine learning.
11. In 2021, CLIP [87]3 was proposed.
12. It is a new milestone that uses multimodal pretraining to convert classification as a retrieval task that enables the pretrained models to tackle zero-shot recognition.
13. Thus, CLIP is a successful practice that makes full use of large-scale multimodal pretraining to enable zero-shot learning.
14. Recently, the idea of CLIP is further studied, e.g., CLIP pretrained model based zero-shot semantic segmentation [87]4, ALIGN [87]5, CLIP-TD [87]6, ALBEF [87]7, and CoCa [87]8.","What are the key features and breakthroughs of the Vanilla Transformer model in NLP tasks?
How have Transformers been adapted for use in visual domain tasks, and what was the general approach before the introduction of Vision Transformer (ViT)?
What are the contributions of Vision Transformer (ViT) to computer vision tasks, and how does it differ from previous models?
Can you describe the significance of VideoBERT in extending Transformer models to multimodal tasks?
What is the role of CLIP in advancing zero-shot learning capabilities of multimodal pretrained models?","1. What are the key features and breakthroughs of the Vanilla Transformer model in NLP tasks?
2. How have Transformers been adapted for use in visual domain tasks, and what was the general approach before the introduction of Vision Transformer (ViT)?
3. What are the contributions of Vision Transformer (ViT) to computer vision tasks, and how does it differ from previous models?
4. Can you describe the significance of VideoBERT in extending Transformer models to multimodal tasks?
5. What is the role of CLIP in advancing zero-shot learning capabilities of multimodal pretrained models?",None,,"Questions:

1. What are the key features and breakthroughs of the Vanilla Transformer model in NLP tasks?
2. How have Transformers been adapted for use in visual domain tasks, and what was the general approach before the introduction of Vision Transformer (ViT)?
3. What are the contributions of Vision Transformer (ViT) to computer vision tasks, and how does it differ from previous models?
4. Can you describe the significance of VideoBERT in extending Transformer models to multimodal tasks?
5. What is the role of CLIP in advancing zero-shot learning capabilities of multimodal pretrained models?

Answer:

(p4.0) Transformers are emerging as promising learners. Vanilla Transformer [2] benefits from a self-attention mechanism, and is a breakthrough model for sequence-specific representation learning that was originally proposed for NLP, achieving the state-of-the-art on various NLP tasks. Following the great success of Vanilla Transformer, a lot of derivative models have been proposed, e.g., BERT [4], BART [87], GPT [88], Longformer [43], Transformer-XL [89], XLNet [90].

(p4.1) Transformers currently stand at the dominant position in NLP domains, and this motivates researchers try to apply Transformers to other modalities, such as visual domains. In early attempts for visual domain, the general pipeline is ""CNN features + standard Transformer encoder"", and researchers achieved BERT-style pretraining, via preprocessing raw images by resizing to a low resolution and reshaping into a 1D sequence [91].

(p4.2) Vision Transformer (ViT) [5] is a seminal work that contributes an end-to-end solution by applying the encoder of Transformer to images. Both ViT and its variants have been widely applied to various computer vision tasks, including low-level tasks [92], recognition [93], detection [94], segmentation [95], etc, and also work well for both supervised [93] and self-supervised [96], [97], [98] visual learning. Moreover, some recently-released works provide further theoretical understanding for ViT, e.g., its internal representation robustness [99], the continuous behaviour of its latent representation propagation [100], [101]. Motivated by the great success of Transformer, VideoBERT [7] is a breakthrough work that is the first work to extend Transformer to the multimodal tasks. VideoBERT demonstrates the great potential of Transformer in multimodal context. Following VideoBERT, a lot of Transformer based multimodal pretraining models (e.g., ViLBERT [102], LXMERT [103], VisualBERT [104], VL-BERT [105], UNITER [106], CBT [107], Unicoder-VL [108], B2T2 [109], VLP [110], 12-in-1 [111], Oscar [112], Pixel-BERT [113], ActBERT [114], ImageBERT [115], HERO [116], UniVL [117]) have become research topics of increasing interest in the field of machine learning.

(p4.3) In 2021, CLIP [9] was proposed. It is a new milestone that uses multimodal pretraining to convert classification as a retrieval task that enables the pretrained models to tackle zero-shot recognition. Thus, CLIP is a successful practice that makes full use of large-scale multimodal pretraining to enable zero-shot learning. Recently, the idea of CLIP is further studied, e.g., CLIP pretrained model based zero-shot semantic segmentation [118], ALIGN [119], CLIP-TD [120], ALBEF [121], and CoCa [122].","Questions:



Answer:

(p4.0) Transformers are emerging as promising learners. Vanilla Transformer [2] benefits from a self-attention mechanism, and is a breakthrough model for sequence-specific representation learning that was originally proposed for NLP, achieving the state-of-the-art on various NLP tasks. Following the great success of Vanilla Transformer, a lot of derivative models have been proposed, e.g., BERT [4], BART [87], GPT [88], Longformer [43], Transformer-XL [89], XLNet [90].

(p4.1) Transformers currently stand at the dominant position in NLP domains, and this motivates researchers try to apply Transformers to other modalities, such as visual domains. In early attempts for visual domain, the general pipeline is ""CNN features + standard Transformer encoder"", and researchers achieved BERT-style pretraining, via preprocessing raw images by resizing to a low resolution and reshaping into a 1D sequence [91].

(p4.2) Vision Transformer (ViT) [5] is a seminal work that contributes an end-to-end solution by applying the encoder of Transformer to images. Both ViT and its variants have been widely applied to various computer vision tasks, including low-level tasks [92], recognition [93], detection [94], segmentation [95], etc, and also work well for both supervised [93] and self-supervised [96], [97], [98] visual learning. Moreover, some recently-released works provide further theoretical understanding for ViT, e.g., its internal representation robustness [99], the continuous behaviour of its latent representation propagation [100], [101]. Motivated by the great success of Transformer, VideoBERT [7] is a breakthrough work that is the first work to extend Transformer to the multimodal tasks. VideoBERT demonstrates the great potential of Transformer in multimodal context. Following VideoBERT, a lot of Transformer based multimodal pretraining models (e.g., ViLBERT [102], LXMERT [103], VisualBERT [104], VL-BERT [105], UNITER [106], CBT [107], Unicoder-VL [108], B2T2 [109], VLP [110], 12-in-1 [111], Oscar [112], Pixel-BERT [113], ActBERT [114], ImageBERT [115], HERO [116], UniVL [117]) have become research topics of increasing interest in the field of machine learning.

(p4.3) In 2021, CLIP [9] was proposed. It is a new milestone that uses multimodal pretraining to convert classification as a retrieval task that enables the pretrained models to tackle zero-shot recognition. Thus, CLIP is a successful practice that makes full use of large-scale multimodal pretraining to enable zero-shot learning. Recently, the idea of CLIP is further studied, e.g., CLIP pretrained model based zero-shot semantic segmentation [118], ALIGN [119], CLIP-TD [120], ALBEF [121], and CoCa [122]."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s5,Multimodal Big Data,"['p5.0', 'p5.1', 'p5.2', 'p5.3', 'p5.4', 'p5.5', 'p5.6', 'p5.7']","['In the past decade, with the rapid development of internet applications such as social media and online retail, massive multimodal datasets have been proposed, e.g., Conceptual Captions [123], COCO [124], VQA [125], Visual Genome [126], SBU Captions [127], Cooking312K [7], LAIT [115], e-SNLI-VE [128], ARCH [129], Adversarial VQA [130], OTT-QA [18], MULTIMODALQA (MMQA) [131], VALUE [132], Fashion IQ [133], LRS2-BBC [134], ActivityNet [135], VisDial [136].', 'Some emergent new trends among the recently released multimodal datasets are:', '(1) Data scales are larger. Various recently released datasets are million-scale, e.g., Product1M [137], Conceptual 12M [138], RUC-CAS-WenLan [139] (30M), HowToVQA69M [140], HowTo100M [141], ALT200M [142], LAION-400M [143].', '(2) More modalities. In addition to the general modalities of vision, text, and audio, further diverse modalities are emerging, e.g., Pano-AVQA [144] -the first large-scale spatial and audio-visual question answering dataset on 360 • videos, YouTube-360 (YT-360) [145] (360 • videos), AIST++ [146] (a new multimodal dataset of 3D dance motion and music), Artemis [147] (affective language for visual arts). In particular, MultiBench [148] provides a dataset including 10 modalities.', '(3) More scenarios. In addition to common caption and QA datasets, more applications and scenarios have been studied, e.g., CIRR [149] (real-life images), Product1M [137], Bed and Breakfast (BnB) [150] (vision-and-language navigation), M3A [151] (financial dataset), X-World [152] (autonomous drive).', '(4) Tasks are more difficult. Beyond the straightforward tasks, more abstract multimodal tasks are proposed, e.g., MultiMET [153] (a multimodal dataset for metaphor understanding), Hateful Memes [154] (hate speech in multimodal memes).', '(5) Instructional videos have become increasingly popular, e.g., cooking video YouCookII [155]. Aligning a sequence of instructions to a video of someone carrying out a task is an example of a powerful pretraining pretext task [7], [156]. Pretext tasks are pre-designed problems to force the models to learn representation by solving them.', 'Similar to other deep neural network architectures, Transformers are also data hungry. Therefore, their highcapacity models and multimodal big data basis co-create the prosperity of the Transformer based multimodal machine learning. For instance, big data bring zero-shot learning capability to VLP Transformer models.']","In the past decade, with the rapid development of internet applications such as social media and online retail, massive multimodal datasets have been proposed, e.g., Conceptual Captions [123], COCO [124], VQA [125], Visual Genome [126], SBU Captions [127], Cooking312K [7], LAIT [115], e-SNLI-VE [128], ARCH [129], Adversarial VQA [130], OTT-QA [18], MULTIMODALQA (MMQA) [131], VALUE [132], Fashion IQ [133], LRS2-BBC [134], ActivityNet [135], VisDial [136].

Some emergent new trends among the recently released multimodal datasets are:

(1) Data scales are larger. Various recently released datasets are million-scale, e.g., Product1M [137], Conceptual 12M [138], RUC-CAS-WenLan [139] (30M), HowToVQA69M [140], HowTo100M [141], ALT200M [142], LAION-400M [143].

(2) More modalities. In addition to the general modalities of vision, text, and audio, further diverse modalities are emerging, e.g., Pano-AVQA [144] -the first large-scale spatial and audio-visual question answering dataset on 360 • videos, YouTube-360 (YT-360) [145] (360 • videos), AIST++ [146] (a new multimodal dataset of 3D dance motion and music), Artemis [147] (affective language for visual arts). In particular, MultiBench [148] provides a dataset including 10 modalities.

(3) More scenarios. In addition to common caption and QA datasets, more applications and scenarios have been studied, e.g., CIRR [149] (real-life images), Product1M [137], Bed and Breakfast (BnB) [150] (vision-and-language navigation), M3A [151] (financial dataset), X-World [152] (autonomous drive).

(4) Tasks are more difficult. Beyond the straightforward tasks, more abstract multimodal tasks are proposed, e.g., MultiMET [153] (a multimodal dataset for metaphor understanding), Hateful Memes [154] (hate speech in multimodal memes).

(5) Instructional videos have become increasingly popular, e.g., cooking video YouCookII [155]. Aligning a sequence of instructions to a video of someone carrying out a task is an example of a powerful pretraining pretext task [7], [156]. Pretext tasks are pre-designed problems to force the models to learn representation by solving them.

Similar to other deep neural network architectures, Transformers are also data hungry. Therefore, their highcapacity models and multimodal big data basis co-create the prosperity of the Transformer based multimodal machine learning. For instance, big data bring zero-shot learning capability to VLP Transformer models.","(p5.0) In the past decade, with the rapid development of internet applications such as social media and online retail, massive multimodal datasets have been proposed, e.g., Conceptual Captions [123], COCO [124], VQA [125], Visual Genome [126], SBU Captions [127], Cooking312K [7], LAIT [115], e-SNLI-VE [128], ARCH [129], Adversarial VQA [130], OTT-QA [18], MULTIMODALQA (MMQA) [131], VALUE [132], Fashion IQ [133], LRS2-BBC [134], ActivityNet [135], VisDial [136].

(p5.1) Some emergent new trends among the recently released multimodal datasets are:

(p5.2) (1) Data scales are larger. Various recently released datasets are million-scale, e.g., Product1M [137], Conceptual 12M [138], RUC-CAS-WenLan [139] (30M), HowToVQA69M [140], HowTo100M [141], ALT200M [142], LAION-400M [143].

(p5.3) (2) More modalities. In addition to the general modalities of vision, text, and audio, further diverse modalities are emerging, e.g., Pano-AVQA [144] -the first large-scale spatial and audio-visual question answering dataset on 360 • videos, YouTube-360 (YT-360) [145] (360 • videos), AIST++ [146] (a new multimodal dataset of 3D dance motion and music), Artemis [147] (affective language for visual arts). In particular, MultiBench [148] provides a dataset including 10 modalities.

(p5.4) (3) More scenarios. In addition to common caption and QA datasets, more applications and scenarios have been studied, e.g., CIRR [149] (real-life images), Product1M [137], Bed and Breakfast (BnB) [150] (vision-and-language navigation), M3A [151] (financial dataset), X-World [152] (autonomous drive).

(p5.5) (4) Tasks are more difficult. Beyond the straightforward tasks, more abstract multimodal tasks are proposed, e.g., MultiMET [153] (a multimodal dataset for metaphor understanding), Hateful Memes [154] (hate speech in multimodal memes).

(p5.6) (5) Instructional videos have become increasingly popular, e.g., cooking video YouCookII [155]. Aligning a sequence of instructions to a video of someone carrying out a task is an example of a powerful pretraining pretext task [7], [156]. Pretext tasks are pre-designed problems to force the models to learn representation by solving them.

(p5.7) Similar to other deep neural network architectures, Transformers are also data hungry. Therefore, their highcapacity models and multimodal big data basis co-create the prosperity of the Transformer based multimodal machine learning. For instance, big data bring zero-shot learning capability to VLP Transformer models.","[['b17', 'b133', 'b129', 'b124', 'b131', 'b134', 'b6', 'b114', 'b128', 'b132', 'b123', 'b127', 'b130', 'b126', 'b135', 'b125', 'b122'], [], ['b136', 'b138', 'b140', 'b141', 'b139', 'b142', 'b137'], ['b147', 'b143', 'b145', 'b144', 'b146'], ['b136', 'b151', 'b150', 'b148', 'b149'], ['b153', 'b152'], ['b155', 'b154', 'b6'], []]","[['b17', 'b133', 'b129', 'b124', 'b131', 'b134', 'b6', 'b114', 'b128', 'b132', 'b123', 'b127', 'b130', 'b126', 'b135', 'b125', 'b122'], [], ['b136', 'b138', 'b140', 'b141', 'b139', 'b142', 'b137'], ['b147', 'b143', 'b145', 'b144', 'b146'], ['b136', 'b151', 'b150', 'b148', 'b149'], ['b153', 'b152'], ['b155', 'b154', 'b6'], []]",39,"1. In the past decade, with the rapid development of internet applications such as social media and online retail, massive multimodal datasets have been proposed, e.g., Conceptual Captions [123], COCO [124], VQA [125], Visual Genome [126], SBU Captions [127], Cooking312K [7], LAIT [115], e-SNLI-VE [128], ARCH [129], Adversarial VQA [130], OTT-QA [124]0, MULTIMODALQA [124]1 [124]2, VALUE [124]3, Fashion IQ [124]4, LRS2-BBC [124]5, ActivityNet [124]6, VisDial [124]7.
2. Some emergent new trends among the recently released multimodal datasets are:
3. [124]8 Data scales are larger. Various recently released datasets are million-scale, e.g., Product1M [127]0, Conceptual 12M [125]0, RUC-CAS-WenLan [125]1 [125]2, HowToVQA69M [125]3, HowTo100M [125]4, ALT200M [125]5, LAION-400M [125]6.
4. [125]7 More modalities. In addition to the general modalities of vision, text, and audio, further diverse modalities are emerging, e.g., Pano-AVQA [125]8 -the first large-scale spatial and audio-visual question answering dataset on 360  videos, YouTube-360 [125]9 [126]0 [126]1, AISTetc.0 [126]2 [126]3, Artemis [126]4 [126]5.
5. In particular, MultiBench [126]6 provides a dataset including 10 modalities.
6. [126]7 More scenarios. In addition to common caption and QA datasets, more applications and scenarios have been studied, e.g., CIRR [126]8 [126]9, Product1M [127]0, Bed and Breakfast [127]1 [127]2 [127]3, M3A [127]4 [127]5, X-World [127]6 [127]7.
7. [127]8 Tasks are more difficult.
8. Beyond the straightforward tasks, more abstract multimodal tasks are proposed, e.g., MultiMET [127]9 (a multimodal dataset for metaphor understanding), Hateful Memes [154] (hate speech in multimodal memes).
9. (5) Instructional videos have become increasingly popular, e.g., cooking video YouCookII [155].
10. Aligning a sequence of instructions to a video of someone carrying out a task is an example of a powerful pretraining pretext task [7], [156].
11. Pretext tasks are pre-designed problems to force the models to learn representation by solving them.
12. Similar to other deep neural network architectures, Transformers are also data hungry.
13. Therefore, their highcapacity models and multimodal big data basis co-create the prosperity of the Transformer based multimodal machine learning.
14. For instance, big data bring zero-shot learning capability to VLP Transformer models.","What are the characteristics of the emergent multimodal datasets in terms of data scale and diversity of modalities?
How do the newly introduced modalities in multimodal learning datasets enhance the capabilities of machine learning models?
In what ways have the tasks in multimodal learning evolved to become more abstract, and what are some examples of these tasks?
How has the inclusion of instructional videos in multimodal datasets influenced the development of pretraining tasks for machine learning models?
What role does the scale of multimodal big data play in enhancing the zero-shot learning capabilities of VLP Transformer models?","1. What are the characteristics of the emergent multimodal datasets in terms of data scale and diversity of modalities?
2. How do the newly introduced modalities in multimodal learning datasets enhance the capabilities of machine learning models?
3. In what ways have the tasks in multimodal learning evolved to become more abstract, and what are some examples of these tasks?
4. How has the inclusion of instructional videos in multimodal datasets influenced the development of pretraining tasks for machine learning models?
5. What role does the scale of multimodal big data play in enhancing the zero-shot learning capabilities of VLP Transformer models?",None,,"Questions:

1. What are the characteristics of the emergent multimodal datasets in terms of data scale and diversity of modalities?
2. How do the newly introduced modalities in multimodal learning datasets enhance the capabilities of machine learning models?
3. In what ways have the tasks in multimodal learning evolved to become more abstract, and what are some examples of these tasks?
4. How has the inclusion of instructional videos in multimodal datasets influenced the development of pretraining tasks for machine learning models?
5. What role does the scale of multimodal big data play in enhancing the zero-shot learning capabilities of VLP Transformer models?

Answer:

(p5.0) In the past decade, with the rapid development of internet applications such as social media and online retail, massive multimodal datasets have been proposed, e.g., Conceptual Captions [123], COCO [124], VQA [125], Visual Genome [126], SBU Captions [127], Cooking312K [7], LAIT [115], e-SNLI-VE [128], ARCH [129], Adversarial VQA [130], OTT-QA [18], MULTIMODALQA (MMQA) [131], VALUE [132], Fashion IQ [133], LRS2-BBC [134], ActivityNet [135], VisDial [136].

(p5.1) Some emergent new trends among the recently released multimodal datasets are:

(p5.2) (1) Data scales are larger. Various recently released datasets are million-scale, e.g., Product1M [137], Conceptual 12M [138], RUC-CAS-WenLan [139] (30M), HowToVQA69M [140], HowTo100M [141], ALT200M [142], LAION-400M [143].

(p5.3) (2) More modalities. In addition to the general modalities of vision, text, and audio, further diverse modalities are emerging, e.g., Pano-AVQA [144] -the first large-scale spatial and audio-visual question answering dataset on 360 • videos, YouTube-360 (YT-360) [145] (360 • videos), AIST++ [146] (a new multimodal dataset of 3D dance motion and music), Artemis [147] (affective language for visual arts). In particular, MultiBench [148] provides a dataset including 10 modalities.

(p5.4) (3) More scenarios. In addition to common caption and QA datasets, more applications and scenarios have been studied, e.g., CIRR [149] (real-life images), Product1M [137], Bed and Breakfast (BnB) [150] (vision-and-language navigation), M3A [151] (financial dataset), X-World [152] (autonomous drive).

(p5.5) (4) Tasks are more difficult. Beyond the straightforward tasks, more abstract multimodal tasks are proposed, e.g., MultiMET [153] (a multimodal dataset for metaphor understanding), Hateful Memes [154] (hate speech in multimodal memes).

(p5.6) (5) Instructional videos have become increasingly popular, e.g., cooking video YouCookII [155]. Aligning a sequence of instructions to a video of someone carrying out a task is an example of a powerful pretraining pretext task [7], [156]. Pretext tasks are pre-designed problems to force the models to learn representation by solving them.

(p5.7) Similar to other deep neural network architectures, Transformers are also data hungry. Therefore, their highcapacity models and multimodal big data basis co-create the prosperity of the Transformer based multimodal machine learning. For instance, big data bring zero-shot learning capability to VLP Transformer models.","Questions:



Answer:

(p5.0) In the past decade, with the rapid development of internet applications such as social media and online retail, massive multimodal datasets have been proposed, e.g., Conceptual Captions [123], COCO [124], VQA [125], Visual Genome [126], SBU Captions [127], Cooking312K [7], LAIT [115], e-SNLI-VE [128], ARCH [129], Adversarial VQA [130], OTT-QA [18], MULTIMODALQA (MMQA) [131], VALUE [132], Fashion IQ [133], LRS2-BBC [134], ActivityNet [135], VisDial [136].

(p5.1) Some emergent new trends among the recently released multimodal datasets are:

(p5.2) (1) Data scales are larger. Various recently released datasets are million-scale, e.g., Product1M [137], Conceptual 12M [138], RUC-CAS-WenLan [139] (30M), HowToVQA69M [140], HowTo100M [141], ALT200M [142], LAION-400M [143].

(p5.3) (2) More modalities. In addition to the general modalities of vision, text, and audio, further diverse modalities are emerging, e.g., Pano-AVQA [144] -the first large-scale spatial and audio-visual question answering dataset on 360 • videos, YouTube-360 (YT-360) [145] (360 • videos), AIST++ [146] (a new multimodal dataset of 3D dance motion and music), Artemis [147] (affective language for visual arts). In particular, MultiBench [148] provides a dataset including 10 modalities.

(p5.4) (3) More scenarios. In addition to common caption and QA datasets, more applications and scenarios have been studied, e.g., CIRR [149] (real-life images), Product1M [137], Bed and Breakfast (BnB) [150] (vision-and-language navigation), M3A [151] (financial dataset), X-World [152] (autonomous drive).

(p5.5) (4) Tasks are more difficult. Beyond the straightforward tasks, more abstract multimodal tasks are proposed, e.g., MultiMET [153] (a multimodal dataset for metaphor understanding), Hateful Memes [154] (hate speech in multimodal memes).

(p5.6) (5) Instructional videos have become increasingly popular, e.g., cooking video YouCookII [155]. Aligning a sequence of instructions to a video of someone carrying out a task is an example of a powerful pretraining pretext task [7], [156]. Pretext tasks are pre-designed problems to force the models to learn representation by solving them.

(p5.7) Similar to other deep neural network architectures, Transformers are also data hungry. Therefore, their highcapacity models and multimodal big data basis co-create the prosperity of the Transformer based multimodal machine learning. For instance, big data bring zero-shot learning capability to VLP Transformer models."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s6,TRANSFORMERS,['p6.0'],"['In this section, we use mathematical formulations to review the key techniques of Vanilla Transformer [2], Vision Transformer [5], and multimodal Transformers 1 , including tokenized inputs, self-attention, multi-head attention, basic Transformer layers/blocks, etc. We highlight that Vanilla Transformers can be understood from a geometrically topological perspective [157], because due to the self-attention mechanism, given each tokenized input from any modalities, Vanilla self-attention (Transformer) can model it as a fully-connected graph in topological geometry space [158]. Compared with other deep networks (for instance, CNN is restricted in the aligned grid spaces/matrices), Transformers intrinsically have a more general and flexible modelling space. This is a notable advantage of Transformers for multimodal tasks. Sections 3.1, 3.2, and 3.3 will review the key designs of Vanilla Transformer, Vision Transformer, and multimodal Transformers, respectively.']","In this section, we use mathematical formulations to review the key techniques of Vanilla Transformer [2], Vision Transformer [5], and multimodal Transformers 1 , including tokenized inputs, self-attention, multi-head attention, basic Transformer layers/blocks, etc. We highlight that Vanilla Transformers can be understood from a geometrically topological perspective [157], because due to the self-attention mechanism, given each tokenized input from any modalities, Vanilla self-attention (Transformer) can model it as a fully-connected graph in topological geometry space [158]. Compared with other deep networks (for instance, CNN is restricted in the aligned grid spaces/matrices), Transformers intrinsically have a more general and flexible modelling space. This is a notable advantage of Transformers for multimodal tasks. Sections 3.1, 3.2, and 3.3 will review the key designs of Vanilla Transformer, Vision Transformer, and multimodal Transformers, respectively.","(p6.0) In this section, we use mathematical formulations to review the key techniques of Vanilla Transformer [2], Vision Transformer [5], and multimodal Transformers 1 , including tokenized inputs, self-attention, multi-head attention, basic Transformer layers/blocks, etc. We highlight that Vanilla Transformers can be understood from a geometrically topological perspective [157], because due to the self-attention mechanism, given each tokenized input from any modalities, Vanilla self-attention (Transformer) can model it as a fully-connected graph in topological geometry space [158]. Compared with other deep networks (for instance, CNN is restricted in the aligned grid spaces/matrices), Transformers intrinsically have a more general and flexible modelling space. This is a notable advantage of Transformers for multimodal tasks. Sections 3.1, 3.2, and 3.3 will review the key designs of Vanilla Transformer, Vision Transformer, and multimodal Transformers, respectively.","[['b4', 'b157', 'b1', 'b156']]","[['b4', 'b157', 'b1', 'b156']]",4,"1. In this section, we use mathematical formulations to review the key techniques of Vanilla Transformer [2], Vision Transformer [5], and multimodal Transformers 1 , including tokenized inputs, self-attention, multi-head attention, basic Transformer layers/blocks, etc. We highlight that Vanilla Transformers can be understood from a geometrically topological perspective [157], because due to the self-attention mechanism, given each tokenized input from any modalities, Vanilla self-attention (Transformer) can model it as a fully-connected graph in topological geometry space [158].
2. Compared with other deep networks (for instance, CNN is restricted in the aligned grid spaces/matrices), Transformers intrinsically have a more general and flexible modelling space.
3. This is a notable advantage of Transformers for multimodal tasks.
4. Sections 3.1, 3.2, and 3.3 will review the key designs of Vanilla Transformer, Vision Transformer, and multimodal Transformers, respectively.","What are the key techniques reviewed in the survey of multimodal learning with Transformers?
How does the Vanilla Transformer model inputs from different modalities in a geometric topological space?
What advantage do Transformers have over CNNs in terms of modeling space for multimodal tasks?
What specific aspects of Vanilla Transformer, Vision Transformer, and multimodal Transformers are discussed in sections 3.1, 3.2, and 3.3 of the survey?
How does the self-attention mechanism in Transformers contribute to their ability to handle multimodal inputs?","1. What are the key techniques reviewed in the survey of multimodal learning with Transformers?
2. How does the Vanilla Transformer model inputs from different modalities in a geometric topological space?
3. What advantage do Transformers have over CNNs in terms of modeling space for multimodal tasks?
4. What specific aspects of Vanilla Transformer, Vision Transformer, and multimodal Transformers are discussed in sections 3.1, 3.2, and 3.3 of the survey?
5. How does the self-attention mechanism in Transformers contribute to their ability to handle multimodal inputs?",None,,"Questions:

1. What are the key techniques reviewed in the survey of multimodal learning with Transformers?
2. How does the Vanilla Transformer model inputs from different modalities in a geometric topological space?
3. What advantage do Transformers have over CNNs in terms of modeling space for multimodal tasks?
4. What specific aspects of Vanilla Transformer, Vision Transformer, and multimodal Transformers are discussed in sections 3.1, 3.2, and 3.3 of the survey?
5. How does the self-attention mechanism in Transformers contribute to their ability to handle multimodal inputs?

Answer:

(p6.0) In this section, we use mathematical formulations to review the key techniques of Vanilla Transformer [2], Vision Transformer [5], and multimodal Transformers 1 , including tokenized inputs, self-attention, multi-head attention, basic Transformer layers/blocks, etc. We highlight that Vanilla Transformers can be understood from a geometrically topological perspective [157], because due to the self-attention mechanism, given each tokenized input from any modalities, Vanilla self-attention (Transformer) can model it as a fully-connected graph in topological geometry space [158]. Compared with other deep networks (for instance, CNN is restricted in the aligned grid spaces/matrices), Transformers intrinsically have a more general and flexible modelling space. This is a notable advantage of Transformers for multimodal tasks. Sections 3.1, 3.2, and 3.3 will review the key designs of Vanilla Transformer, Vision Transformer, and multimodal Transformers, respectively.","Questions:



Answer:

(p6.0) In this section, we use mathematical formulations to review the key techniques of Vanilla Transformer [2], Vision Transformer [5], and multimodal Transformers 1 , including tokenized inputs, self-attention, multi-head attention, basic Transformer layers/blocks, etc. We highlight that Vanilla Transformers can be understood from a geometrically topological perspective [157], because due to the self-attention mechanism, given each tokenized input from any modalities, Vanilla self-attention (Transformer) can model it as a fully-connected graph in topological geometry space [158]. Compared with other deep networks (for instance, CNN is restricted in the aligned grid spaces/matrices), Transformers intrinsically have a more general and flexible modelling space. This is a notable advantage of Transformers for multimodal tasks. Sections 3.1, 3.2, and 3.3 will review the key designs of Vanilla Transformer, Vision Transformer, and multimodal Transformers, respectively."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s7,Vanilla Transformer,"['p7.0', 'p7.1']","['Vanilla Transformer has an encoder-decoder structure and is the origin of the Transformer-based research field. It takes tokenized input (see Section 3.1.1). Both its encoder and decoder are stacked by the Transformer layers/blocks, as demonstrated in Figure 1. Each block has two sub-layers, i.e., a multi-head self-attention (MHSA) layer (see Section 3.1.2) and a position-wise fully-connected feed-forward network (FFN) (see Section 3.1.3). To help the back propagation of the gradient, both MHSA and FFN use Residual Connection [159] (given an input x, the residual connection of any mapping f (·) is defined as x ← f (x) + x), followed by normalization layer. Thus, assuming that the input tensor is Z, the output of MHSA and FFN sub-layers can be formulated as:', 'where sublayer(·) is the mapping implemented by the sublayer itself and N (·) denotes normalization, e.g., BN (·) [160], LN (·) [161]. Discussion There is an important unsolved problem that is post-normalization versus pre-normalization. The original Vanilla Transformer uses post-normalization for each MHSA and FFN sub-layer. However, if we consider this from the mathematical perspective, pre-normalization makes more sense [162]. This is similar to the basic principle of the theory of matrix, that normalization should be performed before projection, e.g., Gram-Schmidt process 2 . This problem should be studied further by both theoretical research and experimental validation.']","Vanilla Transformer has an encoder-decoder structure and is the origin of the Transformer-based research field. It takes tokenized input (see Section 3.1.1). Both its encoder and decoder are stacked by the Transformer layers/blocks, as demonstrated in Figure 1. Each block has two sub-layers, i.e., a multi-head self-attention (MHSA) layer (see Section 3.1.2) and a position-wise fully-connected feed-forward network (FFN) (see Section 3.1.3). To help the back propagation of the gradient, both MHSA and FFN use Residual Connection [159] (given an input x, the residual connection of any mapping f (·) is defined as x ← f (x) + x), followed by normalization layer. Thus, assuming that the input tensor is Z, the output of MHSA and FFN sub-layers can be formulated as:

where sublayer(·) is the mapping implemented by the sublayer itself and N (·) denotes normalization, e.g., BN (·) [160], LN (·) [161]. Discussion There is an important unsolved problem that is post-normalization versus pre-normalization. The original Vanilla Transformer uses post-normalization for each MHSA and FFN sub-layer. However, if we consider this from the mathematical perspective, pre-normalization makes more sense [162]. This is similar to the basic principle of the theory of matrix, that normalization should be performed before projection, e.g., Gram-Schmidt process 2 . This problem should be studied further by both theoretical research and experimental validation.","(p7.0) Vanilla Transformer has an encoder-decoder structure and is the origin of the Transformer-based research field. It takes tokenized input (see Section 3.1.1). Both its encoder and decoder are stacked by the Transformer layers/blocks, as demonstrated in Figure 1. Each block has two sub-layers, i.e., a multi-head self-attention (MHSA) layer (see Section 3.1.2) and a position-wise fully-connected feed-forward network (FFN) (see Section 3.1.3). To help the back propagation of the gradient, both MHSA and FFN use Residual Connection [159] (given an input x, the residual connection of any mapping f (·) is defined as x ← f (x) + x), followed by normalization layer. Thus, assuming that the input tensor is Z, the output of MHSA and FFN sub-layers can be formulated as:

(p7.1) where sublayer(·) is the mapping implemented by the sublayer itself and N (·) denotes normalization, e.g., BN (·) [160], LN (·) [161]. Discussion There is an important unsolved problem that is post-normalization versus pre-normalization. The original Vanilla Transformer uses post-normalization for each MHSA and FFN sub-layer. However, if we consider this from the mathematical perspective, pre-normalization makes more sense [162]. This is similar to the basic principle of the theory of matrix, that normalization should be performed before projection, e.g., Gram-Schmidt process 2 . This problem should be studied further by both theoretical research and experimental validation.","[['b158'], ['b160', 'b161', 'b159']]","[['b158'], ['b160', 'b161', 'b159']]",4,"1. Vanilla Transformer has an encoder-decoder structure and is the origin of the Transformer-based research field.
2. It takes tokenized input (see Section 3.1.1).
3. Both its encoder and decoder are stacked by the Transformer layers/blocks, as demonstrated in Figure 1.
4. Each block has two sub-layers, i.e., a multi-head self-attention (MHSA) layer (see Section 3.1.2) and a position-wise fully-connected feed-forward network (FFN) (see Section 3.1.3).
5. To help the back propagation of the gradient, both MHSA and FFN use Residual Connection [159] (given an input x, the residual connection of any mapping f (·) is defined as x ← f (x) + x), followed by normalization layer.
6. Thus, assuming that the input tensor is Z, the output of MHSA and FFN sub-layers can be formulated as:where sublayer(MHSA)2 is the mapping implemented by the sublayer itself and N (MHSA)2 denotes normalization, e.g., BN (MHSA)2 (MHSA)1, LN (MHSA)2 (MHSA)3.
7. Discussion There is an important unsolved problem that is post-normalization versus pre-normalization.
8. The original Vanilla Transformer uses post-normalization for each MHSA and FFN sub-layer.
9. However, if we consider this from the mathematical perspective, pre-normalization makes more sense (MHSA)4.
10. This is similar to the basic principle of the theory of matrix, that normalization should be performed before projection, e.g., Gram-Schmidt process 2 .
11. This problem should be studied further by both theoretical research and experimental validation.","What is the basic structure of the Vanilla Transformer in Transformer-based research?
How do the multi-head self-attention (MHSA) and position-wise fully-connected feed-forward network (FFN) layers function within the Vanilla Transformer architecture?
What role do Residual Connections play in the Vanilla Transformer's encoder and decoder blocks?
Why is there a debate between using post-normalization and pre-normalization in the context of Vanilla Transformer's architecture?
How might future research address the unresolved issue of post-normalization versus pre-normalization in Transformer models?","1. What is the basic structure of the Vanilla Transformer in Transformer-based research?
2. How do the multi-head self-attention (MHSA) and position-wise fully-connected feed-forward network (FFN) layers function within the Vanilla Transformer architecture?
3. What role do Residual Connections play in the Vanilla Transformer's encoder and decoder blocks?
4. Why is there a debate between using post-normalization and pre-normalization in the context of Vanilla Transformer's architecture?
5. How might future research address the unresolved issue of post-normalization versus pre-normalization in Transformer models?",None,,"Questions:

1. What is the basic structure of the Vanilla Transformer in Transformer-based research?
2. How do the multi-head self-attention (MHSA) and position-wise fully-connected feed-forward network (FFN) layers function within the Vanilla Transformer architecture?
3. What role do Residual Connections play in the Vanilla Transformer's encoder and decoder blocks?
4. Why is there a debate between using post-normalization and pre-normalization in the context of Vanilla Transformer's architecture?
5. How might future research address the unresolved issue of post-normalization versus pre-normalization in Transformer models?

Answer:

(p7.0) Vanilla Transformer has an encoder-decoder structure and is the origin of the Transformer-based research field. It takes tokenized input (see Section 3.1.1). Both its encoder and decoder are stacked by the Transformer layers/blocks, as demonstrated in Figure 1. Each block has two sub-layers, i.e., a multi-head self-attention (MHSA) layer (see Section 3.1.2) and a position-wise fully-connected feed-forward network (FFN) (see Section 3.1.3). To help the back propagation of the gradient, both MHSA and FFN use Residual Connection [159] (given an input x, the residual connection of any mapping f (·) is defined as x ← f (x) + x), followed by normalization layer. Thus, assuming that the input tensor is Z, the output of MHSA and FFN sub-layers can be formulated as:

(p7.1) where sublayer(·) is the mapping implemented by the sublayer itself and N (·) denotes normalization, e.g., BN (·) [160], LN (·) [161]. Discussion There is an important unsolved problem that is post-normalization versus pre-normalization. The original Vanilla Transformer uses post-normalization for each MHSA and FFN sub-layer. However, if we consider this from the mathematical perspective, pre-normalization makes more sense [162]. This is similar to the basic principle of the theory of matrix, that normalization should be performed before projection, e.g., Gram-Schmidt process 2 . This problem should be studied further by both theoretical research and experimental validation.","Questions:



Answer:

(p7.0) Vanilla Transformer has an encoder-decoder structure and is the origin of the Transformer-based research field. It takes tokenized input (see Section 3.1.1). Both its encoder and decoder are stacked by the Transformer layers/blocks, as demonstrated in Figure 1. Each block has two sub-layers, i.e., a multi-head self-attention (MHSA) layer (see Section 3.1.2) and a position-wise fully-connected feed-forward network (FFN) (see Section 3.1.3). To help the back propagation of the gradient, both MHSA and FFN use Residual Connection [159] (given an input x, the residual connection of any mapping f (·) is defined as x ← f (x) + x), followed by normalization layer. Thus, assuming that the input tensor is Z, the output of MHSA and FFN sub-layers can be formulated as:

(p7.1) where sublayer(·) is the mapping implemented by the sublayer itself and N (·) denotes normalization, e.g., BN (·) [160], LN (·) [161]. Discussion There is an important unsolved problem that is post-normalization versus pre-normalization. The original Vanilla Transformer uses post-normalization for each MHSA and FFN sub-layer. However, if we consider this from the mathematical perspective, pre-normalization makes more sense [162]. This is similar to the basic principle of the theory of matrix, that normalization should be performed before projection, e.g., Gram-Schmidt process 2 . This problem should be studied further by both theoretical research and experimental validation."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s10,Discussion How to understand position embedding to,['p10.0'],"['Transformers is an open problem. It can be understood as a kind of implicit coordinate basis of feature space, to provide temporal or spatial information to the Transformer. For cloud point [164] and sketch drawing stroke [163], their token element is already a coordinate, meaning that position embedding is optional, not necessary. Furthermore, position embedding can be regarded as a kind of general additional information. In other words, from a mathematical point of view, any additional information can be added, such as detail of the manner of position embedding, e.g., the pen state of sketch drawing stroke [163], cameras and viewpoints in surveillance [165]. There is a comprehensive survey [166] discussing the position information in Transformers. For both sentence structures (sequential) and general graph structures (sparse, arbitrary, and irregular), position embeddings help Transformers to learn or encode the underlying structures. Considered from the mathematical perspective of self-attention, i.e., scaled dot-product attention, attentions are invariant to the positions of words (in text) or nodes (in graphs), if position embedding information is missing. Thus, in most cases, position embedding is necessary for Transformers.']","Transformers is an open problem. It can be understood as a kind of implicit coordinate basis of feature space, to provide temporal or spatial information to the Transformer. For cloud point [164] and sketch drawing stroke [163], their token element is already a coordinate, meaning that position embedding is optional, not necessary. Furthermore, position embedding can be regarded as a kind of general additional information. In other words, from a mathematical point of view, any additional information can be added, such as detail of the manner of position embedding, e.g., the pen state of sketch drawing stroke [163], cameras and viewpoints in surveillance [165]. There is a comprehensive survey [166] discussing the position information in Transformers. For both sentence structures (sequential) and general graph structures (sparse, arbitrary, and irregular), position embeddings help Transformers to learn or encode the underlying structures. Considered from the mathematical perspective of self-attention, i.e., scaled dot-product attention, attentions are invariant to the positions of words (in text) or nodes (in graphs), if position embedding information is missing. Thus, in most cases, position embedding is necessary for Transformers.","(p10.0) Transformers is an open problem. It can be understood as a kind of implicit coordinate basis of feature space, to provide temporal or spatial information to the Transformer. For cloud point [164] and sketch drawing stroke [163], their token element is already a coordinate, meaning that position embedding is optional, not necessary. Furthermore, position embedding can be regarded as a kind of general additional information. In other words, from a mathematical point of view, any additional information can be added, such as detail of the manner of position embedding, e.g., the pen state of sketch drawing stroke [163], cameras and viewpoints in surveillance [165]. There is a comprehensive survey [166] discussing the position information in Transformers. For both sentence structures (sequential) and general graph structures (sparse, arbitrary, and irregular), position embeddings help Transformers to learn or encode the underlying structures. Considered from the mathematical perspective of self-attention, i.e., scaled dot-product attention, attentions are invariant to the positions of words (in text) or nodes (in graphs), if position embedding information is missing. Thus, in most cases, position embedding is necessary for Transformers.","[['b165', 'b164', 'b162', 'b163']]","[['b165', 'b164', 'b162', 'b163']]",4,"1. Transformers is an open problem.
2. It can be understood as a kind of implicit coordinate basis of feature space, to provide temporal or spatial information to the Transformer.
3. For cloud point [164] and sketch drawing stroke [163], their token element is already a coordinate, meaning that position embedding is optional, not necessary.
4. Furthermore, position embedding can be regarded as a kind of general additional information.
5. In other words, from a mathematical point of view, any additional information can be added, such as detail of the manner of position embedding, e.g., the pen state of sketch drawing stroke [163], cameras and viewpoints in surveillance [165].
6. There is a comprehensive survey [166] discussing the position information in Transformers.
7. For both sentence structures (sequential) and general graph structures (sparse, arbitrary, and irregular), position embeddings help Transformers to learn or encode the underlying structures.
8. Considered from the mathematical perspective of self-attention, i.e., scaled dot-product attention, attentions are invariant to the positions of words (in text) or nodes (in graphs), if position embedding information is missing.
9. Thus, in most cases, position embedding is necessary for Transformers.","How does position embedding contribute to the functionality of Transformers in processing temporal or spatial information?
What are the implications of token elements being coordinates themselves, such as in cloud point and sketch drawing stroke, on the necessity of position embedding in Transformers?
In what ways can position embedding be considered as general additional information in the context of Transformers, and what are some examples of this?
How do position embeddings assist Transformers in learning or encoding structures in both sequential and graph-based data?
Why is position embedding considered necessary for maintaining the invariance of attentions in the self-attention mechanism of Transformers?","1. How does position embedding contribute to the functionality of Transformers in processing temporal or spatial information?
2. What are the implications of token elements being coordinates themselves, such as in cloud point and sketch drawing stroke, on the necessity of position embedding in Transformers?
3. In what ways can position embedding be considered as general additional information in the context of Transformers, and what are some examples of this?
4. How do position embeddings assist Transformers in learning or encoding structures in both sequential and graph-based data?
5. Why is position embedding considered necessary for maintaining the invariance of attentions in the self-attention mechanism of Transformers?",None,,"Questions:

1. How does position embedding contribute to the functionality of Transformers in processing temporal or spatial information?
2. What are the implications of token elements being coordinates themselves, such as in cloud point and sketch drawing stroke, on the necessity of position embedding in Transformers?
3. In what ways can position embedding be considered as general additional information in the context of Transformers, and what are some examples of this?
4. How do position embeddings assist Transformers in learning or encoding structures in both sequential and graph-based data?
5. Why is position embedding considered necessary for maintaining the invariance of attentions in the self-attention mechanism of Transformers?

Answer:

(p10.0) Transformers is an open problem. It can be understood as a kind of implicit coordinate basis of feature space, to provide temporal or spatial information to the Transformer. For cloud point [164] and sketch drawing stroke [163], their token element is already a coordinate, meaning that position embedding is optional, not necessary. Furthermore, position embedding can be regarded as a kind of general additional information. In other words, from a mathematical point of view, any additional information can be added, such as detail of the manner of position embedding, e.g., the pen state of sketch drawing stroke [163], cameras and viewpoints in surveillance [165]. There is a comprehensive survey [166] discussing the position information in Transformers. For both sentence structures (sequential) and general graph structures (sparse, arbitrary, and irregular), position embeddings help Transformers to learn or encode the underlying structures. Considered from the mathematical perspective of self-attention, i.e., scaled dot-product attention, attentions are invariant to the positions of words (in text) or nodes (in graphs), if position embedding information is missing. Thus, in most cases, position embedding is necessary for Transformers.","Questions:



Answer:

(p10.0) Transformers is an open problem. It can be understood as a kind of implicit coordinate basis of feature space, to provide temporal or spatial information to the Transformer. For cloud point [164] and sketch drawing stroke [163], their token element is already a coordinate, meaning that position embedding is optional, not necessary. Furthermore, position embedding can be regarded as a kind of general additional information. In other words, from a mathematical point of view, any additional information can be added, such as detail of the manner of position embedding, e.g., the pen state of sketch drawing stroke [163], cameras and viewpoints in surveillance [165]. There is a comprehensive survey [166] discussing the position information in Transformers. For both sentence structures (sequential) and general graph structures (sparse, arbitrary, and irregular), position embeddings help Transformers to learn or encode the underlying structures. Considered from the mathematical perspective of self-attention, i.e., scaled dot-product attention, attentions are invariant to the positions of words (in text) or nodes (in graphs), if position embedding information is missing. Thus, in most cases, position embedding is necessary for Transformers."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s12,Self-Attention (SA),"['p12.0', 'p12.1', 'p12.2', 'p12.3', 'p12.4', 'p12.5']","['After preprocessing, embedding Z will go through three projection matrices (W Q ∈ R d×dq , W K ∈ R d×d k , and W V ∈ R d×dv , d q = d k ) to generate three embeddings Q (Query), K (Key), and V (Value):', 'The output of self-attention is defined as', 'Given an input sequence, self-attention allows each element to attend to all the other elements, so that self-attention encodes the input as a fully-connected graph. Therefore, the encoder of Vanilla Transformer can be regarded as a fullyconnected GNN encoder, and the Transformer family has the non-local ability of global perception, similar to the Non-Local Network [167].', 'Masked Self-Attention (MSA) In practice, modification of self-attention is needed to help the decoder of Transformer to learn contextual dependence, to prevent positions from attending to subsequent positions, as', 'where M is a masking matrix. For instance, in GPT [88], an upper triangular mask to enable look-ahead attention where each token can only look at the past tokens. Masking can be used in both encoder [163], [168] and decoder of Transformer, and has flexible implementations, e.g., 0-1 hard mask [163], soft mask [168].', 'In both uni-modal and multimodal practices, specific masks are designed based on domain knowledge and prior knowledge. Essentially, MSA is used to inject additional knowledge to Transformer models, e.g., [24], [163], [169], [170].']","After preprocessing, embedding Z will go through three projection matrices (W Q ∈ R d×dq , W K ∈ R d×d k , and W V ∈ R d×dv , d q = d k ) to generate three embeddings Q (Query), K (Key), and V (Value):

The output of self-attention is defined as

Given an input sequence, self-attention allows each element to attend to all the other elements, so that self-attention encodes the input as a fully-connected graph. Therefore, the encoder of Vanilla Transformer can be regarded as a fullyconnected GNN encoder, and the Transformer family has the non-local ability of global perception, similar to the Non-Local Network [167].

Masked Self-Attention (MSA) In practice, modification of self-attention is needed to help the decoder of Transformer to learn contextual dependence, to prevent positions from attending to subsequent positions, as

where M is a masking matrix. For instance, in GPT [88], an upper triangular mask to enable look-ahead attention where each token can only look at the past tokens. Masking can be used in both encoder [163], [168] and decoder of Transformer, and has flexible implementations, e.g., 0-1 hard mask [163], soft mask [168].

In both uni-modal and multimodal practices, specific masks are designed based on domain knowledge and prior knowledge. Essentially, MSA is used to inject additional knowledge to Transformer models, e.g., [24], [163], [169], [170].","(p12.0) After preprocessing, embedding Z will go through three projection matrices (W Q ∈ R d×dq , W K ∈ R d×d k , and W V ∈ R d×dv , d q = d k ) to generate three embeddings Q (Query), K (Key), and V (Value):

(p12.1) The output of self-attention is defined as

(p12.2) Given an input sequence, self-attention allows each element to attend to all the other elements, so that self-attention encodes the input as a fully-connected graph. Therefore, the encoder of Vanilla Transformer can be regarded as a fullyconnected GNN encoder, and the Transformer family has the non-local ability of global perception, similar to the Non-Local Network [167].

(p12.3) Masked Self-Attention (MSA) In practice, modification of self-attention is needed to help the decoder of Transformer to learn contextual dependence, to prevent positions from attending to subsequent positions, as

(p12.4) where M is a masking matrix. For instance, in GPT [88], an upper triangular mask to enable look-ahead attention where each token can only look at the past tokens. Masking can be used in both encoder [163], [168] and decoder of Transformer, and has flexible implementations, e.g., 0-1 hard mask [163], soft mask [168].

(p12.5) In both uni-modal and multimodal practices, specific masks are designed based on domain knowledge and prior knowledge. Essentially, MSA is used to inject additional knowledge to Transformer models, e.g., [24], [163], [169], [170].","[[], [], ['b166'], [], ['b162', 'b87', 'b167'], ['b23', 'b169', 'b168', 'b162']]","[[], [], ['b166'], [], ['b162', 'b87', 'b167'], ['b23', 'b169', 'b168', 'b162']]",8,"1. After preprocessing, embedding Z will go through three projection matrices (W Q ∈ R d×dq , W K ∈ R d×d k , and W V ∈ R d×dv , d q = d k ) to generate three embeddings Q (Query), K (Key), and V (Value):
2. The output of self-attention is defined asGiven an input sequence, self-attention allows each element to attend to all the other elements, so that self-attention encodes the input as a fully-connected graph.
3. Therefore, the encoder of Vanilla Transformer can be regarded as a fullyconnected GNN encoder, and the Transformer family has the non-local ability of global perception, similar to the Non-Local Network [167].
4. Masked Self-Attention (MSA) In practice, modification of self-attention is needed to help the decoder of Transformer to learn contextual dependence, to prevent positions from attending to subsequent positions, aswhere M is a masking matrix.
5. For instance, in GPT [88], an upper triangular mask to enable look-ahead attention where each token can only look at the past tokens.
6. Masking can be used in both encoder (Query)2, (Query)0 and decoder of Transformer, and has flexible implementations, e.g., 0-1 hard mask (Query)2, soft mask (Query)0.
7. In both uni-modal and multimodal practices, specific masks are designed based on domain knowledge and prior knowledge.
8. Essentially, MSA is used to inject additional knowledge to Transformer models, e.g., (Query)1, (Query)2, (Query)3, (Query)4.","How does self-attention in Transformers facilitate the encoding of input sequences as fully-connected graphs?
What role do the projection matrices W Q, W K, and W V play in generating the embeddings Q, K, and V in the context of self-attention?
In what way does masked self-attention (MSA) modify the decoder's ability to learn contextual dependence in Transformer models?
How do specific masks in MSA implementations leverage domain and prior knowledge to enhance Transformer models?
Can you describe the differences in the application of masking techniques between the encoder and decoder phases of Transformer models?","1. How does self-attention in Transformers facilitate the encoding of input sequences as fully-connected graphs?
2. What role do the projection matrices W Q, W K, and W V play in generating the embeddings Q, K, and V in the context of self-attention?
3. In what way does masked self-attention (MSA) modify the decoder's ability to learn contextual dependence in Transformer models?
4. How do specific masks in MSA implementations leverage domain and prior knowledge to enhance Transformer models?
5. Can you describe the differences in the application of masking techniques between the encoder and decoder phases of Transformer models?",None,,"Questions:

1. How does self-attention in Transformers facilitate the encoding of input sequences as fully-connected graphs?
2. What role do the projection matrices W Q, W K, and W V play in generating the embeddings Q, K, and V in the context of self-attention?
3. In what way does masked self-attention (MSA) modify the decoder's ability to learn contextual dependence in Transformer models?
4. How do specific masks in MSA implementations leverage domain and prior knowledge to enhance Transformer models?
5. Can you describe the differences in the application of masking techniques between the encoder and decoder phases of Transformer models?

Answer:

(p12.0) After preprocessing, embedding Z will go through three projection matrices (W Q ∈ R d×dq , W K ∈ R d×d k , and W V ∈ R d×dv , d q = d k ) to generate three embeddings Q (Query), K (Key), and V (Value):

(p12.1) The output of self-attention is defined as

(p12.2) Given an input sequence, self-attention allows each element to attend to all the other elements, so that self-attention encodes the input as a fully-connected graph. Therefore, the encoder of Vanilla Transformer can be regarded as a fullyconnected GNN encoder, and the Transformer family has the non-local ability of global perception, similar to the Non-Local Network [167].

(p12.3) Masked Self-Attention (MSA) In practice, modification of self-attention is needed to help the decoder of Transformer to learn contextual dependence, to prevent positions from attending to subsequent positions, as

(p12.4) where M is a masking matrix. For instance, in GPT [88], an upper triangular mask to enable look-ahead attention where each token can only look at the past tokens. Masking can be used in both encoder [163], [168] and decoder of Transformer, and has flexible implementations, e.g., 0-1 hard mask [163], soft mask [168].

(p12.5) In both uni-modal and multimodal practices, specific masks are designed based on domain knowledge and prior knowledge. Essentially, MSA is used to inject additional knowledge to Transformer models, e.g., [24], [163], [169], [170].","Questions:



Answer:

(p12.0) After preprocessing, embedding Z will go through three projection matrices (W Q ∈ R d×dq , W K ∈ R d×d k , and W V ∈ R d×dv , d q = d k ) to generate three embeddings Q (Query), K (Key), and V (Value):

(p12.1) The output of self-attention is defined as

(p12.2) Given an input sequence, self-attention allows each element to attend to all the other elements, so that self-attention encodes the input as a fully-connected graph. Therefore, the encoder of Vanilla Transformer can be regarded as a fullyconnected GNN encoder, and the Transformer family has the non-local ability of global perception, similar to the Non-Local Network [167].

(p12.3) Masked Self-Attention (MSA) In practice, modification of self-attention is needed to help the decoder of Transformer to learn contextual dependence, to prevent positions from attending to subsequent positions, as

(p12.4) where M is a masking matrix. For instance, in GPT [88], an upper triangular mask to enable look-ahead attention where each token can only look at the past tokens. Masking can be used in both encoder [163], [168] and decoder of Transformer, and has flexible implementations, e.g., 0-1 hard mask [163], soft mask [168].

(p12.5) In both uni-modal and multimodal practices, specific masks are designed based on domain knowledge and prior knowledge. Essentially, MSA is used to inject additional knowledge to Transformer models, e.g., [24], [163], [169], [170]."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s18,Tokenization and Embedding Processing,"['p18.0', 'p18.1']","['Given an input from an arbitrary modality, users only need to perform two main steps, (1) tokenize the input, and (2) select an embedding space to represent the tokens, before inputting the data into Transformers. In practice, both the tokenizing input and selecting embedding for the token are vital for Transformers but highly flexible, with many alternatives. For instance, given an image, the solution of tokenizing and embedding is not unique. Users can choose or design tokenization at multiple granularity levels -coarse-grained vs. fine-grained. e.g., use ROIs (obtained by an object detector) and CNN features as tokens and token embeddings [102], use patches and linear projection as tokens and token embeddings [5], or use graph node (obtained by object detector and graph generator) and GNN features as tokens and token embeddings [181]. Given a tokenization plan, the subsequent embedding approaches can be diverse. For example, for video input, a common tokenization is to treat the non-overlapping windows (down-sampled) over the video as tokens, and their embeddings can then be extracted by various 3D CNNs, e.g., VideoBERT [7], CBT [107], and UniVL [117] use S3D [186], ActBERT uses ResNet-3D [187]. Table 1 summarizes some common practices of multimodal inputs for Transformers, including RGB, video, audio/speech/music, text, graph, etc.', 'Discussion When considered from the perspective of geometric topology, each of the modalities listed in Table 1 can be regarded as a graph. An RGB image is essentially a neat grid graph in the pixel space. Both video and audio are clip/segment based graphs over a complex space involving temporal and semantic patterns. Both 2D and 3D drawing sketches [78], [163] are a kind of sparse graph if we consider their key points along the drawing strokes. Similar to sketches, the human pose also is a kind of graph. 3D point cloud is a graph in which each coordinate is a node. Other abstract modalities also can be interpreted as graphs, e.g., source code [44], data flow of source code [44], table [18], SQL database schema [25], text question graph [24], and electronic health records (EHRs) [184].']","Given an input from an arbitrary modality, users only need to perform two main steps, (1) tokenize the input, and (2) select an embedding space to represent the tokens, before inputting the data into Transformers. In practice, both the tokenizing input and selecting embedding for the token are vital for Transformers but highly flexible, with many alternatives. For instance, given an image, the solution of tokenizing and embedding is not unique. Users can choose or design tokenization at multiple granularity levels -coarse-grained vs. fine-grained. e.g., use ROIs (obtained by an object detector) and CNN features as tokens and token embeddings [102], use patches and linear projection as tokens and token embeddings [5], or use graph node (obtained by object detector and graph generator) and GNN features as tokens and token embeddings [181]. Given a tokenization plan, the subsequent embedding approaches can be diverse. For example, for video input, a common tokenization is to treat the non-overlapping windows (down-sampled) over the video as tokens, and their embeddings can then be extracted by various 3D CNNs, e.g., VideoBERT [7], CBT [107], and UniVL [117] use S3D [186], ActBERT uses ResNet-3D [187]. Table 1 summarizes some common practices of multimodal inputs for Transformers, including RGB, video, audio/speech/music, text, graph, etc.

Discussion When considered from the perspective of geometric topology, each of the modalities listed in Table 1 can be regarded as a graph. An RGB image is essentially a neat grid graph in the pixel space. Both video and audio are clip/segment based graphs over a complex space involving temporal and semantic patterns. Both 2D and 3D drawing sketches [78], [163] are a kind of sparse graph if we consider their key points along the drawing strokes. Similar to sketches, the human pose also is a kind of graph. 3D point cloud is a graph in which each coordinate is a node. Other abstract modalities also can be interpreted as graphs, e.g., source code [44], data flow of source code [44], table [18], SQL database schema [25], text question graph [24], and electronic health records (EHRs) [184].","(p18.0) Given an input from an arbitrary modality, users only need to perform two main steps, (1) tokenize the input, and (2) select an embedding space to represent the tokens, before inputting the data into Transformers. In practice, both the tokenizing input and selecting embedding for the token are vital for Transformers but highly flexible, with many alternatives. For instance, given an image, the solution of tokenizing and embedding is not unique. Users can choose or design tokenization at multiple granularity levels -coarse-grained vs. fine-grained. e.g., use ROIs (obtained by an object detector) and CNN features as tokens and token embeddings [102], use patches and linear projection as tokens and token embeddings [5], or use graph node (obtained by object detector and graph generator) and GNN features as tokens and token embeddings [181]. Given a tokenization plan, the subsequent embedding approaches can be diverse. For example, for video input, a common tokenization is to treat the non-overlapping windows (down-sampled) over the video as tokens, and their embeddings can then be extracted by various 3D CNNs, e.g., VideoBERT [7], CBT [107], and UniVL [117] use S3D [186], ActBERT uses ResNet-3D [187]. Table 1 summarizes some common practices of multimodal inputs for Transformers, including RGB, video, audio/speech/music, text, graph, etc.

(p18.1) Discussion When considered from the perspective of geometric topology, each of the modalities listed in Table 1 can be regarded as a graph. An RGB image is essentially a neat grid graph in the pixel space. Both video and audio are clip/segment based graphs over a complex space involving temporal and semantic patterns. Both 2D and 3D drawing sketches [78], [163] are a kind of sparse graph if we consider their key points along the drawing strokes. Similar to sketches, the human pose also is a kind of graph. 3D point cloud is a graph in which each coordinate is a node. Other abstract modalities also can be interpreted as graphs, e.g., source code [44], data flow of source code [44], table [18], SQL database schema [25], text question graph [24], and electronic health records (EHRs) [184].","[['b4', 'b6', 'b185', 'b116', 'b106', 'b180', 'b186'], ['b17', 'b77', 'b24', 'b23', 'b43', 'b162', 'b183']]","[['b4', 'b6', 'b185', 'b116', 'b106', 'b180', 'b186'], ['b17', 'b77', 'b24', 'b23', 'b43', 'b162', 'b183']]",14,"1. Given an input from an arbitrary modality, users only need to perform two main steps, (1) tokenize the input, and (2) select an embedding space to represent the tokens, before inputting the data into Transformers.
2. In practice, both the tokenizing input and selecting embedding for the token are vital for Transformers but highly flexible, with many alternatives.
3. For instance, given an image, the solution of tokenizing and embedding is not unique.
4. Users can choose or design tokenization at multiple granularity levels -coarse-grained vs. fine-grained.
5. e.g., use ROIs (obtained by an object detector) and CNN features as tokens and token embeddings [102], use patches and linear projection as tokens and token embeddings [5], or use graph node (obtained by object detector and graph generator) and GNN features as tokens and token embeddings [181].
6. Given a tokenization plan, the subsequent embedding approaches can be diverse.
7. For example, for video input, a common tokenization is to treat the non-overlapping windows (down-sampled) over the video as tokens, and their embeddings can then be extracted by various 3D CNNs, e.g., VideoBERT [7], CBT [107], and UniVL (2)0 use S3D (2)1, ActBERT uses ResNet-3D (2)2.
8. Table 1 summarizes some common practices of multimodal inputs for Transformers, including RGB, video, audio/speech/music, text, graph, etc.Discussion When considered from the perspective of geometric topology, each of the modalities listed in Table 1 can be regarded as a graph.
9. An RGB image is essentially a neat grid graph in the pixel space.
10. Both video and audio are clip/segment based graphs over a complex space involving temporal and semantic patterns.
11. Both 2D and 3D drawing sketches (2)3, (2)4 are a kind of sparse graph if we consider their key points along the drawing strokes.
12. Similar to sketches, the human pose also is a kind of graph.
13. 3D point cloud is a graph in which each coordinate is a node.
14. Other abstract modalities also can be interpreted as graphs, e.g., source code (2)6, data flow of source code (2)6, table (2)7, SQL database schema (2)8, text question graph (2)9, and electronic health records (obtained by an object detector)0 (obtained by an object detector)1.","What are the two main steps required for processing input data before using Transformers in multimodal learning?
How does the granularity of tokenization affect the representation of images in Transformers?
Can you describe the diversity in embedding approaches for video inputs in the context of Transformers?
How do different modalities, such as RGB images or audio, translate into graph representations in geometric topology according to the survey?
What are some examples of abstract modalities that can be interpreted as graphs in the context of Transformers, and how are they represented?","1. What are the two main steps required for processing input data before using Transformers in multimodal learning?
2. How does the granularity of tokenization affect the representation of images in Transformers?
3. Can you describe the diversity in embedding approaches for video inputs in the context of Transformers?
4. How do different modalities, such as RGB images or audio, translate into graph representations in geometric topology according to the survey?
5. What are some examples of abstract modalities that can be interpreted as graphs in the context of Transformers, and how are they represented?",None,,"Questions:

1. What are the two main steps required for processing input data before using Transformers in multimodal learning?
2. How does the granularity of tokenization affect the representation of images in Transformers?
3. Can you describe the diversity in embedding approaches for video inputs in the context of Transformers?
4. How do different modalities, such as RGB images or audio, translate into graph representations in geometric topology according to the survey?
5. What are some examples of abstract modalities that can be interpreted as graphs in the context of Transformers, and how are they represented?

Answer:

(p18.0) Given an input from an arbitrary modality, users only need to perform two main steps, (1) tokenize the input, and (2) select an embedding space to represent the tokens, before inputting the data into Transformers. In practice, both the tokenizing input and selecting embedding for the token are vital for Transformers but highly flexible, with many alternatives. For instance, given an image, the solution of tokenizing and embedding is not unique. Users can choose or design tokenization at multiple granularity levels -coarse-grained vs. fine-grained. e.g., use ROIs (obtained by an object detector) and CNN features as tokens and token embeddings [102], use patches and linear projection as tokens and token embeddings [5], or use graph node (obtained by object detector and graph generator) and GNN features as tokens and token embeddings [181]. Given a tokenization plan, the subsequent embedding approaches can be diverse. For example, for video input, a common tokenization is to treat the non-overlapping windows (down-sampled) over the video as tokens, and their embeddings can then be extracted by various 3D CNNs, e.g., VideoBERT [7], CBT [107], and UniVL [117] use S3D [186], ActBERT uses ResNet-3D [187]. Table 1 summarizes some common practices of multimodal inputs for Transformers, including RGB, video, audio/speech/music, text, graph, etc.

(p18.1) Discussion When considered from the perspective of geometric topology, each of the modalities listed in Table 1 can be regarded as a graph. An RGB image is essentially a neat grid graph in the pixel space. Both video and audio are clip/segment based graphs over a complex space involving temporal and semantic patterns. Both 2D and 3D drawing sketches [78], [163] are a kind of sparse graph if we consider their key points along the drawing strokes. Similar to sketches, the human pose also is a kind of graph. 3D point cloud is a graph in which each coordinate is a node. Other abstract modalities also can be interpreted as graphs, e.g., source code [44], data flow of source code [44], table [18], SQL database schema [25], text question graph [24], and electronic health records (EHRs) [184].","Questions:



Answer:

(p18.0) Given an input from an arbitrary modality, users only need to perform two main steps, (1) tokenize the input, and (2) select an embedding space to represent the tokens, before inputting the data into Transformers. In practice, both the tokenizing input and selecting embedding for the token are vital for Transformers but highly flexible, with many alternatives. For instance, given an image, the solution of tokenizing and embedding is not unique. Users can choose or design tokenization at multiple granularity levels -coarse-grained vs. fine-grained. e.g., use ROIs (obtained by an object detector) and CNN features as tokens and token embeddings [102], use patches and linear projection as tokens and token embeddings [5], or use graph node (obtained by object detector and graph generator) and GNN features as tokens and token embeddings [181]. Given a tokenization plan, the subsequent embedding approaches can be diverse. For example, for video input, a common tokenization is to treat the non-overlapping windows (down-sampled) over the video as tokens, and their embeddings can then be extracted by various 3D CNNs, e.g., VideoBERT [7], CBT [107], and UniVL [117] use S3D [186], ActBERT uses ResNet-3D [187]. Table 1 summarizes some common practices of multimodal inputs for Transformers, including RGB, video, audio/speech/music, text, graph, etc.

(p18.1) Discussion When considered from the perspective of geometric topology, each of the modalities listed in Table 1 can be regarded as a graph. An RGB image is essentially a neat grid graph in the pixel space. Both video and audio are clip/segment based graphs over a complex space involving temporal and semantic patterns. Both 2D and 3D drawing sketches [78], [163] are a kind of sparse graph if we consider their key points along the drawing strokes. Similar to sketches, the human pose also is a kind of graph. 3D point cloud is a graph in which each coordinate is a node. Other abstract modalities also can be interpreted as graphs, e.g., source code [44], data flow of source code [44], table [18], SQL database schema [25], text question graph [24], and electronic health records (EHRs) [184]."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s19,Token Embedding Fusion,"['p19.0', 'p19.1']","['In practice, Transformers allow each token position to contain multiple embeddings. This is essentially a kind of early-fusion of embeddings, for both uni-modal and multimodal Transformer models.', '(This will be discussed further in subsequent sections.) The most common fusion is the token-wise summing of the multiple embeddings, e.g., a specific token embedding ⊕ position embedding. Similar to the flexible tokenization, token embedding fusion is also flexible and widely applied to both uni-modal and multimodal Transformer applications. In [81], token-wise weighted summing is used to perform early-fusion of RGB and grey-scale images for multimodal surveillance AI. In particular, token embedding fusion has an important role in multimodal Transformer applications as various embeddings can be fused by tokenwise operators, e.g., in VisualBERT [104] and Unicoder-VL [108], segment embeddings are token-wise added to indicate which modality (vision or language) each token is from, VL-BERT [105] injects global visual context to linguistic domain by ""linguistic token embedding ⊕ full image visual feature embedding"", InterBERT [188] adds location information for ROI by ""ROI embedding ⊕ location embedding"", in Im-ageBERT [115], five kinds of embeddings are fused ""image embedding ⊕ position embedding ⊕ linguistic embedding ⊕ segment embedding ⊕ sequence position embedding"".']","In practice, Transformers allow each token position to contain multiple embeddings. This is essentially a kind of early-fusion of embeddings, for both uni-modal and multimodal Transformer models.

(This will be discussed further in subsequent sections.) The most common fusion is the token-wise summing of the multiple embeddings, e.g., a specific token embedding ⊕ position embedding. Similar to the flexible tokenization, token embedding fusion is also flexible and widely applied to both uni-modal and multimodal Transformer applications. In [81], token-wise weighted summing is used to perform early-fusion of RGB and grey-scale images for multimodal surveillance AI. In particular, token embedding fusion has an important role in multimodal Transformer applications as various embeddings can be fused by tokenwise operators, e.g., in VisualBERT [104] and Unicoder-VL [108], segment embeddings are token-wise added to indicate which modality (vision or language) each token is from, VL-BERT [105] injects global visual context to linguistic domain by ""linguistic token embedding ⊕ full image visual feature embedding"", InterBERT [188] adds location information for ROI by ""ROI embedding ⊕ location embedding"", in Im-ageBERT [115], five kinds of embeddings are fused ""image embedding ⊕ position embedding ⊕ linguistic embedding ⊕ segment embedding ⊕ sequence position embedding"".","(p19.0) In practice, Transformers allow each token position to contain multiple embeddings. This is essentially a kind of early-fusion of embeddings, for both uni-modal and multimodal Transformer models.

(p19.1) (This will be discussed further in subsequent sections.) The most common fusion is the token-wise summing of the multiple embeddings, e.g., a specific token embedding ⊕ position embedding. Similar to the flexible tokenization, token embedding fusion is also flexible and widely applied to both uni-modal and multimodal Transformer applications. In [81], token-wise weighted summing is used to perform early-fusion of RGB and grey-scale images for multimodal surveillance AI. In particular, token embedding fusion has an important role in multimodal Transformer applications as various embeddings can be fused by tokenwise operators, e.g., in VisualBERT [104] and Unicoder-VL [108], segment embeddings are token-wise added to indicate which modality (vision or language) each token is from, VL-BERT [105] injects global visual context to linguistic domain by ""linguistic token embedding ⊕ full image visual feature embedding"", InterBERT [188] adds location information for ROI by ""ROI embedding ⊕ location embedding"", in Im-ageBERT [115], five kinds of embeddings are fused ""image embedding ⊕ position embedding ⊕ linguistic embedding ⊕ segment embedding ⊕ sequence position embedding"".","[[], ['b114', 'b80', 'b107', 'b103', 'b187', 'b104']]","[[], ['b114', 'b80', 'b107', 'b103', 'b187', 'b104']]",6,"1. In practice, Transformers allow each token position to contain multiple embeddings.
2. This is essentially a kind of early-fusion of embeddings, for both uni-modal and multimodal Transformer models.
3. (This will be discussed further in subsequent sections.)
4. The most common fusion is the token-wise summing of the multiple embeddings, e.g., a specific token embedding ⊕ position embedding.
5. Similar to the flexible tokenization, token embedding fusion is also flexible and widely applied to both uni-modal and multimodal Transformer applications.
6. In [81], token-wise weighted summing is used to perform early-fusion of RGB and grey-scale images for multimodal surveillance AI.
7. In particular, token embedding fusion has an important role in multimodal Transformer applications as various embeddings can be fused by tokenwise operators, e.g., in VisualBERT [104] and Unicoder-VL [108], segment embeddings are token-wise added to indicate which modality (vision or language) each token is from, VL-BERT [105] injects global visual context to linguistic domain by ""linguistic token embedding ⊕ full image visual feature embedding"", InterBERT [188] adds location information for ROI by ""ROI embedding ⊕ location embedding"", in Im-ageBERT [115], five kinds of embeddings are fused ""image embedding ⊕ position embedding ⊕ linguistic embedding ⊕ segment embedding ⊕ sequence position embedding"".","What are the common methods for embedding fusion in multimodal Transformer models?
How does token embedding fusion contribute to the flexibility of Transformer applications across uni-modal and multimodal contexts?
In what way does token-wise weighted summing enhance the performance of multimodal surveillance AI systems?
How do different multimodal Transformer models like VisualBERT and VL-BERT utilize token embedding fusion to integrate visual and linguistic information?
What role does embedding fusion play in incorporating global visual context and location information into linguistic domains in Transformer models?","1. What are the common methods for embedding fusion in multimodal Transformer models?
2. How does token embedding fusion contribute to the flexibility of Transformer applications across uni-modal and multimodal contexts?
3. In what way does token-wise weighted summing enhance the performance of multimodal surveillance AI systems?
4. How do different multimodal Transformer models like VisualBERT and VL-BERT utilize token embedding fusion to integrate visual and linguistic information?
5. What role does embedding fusion play in incorporating global visual context and location information into linguistic domains in Transformer models?",None,,"Questions:

1. What are the common methods for embedding fusion in multimodal Transformer models?
2. How does token embedding fusion contribute to the flexibility of Transformer applications across uni-modal and multimodal contexts?
3. In what way does token-wise weighted summing enhance the performance of multimodal surveillance AI systems?
4. How do different multimodal Transformer models like VisualBERT and VL-BERT utilize token embedding fusion to integrate visual and linguistic information?
5. What role does embedding fusion play in incorporating global visual context and location information into linguistic domains in Transformer models?

Answer:

(p19.0) In practice, Transformers allow each token position to contain multiple embeddings. This is essentially a kind of early-fusion of embeddings, for both uni-modal and multimodal Transformer models.

(p19.1) (This will be discussed further in subsequent sections.) The most common fusion is the token-wise summing of the multiple embeddings, e.g., a specific token embedding ⊕ position embedding. Similar to the flexible tokenization, token embedding fusion is also flexible and widely applied to both uni-modal and multimodal Transformer applications. In [81], token-wise weighted summing is used to perform early-fusion of RGB and grey-scale images for multimodal surveillance AI. In particular, token embedding fusion has an important role in multimodal Transformer applications as various embeddings can be fused by tokenwise operators, e.g., in VisualBERT [104] and Unicoder-VL [108], segment embeddings are token-wise added to indicate which modality (vision or language) each token is from, VL-BERT [105] injects global visual context to linguistic domain by ""linguistic token embedding ⊕ full image visual feature embedding"", InterBERT [188] adds location information for ROI by ""ROI embedding ⊕ location embedding"", in Im-ageBERT [115], five kinds of embeddings are fused ""image embedding ⊕ position embedding ⊕ linguistic embedding ⊕ segment embedding ⊕ sequence position embedding"".","Questions:



Answer:

(p19.0) In practice, Transformers allow each token position to contain multiple embeddings. This is essentially a kind of early-fusion of embeddings, for both uni-modal and multimodal Transformer models.

(p19.1) (This will be discussed further in subsequent sections.) The most common fusion is the token-wise summing of the multiple embeddings, e.g., a specific token embedding ⊕ position embedding. Similar to the flexible tokenization, token embedding fusion is also flexible and widely applied to both uni-modal and multimodal Transformer applications. In [81], token-wise weighted summing is used to perform early-fusion of RGB and grey-scale images for multimodal surveillance AI. In particular, token embedding fusion has an important role in multimodal Transformer applications as various embeddings can be fused by tokenwise operators, e.g., in VisualBERT [104] and Unicoder-VL [108], segment embeddings are token-wise added to indicate which modality (vision or language) each token is from, VL-BERT [105] injects global visual context to linguistic domain by ""linguistic token embedding ⊕ full image visual feature embedding"", InterBERT [188] adds location information for ROI by ""ROI embedding ⊕ location embedding"", in Im-ageBERT [115], five kinds of embeddings are fused ""image embedding ⊕ position embedding ⊕ linguistic embedding ⊕ segment embedding ⊕ sequence position embedding""."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s21,Self-Attention Definitions Streams Formulations Complexities References,"['p21.0', 'p21.1', 'p21.2', 'p21.3', 'p21.4', 'p21.5', 'p21.6', 'p21.7', 'p21.8', 'p21.9', 'p21.10', 'p21.11', 'p21.12', 'p21.13', 'p21.14', 'p21.15', 'p21.16']","['Early Summation token-wise sum before Tfs 1', 'token sequence concat. before Tfs 1', ', [44], [178], [180] Hierarchical Att.', '2-stream Tfs followed by concat.', 'Hierarchical Att. early concat. followed by 2-stream Tfs', 'Cross-Attention exchange query 2', ', [144] Cross-Att. to Con. 2-stream cross-att. followed by concat. [137], [189] cross-attention, and (6) cross-attention to concatenation. See Table 2 and Figure 2. For brevity, we will state and compare the mathematical formulations in two-modality cases. Please note that all discussed self-attention and its variants are such flexible that can be extended to multiple modality cases. Specifically, the following formulations are modality-, tokenization-, and embedding-agnostic, as self-attention models the embedding of arbitrary token from arbitrary modality as a node of a graph.', 'Given inputs X A and X B from two arbitrary modalities, Z (A) and Z (B) denote their respective token embeddings. Let Z denoting the token embedding (sequence) produced by the multimodal interactions. T f (·) stands for the processing of Transformer layers/blocks.', '(1) Early Summation In practice, early summation [46], [81] is a simple and effective multimodal interaction, where the token embeddings from multiple modalities can be weighted summed at each token position and then processed by Transformer layers:', 'where ⊕ is element-wise sum, and α and β are weightings. Concretely,', '. Its main advantage is that it does not increase computational complexity. However, its main disadvantage is due to the manually set weightings. As discussed in Section 3.1.1 and 3.3.1, summing position embedding is intrinsically a case of early summation.', '(2) Early Concatenation Another straightforward solution is early concatenation [7], [44], [178], [180] that the token embedding sequences from multiple modalities are concatenated and input into Transformer layers as', 'Thus, all the multimodal token positions can be attended as a whole sequence, such that the positions of each modality can be encoded well by conditioning the context of other modalities. VideoBERT [7] is the one of the first multimodal Transformer works, where video and text are fused via early concatenation that can encode the global multimodal context well [188]. However, the longer sequence after concatenation will increase computational complexity. Early concatenation is also termed ""all-attention"" or ""Co-Transformer"" [137].', '(3) Hierarchical Attention (multi-stream to one-stream) Transformer layers can be combined hierarchically to attend to the cross-modal interactions. A common practice is that multimodal inputs are encoded by independent Transformer streams and their outputs are concatenated and fused by another Transformer [146]:', 'This kind of hierarchical attention is an implementation of late interaction/fusion, and can be treated as a special case of early concatenation.', '(4) Hierarchical Attention (one-stream to multi-stream) In-terBERT [188] is another good practice of hierarchical attention where concatenated multimodal inputs are encoded by a shared single-stream Transformer that is followed by two separate Transformer streams. This flow can be formulated as', 'This method perceives the cross-modal interactions and meanwhile preserves the independence of uni-modal representation.']","Early Summation token-wise sum before Tfs 1

token sequence concat. before Tfs 1

, [44], [178], [180] Hierarchical Att.

2-stream Tfs followed by concat.

Hierarchical Att. early concat. followed by 2-stream Tfs

Cross-Attention exchange query 2

, [144] Cross-Att. to Con. 2-stream cross-att. followed by concat. [137], [189] cross-attention, and (6) cross-attention to concatenation. See Table 2 and Figure 2. For brevity, we will state and compare the mathematical formulations in two-modality cases. Please note that all discussed self-attention and its variants are such flexible that can be extended to multiple modality cases. Specifically, the following formulations are modality-, tokenization-, and embedding-agnostic, as self-attention models the embedding of arbitrary token from arbitrary modality as a node of a graph.

Given inputs X A and X B from two arbitrary modalities, Z (A) and Z (B) denote their respective token embeddings. Let Z denoting the token embedding (sequence) produced by the multimodal interactions. T f (·) stands for the processing of Transformer layers/blocks.

(1) Early Summation In practice, early summation [46], [81] is a simple and effective multimodal interaction, where the token embeddings from multiple modalities can be weighted summed at each token position and then processed by Transformer layers:

where ⊕ is element-wise sum, and α and β are weightings. Concretely,

. Its main advantage is that it does not increase computational complexity. However, its main disadvantage is due to the manually set weightings. As discussed in Section 3.1.1 and 3.3.1, summing position embedding is intrinsically a case of early summation.

(2) Early Concatenation Another straightforward solution is early concatenation [7], [44], [178], [180] that the token embedding sequences from multiple modalities are concatenated and input into Transformer layers as

Thus, all the multimodal token positions can be attended as a whole sequence, such that the positions of each modality can be encoded well by conditioning the context of other modalities. VideoBERT [7] is the one of the first multimodal Transformer works, where video and text are fused via early concatenation that can encode the global multimodal context well [188]. However, the longer sequence after concatenation will increase computational complexity. Early concatenation is also termed ""all-attention"" or ""Co-Transformer"" [137].

(3) Hierarchical Attention (multi-stream to one-stream) Transformer layers can be combined hierarchically to attend to the cross-modal interactions. A common practice is that multimodal inputs are encoded by independent Transformer streams and their outputs are concatenated and fused by another Transformer [146]:

This kind of hierarchical attention is an implementation of late interaction/fusion, and can be treated as a special case of early concatenation.

(4) Hierarchical Attention (one-stream to multi-stream) In-terBERT [188] is another good practice of hierarchical attention where concatenated multimodal inputs are encoded by a shared single-stream Transformer that is followed by two separate Transformer streams. This flow can be formulated as

This method perceives the cross-modal interactions and meanwhile preserves the independence of uni-modal representation.","(p21.0) Early Summation token-wise sum before Tfs 1

(p21.1) token sequence concat. before Tfs 1

(p21.2) , [44], [178], [180] Hierarchical Att.

(p21.3) 2-stream Tfs followed by concat.

(p21.4) Hierarchical Att. early concat. followed by 2-stream Tfs

(p21.5) Cross-Attention exchange query 2

(p21.6) , [144] Cross-Att. to Con. 2-stream cross-att. followed by concat. [137], [189] cross-attention, and (6) cross-attention to concatenation. See Table 2 and Figure 2. For brevity, we will state and compare the mathematical formulations in two-modality cases. Please note that all discussed self-attention and its variants are such flexible that can be extended to multiple modality cases. Specifically, the following formulations are modality-, tokenization-, and embedding-agnostic, as self-attention models the embedding of arbitrary token from arbitrary modality as a node of a graph.

(p21.7) Given inputs X A and X B from two arbitrary modalities, Z (A) and Z (B) denote their respective token embeddings. Let Z denoting the token embedding (sequence) produced by the multimodal interactions. T f (·) stands for the processing of Transformer layers/blocks.

(p21.8) (1) Early Summation In practice, early summation [46], [81] is a simple and effective multimodal interaction, where the token embeddings from multiple modalities can be weighted summed at each token position and then processed by Transformer layers:

(p21.9) where ⊕ is element-wise sum, and α and β are weightings. Concretely,

(p21.10) . Its main advantage is that it does not increase computational complexity. However, its main disadvantage is due to the manually set weightings. As discussed in Section 3.1.1 and 3.3.1, summing position embedding is intrinsically a case of early summation.

(p21.11) (2) Early Concatenation Another straightforward solution is early concatenation [7], [44], [178], [180] that the token embedding sequences from multiple modalities are concatenated and input into Transformer layers as

(p21.12) Thus, all the multimodal token positions can be attended as a whole sequence, such that the positions of each modality can be encoded well by conditioning the context of other modalities. VideoBERT [7] is the one of the first multimodal Transformer works, where video and text are fused via early concatenation that can encode the global multimodal context well [188]. However, the longer sequence after concatenation will increase computational complexity. Early concatenation is also termed ""all-attention"" or ""Co-Transformer"" [137].

(p21.13) (3) Hierarchical Attention (multi-stream to one-stream) Transformer layers can be combined hierarchically to attend to the cross-modal interactions. A common practice is that multimodal inputs are encoded by independent Transformer streams and their outputs are concatenated and fused by another Transformer [146]:

(p21.14) This kind of hierarchical attention is an implementation of late interaction/fusion, and can be treated as a special case of early concatenation.

(p21.15) (4) Hierarchical Attention (one-stream to multi-stream) In-terBERT [188] is another good practice of hierarchical attention where concatenated multimodal inputs are encoded by a shared single-stream Transformer that is followed by two separate Transformer streams. This flow can be formulated as

(p21.16) This method perceives the cross-modal interactions and meanwhile preserves the independence of uni-modal representation.","[[], [], ['b177', 'b179', 'b43'], [], [], [], ['b136', 'b188', 'b143'], [], ['b80', 'b45'], [], [], ['b177', 'b179', 'b43', 'b6'], ['b136', 'b187', 'b6'], ['b145'], [], ['b187'], []]","[[], [], ['b177', 'b179', 'b43'], [], [], [], ['b136', 'b188', 'b143'], [], ['b80', 'b45'], [], [], ['b177', 'b179', 'b43', 'b6'], ['b136', 'b187', 'b6'], ['b145'], [], ['b187'], []]",17,"1. Early Summation token-wise sum before Tfs 1token sequence concat.
2. before Tfs 1, [44], [178], [180] Hierarchical Att.2-stream Tfs followed by concat.
3. Hierarchical Att. early concat.
4. followed by 2-stream TfsCross-Attention exchange query 2, [144] Cross-Att.
5. to Con. 2-stream cross-att. followed by concat.
6. [137], [189] cross-attention, and (6) cross-attention to concatenation.
7. See Table 2 and Figure 2. For brevity, we will state and compare the mathematical formulations in two-modality cases.
8. Please note that all discussed self-attention and its variants are such flexible that can be extended to multiple modality cases.
9. Specifically, the following formulations are modality-, tokenization-, and embedding-agnostic, as self-attention models the embedding of arbitrary token from arbitrary modality as a node of a graph.
10. Given inputs X A and X B from two arbitrary modalities, Z (A) and Z (B) denote their respective token embeddings.
11. Let Z denoting the token embedding (sequence) produced by the multimodal interactions.
12. T f (·) stands for the processing of Transformer layers/blocks.
13. (1) Early Summation In practice, early summation [46], [81] is a simple and effective multimodal interaction, where the token embeddings from multiple modalities can be weighted summed at each token position and then processed by Transformer layers:where ⊕ is element-wise sum, and α and β are weightings.
14. Concretely,. Its main advantage is that it does not increase computational complexity.
15. However, its main disadvantage is due to the manually set weightings.
16. As discussed in Section 3.1.1 and 3.3.1, summing position embedding is intrinsically a case of early summation.
17. (2) Early Concatenation Another straightforward solution is early concatenation [7], [44], [178], [180] that the token embedding sequences from multiple modalities are concatenated and input into Transformer layers asThus, all the multimodal token positions can be attended as a whole sequence, such that the positions of each modality can be encoded well by conditioning the context of other modalities.
18. VideoBERT [7] is the one of the first multimodal Transformer works, where video and text are fused via early concatenation that can encode the global multimodal context well [188].
19. However, the longer sequence after concatenation will increase computational complexity.
20. Early concatenation is also termed ""all-attention"" or ""Co-Transformer"" [137].(3) Hierarchical Attention (multi-stream to one-stream) Transformer layers can be combined hierarchically to attend to the cross-modal interactions.
21. A common practice is that multimodal inputs are encoded by independent Transformer streams and their outputs are concatenated and fused by another Transformer [146]:This kind of hierarchical attention is an implementation of late interaction/fusion, and can be treated as a special case of early concatenation.
22. (4) Hierarchical Attention (one-stream to multi-stream) In-terBERT [188] is another good practice of hierarchical attention where concatenated multimodal inputs are encoded by a shared single-stream Transformer that is followed by two separate Transformer streams.
23. This flow can be formulated asThis method perceives the cross-modal interactions and meanwhile preserves the independence of uni-modal representation.","What are the advantages and disadvantages of early summation in multimodal learning as discussed in the survey?
How does early concatenation in multimodal transformers facilitate the encoding of global multimodal context?
In what way does hierarchical attention differ when transitioning from multi-stream to one-stream versus one-stream to multi-stream in multimodal learning?
What are the key characteristics that make self-attention models flexible for extending to multiple modality cases?
How do the mathematical formulations of self-attention and its variants remain agnostic to modality, tokenization, and embedding in multimodal learning?","1. What are the advantages and disadvantages of early summation in multimodal learning as discussed in the survey?
2. How does early concatenation in multimodal transformers facilitate the encoding of global multimodal context?
3. In what way does hierarchical attention differ when transitioning from multi-stream to one-stream versus one-stream to multi-stream in multimodal learning?
4. What are the key characteristics that make self-attention models flexible for extending to multiple modality cases?
5. How do the mathematical formulations of self-attention and its variants remain agnostic to modality, tokenization, and embedding in multimodal learning?",None,,"Questions:

1. What are the advantages and disadvantages of early summation in multimodal learning as discussed in the survey?
2. How does early concatenation in multimodal transformers facilitate the encoding of global multimodal context?
3. In what way does hierarchical attention differ when transitioning from multi-stream to one-stream versus one-stream to multi-stream in multimodal learning?
4. What are the key characteristics that make self-attention models flexible for extending to multiple modality cases?
5. How do the mathematical formulations of self-attention and its variants remain agnostic to modality, tokenization, and embedding in multimodal learning?

Answer:

(p21.0) Early Summation token-wise sum before Tfs 1

(p21.1) token sequence concat. before Tfs 1

(p21.2) , [44], [178], [180] Hierarchical Att.

(p21.3) 2-stream Tfs followed by concat.

(p21.4) Hierarchical Att. early concat. followed by 2-stream Tfs

(p21.5) Cross-Attention exchange query 2

(p21.6) , [144] Cross-Att. to Con. 2-stream cross-att. followed by concat. [137], [189] cross-attention, and (6) cross-attention to concatenation. See Table 2 and Figure 2. For brevity, we will state and compare the mathematical formulations in two-modality cases. Please note that all discussed self-attention and its variants are such flexible that can be extended to multiple modality cases. Specifically, the following formulations are modality-, tokenization-, and embedding-agnostic, as self-attention models the embedding of arbitrary token from arbitrary modality as a node of a graph.

(p21.7) Given inputs X A and X B from two arbitrary modalities, Z (A) and Z (B) denote their respective token embeddings. Let Z denoting the token embedding (sequence) produced by the multimodal interactions. T f (·) stands for the processing of Transformer layers/blocks.

(p21.8) (1) Early Summation In practice, early summation [46], [81] is a simple and effective multimodal interaction, where the token embeddings from multiple modalities can be weighted summed at each token position and then processed by Transformer layers:

(p21.9) where ⊕ is element-wise sum, and α and β are weightings. Concretely,

(p21.10) . Its main advantage is that it does not increase computational complexity. However, its main disadvantage is due to the manually set weightings. As discussed in Section 3.1.1 and 3.3.1, summing position embedding is intrinsically a case of early summation.

(p21.11) (2) Early Concatenation Another straightforward solution is early concatenation [7], [44], [178], [180] that the token embedding sequences from multiple modalities are concatenated and input into Transformer layers as

(p21.12) Thus, all the multimodal token positions can be attended as a whole sequence, such that the positions of each modality can be encoded well by conditioning the context of other modalities. VideoBERT [7] is the one of the first multimodal Transformer works, where video and text are fused via early concatenation that can encode the global multimodal context well [188]. However, the longer sequence after concatenation will increase computational complexity. Early concatenation is also termed ""all-attention"" or ""Co-Transformer"" [137].

(p21.13) (3) Hierarchical Attention (multi-stream to one-stream) Transformer layers can be combined hierarchically to attend to the cross-modal interactions. A common practice is that multimodal inputs are encoded by independent Transformer streams and their outputs are concatenated and fused by another Transformer [146]:

(p21.14) This kind of hierarchical attention is an implementation of late interaction/fusion, and can be treated as a special case of early concatenation.

(p21.15) (4) Hierarchical Attention (one-stream to multi-stream) In-terBERT [188] is another good practice of hierarchical attention where concatenated multimodal inputs are encoded by a shared single-stream Transformer that is followed by two separate Transformer streams. This flow can be formulated as

(p21.16) This method perceives the cross-modal interactions and meanwhile preserves the independence of uni-modal representation.","Questions:



Answer:

(p21.0) Early Summation token-wise sum before Tfs 1

(p21.1) token sequence concat. before Tfs 1

(p21.2) , [44], [178], [180] Hierarchical Att.

(p21.3) 2-stream Tfs followed by concat.

(p21.4) Hierarchical Att. early concat. followed by 2-stream Tfs

(p21.5) Cross-Attention exchange query 2

(p21.6) , [144] Cross-Att. to Con. 2-stream cross-att. followed by concat. [137], [189] cross-attention, and (6) cross-attention to concatenation. See Table 2 and Figure 2. For brevity, we will state and compare the mathematical formulations in two-modality cases. Please note that all discussed self-attention and its variants are such flexible that can be extended to multiple modality cases. Specifically, the following formulations are modality-, tokenization-, and embedding-agnostic, as self-attention models the embedding of arbitrary token from arbitrary modality as a node of a graph.

(p21.7) Given inputs X A and X B from two arbitrary modalities, Z (A) and Z (B) denote their respective token embeddings. Let Z denoting the token embedding (sequence) produced by the multimodal interactions. T f (·) stands for the processing of Transformer layers/blocks.

(p21.8) (1) Early Summation In practice, early summation [46], [81] is a simple and effective multimodal interaction, where the token embeddings from multiple modalities can be weighted summed at each token position and then processed by Transformer layers:

(p21.9) where ⊕ is element-wise sum, and α and β are weightings. Concretely,

(p21.10) . Its main advantage is that it does not increase computational complexity. However, its main disadvantage is due to the manually set weightings. As discussed in Section 3.1.1 and 3.3.1, summing position embedding is intrinsically a case of early summation.

(p21.11) (2) Early Concatenation Another straightforward solution is early concatenation [7], [44], [178], [180] that the token embedding sequences from multiple modalities are concatenated and input into Transformer layers as

(p21.12) Thus, all the multimodal token positions can be attended as a whole sequence, such that the positions of each modality can be encoded well by conditioning the context of other modalities. VideoBERT [7] is the one of the first multimodal Transformer works, where video and text are fused via early concatenation that can encode the global multimodal context well [188]. However, the longer sequence after concatenation will increase computational complexity. Early concatenation is also termed ""all-attention"" or ""Co-Transformer"" [137].

(p21.13) (3) Hierarchical Attention (multi-stream to one-stream) Transformer layers can be combined hierarchically to attend to the cross-modal interactions. A common practice is that multimodal inputs are encoded by independent Transformer streams and their outputs are concatenated and fused by another Transformer [146]:

(p21.14) This kind of hierarchical attention is an implementation of late interaction/fusion, and can be treated as a special case of early concatenation.

(p21.15) (4) Hierarchical Attention (one-stream to multi-stream) In-terBERT [188] is another good practice of hierarchical attention where concatenated multimodal inputs are encoded by a shared single-stream Transformer that is followed by two separate Transformer streams. This flow can be formulated as

(p21.16) This method perceives the cross-modal interactions and meanwhile preserves the independence of uni-modal representation."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s23,(6) Cross-Attention to Concatenation,"['p23.0', 'p23.1']","['The two streams of cross-attention [102] can be further concatenated and processed by another Transformer to model the global context. This kind of hierarchically cross-modal interaction is also widely studied [137], [189], and alleviates the drawback of cross-attention.', 'Discussion All these aforementioned self-attention variants for multimodal interactions are modality-generic, and can be applied in flexible strategies and for multi-granular tasks. Specifically, these interactions can be flexibly combined and nested. For instance, multiple cross-attention streams are used in hierarchical attention (one-stream to multi-stream) that in a two-stream decoupled model [191] T f 2 and T f 3 of Eq. 11 are implemented by cross-attention defined in Eq. 12. Moreover, they can be extended to multiple (≥ 3) modalities. TriBERT [183] is a tri-modal cross-attention (coattention) for vision, pose, and audio, where given a Query embedding, its Key and Value embeddings are the concatenation from the other modalities. Cross-attention to concatenation is applied to three modalities (i.e., language, video, and audio) in [189].']","The two streams of cross-attention [102] can be further concatenated and processed by another Transformer to model the global context. This kind of hierarchically cross-modal interaction is also widely studied [137], [189], and alleviates the drawback of cross-attention.

Discussion All these aforementioned self-attention variants for multimodal interactions are modality-generic, and can be applied in flexible strategies and for multi-granular tasks. Specifically, these interactions can be flexibly combined and nested. For instance, multiple cross-attention streams are used in hierarchical attention (one-stream to multi-stream) that in a two-stream decoupled model [191] T f 2 and T f 3 of Eq. 11 are implemented by cross-attention defined in Eq. 12. Moreover, they can be extended to multiple (≥ 3) modalities. TriBERT [183] is a tri-modal cross-attention (coattention) for vision, pose, and audio, where given a Query embedding, its Key and Value embeddings are the concatenation from the other modalities. Cross-attention to concatenation is applied to three modalities (i.e., language, video, and audio) in [189].","(p23.0) The two streams of cross-attention [102] can be further concatenated and processed by another Transformer to model the global context. This kind of hierarchically cross-modal interaction is also widely studied [137], [189], and alleviates the drawback of cross-attention.

(p23.1) Discussion All these aforementioned self-attention variants for multimodal interactions are modality-generic, and can be applied in flexible strategies and for multi-granular tasks. Specifically, these interactions can be flexibly combined and nested. For instance, multiple cross-attention streams are used in hierarchical attention (one-stream to multi-stream) that in a two-stream decoupled model [191] T f 2 and T f 3 of Eq. 11 are implemented by cross-attention defined in Eq. 12. Moreover, they can be extended to multiple (≥ 3) modalities. TriBERT [183] is a tri-modal cross-attention (coattention) for vision, pose, and audio, where given a Query embedding, its Key and Value embeddings are the concatenation from the other modalities. Cross-attention to concatenation is applied to three modalities (i.e., language, video, and audio) in [189].","[['b136', 'b188', 'b101'], ['b188', 'b190', 'b182']]","[['b136', 'b188', 'b101'], ['b188', 'b190', 'b182']]",6,"1. The two streams of cross-attention [102] can be further concatenated and processed by another Transformer to model the global context.
2. This kind of hierarchically cross-modal interaction is also widely studied [137], [189], and alleviates the drawback of cross-attention.
3. Discussion All these aforementioned self-attention variants for multimodal interactions are modality-generic, and can be applied in flexible strategies and for multi-granular tasks.
4. Specifically, these interactions can be flexibly combined and nested.
5. For instance, multiple cross-attention streams are used in hierarchical attention (one-stream to multi-stream) that in a two-stream decoupled model
6. [191] T f 2 and T f 3 of Eq. 11 are implemented by cross-attention defined in Eq. 12.
7. Moreover, they can be extended to multiple (≥ 3) modalities.
8. TriBERT [183] is a tri-modal cross-attention (coattention) for vision, pose, and audio, where given a Query embedding, its Key and Value embeddings are the concatenation from the other modalities.
9. Cross-attention to concatenation is applied to three modalities (i.e., language, video, and audio) in [189].","How does cross-attention to concatenation enhance the modeling of global context in multimodal learning?
What are the benefits of using hierarchically cross-modal interactions in multimodal learning?
In what ways can self-attention variants be adapted for multi-granular tasks in multimodal interactions?
How does the hierarchical attention mechanism in multimodal learning utilize multiple cross-attention streams?
What is the role of TriBERT in tri-modal cross-attention, and how does it integrate vision, pose, and audio modalities?","1. How does cross-attention to concatenation enhance the modeling of global context in multimodal learning?
2. What are the benefits of using hierarchically cross-modal interactions in multimodal learning?
3. In what ways can self-attention variants be adapted for multi-granular tasks in multimodal interactions?
4. How does the hierarchical attention mechanism in multimodal learning utilize multiple cross-attention streams?
5. What is the role of TriBERT in tri-modal cross-attention, and how does it integrate vision, pose, and audio modalities?",None,,"Questions:

1. How does cross-attention to concatenation enhance the modeling of global context in multimodal learning?
2. What are the benefits of using hierarchically cross-modal interactions in multimodal learning?
3. In what ways can self-attention variants be adapted for multi-granular tasks in multimodal interactions?
4. How does the hierarchical attention mechanism in multimodal learning utilize multiple cross-attention streams?
5. What is the role of TriBERT in tri-modal cross-attention, and how does it integrate vision, pose, and audio modalities?

Answer:

(p23.0) The two streams of cross-attention [102] can be further concatenated and processed by another Transformer to model the global context. This kind of hierarchically cross-modal interaction is also widely studied [137], [189], and alleviates the drawback of cross-attention.

(p23.1) Discussion All these aforementioned self-attention variants for multimodal interactions are modality-generic, and can be applied in flexible strategies and for multi-granular tasks. Specifically, these interactions can be flexibly combined and nested. For instance, multiple cross-attention streams are used in hierarchical attention (one-stream to multi-stream) that in a two-stream decoupled model [191] T f 2 and T f 3 of Eq. 11 are implemented by cross-attention defined in Eq. 12. Moreover, they can be extended to multiple (≥ 3) modalities. TriBERT [183] is a tri-modal cross-attention (coattention) for vision, pose, and audio, where given a Query embedding, its Key and Value embeddings are the concatenation from the other modalities. Cross-attention to concatenation is applied to three modalities (i.e., language, video, and audio) in [189].","Questions:



Answer:

(p23.0) The two streams of cross-attention [102] can be further concatenated and processed by another Transformer to model the global context. This kind of hierarchically cross-modal interaction is also widely studied [137], [189], and alleviates the drawback of cross-attention.

(p23.1) Discussion All these aforementioned self-attention variants for multimodal interactions are modality-generic, and can be applied in flexible strategies and for multi-granular tasks. Specifically, these interactions can be flexibly combined and nested. For instance, multiple cross-attention streams are used in hierarchical attention (one-stream to multi-stream) that in a two-stream decoupled model [191] T f 2 and T f 3 of Eq. 11 are implemented by cross-attention defined in Eq. 12. Moreover, they can be extended to multiple (≥ 3) modalities. TriBERT [183] is a tri-modal cross-attention (coattention) for vision, pose, and audio, where given a Query embedding, its Key and Value embeddings are the concatenation from the other modalities. Cross-attention to concatenation is applied to three modalities (i.e., language, video, and audio) in [189]."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s24,Network Architectures,"['p24.0', 'p24.1', 'p24.2', 'p24.3']","['Essentially, various multimodal Transformers work due to their internal multimodal attentions that are the aforementioned self-attention variants. Meanwhile, as illustrated in Figure 2, these attentions determine the external network structures of the multimodal Transformers where they are embedded.', 'In general, if we consider from the angle of network structures, (1) early summation and early concatenation work in single-stream, (2) cross-attention work in multistreams, (3) hierarchical attention and cross-attention to concatenation work in hybrid-streams. Thus, multimodal Transformers can be divided into single-stream (e.g., Uniter [106], Visualbert [104], Vl-bert [105] , Unified VLP [110]), multi-stream (e.g., ViLBERT [102], Lxmert [103], ActBERT [114]), hybrid-stream (e.g., InterBERT [188]), etc.', 'From the perspective of timing of interaction, these multimodal attentions fall into three categories, i.e., early interaction: early summation, early concatenation, and hierarchical attention (one-stream to multi-stream), late interaction: hierarchical attention (multi-stream to one-stream), or throughout interaction: cross-attention, cross-attention to concatenation.', 'As demonstrated in Figure 2 in [192], the multimodal Transformer models have another architecture taxonomy based on the computational size of the components.']","Essentially, various multimodal Transformers work due to their internal multimodal attentions that are the aforementioned self-attention variants. Meanwhile, as illustrated in Figure 2, these attentions determine the external network structures of the multimodal Transformers where they are embedded.

In general, if we consider from the angle of network structures, (1) early summation and early concatenation work in single-stream, (2) cross-attention work in multistreams, (3) hierarchical attention and cross-attention to concatenation work in hybrid-streams. Thus, multimodal Transformers can be divided into single-stream (e.g., Uniter [106], Visualbert [104], Vl-bert [105] , Unified VLP [110]), multi-stream (e.g., ViLBERT [102], Lxmert [103], ActBERT [114]), hybrid-stream (e.g., InterBERT [188]), etc.

From the perspective of timing of interaction, these multimodal attentions fall into three categories, i.e., early interaction: early summation, early concatenation, and hierarchical attention (one-stream to multi-stream), late interaction: hierarchical attention (multi-stream to one-stream), or throughout interaction: cross-attention, cross-attention to concatenation.

As demonstrated in Figure 2 in [192], the multimodal Transformer models have another architecture taxonomy based on the computational size of the components.","(p24.0) Essentially, various multimodal Transformers work due to their internal multimodal attentions that are the aforementioned self-attention variants. Meanwhile, as illustrated in Figure 2, these attentions determine the external network structures of the multimodal Transformers where they are embedded.

(p24.1) In general, if we consider from the angle of network structures, (1) early summation and early concatenation work in single-stream, (2) cross-attention work in multistreams, (3) hierarchical attention and cross-attention to concatenation work in hybrid-streams. Thus, multimodal Transformers can be divided into single-stream (e.g., Uniter [106], Visualbert [104], Vl-bert [105] , Unified VLP [110]), multi-stream (e.g., ViLBERT [102], Lxmert [103], ActBERT [114]), hybrid-stream (e.g., InterBERT [188]), etc.

(p24.2) From the perspective of timing of interaction, these multimodal attentions fall into three categories, i.e., early interaction: early summation, early concatenation, and hierarchical attention (one-stream to multi-stream), late interaction: hierarchical attention (multi-stream to one-stream), or throughout interaction: cross-attention, cross-attention to concatenation.

(p24.3) As demonstrated in Figure 2 in [192], the multimodal Transformer models have another architecture taxonomy based on the computational size of the components.","[[], ['b101', 'b109', 'b113', 'b102', 'b105', 'b103', 'b187', 'b104'], [], ['b191']]","[[], ['b101', 'b109', 'b113', 'b102', 'b105', 'b103', 'b187', 'b104'], [], ['b191']]",9,"1. Essentially, various multimodal Transformers work due to their internal multimodal attentions that are the aforementioned self-attention variants.
2. Meanwhile, as illustrated in Figure 2, these attentions determine the external network structures of the multimodal Transformers where they are embedded.
3. In general, if we consider from the angle of network structures, (1) early summation and early concatenation work in single-stream, (2) cross-attention work in multistreams, (3) hierarchical attention and cross-attention to concatenation work in hybrid-streams.
4. Thus, multimodal Transformers can be divided into single-stream (e.g., Uniter [106], Visualbert [104], Vl-bert [105] , Unified VLP [110]), multi-stream (e.g., ViLBERT [102], Lxmert [103], ActBERT [114]), hybrid-stream (e.g., InterBERT [188]), etc.From the perspective of timing of interaction, these multimodal attentions fall into three categories, i.e., early interaction: early summation, early concatenation, and hierarchical attention (one-stream to multi-stream), late interaction: hierarchical attention (multi-stream to one-stream), or throughout interaction: cross-attention, cross-attention to concatenation.
5. As demonstrated in Figure 2 in [192], the multimodal Transformer models have another architecture taxonomy based on the computational size of the components.","How do the internal multimodal attentions in Transformers influence their external network structures?
What are the key differences between single-stream, multi-stream, and hybrid-stream architectures in multimodal Transformers?
Can you explain the role of timing of interaction in the classification of multimodal attentions within Transformers?
How do early interaction techniques like early summation and concatenation differ from late interaction techniques in multimodal Transformers?
What is the significance of computational size in the architecture taxonomy of multimodal Transformer models?","1. How do the internal multimodal attentions in Transformers influence their external network structures?
2. What are the key differences between single-stream, multi-stream, and hybrid-stream architectures in multimodal Transformers?
3. Can you explain the role of timing of interaction in the classification of multimodal attentions within Transformers?
4. How do early interaction techniques like early summation and concatenation differ from late interaction techniques in multimodal Transformers?
5. What is the significance of computational size in the architecture taxonomy of multimodal Transformer models?",None,,"Questions:

1. How do the internal multimodal attentions in Transformers influence their external network structures?
2. What are the key differences between single-stream, multi-stream, and hybrid-stream architectures in multimodal Transformers?
3. Can you explain the role of timing of interaction in the classification of multimodal attentions within Transformers?
4. How do early interaction techniques like early summation and concatenation differ from late interaction techniques in multimodal Transformers?
5. What is the significance of computational size in the architecture taxonomy of multimodal Transformer models?

Answer:

(p24.0) Essentially, various multimodal Transformers work due to their internal multimodal attentions that are the aforementioned self-attention variants. Meanwhile, as illustrated in Figure 2, these attentions determine the external network structures of the multimodal Transformers where they are embedded.

(p24.1) In general, if we consider from the angle of network structures, (1) early summation and early concatenation work in single-stream, (2) cross-attention work in multistreams, (3) hierarchical attention and cross-attention to concatenation work in hybrid-streams. Thus, multimodal Transformers can be divided into single-stream (e.g., Uniter [106], Visualbert [104], Vl-bert [105] , Unified VLP [110]), multi-stream (e.g., ViLBERT [102], Lxmert [103], ActBERT [114]), hybrid-stream (e.g., InterBERT [188]), etc.

(p24.2) From the perspective of timing of interaction, these multimodal attentions fall into three categories, i.e., early interaction: early summation, early concatenation, and hierarchical attention (one-stream to multi-stream), late interaction: hierarchical attention (multi-stream to one-stream), or throughout interaction: cross-attention, cross-attention to concatenation.

(p24.3) As demonstrated in Figure 2 in [192], the multimodal Transformer models have another architecture taxonomy based on the computational size of the components.","Questions:



Answer:

(p24.0) Essentially, various multimodal Transformers work due to their internal multimodal attentions that are the aforementioned self-attention variants. Meanwhile, as illustrated in Figure 2, these attentions determine the external network structures of the multimodal Transformers where they are embedded.

(p24.1) In general, if we consider from the angle of network structures, (1) early summation and early concatenation work in single-stream, (2) cross-attention work in multistreams, (3) hierarchical attention and cross-attention to concatenation work in hybrid-streams. Thus, multimodal Transformers can be divided into single-stream (e.g., Uniter [106], Visualbert [104], Vl-bert [105] , Unified VLP [110]), multi-stream (e.g., ViLBERT [102], Lxmert [103], ActBERT [114]), hybrid-stream (e.g., InterBERT [188]), etc.

(p24.2) From the perspective of timing of interaction, these multimodal attentions fall into three categories, i.e., early interaction: early summation, early concatenation, and hierarchical attention (one-stream to multi-stream), late interaction: hierarchical attention (multi-stream to one-stream), or throughout interaction: cross-attention, cross-attention to concatenation.

(p24.3) As demonstrated in Figure 2 in [192], the multimodal Transformer models have another architecture taxonomy based on the computational size of the components."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s26,Transformers for Multimodal Pretraining,"['p26.0', 'p26.1']","['Inspired by the great success of Transformer based pretraining in NLP community, Transformers are also widely studied for multimodal pretraining as the various large-scale multimodal corpora is emerging. Recent work has demonstrated that if pretrained on large scale multimodal corpora Transformer based models [7], [102], [103], [104], [105], [106], [110] clearly outperform other competitors in a wide range of multimodal down-stream tasks, and moreover achieve the zero-shot generalization ability. These superiorities have led Transformer-based multimodal pretraining to become a hot topic, which has two main directions, i.e., general pretraining for agnostic down-stream tasks (Section 4.1.1), goal-oriented pretraining for specific down-stream tasks (Section 4.1.2).', 'We focus on these key points: (1) What trends are emerging? (2) Where/how do the cross-modal interactions take place during pretraining? (3) How to sort out and understand the pretraining pretext objectives? How can they drive Transformers to learn the cross-modal interactions?']","Inspired by the great success of Transformer based pretraining in NLP community, Transformers are also widely studied for multimodal pretraining as the various large-scale multimodal corpora is emerging. Recent work has demonstrated that if pretrained on large scale multimodal corpora Transformer based models [7], [102], [103], [104], [105], [106], [110] clearly outperform other competitors in a wide range of multimodal down-stream tasks, and moreover achieve the zero-shot generalization ability. These superiorities have led Transformer-based multimodal pretraining to become a hot topic, which has two main directions, i.e., general pretraining for agnostic down-stream tasks (Section 4.1.1), goal-oriented pretraining for specific down-stream tasks (Section 4.1.2).

We focus on these key points: (1) What trends are emerging? (2) Where/how do the cross-modal interactions take place during pretraining? (3) How to sort out and understand the pretraining pretext objectives? How can they drive Transformers to learn the cross-modal interactions?","(p26.0) Inspired by the great success of Transformer based pretraining in NLP community, Transformers are also widely studied for multimodal pretraining as the various large-scale multimodal corpora is emerging. Recent work has demonstrated that if pretrained on large scale multimodal corpora Transformer based models [7], [102], [103], [104], [105], [106], [110] clearly outperform other competitors in a wide range of multimodal down-stream tasks, and moreover achieve the zero-shot generalization ability. These superiorities have led Transformer-based multimodal pretraining to become a hot topic, which has two main directions, i.e., general pretraining for agnostic down-stream tasks (Section 4.1.1), goal-oriented pretraining for specific down-stream tasks (Section 4.1.2).

(p26.1) We focus on these key points: (1) What trends are emerging? (2) Where/how do the cross-modal interactions take place during pretraining? (3) How to sort out and understand the pretraining pretext objectives? How can they drive Transformers to learn the cross-modal interactions?","[['b101', 'b6', 'b109', 'b102', 'b105', 'b103', 'b104'], []]","[['b101', 'b6', 'b109', 'b102', 'b105', 'b103', 'b104'], []]",7,"1. Inspired by the great success of Transformer based pretraining in NLP community, Transformers are also widely studied for multimodal pretraining as the various large-scale multimodal corpora is emerging.
2. Recent work has demonstrated that if pretrained on large scale multimodal corpora Transformer based models [7], [102], [103], [104], [105], [106], [110] clearly outperform other competitors in a wide range of multimodal down-stream tasks, and moreover achieve the zero-shot generalization ability.
3. These superiorities have led Transformer-based multimodal pretraining to become a hot topic, which has two main directions, i.e., general pretraining for agnostic down-stream tasks (Section 4.1.1), goal-oriented pretraining for specific down-stream tasks (Section 4.1.2).
4. We focus on these key points: (1)
5. What trends are emerging? [102]0 Where/how do the cross-modal interactions take place during pretraining?
6. [102]1 How to sort out and understand the pretraining pretext objectives?
7. How can they drive Transformers to learn the cross-modal interactions?","What are the emerging trends in Transformer-based multimodal pretraining?
How do Transformer-based models outperform competitors in multimodal tasks through pretraining?
In what ways do Transformers achieve zero-shot generalization in multimodal learning?
How do cross-modal interactions occur during the pretraining of Transformer-based models?
What are the differences between general pretraining and goal-oriented pretraining for Transformer-based multimodal models?","1. What are the emerging trends in Transformer-based multimodal pretraining?
2. How do Transformer-based models outperform competitors in multimodal tasks through pretraining?
3. In what ways do Transformers achieve zero-shot generalization in multimodal learning?
4. How do cross-modal interactions occur during the pretraining of Transformer-based models?
5. What are the differences between general pretraining and goal-oriented pretraining for Transformer-based multimodal models?",None,,"Questions:

1. What are the emerging trends in Transformer-based multimodal pretraining?
2. How do Transformer-based models outperform competitors in multimodal tasks through pretraining?
3. In what ways do Transformers achieve zero-shot generalization in multimodal learning?
4. How do cross-modal interactions occur during the pretraining of Transformer-based models?
5. What are the differences between general pretraining and goal-oriented pretraining for Transformer-based multimodal models?

Answer:

(p26.0) Inspired by the great success of Transformer based pretraining in NLP community, Transformers are also widely studied for multimodal pretraining as the various large-scale multimodal corpora is emerging. Recent work has demonstrated that if pretrained on large scale multimodal corpora Transformer based models [7], [102], [103], [104], [105], [106], [110] clearly outperform other competitors in a wide range of multimodal down-stream tasks, and moreover achieve the zero-shot generalization ability. These superiorities have led Transformer-based multimodal pretraining to become a hot topic, which has two main directions, i.e., general pretraining for agnostic down-stream tasks (Section 4.1.1), goal-oriented pretraining for specific down-stream tasks (Section 4.1.2).

(p26.1) We focus on these key points: (1) What trends are emerging? (2) Where/how do the cross-modal interactions take place during pretraining? (3) How to sort out and understand the pretraining pretext objectives? How can they drive Transformers to learn the cross-modal interactions?","Questions:



Answer:

(p26.0) Inspired by the great success of Transformer based pretraining in NLP community, Transformers are also widely studied for multimodal pretraining as the various large-scale multimodal corpora is emerging. Recent work has demonstrated that if pretrained on large scale multimodal corpora Transformer based models [7], [102], [103], [104], [105], [106], [110] clearly outperform other competitors in a wide range of multimodal down-stream tasks, and moreover achieve the zero-shot generalization ability. These superiorities have led Transformer-based multimodal pretraining to become a hot topic, which has two main directions, i.e., general pretraining for agnostic down-stream tasks (Section 4.1.1), goal-oriented pretraining for specific down-stream tasks (Section 4.1.2).

(p26.1) We focus on these key points: (1) What trends are emerging? (2) Where/how do the cross-modal interactions take place during pretraining? (3) How to sort out and understand the pretraining pretext objectives? How can they drive Transformers to learn the cross-modal interactions?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s27,Task-Agnostic Multimodal Pretraining,"['p27.0', 'p27.1', 'p27.2', 'p27.3', 'p27.4', 'p27.5', 'p27.6', 'p27.7', 'p27.8']","['Recently Transformer-oriented pretraining has been widely studied involving diverse modality combinations, e.g., video-text [7], [107], [117], image-text [102], [103], [104], [193], [194], [195], acoustic-text [180].', 'Among existing work, the following main trends are emerging:', '(1) Vision-language pretraining (VLP) is a major research problem in this field. VLP is including both ""image + language"" and ""video + language"", also termed visual-linguistic pretraining. A great deal of excellent work has been proposed, e.g., VideoBERT [7], ViLBERT [102], LXMERT [103], VisualBERT [104], VL-BERT [105], UNITER [106], CBT [107], Unicoder-VL [108], B2T2 [109], VLP [110], 12-in-1 [111], Oscar [112], Pixel-BERT [113], ActBERT [114], ImageBERT [115], HERO [116], UniVL [117], SemVLP [196].', '(2) Speech can be used as text. Thanks to recent advances in automatic speech recognition (ASR) techniques, in a multimodal context, speech can be converted to text by the offthe-shelf speech recognition tools. For instance, VideoBERT [7] and CBT [107] make full use of speech rather than lowlevel sounds as a source of cross-modal supervision, by extracting high-level semantic text.', '(3) Overly dependent on the well-aligned multimodal data. A majority of Transformer-based multimodal pretraining works in a self-supervised manner, however, it is overly dependent on the well-aligned multimodal sample pairs/tuples. For instance, large amount of imagelanguage pretraining Transformer models are pretrained on large-scale image-text pairs, e.g., VisualBERT [104], VL-BERT [105], ViLBERT [102], LXMERT [103], UNITER [106]. For another example, the instructional videos (e.g., cooking) 3 are widely used as the pretraining corpora, e.g., HowToVQA69M [140], HowTo100M [141], as in general, their visual clues/content and the spoken words have a higher probability to align with each other, if compared with other videos. However, using cross-modal alignment as cross-modal supervision is costly for large-scale applications. Thus, how to use the weakly-aligned or even unpaired/unaligned multimodal data as the pretraining corpora is still understudied. Some recent attempts [137], [199] study the use of weakly-aligned cross-modal supervision to train Transformers to learn the cross-modal interactions.', '(4) Most of the existing pretext tasks transfer well across modalities. For instance, Masked Language Modelling (MLM) in the text domain has been applied to audio and image, e.g., Masked Acoustic Modelling [180], [200], Masked Image Region Prediction [190], while both Sentence Ordering Modelling (SOM) [201] in text domain and Frame Ordering Modelling (FOM) [116] in video domain share the same idea. We will further discuss the pretext tasks for multimodal Transformer pretraining in the follows.', '(5) Model structures are mainly in three categories. Essentially, in multimodal pretraining scenarios, Transformer models work based on those self-attention variants that are discussed in Section 3.3.2. Thus, if considered from the perspective of model structures, the existing Transformers for multimodal pretraining are also mainly in three categories, i.e., single-stream, multi-stream, hybrid-stream.', '(6) Cross-modal interactions can perform within various components/levels in the pretraining pipelines. For Transformer based multimodal pretraining, the key is to drive the Transformer (encoder w/, w/o decoder) to learn the cross-modal interactions. In the existing Transformerbased multimodal pretraining practices, the cross-modal interactions are flexible, which can perform within various components/levels in the pretraining pipelines. In general, Transformer-based multimodal pretraining pipelines have three key components, from bottom to top, i.e., tokenization, Transformer representation, objective supervision. For not only the multimodal pretraining but also the specific multimodal tasks, the cross-modal interactions can perform within arbitrary component(s) of the three. As discussed in Section 3.3.2, because self-attention models the embedding of an arbitrary token from an arbitrary modality as a node of a graph, the existing pretraining pipelines can, in general, be transferred independently across modalities, unless considered with modality-specific objectives.', 'Discussion Vision Language Pretraining (VLP) follows two general pipelines: two-stage (need object detector, e.g., Faster R-CNN [202]) (e.g., LXMERT [103], ViLBert [102], VL-Bert [105], UNITER [106]) and end-to-end (e.g., Pixel-Bert [113], SOHO [203], KD-VLP [204], Simvlm [199]). Two-stage pipelines have a main advantage -object-aware perceiving, by using the supervised pre-trained visual detectors, however these are based on a strong assumption that the visual representations can be fixed.']","Recently Transformer-oriented pretraining has been widely studied involving diverse modality combinations, e.g., video-text [7], [107], [117], image-text [102], [103], [104], [193], [194], [195], acoustic-text [180].

Among existing work, the following main trends are emerging:

(1) Vision-language pretraining (VLP) is a major research problem in this field. VLP is including both ""image + language"" and ""video + language"", also termed visual-linguistic pretraining. A great deal of excellent work has been proposed, e.g., VideoBERT [7], ViLBERT [102], LXMERT [103], VisualBERT [104], VL-BERT [105], UNITER [106], CBT [107], Unicoder-VL [108], B2T2 [109], VLP [110], 12-in-1 [111], Oscar [112], Pixel-BERT [113], ActBERT [114], ImageBERT [115], HERO [116], UniVL [117], SemVLP [196].

(2) Speech can be used as text. Thanks to recent advances in automatic speech recognition (ASR) techniques, in a multimodal context, speech can be converted to text by the offthe-shelf speech recognition tools. For instance, VideoBERT [7] and CBT [107] make full use of speech rather than lowlevel sounds as a source of cross-modal supervision, by extracting high-level semantic text.

(3) Overly dependent on the well-aligned multimodal data. A majority of Transformer-based multimodal pretraining works in a self-supervised manner, however, it is overly dependent on the well-aligned multimodal sample pairs/tuples. For instance, large amount of imagelanguage pretraining Transformer models are pretrained on large-scale image-text pairs, e.g., VisualBERT [104], VL-BERT [105], ViLBERT [102], LXMERT [103], UNITER [106]. For another example, the instructional videos (e.g., cooking) 3 are widely used as the pretraining corpora, e.g., HowToVQA69M [140], HowTo100M [141], as in general, their visual clues/content and the spoken words have a higher probability to align with each other, if compared with other videos. However, using cross-modal alignment as cross-modal supervision is costly for large-scale applications. Thus, how to use the weakly-aligned or even unpaired/unaligned multimodal data as the pretraining corpora is still understudied. Some recent attempts [137], [199] study the use of weakly-aligned cross-modal supervision to train Transformers to learn the cross-modal interactions.

(4) Most of the existing pretext tasks transfer well across modalities. For instance, Masked Language Modelling (MLM) in the text domain has been applied to audio and image, e.g., Masked Acoustic Modelling [180], [200], Masked Image Region Prediction [190], while both Sentence Ordering Modelling (SOM) [201] in text domain and Frame Ordering Modelling (FOM) [116] in video domain share the same idea. We will further discuss the pretext tasks for multimodal Transformer pretraining in the follows.

(5) Model structures are mainly in three categories. Essentially, in multimodal pretraining scenarios, Transformer models work based on those self-attention variants that are discussed in Section 3.3.2. Thus, if considered from the perspective of model structures, the existing Transformers for multimodal pretraining are also mainly in three categories, i.e., single-stream, multi-stream, hybrid-stream.

(6) Cross-modal interactions can perform within various components/levels in the pretraining pipelines. For Transformer based multimodal pretraining, the key is to drive the Transformer (encoder w/, w/o decoder) to learn the cross-modal interactions. In the existing Transformerbased multimodal pretraining practices, the cross-modal interactions are flexible, which can perform within various components/levels in the pretraining pipelines. In general, Transformer-based multimodal pretraining pipelines have three key components, from bottom to top, i.e., tokenization, Transformer representation, objective supervision. For not only the multimodal pretraining but also the specific multimodal tasks, the cross-modal interactions can perform within arbitrary component(s) of the three. As discussed in Section 3.3.2, because self-attention models the embedding of an arbitrary token from an arbitrary modality as a node of a graph, the existing pretraining pipelines can, in general, be transferred independently across modalities, unless considered with modality-specific objectives.

Discussion Vision Language Pretraining (VLP) follows two general pipelines: two-stage (need object detector, e.g., Faster R-CNN [202]) (e.g., LXMERT [103], ViLBert [102], VL-Bert [105], UNITER [106]) and end-to-end (e.g., Pixel-Bert [113], SOHO [203], KD-VLP [204], Simvlm [199]). Two-stage pipelines have a main advantage -object-aware perceiving, by using the supervised pre-trained visual detectors, however these are based on a strong assumption that the visual representations can be fixed.","(p27.0) Recently Transformer-oriented pretraining has been widely studied involving diverse modality combinations, e.g., video-text [7], [107], [117], image-text [102], [103], [104], [193], [194], [195], acoustic-text [180].

(p27.1) Among existing work, the following main trends are emerging:

(p27.2) (1) Vision-language pretraining (VLP) is a major research problem in this field. VLP is including both ""image + language"" and ""video + language"", also termed visual-linguistic pretraining. A great deal of excellent work has been proposed, e.g., VideoBERT [7], ViLBERT [102], LXMERT [103], VisualBERT [104], VL-BERT [105], UNITER [106], CBT [107], Unicoder-VL [108], B2T2 [109], VLP [110], 12-in-1 [111], Oscar [112], Pixel-BERT [113], ActBERT [114], ImageBERT [115], HERO [116], UniVL [117], SemVLP [196].

(p27.3) (2) Speech can be used as text. Thanks to recent advances in automatic speech recognition (ASR) techniques, in a multimodal context, speech can be converted to text by the offthe-shelf speech recognition tools. For instance, VideoBERT [7] and CBT [107] make full use of speech rather than lowlevel sounds as a source of cross-modal supervision, by extracting high-level semantic text.

(p27.4) (3) Overly dependent on the well-aligned multimodal data. A majority of Transformer-based multimodal pretraining works in a self-supervised manner, however, it is overly dependent on the well-aligned multimodal sample pairs/tuples. For instance, large amount of imagelanguage pretraining Transformer models are pretrained on large-scale image-text pairs, e.g., VisualBERT [104], VL-BERT [105], ViLBERT [102], LXMERT [103], UNITER [106]. For another example, the instructional videos (e.g., cooking) 3 are widely used as the pretraining corpora, e.g., HowToVQA69M [140], HowTo100M [141], as in general, their visual clues/content and the spoken words have a higher probability to align with each other, if compared with other videos. However, using cross-modal alignment as cross-modal supervision is costly for large-scale applications. Thus, how to use the weakly-aligned or even unpaired/unaligned multimodal data as the pretraining corpora is still understudied. Some recent attempts [137], [199] study the use of weakly-aligned cross-modal supervision to train Transformers to learn the cross-modal interactions.

(p27.5) (4) Most of the existing pretext tasks transfer well across modalities. For instance, Masked Language Modelling (MLM) in the text domain has been applied to audio and image, e.g., Masked Acoustic Modelling [180], [200], Masked Image Region Prediction [190], while both Sentence Ordering Modelling (SOM) [201] in text domain and Frame Ordering Modelling (FOM) [116] in video domain share the same idea. We will further discuss the pretext tasks for multimodal Transformer pretraining in the follows.

(p27.6) (5) Model structures are mainly in three categories. Essentially, in multimodal pretraining scenarios, Transformer models work based on those self-attention variants that are discussed in Section 3.3.2. Thus, if considered from the perspective of model structures, the existing Transformers for multimodal pretraining are also mainly in three categories, i.e., single-stream, multi-stream, hybrid-stream.

(p27.7) (6) Cross-modal interactions can perform within various components/levels in the pretraining pipelines. For Transformer based multimodal pretraining, the key is to drive the Transformer (encoder w/, w/o decoder) to learn the cross-modal interactions. In the existing Transformerbased multimodal pretraining practices, the cross-modal interactions are flexible, which can perform within various components/levels in the pretraining pipelines. In general, Transformer-based multimodal pretraining pipelines have three key components, from bottom to top, i.e., tokenization, Transformer representation, objective supervision. For not only the multimodal pretraining but also the specific multimodal tasks, the cross-modal interactions can perform within arbitrary component(s) of the three. As discussed in Section 3.3.2, because self-attention models the embedding of an arbitrary token from an arbitrary modality as a node of a graph, the existing pretraining pipelines can, in general, be transferred independently across modalities, unless considered with modality-specific objectives.

(p27.8) Discussion Vision Language Pretraining (VLP) follows two general pipelines: two-stage (need object detector, e.g., Faster R-CNN [202]) (e.g., LXMERT [103], ViLBert [102], VL-Bert [105], UNITER [106]) and end-to-end (e.g., Pixel-Bert [113], SOHO [203], KD-VLP [204], Simvlm [199]). Two-stage pipelines have a main advantage -object-aware perceiving, by using the supervised pre-trained visual detectors, however these are based on a strong assumption that the visual representations can be fixed.","[['b101', 'b6', 'b193', 'b102', 'b192', 'b116', 'b179', 'b106', 'b103', 'b194'], [], ['b101', 'b110', 'b6', 'b108', 'b109', 'b111', 'b113', 'b114', 'b102', 'b112', 'b115', 'b116', 'b105', 'b195', 'b107', 'b106', 'b103', 'b104'], ['b106', 'b6'], ['b136', 'b101', 'b2', 'b140', 'b198', 'b102', 'b105', 'b139', 'b103', 'b104'], ['b199', 'b115', 'b179', 'b200', 'b189'], [], [], ['b101', 'b198', 'b201', 'b203', 'b202', 'b102', 'b112', 'b105', 'b104']]","[['b101', 'b6', 'b193', 'b102', 'b192', 'b116', 'b179', 'b106', 'b103', 'b194'], [], ['b101', 'b110', 'b6', 'b108', 'b109', 'b111', 'b113', 'b114', 'b102', 'b112', 'b115', 'b116', 'b105', 'b195', 'b107', 'b106', 'b103', 'b104'], ['b106', 'b6'], ['b136', 'b101', 'b2', 'b140', 'b198', 'b102', 'b105', 'b139', 'b103', 'b104'], ['b199', 'b115', 'b179', 'b200', 'b189'], [], [], ['b101', 'b198', 'b201', 'b203', 'b202', 'b102', 'b112', 'b105', 'b104']]",54,"1. Recently Transformer-oriented pretraining has been widely studied involving diverse modality combinations, e.g., video-text [7], [107], [117], image-text [102], [103], [104], [193], [194], [195], acoustic-text [180].
2. Among existing work, the following main trends are emerging:(1) Vision-language pretraining (VLP) is a major research problem in this field.
3. VLP is including both ""image + language"" and ""video + language"", also termed visual-linguistic pretraining.
4. A great deal of excellent work has been proposed, e.g., VideoBERT [7], ViLBERT [102], LXMERT [103], VisualBERT [104], VL-BERT [105], UNITER [106], CBT [107], Unicoder-VL [108], B2T2 [109], VLP [110], 12-in-1 [111], Oscar [112], Pixel-BERT [113], ActBERT [114], ImageBERT [115], HERO [116], UniVL [117], SemVLP [196].
5. (2) Speech can be used as text.
6. Thanks to recent advances in automatic speech recognition (ASR) techniques, in a multimodal context, speech can be converted to text by the offthe-shelf speech recognition tools.
7. For instance, VideoBERT [7] and CBT [107] make full use of speech rather than lowlevel sounds as a source of cross-modal supervision, by extracting high-level semantic text.
8. (3) Overly dependent on the well-aligned multimodal data.
9. A majority of Transformer-based multimodal pretraining works in a self-supervised manner, however, it is overly dependent on the well-aligned multimodal sample pairs/tuples.
10. For instance, large amount of imagelanguage pretraining Transformer models are pretrained on large-scale image-text pairs, e.g., VisualBERT [104], VL-BERT [105], ViLBERT [102], LXMERT [103], UNITER [106].
11. For another example, the instructional videos (e.g., cooking) 3 are widely used as the pretraining corpora, e.g., HowToVQA69M [140], HowTo100M [141], as in general, their visual clues/content and the spoken words have a higher probability to align with each other, if compared with other videos.
12. However, using cross-modal alignment as cross-modal supervision is costly for large-scale applications.
13. Thus, how to use the weakly-aligned or even unpaired/unaligned multimodal data as the pretraining corpora is still understudied.
14. Some recent attempts [137], [199] study the use of weakly-aligned cross-modal supervision to train Transformers to learn the cross-modal interactions.
15. (4) Most of the existing pretext tasks transfer well across modalities.
16. For instance, Masked Language Modelling (MLM) in the text domain has been applied to audio and image, e.g., Masked Acoustic Modelling [180], [200], Masked Image Region Prediction [190], while both Sentence Ordering Modelling (SOM) [201] in text domain and Frame Ordering Modelling (FOM) [116] in video domain share the same idea.
17. We will further discuss the pretext tasks for multimodal Transformer pretraining in the follows.
18. (5) Model structures are mainly in three categories.
19. Essentially, in multimodal pretraining scenarios, Transformer models work based on those self-attention variants that are discussed in Section 3.3.2.
20. Thus, if considered from the perspective of model structures, the existing Transformers for multimodal pretraining are also mainly in three categories, i.e., single-stream, multi-stream, hybrid-stream.
21. (6) Cross-modal interactions can perform within various components/levels in the pretraining pipelines.
22. For Transformer based multimodal pretraining, the key is to drive the Transformer (encoder w/, w/o decoder) to learn the cross-modal interactions.
23. In the existing Transformerbased multimodal pretraining practices, the cross-modal interactions are flexible, which can perform within various components/levels in the pretraining pipelines.
24. In general, Transformer-based multimodal pretraining pipelines have three key components, from bottom to top, i.e., tokenization, Transformer representation, objective supervision.
25. For not only the multimodal pretraining but also the specific multimodal tasks, the cross-modal interactions can perform within arbitrary component(s) of the three.
26. As discussed in Section 3.3.2, because self-attention models the embedding of an arbitrary token from an arbitrary modality as a node of a graph, the existing pretraining pipelines can, in general, be transferred independently across modalities, unless considered with modality-specific objectives.
27. Discussion Vision Language Pretraining (VLP) follows two general pipelines: two-stage (need object detector, e.g., Faster R-CNN [202]) (e.g., LXMERT [103], ViLBert [102], VL-Bert [105], UNITER [106]) and end-to-end (e.g., Pixel-Bert [113], SOHO [203], KD-VLP
28. [204], Simvlm [199]). Two-stage pipelines have a main advantage -object-aware perceiving, by using the supervised pre-trained visual detectors, however these are based on a strong assumption that the visual representations can be fixed.","What are the main trends identified in Transformer-oriented multimodal pretraining involving diverse modality combinations?
How does the use of automatic speech recognition (ASR) techniques facilitate the integration of speech as text in multimodal learning?
What challenges are associated with the dependency on well-aligned multimodal data for Transformer-based multimodal pretraining?
In what ways do existing pretext tasks in multimodal Transformer pretraining transfer across different modalities?
How do the model structures in multimodal pretraining scenarios vary, and what are their main categories?","1. What are the main trends identified in Transformer-oriented multimodal pretraining involving diverse modality combinations?
2. How does the use of automatic speech recognition (ASR) techniques facilitate the integration of speech as text in multimodal learning?
3. What challenges are associated with the dependency on well-aligned multimodal data for Transformer-based multimodal pretraining?
4. In what ways do existing pretext tasks in multimodal Transformer pretraining transfer across different modalities?
5. How do the model structures in multimodal pretraining scenarios vary, and what are their main categories?",None,,"Questions:

1. What are the main trends identified in Transformer-oriented multimodal pretraining involving diverse modality combinations?
2. How does the use of automatic speech recognition (ASR) techniques facilitate the integration of speech as text in multimodal learning?
3. What challenges are associated with the dependency on well-aligned multimodal data for Transformer-based multimodal pretraining?
4. In what ways do existing pretext tasks in multimodal Transformer pretraining transfer across different modalities?
5. How do the model structures in multimodal pretraining scenarios vary, and what are their main categories?

Answer:

(p27.0) Recently Transformer-oriented pretraining has been widely studied involving diverse modality combinations, e.g., video-text [7], [107], [117], image-text [102], [103], [104], [193], [194], [195], acoustic-text [180].

(p27.1) Among existing work, the following main trends are emerging:

(p27.2) (1) Vision-language pretraining (VLP) is a major research problem in this field. VLP is including both ""image + language"" and ""video + language"", also termed visual-linguistic pretraining. A great deal of excellent work has been proposed, e.g., VideoBERT [7], ViLBERT [102], LXMERT [103], VisualBERT [104], VL-BERT [105], UNITER [106], CBT [107], Unicoder-VL [108], B2T2 [109], VLP [110], 12-in-1 [111], Oscar [112], Pixel-BERT [113], ActBERT [114], ImageBERT [115], HERO [116], UniVL [117], SemVLP [196].

(p27.3) (2) Speech can be used as text. Thanks to recent advances in automatic speech recognition (ASR) techniques, in a multimodal context, speech can be converted to text by the offthe-shelf speech recognition tools. For instance, VideoBERT [7] and CBT [107] make full use of speech rather than lowlevel sounds as a source of cross-modal supervision, by extracting high-level semantic text.

(p27.4) (3) Overly dependent on the well-aligned multimodal data. A majority of Transformer-based multimodal pretraining works in a self-supervised manner, however, it is overly dependent on the well-aligned multimodal sample pairs/tuples. For instance, large amount of imagelanguage pretraining Transformer models are pretrained on large-scale image-text pairs, e.g., VisualBERT [104], VL-BERT [105], ViLBERT [102], LXMERT [103], UNITER [106]. For another example, the instructional videos (e.g., cooking) 3 are widely used as the pretraining corpora, e.g., HowToVQA69M [140], HowTo100M [141], as in general, their visual clues/content and the spoken words have a higher probability to align with each other, if compared with other videos. However, using cross-modal alignment as cross-modal supervision is costly for large-scale applications. Thus, how to use the weakly-aligned or even unpaired/unaligned multimodal data as the pretraining corpora is still understudied. Some recent attempts [137], [199] study the use of weakly-aligned cross-modal supervision to train Transformers to learn the cross-modal interactions.

(p27.5) (4) Most of the existing pretext tasks transfer well across modalities. For instance, Masked Language Modelling (MLM) in the text domain has been applied to audio and image, e.g., Masked Acoustic Modelling [180], [200], Masked Image Region Prediction [190], while both Sentence Ordering Modelling (SOM) [201] in text domain and Frame Ordering Modelling (FOM) [116] in video domain share the same idea. We will further discuss the pretext tasks for multimodal Transformer pretraining in the follows.

(p27.6) (5) Model structures are mainly in three categories. Essentially, in multimodal pretraining scenarios, Transformer models work based on those self-attention variants that are discussed in Section 3.3.2. Thus, if considered from the perspective of model structures, the existing Transformers for multimodal pretraining are also mainly in three categories, i.e., single-stream, multi-stream, hybrid-stream.

(p27.7) (6) Cross-modal interactions can perform within various components/levels in the pretraining pipelines. For Transformer based multimodal pretraining, the key is to drive the Transformer (encoder w/, w/o decoder) to learn the cross-modal interactions. In the existing Transformerbased multimodal pretraining practices, the cross-modal interactions are flexible, which can perform within various components/levels in the pretraining pipelines. In general, Transformer-based multimodal pretraining pipelines have three key components, from bottom to top, i.e., tokenization, Transformer representation, objective supervision. For not only the multimodal pretraining but also the specific multimodal tasks, the cross-modal interactions can perform within arbitrary component(s) of the three. As discussed in Section 3.3.2, because self-attention models the embedding of an arbitrary token from an arbitrary modality as a node of a graph, the existing pretraining pipelines can, in general, be transferred independently across modalities, unless considered with modality-specific objectives.

(p27.8) Discussion Vision Language Pretraining (VLP) follows two general pipelines: two-stage (need object detector, e.g., Faster R-CNN [202]) (e.g., LXMERT [103], ViLBert [102], VL-Bert [105], UNITER [106]) and end-to-end (e.g., Pixel-Bert [113], SOHO [203], KD-VLP [204], Simvlm [199]). Two-stage pipelines have a main advantage -object-aware perceiving, by using the supervised pre-trained visual detectors, however these are based on a strong assumption that the visual representations can be fixed.","Questions:



Answer:

(p27.0) Recently Transformer-oriented pretraining has been widely studied involving diverse modality combinations, e.g., video-text [7], [107], [117], image-text [102], [103], [104], [193], [194], [195], acoustic-text [180].

(p27.1) Among existing work, the following main trends are emerging:

(p27.2) (1) Vision-language pretraining (VLP) is a major research problem in this field. VLP is including both ""image + language"" and ""video + language"", also termed visual-linguistic pretraining. A great deal of excellent work has been proposed, e.g., VideoBERT [7], ViLBERT [102], LXMERT [103], VisualBERT [104], VL-BERT [105], UNITER [106], CBT [107], Unicoder-VL [108], B2T2 [109], VLP [110], 12-in-1 [111], Oscar [112], Pixel-BERT [113], ActBERT [114], ImageBERT [115], HERO [116], UniVL [117], SemVLP [196].

(p27.3) (2) Speech can be used as text. Thanks to recent advances in automatic speech recognition (ASR) techniques, in a multimodal context, speech can be converted to text by the offthe-shelf speech recognition tools. For instance, VideoBERT [7] and CBT [107] make full use of speech rather than lowlevel sounds as a source of cross-modal supervision, by extracting high-level semantic text.

(p27.4) (3) Overly dependent on the well-aligned multimodal data. A majority of Transformer-based multimodal pretraining works in a self-supervised manner, however, it is overly dependent on the well-aligned multimodal sample pairs/tuples. For instance, large amount of imagelanguage pretraining Transformer models are pretrained on large-scale image-text pairs, e.g., VisualBERT [104], VL-BERT [105], ViLBERT [102], LXMERT [103], UNITER [106]. For another example, the instructional videos (e.g., cooking) 3 are widely used as the pretraining corpora, e.g., HowToVQA69M [140], HowTo100M [141], as in general, their visual clues/content and the spoken words have a higher probability to align with each other, if compared with other videos. However, using cross-modal alignment as cross-modal supervision is costly for large-scale applications. Thus, how to use the weakly-aligned or even unpaired/unaligned multimodal data as the pretraining corpora is still understudied. Some recent attempts [137], [199] study the use of weakly-aligned cross-modal supervision to train Transformers to learn the cross-modal interactions.

(p27.5) (4) Most of the existing pretext tasks transfer well across modalities. For instance, Masked Language Modelling (MLM) in the text domain has been applied to audio and image, e.g., Masked Acoustic Modelling [180], [200], Masked Image Region Prediction [190], while both Sentence Ordering Modelling (SOM) [201] in text domain and Frame Ordering Modelling (FOM) [116] in video domain share the same idea. We will further discuss the pretext tasks for multimodal Transformer pretraining in the follows.

(p27.6) (5) Model structures are mainly in three categories. Essentially, in multimodal pretraining scenarios, Transformer models work based on those self-attention variants that are discussed in Section 3.3.2. Thus, if considered from the perspective of model structures, the existing Transformers for multimodal pretraining are also mainly in three categories, i.e., single-stream, multi-stream, hybrid-stream.

(p27.7) (6) Cross-modal interactions can perform within various components/levels in the pretraining pipelines. For Transformer based multimodal pretraining, the key is to drive the Transformer (encoder w/, w/o decoder) to learn the cross-modal interactions. In the existing Transformerbased multimodal pretraining practices, the cross-modal interactions are flexible, which can perform within various components/levels in the pretraining pipelines. In general, Transformer-based multimodal pretraining pipelines have three key components, from bottom to top, i.e., tokenization, Transformer representation, objective supervision. For not only the multimodal pretraining but also the specific multimodal tasks, the cross-modal interactions can perform within arbitrary component(s) of the three. As discussed in Section 3.3.2, because self-attention models the embedding of an arbitrary token from an arbitrary modality as a node of a graph, the existing pretraining pipelines can, in general, be transferred independently across modalities, unless considered with modality-specific objectives.

(p27.8) Discussion Vision Language Pretraining (VLP) follows two general pipelines: two-stage (need object detector, e.g., Faster R-CNN [202]) (e.g., LXMERT [103], ViLBert [102], VL-Bert [105], UNITER [106]) and end-to-end (e.g., Pixel-Bert [113], SOHO [203], KD-VLP [204], Simvlm [199]). Two-stage pipelines have a main advantage -object-aware perceiving, by using the supervised pre-trained visual detectors, however these are based on a strong assumption that the visual representations can be fixed."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s28,Discussion,['p28.0'],"['How to look for more corpora that intrinsically have well-aligned cross-modal supervision, such as instructional videos, is still an open problem. However, weaklyaligned cross-modal samples are popular in the real-life scenarios, for instance, enormous weakly aligned multimodal data samples are emerging in e-commerce [137], due to fine-grained categories, complex combinations, and fuzzy correspondence. Well labelled/aligned cross-modal datasets are very costly in collecting and annotating; how to use weakly-aligned or even unaligned corpora crawled from the web is a promising question. Some recently successful practice [9], [199], [205] used weakly aligned image-text pairs to perform pretraining, and achieve both competitive performance and zero-shot learning capability for image classification, image-text retrieval, and open-ended visual question answering, etc. Because these practices in weak supervision make full use of large-scale pretraining corpora, they yield greater promise of zero-shot generalization.']","How to look for more corpora that intrinsically have well-aligned cross-modal supervision, such as instructional videos, is still an open problem. However, weaklyaligned cross-modal samples are popular in the real-life scenarios, for instance, enormous weakly aligned multimodal data samples are emerging in e-commerce [137], due to fine-grained categories, complex combinations, and fuzzy correspondence. Well labelled/aligned cross-modal datasets are very costly in collecting and annotating; how to use weakly-aligned or even unaligned corpora crawled from the web is a promising question. Some recently successful practice [9], [199], [205] used weakly aligned image-text pairs to perform pretraining, and achieve both competitive performance and zero-shot learning capability for image classification, image-text retrieval, and open-ended visual question answering, etc. Because these practices in weak supervision make full use of large-scale pretraining corpora, they yield greater promise of zero-shot generalization.","(p28.0) How to look for more corpora that intrinsically have well-aligned cross-modal supervision, such as instructional videos, is still an open problem. However, weaklyaligned cross-modal samples are popular in the real-life scenarios, for instance, enormous weakly aligned multimodal data samples are emerging in e-commerce [137], due to fine-grained categories, complex combinations, and fuzzy correspondence. Well labelled/aligned cross-modal datasets are very costly in collecting and annotating; how to use weakly-aligned or even unaligned corpora crawled from the web is a promising question. Some recently successful practice [9], [199], [205] used weakly aligned image-text pairs to perform pretraining, and achieve both competitive performance and zero-shot learning capability for image classification, image-text retrieval, and open-ended visual question answering, etc. Because these practices in weak supervision make full use of large-scale pretraining corpora, they yield greater promise of zero-shot generalization.","[['b204', 'b136', 'b198', 'b8']]","[['b204', 'b136', 'b198', 'b8']]",4,"1. How to look for more corpora that intrinsically have well-aligned cross-modal supervision, such as instructional videos, is still an open problem.
2. However, weaklyaligned cross-modal samples are popular in the real-life scenarios, for instance, enormous weakly aligned multimodal data samples are emerging in e-commerce [137], due to fine-grained categories, complex combinations, and fuzzy correspondence.
3. Well labelled/aligned cross-modal datasets are very costly in collecting and annotating; how to use weakly-aligned or even unaligned corpora crawled from the web is a promising question.
4. Some recently successful practice [9], [199], [205] used weakly aligned image-text pairs to perform pretraining, and achieve both competitive performance and zero-shot learning capability for image classification, image-text retrieval, and open-ended visual question answering, etc. Because these practices in weak supervision make full use of large-scale pretraining corpora, they yield greater promise of zero-shot generalization.","What are the challenges and opportunities in utilizing weakly-aligned cross-modal samples for multimodal learning?
How do instructional videos serve as a source of well-aligned cross-modal supervision in multimodal learning?
What strategies have been successful in leveraging weakly-aligned image-text pairs for pretraining in multimodal learning?
In what ways does weak supervision contribute to zero-shot learning capabilities in tasks such as image classification and visual question answering?
What are the implications of using large-scale pretraining corpora for achieving zero-shot generalization in multimodal learning?","1. What are the challenges and opportunities in utilizing weakly-aligned cross-modal samples for multimodal learning?
2. How do instructional videos serve as a source of well-aligned cross-modal supervision in multimodal learning?
3. What strategies have been successful in leveraging weakly-aligned image-text pairs for pretraining in multimodal learning?
4. In what ways does weak supervision contribute to zero-shot learning capabilities in tasks such as image classification and visual question answering?
5. What are the implications of using large-scale pretraining corpora for achieving zero-shot generalization in multimodal learning?",None,,"Questions:

1. What are the challenges and opportunities in utilizing weakly-aligned cross-modal samples for multimodal learning?
2. How do instructional videos serve as a source of well-aligned cross-modal supervision in multimodal learning?
3. What strategies have been successful in leveraging weakly-aligned image-text pairs for pretraining in multimodal learning?
4. In what ways does weak supervision contribute to zero-shot learning capabilities in tasks such as image classification and visual question answering?
5. What are the implications of using large-scale pretraining corpora for achieving zero-shot generalization in multimodal learning?

Answer:

(p28.0) How to look for more corpora that intrinsically have well-aligned cross-modal supervision, such as instructional videos, is still an open problem. However, weaklyaligned cross-modal samples are popular in the real-life scenarios, for instance, enormous weakly aligned multimodal data samples are emerging in e-commerce [137], due to fine-grained categories, complex combinations, and fuzzy correspondence. Well labelled/aligned cross-modal datasets are very costly in collecting and annotating; how to use weakly-aligned or even unaligned corpora crawled from the web is a promising question. Some recently successful practice [9], [199], [205] used weakly aligned image-text pairs to perform pretraining, and achieve both competitive performance and zero-shot learning capability for image classification, image-text retrieval, and open-ended visual question answering, etc. Because these practices in weak supervision make full use of large-scale pretraining corpora, they yield greater promise of zero-shot generalization.","Questions:



Answer:

(p28.0) How to look for more corpora that intrinsically have well-aligned cross-modal supervision, such as instructional videos, is still an open problem. However, weaklyaligned cross-modal samples are popular in the real-life scenarios, for instance, enormous weakly aligned multimodal data samples are emerging in e-commerce [137], due to fine-grained categories, complex combinations, and fuzzy correspondence. Well labelled/aligned cross-modal datasets are very costly in collecting and annotating; how to use weakly-aligned or even unaligned corpora crawled from the web is a promising question. Some recently successful practice [9], [199], [205] used weakly aligned image-text pairs to perform pretraining, and achieve both competitive performance and zero-shot learning capability for image classification, image-text retrieval, and open-ended visual question answering, etc. Because these practices in weak supervision make full use of large-scale pretraining corpora, they yield greater promise of zero-shot generalization."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s29,Pretext Tasks,"['p29.0', 'p29.1', 'p29.2', 'p29.3', 'p29.4', 'p29.5', 'p29.6', 'p29.7']","['In Transformer based multimodal pretraining, the pretraining tasks/objectives are also termed pretext tasks/objectives. To date, various pretext tasks have been studied, e.g., masked language modelling (MLM) [137], masked image region prediction/classification (also termed masked object classification (MOC)) [137], [190], masked region regression (MRR) [115], visual-linguistic matching (VLM) (e.g., image-text matching (ITM) [188], image text matching (ITM), phrase-region alignment (PRA) [204], word-region alignment (WRA) [106], video-subtitle matching (VSM) [116]), masked frame modelling (MFM) [116], frame order modelling (FOM) [116], next sentence prediction (NSP) [4], [102], [190], masked sentence generation (MSG) [191], masked group modelling (MGM) [188], prefix language modelling (PrefixLM) [199], video conditioned masked language model [117], text conditioned masked frame model [117], visual translation language modelling (VTLM) [206], and image-conditioned masked language modelling (also termed image-attended masked language modelling) [207]. These down-stream task -agnostic pretext pretraining is optional, and the down-stream task objectives can be trained directly, which will be discussed in Section 4.1.2. Table 3 provides the common and representative pretext tasks for Transformer based multimodal pretraining. In practice, pretext tasks can be combined, and some representative cases are summarized in Table 3 of [57], Table 2 of [58].', 'The pretext tasks have multiple taxonomies:', '(1) Supervision. The common multimodal pretraining Transformers use well-aligned, weakly-aligned, and even unaligned multimodal sample pairs/tuples, to work in supervised, weakly-supervised, and unsupervised manners, respectively. Meanwhile, if we consider the definitions of their pretext tasks/objectives from supervision, the pretexts can be sorted into unsupervised/self-supervised (e.g., masked language modelling (MLM) [7], [137]) and supervised (e.g., image-text matching (ITM) [188] [102], [103], [104], [106], [209]), etc. Nowadays, self-supervised attempts are the majority.', '(2) Modality. Considering the mathematical formulations, some pretexts are defined on single modality, e.g., masked language modelling [7], masked acoustic modelling [200], masked region regression (MRR) [115], while other pretexts are defined on multiple modalities, e.g., imageconditioned masked language modelling (IMLM) [208], image-text matching (ITM) [188], video-subtitle matching (VSM) [116]. Thus, from this mathematical view, the pretext tasks can be divided into two categories, i.e., uni-modal and multimodal.', 'However, this classification is not really accurate. It should be highlighted that in multimodal pretraining Transformer models, even if the pretext objective formulations only include uni-modal elements, pretexts can still involve other modalities, essentially conditioned on the clues from other modalities, by (a) prepositive token level interactions and/or Transformer level interactions, (b) co-training with other pretexts that involve other modalities. For instance, VL-BERT [105] uses two dual pretext tasks, i.e., masked language modelling and masked RoI classification.', '(3) Motivation. If consider their motivations, the pretext tasks include masking, describing, matching, ordering, etc.', 'Some recent surveys focus on VLP and compare the existing VLP Transformer models from the angles of domain (image-text or video-text), vision feature extraction, language feature extraction, architecture (single-or dualstream), decoder (w/, w/o), pretext tasks/objectives, pretraining datasets, and down-stream tasks, e.g., Table 3 of [57], Table 2 of [58]. Different from these views, in this survey, we would propose our comparisons from some new perspectives. Specifically: (1) The core of Transformer ecosystem is self-attention, thus we would compare the existing multimodal pretraining Transformer models from the angles of how and when the self-attention or its variants perform cross-modal interactions. (2) Considering from a geometrically topological perspective, self-attention helps Transformers intrinsically work in a modality agnostic pipeline that is compatible with various modalities by taking in the embedding of each token as a node of graph, thus we would highlight that the existing VLP can be applied to other modalities, beyond visual and linguistic domains.', '(3) We suggest to treat the Transformer-based multimodal pretraining pipelines having three key components, from bottom to top, i.e., tokenization, Transformer representation, objective supervision.']","In Transformer based multimodal pretraining, the pretraining tasks/objectives are also termed pretext tasks/objectives. To date, various pretext tasks have been studied, e.g., masked language modelling (MLM) [137], masked image region prediction/classification (also termed masked object classification (MOC)) [137], [190], masked region regression (MRR) [115], visual-linguistic matching (VLM) (e.g., image-text matching (ITM) [188], image text matching (ITM), phrase-region alignment (PRA) [204], word-region alignment (WRA) [106], video-subtitle matching (VSM) [116]), masked frame modelling (MFM) [116], frame order modelling (FOM) [116], next sentence prediction (NSP) [4], [102], [190], masked sentence generation (MSG) [191], masked group modelling (MGM) [188], prefix language modelling (PrefixLM) [199], video conditioned masked language model [117], text conditioned masked frame model [117], visual translation language modelling (VTLM) [206], and image-conditioned masked language modelling (also termed image-attended masked language modelling) [207]. These down-stream task -agnostic pretext pretraining is optional, and the down-stream task objectives can be trained directly, which will be discussed in Section 4.1.2. Table 3 provides the common and representative pretext tasks for Transformer based multimodal pretraining. In practice, pretext tasks can be combined, and some representative cases are summarized in Table 3 of [57], Table 2 of [58].

The pretext tasks have multiple taxonomies:

(1) Supervision. The common multimodal pretraining Transformers use well-aligned, weakly-aligned, and even unaligned multimodal sample pairs/tuples, to work in supervised, weakly-supervised, and unsupervised manners, respectively. Meanwhile, if we consider the definitions of their pretext tasks/objectives from supervision, the pretexts can be sorted into unsupervised/self-supervised (e.g., masked language modelling (MLM) [7], [137]) and supervised (e.g., image-text matching (ITM) [188] [102], [103], [104], [106], [209]), etc. Nowadays, self-supervised attempts are the majority.

(2) Modality. Considering the mathematical formulations, some pretexts are defined on single modality, e.g., masked language modelling [7], masked acoustic modelling [200], masked region regression (MRR) [115], while other pretexts are defined on multiple modalities, e.g., imageconditioned masked language modelling (IMLM) [208], image-text matching (ITM) [188], video-subtitle matching (VSM) [116]. Thus, from this mathematical view, the pretext tasks can be divided into two categories, i.e., uni-modal and multimodal.

However, this classification is not really accurate. It should be highlighted that in multimodal pretraining Transformer models, even if the pretext objective formulations only include uni-modal elements, pretexts can still involve other modalities, essentially conditioned on the clues from other modalities, by (a) prepositive token level interactions and/or Transformer level interactions, (b) co-training with other pretexts that involve other modalities. For instance, VL-BERT [105] uses two dual pretext tasks, i.e., masked language modelling and masked RoI classification.

(3) Motivation. If consider their motivations, the pretext tasks include masking, describing, matching, ordering, etc.

Some recent surveys focus on VLP and compare the existing VLP Transformer models from the angles of domain (image-text or video-text), vision feature extraction, language feature extraction, architecture (single-or dualstream), decoder (w/, w/o), pretext tasks/objectives, pretraining datasets, and down-stream tasks, e.g., Table 3 of [57], Table 2 of [58]. Different from these views, in this survey, we would propose our comparisons from some new perspectives. Specifically: (1) The core of Transformer ecosystem is self-attention, thus we would compare the existing multimodal pretraining Transformer models from the angles of how and when the self-attention or its variants perform cross-modal interactions. (2) Considering from a geometrically topological perspective, self-attention helps Transformers intrinsically work in a modality agnostic pipeline that is compatible with various modalities by taking in the embedding of each token as a node of graph, thus we would highlight that the existing VLP can be applied to other modalities, beyond visual and linguistic domains.

(3) We suggest to treat the Transformer-based multimodal pretraining pipelines having three key components, from bottom to top, i.e., tokenization, Transformer representation, objective supervision.","(p29.0) In Transformer based multimodal pretraining, the pretraining tasks/objectives are also termed pretext tasks/objectives. To date, various pretext tasks have been studied, e.g., masked language modelling (MLM) [137], masked image region prediction/classification (also termed masked object classification (MOC)) [137], [190], masked region regression (MRR) [115], visual-linguistic matching (VLM) (e.g., image-text matching (ITM) [188], image text matching (ITM), phrase-region alignment (PRA) [204], word-region alignment (WRA) [106], video-subtitle matching (VSM) [116]), masked frame modelling (MFM) [116], frame order modelling (FOM) [116], next sentence prediction (NSP) [4], [102], [190], masked sentence generation (MSG) [191], masked group modelling (MGM) [188], prefix language modelling (PrefixLM) [199], video conditioned masked language model [117], text conditioned masked frame model [117], visual translation language modelling (VTLM) [206], and image-conditioned masked language modelling (also termed image-attended masked language modelling) [207]. These down-stream task -agnostic pretext pretraining is optional, and the down-stream task objectives can be trained directly, which will be discussed in Section 4.1.2. Table 3 provides the common and representative pretext tasks for Transformer based multimodal pretraining. In practice, pretext tasks can be combined, and some representative cases are summarized in Table 3 of [57], Table 2 of [58].

(p29.1) The pretext tasks have multiple taxonomies:

(p29.2) (1) Supervision. The common multimodal pretraining Transformers use well-aligned, weakly-aligned, and even unaligned multimodal sample pairs/tuples, to work in supervised, weakly-supervised, and unsupervised manners, respectively. Meanwhile, if we consider the definitions of their pretext tasks/objectives from supervision, the pretexts can be sorted into unsupervised/self-supervised (e.g., masked language modelling (MLM) [7], [137]) and supervised (e.g., image-text matching (ITM) [188] [102], [103], [104], [106], [209]), etc. Nowadays, self-supervised attempts are the majority.

(p29.3) (2) Modality. Considering the mathematical formulations, some pretexts are defined on single modality, e.g., masked language modelling [7], masked acoustic modelling [200], masked region regression (MRR) [115], while other pretexts are defined on multiple modalities, e.g., imageconditioned masked language modelling (IMLM) [208], image-text matching (ITM) [188], video-subtitle matching (VSM) [116]. Thus, from this mathematical view, the pretext tasks can be divided into two categories, i.e., uni-modal and multimodal.

(p29.4) However, this classification is not really accurate. It should be highlighted that in multimodal pretraining Transformer models, even if the pretext objective formulations only include uni-modal elements, pretexts can still involve other modalities, essentially conditioned on the clues from other modalities, by (a) prepositive token level interactions and/or Transformer level interactions, (b) co-training with other pretexts that involve other modalities. For instance, VL-BERT [105] uses two dual pretext tasks, i.e., masked language modelling and masked RoI classification.

(p29.5) (3) Motivation. If consider their motivations, the pretext tasks include masking, describing, matching, ordering, etc.

(p29.6) Some recent surveys focus on VLP and compare the existing VLP Transformer models from the angles of domain (image-text or video-text), vision feature extraction, language feature extraction, architecture (single-or dualstream), decoder (w/, w/o), pretext tasks/objectives, pretraining datasets, and down-stream tasks, e.g., Table 3 of [57], Table 2 of [58]. Different from these views, in this survey, we would propose our comparisons from some new perspectives. Specifically: (1) The core of Transformer ecosystem is self-attention, thus we would compare the existing multimodal pretraining Transformer models from the angles of how and when the self-attention or its variants perform cross-modal interactions. (2) Considering from a geometrically topological perspective, self-attention helps Transformers intrinsically work in a modality agnostic pipeline that is compatible with various modalities by taking in the embedding of each token as a node of graph, thus we would highlight that the existing VLP can be applied to other modalities, beyond visual and linguistic domains.

(p29.7) (3) We suggest to treat the Transformer-based multimodal pretraining pipelines having three key components, from bottom to top, i.e., tokenization, Transformer representation, objective supervision.","[['b189', 'b136', 'b101', 'b57', 'b205', 'b198', 'b206', 'b114', 'b203', 'b190', 'b56', 'b115', 'b116', 'b105', 'b3', 'b187'], [], ['b136', 'b6', 'b102', 'b208', 'b105', 'b103', 'b187'], ['b199', 'b6', 'b114', 'b207', 'b115', 'b187'], ['b104'], [], ['b57', 'b56'], []]","[['b189', 'b136', 'b101', 'b57', 'b205', 'b198', 'b206', 'b114', 'b203', 'b190', 'b56', 'b115', 'b116', 'b105', 'b3', 'b187'], [], ['b136', 'b6', 'b102', 'b208', 'b105', 'b103', 'b187'], ['b199', 'b6', 'b114', 'b207', 'b115', 'b187'], ['b104'], [], ['b57', 'b56'], []]",32,"1. In Transformer based multimodal pretraining, the pretraining tasks/objectives are also termed pretext tasks/objectives.
2. To date, various pretext tasks have been studied, e.g., masked language modelling (MLM) [137], masked image region prediction/classification (also termed masked object classification (MOC)) [137], (also termed masked object classification (MOC)4, masked region regression (MRR) [115], visual-linguistic matching (VLM) (e.g., image-text matching (ITM) [188], image text matching (ITM), phrase-region alignment (PRA) [204], word-region alignment (WRA) [106], video-subtitle matching (VSM) [116]), masked frame modelling (MFM) [116], frame order modelling (FOM) [116], next sentence prediction (also termed masked object classification (MOC)1 (also termed masked object classification (MOC)2, [102], (also termed masked object classification (MOC)4, masked sentence generation (also termed masked object classification (MOC)5 (also termed masked object classification (MOC)6, masked group modelling (also termed masked object classification (MOC)7 [188], prefix language modelling (also termed masked object classification (MOC)9 [199], video conditioned masked language model [117], text conditioned masked frame model [117], visual translation language modelling (VTLM) [206], and image-conditioned masked language modelling (also termed image-attended masked language modelling) [207].
3. These down-stream task -agnostic pretext pretraining is optional, and the down-stream task objectives can be trained directly, which will be discussed in Section 4.1.2.
4. Table 3 provides the common and representative pretext tasks for Transformer based multimodal pretraining.
5. In practice, pretext tasks can be combined, and some representative cases are summarized in Table 3 of [57], Table 2 of [58].
6. The pretext tasks have multiple taxonomies:(VLM)0 Supervision.
7. The common multimodal pretraining Transformers use well-aligned, weakly-aligned, and even unaligned multimodal sample pairs/tuples, to work in supervised, weakly-supervised, and unsupervised manners, respectively.
8. Meanwhile, if we consider the definitions of their pretext tasks/objectives from supervision, the pretexts can be sorted into unsupervised/self-supervised (e.g., masked language modelling (MLM) [7], [137]) and supervised (e.g., image-text matching (ITM) [188] [102], [103], [104], [106], [209]), etc. Nowadays, self-supervised attempts are the majority.
9. (VLM)1 Modality. Considering the mathematical formulations, some pretexts are defined on single modality, e.g., masked language modelling [7], masked acoustic modelling [200], masked region regression (MRR) [115], while other pretexts are defined on multiple modalities, e.g., imageconditioned masked language modelling (IMLM) [208], image-text matching (ITM) [188], video-subtitle matching (VSM) [116].
10. Thus, from this mathematical view, the pretext tasks can be divided into two categories, i.e., uni-modal and multimodal.
11. However, this classification is not really accurate.
12. It should be highlighted that in multimodal pretraining Transformer models, even if the pretext objective formulations only include uni-modal elements, pretexts can still involve other modalities, essentially conditioned on the clues from other modalities, by (a) prepositive token level interactions and/or Transformer level interactions, (b) co-training with other pretexts that involve other modalities.
13. For instance, VL-BERT [105] uses two dual pretext tasks, i.e., masked language modelling and masked RoI classification.
14. (VLM)2 Motivation. If consider their motivations, the pretext tasks include masking, describing, matching, ordering, etc.Some recent surveys focus on VLP and compare the existing VLP Transformer models from the angles of domain (image-text or video-text), vision feature extraction, language feature extraction, architecture (single-or dualstream), decoder (w/, w/o), pretext tasks/objectives, pretraining datasets, and down-stream tasks, e.g., Table 3 of [57], Table 2 of [58].
15. Different from these views, in this survey, we would propose our comparisons from some new perspectives.
16. Specifically: (VLM)0 The core of Transformer ecosystem is self-attention, thus we would compare the existing multimodal pretraining Transformer models from the angles of how and when the self-attention or its variants perform cross-modal interactions.
17. (VLM)1 Considering from a geometrically topological perspective, self-attention helps Transformers intrinsically work in a modality agnostic pipeline that is compatible with various modalities by taking in the embedding of each token as a node of graph, thus we would highlight that the existing VLP can be applied to other modalities, beyond visual and linguistic domains.
18. (VLM)2 We suggest to treat the Transformer-based multimodal pretraining pipelines having three key components, from bottom to top, i.e., tokenization, Transformer representation, objective supervision.","What are the different types of supervision methods used in Transformer-based multimodal pretraining and how do they impact the training process?
How do pretext tasks in Transformer-based multimodal learning differ in their approach to handling single vs. multiple modalities?
In what ways do the motivations behind various pretext tasks in multimodal learning with Transformers influence their design and implementation?
How does the self-attention mechanism in Transformers facilitate cross-modal interactions in multimodal pretraining?
What are the three key components of Transformer-based multimodal pretraining pipelines and how do they contribute to the learning process?","1. What are the different types of supervision methods used in Transformer-based multimodal pretraining and how do they impact the training process?
2. How do pretext tasks in Transformer-based multimodal learning differ in their approach to handling single vs. multiple modalities?
3. In what ways do the motivations behind various pretext tasks in multimodal learning with Transformers influence their design and implementation?
4. How does the self-attention mechanism in Transformers facilitate cross-modal interactions in multimodal pretraining?
5. What are the three key components of Transformer-based multimodal pretraining pipelines and how do they contribute to the learning process?",None,,"Questions:

1. What are the different types of supervision methods used in Transformer-based multimodal pretraining and how do they impact the training process?
2. How do pretext tasks in Transformer-based multimodal learning differ in their approach to handling single vs. multiple modalities?
3. In what ways do the motivations behind various pretext tasks in multimodal learning with Transformers influence their design and implementation?
4. How does the self-attention mechanism in Transformers facilitate cross-modal interactions in multimodal pretraining?
5. What are the three key components of Transformer-based multimodal pretraining pipelines and how do they contribute to the learning process?

Answer:

(p29.0) In Transformer based multimodal pretraining, the pretraining tasks/objectives are also termed pretext tasks/objectives. To date, various pretext tasks have been studied, e.g., masked language modelling (MLM) [137], masked image region prediction/classification (also termed masked object classification (MOC)) [137], [190], masked region regression (MRR) [115], visual-linguistic matching (VLM) (e.g., image-text matching (ITM) [188], image text matching (ITM), phrase-region alignment (PRA) [204], word-region alignment (WRA) [106], video-subtitle matching (VSM) [116]), masked frame modelling (MFM) [116], frame order modelling (FOM) [116], next sentence prediction (NSP) [4], [102], [190], masked sentence generation (MSG) [191], masked group modelling (MGM) [188], prefix language modelling (PrefixLM) [199], video conditioned masked language model [117], text conditioned masked frame model [117], visual translation language modelling (VTLM) [206], and image-conditioned masked language modelling (also termed image-attended masked language modelling) [207]. These down-stream task -agnostic pretext pretraining is optional, and the down-stream task objectives can be trained directly, which will be discussed in Section 4.1.2. Table 3 provides the common and representative pretext tasks for Transformer based multimodal pretraining. In practice, pretext tasks can be combined, and some representative cases are summarized in Table 3 of [57], Table 2 of [58].

(p29.1) The pretext tasks have multiple taxonomies:

(p29.2) (1) Supervision. The common multimodal pretraining Transformers use well-aligned, weakly-aligned, and even unaligned multimodal sample pairs/tuples, to work in supervised, weakly-supervised, and unsupervised manners, respectively. Meanwhile, if we consider the definitions of their pretext tasks/objectives from supervision, the pretexts can be sorted into unsupervised/self-supervised (e.g., masked language modelling (MLM) [7], [137]) and supervised (e.g., image-text matching (ITM) [188] [102], [103], [104], [106], [209]), etc. Nowadays, self-supervised attempts are the majority.

(p29.3) (2) Modality. Considering the mathematical formulations, some pretexts are defined on single modality, e.g., masked language modelling [7], masked acoustic modelling [200], masked region regression (MRR) [115], while other pretexts are defined on multiple modalities, e.g., imageconditioned masked language modelling (IMLM) [208], image-text matching (ITM) [188], video-subtitle matching (VSM) [116]. Thus, from this mathematical view, the pretext tasks can be divided into two categories, i.e., uni-modal and multimodal.

(p29.4) However, this classification is not really accurate. It should be highlighted that in multimodal pretraining Transformer models, even if the pretext objective formulations only include uni-modal elements, pretexts can still involve other modalities, essentially conditioned on the clues from other modalities, by (a) prepositive token level interactions and/or Transformer level interactions, (b) co-training with other pretexts that involve other modalities. For instance, VL-BERT [105] uses two dual pretext tasks, i.e., masked language modelling and masked RoI classification.

(p29.5) (3) Motivation. If consider their motivations, the pretext tasks include masking, describing, matching, ordering, etc.

(p29.6) Some recent surveys focus on VLP and compare the existing VLP Transformer models from the angles of domain (image-text or video-text), vision feature extraction, language feature extraction, architecture (single-or dualstream), decoder (w/, w/o), pretext tasks/objectives, pretraining datasets, and down-stream tasks, e.g., Table 3 of [57], Table 2 of [58]. Different from these views, in this survey, we would propose our comparisons from some new perspectives. Specifically: (1) The core of Transformer ecosystem is self-attention, thus we would compare the existing multimodal pretraining Transformer models from the angles of how and when the self-attention or its variants perform cross-modal interactions. (2) Considering from a geometrically topological perspective, self-attention helps Transformers intrinsically work in a modality agnostic pipeline that is compatible with various modalities by taking in the embedding of each token as a node of graph, thus we would highlight that the existing VLP can be applied to other modalities, beyond visual and linguistic domains.

(p29.7) (3) We suggest to treat the Transformer-based multimodal pretraining pipelines having three key components, from bottom to top, i.e., tokenization, Transformer representation, objective supervision.","Questions:



Answer:

(p29.0) In Transformer based multimodal pretraining, the pretraining tasks/objectives are also termed pretext tasks/objectives. To date, various pretext tasks have been studied, e.g., masked language modelling (MLM) [137], masked image region prediction/classification (also termed masked object classification (MOC)) [137], [190], masked region regression (MRR) [115], visual-linguistic matching (VLM) (e.g., image-text matching (ITM) [188], image text matching (ITM), phrase-region alignment (PRA) [204], word-region alignment (WRA) [106], video-subtitle matching (VSM) [116]), masked frame modelling (MFM) [116], frame order modelling (FOM) [116], next sentence prediction (NSP) [4], [102], [190], masked sentence generation (MSG) [191], masked group modelling (MGM) [188], prefix language modelling (PrefixLM) [199], video conditioned masked language model [117], text conditioned masked frame model [117], visual translation language modelling (VTLM) [206], and image-conditioned masked language modelling (also termed image-attended masked language modelling) [207]. These down-stream task -agnostic pretext pretraining is optional, and the down-stream task objectives can be trained directly, which will be discussed in Section 4.1.2. Table 3 provides the common and representative pretext tasks for Transformer based multimodal pretraining. In practice, pretext tasks can be combined, and some representative cases are summarized in Table 3 of [57], Table 2 of [58].

(p29.1) The pretext tasks have multiple taxonomies:

(p29.2) (1) Supervision. The common multimodal pretraining Transformers use well-aligned, weakly-aligned, and even unaligned multimodal sample pairs/tuples, to work in supervised, weakly-supervised, and unsupervised manners, respectively. Meanwhile, if we consider the definitions of their pretext tasks/objectives from supervision, the pretexts can be sorted into unsupervised/self-supervised (e.g., masked language modelling (MLM) [7], [137]) and supervised (e.g., image-text matching (ITM) [188] [102], [103], [104], [106], [209]), etc. Nowadays, self-supervised attempts are the majority.

(p29.3) (2) Modality. Considering the mathematical formulations, some pretexts are defined on single modality, e.g., masked language modelling [7], masked acoustic modelling [200], masked region regression (MRR) [115], while other pretexts are defined on multiple modalities, e.g., imageconditioned masked language modelling (IMLM) [208], image-text matching (ITM) [188], video-subtitle matching (VSM) [116]. Thus, from this mathematical view, the pretext tasks can be divided into two categories, i.e., uni-modal and multimodal.

(p29.4) However, this classification is not really accurate. It should be highlighted that in multimodal pretraining Transformer models, even if the pretext objective formulations only include uni-modal elements, pretexts can still involve other modalities, essentially conditioned on the clues from other modalities, by (a) prepositive token level interactions and/or Transformer level interactions, (b) co-training with other pretexts that involve other modalities. For instance, VL-BERT [105] uses two dual pretext tasks, i.e., masked language modelling and masked RoI classification.

(p29.5) (3) Motivation. If consider their motivations, the pretext tasks include masking, describing, matching, ordering, etc.

(p29.6) Some recent surveys focus on VLP and compare the existing VLP Transformer models from the angles of domain (image-text or video-text), vision feature extraction, language feature extraction, architecture (single-or dualstream), decoder (w/, w/o), pretext tasks/objectives, pretraining datasets, and down-stream tasks, e.g., Table 3 of [57], Table 2 of [58]. Different from these views, in this survey, we would propose our comparisons from some new perspectives. Specifically: (1) The core of Transformer ecosystem is self-attention, thus we would compare the existing multimodal pretraining Transformer models from the angles of how and when the self-attention or its variants perform cross-modal interactions. (2) Considering from a geometrically topological perspective, self-attention helps Transformers intrinsically work in a modality agnostic pipeline that is compatible with various modalities by taking in the embedding of each token as a node of graph, thus we would highlight that the existing VLP can be applied to other modalities, beyond visual and linguistic domains.

(p29.7) (3) We suggest to treat the Transformer-based multimodal pretraining pipelines having three key components, from bottom to top, i.e., tokenization, Transformer representation, objective supervision."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s30,Discussion,"['p30.0', 'p30.1', 'p30.2']","['In spite of the recent advances, multimodal pretraining Transformer methods still have some obvious bottlenecks. For instance, as discussed by [208] in VLP field, while the BERT-style cross-modal pretraining models produce excellent results on various down-stream visionlanguage tasks, they fail to be applied to generative tasks directly. As discussed in [208], both VideoBERT [7] and CBT [107] have to train a separate video-to-text decoder for video captioning. This is a significant gap between the pretraining models designed for discriminative and generative tasks, as the main reason is discriminative task oriented pretraining models do not involve the decoders of Transformer. Therefore, how to design more unified pipelines that can work for both discriminative and generative down-stream tasks is also an open problem to be solved. Again for instance, common multimodal pretraining models often underperform for fine-grained/instance-level tasks as discussed by [137].', 'Discussion As discussed in [208], the masked language and region modelling as pre-training task have a main advantage that the Transformer encoder learned from these supervisions can encode both vision and language patterns based on bidirectional context and it is naturally fit for the semantic understanding tasks, e.g., VQA, image-text retrieval.', 'Discussion How to boost the performance for multimodal pretraining Transformers is an open problem. Some practices demonstrate that multi-task training (by adding auxiliary loss) [111], [137] and adversarial training [210] improve multimodal pretraining Transformers to further boost the performance. Meanwhile, overly compound pretraining objectives potentially upgrade the challenge of balancing among different loss terms, thus complicate the training optimization [199]. Moreover, the difficulty of the pretexts is also worth discussing. In general, if aim to learn more explicit object concepts, more complex pretext losses will be used [204]. However, for pretexts, whether more complexity is better remains a question.']","In spite of the recent advances, multimodal pretraining Transformer methods still have some obvious bottlenecks. For instance, as discussed by [208] in VLP field, while the BERT-style cross-modal pretraining models produce excellent results on various down-stream visionlanguage tasks, they fail to be applied to generative tasks directly. As discussed in [208], both VideoBERT [7] and CBT [107] have to train a separate video-to-text decoder for video captioning. This is a significant gap between the pretraining models designed for discriminative and generative tasks, as the main reason is discriminative task oriented pretraining models do not involve the decoders of Transformer. Therefore, how to design more unified pipelines that can work for both discriminative and generative down-stream tasks is also an open problem to be solved. Again for instance, common multimodal pretraining models often underperform for fine-grained/instance-level tasks as discussed by [137].

Discussion As discussed in [208], the masked language and region modelling as pre-training task have a main advantage that the Transformer encoder learned from these supervisions can encode both vision and language patterns based on bidirectional context and it is naturally fit for the semantic understanding tasks, e.g., VQA, image-text retrieval.

Discussion How to boost the performance for multimodal pretraining Transformers is an open problem. Some practices demonstrate that multi-task training (by adding auxiliary loss) [111], [137] and adversarial training [210] improve multimodal pretraining Transformers to further boost the performance. Meanwhile, overly compound pretraining objectives potentially upgrade the challenge of balancing among different loss terms, thus complicate the training optimization [199]. Moreover, the difficulty of the pretexts is also worth discussing. In general, if aim to learn more explicit object concepts, more complex pretext losses will be used [204]. However, for pretexts, whether more complexity is better remains a question.","(p30.0) In spite of the recent advances, multimodal pretraining Transformer methods still have some obvious bottlenecks. For instance, as discussed by [208] in VLP field, while the BERT-style cross-modal pretraining models produce excellent results on various down-stream visionlanguage tasks, they fail to be applied to generative tasks directly. As discussed in [208], both VideoBERT [7] and CBT [107] have to train a separate video-to-text decoder for video captioning. This is a significant gap between the pretraining models designed for discriminative and generative tasks, as the main reason is discriminative task oriented pretraining models do not involve the decoders of Transformer. Therefore, how to design more unified pipelines that can work for both discriminative and generative down-stream tasks is also an open problem to be solved. Again for instance, common multimodal pretraining models often underperform for fine-grained/instance-level tasks as discussed by [137].

(p30.1) Discussion As discussed in [208], the masked language and region modelling as pre-training task have a main advantage that the Transformer encoder learned from these supervisions can encode both vision and language patterns based on bidirectional context and it is naturally fit for the semantic understanding tasks, e.g., VQA, image-text retrieval.

(p30.2) Discussion How to boost the performance for multimodal pretraining Transformers is an open problem. Some practices demonstrate that multi-task training (by adding auxiliary loss) [111], [137] and adversarial training [210] improve multimodal pretraining Transformers to further boost the performance. Meanwhile, overly compound pretraining objectives potentially upgrade the challenge of balancing among different loss terms, thus complicate the training optimization [199]. Moreover, the difficulty of the pretexts is also worth discussing. In general, if aim to learn more explicit object concepts, more complex pretext losses will be used [204]. However, for pretexts, whether more complexity is better remains a question.","[['b136', 'b106', 'b207', 'b6'], ['b207'], ['b136', 'b110', 'b198', 'b203', 'b209']]","[['b136', 'b106', 'b207', 'b6'], ['b207'], ['b136', 'b110', 'b198', 'b203', 'b209']]",10,"1. In spite of the recent advances, multimodal pretraining Transformer methods still have some obvious bottlenecks.
2. For instance, as discussed by [208] in VLP field, while the BERT-style cross-modal pretraining models produce excellent results on various down-stream visionlanguage tasks, they fail to be applied to generative tasks directly.
3. As discussed in [208], both VideoBERT [7] and CBT [107] have to train a separate video-to-text decoder for video captioning.
4. This is a significant gap between the pretraining models designed for discriminative and generative tasks, as the main reason is discriminative task oriented pretraining models do not involve the decoders of Transformer.
5. Therefore, how to design more unified pipelines that can work for both discriminative and generative down-stream tasks is also an open problem to be solved.
6. Again for instance, common multimodal pretraining models often underperform for fine-grained/instance-level tasks as discussed by [137].Discussion As discussed in [208], the masked language and region modelling as pre-training task have a main advantage that the Transformer encoder learned from these supervisions can encode both vision and language patterns based on bidirectional context and it is naturally fit for the semantic understanding tasks, e.g., VQA, image-text retrieval.
7. Discussion How to boost the performance for multimodal pretraining Transformers is an open problem.
8. Some practices demonstrate that multi-task training (by adding auxiliary loss) [111], [137] and adversarial training [210] improve multimodal pretraining Transformers to further boost the performance.
9. Meanwhile, overly compound pretraining objectives potentially upgrade the challenge of balancing among different loss terms, thus complicate the training optimization [199].
10. Moreover, the difficulty of the pretexts is also worth discussing.
11. In general, if aim to learn more explicit object concepts, more complex pretext losses will be used [204].
12. However, for pretexts, whether more complexity is better remains a question.","What are the main bottlenecks in multimodal pretraining Transformer methods for generative tasks as identified in recent studies?
How do BERT-style cross-modal pretraining models perform on generative tasks compared to discriminative tasks?
What strategies have been proposed to bridge the gap between pretraining models for discriminative and generative tasks in the context of multimodal learning?
How does the complexity of pretext losses in multimodal pretraining affect the learning of explicit object concepts?
What are the benefits and challenges of incorporating multi-task and adversarial training in enhancing the performance of multimodal pretraining Transformers?","1. What are the main bottlenecks in multimodal pretraining Transformer methods for generative tasks as identified in recent studies?
2. How do BERT-style cross-modal pretraining models perform on generative tasks compared to discriminative tasks?
3. What strategies have been proposed to bridge the gap between pretraining models for discriminative and generative tasks in the context of multimodal learning?
4. How does the complexity of pretext losses in multimodal pretraining affect the learning of explicit object concepts?
5. What are the benefits and challenges of incorporating multi-task and adversarial training in enhancing the performance of multimodal pretraining Transformers?",None,,"Questions:

1. What are the main bottlenecks in multimodal pretraining Transformer methods for generative tasks as identified in recent studies?
2. How do BERT-style cross-modal pretraining models perform on generative tasks compared to discriminative tasks?
3. What strategies have been proposed to bridge the gap between pretraining models for discriminative and generative tasks in the context of multimodal learning?
4. How does the complexity of pretext losses in multimodal pretraining affect the learning of explicit object concepts?
5. What are the benefits and challenges of incorporating multi-task and adversarial training in enhancing the performance of multimodal pretraining Transformers?

Answer:

(p30.0) In spite of the recent advances, multimodal pretraining Transformer methods still have some obvious bottlenecks. For instance, as discussed by [208] in VLP field, while the BERT-style cross-modal pretraining models produce excellent results on various down-stream visionlanguage tasks, they fail to be applied to generative tasks directly. As discussed in [208], both VideoBERT [7] and CBT [107] have to train a separate video-to-text decoder for video captioning. This is a significant gap between the pretraining models designed for discriminative and generative tasks, as the main reason is discriminative task oriented pretraining models do not involve the decoders of Transformer. Therefore, how to design more unified pipelines that can work for both discriminative and generative down-stream tasks is also an open problem to be solved. Again for instance, common multimodal pretraining models often underperform for fine-grained/instance-level tasks as discussed by [137].

(p30.1) Discussion As discussed in [208], the masked language and region modelling as pre-training task have a main advantage that the Transformer encoder learned from these supervisions can encode both vision and language patterns based on bidirectional context and it is naturally fit for the semantic understanding tasks, e.g., VQA, image-text retrieval.

(p30.2) Discussion How to boost the performance for multimodal pretraining Transformers is an open problem. Some practices demonstrate that multi-task training (by adding auxiliary loss) [111], [137] and adversarial training [210] improve multimodal pretraining Transformers to further boost the performance. Meanwhile, overly compound pretraining objectives potentially upgrade the challenge of balancing among different loss terms, thus complicate the training optimization [199]. Moreover, the difficulty of the pretexts is also worth discussing. In general, if aim to learn more explicit object concepts, more complex pretext losses will be used [204]. However, for pretexts, whether more complexity is better remains a question.","Questions:



Answer:

(p30.0) In spite of the recent advances, multimodal pretraining Transformer methods still have some obvious bottlenecks. For instance, as discussed by [208] in VLP field, while the BERT-style cross-modal pretraining models produce excellent results on various down-stream visionlanguage tasks, they fail to be applied to generative tasks directly. As discussed in [208], both VideoBERT [7] and CBT [107] have to train a separate video-to-text decoder for video captioning. This is a significant gap between the pretraining models designed for discriminative and generative tasks, as the main reason is discriminative task oriented pretraining models do not involve the decoders of Transformer. Therefore, how to design more unified pipelines that can work for both discriminative and generative down-stream tasks is also an open problem to be solved. Again for instance, common multimodal pretraining models often underperform for fine-grained/instance-level tasks as discussed by [137].

(p30.1) Discussion As discussed in [208], the masked language and region modelling as pre-training task have a main advantage that the Transformer encoder learned from these supervisions can encode both vision and language patterns based on bidirectional context and it is naturally fit for the semantic understanding tasks, e.g., VQA, image-text retrieval.

(p30.2) Discussion How to boost the performance for multimodal pretraining Transformers is an open problem. Some practices demonstrate that multi-task training (by adding auxiliary loss) [111], [137] and adversarial training [210] improve multimodal pretraining Transformers to further boost the performance. Meanwhile, overly compound pretraining objectives potentially upgrade the challenge of balancing among different loss terms, thus complicate the training optimization [199]. Moreover, the difficulty of the pretexts is also worth discussing. In general, if aim to learn more explicit object concepts, more complex pretext losses will be used [204]. However, for pretexts, whether more complexity is better remains a question."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s31,Task-Specific Multimodal Pretraining,"['p31.0', 'p31.1', 'p31.2']","['In practices of multimodal Transformers, the aforementioned down-stream task -agnostic pretraining is optional, not necessary, and down-stream task specific pretraining is also widely studied [150], [190], [208], [211]. The main reasons include: (1) Limited by the existing technique, it is extremely difficult to design a set of highly universal network architectures, pretext tasks, and corpora that work for all the various down-stream applications. (2) There are nonnegligible gaps among various down-stream applications, e.g., task logic, data form, making it difficult to transfer from pretraining to down-stream applications.', 'Therefore, a large number of down-stream tasks still need tailor-made pretraining to improve the performance. Guhur et al. [150] propose in-domain pretraining for visionand-language navigation, as the general VLP focuses on TABLE 3 Pretext task comparison of multi-modal pretraining Transformer models (for agnostic down-stream tasks). ""C-M Loss"": cross-modal loss; ""Con.', 'Loss"": loss conditioned on other modality/modalities.']","In practices of multimodal Transformers, the aforementioned down-stream task -agnostic pretraining is optional, not necessary, and down-stream task specific pretraining is also widely studied [150], [190], [208], [211]. The main reasons include: (1) Limited by the existing technique, it is extremely difficult to design a set of highly universal network architectures, pretext tasks, and corpora that work for all the various down-stream applications. (2) There are nonnegligible gaps among various down-stream applications, e.g., task logic, data form, making it difficult to transfer from pretraining to down-stream applications.

Therefore, a large number of down-stream tasks still need tailor-made pretraining to improve the performance. Guhur et al. [150] propose in-domain pretraining for visionand-language navigation, as the general VLP focuses on TABLE 3 Pretext task comparison of multi-modal pretraining Transformer models (for agnostic down-stream tasks). ""C-M Loss"": cross-modal loss; ""Con.

Loss"": loss conditioned on other modality/modalities.","(p31.0) In practices of multimodal Transformers, the aforementioned down-stream task -agnostic pretraining is optional, not necessary, and down-stream task specific pretraining is also widely studied [150], [190], [208], [211]. The main reasons include: (1) Limited by the existing technique, it is extremely difficult to design a set of highly universal network architectures, pretext tasks, and corpora that work for all the various down-stream applications. (2) There are nonnegligible gaps among various down-stream applications, e.g., task logic, data form, making it difficult to transfer from pretraining to down-stream applications.

(p31.1) Therefore, a large number of down-stream tasks still need tailor-made pretraining to improve the performance. Guhur et al. [150] propose in-domain pretraining for visionand-language navigation, as the general VLP focuses on TABLE 3 Pretext task comparison of multi-modal pretraining Transformer models (for agnostic down-stream tasks). ""C-M Loss"": cross-modal loss; ""Con.

(p31.2) Loss"": loss conditioned on other modality/modalities.","[['b189', 'b210', 'b207', 'b149'], [None, 'b149'], []]","[['b189', 'b210', 'b207', 'b149'], [None, 'b149'], []]",6,"1. In practices of multimodal Transformers, the aforementioned down-stream task -agnostic pretraining is optional, not necessary, and down-stream task specific pretraining is also widely studied [150], [190], [208], [211].
2. The main reasons include: (1) Limited by the existing technique, it is extremely difficult to design a set of highly universal network architectures, pretext tasks, and corpora that work for all the various down-stream applications.
3. (2) There are nonnegligible gaps among various down-stream applications, e.g., task logic, data form, making it difficult to transfer from pretraining to down-stream applications.
4. Therefore, a large number of down-stream tasks still need tailor-made pretraining to improve the performance.
5. Guhur et al. [150] propose in-domain pretraining for visionand-language navigation, as the general VLP focuses on TABLE 3 Pretext task comparison of multi-modal pretraining Transformer models (for agnostic down-stream tasks).
6. ""C-M Loss"": cross-modal loss; ""
7. Con.Loss"": loss conditioned on other modality/modalities.","What are the main challenges in designing a universal network architecture for multimodal Transformers that is effective across various down-stream applications?
How do the specific requirements of down-stream tasks affect the transferability of general pretraining in multimodal learning?
Why is in-domain pretraining considered necessary for certain down-stream tasks in multimodal learning, such as vision-and-language navigation?
What role does cross-modal loss play in the pretraining of multimodal Transformer models for agnostic down-stream tasks?
How does the concept of loss conditioned on other modalities enhance the effectiveness of multimodal pretraining strategies?","1. What are the main challenges in designing a universal network architecture for multimodal Transformers that is effective across various down-stream applications?
2. How do the specific requirements of down-stream tasks affect the transferability of general pretraining in multimodal learning?
3. Why is in-domain pretraining considered necessary for certain down-stream tasks in multimodal learning, such as vision-and-language navigation?
4. What role does cross-modal loss play in the pretraining of multimodal Transformer models for agnostic down-stream tasks?
5. How does the concept of loss conditioned on other modalities enhance the effectiveness of multimodal pretraining strategies?",None,,"Questions:

1. What are the main challenges in designing a universal network architecture for multimodal Transformers that is effective across various down-stream applications?
2. How do the specific requirements of down-stream tasks affect the transferability of general pretraining in multimodal learning?
3. Why is in-domain pretraining considered necessary for certain down-stream tasks in multimodal learning, such as vision-and-language navigation?
4. What role does cross-modal loss play in the pretraining of multimodal Transformer models for agnostic down-stream tasks?
5. How does the concept of loss conditioned on other modalities enhance the effectiveness of multimodal pretraining strategies?

Answer:

(p31.0) In practices of multimodal Transformers, the aforementioned down-stream task -agnostic pretraining is optional, not necessary, and down-stream task specific pretraining is also widely studied [150], [190], [208], [211]. The main reasons include: (1) Limited by the existing technique, it is extremely difficult to design a set of highly universal network architectures, pretext tasks, and corpora that work for all the various down-stream applications. (2) There are nonnegligible gaps among various down-stream applications, e.g., task logic, data form, making it difficult to transfer from pretraining to down-stream applications.

(p31.1) Therefore, a large number of down-stream tasks still need tailor-made pretraining to improve the performance. Guhur et al. [150] propose in-domain pretraining for visionand-language navigation, as the general VLP focuses on TABLE 3 Pretext task comparison of multi-modal pretraining Transformer models (for agnostic down-stream tasks). ""C-M Loss"": cross-modal loss; ""Con.

(p31.2) Loss"": loss conditioned on other modality/modalities.","Questions:



Answer:

(p31.0) In practices of multimodal Transformers, the aforementioned down-stream task -agnostic pretraining is optional, not necessary, and down-stream task specific pretraining is also widely studied [150], [190], [208], [211]. The main reasons include: (1) Limited by the existing technique, it is extremely difficult to design a set of highly universal network architectures, pretext tasks, and corpora that work for all the various down-stream applications. (2) There are nonnegligible gaps among various down-stream applications, e.g., task logic, data form, making it difficult to transfer from pretraining to down-stream applications.

(p31.1) Therefore, a large number of down-stream tasks still need tailor-made pretraining to improve the performance. Guhur et al. [150] propose in-domain pretraining for visionand-language navigation, as the general VLP focuses on TABLE 3 Pretext task comparison of multi-modal pretraining Transformer models (for agnostic down-stream tasks). ""C-M Loss"": cross-modal loss; ""Con.

(p31.2) Loss"": loss conditioned on other modality/modalities."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s32,Types (Motivations) Tasks C-M Loss Con. Loss References,['p32.0'],"['Masking Masked Language Modelling (MLM) [7], [137] Image-Conditioned Masked Language Modelling (IMLM) [206], [207] [208] Text-Conditioned Masked Region Prediction [206] Masked Acoustic Modelling [180], [200] Masked Image Region Regression [115] Masked Image Region Prediction [190] Masked Frame Modelling (MFM) [116] Masked Sentence Generation (MSG) [191] Video Conditioned Masked Language Model [117] Text Conditioned Masked Frame Model [117] Describing Image-conditioned Denoising Autoencoding (IDA) [208] Text-conditioned Image Feature Generation (TIFG) [208] Prefix Language Modelling (PrefixLM) [199] Matching Image-Text Matching (ITM) [188] [102], [103], [104], [106], [209], Phrase-Region Alignment (PRA) [204] Word-Region Alignment (WRA) [106], [192] Video-Subtitle Matching (VSM) [116] Next Sentence Prediction (NSP) [4], [102], [190] Ordering Sentence Ordering Modelling (SOM) [201] Frame Ordering Modelling (FOM) [116] learning vision-language correlations, not designed for sequential decision making as required in embodied VLN. Murahari et al. [190] present a visual dialogue oriented approach to leverage pretraining on general vision-language datasets. XGPT [208] is tailor-made for image captioning, to overcome the limitation that BERT-based cross-modal pretrained models fail to be applied to generative tasks directly. ERNIE-ViLG [211] is designed for bidirectional image-text generation with Transformers. Special modalities have their own unique domain knowledge that can be used to design the specific pretrain pretexts. GraphCodeBERT [44] uses two structure-aware pretext tasks (i.e., predict where a variable is identified from, data flow edge prediction between variables) for programming source code. To learn from the spatial cues in 360 • video, Morgado et al. [145] propose to perform contrastive audio-visual spatial alignment of 360 • video and spatial audio. Med-BERT [184] is a contextualized embedding model pretrained on a structured electronic health record dataset of two million patients. Kaleido-BERT [212] is a VLP Transformer model tailor-made for the fashion domain.']","Masking Masked Language Modelling (MLM) [7], [137] Image-Conditioned Masked Language Modelling (IMLM) [206], [207] [208] Text-Conditioned Masked Region Prediction [206] Masked Acoustic Modelling [180], [200] Masked Image Region Regression [115] Masked Image Region Prediction [190] Masked Frame Modelling (MFM) [116] Masked Sentence Generation (MSG) [191] Video Conditioned Masked Language Model [117] Text Conditioned Masked Frame Model [117] Describing Image-conditioned Denoising Autoencoding (IDA) [208] Text-conditioned Image Feature Generation (TIFG) [208] Prefix Language Modelling (PrefixLM) [199] Matching Image-Text Matching (ITM) [188] [102], [103], [104], [106], [209], Phrase-Region Alignment (PRA) [204] Word-Region Alignment (WRA) [106], [192] Video-Subtitle Matching (VSM) [116] Next Sentence Prediction (NSP) [4], [102], [190] Ordering Sentence Ordering Modelling (SOM) [201] Frame Ordering Modelling (FOM) [116] learning vision-language correlations, not designed for sequential decision making as required in embodied VLN. Murahari et al. [190] present a visual dialogue oriented approach to leverage pretraining on general vision-language datasets. XGPT [208] is tailor-made for image captioning, to overcome the limitation that BERT-based cross-modal pretrained models fail to be applied to generative tasks directly. ERNIE-ViLG [211] is designed for bidirectional image-text generation with Transformers. Special modalities have their own unique domain knowledge that can be used to design the specific pretrain pretexts. GraphCodeBERT [44] uses two structure-aware pretext tasks (i.e., predict where a variable is identified from, data flow edge prediction between variables) for programming source code. To learn from the spatial cues in 360 • video, Morgado et al. [145] propose to perform contrastive audio-visual spatial alignment of 360 • video and spatial audio. Med-BERT [184] is a contextualized embedding model pretrained on a structured electronic health record dataset of two million patients. Kaleido-BERT [212] is a VLP Transformer model tailor-made for the fashion domain.","(p32.0) Masking Masked Language Modelling (MLM) [7], [137] Image-Conditioned Masked Language Modelling (IMLM) [206], [207] [208] Text-Conditioned Masked Region Prediction [206] Masked Acoustic Modelling [180], [200] Masked Image Region Regression [115] Masked Image Region Prediction [190] Masked Frame Modelling (MFM) [116] Masked Sentence Generation (MSG) [191] Video Conditioned Masked Language Model [117] Text Conditioned Masked Frame Model [117] Describing Image-conditioned Denoising Autoencoding (IDA) [208] Text-conditioned Image Feature Generation (TIFG) [208] Prefix Language Modelling (PrefixLM) [199] Matching Image-Text Matching (ITM) [188] [102], [103], [104], [106], [209], Phrase-Region Alignment (PRA) [204] Word-Region Alignment (WRA) [106], [192] Video-Subtitle Matching (VSM) [116] Next Sentence Prediction (NSP) [4], [102], [190] Ordering Sentence Ordering Modelling (SOM) [201] Frame Ordering Modelling (FOM) [116] learning vision-language correlations, not designed for sequential decision making as required in embodied VLN. Murahari et al. [190] present a visual dialogue oriented approach to leverage pretraining on general vision-language datasets. XGPT [208] is tailor-made for image captioning, to overcome the limitation that BERT-based cross-modal pretrained models fail to be applied to generative tasks directly. ERNIE-ViLG [211] is designed for bidirectional image-text generation with Transformers. Special modalities have their own unique domain knowledge that can be used to design the specific pretrain pretexts. GraphCodeBERT [44] uses two structure-aware pretext tasks (i.e., predict where a variable is identified from, data flow edge prediction between variables) for programming source code. To learn from the spatial cues in 360 • video, Morgado et al. [145] propose to perform contrastive audio-visual spatial alignment of 360 • video and spatial audio. Med-BERT [184] is a contextualized embedding model pretrained on a structured electronic health record dataset of two million patients. Kaleido-BERT [212] is a VLP Transformer model tailor-made for the fashion domain.","[['b210', 'b205', 'b199', 'b6', 'b114', 'b203', 'b190', 'b183', 'b200', 'b144', 'b101', 'b191', 'b116', 'b105', 'b3', 'b103', 'b189', 'b206', 'b102', 'b208', 'b179', 'b43', 'b136', 'b198', 'b207', 'b115', 'b211', 'b187']]","[['b210', 'b205', 'b199', 'b6', 'b114', 'b203', 'b190', 'b183', 'b200', 'b144', 'b101', 'b191', 'b116', 'b105', 'b3', 'b103', 'b189', 'b206', 'b102', 'b208', 'b179', 'b43', 'b136', 'b198', 'b207', 'b115', 'b211', 'b187']]",28,"1. Masking Masked Language Modelling (MLM)
2. [7], [137] Image-Conditioned Masked Language Modelling (IMLM) [206], [207] [208] Text-Conditioned Masked Region Prediction [206]
3. Masked Acoustic Modelling [180], [200] Masked Image Region Regression [7]0 Masked Image Region Prediction [190] Masked Frame
4. Modelling [7]2 [116] Masked Sentence Generation [7]4 [7]5 Video Conditioned Masked Language Model [7]7 Text Conditioned Masked Frame Model [7]7 Describing Image-conditioned Denoising Autoencoding [7]8 [208] Text-conditioned Image Feature Generation [137]0 [208] Prefix Language Modelling [137]2 [137]3 Matching Image-Text Matching [137]4 [137]5 [102], [137]7, [137]8, (IMLM)4, (IMLM)0, Phrase-Region Alignment (IMLM)1 (IMLM)2 Word-Region Alignment (IMLM)3 (IMLM)4, (IMLM)5 Video-Subtitle Matching (IMLM)6 [116] Next Sentence Prediction (IMLM)8 (IMLM)9, [102], [190] Ordering Sentence Ordering Modelling (SOM) [201] Frame Ordering Modelling (FOM) [116] learning vision-language correlations, not designed for sequential decision making as required in embodied VLN.
5. Murahari et al. [190] present a visual dialogue oriented approach to leverage pretraining on general vision-language datasets.
6. XGPT [208] is tailor-made for image captioning, to overcome the limitation that BERT-based cross-modal pretrained models fail to be applied to generative tasks directly.
7. ERNIE-ViLG [211] is designed for bidirectional image-text generation with Transformers.
8. Special modalities have their own unique domain knowledge that can be used to design the specific pretrain pretexts.
9. GraphCodeBERT [44] uses two structure-aware pretext tasks [207]0 for programming source code.
10. To learn from the spatial cues in 360  video, Morgado et al. [207]1 propose to perform contrastive audio-visual spatial alignment of 360  video and spatial audio.
11. Med-BERT [207]2 is a contextualized embedding model pretrained on a structured electronic health record dataset of two million patients.
12. Kaleido-BERT [207]3 is a VLP Transformer model tailor-made for the fashion domain.","What are the specific tasks and motivations behind the development of Masked Language Modelling (MLM) in multimodal learning?
How does Image-Conditioned Masked Language Modelling (IMLM) differ from traditional MLM approaches in processing visual information?
What strategies are employed in Text-Conditioned Masked Region Prediction to enhance understanding between text and image regions?
In what ways does Prefix Language Modelling (PrefixLM) contribute to advancements in multimodal learning?
How do Matching tasks like Image-Text Matching (ITM) and Phrase-Region Alignment (PRA) facilitate the understanding of correlations between visual and textual data?","1. What are the specific tasks and motivations behind the development of Masked Language Modelling (MLM) in multimodal learning?
2. How does Image-Conditioned Masked Language Modelling (IMLM) differ from traditional MLM approaches in processing visual information?
3. What strategies are employed in Text-Conditioned Masked Region Prediction to enhance understanding between text and image regions?
4. In what ways does Prefix Language Modelling (PrefixLM) contribute to advancements in multimodal learning?
5. How do Matching tasks like Image-Text Matching (ITM) and Phrase-Region Alignment (PRA) facilitate the understanding of correlations between visual and textual data?",None,,"Questions:

1. What are the specific tasks and motivations behind the development of Masked Language Modelling (MLM) in multimodal learning?
2. How does Image-Conditioned Masked Language Modelling (IMLM) differ from traditional MLM approaches in processing visual information?
3. What strategies are employed in Text-Conditioned Masked Region Prediction to enhance understanding between text and image regions?
4. In what ways does Prefix Language Modelling (PrefixLM) contribute to advancements in multimodal learning?
5. How do Matching tasks like Image-Text Matching (ITM) and Phrase-Region Alignment (PRA) facilitate the understanding of correlations between visual and textual data?

Answer:

(p32.0) Masking Masked Language Modelling (MLM) [7], [137] Image-Conditioned Masked Language Modelling (IMLM) [206], [207] [208] Text-Conditioned Masked Region Prediction [206] Masked Acoustic Modelling [180], [200] Masked Image Region Regression [115] Masked Image Region Prediction [190] Masked Frame Modelling (MFM) [116] Masked Sentence Generation (MSG) [191] Video Conditioned Masked Language Model [117] Text Conditioned Masked Frame Model [117] Describing Image-conditioned Denoising Autoencoding (IDA) [208] Text-conditioned Image Feature Generation (TIFG) [208] Prefix Language Modelling (PrefixLM) [199] Matching Image-Text Matching (ITM) [188] [102], [103], [104], [106], [209], Phrase-Region Alignment (PRA) [204] Word-Region Alignment (WRA) [106], [192] Video-Subtitle Matching (VSM) [116] Next Sentence Prediction (NSP) [4], [102], [190] Ordering Sentence Ordering Modelling (SOM) [201] Frame Ordering Modelling (FOM) [116] learning vision-language correlations, not designed for sequential decision making as required in embodied VLN. Murahari et al. [190] present a visual dialogue oriented approach to leverage pretraining on general vision-language datasets. XGPT [208] is tailor-made for image captioning, to overcome the limitation that BERT-based cross-modal pretrained models fail to be applied to generative tasks directly. ERNIE-ViLG [211] is designed for bidirectional image-text generation with Transformers. Special modalities have their own unique domain knowledge that can be used to design the specific pretrain pretexts. GraphCodeBERT [44] uses two structure-aware pretext tasks (i.e., predict where a variable is identified from, data flow edge prediction between variables) for programming source code. To learn from the spatial cues in 360 • video, Morgado et al. [145] propose to perform contrastive audio-visual spatial alignment of 360 • video and spatial audio. Med-BERT [184] is a contextualized embedding model pretrained on a structured electronic health record dataset of two million patients. Kaleido-BERT [212] is a VLP Transformer model tailor-made for the fashion domain.","Questions:



Answer:

(p32.0) Masking Masked Language Modelling (MLM) [7], [137] Image-Conditioned Masked Language Modelling (IMLM) [206], [207] [208] Text-Conditioned Masked Region Prediction [206] Masked Acoustic Modelling [180], [200] Masked Image Region Regression [115] Masked Image Region Prediction [190] Masked Frame Modelling (MFM) [116] Masked Sentence Generation (MSG) [191] Video Conditioned Masked Language Model [117] Text Conditioned Masked Frame Model [117] Describing Image-conditioned Denoising Autoencoding (IDA) [208] Text-conditioned Image Feature Generation (TIFG) [208] Prefix Language Modelling (PrefixLM) [199] Matching Image-Text Matching (ITM) [188] [102], [103], [104], [106], [209], Phrase-Region Alignment (PRA) [204] Word-Region Alignment (WRA) [106], [192] Video-Subtitle Matching (VSM) [116] Next Sentence Prediction (NSP) [4], [102], [190] Ordering Sentence Ordering Modelling (SOM) [201] Frame Ordering Modelling (FOM) [116] learning vision-language correlations, not designed for sequential decision making as required in embodied VLN. Murahari et al. [190] present a visual dialogue oriented approach to leverage pretraining on general vision-language datasets. XGPT [208] is tailor-made for image captioning, to overcome the limitation that BERT-based cross-modal pretrained models fail to be applied to generative tasks directly. ERNIE-ViLG [211] is designed for bidirectional image-text generation with Transformers. Special modalities have their own unique domain knowledge that can be used to design the specific pretrain pretexts. GraphCodeBERT [44] uses two structure-aware pretext tasks (i.e., predict where a variable is identified from, data flow edge prediction between variables) for programming source code. To learn from the spatial cues in 360 • video, Morgado et al. [145] propose to perform contrastive audio-visual spatial alignment of 360 • video and spatial audio. Med-BERT [184] is a contextualized embedding model pretrained on a structured electronic health record dataset of two million patients. Kaleido-BERT [212] is a VLP Transformer model tailor-made for the fashion domain."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s33,Transformers for Specific Multimodal Tasks,"['p33.0', 'p33.1']","['Recent work has demonstrated that Transformer models can encode various multimodal inputs in both classical and novel discriminative applications, e.g., RGB & optical flow [46], RGB & depth [213], RGB & point cloud [214], RGB & LiDAR [215], [216], textual description & point cloud [31], acoustic & text [180], audio & visual observation for Audio-Visual Navigation [76], speech query & schema of SQL database [25], text question/query & the schema SQL database [24], audio & tags [217], multimodal representation for video [218], [ [44], image & text for retrieval [222].', 'Meanwhile, Transformers also contribute to various multimodal generative tasks, including single-modality to single-modality (e.g., raw audio to 3D mesh sequence [39], RGB to 3D scene [40], single image to 3D human texture estimation [223], RGB to scene graph [19], [224], [225], [226], graph to graph [33], knowledge graph to text [227], video to scene graph [228], video to caption [229], [230], [231], [232], image to caption [233], [234], [235], [236], [237], text to speech [238], text to image [205], [239], text to shape [240], RGB to 3D human pose and mesh [41], music to dance [241]), multimodality to single modality (e.g., image & text to scene graph [242], Video Dialogue (text & audio & visual to text) [243], Mono Audio & Depth to Binaural Audio [14], music piece & seed 3D motion to long-range future 3D motions [146], X-raying image & question to answer [244], video & text & audio to text [245]), and multimodality to multimodality (e.g., [246]).']","Recent work has demonstrated that Transformer models can encode various multimodal inputs in both classical and novel discriminative applications, e.g., RGB & optical flow [46], RGB & depth [213], RGB & point cloud [214], RGB & LiDAR [215], [216], textual description & point cloud [31], acoustic & text [180], audio & visual observation for Audio-Visual Navigation [76], speech query & schema of SQL database [25], text question/query & the schema SQL database [24], audio & tags [217], multimodal representation for video [218], [ [44], image & text for retrieval [222].

Meanwhile, Transformers also contribute to various multimodal generative tasks, including single-modality to single-modality (e.g., raw audio to 3D mesh sequence [39], RGB to 3D scene [40], single image to 3D human texture estimation [223], RGB to scene graph [19], [224], [225], [226], graph to graph [33], knowledge graph to text [227], video to scene graph [228], video to caption [229], [230], [231], [232], image to caption [233], [234], [235], [236], [237], text to speech [238], text to image [205], [239], text to shape [240], RGB to 3D human pose and mesh [41], music to dance [241]), multimodality to single modality (e.g., image & text to scene graph [242], Video Dialogue (text & audio & visual to text) [243], Mono Audio & Depth to Binaural Audio [14], music piece & seed 3D motion to long-range future 3D motions [146], X-raying image & question to answer [244], video & text & audio to text [245]), and multimodality to multimodality (e.g., [246]).","(p33.0) Recent work has demonstrated that Transformer models can encode various multimodal inputs in both classical and novel discriminative applications, e.g., RGB & optical flow [46], RGB & depth [213], RGB & point cloud [214], RGB & LiDAR [215], [216], textual description & point cloud [31], acoustic & text [180], audio & visual observation for Audio-Visual Navigation [76], speech query & schema of SQL database [25], text question/query & the schema SQL database [24], audio & tags [217], multimodal representation for video [218], [ [44], image & text for retrieval [222].

(p33.1) Meanwhile, Transformers also contribute to various multimodal generative tasks, including single-modality to single-modality (e.g., raw audio to 3D mesh sequence [39], RGB to 3D scene [40], single image to 3D human texture estimation [223], RGB to scene graph [19], [224], [225], [226], graph to graph [33], knowledge graph to text [227], video to scene graph [228], video to caption [229], [230], [231], [232], image to caption [233], [234], [235], [236], [237], text to speech [238], text to image [205], [239], text to shape [240], RGB to 3D human pose and mesh [41], music to dance [241]), multimodality to single modality (e.g., image & text to scene graph [242], Video Dialogue (text & audio & visual to text) [243], Mono Audio & Depth to Binaural Audio [14], music piece & seed 3D motion to long-range future 3D motions [146], X-raying image & question to answer [244], video & text & audio to text [245]), and multimodality to multimodality (e.g., [246]).","[['b213', 'b214', 'b216', 'b24', 'b23', 'b30', 'b217', 'b215', 'b179', None, 'b212', 'b45', 'b43', 'b75', 'b221'], ['b18', 'b234', 'b32', 'b225', 'b204', 'b227', 'b230', 'b231', 'b235', 'b245', 'b38', 'b228', 'b243', 'b145', 'b238', 'b223', 'b224', 'b241', 'b244', 'b40', 'b236', 'b237', 'b239', 'b240', 'b233', 'b226', 'b242', 'b232', 'b13', 'b39', 'b229', 'b222']]","[['b213', 'b214', 'b216', 'b24', 'b23', 'b30', 'b217', 'b215', 'b179', None, 'b212', 'b45', 'b43', 'b75', 'b221'], ['b18', 'b234', 'b32', 'b225', 'b204', 'b227', 'b230', 'b231', 'b235', 'b245', 'b38', 'b228', 'b243', 'b145', 'b238', 'b223', 'b224', 'b241', 'b244', 'b40', 'b236', 'b237', 'b239', 'b240', 'b233', 'b226', 'b242', 'b232', 'b13', 'b39', 'b229', 'b222']]",47,"1. Recent work has demonstrated that Transformer models can encode various multimodal inputs in both classical and novel discriminative applications, e.g., RGB & optical flow [46], RGB & depth [213], RGB & point cloud [214], RGB & LiDAR [215], [216], textual description & point cloud [31], acoustic & text [180], audio & visual observation for Audio-Visual Navigation [76], speech query & schema of SQL database [25], text question/query & the schema SQL database [24], audio & tags [213]0, multimodal representation for video [213]1, [213]2, image & text for retrieval [213]3.
2. Meanwhile, Transformers also contribute to various multimodal generative tasks, including single-modality to single-modality [213]4, multimodality to single modality [213]5 [213]6, Mono Audio & Depth to Binaural Audio [213]7, music piece & seed 3D motion to long-range future 3D motions [213]8, X-raying image & question to answer [213]9, video & text & audio to text [214]0), and multimodality to multimodality [214]1.","How do Transformer models facilitate the encoding of various multimodal inputs for discriminative applications?
What are examples of classical and novel discriminative applications that utilize multimodal inputs with Transformer models?
In what ways do Transformers support generative tasks across different modalities, including single-modality to single-modality transformations?
Can you detail how Transformers are applied in multimodality to single modality generative tasks, providing specific examples?
What advancements have been made in using Transformers for multimodality to multimodality tasks, and what are the implications of these advancements?","1. How do Transformer models facilitate the encoding of various multimodal inputs for discriminative applications?
2. What are examples of classical and novel discriminative applications that utilize multimodal inputs with Transformer models?
3. In what ways do Transformers support generative tasks across different modalities, including single-modality to single-modality transformations?
4. Can you detail how Transformers are applied in multimodality to single modality generative tasks, providing specific examples?
5. What advancements have been made in using Transformers for multimodality to multimodality tasks, and what are the implications of these advancements?",None,,"Questions:

1. How do Transformer models facilitate the encoding of various multimodal inputs for discriminative applications?
2. What are examples of classical and novel discriminative applications that utilize multimodal inputs with Transformer models?
3. In what ways do Transformers support generative tasks across different modalities, including single-modality to single-modality transformations?
4. Can you detail how Transformers are applied in multimodality to single modality generative tasks, providing specific examples?
5. What advancements have been made in using Transformers for multimodality to multimodality tasks, and what are the implications of these advancements?

Answer:

(p33.0) Recent work has demonstrated that Transformer models can encode various multimodal inputs in both classical and novel discriminative applications, e.g., RGB & optical flow [46], RGB & depth [213], RGB & point cloud [214], RGB & LiDAR [215], [216], textual description & point cloud [31], acoustic & text [180], audio & visual observation for Audio-Visual Navigation [76], speech query & schema of SQL database [25], text question/query & the schema SQL database [24], audio & tags [217], multimodal representation for video [218], [ [44], image & text for retrieval [222].

(p33.1) Meanwhile, Transformers also contribute to various multimodal generative tasks, including single-modality to single-modality (e.g., raw audio to 3D mesh sequence [39], RGB to 3D scene [40], single image to 3D human texture estimation [223], RGB to scene graph [19], [224], [225], [226], graph to graph [33], knowledge graph to text [227], video to scene graph [228], video to caption [229], [230], [231], [232], image to caption [233], [234], [235], [236], [237], text to speech [238], text to image [205], [239], text to shape [240], RGB to 3D human pose and mesh [41], music to dance [241]), multimodality to single modality (e.g., image & text to scene graph [242], Video Dialogue (text & audio & visual to text) [243], Mono Audio & Depth to Binaural Audio [14], music piece & seed 3D motion to long-range future 3D motions [146], X-raying image & question to answer [244], video & text & audio to text [245]), and multimodality to multimodality (e.g., [246]).","Questions:



Answer:

(p33.0) Recent work has demonstrated that Transformer models can encode various multimodal inputs in both classical and novel discriminative applications, e.g., RGB & optical flow [46], RGB & depth [213], RGB & point cloud [214], RGB & LiDAR [215], [216], textual description & point cloud [31], acoustic & text [180], audio & visual observation for Audio-Visual Navigation [76], speech query & schema of SQL database [25], text question/query & the schema SQL database [24], audio & tags [217], multimodal representation for video [218], [ [44], image & text for retrieval [222].

(p33.1) Meanwhile, Transformers also contribute to various multimodal generative tasks, including single-modality to single-modality (e.g., raw audio to 3D mesh sequence [39], RGB to 3D scene [40], single image to 3D human texture estimation [223], RGB to scene graph [19], [224], [225], [226], graph to graph [33], knowledge graph to text [227], video to scene graph [228], video to caption [229], [230], [231], [232], image to caption [233], [234], [235], [236], [237], text to speech [238], text to image [205], [239], text to shape [240], RGB to 3D human pose and mesh [41], music to dance [241]), multimodality to single modality (e.g., image & text to scene graph [242], Video Dialogue (text & audio & visual to text) [243], Mono Audio & Depth to Binaural Audio [14], music piece & seed 3D motion to long-range future 3D motions [146], X-raying image & question to answer [244], video & text & audio to text [245]), and multimodality to multimodality (e.g., [246])."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s35,Fusion,['p35.0'],"['In general, MML Transformers fuse information across multiple modalities primarily at three levels: input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion). Common early fusion based MML Transformer models [7], [104], [108] are also known as one-stream architecture, allowing the adoption of the merits of BERT due to minimal architectural modification. The main difference between these one-stream models is the usage of problem-specific modalities with variant masking techniques. With attention operation, a noticeable fusion scheme is introduced based on a notion of bottleneck tokens [175]. It applies for both early and middle fusion by simply choosing to-be-fused layers. We note that the simple prediction-based late fusion [247], [248] is less adopted in MML Transformers. This makes sense considering the motivations of learning stronger multimodal contextual representations and great advance of computing power. For enhancing and interpreting the fusion of MML, probing the interaction and measuring the fusion between modalities [249] would be an interesting direction to explore.']","In general, MML Transformers fuse information across multiple modalities primarily at three levels: input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion). Common early fusion based MML Transformer models [7], [104], [108] are also known as one-stream architecture, allowing the adoption of the merits of BERT due to minimal architectural modification. The main difference between these one-stream models is the usage of problem-specific modalities with variant masking techniques. With attention operation, a noticeable fusion scheme is introduced based on a notion of bottleneck tokens [175]. It applies for both early and middle fusion by simply choosing to-be-fused layers. We note that the simple prediction-based late fusion [247], [248] is less adopted in MML Transformers. This makes sense considering the motivations of learning stronger multimodal contextual representations and great advance of computing power. For enhancing and interpreting the fusion of MML, probing the interaction and measuring the fusion between modalities [249] would be an interesting direction to explore.","(p35.0) In general, MML Transformers fuse information across multiple modalities primarily at three levels: input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion). Common early fusion based MML Transformer models [7], [104], [108] are also known as one-stream architecture, allowing the adoption of the merits of BERT due to minimal architectural modification. The main difference between these one-stream models is the usage of problem-specific modalities with variant masking techniques. With attention operation, a noticeable fusion scheme is introduced based on a notion of bottleneck tokens [175]. It applies for both early and middle fusion by simply choosing to-be-fused layers. We note that the simple prediction-based late fusion [247], [248] is less adopted in MML Transformers. This makes sense considering the motivations of learning stronger multimodal contextual representations and great advance of computing power. For enhancing and interpreting the fusion of MML, probing the interaction and measuring the fusion between modalities [249] would be an interesting direction to explore.","[['b6', 'b248', 'b107', 'b246', 'b103', 'b247', 'b174']]","[['b6', 'b248', 'b107', 'b246', 'b103', 'b247', 'b174']]",7,"1. In general, MML Transformers fuse information across multiple modalities primarily at three levels: input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion).
2. Common early fusion based MML Transformer models [7], [104], [108] are also known as one-stream architecture, allowing the adoption of the merits of BERT due to minimal architectural modification.
3. The main difference between these one-stream models is the usage of problem-specific modalities with variant masking techniques.
4. With attention operation, a noticeable fusion scheme is introduced based on a notion of bottleneck tokens [175].
5. It applies for both early and middle fusion by simply choosing to-be-fused layers.
6. We note that the simple prediction-based late fusion [247], [248] is less adopted in MML Transformers.
7. This makes sense considering the motivations of learning stronger multimodal contextual representations and great advance of computing power.
8. For enhancing and interpreting the fusion of MML, probing the interaction and measuring the fusion between modalities [249] would be an interesting direction to explore.","What are the three primary levels at which MML Transformers fuse information across multiple modalities?
How do early fusion based MML Transformer models benefit from the architecture of BERT?
What is the main difference between one-stream MML Transformer models in terms of their approach to multimodal learning?
How does the attention operation facilitate fusion in both early and middle fusion levels in MML Transformers?
Why is late fusion less adopted in MML Transformers, and what are the implications for multimodal contextual representation learning?","1. What are the three primary levels at which MML Transformers fuse information across multiple modalities?
2. How do early fusion based MML Transformer models benefit from the architecture of BERT?
3. What is the main difference between one-stream MML Transformer models in terms of their approach to multimodal learning?
4. How does the attention operation facilitate fusion in both early and middle fusion levels in MML Transformers?
5. Why is late fusion less adopted in MML Transformers, and what are the implications for multimodal contextual representation learning?",None,,"Questions:

1. What are the three primary levels at which MML Transformers fuse information across multiple modalities?
2. How do early fusion based MML Transformer models benefit from the architecture of BERT?
3. What is the main difference between one-stream MML Transformer models in terms of their approach to multimodal learning?
4. How does the attention operation facilitate fusion in both early and middle fusion levels in MML Transformers?
5. Why is late fusion less adopted in MML Transformers, and what are the implications for multimodal contextual representation learning?

Answer:

(p35.0) In general, MML Transformers fuse information across multiple modalities primarily at three levels: input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion). Common early fusion based MML Transformer models [7], [104], [108] are also known as one-stream architecture, allowing the adoption of the merits of BERT due to minimal architectural modification. The main difference between these one-stream models is the usage of problem-specific modalities with variant masking techniques. With attention operation, a noticeable fusion scheme is introduced based on a notion of bottleneck tokens [175]. It applies for both early and middle fusion by simply choosing to-be-fused layers. We note that the simple prediction-based late fusion [247], [248] is less adopted in MML Transformers. This makes sense considering the motivations of learning stronger multimodal contextual representations and great advance of computing power. For enhancing and interpreting the fusion of MML, probing the interaction and measuring the fusion between modalities [249] would be an interesting direction to explore.","Questions:



Answer:

(p35.0) In general, MML Transformers fuse information across multiple modalities primarily at three levels: input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion). Common early fusion based MML Transformer models [7], [104], [108] are also known as one-stream architecture, allowing the adoption of the merits of BERT due to minimal architectural modification. The main difference between these one-stream models is the usage of problem-specific modalities with variant masking techniques. With attention operation, a noticeable fusion scheme is introduced based on a notion of bottleneck tokens [175]. It applies for both early and middle fusion by simply choosing to-be-fused layers. We note that the simple prediction-based late fusion [247], [248] is less adopted in MML Transformers. This makes sense considering the motivations of learning stronger multimodal contextual representations and great advance of computing power. For enhancing and interpreting the fusion of MML, probing the interaction and measuring the fusion between modalities [249] would be an interesting direction to explore."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s36,Alignment,"['p36.0', 'p36.1']","['Cross-modal alignment is the key to a number of real-world multimodal applications. Transformer based cross-modal alignment has been studied for various tasks, e.g., speaker localization in multi-speaker videos [250], speech translation [180], text-to-speech alignment [251], text-to-video retrieval [252], [253], [254], and visual grounding of natural language [255], [256], [257], [258], [259]. Recently, Transformer based alignment [9], [119], [260], [261], [262] has led to a surge of leveraging large quantities of web data (e.g., image-text pairs) for vision and language tasks.', 'A representative practice is to map two modalities into a common representation space with contrastive learning over paired samples. The models based on this idea are often enormous in size and expensive to optimize from millions or billions of training data. Consequently, successive works mostly exploit pretrained models for tackling various downstream tasks [120], [263], [264], [265], [266]. These alignment models have the ability of zero-shot transfer particularly for image classification via prompt engineering [267]. This novel perspective is mind-blowing, given that image classification is conventionally regarded as a unimodal learning problem and zero-shot classification remains an unsolved challenge despite extensive research [268]. This has been studied for more challenging and fine-grained tasks (e.g., object detection [269], visual question answering [103], [106], [112], [263], and instance retrieval [222], [263]) by imposing region (semantic parts such as objects) level alignment. Finegrained alignment will however incur more computational costs from explicit region detection and how to eliminate this whilst keeping the region-level learning capability becomes a challenge. Several ideas introduced recently include random sampling [113], learning concept dictionary [203], uniform masking [270], patch projection [192], joint learning of a region detector [271], and representation aligning before mask prediction [263].']","Cross-modal alignment is the key to a number of real-world multimodal applications. Transformer based cross-modal alignment has been studied for various tasks, e.g., speaker localization in multi-speaker videos [250], speech translation [180], text-to-speech alignment [251], text-to-video retrieval [252], [253], [254], and visual grounding of natural language [255], [256], [257], [258], [259]. Recently, Transformer based alignment [9], [119], [260], [261], [262] has led to a surge of leveraging large quantities of web data (e.g., image-text pairs) for vision and language tasks.

A representative practice is to map two modalities into a common representation space with contrastive learning over paired samples. The models based on this idea are often enormous in size and expensive to optimize from millions or billions of training data. Consequently, successive works mostly exploit pretrained models for tackling various downstream tasks [120], [263], [264], [265], [266]. These alignment models have the ability of zero-shot transfer particularly for image classification via prompt engineering [267]. This novel perspective is mind-blowing, given that image classification is conventionally regarded as a unimodal learning problem and zero-shot classification remains an unsolved challenge despite extensive research [268]. This has been studied for more challenging and fine-grained tasks (e.g., object detection [269], visual question answering [103], [106], [112], [263], and instance retrieval [222], [263]) by imposing region (semantic parts such as objects) level alignment. Finegrained alignment will however incur more computational costs from explicit region detection and how to eliminate this whilst keeping the region-level learning capability becomes a challenge. Several ideas introduced recently include random sampling [113], learning concept dictionary [203], uniform masking [270], patch projection [192], joint learning of a region detector [271], and representation aligning before mask prediction [263].","(p36.0) Cross-modal alignment is the key to a number of real-world multimodal applications. Transformer based cross-modal alignment has been studied for various tasks, e.g., speaker localization in multi-speaker videos [250], speech translation [180], text-to-speech alignment [251], text-to-video retrieval [252], [253], [254], and visual grounding of natural language [255], [256], [257], [258], [259]. Recently, Transformer based alignment [9], [119], [260], [261], [262] has led to a surge of leveraging large quantities of web data (e.g., image-text pairs) for vision and language tasks.

(p36.1) A representative practice is to map two modalities into a common representation space with contrastive learning over paired samples. The models based on this idea are often enormous in size and expensive to optimize from millions or billions of training data. Consequently, successive works mostly exploit pretrained models for tackling various downstream tasks [120], [263], [264], [265], [266]. These alignment models have the ability of zero-shot transfer particularly for image classification via prompt engineering [267]. This novel perspective is mind-blowing, given that image classification is conventionally regarded as a unimodal learning problem and zero-shot classification remains an unsolved challenge despite extensive research [268]. This has been studied for more challenging and fine-grained tasks (e.g., object detection [269], visual question answering [103], [106], [112], [263], and instance retrieval [222], [263]) by imposing region (semantic parts such as objects) level alignment. Finegrained alignment will however incur more computational costs from explicit region detection and how to eliminate this whilst keeping the region-level learning capability becomes a challenge. Several ideas introduced recently include random sampling [113], learning concept dictionary [203], uniform masking [270], patch projection [192], joint learning of a region detector [271], and representation aligning before mask prediction [263].","[['b257', 'b251', 'b260', 'b256', 'b254', 'b258', 'b249', 'b8', 'b253', 'b179', 'b118', 'b259', 'b255', 'b252', 'b261', 'b250'], ['b266', 'b267', 'b111', 'b191', 'b264', 'b119', 'b202', 'b265', 'b102', 'b112', 'b263', 'b270', 'b105', 'b262', 'b269', 'b268', 'b221']]","[['b257', 'b251', 'b260', 'b256', 'b254', 'b258', 'b249', 'b8', 'b253', 'b179', 'b118', 'b259', 'b255', 'b252', 'b261', 'b250'], ['b266', 'b267', 'b111', 'b191', 'b264', 'b119', 'b202', 'b265', 'b102', 'b112', 'b263', 'b270', 'b105', 'b262', 'b269', 'b268', 'b221']]",33,"1. Cross-modal alignment is the key to a number of real-world multimodal applications.
2. Transformer based cross-modal alignment has been studied for various tasks, e.g., speaker localization in multi-speaker videos [250], speech translation [180], text-to-speech alignment [251], text-to-video retrieval [252], [253], [254], and visual grounding of natural language [255], [256], [257], [258], [180]0.
3. Recently, Transformer based alignment [180]1, [180]2, [180]3, [180]4, [180]5 has led to a surge of leveraging large quantities of web data [180]6 for vision and language tasks.
4. A representative practice is to map two modalities into a common representation space with contrastive learning over paired samples.
5. The models based on this idea are often enormous in size and expensive to optimize from millions or billions of training data.
6. Consequently, successive works mostly exploit pretrained models for tackling various downstream tasks [180]7, [252]1, [180]9, [251]0, [251]1.
7. These alignment models have the ability of zero-shot transfer particularly for image classification via prompt engineering [251]2.
8. This novel perspective is mind-blowing, given that image classification is conventionally regarded as a unimodal learning problem and zero-shot classification remains an unsolved challenge despite extensive research [251]3.
9. This has been studied for more challenging and fine-grained tasks (e.g., object detection
10. [269], visual question answering [103], [106], [112], [252]1, and instance retrieval [222], [252]1) by imposing region [251]5 level alignment.
11. Finegrained alignment will however incur more computational costs from explicit region detection and how to eliminate this whilst keeping the region-level learning capability becomes a challenge.
12. Several ideas introduced recently include random sampling [251]6, learning concept dictionary [251]7, uniform masking [251]8, patch projection [251]9, joint learning of a region detector [252]0, and representation aligning before mask prediction [252]1.","What are the key applications of Transformer based cross-modal alignment in multimodal learning?
How does contrastive learning contribute to mapping two modalities into a common representation space in Transformer models?
What are the challenges and computational costs associated with fine-grained alignment in multimodal learning?
How do pretrained models facilitate tackling various downstream tasks in the context of cross-modal alignment?
What novel strategies have been proposed to reduce computational costs while maintaining region-level learning capability in multimodal learning?","1. What are the key applications of Transformer based cross-modal alignment in multimodal learning?
2. How does contrastive learning contribute to mapping two modalities into a common representation space in Transformer models?
3. What are the challenges and computational costs associated with fine-grained alignment in multimodal learning?
4. How do pretrained models facilitate tackling various downstream tasks in the context of cross-modal alignment?
5. What novel strategies have been proposed to reduce computational costs while maintaining region-level learning capability in multimodal learning?",None,,"Questions:

1. What are the key applications of Transformer based cross-modal alignment in multimodal learning?
2. How does contrastive learning contribute to mapping two modalities into a common representation space in Transformer models?
3. What are the challenges and computational costs associated with fine-grained alignment in multimodal learning?
4. How do pretrained models facilitate tackling various downstream tasks in the context of cross-modal alignment?
5. What novel strategies have been proposed to reduce computational costs while maintaining region-level learning capability in multimodal learning?

Answer:

(p36.0) Cross-modal alignment is the key to a number of real-world multimodal applications. Transformer based cross-modal alignment has been studied for various tasks, e.g., speaker localization in multi-speaker videos [250], speech translation [180], text-to-speech alignment [251], text-to-video retrieval [252], [253], [254], and visual grounding of natural language [255], [256], [257], [258], [259]. Recently, Transformer based alignment [9], [119], [260], [261], [262] has led to a surge of leveraging large quantities of web data (e.g., image-text pairs) for vision and language tasks.

(p36.1) A representative practice is to map two modalities into a common representation space with contrastive learning over paired samples. The models based on this idea are often enormous in size and expensive to optimize from millions or billions of training data. Consequently, successive works mostly exploit pretrained models for tackling various downstream tasks [120], [263], [264], [265], [266]. These alignment models have the ability of zero-shot transfer particularly for image classification via prompt engineering [267]. This novel perspective is mind-blowing, given that image classification is conventionally regarded as a unimodal learning problem and zero-shot classification remains an unsolved challenge despite extensive research [268]. This has been studied for more challenging and fine-grained tasks (e.g., object detection [269], visual question answering [103], [106], [112], [263], and instance retrieval [222], [263]) by imposing region (semantic parts such as objects) level alignment. Finegrained alignment will however incur more computational costs from explicit region detection and how to eliminate this whilst keeping the region-level learning capability becomes a challenge. Several ideas introduced recently include random sampling [113], learning concept dictionary [203], uniform masking [270], patch projection [192], joint learning of a region detector [271], and representation aligning before mask prediction [263].","Questions:



Answer:

(p36.0) Cross-modal alignment is the key to a number of real-world multimodal applications. Transformer based cross-modal alignment has been studied for various tasks, e.g., speaker localization in multi-speaker videos [250], speech translation [180], text-to-speech alignment [251], text-to-video retrieval [252], [253], [254], and visual grounding of natural language [255], [256], [257], [258], [259]. Recently, Transformer based alignment [9], [119], [260], [261], [262] has led to a surge of leveraging large quantities of web data (e.g., image-text pairs) for vision and language tasks.

(p36.1) A representative practice is to map two modalities into a common representation space with contrastive learning over paired samples. The models based on this idea are often enormous in size and expensive to optimize from millions or billions of training data. Consequently, successive works mostly exploit pretrained models for tackling various downstream tasks [120], [263], [264], [265], [266]. These alignment models have the ability of zero-shot transfer particularly for image classification via prompt engineering [267]. This novel perspective is mind-blowing, given that image classification is conventionally regarded as a unimodal learning problem and zero-shot classification remains an unsolved challenge despite extensive research [268]. This has been studied for more challenging and fine-grained tasks (e.g., object detection [269], visual question answering [103], [106], [112], [263], and instance retrieval [222], [263]) by imposing region (semantic parts such as objects) level alignment. Finegrained alignment will however incur more computational costs from explicit region detection and how to eliminate this whilst keeping the region-level learning capability becomes a challenge. Several ideas introduced recently include random sampling [113], learning concept dictionary [203], uniform masking [270], patch projection [192], joint learning of a region detector [271], and representation aligning before mask prediction [263]."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s37,Transferability,"['p37.0', 'p37.1', 'p37.2', 'p37.3', 'p37.4', 'p37.5']","['Transferability is a major challenge for Transformer based multimodal learning, involving the question of how to transfer models across different datasets and applications.', 'Data augmentation and adversarial perturbation strategies help multimodal Transformers to improve the generalization ability. VILLA [210] is a two-stage strategy (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning) that improves VLP Transformers.', 'In practice, the distribution gap between training data and practical data is noticeable. For instance, supervised data samples (well-labelled, well-aligned) are costly in practical applications, thus how to transfer the supervised multimodal Transformers pretrained on well-aligned cross-modal pairs/tuples to the weakly aligned test bed is challenging [137]. CLIP [9] is an inspiring solution that transfers knowledge across modalities by learning a shared multimodal embedding space, enabling zero-shot transfer of the model to down-stream tasks. The main inspiration that CLIP presents the community is that the pretrained multimodal (image and text) knowledge can be transferred to down-stream zero-shot image prediction by using a prompt template ""A photo of a {label}."" to bridge the distribution gap between training and test datasets.', 'Over-fitting is a major obstacle to transfer. Multimodal Transformers can be overly fitted to the dataset biases during training, due to the large modelling capability. Some recent practices exploit how to transfer the oracle model trained on noiseless dataset to real dataset. For instance, Kervadec et al. [272], [273] explore how transferable reasoning patterns are in VQA, and demonstrate that for LXMERT [103]/BERT-like reasoning patterns can be partially transferred from an ideal dataset to a real dataset.', 'Cross-task gap is another major obstacle to transfer [208], [274], due to the different reasoning and input-output workflows, e.g., how to use multimodal datasets to finetune the language pretrained model is difficult [274]. In real applications, multimodal pretrained Transformers sometimes need to handle the uni-modal data at inference stage due to the issue of missing modalities. One solution is using knowledge distillation, e.g., distilling from multimodal to uni-modal attention in Transformers [275], distilling from multiple uni-modal Transformer teachers to a shared Transformer encoder [276]. There is a huge gap across discriminative and generative multimodal tasks. As discussed in [208], the BERT-like encoder-only multimodal Transformers (e.g., VideoBERT [7], CBT [107]) need separately to train decoders for generation tasks. This could create a pretrain-finetune discrepancy detrimental to the generality. Recently, more and more attempts study this issue further, e.g., GilBERT [222] is a generative VLP models for a discriminative task, i.e., image-text retrieval.', 'Cross-lingual gap also should be considered for the transferability of Transformer based multimodal learning, e.g., universal cross-lingual generalization from English to non-English multimodal contexts [206], [277].']","Transferability is a major challenge for Transformer based multimodal learning, involving the question of how to transfer models across different datasets and applications.

Data augmentation and adversarial perturbation strategies help multimodal Transformers to improve the generalization ability. VILLA [210] is a two-stage strategy (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning) that improves VLP Transformers.

In practice, the distribution gap between training data and practical data is noticeable. For instance, supervised data samples (well-labelled, well-aligned) are costly in practical applications, thus how to transfer the supervised multimodal Transformers pretrained on well-aligned cross-modal pairs/tuples to the weakly aligned test bed is challenging [137]. CLIP [9] is an inspiring solution that transfers knowledge across modalities by learning a shared multimodal embedding space, enabling zero-shot transfer of the model to down-stream tasks. The main inspiration that CLIP presents the community is that the pretrained multimodal (image and text) knowledge can be transferred to down-stream zero-shot image prediction by using a prompt template ""A photo of a {label}."" to bridge the distribution gap between training and test datasets.

Over-fitting is a major obstacle to transfer. Multimodal Transformers can be overly fitted to the dataset biases during training, due to the large modelling capability. Some recent practices exploit how to transfer the oracle model trained on noiseless dataset to real dataset. For instance, Kervadec et al. [272], [273] explore how transferable reasoning patterns are in VQA, and demonstrate that for LXMERT [103]/BERT-like reasoning patterns can be partially transferred from an ideal dataset to a real dataset.

Cross-task gap is another major obstacle to transfer [208], [274], due to the different reasoning and input-output workflows, e.g., how to use multimodal datasets to finetune the language pretrained model is difficult [274]. In real applications, multimodal pretrained Transformers sometimes need to handle the uni-modal data at inference stage due to the issue of missing modalities. One solution is using knowledge distillation, e.g., distilling from multimodal to uni-modal attention in Transformers [275], distilling from multiple uni-modal Transformer teachers to a shared Transformer encoder [276]. There is a huge gap across discriminative and generative multimodal tasks. As discussed in [208], the BERT-like encoder-only multimodal Transformers (e.g., VideoBERT [7], CBT [107]) need separately to train decoders for generation tasks. This could create a pretrain-finetune discrepancy detrimental to the generality. Recently, more and more attempts study this issue further, e.g., GilBERT [222] is a generative VLP models for a discriminative task, i.e., image-text retrieval.

Cross-lingual gap also should be considered for the transferability of Transformer based multimodal learning, e.g., universal cross-lingual generalization from English to non-English multimodal contexts [206], [277].","(p37.0) Transferability is a major challenge for Transformer based multimodal learning, involving the question of how to transfer models across different datasets and applications.

(p37.1) Data augmentation and adversarial perturbation strategies help multimodal Transformers to improve the generalization ability. VILLA [210] is a two-stage strategy (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning) that improves VLP Transformers.

(p37.2) In practice, the distribution gap between training data and practical data is noticeable. For instance, supervised data samples (well-labelled, well-aligned) are costly in practical applications, thus how to transfer the supervised multimodal Transformers pretrained on well-aligned cross-modal pairs/tuples to the weakly aligned test bed is challenging [137]. CLIP [9] is an inspiring solution that transfers knowledge across modalities by learning a shared multimodal embedding space, enabling zero-shot transfer of the model to down-stream tasks. The main inspiration that CLIP presents the community is that the pretrained multimodal (image and text) knowledge can be transferred to down-stream zero-shot image prediction by using a prompt template ""A photo of a {label}."" to bridge the distribution gap between training and test datasets.

(p37.3) Over-fitting is a major obstacle to transfer. Multimodal Transformers can be overly fitted to the dataset biases during training, due to the large modelling capability. Some recent practices exploit how to transfer the oracle model trained on noiseless dataset to real dataset. For instance, Kervadec et al. [272], [273] explore how transferable reasoning patterns are in VQA, and demonstrate that for LXMERT [103]/BERT-like reasoning patterns can be partially transferred from an ideal dataset to a real dataset.

(p37.4) Cross-task gap is another major obstacle to transfer [208], [274], due to the different reasoning and input-output workflows, e.g., how to use multimodal datasets to finetune the language pretrained model is difficult [274]. In real applications, multimodal pretrained Transformers sometimes need to handle the uni-modal data at inference stage due to the issue of missing modalities. One solution is using knowledge distillation, e.g., distilling from multimodal to uni-modal attention in Transformers [275], distilling from multiple uni-modal Transformer teachers to a shared Transformer encoder [276]. There is a huge gap across discriminative and generative multimodal tasks. As discussed in [208], the BERT-like encoder-only multimodal Transformers (e.g., VideoBERT [7], CBT [107]) need separately to train decoders for generation tasks. This could create a pretrain-finetune discrepancy detrimental to the generality. Recently, more and more attempts study this issue further, e.g., GilBERT [222] is a generative VLP models for a discriminative task, i.e., image-text retrieval.

(p37.5) Cross-lingual gap also should be considered for the transferability of Transformer based multimodal learning, e.g., universal cross-lingual generalization from English to non-English multimodal contexts [206], [277].","[[], ['b209'], ['b136', 'b8'], ['b272', 'b271', 'b102'], ['b6', 'b207', 'b273', 'b274', 'b275', 'b106', 'b221'], ['b276', 'b205']]","[[], ['b209'], ['b136', 'b8'], ['b272', 'b271', 'b102'], ['b6', 'b207', 'b273', 'b274', 'b275', 'b106', 'b221'], ['b276', 'b205']]",15,"1. Transferability is a major challenge for Transformer based multimodal learning, involving the question of how to transfer models across different datasets and applications.
2. Data augmentation and adversarial perturbation strategies help multimodal Transformers to improve the generalization ability.
3. VILLA [210] is a two-stage strategy (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning)
4. that improves VLP Transformers.
5. In practice, the distribution gap between training data and practical data is noticeable.
6. For instance, supervised data samples (well-labelled, well-aligned) are costly in practical applications, thus how to transfer the supervised multimodal Transformers pretrained on well-aligned cross-modal pairs/tuples to the weakly aligned test bed is challenging [137].
7. CLIP [9] is an inspiring solution that transfers knowledge across modalities by learning a shared multimodal embedding space, enabling zero-shot transfer of the model to down-stream tasks.
8. The main inspiration that CLIP presents the community is that the pretrained multimodal (image and text) knowledge can be transferred to down-stream zero-shot image prediction by using a prompt template ""A photo of a {label}.""
9. to bridge the distribution gap between training and test datasets.
10. Over-fitting is a major obstacle to transfer.
11. Multimodal Transformers can be overly fitted to the dataset biases during training, due to the large modelling capability.
12. Some recent practices exploit how to transfer the oracle model trained on noiseless dataset to real dataset.
13. For instance, Kervadec et al. [272], [273] explore how transferable reasoning patterns are in VQA, and demonstrate that for LXMERT [103]/BERT-like reasoning patterns can be partially transferred from an ideal dataset to a real dataset.
14. Cross-task gap is another major obstacle to transfer (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning)4, (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning)1, due to the different reasoning and input-output workflows, e.g., how to use multimodal datasets to finetune the language pretrained model is difficult (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning)1.
15. In real applications, multimodal pretrained Transformers sometimes need to handle the uni-modal data at inference stage due to the issue of missing modalities.
16. One solution is using knowledge distillation, e.g., distilling from multimodal to uni-modal attention in Transformers (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning)2, distilling from multiple uni-modal Transformer teachers to a shared Transformer encoder (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning)3.
17. There is a huge gap across discriminative and generative multimodal tasks.
18. As discussed in (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning)4, the BERT-like encoder-only multimodal Transformers (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning)5 need separately to train decoders for generation tasks.
19. This could create a pretrain-finetune discrepancy detrimental to the generality.
20. Recently, more and more attempts study this issue further, e.g., GilBERT (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning)6 is a generative VLP models for a discriminative task, i.e., image-text retrieval.
21. Cross-lingual gap also should be considered for the transferability of Transformer based multimodal learning, e.g., universal cross-lingual generalization from English to non-English multimodal contexts (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning)7, (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning)8.","What strategies enhance the generalization ability of multimodal Transformers across different datasets and applications?
How does the CLIP model facilitate zero-shot transfer learning in multimodal learning environments?
What are the challenges and solutions in transferring oracle models trained on noiseless datasets to real-world datasets in the context of multimodal learning?
How does knowledge distillation contribute to addressing the issue of missing modalities in multimodal pretrained Transformers during inference?
What are the implications of the cross-lingual gap for the transferability of Transformer-based multimodal learning models?","1. What strategies enhance the generalization ability of multimodal Transformers across different datasets and applications?
2. How does the CLIP model facilitate zero-shot transfer learning in multimodal learning environments?
3. What are the challenges and solutions in transferring oracle models trained on noiseless datasets to real-world datasets in the context of multimodal learning?
4. How does knowledge distillation contribute to addressing the issue of missing modalities in multimodal pretrained Transformers during inference?
5. What are the implications of the cross-lingual gap for the transferability of Transformer-based multimodal learning models?",None,,"Questions:

1. What strategies enhance the generalization ability of multimodal Transformers across different datasets and applications?
2. How does the CLIP model facilitate zero-shot transfer learning in multimodal learning environments?
3. What are the challenges and solutions in transferring oracle models trained on noiseless datasets to real-world datasets in the context of multimodal learning?
4. How does knowledge distillation contribute to addressing the issue of missing modalities in multimodal pretrained Transformers during inference?
5. What are the implications of the cross-lingual gap for the transferability of Transformer-based multimodal learning models?

Answer:

(p37.0) Transferability is a major challenge for Transformer based multimodal learning, involving the question of how to transfer models across different datasets and applications.

(p37.1) Data augmentation and adversarial perturbation strategies help multimodal Transformers to improve the generalization ability. VILLA [210] is a two-stage strategy (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning) that improves VLP Transformers.

(p37.2) In practice, the distribution gap between training data and practical data is noticeable. For instance, supervised data samples (well-labelled, well-aligned) are costly in practical applications, thus how to transfer the supervised multimodal Transformers pretrained on well-aligned cross-modal pairs/tuples to the weakly aligned test bed is challenging [137]. CLIP [9] is an inspiring solution that transfers knowledge across modalities by learning a shared multimodal embedding space, enabling zero-shot transfer of the model to down-stream tasks. The main inspiration that CLIP presents the community is that the pretrained multimodal (image and text) knowledge can be transferred to down-stream zero-shot image prediction by using a prompt template ""A photo of a {label}."" to bridge the distribution gap between training and test datasets.

(p37.3) Over-fitting is a major obstacle to transfer. Multimodal Transformers can be overly fitted to the dataset biases during training, due to the large modelling capability. Some recent practices exploit how to transfer the oracle model trained on noiseless dataset to real dataset. For instance, Kervadec et al. [272], [273] explore how transferable reasoning patterns are in VQA, and demonstrate that for LXMERT [103]/BERT-like reasoning patterns can be partially transferred from an ideal dataset to a real dataset.

(p37.4) Cross-task gap is another major obstacle to transfer [208], [274], due to the different reasoning and input-output workflows, e.g., how to use multimodal datasets to finetune the language pretrained model is difficult [274]. In real applications, multimodal pretrained Transformers sometimes need to handle the uni-modal data at inference stage due to the issue of missing modalities. One solution is using knowledge distillation, e.g., distilling from multimodal to uni-modal attention in Transformers [275], distilling from multiple uni-modal Transformer teachers to a shared Transformer encoder [276]. There is a huge gap across discriminative and generative multimodal tasks. As discussed in [208], the BERT-like encoder-only multimodal Transformers (e.g., VideoBERT [7], CBT [107]) need separately to train decoders for generation tasks. This could create a pretrain-finetune discrepancy detrimental to the generality. Recently, more and more attempts study this issue further, e.g., GilBERT [222] is a generative VLP models for a discriminative task, i.e., image-text retrieval.

(p37.5) Cross-lingual gap also should be considered for the transferability of Transformer based multimodal learning, e.g., universal cross-lingual generalization from English to non-English multimodal contexts [206], [277].","Questions:



Answer:

(p37.0) Transferability is a major challenge for Transformer based multimodal learning, involving the question of how to transfer models across different datasets and applications.

(p37.1) Data augmentation and adversarial perturbation strategies help multimodal Transformers to improve the generalization ability. VILLA [210] is a two-stage strategy (taskagnostic adversarial pretraining, followed by task-specific adversarial finetuning) that improves VLP Transformers.

(p37.2) In practice, the distribution gap between training data and practical data is noticeable. For instance, supervised data samples (well-labelled, well-aligned) are costly in practical applications, thus how to transfer the supervised multimodal Transformers pretrained on well-aligned cross-modal pairs/tuples to the weakly aligned test bed is challenging [137]. CLIP [9] is an inspiring solution that transfers knowledge across modalities by learning a shared multimodal embedding space, enabling zero-shot transfer of the model to down-stream tasks. The main inspiration that CLIP presents the community is that the pretrained multimodal (image and text) knowledge can be transferred to down-stream zero-shot image prediction by using a prompt template ""A photo of a {label}."" to bridge the distribution gap between training and test datasets.

(p37.3) Over-fitting is a major obstacle to transfer. Multimodal Transformers can be overly fitted to the dataset biases during training, due to the large modelling capability. Some recent practices exploit how to transfer the oracle model trained on noiseless dataset to real dataset. For instance, Kervadec et al. [272], [273] explore how transferable reasoning patterns are in VQA, and demonstrate that for LXMERT [103]/BERT-like reasoning patterns can be partially transferred from an ideal dataset to a real dataset.

(p37.4) Cross-task gap is another major obstacle to transfer [208], [274], due to the different reasoning and input-output workflows, e.g., how to use multimodal datasets to finetune the language pretrained model is difficult [274]. In real applications, multimodal pretrained Transformers sometimes need to handle the uni-modal data at inference stage due to the issue of missing modalities. One solution is using knowledge distillation, e.g., distilling from multimodal to uni-modal attention in Transformers [275], distilling from multiple uni-modal Transformer teachers to a shared Transformer encoder [276]. There is a huge gap across discriminative and generative multimodal tasks. As discussed in [208], the BERT-like encoder-only multimodal Transformers (e.g., VideoBERT [7], CBT [107]) need separately to train decoders for generation tasks. This could create a pretrain-finetune discrepancy detrimental to the generality. Recently, more and more attempts study this issue further, e.g., GilBERT [222] is a generative VLP models for a discriminative task, i.e., image-text retrieval.

(p37.5) Cross-lingual gap also should be considered for the transferability of Transformer based multimodal learning, e.g., universal cross-lingual generalization from English to non-English multimodal contexts [206], [277]."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s38,Efficiency,"['p38.0', 'p38.1', 'p38.2', 'p38.3', 'p38.4', 'p38.5', 'p38.6', 'p38.7', 'p38.8']","['Multimodal Transformers suffer from two major efficiency issues: (1) Due to the large model parameter capacity, they are data hungry and thus dependent on huge scale training datasets. (2) They are limited by the time and memory complexities that grow quadratically with the input sequence length, which are caused by the self-attention. In multimodal contexts, calculation explosion will become worse due to jointly high dimension representations. These two bottlenecks are interdependent and should be considered together.', 'To improve the training and/or inferring efficiency for multimodal Transformers, recent efforts have attempted to find various solutions, to use fewer training data and/or parameters. The main ideas can be summarized as the follows.', '(1) Knowledge distillation. Distill the knowledge from the trained larger Transformers to smaller Transformers [93]. Miech et al. [278] conduct distillation from a slower model (early concatenation based Transformers, O((N (A) +N (B) ) 2 )) to a faster one (independently dual branch Transformers, O(N 2 (A) )). (2) Simplifying and compressing model. Remove the components to simplify the pipelines. Taking the VLP Transformer models as an example, two-stage pipeline is costly as they need object detector. One simplifying is processing the visual input in convolution-free manner, e.g., E2E-VLP [271], ViLT [192]. DropToken [174] reduces the training complexity via random dropping a portion of the video and audio tokens from input sequence during training. DropToken can be treated as an implementation of dropout or adversarial training. Weight-sharing is also a common practice for simplifying multimodal Transformer models. Wen et al. [279] present a weight-sharing Transformer on top of the visual and textual encoders to align text and image. Lee et al. [280] propose a novel parameter sharing scheme based on lowrank approximation.', '(3) Asymmetrical network structures. Assign different model capacities and computational size properly for different modalities, to save parameters. See Figure 2 in [192].', '(4) Improving utilization of training samples. Liu et al. [281] train a simplified LXMERT by making full use of fewer samples at different granularities. Li et al. [282] use fewer data to train CLIP by fully mining the potential self-supervised signals of (a) self-supervision within each modality, (b) multi-view supervision across modalities, and (c) nearest-neighbour supervision from other similar pairs.', '(5) Compressing and pruning model. Search the optimal sub-structures/sub-networks of multimodal Transformers, e.g., playing Lottery Tickets with the VLP Transformer models [283], adaptively freezing some layers during training [284].', '(6) Optimizing the complexity of self-attention. Transformers cost time and memory that grows quadratically with the input sequence length [285]. One potential solution is optimizing the O(N 2 ) complexity, e.g., Child et al. [286] present sparse factorizations of the attention matrix to reduce the quadratical complexity to O(n √ n), Transformer-LS [287] is an efficient Transformer for both language and vision long sequence, with linear computational and memory complexity.', '(7) Optimizing the complexity of self-attention based multimodal interaction/fusion. Nagrani et al. [175] propose Fusion via Attention Bottlenecks (FSN, fusion bottleneck) to improve the early concatenation based multimodal interaction. FSN passes on the messages through a small number of bottleneck latents, thus requiring the model to purify the most necessary information from each modality for crossmodal sharing. This strategy uses the fusion bottleneck as a bridge, and not only improves fusion performance, but also reduces computational cost.', '(8) Optimizing other strategies. Use optimal strategies to perform the common Transformer based multimodal interactions. Given the quadratic complexity of self-attention, using early concatenation based multimodal interaction to synchronously fuse the inputs from multiple modalities/views is costly. Yan et al. [288] present an efficient solution that sequentially fuses information between all pairs of two adjacent views in ascending order of sequence length. This is intrinsically a greedy strategy.']","Multimodal Transformers suffer from two major efficiency issues: (1) Due to the large model parameter capacity, they are data hungry and thus dependent on huge scale training datasets. (2) They are limited by the time and memory complexities that grow quadratically with the input sequence length, which are caused by the self-attention. In multimodal contexts, calculation explosion will become worse due to jointly high dimension representations. These two bottlenecks are interdependent and should be considered together.

To improve the training and/or inferring efficiency for multimodal Transformers, recent efforts have attempted to find various solutions, to use fewer training data and/or parameters. The main ideas can be summarized as the follows.

(1) Knowledge distillation. Distill the knowledge from the trained larger Transformers to smaller Transformers [93]. Miech et al. [278] conduct distillation from a slower model (early concatenation based Transformers, O((N (A) +N (B) ) 2 )) to a faster one (independently dual branch Transformers, O(N 2 (A) )). (2) Simplifying and compressing model. Remove the components to simplify the pipelines. Taking the VLP Transformer models as an example, two-stage pipeline is costly as they need object detector. One simplifying is processing the visual input in convolution-free manner, e.g., E2E-VLP [271], ViLT [192]. DropToken [174] reduces the training complexity via random dropping a portion of the video and audio tokens from input sequence during training. DropToken can be treated as an implementation of dropout or adversarial training. Weight-sharing is also a common practice for simplifying multimodal Transformer models. Wen et al. [279] present a weight-sharing Transformer on top of the visual and textual encoders to align text and image. Lee et al. [280] propose a novel parameter sharing scheme based on lowrank approximation.

(3) Asymmetrical network structures. Assign different model capacities and computational size properly for different modalities, to save parameters. See Figure 2 in [192].

(4) Improving utilization of training samples. Liu et al. [281] train a simplified LXMERT by making full use of fewer samples at different granularities. Li et al. [282] use fewer data to train CLIP by fully mining the potential self-supervised signals of (a) self-supervision within each modality, (b) multi-view supervision across modalities, and (c) nearest-neighbour supervision from other similar pairs.

(5) Compressing and pruning model. Search the optimal sub-structures/sub-networks of multimodal Transformers, e.g., playing Lottery Tickets with the VLP Transformer models [283], adaptively freezing some layers during training [284].

(6) Optimizing the complexity of self-attention. Transformers cost time and memory that grows quadratically with the input sequence length [285]. One potential solution is optimizing the O(N 2 ) complexity, e.g., Child et al. [286] present sparse factorizations of the attention matrix to reduce the quadratical complexity to O(n √ n), Transformer-LS [287] is an efficient Transformer for both language and vision long sequence, with linear computational and memory complexity.

(7) Optimizing the complexity of self-attention based multimodal interaction/fusion. Nagrani et al. [175] propose Fusion via Attention Bottlenecks (FSN, fusion bottleneck) to improve the early concatenation based multimodal interaction. FSN passes on the messages through a small number of bottleneck latents, thus requiring the model to purify the most necessary information from each modality for crossmodal sharing. This strategy uses the fusion bottleneck as a bridge, and not only improves fusion performance, but also reduces computational cost.

(8) Optimizing other strategies. Use optimal strategies to perform the common Transformer based multimodal interactions. Given the quadratic complexity of self-attention, using early concatenation based multimodal interaction to synchronously fuse the inputs from multiple modalities/views is costly. Yan et al. [288] present an efficient solution that sequentially fuses information between all pairs of two adjacent views in ascending order of sequence length. This is intrinsically a greedy strategy.","(p38.0) Multimodal Transformers suffer from two major efficiency issues: (1) Due to the large model parameter capacity, they are data hungry and thus dependent on huge scale training datasets. (2) They are limited by the time and memory complexities that grow quadratically with the input sequence length, which are caused by the self-attention. In multimodal contexts, calculation explosion will become worse due to jointly high dimension representations. These two bottlenecks are interdependent and should be considered together.

(p38.1) To improve the training and/or inferring efficiency for multimodal Transformers, recent efforts have attempted to find various solutions, to use fewer training data and/or parameters. The main ideas can be summarized as the follows.

(p38.2) (1) Knowledge distillation. Distill the knowledge from the trained larger Transformers to smaller Transformers [93]. Miech et al. [278] conduct distillation from a slower model (early concatenation based Transformers, O((N (A) +N (B) ) 2 )) to a faster one (independently dual branch Transformers, O(N 2 (A) )). (2) Simplifying and compressing model. Remove the components to simplify the pipelines. Taking the VLP Transformer models as an example, two-stage pipeline is costly as they need object detector. One simplifying is processing the visual input in convolution-free manner, e.g., E2E-VLP [271], ViLT [192]. DropToken [174] reduces the training complexity via random dropping a portion of the video and audio tokens from input sequence during training. DropToken can be treated as an implementation of dropout or adversarial training. Weight-sharing is also a common practice for simplifying multimodal Transformer models. Wen et al. [279] present a weight-sharing Transformer on top of the visual and textual encoders to align text and image. Lee et al. [280] propose a novel parameter sharing scheme based on lowrank approximation.

(p38.3) (3) Asymmetrical network structures. Assign different model capacities and computational size properly for different modalities, to save parameters. See Figure 2 in [192].

(p38.4) (4) Improving utilization of training samples. Liu et al. [281] train a simplified LXMERT by making full use of fewer samples at different granularities. Li et al. [282] use fewer data to train CLIP by fully mining the potential self-supervised signals of (a) self-supervision within each modality, (b) multi-view supervision across modalities, and (c) nearest-neighbour supervision from other similar pairs.

(p38.5) (5) Compressing and pruning model. Search the optimal sub-structures/sub-networks of multimodal Transformers, e.g., playing Lottery Tickets with the VLP Transformer models [283], adaptively freezing some layers during training [284].

(p38.6) (6) Optimizing the complexity of self-attention. Transformers cost time and memory that grows quadratically with the input sequence length [285]. One potential solution is optimizing the O(N 2 ) complexity, e.g., Child et al. [286] present sparse factorizations of the attention matrix to reduce the quadratical complexity to O(n √ n), Transformer-LS [287] is an efficient Transformer for both language and vision long sequence, with linear computational and memory complexity.

(p38.7) (7) Optimizing the complexity of self-attention based multimodal interaction/fusion. Nagrani et al. [175] propose Fusion via Attention Bottlenecks (FSN, fusion bottleneck) to improve the early concatenation based multimodal interaction. FSN passes on the messages through a small number of bottleneck latents, thus requiring the model to purify the most necessary information from each modality for crossmodal sharing. This strategy uses the fusion bottleneck as a bridge, and not only improves fusion performance, but also reduces computational cost.

(p38.8) (8) Optimizing other strategies. Use optimal strategies to perform the common Transformer based multimodal interactions. Given the quadratic complexity of self-attention, using early concatenation based multimodal interaction to synchronously fuse the inputs from multiple modalities/views is costly. Yan et al. [288] present an efficient solution that sequentially fuses information between all pairs of two adjacent views in ascending order of sequence length. This is intrinsically a greedy strategy.","[['b0'], [], ['b173', 'b278', 'b191', 'b279', 'b270', 'b277', 'b92'], ['b191'], ['b280', 'b281'], ['b282', 'b283'], ['b285', 'b286', 'b284'], ['b174'], ['b287']]","[['b0'], [], ['b173', 'b278', 'b191', 'b279', 'b270', 'b277', 'b92'], ['b191'], ['b280', 'b281'], ['b282', 'b283'], ['b285', 'b286', 'b284'], ['b174'], ['b287']]",18,"1. Multimodal Transformers suffer from two major efficiency issues: (1) Due to the large model parameter capacity, they are data hungry and thus dependent on huge scale training datasets.
2. (2) They are limited by the time and memory complexities that grow quadratically with the input sequence length, which are caused by the self-attention.
3. In multimodal contexts, calculation explosion will become worse due to jointly high dimension representations.
4. These two bottlenecks are interdependent and should be considered together.
5. To improve the training and/or inferring efficiency for multimodal Transformers, recent efforts have attempted to find various solutions, to use fewer training data and/or parameters.
6. The main ideas can be summarized as the follows.
7. (1) Knowledge distillation. Distill the knowledge from the trained larger Transformers to smaller Transformers [93].
8. Miech et al. [278] conduct distillation from a slower model (early concatenation based Transformers, O((N (A)
9. +N (B) ) 2 )) to a faster one (independently dual branch Transformers, O(N 2 (A) )).
10. (2) Simplifying and compressing model.
11. Remove the components to simplify the pipelines.
12. Taking the VLP Transformer models as an example, two-stage pipeline is costly as they need object detector.
13. One simplifying is processing the visual input in convolution-free manner, e.g., E2E-VLP [271], ViLT [192].
14. DropToken [174] reduces the training complexity via random dropping a portion of the video and audio tokens from input sequence during training.
15. DropToken can be treated as an implementation of dropout or adversarial training.
16. Weight-sharing is also a common practice for simplifying multimodal Transformer models.
17. Wen et al. [279] present a weight-sharing Transformer on top of the visual and textual encoders to align text and image.
18. Lee et al. [280] propose a novel parameter sharing scheme based on lowrank approximation.
19. (3) Asymmetrical network structures.
20. Assign different model capacities and computational size properly for different modalities, to save parameters.
21. See Figure 2 in [192]. (4) Improving utilization of training samples.
22. Liu et al. [281] train a simplified LXMERT by making full use of fewer samples at different granularities.
23. Li et al. [282] use fewer data to train CLIP by fully mining the potential self-supervised signals of (a) self-supervision within each modality, (1)0 multi-view supervision across modalities, and (1)1 nearest-neighbour supervision from other similar pairs.
24. (1)2 Compressing and pruning model.
25. Search the optimal sub-structures/sub-networks of multimodal Transformers, e.g., playing Lottery Tickets with the VLP Transformer models (1)3, adaptively freezing some layers during training (1)4.
26. (1)5 Optimizing the complexity of self-attention.
27. Transformers cost time and memory that grows quadratically with the input sequence length (1)6.
28. One potential solution is optimizing the O(1)7 complexity, e.g., Child et al. (1)8 present sparse factorizations of the attention matrix to reduce the quadratical complexity to O(1)9, Transformer-LS [93]0 is an efficient Transformer for both language and vision long sequence, with linear computational and memory complexity.
29. [93]1 Optimizing the complexity of self-attention based multimodal interaction/fusion.
30. Nagrani et al. [93]2 propose Fusion via Attention Bottlenecks [93]3 to improve the early concatenation based multimodal interaction.
31. FSN passes on the messages through a small number of bottleneck latents, thus requiring the model to purify the most necessary information from each modality for crossmodal sharing.
32. This strategy uses the fusion bottleneck as a bridge, and not only improves fusion performance, but also reduces computational cost.
33. [93]4 Optimizing other strategies.
34. Use optimal strategies to perform the common Transformer based multimodal interactions.
35. Given the quadratic complexity of self-attention, using early concatenation based multimodal interaction to synchronously fuse the inputs from multiple modalities/views is costly.
36. Yan et al. [93]5 present an efficient solution that sequentially fuses information between all pairs of two adjacent views in ascending order of sequence length.
37. This is intrinsically a greedy strategy.","What are the primary efficiency issues faced by multimodal Transformers and how are they interdependent?
How does knowledge distillation contribute to improving the efficiency of multimodal Transformers?
In what ways can simplifying and compressing the model architecture enhance the efficiency of multimodal Transformers?
What role does optimizing the complexity of self-attention play in addressing the efficiency challenges of multimodal Transformers?
How do asymmetrical network structures and optimizing multimodal interaction/fusion contribute to the efficiency of multimodal learning systems?","1. What are the primary efficiency issues faced by multimodal Transformers and how are they interdependent?
2. How does knowledge distillation contribute to improving the efficiency of multimodal Transformers?
3. In what ways can simplifying and compressing the model architecture enhance the efficiency of multimodal Transformers?
4. What role does optimizing the complexity of self-attention play in addressing the efficiency challenges of multimodal Transformers?
5. How do asymmetrical network structures and optimizing multimodal interaction/fusion contribute to the efficiency of multimodal learning systems?","# What are the major efficiency issues faced by multimodal Transformers and how can they be addressed?
  - Multimodal Transformers are data hungry due to their large model parameter capacity, requiring huge scale training datasets.
  - They suffer from time and memory complexities that grow quadratically with the input sequence length due to self-attention mechanisms, which worsens in multimodal contexts due to high dimension representations.

# What strategies have been proposed to improve the training and/or inferring efficiency for multimodal Transformers?
  - **Knowledge distillation**: Distilling knowledge from larger to smaller Transformers to use fewer training data and/or parameters.
  - **Simplifying and compressing model**: Removing components to simplify pipelines, using convolution-free methods for visual input, implementing dropout or adversarial training techniques like DropToken, and employing weight-sharing to align text and image.
  - **Asymmetrical network structures**: Assigning different model capacities and computational sizes for different modalities to save parameters.
  - **Improving utilization of training samples**: Training models with fewer samples at different granularities and fully mining the potential self-supervised signals within and across modalities.
  - **Compressing and pruning model**: Searching for optimal sub-structures/sub-networks and adaptively freezing some layers during training.
  - **Optimizing the complexity of self-attention**: Reducing the quadratic complexity of self-attention to linear or sub-linear to save time and memory.
  - **Optimizing the complexity of self-attention based multimodal interaction/fusion**: Using strategies like Fusion via Attention Bottlenecks to improve performance and reduce computational cost.
  - **Optimizing other strategies**: Employing efficient solutions for multimodal interactions to reduce the computational cost of early concatenation based multimodal interaction.","1. What are the major efficiency issues faced by multimodal Transformers and how can they be addressed?
2. What strategies have been proposed to improve the training and/or inferring efficiency for multimodal Transformers?","Questions:

1. What are the primary efficiency issues faced by multimodal Transformers and how are they interdependent?
2. How does knowledge distillation contribute to improving the efficiency of multimodal Transformers?
3. In what ways can simplifying and compressing the model architecture enhance the efficiency of multimodal Transformers?
4. What role does optimizing the complexity of self-attention play in addressing the efficiency challenges of multimodal Transformers?
5. How do asymmetrical network structures and optimizing multimodal interaction/fusion contribute to the efficiency of multimodal learning systems?

Answer:

(p38.0) Multimodal Transformers suffer from two major efficiency issues: (1) Due to the large model parameter capacity, they are data hungry and thus dependent on huge scale training datasets. (2) They are limited by the time and memory complexities that grow quadratically with the input sequence length, which are caused by the self-attention. In multimodal contexts, calculation explosion will become worse due to jointly high dimension representations. These two bottlenecks are interdependent and should be considered together.

(p38.1) To improve the training and/or inferring efficiency for multimodal Transformers, recent efforts have attempted to find various solutions, to use fewer training data and/or parameters. The main ideas can be summarized as the follows.

(p38.2) (1) Knowledge distillation. Distill the knowledge from the trained larger Transformers to smaller Transformers [93]. Miech et al. [278] conduct distillation from a slower model (early concatenation based Transformers, O((N (A) +N (B) ) 2 )) to a faster one (independently dual branch Transformers, O(N 2 (A) )). (2) Simplifying and compressing model. Remove the components to simplify the pipelines. Taking the VLP Transformer models as an example, two-stage pipeline is costly as they need object detector. One simplifying is processing the visual input in convolution-free manner, e.g., E2E-VLP [271], ViLT [192]. DropToken [174] reduces the training complexity via random dropping a portion of the video and audio tokens from input sequence during training. DropToken can be treated as an implementation of dropout or adversarial training. Weight-sharing is also a common practice for simplifying multimodal Transformer models. Wen et al. [279] present a weight-sharing Transformer on top of the visual and textual encoders to align text and image. Lee et al. [280] propose a novel parameter sharing scheme based on lowrank approximation.

(p38.3) (3) Asymmetrical network structures. Assign different model capacities and computational size properly for different modalities, to save parameters. See Figure 2 in [192].

(p38.4) (4) Improving utilization of training samples. Liu et al. [281] train a simplified LXMERT by making full use of fewer samples at different granularities. Li et al. [282] use fewer data to train CLIP by fully mining the potential self-supervised signals of (a) self-supervision within each modality, (b) multi-view supervision across modalities, and (c) nearest-neighbour supervision from other similar pairs.

(p38.5) (5) Compressing and pruning model. Search the optimal sub-structures/sub-networks of multimodal Transformers, e.g., playing Lottery Tickets with the VLP Transformer models [283], adaptively freezing some layers during training [284].

(p38.6) (6) Optimizing the complexity of self-attention. Transformers cost time and memory that grows quadratically with the input sequence length [285]. One potential solution is optimizing the O(N 2 ) complexity, e.g., Child et al. [286] present sparse factorizations of the attention matrix to reduce the quadratical complexity to O(n √ n), Transformer-LS [287] is an efficient Transformer for both language and vision long sequence, with linear computational and memory complexity.

(p38.7) (7) Optimizing the complexity of self-attention based multimodal interaction/fusion. Nagrani et al. [175] propose Fusion via Attention Bottlenecks (FSN, fusion bottleneck) to improve the early concatenation based multimodal interaction. FSN passes on the messages through a small number of bottleneck latents, thus requiring the model to purify the most necessary information from each modality for crossmodal sharing. This strategy uses the fusion bottleneck as a bridge, and not only improves fusion performance, but also reduces computational cost.

(p38.8) (8) Optimizing other strategies. Use optimal strategies to perform the common Transformer based multimodal interactions. Given the quadratic complexity of self-attention, using early concatenation based multimodal interaction to synchronously fuse the inputs from multiple modalities/views is costly. Yan et al. [288] present an efficient solution that sequentially fuses information between all pairs of two adjacent views in ascending order of sequence length. This is intrinsically a greedy strategy.","Questions:

1. What are the major efficiency issues faced by multimodal Transformers and how can they be addressed?
2. What strategies have been proposed to improve the training and/or inferring efficiency for multimodal Transformers?

Answer:

(p38.0) Multimodal Transformers suffer from two major efficiency issues: (1) Due to the large model parameter capacity, they are data hungry and thus dependent on huge scale training datasets. (2) They are limited by the time and memory complexities that grow quadratically with the input sequence length, which are caused by the self-attention. In multimodal contexts, calculation explosion will become worse due to jointly high dimension representations. These two bottlenecks are interdependent and should be considered together.

(p38.1) To improve the training and/or inferring efficiency for multimodal Transformers, recent efforts have attempted to find various solutions, to use fewer training data and/or parameters. The main ideas can be summarized as the follows.

(p38.2) (1) Knowledge distillation. Distill the knowledge from the trained larger Transformers to smaller Transformers [93]. Miech et al. [278] conduct distillation from a slower model (early concatenation based Transformers, O((N (A) +N (B) ) 2 )) to a faster one (independently dual branch Transformers, O(N 2 (A) )). (2) Simplifying and compressing model. Remove the components to simplify the pipelines. Taking the VLP Transformer models as an example, two-stage pipeline is costly as they need object detector. One simplifying is processing the visual input in convolution-free manner, e.g., E2E-VLP [271], ViLT [192]. DropToken [174] reduces the training complexity via random dropping a portion of the video and audio tokens from input sequence during training. DropToken can be treated as an implementation of dropout or adversarial training. Weight-sharing is also a common practice for simplifying multimodal Transformer models. Wen et al. [279] present a weight-sharing Transformer on top of the visual and textual encoders to align text and image. Lee et al. [280] propose a novel parameter sharing scheme based on lowrank approximation.

(p38.3) (3) Asymmetrical network structures. Assign different model capacities and computational size properly for different modalities, to save parameters. See Figure 2 in [192].

(p38.4) (4) Improving utilization of training samples. Liu et al. [281] train a simplified LXMERT by making full use of fewer samples at different granularities. Li et al. [282] use fewer data to train CLIP by fully mining the potential self-supervised signals of (a) self-supervision within each modality, (b) multi-view supervision across modalities, and (c) nearest-neighbour supervision from other similar pairs.

(p38.5) (5) Compressing and pruning model. Search the optimal sub-structures/sub-networks of multimodal Transformers, e.g., playing Lottery Tickets with the VLP Transformer models [283], adaptively freezing some layers during training [284].

(p38.6) (6) Optimizing the complexity of self-attention. Transformers cost time and memory that grows quadratically with the input sequence length [285]. One potential solution is optimizing the O(N 2 ) complexity, e.g., Child et al. [286] present sparse factorizations of the attention matrix to reduce the quadratical complexity to O(n √ n), Transformer-LS [287] is an efficient Transformer for both language and vision long sequence, with linear computational and memory complexity.

(p38.7) (7) Optimizing the complexity of self-attention based multimodal interaction/fusion. Nagrani et al. [175] propose Fusion via Attention Bottlenecks (FSN, fusion bottleneck) to improve the early concatenation based multimodal interaction. FSN passes on the messages through a small number of bottleneck latents, thus requiring the model to purify the most necessary information from each modality for crossmodal sharing. This strategy uses the fusion bottleneck as a bridge, and not only improves fusion performance, but also reduces computational cost.

(p38.8) (8) Optimizing other strategies. Use optimal strategies to perform the common Transformer based multimodal interactions. Given the quadratic complexity of self-attention, using early concatenation based multimodal interaction to synchronously fuse the inputs from multiple modalities/views is costly. Yan et al. [288] present an efficient solution that sequentially fuses information between all pairs of two adjacent views in ascending order of sequence length. This is intrinsically a greedy strategy."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s39,Robustness,"['p39.0', 'p39.1', 'p39.2']","['Multimodal Transformers pretrained on large-scale corpora achieve the state-of-the-art for various multimodal applications, while their robustness is still unclear and understudied. This at least involves two key challenges, i.e., how to theoretically analyse the robustness, how to improve the robustness.', 'Although that recent attempts [99], [182], [289], [290] study and evaluate how the Transformer components/sublayers contribute to the robustness, the main bottleneck is that the community lacks theoretical tools to analyse the Transformer family. Recently, the common practices to analyse robustness are mainly based on experiment evaluations [291], e.g., cross-dataset evaluations, perturbationbased evaluations. Thus, some multimodal datasets [130], [292] are proposed for evaluating the robustness.', 'Recent attempts mainly use two straightforward methods to improve the robustness for multimodal Transformer models: (1) augmentation and adversarial learning based strategies [293], [294], (2) fine-grained loss functions [295]. For instance: VILLA [210] is a generic adversarial training framework that can be applied to various multimodal Transformers. Akula et al. [292] empirically demonstrate that ViL-BERT fails to exploit linguistic structure, and they propose two methods to improve the robustness of ViLBERT, one based on contrastive learning and the other based on multitask learning.']","Multimodal Transformers pretrained on large-scale corpora achieve the state-of-the-art for various multimodal applications, while their robustness is still unclear and understudied. This at least involves two key challenges, i.e., how to theoretically analyse the robustness, how to improve the robustness.

Although that recent attempts [99], [182], [289], [290] study and evaluate how the Transformer components/sublayers contribute to the robustness, the main bottleneck is that the community lacks theoretical tools to analyse the Transformer family. Recently, the common practices to analyse robustness are mainly based on experiment evaluations [291], e.g., cross-dataset evaluations, perturbationbased evaluations. Thus, some multimodal datasets [130], [292] are proposed for evaluating the robustness.

Recent attempts mainly use two straightforward methods to improve the robustness for multimodal Transformer models: (1) augmentation and adversarial learning based strategies [293], [294], (2) fine-grained loss functions [295]. For instance: VILLA [210] is a generic adversarial training framework that can be applied to various multimodal Transformers. Akula et al. [292] empirically demonstrate that ViL-BERT fails to exploit linguistic structure, and they propose two methods to improve the robustness of ViLBERT, one based on contrastive learning and the other based on multitask learning.","(p39.0) Multimodal Transformers pretrained on large-scale corpora achieve the state-of-the-art for various multimodal applications, while their robustness is still unclear and understudied. This at least involves two key challenges, i.e., how to theoretically analyse the robustness, how to improve the robustness.

(p39.1) Although that recent attempts [99], [182], [289], [290] study and evaluate how the Transformer components/sublayers contribute to the robustness, the main bottleneck is that the community lacks theoretical tools to analyse the Transformer family. Recently, the common practices to analyse robustness are mainly based on experiment evaluations [291], e.g., cross-dataset evaluations, perturbationbased evaluations. Thus, some multimodal datasets [130], [292] are proposed for evaluating the robustness.

(p39.2) Recent attempts mainly use two straightforward methods to improve the robustness for multimodal Transformer models: (1) augmentation and adversarial learning based strategies [293], [294], (2) fine-grained loss functions [295]. For instance: VILLA [210] is a generic adversarial training framework that can be applied to various multimodal Transformers. Akula et al. [292] empirically demonstrate that ViL-BERT fails to exploit linguistic structure, and they propose two methods to improve the robustness of ViLBERT, one based on contrastive learning and the other based on multitask learning.","[[], ['b98', 'b288', 'b129', 'b181', 'b289', 'b290', 'b291'], ['b294', 'b292', 'b209', 'b293', 'b291']]","[[], ['b98', 'b288', 'b129', 'b181', 'b289', 'b290', 'b291'], ['b294', 'b292', 'b209', 'b293', 'b291']]",12,"1. Multimodal Transformers pretrained on large-scale corpora achieve the state-of-the-art for various multimodal applications, while their robustness is still unclear and understudied.
2. This at least involves two key challenges, i.e., how to theoretically analyse the robustness, how to improve the robustness.
3. Although that recent attempts [99], [182], [289], [290] study and evaluate how the Transformer components/sublayers contribute to the robustness, the main bottleneck is that the community lacks theoretical tools to analyse the Transformer family.
4. Recently, the common practices to analyse robustness are mainly based on experiment evaluations [291], e.g., cross-dataset evaluations, perturbationbased evaluations.
5. Thus, some multimodal datasets [130], [182]3 are proposed for evaluating the robustness.
6. Recent attempts mainly use two straightforward methods to improve the robustness for multimodal Transformer models: (1) augmentation and adversarial learning based strategies [293], [294], [182]0 fine-grained loss functions [182]1.
7. For instance: VILLA [182]2 is a generic adversarial training framework that can be applied to various multimodal Transformers.
8. Akula et al. [182]3 empirically demonstrate that ViL-BERT fails to exploit linguistic structure, and they propose two methods to improve the robustness of ViLBERT, one based on contrastive learning and the other based on multitask learning.","What are the key challenges in analyzing and improving the robustness of multimodal Transformers?
How do current studies evaluate the robustness of Transformer components in multimodal applications?
What are the common practices for analyzing the robustness of multimodal Transformers?
What strategies have recent attempts employed to enhance the robustness of multimodal Transformer models?
How does the VILLA framework contribute to the robustness of various multimodal Transformers?","1. What are the key challenges in analyzing and improving the robustness of multimodal Transformers?
2. How do current studies evaluate the robustness of Transformer components in multimodal applications?
3. What are the common practices for analyzing the robustness of multimodal Transformers?
4. What strategies have recent attempts employed to enhance the robustness of multimodal Transformer models?
5. How does the VILLA framework contribute to the robustness of various multimodal Transformers?",None,,"Questions:

1. What are the key challenges in analyzing and improving the robustness of multimodal Transformers?
2. How do current studies evaluate the robustness of Transformer components in multimodal applications?
3. What are the common practices for analyzing the robustness of multimodal Transformers?
4. What strategies have recent attempts employed to enhance the robustness of multimodal Transformer models?
5. How does the VILLA framework contribute to the robustness of various multimodal Transformers?

Answer:

(p39.0) Multimodal Transformers pretrained on large-scale corpora achieve the state-of-the-art for various multimodal applications, while their robustness is still unclear and understudied. This at least involves two key challenges, i.e., how to theoretically analyse the robustness, how to improve the robustness.

(p39.1) Although that recent attempts [99], [182], [289], [290] study and evaluate how the Transformer components/sublayers contribute to the robustness, the main bottleneck is that the community lacks theoretical tools to analyse the Transformer family. Recently, the common practices to analyse robustness are mainly based on experiment evaluations [291], e.g., cross-dataset evaluations, perturbationbased evaluations. Thus, some multimodal datasets [130], [292] are proposed for evaluating the robustness.

(p39.2) Recent attempts mainly use two straightforward methods to improve the robustness for multimodal Transformer models: (1) augmentation and adversarial learning based strategies [293], [294], (2) fine-grained loss functions [295]. For instance: VILLA [210] is a generic adversarial training framework that can be applied to various multimodal Transformers. Akula et al. [292] empirically demonstrate that ViL-BERT fails to exploit linguistic structure, and they propose two methods to improve the robustness of ViLBERT, one based on contrastive learning and the other based on multitask learning.","Questions:



Answer:

(p39.0) Multimodal Transformers pretrained on large-scale corpora achieve the state-of-the-art for various multimodal applications, while their robustness is still unclear and understudied. This at least involves two key challenges, i.e., how to theoretically analyse the robustness, how to improve the robustness.

(p39.1) Although that recent attempts [99], [182], [289], [290] study and evaluate how the Transformer components/sublayers contribute to the robustness, the main bottleneck is that the community lacks theoretical tools to analyse the Transformer family. Recently, the common practices to analyse robustness are mainly based on experiment evaluations [291], e.g., cross-dataset evaluations, perturbationbased evaluations. Thus, some multimodal datasets [130], [292] are proposed for evaluating the robustness.

(p39.2) Recent attempts mainly use two straightforward methods to improve the robustness for multimodal Transformer models: (1) augmentation and adversarial learning based strategies [293], [294], (2) fine-grained loss functions [295]. For instance: VILLA [210] is a generic adversarial training framework that can be applied to various multimodal Transformers. Akula et al. [292] empirically demonstrate that ViL-BERT fails to exploit linguistic structure, and they propose two methods to improve the robustness of ViLBERT, one based on contrastive learning and the other based on multitask learning."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s40,Universalness,"['p40.0', 'p40.1', 'p40.2', 'p40.3', 'p40.4', 'p40.5', 'p40.6']","['Due to the highly diversity of tasks and modalities of multimodal learning, universalness is an important problem for multimodal Transformer models. A large amount of recent attempts [117], [296], [297], [298] study how to use as unified as possible pipelines to handle various modalities and multimodal tasks. Ideally, the unified multimodal Transformers can be compatible with various data (e.g., aligned and unaligned, uni-modal and multimodal) and tasks (e.g., supervised and unsupervised, uni-modal and multimodal, discriminative and generative), and meanwhile have either few-shot or even zero-shot generalization ability. Thus, the current solutions for universalness goal for multimodal Transformers are preliminary probes.', 'The currently unifying-oriented attempts mainly include:', '(1) Unifying the pipelines for both uni-modal and multimodal inputs/tasks. As discussed Section 5.3, in practical scenarios, multimodal Transformers need to handle unimodal data due to the issue of missing modalities. Distilling multimodal knowledge into small models that are adaptable to uni-modal data and tasks is a successful practice [275], [276].', '(2) Unifying the pipelines for both multimodal understanding and generation. In general, for multimodal Transformer pipelines, understanding and discriminative tasks require Transformer encoders only, while generation/generative tasks require both Transformer encoders and decoders. Existing attempts use multi-task learning to combine the understanding and generation workflows, where two kinds of workflows are jointly trained by multitask loss functions. From the perspective of model structures, typical solutions include: (a) encoder + decoder, e.g., E2E-VLP [271]. (b) separate encoders + cross encoder + decoder, e.g., UniVL [117], CBT [107]. (c) single unified/combined encoder-decoder, e.g., VLP [110]. (d) twostream decoupled design [191].', '(3) Unifying and converting the tasks themselves, e.g., CLIP [9] converts zero-shot recognition to retrieval, thus reduces the costs of modifying the model. However, the aforementioned practices suffer some obvious challenges and bottlenecks, at least including:', '(1) Due to modality and task gaps, universal models should consider the trade-off between universalness and cost. Unifying the pipelines of different modalities and tasks generally cause larger or more complicated model configuration, whereas for a specific modality or task, some components are redundant.', '(2) Multi-task loss functions increase the complexity of training. How to co-train multiple objectives properly and effectively is challenging, due to that different objectives generally should be optimized in different strategies.']","Due to the highly diversity of tasks and modalities of multimodal learning, universalness is an important problem for multimodal Transformer models. A large amount of recent attempts [117], [296], [297], [298] study how to use as unified as possible pipelines to handle various modalities and multimodal tasks. Ideally, the unified multimodal Transformers can be compatible with various data (e.g., aligned and unaligned, uni-modal and multimodal) and tasks (e.g., supervised and unsupervised, uni-modal and multimodal, discriminative and generative), and meanwhile have either few-shot or even zero-shot generalization ability. Thus, the current solutions for universalness goal for multimodal Transformers are preliminary probes.

The currently unifying-oriented attempts mainly include:

(1) Unifying the pipelines for both uni-modal and multimodal inputs/tasks. As discussed Section 5.3, in practical scenarios, multimodal Transformers need to handle unimodal data due to the issue of missing modalities. Distilling multimodal knowledge into small models that are adaptable to uni-modal data and tasks is a successful practice [275], [276].

(2) Unifying the pipelines for both multimodal understanding and generation. In general, for multimodal Transformer pipelines, understanding and discriminative tasks require Transformer encoders only, while generation/generative tasks require both Transformer encoders and decoders. Existing attempts use multi-task learning to combine the understanding and generation workflows, where two kinds of workflows are jointly trained by multitask loss functions. From the perspective of model structures, typical solutions include: (a) encoder + decoder, e.g., E2E-VLP [271]. (b) separate encoders + cross encoder + decoder, e.g., UniVL [117], CBT [107]. (c) single unified/combined encoder-decoder, e.g., VLP [110]. (d) twostream decoupled design [191].

(3) Unifying and converting the tasks themselves, e.g., CLIP [9] converts zero-shot recognition to retrieval, thus reduces the costs of modifying the model. However, the aforementioned practices suffer some obvious challenges and bottlenecks, at least including:

(1) Due to modality and task gaps, universal models should consider the trade-off between universalness and cost. Unifying the pipelines of different modalities and tasks generally cause larger or more complicated model configuration, whereas for a specific modality or task, some components are redundant.

(2) Multi-task loss functions increase the complexity of training. How to co-train multiple objectives properly and effectively is challenging, due to that different objectives generally should be optimized in different strategies.","(p40.0) Due to the highly diversity of tasks and modalities of multimodal learning, universalness is an important problem for multimodal Transformer models. A large amount of recent attempts [117], [296], [297], [298] study how to use as unified as possible pipelines to handle various modalities and multimodal tasks. Ideally, the unified multimodal Transformers can be compatible with various data (e.g., aligned and unaligned, uni-modal and multimodal) and tasks (e.g., supervised and unsupervised, uni-modal and multimodal, discriminative and generative), and meanwhile have either few-shot or even zero-shot generalization ability. Thus, the current solutions for universalness goal for multimodal Transformers are preliminary probes.

(p40.1) The currently unifying-oriented attempts mainly include:

(p40.2) (1) Unifying the pipelines for both uni-modal and multimodal inputs/tasks. As discussed Section 5.3, in practical scenarios, multimodal Transformers need to handle unimodal data due to the issue of missing modalities. Distilling multimodal knowledge into small models that are adaptable to uni-modal data and tasks is a successful practice [275], [276].

(p40.3) (2) Unifying the pipelines for both multimodal understanding and generation. In general, for multimodal Transformer pipelines, understanding and discriminative tasks require Transformer encoders only, while generation/generative tasks require both Transformer encoders and decoders. Existing attempts use multi-task learning to combine the understanding and generation workflows, where two kinds of workflows are jointly trained by multitask loss functions. From the perspective of model structures, typical solutions include: (a) encoder + decoder, e.g., E2E-VLP [271]. (b) separate encoders + cross encoder + decoder, e.g., UniVL [117], CBT [107]. (c) single unified/combined encoder-decoder, e.g., VLP [110]. (d) twostream decoupled design [191].

(p40.4) (3) Unifying and converting the tasks themselves, e.g., CLIP [9] converts zero-shot recognition to retrieval, thus reduces the costs of modifying the model. However, the aforementioned practices suffer some obvious challenges and bottlenecks, at least including:

(p40.5) (1) Due to modality and task gaps, universal models should consider the trade-off between universalness and cost. Unifying the pipelines of different modalities and tasks generally cause larger or more complicated model configuration, whereas for a specific modality or task, some components are redundant.

(p40.6) (2) Multi-task loss functions increase the complexity of training. How to co-train multiple objectives properly and effectively is challenging, due to that different objectives generally should be optimized in different strategies.","[['b116', 'b297', 'b296', 'b295'], [], ['b274', 'b275'], ['b109', 'b190', 'b270', 'b116', 'b106'], ['b8'], [], []]","[['b116', 'b297', 'b296', 'b295'], [], ['b274', 'b275'], ['b109', 'b190', 'b270', 'b116', 'b106'], ['b8'], [], []]",12,"1. Due to the highly diversity of tasks and modalities of multimodal learning, universalness is an important problem for multimodal Transformer models.
2. A large amount of recent attempts [117], [296], [297], [298] study how to use as unified as possible pipelines to handle various modalities and multimodal tasks.
3. Ideally, the unified multimodal Transformers can be compatible with various data (e.g., aligned and unaligned, uni-modal and multimodal) and tasks (e.g., supervised and unsupervised, uni-modal and multimodal, discriminative and generative), and meanwhile have either few-shot or even zero-shot generalization ability.
4. Thus, the current solutions for universalness goal for multimodal Transformers are preliminary probes.
5. The currently unifying-oriented attempts mainly include:[297]1 Unifying the pipelines for both uni-modal and multimodal inputs/tasks.
6. As discussed Section 5.3, in practical scenarios, multimodal Transformers need to handle unimodal data due to the issue of missing modalities.
7. Distilling multimodal knowledge into small models that are adaptable to uni-modal data and tasks is a successful practice [275], [276].
8. [297]2 Unifying the pipelines for both multimodal understanding and generation.
9. In general, for multimodal Transformer pipelines, understanding and discriminative tasks require Transformer encoders only, while generation/generative tasks require both Transformer encoders and decoders.
10. Existing attempts use multi-task learning to combine the understanding and generation workflows, where two kinds of workflows are jointly trained by multitask loss functions.
11. From the perspective of model structures, typical solutions include: [296]0 encoder + decoder, e.g., E2E-VLP [296]1.
12. [296]2 separate encoders + cross encoder + decoder, e.g., UniVL [117], CBT [296]4.
13. [296]5 single unified/combined encoder-decoder, e.g., VLP [296]6.
14. [296]7 twostream decoupled design [296]8.
15. [296]9 Unifying and converting the tasks themselves, e.g., CLIP [297]0 converts zero-shot recognition to retrieval, thus reduces the costs of modifying the model.
16. However, the aforementioned practices suffer some obvious challenges and bottlenecks, at least including:[297]1 Due to modality and task gaps, universal models should consider the trade-off between universalness and cost.
17. Unifying the pipelines of different modalities and tasks generally cause larger or more complicated model configuration, whereas for a specific modality or task, some components are redundant.
18. [297]2 Multi-task loss functions increase the complexity of training.
19. How to co-train multiple objectives properly and effectively is challenging, due to that different objectives generally should be optimized in different strategies.","What are the key strategies for achieving universalness in multimodal Transformer models?
How do multimodal Transformers handle the issue of missing modalities in practical scenarios?
What are the differences between the model structures used for unifying multimodal understanding and generation tasks in Transformers?
What challenges are associated with unifying the pipelines for different modalities and tasks in multimodal Transformers?
How do multi-task loss functions affect the training complexity of multimodal Transformer models?","1. What are the key strategies for achieving universalness in multimodal Transformer models?
2. How do multimodal Transformers handle the issue of missing modalities in practical scenarios?
3. What are the differences between the model structures used for unifying multimodal understanding and generation tasks in Transformers?
4. What challenges are associated with unifying the pipelines for different modalities and tasks in multimodal Transformers?
5. How do multi-task loss functions affect the training complexity of multimodal Transformer models?","# What are the strategies and challenges in achieving universalness in multimodal Transformer models?
  - Unifying the pipelines for both uni-modal and multimodal inputs/tasks involves distilling multimodal knowledge into smaller models adaptable to uni-modal data and tasks, addressing the issue of missing modalities.
  - Unifying the pipelines for both multimodal understanding and generation combines understanding/discriminative tasks (requiring Transformer encoders) and generation/generative tasks (requiring both Transformer encoders and decoders) through multi-task learning and various model structures like encoder + decoder, separate encoders + cross encoder + decoder, single unified/combined encoder-decoder, and twostream decoupled design.
  - Unifying and converting the tasks themselves, such as converting zero-shot recognition to retrieval to reduce model modification costs, presents a novel approach to universalness.
  - Challenges include the trade-off between universalness and cost due to modality and task gaps leading to larger or more complicated models with potentially redundant components for specific tasks, and the increased complexity of training with multi-task loss functions requiring optimized strategies for co-training multiple objectives.",1. What are the strategies and challenges in achieving universalness in multimodal Transformer models?,"Questions:

1. What are the key strategies for achieving universalness in multimodal Transformer models?
2. How do multimodal Transformers handle the issue of missing modalities in practical scenarios?
3. What are the differences between the model structures used for unifying multimodal understanding and generation tasks in Transformers?
4. What challenges are associated with unifying the pipelines for different modalities and tasks in multimodal Transformers?
5. How do multi-task loss functions affect the training complexity of multimodal Transformer models?

Answer:

(p40.0) Due to the highly diversity of tasks and modalities of multimodal learning, universalness is an important problem for multimodal Transformer models. A large amount of recent attempts [117], [296], [297], [298] study how to use as unified as possible pipelines to handle various modalities and multimodal tasks. Ideally, the unified multimodal Transformers can be compatible with various data (e.g., aligned and unaligned, uni-modal and multimodal) and tasks (e.g., supervised and unsupervised, uni-modal and multimodal, discriminative and generative), and meanwhile have either few-shot or even zero-shot generalization ability. Thus, the current solutions for universalness goal for multimodal Transformers are preliminary probes.

(p40.1) The currently unifying-oriented attempts mainly include:

(p40.2) (1) Unifying the pipelines for both uni-modal and multimodal inputs/tasks. As discussed Section 5.3, in practical scenarios, multimodal Transformers need to handle unimodal data due to the issue of missing modalities. Distilling multimodal knowledge into small models that are adaptable to uni-modal data and tasks is a successful practice [275], [276].

(p40.3) (2) Unifying the pipelines for both multimodal understanding and generation. In general, for multimodal Transformer pipelines, understanding and discriminative tasks require Transformer encoders only, while generation/generative tasks require both Transformer encoders and decoders. Existing attempts use multi-task learning to combine the understanding and generation workflows, where two kinds of workflows are jointly trained by multitask loss functions. From the perspective of model structures, typical solutions include: (a) encoder + decoder, e.g., E2E-VLP [271]. (b) separate encoders + cross encoder + decoder, e.g., UniVL [117], CBT [107]. (c) single unified/combined encoder-decoder, e.g., VLP [110]. (d) twostream decoupled design [191].

(p40.4) (3) Unifying and converting the tasks themselves, e.g., CLIP [9] converts zero-shot recognition to retrieval, thus reduces the costs of modifying the model. However, the aforementioned practices suffer some obvious challenges and bottlenecks, at least including:

(p40.5) (1) Due to modality and task gaps, universal models should consider the trade-off between universalness and cost. Unifying the pipelines of different modalities and tasks generally cause larger or more complicated model configuration, whereas for a specific modality or task, some components are redundant.

(p40.6) (2) Multi-task loss functions increase the complexity of training. How to co-train multiple objectives properly and effectively is challenging, due to that different objectives generally should be optimized in different strategies.","Questions:

1. What are the strategies and challenges in achieving universalness in multimodal Transformer models?

Answer:

(p40.0) Due to the highly diversity of tasks and modalities of multimodal learning, universalness is an important problem for multimodal Transformer models. A large amount of recent attempts [117], [296], [297], [298] study how to use as unified as possible pipelines to handle various modalities and multimodal tasks. Ideally, the unified multimodal Transformers can be compatible with various data (e.g., aligned and unaligned, uni-modal and multimodal) and tasks (e.g., supervised and unsupervised, uni-modal and multimodal, discriminative and generative), and meanwhile have either few-shot or even zero-shot generalization ability. Thus, the current solutions for universalness goal for multimodal Transformers are preliminary probes.

(p40.1) The currently unifying-oriented attempts mainly include:

(p40.2) (1) Unifying the pipelines for both uni-modal and multimodal inputs/tasks. As discussed Section 5.3, in practical scenarios, multimodal Transformers need to handle unimodal data due to the issue of missing modalities. Distilling multimodal knowledge into small models that are adaptable to uni-modal data and tasks is a successful practice [275], [276].

(p40.3) (2) Unifying the pipelines for both multimodal understanding and generation. In general, for multimodal Transformer pipelines, understanding and discriminative tasks require Transformer encoders only, while generation/generative tasks require both Transformer encoders and decoders. Existing attempts use multi-task learning to combine the understanding and generation workflows, where two kinds of workflows are jointly trained by multitask loss functions. From the perspective of model structures, typical solutions include: (a) encoder + decoder, e.g., E2E-VLP [271]. (b) separate encoders + cross encoder + decoder, e.g., UniVL [117], CBT [107]. (c) single unified/combined encoder-decoder, e.g., VLP [110]. (d) twostream decoupled design [191].

(p40.4) (3) Unifying and converting the tasks themselves, e.g., CLIP [9] converts zero-shot recognition to retrieval, thus reduces the costs of modifying the model. However, the aforementioned practices suffer some obvious challenges and bottlenecks, at least including:

(p40.5) (1) Due to modality and task gaps, universal models should consider the trade-off between universalness and cost. Unifying the pipelines of different modalities and tasks generally cause larger or more complicated model configuration, whereas for a specific modality or task, some components are redundant.

(p40.6) (2) Multi-task loss functions increase the complexity of training. How to co-train multiple objectives properly and effectively is challenging, due to that different objectives generally should be optimized in different strategies."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s41,Interpretability,['p41.0'],"['Why and how Transformers perform so well in multimodal learning has been investigated [106], [299], [300], [301], [302], [303], [304], [305], [306]. These attempts mainly use probing task and ablation study. Cao et al. [299] design a set of probing tasks on UNITER [106] and LXMERT [103], to evaluate what patterns are learned in pretraining. Hendricks et al. [301] probe the image-language Transformers by fine-grained image-sentence pairs, and find that verb understanding is harder than subject or object understanding. Chen et al. [106] examine the optimal combination of pretraining tasks via ablation study, to compare how different pretexts contribute to the Transformers. Despite these attempts, the interpretability of multimodal Transformers is still under-studied to date.']","Why and how Transformers perform so well in multimodal learning has been investigated [106], [299], [300], [301], [302], [303], [304], [305], [306]. These attempts mainly use probing task and ablation study. Cao et al. [299] design a set of probing tasks on UNITER [106] and LXMERT [103], to evaluate what patterns are learned in pretraining. Hendricks et al. [301] probe the image-language Transformers by fine-grained image-sentence pairs, and find that verb understanding is harder than subject or object understanding. Chen et al. [106] examine the optimal combination of pretraining tasks via ablation study, to compare how different pretexts contribute to the Transformers. Despite these attempts, the interpretability of multimodal Transformers is still under-studied to date.","(p41.0) Why and how Transformers perform so well in multimodal learning has been investigated [106], [299], [300], [301], [302], [303], [304], [305], [306]. These attempts mainly use probing task and ablation study. Cao et al. [299] design a set of probing tasks on UNITER [106] and LXMERT [103], to evaluate what patterns are learned in pretraining. Hendricks et al. [301] probe the image-language Transformers by fine-grained image-sentence pairs, and find that verb understanding is harder than subject or object understanding. Chen et al. [106] examine the optimal combination of pretraining tasks via ablation study, to compare how different pretexts contribute to the Transformers. Despite these attempts, the interpretability of multimodal Transformers is still under-studied to date.","[['b302', 'b298', 'b102', 'b105', 'b299', 'b304', 'b300', 'b305', 'b303', 'b301']]","[['b302', 'b298', 'b102', 'b105', 'b299', 'b304', 'b300', 'b305', 'b303', 'b301']]",10,"1. Why and how Transformers perform so well in multimodal learning has been investigated [106], [299], [300], [301], [302], [303], [304], [305], [306].
2. These attempts mainly use probing task and ablation study.
3. Cao et al. [299] design a set of probing tasks on UNITER [106] and LXMERT [103], to evaluate what patterns are learned in pretraining.
4. Hendricks et al. [301] probe the image-language Transformers by fine-grained image-sentence pairs, and find that verb understanding is harder than subject or object understanding.
5. Chen et al. [106] examine the optimal combination of pretraining tasks via ablation study, to compare how different pretexts contribute to the Transformers.
6. Despite these attempts, the interpretability of multimodal Transformers is still under-studied to date.","What methodologies have been employed to investigate the performance of Transformers in multimodal learning?
In what ways do probing tasks contribute to evaluating the learned patterns in multimodal Transformers during pretraining?
How do findings from Hendricks et al. differentiate between verb, subject, and object understanding in image-language Transformers?
What approach is taken to assess the impact of different pretraining tasks on the effectiveness of Transformers?
Why is the interpretability of multimodal Transformers still considered an under-explored area despite various research attempts?","1. What methodologies have been employed to investigate the performance of Transformers in multimodal learning?
2. In what ways do probing tasks contribute to evaluating the learned patterns in multimodal Transformers during pretraining?
3. How do findings from Hendricks et al. differentiate between verb, subject, and object understanding in image-language Transformers?
4. What approach is taken to assess the impact of different pretraining tasks on the effectiveness of Transformers?
5. Why is the interpretability of multimodal Transformers still considered an under-explored area despite various research attempts?",None,,"Questions:

1. What methodologies have been employed to investigate the performance of Transformers in multimodal learning?
2. In what ways do probing tasks contribute to evaluating the learned patterns in multimodal Transformers during pretraining?
3. How do findings from Hendricks et al. differentiate between verb, subject, and object understanding in image-language Transformers?
4. What approach is taken to assess the impact of different pretraining tasks on the effectiveness of Transformers?
5. Why is the interpretability of multimodal Transformers still considered an under-explored area despite various research attempts?

Answer:

(p41.0) Why and how Transformers perform so well in multimodal learning has been investigated [106], [299], [300], [301], [302], [303], [304], [305], [306]. These attempts mainly use probing task and ablation study. Cao et al. [299] design a set of probing tasks on UNITER [106] and LXMERT [103], to evaluate what patterns are learned in pretraining. Hendricks et al. [301] probe the image-language Transformers by fine-grained image-sentence pairs, and find that verb understanding is harder than subject or object understanding. Chen et al. [106] examine the optimal combination of pretraining tasks via ablation study, to compare how different pretexts contribute to the Transformers. Despite these attempts, the interpretability of multimodal Transformers is still under-studied to date.","Questions:



Answer:

(p41.0) Why and how Transformers perform so well in multimodal learning has been investigated [106], [299], [300], [301], [302], [303], [304], [305], [306]. These attempts mainly use probing task and ablation study. Cao et al. [299] design a set of probing tasks on UNITER [106] and LXMERT [103], to evaluate what patterns are learned in pretraining. Hendricks et al. [301] probe the image-language Transformers by fine-grained image-sentence pairs, and find that verb understanding is harder than subject or object understanding. Chen et al. [106] examine the optimal combination of pretraining tasks via ablation study, to compare how different pretexts contribute to the Transformers. Despite these attempts, the interpretability of multimodal Transformers is still under-studied to date."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s42,DISCUSSION AND OUTLOOK,"['p42.0', 'p42.1', 'p42.2', 'p42.3', 'p42.4']","[""Designing the universal MML models to excel across all the unimodal and multimodal down-stream tasks with different characteristics simultaneously [115], [299] is a non-trivial challenge. For instance, two-stream architectures [9], [263] are typically preferred over one-stream ones for cross-modal retrieval-like tasks in efficiency, since the representation of each modality can be pre-computed beforehand and reused repeatedly. That being said, how to design task-agnostic MML architectures is still an open challenge, in addition to other design choices such as pretext and objective loss functions. Furthermore, a clear gap remains between the state-of-the-art and this ultimate goal. In general, existing multimodal Transformer models [9], [199], [263] are superior only for specific MML tasks, as they are designed specifically for only a subset of specific tasks [137], [142], [212], [249], [260], [261], [265], [266]. Encouragingly, several recent studies towards universal modality learning in terms of modality-agnostic network design [3] and more task-generic architecture design [307], [308], [309] have been introduced, and it is hoped this will spark further investigation. To that end, instead of exhaustively exploring the vast model design space, seeking in-depth understanding and interpretation of a MML model's behaviour might be insightful for superior algorithm design, even though the interactions and synergy across different modalities are intrinsically complex and even potentially inconsistent over tasks [249]."", 'For more fine-grained MML, it is widely acknowledged that discovering the latent semantic alignments across modalities is critical. An intuitive strategy is to leverage semantic parts (e.g., objects) pre-extracted by an off-the-shelf detector for MML [103], [104], [105], [106], [112], [204], [310]. This, however, is not only complex and error-prone, but computationally costly [207]. Several remedies introduced recently include random sampling [113], learning concept dictionary [203], jointly learning a region detector [271], and representation aligning before mask prediction [263]. Given the scale of MML training data, exploring this direction needs exhaustive computational costs, and it is supposed that industrial research teams with rich resources are more likely to afford. Ideally, a favourable MML method would leave fine-grained semantic alignment across modalities to emerge on its own, which is worthy of careful investigation in the future.', 'As the learning scale expands exponentially, the training data become inevitably noisy and heterogeneous [9], [199], [263]. It has been recently shown that properly tackling the noise issue is useful [263], [309]. Another related facet is training strategy, e.g., how many stages of training is superior over the common one-stage policy [115]. Further, the quadratic complexity with Transformers becomes more acute for multimodal data due to longer input. Despite extensive research on efficient variants [49], dedicated efficiency study for MML is still underestimated even empirically and call for more investigation.', 'Identifying the strengths of Transformers for multimodal machine learning is a big open problem. The following main points can be summarized from the literature: (1) Transformers can encode implicit knowledge [32]. (2) The multi-head brings multiple modelling sub-spaces that can further enhance the expressive ability of the model. Ideally, multiple heads after training are good and different. This is essentially a good practice of ensemble learning.', '(3) Transformers intrinsically have a nature of global aggregation that perceives the non-local patterns. (4) Thanks to the large model capacity, Transformer models handle the challenging domain gaps and shifts (e.g., linguistic and visual) better via effective pretraining on large-scale corpora [294]. (5) Transformers can represent the inputs as graphs, which are intrinsically compatible with more modalities, e.g., table and SQL. (6) For modelling series and sequence patterns (e.g., time-series), Transformers have better training and inference efficiency against RNN-based models, thanks to their parallel computation in training and/or inference. Transformers are inherently permutation invariant for processing a sequence of points, e.g., well-suited for point cloud learning [164]. (7) Tokenization makes Transformers flexible to organize multimodal inputs, as discussed in Section 3.1.1.']","Designing the universal MML models to excel across all the unimodal and multimodal down-stream tasks with different characteristics simultaneously [115], [299] is a non-trivial challenge. For instance, two-stream architectures [9], [263] are typically preferred over one-stream ones for cross-modal retrieval-like tasks in efficiency, since the representation of each modality can be pre-computed beforehand and reused repeatedly. That being said, how to design task-agnostic MML architectures is still an open challenge, in addition to other design choices such as pretext and objective loss functions. Furthermore, a clear gap remains between the state-of-the-art and this ultimate goal. In general, existing multimodal Transformer models [9], [199], [263] are superior only for specific MML tasks, as they are designed specifically for only a subset of specific tasks [137], [142], [212], [249], [260], [261], [265], [266]. Encouragingly, several recent studies towards universal modality learning in terms of modality-agnostic network design [3] and more task-generic architecture design [307], [308], [309] have been introduced, and it is hoped this will spark further investigation. To that end, instead of exhaustively exploring the vast model design space, seeking in-depth understanding and interpretation of a MML model's behaviour might be insightful for superior algorithm design, even though the interactions and synergy across different modalities are intrinsically complex and even potentially inconsistent over tasks [249].

For more fine-grained MML, it is widely acknowledged that discovering the latent semantic alignments across modalities is critical. An intuitive strategy is to leverage semantic parts (e.g., objects) pre-extracted by an off-the-shelf detector for MML [103], [104], [105], [106], [112], [204], [310]. This, however, is not only complex and error-prone, but computationally costly [207]. Several remedies introduced recently include random sampling [113], learning concept dictionary [203], jointly learning a region detector [271], and representation aligning before mask prediction [263]. Given the scale of MML training data, exploring this direction needs exhaustive computational costs, and it is supposed that industrial research teams with rich resources are more likely to afford. Ideally, a favourable MML method would leave fine-grained semantic alignment across modalities to emerge on its own, which is worthy of careful investigation in the future.

As the learning scale expands exponentially, the training data become inevitably noisy and heterogeneous [9], [199], [263]. It has been recently shown that properly tackling the noise issue is useful [263], [309]. Another related facet is training strategy, e.g., how many stages of training is superior over the common one-stage policy [115]. Further, the quadratic complexity with Transformers becomes more acute for multimodal data due to longer input. Despite extensive research on efficient variants [49], dedicated efficiency study for MML is still underestimated even empirically and call for more investigation.

Identifying the strengths of Transformers for multimodal machine learning is a big open problem. The following main points can be summarized from the literature: (1) Transformers can encode implicit knowledge [32]. (2) The multi-head brings multiple modelling sub-spaces that can further enhance the expressive ability of the model. Ideally, multiple heads after training are good and different. This is essentially a good practice of ensemble learning.

(3) Transformers intrinsically have a nature of global aggregation that perceives the non-local patterns. (4) Thanks to the large model capacity, Transformer models handle the challenging domain gaps and shifts (e.g., linguistic and visual) better via effective pretraining on large-scale corpora [294]. (5) Transformers can represent the inputs as graphs, which are intrinsically compatible with more modalities, e.g., table and SQL. (6) For modelling series and sequence patterns (e.g., time-series), Transformers have better training and inference efficiency against RNN-based models, thanks to their parallel computation in training and/or inference. Transformers are inherently permutation invariant for processing a sequence of points, e.g., well-suited for point cloud learning [164]. (7) Tokenization makes Transformers flexible to organize multimodal inputs, as discussed in Section 3.1.1.","(p42.0) Designing the universal MML models to excel across all the unimodal and multimodal down-stream tasks with different characteristics simultaneously [115], [299] is a non-trivial challenge. For instance, two-stream architectures [9], [263] are typically preferred over one-stream ones for cross-modal retrieval-like tasks in efficiency, since the representation of each modality can be pre-computed beforehand and reused repeatedly. That being said, how to design task-agnostic MML architectures is still an open challenge, in addition to other design choices such as pretext and objective loss functions. Furthermore, a clear gap remains between the state-of-the-art and this ultimate goal. In general, existing multimodal Transformer models [9], [199], [263] are superior only for specific MML tasks, as they are designed specifically for only a subset of specific tasks [137], [142], [212], [249], [260], [261], [265], [266]. Encouragingly, several recent studies towards universal modality learning in terms of modality-agnostic network design [3] and more task-generic architecture design [307], [308], [309] have been introduced, and it is hoped this will spark further investigation. To that end, instead of exhaustively exploring the vast model design space, seeking in-depth understanding and interpretation of a MML model's behaviour might be insightful for superior algorithm design, even though the interactions and synergy across different modalities are intrinsically complex and even potentially inconsistent over tasks [249].

(p42.1) For more fine-grained MML, it is widely acknowledged that discovering the latent semantic alignments across modalities is critical. An intuitive strategy is to leverage semantic parts (e.g., objects) pre-extracted by an off-the-shelf detector for MML [103], [104], [105], [106], [112], [204], [310]. This, however, is not only complex and error-prone, but computationally costly [207]. Several remedies introduced recently include random sampling [113], learning concept dictionary [203], jointly learning a region detector [271], and representation aligning before mask prediction [263]. Given the scale of MML training data, exploring this direction needs exhaustive computational costs, and it is supposed that industrial research teams with rich resources are more likely to afford. Ideally, a favourable MML method would leave fine-grained semantic alignment across modalities to emerge on its own, which is worthy of careful investigation in the future.

(p42.2) As the learning scale expands exponentially, the training data become inevitably noisy and heterogeneous [9], [199], [263]. It has been recently shown that properly tackling the noise issue is useful [263], [309]. Another related facet is training strategy, e.g., how many stages of training is superior over the common one-stage policy [115]. Further, the quadratic complexity with Transformers becomes more acute for multimodal data due to longer input. Despite extensive research on efficient variants [49], dedicated efficiency study for MML is still underestimated even empirically and call for more investigation.

(p42.3) Identifying the strengths of Transformers for multimodal machine learning is a big open problem. The following main points can be summarized from the literature: (1) Transformers can encode implicit knowledge [32]. (2) The multi-head brings multiple modelling sub-spaces that can further enhance the expressive ability of the model. Ideally, multiple heads after training are good and different. This is essentially a good practice of ensemble learning.

(p42.4) (3) Transformers intrinsically have a nature of global aggregation that perceives the non-local patterns. (4) Thanks to the large model capacity, Transformer models handle the challenging domain gaps and shifts (e.g., linguistic and visual) better via effective pretraining on large-scale corpora [294]. (5) Transformers can represent the inputs as graphs, which are intrinsically compatible with more modalities, e.g., table and SQL. (6) For modelling series and sequence patterns (e.g., time-series), Transformers have better training and inference efficiency against RNN-based models, thanks to their parallel computation in training and/or inference. Transformers are inherently permutation invariant for processing a sequence of points, e.g., well-suited for point cloud learning [164]. (7) Tokenization makes Transformers flexible to organize multimodal inputs, as discussed in Section 3.1.1.","[['b307', 'b136', 'b308', 'b2', 'b198', 'b248', 'b114', 'b260', 'b264', 'b298', 'b265', 'b8', 'b141', 'b259', 'b262', 'b306', 'b211'], ['b309', 'b111', 'b203', 'b202', 'b102', 'b112', 'b270', 'b105', 'b262', 'b103', 'b206', 'b104'], ['b308', 'b48', 'b198', 'b114', 'b8', 'b262'], ['b31'], ['b293', 'b163']]","[['b307', 'b136', 'b308', 'b2', 'b198', 'b248', 'b114', 'b260', 'b264', 'b298', 'b265', 'b8', 'b141', 'b259', 'b262', 'b306', 'b211'], ['b309', 'b111', 'b203', 'b202', 'b102', 'b112', 'b270', 'b105', 'b262', 'b103', 'b206', 'b104'], ['b308', 'b48', 'b198', 'b114', 'b8', 'b262'], ['b31'], ['b293', 'b163']]",38,"1. Designing the universal MML models to excel across all the unimodal and multimodal down-stream tasks with different characteristics simultaneously [115], [299] is a non-trivial challenge.
2. For instance, two-stream architectures [9], [263] are typically preferred over one-stream ones for cross-modal retrieval-like tasks in efficiency, since the representation of each modality can be pre-computed beforehand and reused repeatedly.
3. That being said, how to design task-agnostic MML architectures is still an open challenge, in addition to other design choices such as pretext and objective loss functions.
4. Furthermore, a clear gap remains between the state-of-the-art and this ultimate goal.
5. In general, existing multimodal Transformer models [9], [199], [263] are superior only for specific MML tasks, as they are designed specifically for only a subset of specific tasks [137], [142], [212], [299]9, [299]1, [299]2, [299]3, [299]4.
6. Encouragingly, several recent studies towards universal modality learning in terms of modality-agnostic network design [299]5 and more task-generic architecture design [299]6, [299]7, [309] have been introduced, and it is hoped this will spark further investigation.
7. To that end, instead of exhaustively exploring the vast model design space, seeking in-depth understanding and interpretation of a MML model's behaviour might be insightful for superior algorithm design, even though the interactions and synergy across different modalities are intrinsically complex and even potentially inconsistent over tasks [299]9.
8. For more fine-grained MML, it is widely acknowledged that discovering the latent semantic alignments across modalities is critical.
9. An intuitive strategy is to leverage semantic parts (e.g., objects) pre-extracted by an off-the-shelf detector for MML [103], [104], [105], [106], [112], [204], [310].
10. This, however, is not only complex and error-prone, but computationally costly [207].
11. Several remedies introduced recently include random sampling [113], learning concept dictionary [203], jointly learning a region detector [271], and representation aligning before mask prediction [263].
12. Given the scale of MML training data, exploring this direction needs exhaustive computational costs, and it is supposed that industrial research teams with rich resources are more likely to afford.
13. Ideally, a favourable MML method would leave fine-grained semantic alignment across modalities to emerge on its own, which is worthy of careful investigation in the future.
14. As the learning scale expands exponentially, the training data become inevitably noisy and heterogeneous [9], [199], [263].
15. It has been recently shown that properly tackling the noise issue is useful [263], [309].
16. Another related facet is training strategy, e.g., how many stages of training is superior over the common one-stage policy [115].
17. Further, the quadratic complexity with Transformers becomes more acute for multimodal data due to longer input.
18. Despite extensive research on efficient variants [49], dedicated efficiency study for MML is still underestimated even empirically and call for more investigation.
19. Identifying the strengths of Transformers for multimodal machine learning is a big open problem.
20. The following main points can be summarized from the literature: (1) Transformers can encode implicit knowledge [32].
21. (2) The multi-head brings multiple modelling sub-spaces that can further enhance the expressive ability of the model.
22. Ideally, multiple heads after training are good and different.
23. This is essentially a good practice of ensemble learning.
24. (3) Transformers intrinsically have a nature of global aggregation that perceives the non-local patterns.
25. (4) Thanks to the large model capacity, Transformer models handle the challenging domain gaps and shifts (e.g., linguistic and visual) better via effective pretraining on large-scale corpora [294].
26. (5) Transformers can represent the inputs as graphs, which are intrinsically compatible with more modalities, e.g., table and SQL.
27. (6) For modelling series and sequence patterns (e.g., time-series), Transformers have better training and inference efficiency against RNN-based models, thanks to their parallel computation in training and/or inference.
28. Transformers are inherently permutation invariant for processing a sequence of points, e.g., well-suited for point cloud learning [164].
29. (7) Tokenization makes Transformers flexible to organize multimodal inputs, as discussed in Section 3.1.1.","What are the challenges and potential solutions in designing universal Multimodal Machine Learning (MML) models that excel across various tasks?
How do two-stream architectures compare to one-stream architectures in terms of efficiency for cross-modal retrieval tasks in MML?
What strategies have been proposed to address the computational challenges of discovering latent semantic alignments across modalities in MML?
In the context of Transformers for MML, how does the multi-head architecture contribute to the model's expressive ability and ensemble learning?
What advantages do Transformers offer in handling multimodal inputs and domain gaps compared to RNN-based models?","1. What are the challenges and potential solutions in designing universal Multimodal Machine Learning (MML) models that excel across various tasks?
2. How do two-stream architectures compare to one-stream architectures in terms of efficiency for cross-modal retrieval tasks in MML?
3. What strategies have been proposed to address the computational challenges of discovering latent semantic alignments across modalities in MML?
4. In the context of Transformers for MML, how does the multi-head architecture contribute to the model's expressive ability and ensemble learning?
5. What advantages do Transformers offer in handling multimodal inputs and domain gaps compared to RNN-based models?",None,,"Questions:

1. What are the challenges and potential solutions in designing universal Multimodal Machine Learning (MML) models that excel across various tasks?
2. How do two-stream architectures compare to one-stream architectures in terms of efficiency for cross-modal retrieval tasks in MML?
3. What strategies have been proposed to address the computational challenges of discovering latent semantic alignments across modalities in MML?
4. In the context of Transformers for MML, how does the multi-head architecture contribute to the model's expressive ability and ensemble learning?
5. What advantages do Transformers offer in handling multimodal inputs and domain gaps compared to RNN-based models?

Answer:

(p42.0) Designing the universal MML models to excel across all the unimodal and multimodal down-stream tasks with different characteristics simultaneously [115], [299] is a non-trivial challenge. For instance, two-stream architectures [9], [263] are typically preferred over one-stream ones for cross-modal retrieval-like tasks in efficiency, since the representation of each modality can be pre-computed beforehand and reused repeatedly. That being said, how to design task-agnostic MML architectures is still an open challenge, in addition to other design choices such as pretext and objective loss functions. Furthermore, a clear gap remains between the state-of-the-art and this ultimate goal. In general, existing multimodal Transformer models [9], [199], [263] are superior only for specific MML tasks, as they are designed specifically for only a subset of specific tasks [137], [142], [212], [249], [260], [261], [265], [266]. Encouragingly, several recent studies towards universal modality learning in terms of modality-agnostic network design [3] and more task-generic architecture design [307], [308], [309] have been introduced, and it is hoped this will spark further investigation. To that end, instead of exhaustively exploring the vast model design space, seeking in-depth understanding and interpretation of a MML model's behaviour might be insightful for superior algorithm design, even though the interactions and synergy across different modalities are intrinsically complex and even potentially inconsistent over tasks [249].

(p42.1) For more fine-grained MML, it is widely acknowledged that discovering the latent semantic alignments across modalities is critical. An intuitive strategy is to leverage semantic parts (e.g., objects) pre-extracted by an off-the-shelf detector for MML [103], [104], [105], [106], [112], [204], [310]. This, however, is not only complex and error-prone, but computationally costly [207]. Several remedies introduced recently include random sampling [113], learning concept dictionary [203], jointly learning a region detector [271], and representation aligning before mask prediction [263]. Given the scale of MML training data, exploring this direction needs exhaustive computational costs, and it is supposed that industrial research teams with rich resources are more likely to afford. Ideally, a favourable MML method would leave fine-grained semantic alignment across modalities to emerge on its own, which is worthy of careful investigation in the future.

(p42.2) As the learning scale expands exponentially, the training data become inevitably noisy and heterogeneous [9], [199], [263]. It has been recently shown that properly tackling the noise issue is useful [263], [309]. Another related facet is training strategy, e.g., how many stages of training is superior over the common one-stage policy [115]. Further, the quadratic complexity with Transformers becomes more acute for multimodal data due to longer input. Despite extensive research on efficient variants [49], dedicated efficiency study for MML is still underestimated even empirically and call for more investigation.

(p42.3) Identifying the strengths of Transformers for multimodal machine learning is a big open problem. The following main points can be summarized from the literature: (1) Transformers can encode implicit knowledge [32]. (2) The multi-head brings multiple modelling sub-spaces that can further enhance the expressive ability of the model. Ideally, multiple heads after training are good and different. This is essentially a good practice of ensemble learning.

(p42.4) (3) Transformers intrinsically have a nature of global aggregation that perceives the non-local patterns. (4) Thanks to the large model capacity, Transformer models handle the challenging domain gaps and shifts (e.g., linguistic and visual) better via effective pretraining on large-scale corpora [294]. (5) Transformers can represent the inputs as graphs, which are intrinsically compatible with more modalities, e.g., table and SQL. (6) For modelling series and sequence patterns (e.g., time-series), Transformers have better training and inference efficiency against RNN-based models, thanks to their parallel computation in training and/or inference. Transformers are inherently permutation invariant for processing a sequence of points, e.g., well-suited for point cloud learning [164]. (7) Tokenization makes Transformers flexible to organize multimodal inputs, as discussed in Section 3.1.1.","Questions:



Answer:

(p42.0) Designing the universal MML models to excel across all the unimodal and multimodal down-stream tasks with different characteristics simultaneously [115], [299] is a non-trivial challenge. For instance, two-stream architectures [9], [263] are typically preferred over one-stream ones for cross-modal retrieval-like tasks in efficiency, since the representation of each modality can be pre-computed beforehand and reused repeatedly. That being said, how to design task-agnostic MML architectures is still an open challenge, in addition to other design choices such as pretext and objective loss functions. Furthermore, a clear gap remains between the state-of-the-art and this ultimate goal. In general, existing multimodal Transformer models [9], [199], [263] are superior only for specific MML tasks, as they are designed specifically for only a subset of specific tasks [137], [142], [212], [249], [260], [261], [265], [266]. Encouragingly, several recent studies towards universal modality learning in terms of modality-agnostic network design [3] and more task-generic architecture design [307], [308], [309] have been introduced, and it is hoped this will spark further investigation. To that end, instead of exhaustively exploring the vast model design space, seeking in-depth understanding and interpretation of a MML model's behaviour might be insightful for superior algorithm design, even though the interactions and synergy across different modalities are intrinsically complex and even potentially inconsistent over tasks [249].

(p42.1) For more fine-grained MML, it is widely acknowledged that discovering the latent semantic alignments across modalities is critical. An intuitive strategy is to leverage semantic parts (e.g., objects) pre-extracted by an off-the-shelf detector for MML [103], [104], [105], [106], [112], [204], [310]. This, however, is not only complex and error-prone, but computationally costly [207]. Several remedies introduced recently include random sampling [113], learning concept dictionary [203], jointly learning a region detector [271], and representation aligning before mask prediction [263]. Given the scale of MML training data, exploring this direction needs exhaustive computational costs, and it is supposed that industrial research teams with rich resources are more likely to afford. Ideally, a favourable MML method would leave fine-grained semantic alignment across modalities to emerge on its own, which is worthy of careful investigation in the future.

(p42.2) As the learning scale expands exponentially, the training data become inevitably noisy and heterogeneous [9], [199], [263]. It has been recently shown that properly tackling the noise issue is useful [263], [309]. Another related facet is training strategy, e.g., how many stages of training is superior over the common one-stage policy [115]. Further, the quadratic complexity with Transformers becomes more acute for multimodal data due to longer input. Despite extensive research on efficient variants [49], dedicated efficiency study for MML is still underestimated even empirically and call for more investigation.

(p42.3) Identifying the strengths of Transformers for multimodal machine learning is a big open problem. The following main points can be summarized from the literature: (1) Transformers can encode implicit knowledge [32]. (2) The multi-head brings multiple modelling sub-spaces that can further enhance the expressive ability of the model. Ideally, multiple heads after training are good and different. This is essentially a good practice of ensemble learning.

(p42.4) (3) Transformers intrinsically have a nature of global aggregation that perceives the non-local patterns. (4) Thanks to the large model capacity, Transformer models handle the challenging domain gaps and shifts (e.g., linguistic and visual) better via effective pretraining on large-scale corpora [294]. (5) Transformers can represent the inputs as graphs, which are intrinsically compatible with more modalities, e.g., table and SQL. (6) For modelling series and sequence patterns (e.g., time-series), Transformers have better training and inference efficiency against RNN-based models, thanks to their parallel computation in training and/or inference. Transformers are inherently permutation invariant for processing a sequence of points, e.g., well-suited for point cloud learning [164]. (7) Tokenization makes Transformers flexible to organize multimodal inputs, as discussed in Section 3.1.1."
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7,s46,Tokens,"['p46.0', 'p46.1', 'p46.2']","['Definitions References', '[CLS] class [44], [47], [110], [114] [SEP]', 'separate [44], [47], [110], [ fusion bottleneck [175] ']","Definitions References

[CLS] class [44], [47], [110], [114] [SEP]

separate [44], [47], [110], [ fusion bottleneck [175] ","(p46.0) Definitions References

(p46.1) [CLS] class [44], [47], [110], [114] [SEP]

(p46.2) separate [44], [47], [110], [ fusion bottleneck [175] ","[[], ['b113', 'b46', 'b43', 'b109'], ['b46', 'b109', None, 'b43', 'b174']]","[[], ['b113', 'b46', 'b43', 'b109'], ['b46', 'b109', None, 'b43', 'b174']]",9,"1. Definitions References[CLS] class [44], [47], [110], [114] [SEP]separate [44], [47], [110], [ fusion bottleneck [175]","What is the role of the [CLS] token in transformer models according to the referenced studies?
How do the referenced studies differentiate between the [CLS] token and other tokens in transformer models?
What challenges are associated with the separation of tokens in multimodal learning as discussed in the cited literature?
How does the concept of a fusion bottleneck impact the performance of transformer models in multimodal learning environments?
Can the referenced articles provide insights into the effectiveness of different tokenization strategies in enhancing transformer model capabilities?","1. What is the role of the [CLS] token in transformer models according to the referenced studies?
2. How do the referenced studies differentiate between the [CLS] token and other tokens in transformer models?
3. What challenges are associated with the separation of tokens in multimodal learning as discussed in the cited literature?
4. How does the concept of a fusion bottleneck impact the performance of transformer models in multimodal learning environments?
5. Can the referenced articles provide insights into the effectiveness of different tokenization strategies in enhancing transformer model capabilities?",None,,"Questions:

1. What is the role of the [CLS] token in transformer models according to the referenced studies?
2. How do the referenced studies differentiate between the [CLS] token and other tokens in transformer models?
3. What challenges are associated with the separation of tokens in multimodal learning as discussed in the cited literature?
4. How does the concept of a fusion bottleneck impact the performance of transformer models in multimodal learning environments?
5. Can the referenced articles provide insights into the effectiveness of different tokenization strategies in enhancing transformer model capabilities?

Answer:

(p46.0) Definitions References

(p46.1) [CLS] class [44], [47], [110], [114] [SEP]

(p46.2) separate [44], [47], [110], [ fusion bottleneck [175] ","Questions:



Answer:

(p46.0) Definitions References

(p46.1) [CLS] class [44], [47], [110], [114] [SEP]

(p46.2) separate [44], [47], [110], [ fusion bottleneck [175] "
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s2,FOUNDATIONS,['p2.0'],"['In general, the core components of transformer-based PLMs like BERT and RoBERTa are embedding and transformer encoder layers (refer Figure 4). The embedding layer takes input tokens and returns a vector for each. The embedding layer has three or more sub-layers each of which provides a vector of specific embedding type for each of the input tokens. The final input vector for each token is obtained by summing all the vectors of each embedding type. The transformer encoder layer enhances each input token vector by encoding global contextual information using the self-attention mechanism. By applying a sequence of such transformer encoder layers, the model can encode complex language information in the input token vectors. Usually, the embedding layer consists of three sublayers with each sub-layer representing a particular embedding type. In some models, there are more than three also. For example, embedding layer of BERT-EHR [27] contains code, position, segment, age and gender embeddings. A detailed description of various embedding types is presented in Section 3.4. The first sublayer converts input tokens to a sequence of vectors while the other two sub-layers provide auxiliary information like position and segmentation. The first sublayer can be char, sub-word, or code embedding based. For example, BioCharBERT [28] uses CharCNN [29] on the top of character embeddings, BERT uses WordPiece [30] embeddings while BEHRT [27], MedBERT [31] and BERT-EHR [32] models use code embeddings. Unlike BioCharBERT and BERT models, the input for BEHRT, MedBERT, BERT-EHR models is patient visits where each patient visit is expressed as a sequence of codes. The final input representation X for the given input tokens {x 1 , x 2 , . . . x n }is obtained by adding the embeddings from the three sub-layers (for simplicity, we have included only three embedding types -refer Figure 5).']","In general, the core components of transformer-based PLMs like BERT and RoBERTa are embedding and transformer encoder layers (refer Figure 4). The embedding layer takes input tokens and returns a vector for each. The embedding layer has three or more sub-layers each of which provides a vector of specific embedding type for each of the input tokens. The final input vector for each token is obtained by summing all the vectors of each embedding type. The transformer encoder layer enhances each input token vector by encoding global contextual information using the self-attention mechanism. By applying a sequence of such transformer encoder layers, the model can encode complex language information in the input token vectors. Usually, the embedding layer consists of three sublayers with each sub-layer representing a particular embedding type. In some models, there are more than three also. For example, embedding layer of BERT-EHR [27] contains code, position, segment, age and gender embeddings. A detailed description of various embedding types is presented in Section 3.4. The first sublayer converts input tokens to a sequence of vectors while the other two sub-layers provide auxiliary information like position and segmentation. The first sublayer can be char, sub-word, or code embedding based. For example, BioCharBERT [28] uses CharCNN [29] on the top of character embeddings, BERT uses WordPiece [30] embeddings while BEHRT [27], MedBERT [31] and BERT-EHR [32] models use code embeddings. Unlike BioCharBERT and BERT models, the input for BEHRT, MedBERT, BERT-EHR models is patient visits where each patient visit is expressed as a sequence of codes. The final input representation X for the given input tokens {x 1 , x 2 , . . . x n }is obtained by adding the embeddings from the three sub-layers (for simplicity, we have included only three embedding types -refer Figure 5).","(p2.0) In general, the core components of transformer-based PLMs like BERT and RoBERTa are embedding and transformer encoder layers (refer Figure 4). The embedding layer takes input tokens and returns a vector for each. The embedding layer has three or more sub-layers each of which provides a vector of specific embedding type for each of the input tokens. The final input vector for each token is obtained by summing all the vectors of each embedding type. The transformer encoder layer enhances each input token vector by encoding global contextual information using the self-attention mechanism. By applying a sequence of such transformer encoder layers, the model can encode complex language information in the input token vectors. Usually, the embedding layer consists of three sublayers with each sub-layer representing a particular embedding type. In some models, there are more than three also. For example, embedding layer of BERT-EHR [27] contains code, position, segment, age and gender embeddings. A detailed description of various embedding types is presented in Section 3.4. The first sublayer converts input tokens to a sequence of vectors while the other two sub-layers provide auxiliary information like position and segmentation. The first sublayer can be char, sub-word, or code embedding based. For example, BioCharBERT [28] uses CharCNN [29] on the top of character embeddings, BERT uses WordPiece [30] embeddings while BEHRT [27], MedBERT [31] and BERT-EHR [32] models use code embeddings. Unlike BioCharBERT and BERT models, the input for BEHRT, MedBERT, BERT-EHR models is patient visits where each patient visit is expressed as a sequence of codes. The final input representation X for the given input tokens {x 1 , x 2 , . . . x n }is obtained by adding the embeddings from the three sub-layers (for simplicity, we have included only three embedding types -refer Figure 5).","[['b28', 'b31', 'b30', 'b27', 'b26', 'b29']]","[['b28', 'b31', 'b30', 'b27', 'b26', 'b29']]",6,"1. In general, the core components of transformer-based PLMs like BERT and RoBERTa are embedding and transformer encoder layers (refer Figure 4).
2. The embedding layer takes input tokens and returns a vector for each.
3. The embedding layer has three or more sub-layers each of which provides a vector of specific embedding type for each of the input tokens.
4. The final input vector for each token is obtained by summing all the vectors of each embedding type.
5. The transformer encoder layer enhances each input token vector by encoding global contextual information using the self-attention mechanism.
6. By applying a sequence of such transformer encoder layers, the model can encode complex language information in the input token vectors.
7. Usually, the embedding layer consists of three sublayers with each sub-layer representing a particular embedding type.
8. In some models, there are more than three also.
9. For example, embedding layer of BERT-EHR [27] contains code, position, segment, age and gender embeddings.
10. A detailed description of various embedding types is presented in Section 3.4.
11. The first sublayer converts input tokens to a sequence of vectors while the other two sub-layers provide auxiliary information like position and segmentation.
12. The first sublayer can be char, sub-word, or code embedding based.
13. For example, BioCharBERT [28] uses CharCNN [29] on the top of character embeddings, BERT uses WordPiece [30] embeddings while BEHRT [27], MedBERT [31] and BERT-EHR [32] models use code embeddings.
14. Unlike BioCharBERT and BERT models, the input for BEHRT, MedBERT, BERT-EHR models is patient visits where each patient visit is expressed as a sequence of codes.
15. The final input representation X for the given input tokens {x 1 , x 2 , . . . x n }is obtained by adding the embeddings from the three sub-layers (for simplicity, we have included only three embedding types -refer Figure 5).","What are the core components of transformer-based Pretrained Language Models (PLMs) like BERT and RoBERTa?
How does the embedding layer in transformer-based PLMs process input tokens?
What role does the transformer encoder layer play in enhancing the input token vectors in PLMs?
Can you describe the variety of embedding types used in transformer-based biomedical PLMs and provide examples?
How do models like BEHRT, MedBERT, and BERT-EHR differ in their approach to input representation compared to traditional BERT models?","1. What are the core components of transformer-based Pretrained Language Models (PLMs) like BERT and RoBERTa?
2. How does the embedding layer in transformer-based PLMs process input tokens?
3. What role does the transformer encoder layer play in enhancing the input token vectors in PLMs?
4. Can you describe the variety of embedding types used in transformer-based biomedical PLMs and provide examples?
5. How do models like BEHRT, MedBERT, and BERT-EHR differ in their approach to input representation compared to traditional BERT models?",None,,"Questions:

1. What are the core components of transformer-based Pretrained Language Models (PLMs) like BERT and RoBERTa?
2. How does the embedding layer in transformer-based PLMs process input tokens?
3. What role does the transformer encoder layer play in enhancing the input token vectors in PLMs?
4. Can you describe the variety of embedding types used in transformer-based biomedical PLMs and provide examples?
5. How do models like BEHRT, MedBERT, and BERT-EHR differ in their approach to input representation compared to traditional BERT models?

Answer:

(p2.0) In general, the core components of transformer-based PLMs like BERT and RoBERTa are embedding and transformer encoder layers (refer Figure 4). The embedding layer takes input tokens and returns a vector for each. The embedding layer has three or more sub-layers each of which provides a vector of specific embedding type for each of the input tokens. The final input vector for each token is obtained by summing all the vectors of each embedding type. The transformer encoder layer enhances each input token vector by encoding global contextual information using the self-attention mechanism. By applying a sequence of such transformer encoder layers, the model can encode complex language information in the input token vectors. Usually, the embedding layer consists of three sublayers with each sub-layer representing a particular embedding type. In some models, there are more than three also. For example, embedding layer of BERT-EHR [27] contains code, position, segment, age and gender embeddings. A detailed description of various embedding types is presented in Section 3.4. The first sublayer converts input tokens to a sequence of vectors while the other two sub-layers provide auxiliary information like position and segmentation. The first sublayer can be char, sub-word, or code embedding based. For example, BioCharBERT [28] uses CharCNN [29] on the top of character embeddings, BERT uses WordPiece [30] embeddings while BEHRT [27], MedBERT [31] and BERT-EHR [32] models use code embeddings. Unlike BioCharBERT and BERT models, the input for BEHRT, MedBERT, BERT-EHR models is patient visits where each patient visit is expressed as a sequence of codes. The final input representation X for the given input tokens {x 1 , x 2 , . . . x n }is obtained by adding the embeddings from the three sub-layers (for simplicity, we have included only three embedding types -refer Figure 5).","Questions:



Answer:

(p2.0) In general, the core components of transformer-based PLMs like BERT and RoBERTa are embedding and transformer encoder layers (refer Figure 4). The embedding layer takes input tokens and returns a vector for each. The embedding layer has three or more sub-layers each of which provides a vector of specific embedding type for each of the input tokens. The final input vector for each token is obtained by summing all the vectors of each embedding type. The transformer encoder layer enhances each input token vector by encoding global contextual information using the self-attention mechanism. By applying a sequence of such transformer encoder layers, the model can encode complex language information in the input token vectors. Usually, the embedding layer consists of three sublayers with each sub-layer representing a particular embedding type. In some models, there are more than three also. For example, embedding layer of BERT-EHR [27] contains code, position, segment, age and gender embeddings. A detailed description of various embedding types is presented in Section 3.4. The first sublayer converts input tokens to a sequence of vectors while the other two sub-layers provide auxiliary information like position and segmentation. The first sublayer can be char, sub-word, or code embedding based. For example, BioCharBERT [28] uses CharCNN [29] on the top of character embeddings, BERT uses WordPiece [30] embeddings while BEHRT [27], MedBERT [31] and BERT-EHR [32] models use code embeddings. Unlike BioCharBERT and BERT models, the input for BEHRT, MedBERT, BERT-EHR models is patient visits where each patient visit is expressed as a sequence of codes. The final input representation X for the given input tokens {x 1 , x 2 , . . . x n }is obtained by adding the embeddings from the three sub-layers (for simplicity, we have included only three embedding types -refer Figure 5)."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s10,Self-Supervised Learning,"['p10.0', 'p10.1']","['Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade. This is because deep learning models can learn features automatically which eliminates the requirement of expensive feature engineering and process the inputs in an end-to-end manner i.e., take raw inputs and give the decisions. The success of the deep learning algorithms comes from the knowledge gained during training from human-labeled instances. However, supervised learning has a lot of limitations a) with less data, the model may get overfitted and prone to bias b) some of the domains like biomedical are supervision starved i.e., difficult to get labeled data. In general, we expect models to be close to human intelligence i.e., more general and make decisions with just a few samples. This desire of developing models with more generalization ability and learning from fewer samples has made the researchers focus on other learning paradigms like Self-Supervised Learning [34].', 'Robotics is the first AI field to use self-supervised learning methods [34]. Over the last five years, selfsupervised learning has become popular in other AI fields like natural language processing [13], [14], [26], computer vision [35], [36], and speech processing [37], [38]. SSL is a new learning paradigm that draws inspiration from both supervised and unsupervised learning methods. SSL is similar to unsupervised learning as it does not depend on human-labeled instances. It is also similar to supervised learning as it learns using supervision. However, in SSL the supervision is provided by the pseudo labels which are generated automatically from the pretraining data. SSL involves pretraining the model over a large unlabelled corpus using one or more pretraining tasks. The pseudo labels are generated depending on the definitions of pre-training tasks. SSL methods fall into three categories namely Generative, Contrastive, and Generate-Contrastive [34]. In Generative SSL, encoder maps input vector x to vector y, and decoder recovers x from y (e.g., masked language modeling). In Contrastive SSL, the encoder maps input vector x to vector y to measure similarity (e.g., mutual information maximization). In Generate-Contrastive SSL, fake samples are generated using encoder-decoder while the discriminator identifies the fake samples (e.g., replaced token detection). For more details about different SSL methods, please refer to the survey paper written by Liu et al. [34].']","Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade. This is because deep learning models can learn features automatically which eliminates the requirement of expensive feature engineering and process the inputs in an end-to-end manner i.e., take raw inputs and give the decisions. The success of the deep learning algorithms comes from the knowledge gained during training from human-labeled instances. However, supervised learning has a lot of limitations a) with less data, the model may get overfitted and prone to bias b) some of the domains like biomedical are supervision starved i.e., difficult to get labeled data. In general, we expect models to be close to human intelligence i.e., more general and make decisions with just a few samples. This desire of developing models with more generalization ability and learning from fewer samples has made the researchers focus on other learning paradigms like Self-Supervised Learning [34].

Robotics is the first AI field to use self-supervised learning methods [34]. Over the last five years, selfsupervised learning has become popular in other AI fields like natural language processing [13], [14], [26], computer vision [35], [36], and speech processing [37], [38]. SSL is a new learning paradigm that draws inspiration from both supervised and unsupervised learning methods. SSL is similar to unsupervised learning as it does not depend on human-labeled instances. It is also similar to supervised learning as it learns using supervision. However, in SSL the supervision is provided by the pseudo labels which are generated automatically from the pretraining data. SSL involves pretraining the model over a large unlabelled corpus using one or more pretraining tasks. The pseudo labels are generated depending on the definitions of pre-training tasks. SSL methods fall into three categories namely Generative, Contrastive, and Generate-Contrastive [34]. In Generative SSL, encoder maps input vector x to vector y, and decoder recovers x from y (e.g., masked language modeling). In Contrastive SSL, the encoder maps input vector x to vector y to measure similarity (e.g., mutual information maximization). In Generate-Contrastive SSL, fake samples are generated using encoder-decoder while the discriminator identifies the fake samples (e.g., replaced token detection). For more details about different SSL methods, please refer to the survey paper written by Liu et al. [34].","(p10.0) Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade. This is because deep learning models can learn features automatically which eliminates the requirement of expensive feature engineering and process the inputs in an end-to-end manner i.e., take raw inputs and give the decisions. The success of the deep learning algorithms comes from the knowledge gained during training from human-labeled instances. However, supervised learning has a lot of limitations a) with less data, the model may get overfitted and prone to bias b) some of the domains like biomedical are supervision starved i.e., difficult to get labeled data. In general, we expect models to be close to human intelligence i.e., more general and make decisions with just a few samples. This desire of developing models with more generalization ability and learning from fewer samples has made the researchers focus on other learning paradigms like Self-Supervised Learning [34].

(p10.1) Robotics is the first AI field to use self-supervised learning methods [34]. Over the last five years, selfsupervised learning has become popular in other AI fields like natural language processing [13], [14], [26], computer vision [35], [36], and speech processing [37], [38]. SSL is a new learning paradigm that draws inspiration from both supervised and unsupervised learning methods. SSL is similar to unsupervised learning as it does not depend on human-labeled instances. It is also similar to supervised learning as it learns using supervision. However, in SSL the supervision is provided by the pseudo labels which are generated automatically from the pretraining data. SSL involves pretraining the model over a large unlabelled corpus using one or more pretraining tasks. The pseudo labels are generated depending on the definitions of pre-training tasks. SSL methods fall into three categories namely Generative, Contrastive, and Generate-Contrastive [34]. In Generative SSL, encoder maps input vector x to vector y, and decoder recovers x from y (e.g., masked language modeling). In Contrastive SSL, the encoder maps input vector x to vector y to measure similarity (e.g., mutual information maximization). In Generate-Contrastive SSL, fake samples are generated using encoder-decoder while the discriminator identifies the fake samples (e.g., replaced token detection). For more details about different SSL methods, please refer to the survey paper written by Liu et al. [34].","[['b33'], ['b25', 'b34', 'b37', 'b13', 'b36', 'b35', 'b33', 'b12']]","[['b33'], ['b25', 'b34', 'b37', 'b13', 'b36', 'b35', 'b33', 'b12']]",9,"1. Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade.
2. This is because deep learning models can learn features automatically which eliminates the requirement of expensive feature engineering and process the inputs in an end-to-end manner i.e., take raw inputs and give the decisions.
3. The success of the deep learning algorithms comes from the knowledge gained during training from human-labeled instances.
4. However, supervised learning has a lot of limitations a) with less data, the model may get overfitted and prone to bias b) some of the domains like biomedical are supervision starved i.e., difficult to get labeled data.
5. In general, we expect models to be close to human intelligence i.e., more general and make decisions with just a few samples.
6. This desire of developing models with more generalization ability and learning from fewer samples has made the researchers focus on other learning paradigms like Self-Supervised Learning [34].Robotics is the first AI field to use self-supervised learning methods [34].
7. Over the last five years, selfsupervised learning has become popular in other AI fields like natural language processing [13], [14], [26], computer vision [35], [36], and speech processing [37], [38].
8. SSL is a new learning paradigm that draws inspiration from both supervised and unsupervised learning methods.
9. SSL is similar to unsupervised learning as it does not depend on human-labeled instances.
10. It is also similar to supervised learning as it learns using supervision.
11. However, in SSL the supervision is provided by the pseudo labels which are generated automatically from the pretraining data.
12. SSL involves pretraining the model over a large unlabelled corpus using one or more pretraining tasks.
13. The pseudo labels are generated depending on the definitions of pre-training tasks.
14. SSL methods fall into three categories namely Generative, Contrastive, and Generate-Contrastive [34].
15. In Generative SSL, encoder maps input vector x to vector y, and decoder recovers x from y (e.g., masked language modeling).
16. In Contrastive SSL, the encoder maps input vector x to vector y to measure similarity (e.g., mutual information maximization).
17. In Generate-Contrastive SSL, fake samples are generated using encoder-decoder while the discriminator identifies the fake samples (e.g., replaced token detection).
18. For more details about different SSL methods, please refer to the survey paper written by Liu et al. [34].","What are the main limitations of supervised learning in the context of biomedical data analysis?
How does self-supervised learning (SSL) differ from supervised and unsupervised learning methods?
What was the first AI field to adopt self-supervised learning methods, and how has its application expanded across other AI domains?
Can you describe the three categories of self-supervised learning methods and provide examples for each?
How do generative, contrastive, and generate-contrastive SSL methods generate pseudo labels for pretraining tasks?","1. What are the main limitations of supervised learning in the context of biomedical data analysis?
2. How does self-supervised learning (SSL) differ from supervised and unsupervised learning methods?
3. What was the first AI field to adopt self-supervised learning methods, and how has its application expanded across other AI domains?
4. Can you describe the three categories of self-supervised learning methods and provide examples for each?
5. How do generative, contrastive, and generate-contrastive SSL methods generate pseudo labels for pretraining tasks?",None,,"Questions:

1. What are the main limitations of supervised learning in the context of biomedical data analysis?
2. How does self-supervised learning (SSL) differ from supervised and unsupervised learning methods?
3. What was the first AI field to adopt self-supervised learning methods, and how has its application expanded across other AI domains?
4. Can you describe the three categories of self-supervised learning methods and provide examples for each?
5. How do generative, contrastive, and generate-contrastive SSL methods generate pseudo labels for pretraining tasks?

Answer:

(p10.0) Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade. This is because deep learning models can learn features automatically which eliminates the requirement of expensive feature engineering and process the inputs in an end-to-end manner i.e., take raw inputs and give the decisions. The success of the deep learning algorithms comes from the knowledge gained during training from human-labeled instances. However, supervised learning has a lot of limitations a) with less data, the model may get overfitted and prone to bias b) some of the domains like biomedical are supervision starved i.e., difficult to get labeled data. In general, we expect models to be close to human intelligence i.e., more general and make decisions with just a few samples. This desire of developing models with more generalization ability and learning from fewer samples has made the researchers focus on other learning paradigms like Self-Supervised Learning [34].

(p10.1) Robotics is the first AI field to use self-supervised learning methods [34]. Over the last five years, selfsupervised learning has become popular in other AI fields like natural language processing [13], [14], [26], computer vision [35], [36], and speech processing [37], [38]. SSL is a new learning paradigm that draws inspiration from both supervised and unsupervised learning methods. SSL is similar to unsupervised learning as it does not depend on human-labeled instances. It is also similar to supervised learning as it learns using supervision. However, in SSL the supervision is provided by the pseudo labels which are generated automatically from the pretraining data. SSL involves pretraining the model over a large unlabelled corpus using one or more pretraining tasks. The pseudo labels are generated depending on the definitions of pre-training tasks. SSL methods fall into three categories namely Generative, Contrastive, and Generate-Contrastive [34]. In Generative SSL, encoder maps input vector x to vector y, and decoder recovers x from y (e.g., masked language modeling). In Contrastive SSL, the encoder maps input vector x to vector y to measure similarity (e.g., mutual information maximization). In Generate-Contrastive SSL, fake samples are generated using encoder-decoder while the discriminator identifies the fake samples (e.g., replaced token detection). For more details about different SSL methods, please refer to the survey paper written by Liu et al. [34].","Questions:



Answer:

(p10.0) Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade. This is because deep learning models can learn features automatically which eliminates the requirement of expensive feature engineering and process the inputs in an end-to-end manner i.e., take raw inputs and give the decisions. The success of the deep learning algorithms comes from the knowledge gained during training from human-labeled instances. However, supervised learning has a lot of limitations a) with less data, the model may get overfitted and prone to bias b) some of the domains like biomedical are supervision starved i.e., difficult to get labeled data. In general, we expect models to be close to human intelligence i.e., more general and make decisions with just a few samples. This desire of developing models with more generalization ability and learning from fewer samples has made the researchers focus on other learning paradigms like Self-Supervised Learning [34].

(p10.1) Robotics is the first AI field to use self-supervised learning methods [34]. Over the last five years, selfsupervised learning has become popular in other AI fields like natural language processing [13], [14], [26], computer vision [35], [36], and speech processing [37], [38]. SSL is a new learning paradigm that draws inspiration from both supervised and unsupervised learning methods. SSL is similar to unsupervised learning as it does not depend on human-labeled instances. It is also similar to supervised learning as it learns using supervision. However, in SSL the supervision is provided by the pseudo labels which are generated automatically from the pretraining data. SSL involves pretraining the model over a large unlabelled corpus using one or more pretraining tasks. The pseudo labels are generated depending on the definitions of pre-training tasks. SSL methods fall into three categories namely Generative, Contrastive, and Generate-Contrastive [34]. In Generative SSL, encoder maps input vector x to vector y, and decoder recovers x from y (e.g., masked language modeling). In Contrastive SSL, the encoder maps input vector x to vector y to measure similarity (e.g., mutual information maximization). In Generate-Contrastive SSL, fake samples are generated using encoder-decoder while the discriminator identifies the fake samples (e.g., replaced token detection). For more details about different SSL methods, please refer to the survey paper written by Liu et al. [34]."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s13,Mixed-Domain Pretraining (MDPT),"['p13.0', 'p13.1', 'p13.2']","['Mixed domain pretraining involves training the model using both general and in-domain text. Depending on whether the pretraining is done simultaneously or not, mixed domain pretraining can be classified into a) Continual pretraining -initially the model is pre-trained over general domain text and then adapted to the biomedical domain [16] and b) Simultaneous pretraining -the model is pre-trained over the combined corpora having both general and in-domain text where the indomain text is up sampled to ensure balanced pretraining [21].', 'Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs. It is also referred to as further pretraining. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7). For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles [16]. In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].', 'Simultaneous Pretraining (SPT) : Continual pretraining achieved good results by adapting general models to the biomedical domain [16], [18], [19], [39], [40]. However, it requires large volumes of in-domain text. Otherwise, CPT may result in suboptimal performance. Simultaneous pretraining comes to the rescue when only a small amount of in-domain text is available. Here, the pretraining corpora consist of both in-domain and general domain text where the in-domain text is up sampled to ensure a balanced pretraining (refer Figure  8). For example, BERT (jpCR+jpW) [41] is developed by simultaneous pretraining over a small amount of Japanese clinical text and a large amount of Japanese Wikipedia text. This model outperformed UTH-BERT in clinical text classification. UTH-BERT [42] is trained from scratch over Japanese clinical text. ']","Mixed domain pretraining involves training the model using both general and in-domain text. Depending on whether the pretraining is done simultaneously or not, mixed domain pretraining can be classified into a) Continual pretraining -initially the model is pre-trained over general domain text and then adapted to the biomedical domain [16] and b) Simultaneous pretraining -the model is pre-trained over the combined corpora having both general and in-domain text where the indomain text is up sampled to ensure balanced pretraining [21].

Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs. It is also referred to as further pretraining. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7). For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles [16]. In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].

Simultaneous Pretraining (SPT) : Continual pretraining achieved good results by adapting general models to the biomedical domain [16], [18], [19], [39], [40]. However, it requires large volumes of in-domain text. Otherwise, CPT may result in suboptimal performance. Simultaneous pretraining comes to the rescue when only a small amount of in-domain text is available. Here, the pretraining corpora consist of both in-domain and general domain text where the in-domain text is up sampled to ensure a balanced pretraining (refer Figure  8). For example, BERT (jpCR+jpW) [41] is developed by simultaneous pretraining over a small amount of Japanese clinical text and a large amount of Japanese Wikipedia text. This model outperformed UTH-BERT in clinical text classification. UTH-BERT [42] is trained from scratch over Japanese clinical text. ","(p13.0) Mixed domain pretraining involves training the model using both general and in-domain text. Depending on whether the pretraining is done simultaneously or not, mixed domain pretraining can be classified into a) Continual pretraining -initially the model is pre-trained over general domain text and then adapted to the biomedical domain [16] and b) Simultaneous pretraining -the model is pre-trained over the combined corpora having both general and in-domain text where the indomain text is up sampled to ensure balanced pretraining [21].

(p13.1) Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs. It is also referred to as further pretraining. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7). For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles [16]. In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].

(p13.2) Simultaneous Pretraining (SPT) : Continual pretraining achieved good results by adapting general models to the biomedical domain [16], [18], [19], [39], [40]. However, it requires large volumes of in-domain text. Otherwise, CPT may result in suboptimal performance. Simultaneous pretraining comes to the rescue when only a small amount of in-domain text is available. Here, the pretraining corpora consist of both in-domain and general domain text where the in-domain text is up sampled to ensure a balanced pretraining (refer Figure  8). For example, BERT (jpCR+jpW) [41] is developed by simultaneous pretraining over a small amount of Japanese clinical text and a large amount of Japanese Wikipedia text. This model outperformed UTH-BERT in clinical text classification. UTH-BERT [42] is trained from scratch over Japanese clinical text. ","[['b15', 'b20'], ['b17', 'b15'], ['b17', 'b15', 'b18', 'b38', 'b40', 'b39', 'b41']]","[['b15', 'b20'], ['b17', 'b15'], ['b17', 'b15', 'b18', 'b38', 'b40', 'b39', 'b41']]",11,"1. Mixed domain pretraining involves training the model using both general and in-domain text.
2. Depending on whether the pretraining is done simultaneously or not, mixed domain pretraining can be classified into a) Continual pretraining -initially the model is pre-trained over general domain text and then adapted to the biomedical domain [16] and b)
3. Simultaneous pretraining -the model is pre-trained over the combined corpora having both general and in-domain text where the indomain text is up sampled to ensure balanced pretraining [21].
4. Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs.
5. It is also referred to as further pretraining.
6. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7).
7. For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles [16].
8. In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].
9. Simultaneous Pretraining (SPT) : Continual pretraining achieved good results by adapting general models to the biomedical domain [16], [18], [19], [21]0, [21]1.
10. However, it requires large volumes of in-domain text.
11. Otherwise, CPT may result in suboptimal performance.
12. Simultaneous pretraining comes to the rescue when only a small amount of in-domain text is available.
13. Here, the pretraining corpora consist of both in-domain and general domain text where the in-domain text is up sampled to ensure a balanced pretraining [21]2.
14. For example, BERT [21]3 [21]4 is developed by simultaneous pretraining over a small amount of Japanese clinical text and a large amount of Japanese Wikipedia text.
15. This model outperformed UTH-BERT in clinical text classification.
16. UTH-BERT [21]5 is trained from scratch over Japanese clinical text.","What are the two main classifications of mixed domain pretraining (MDPT) in transformer-based biomedical pretrained language models?
How does continual pretraining (CPT) differ from simultaneous pretraining (SPT) in the context of transformer-based biomedical pretrained language models?
What is the standard approach in biomedical NLP research for developing transformer-based biomedical pretrained language models (BPLMs)?
Why might simultaneous pretraining (SPT) be preferred over continual pretraining (CPT) when there is a limited amount of in-domain text available?
Can you provide an example of a transformer-based biomedical pretrained language model that utilized simultaneous pretraining and its performance compared to a model trained from scratch?","1. What are the two main classifications of mixed domain pretraining (MDPT) in transformer-based biomedical pretrained language models?
2. How does continual pretraining (CPT) differ from simultaneous pretraining (SPT) in the context of transformer-based biomedical pretrained language models?
3. What is the standard approach in biomedical NLP research for developing transformer-based biomedical pretrained language models (BPLMs)?
4. Why might simultaneous pretraining (SPT) be preferred over continual pretraining (CPT) when there is a limited amount of in-domain text available?
5. Can you provide an example of a transformer-based biomedical pretrained language model that utilized simultaneous pretraining and its performance compared to a model trained from scratch?","# What are the strategies and implications of Mixed-Domain Pretraining (MDPT) for transformer-based Biomedical Pretrained Language Models (BPLMs)?
  - Continual Pretraining (CPT) involves initializing a model with general domain weights and then adapting it to the biomedical domain through further pretraining on in-domain text. This approach is standard in biomedical NLP for developing BPLMs, exemplified by models like BioBERT and BlueBERT.
  - Simultaneous Pretraining (SPT) addresses the challenge of limited in-domain text availability by pretraining models on a mix of in-domain and general domain texts, with in-domain texts being upsampled for balance. This method has shown to outperform models trained solely on in-domain text, such as UTH-BERT, in tasks like clinical text classification.",1. What are the strategies and implications of Mixed-Domain Pretraining (MDPT) for transformer-based Biomedical Pretrained Language Models (BPLMs)?,"Questions:

1. What are the two main classifications of mixed domain pretraining (MDPT) in transformer-based biomedical pretrained language models?
2. How does continual pretraining (CPT) differ from simultaneous pretraining (SPT) in the context of transformer-based biomedical pretrained language models?
3. What is the standard approach in biomedical NLP research for developing transformer-based biomedical pretrained language models (BPLMs)?
4. Why might simultaneous pretraining (SPT) be preferred over continual pretraining (CPT) when there is a limited amount of in-domain text available?
5. Can you provide an example of a transformer-based biomedical pretrained language model that utilized simultaneous pretraining and its performance compared to a model trained from scratch?

Answer:

(p13.0) Mixed domain pretraining involves training the model using both general and in-domain text. Depending on whether the pretraining is done simultaneously or not, mixed domain pretraining can be classified into a) Continual pretraining -initially the model is pre-trained over general domain text and then adapted to the biomedical domain [16] and b) Simultaneous pretraining -the model is pre-trained over the combined corpora having both general and in-domain text where the indomain text is up sampled to ensure balanced pretraining [21].

(p13.1) Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs. It is also referred to as further pretraining. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7). For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles [16]. In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].

(p13.2) Simultaneous Pretraining (SPT) : Continual pretraining achieved good results by adapting general models to the biomedical domain [16], [18], [19], [39], [40]. However, it requires large volumes of in-domain text. Otherwise, CPT may result in suboptimal performance. Simultaneous pretraining comes to the rescue when only a small amount of in-domain text is available. Here, the pretraining corpora consist of both in-domain and general domain text where the in-domain text is up sampled to ensure a balanced pretraining (refer Figure  8). For example, BERT (jpCR+jpW) [41] is developed by simultaneous pretraining over a small amount of Japanese clinical text and a large amount of Japanese Wikipedia text. This model outperformed UTH-BERT in clinical text classification. UTH-BERT [42] is trained from scratch over Japanese clinical text. ","Questions:

1. What are the strategies and implications of Mixed-Domain Pretraining (MDPT) for transformer-based Biomedical Pretrained Language Models (BPLMs)?

Answer:

(p13.0) Mixed domain pretraining involves training the model using both general and in-domain text. Depending on whether the pretraining is done simultaneously or not, mixed domain pretraining can be classified into a) Continual pretraining -initially the model is pre-trained over general domain text and then adapted to the biomedical domain [16] and b) Simultaneous pretraining -the model is pre-trained over the combined corpora having both general and in-domain text where the indomain text is up sampled to ensure balanced pretraining [21].

(p13.1) Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs. It is also referred to as further pretraining. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7). For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles [16]. In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].

(p13.2) Simultaneous Pretraining (SPT) : Continual pretraining achieved good results by adapting general models to the biomedical domain [16], [18], [19], [39], [40]. However, it requires large volumes of in-domain text. Otherwise, CPT may result in suboptimal performance. Simultaneous pretraining comes to the rescue when only a small amount of in-domain text is available. Here, the pretraining corpora consist of both in-domain and general domain text where the in-domain text is up sampled to ensure a balanced pretraining (refer Figure  8). For example, BERT (jpCR+jpW) [41] is developed by simultaneous pretraining over a small amount of Japanese clinical text and a large amount of Japanese Wikipedia text. This model outperformed UTH-BERT in clinical text classification. UTH-BERT [42] is trained from scratch over Japanese clinical text. "
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s14,Domain-Specific Pretraining (DSPT),['p14.0'],"['The main drawback in continual pretraining is the general domain vocabulary. For example, the WordPiece vocabulary in BERT is learned over English Wikipedia and Books Corpus [2]. As a result, the vocabulary does not represent the biomedical domain and hence many of the biomedical words are split into several subwords which hinders the model learning during pretraining and fine-tuning. Moreover, the length of the input sequence also increases as many of the in-domain words are split into several subwords. DSPT over in-domain text allows the model to have the in-domain vocabulary (refer Figure 10). For example, PubMedBERT is trained from scratch using PubMed abstracts and PMC full-text articles [20]. PubMed achieved state-of-the-art results in the BLURB benchmark. Similarly, RoBERTa-base-PM-M3-Voc is trained from scratch over PubMed and PMC and MIMIC-III clinical notes [43]. is based on the hypothesis that pretraining over taskrelated unlabelled text allows the model to learn both domain and task-specific knowledge [44] (refer Figure  11). In TAPT, task-related unlabelled sentences are gathered, and then the model is further pretrained. TAPT is less expensive compared to other pretraining methods as it involves pretraining the model over a relatively small corpus of task-related unlabelled sentences.']","The main drawback in continual pretraining is the general domain vocabulary. For example, the WordPiece vocabulary in BERT is learned over English Wikipedia and Books Corpus [2]. As a result, the vocabulary does not represent the biomedical domain and hence many of the biomedical words are split into several subwords which hinders the model learning during pretraining and fine-tuning. Moreover, the length of the input sequence also increases as many of the in-domain words are split into several subwords. DSPT over in-domain text allows the model to have the in-domain vocabulary (refer Figure 10). For example, PubMedBERT is trained from scratch using PubMed abstracts and PMC full-text articles [20]. PubMed achieved state-of-the-art results in the BLURB benchmark. Similarly, RoBERTa-base-PM-M3-Voc is trained from scratch over PubMed and PMC and MIMIC-III clinical notes [43]. is based on the hypothesis that pretraining over taskrelated unlabelled text allows the model to learn both domain and task-specific knowledge [44] (refer Figure  11). In TAPT, task-related unlabelled sentences are gathered, and then the model is further pretrained. TAPT is less expensive compared to other pretraining methods as it involves pretraining the model over a relatively small corpus of task-related unlabelled sentences.","(p14.0) The main drawback in continual pretraining is the general domain vocabulary. For example, the WordPiece vocabulary in BERT is learned over English Wikipedia and Books Corpus [2]. As a result, the vocabulary does not represent the biomedical domain and hence many of the biomedical words are split into several subwords which hinders the model learning during pretraining and fine-tuning. Moreover, the length of the input sequence also increases as many of the in-domain words are split into several subwords. DSPT over in-domain text allows the model to have the in-domain vocabulary (refer Figure 10). For example, PubMedBERT is trained from scratch using PubMed abstracts and PMC full-text articles [20]. PubMed achieved state-of-the-art results in the BLURB benchmark. Similarly, RoBERTa-base-PM-M3-Voc is trained from scratch over PubMed and PMC and MIMIC-III clinical notes [43]. is based on the hypothesis that pretraining over taskrelated unlabelled text allows the model to learn both domain and task-specific knowledge [44] (refer Figure  11). In TAPT, task-related unlabelled sentences are gathered, and then the model is further pretrained. TAPT is less expensive compared to other pretraining methods as it involves pretraining the model over a relatively small corpus of task-related unlabelled sentences.","[['b42', 'b43', 'b19', 'b1']]","[['b42', 'b43', 'b19', 'b1']]",4,"1. The main drawback in continual pretraining is the general domain vocabulary.
2. For example, the WordPiece vocabulary in BERT is learned over English Wikipedia and Books Corpus [2].
3. As a result, the vocabulary does not represent the biomedical domain and hence many of the biomedical words are split into several subwords which hinders the model learning during pretraining and fine-tuning.
4. Moreover, the length of the input sequence also increases as many of the in-domain words are split into several subwords.
5. DSPT over in-domain text allows the model to have the in-domain vocabulary (refer Figure 10).
6. For example, PubMedBERT is trained from scratch using PubMed abstracts and PMC full-text articles [20].
7. PubMed achieved state-of-the-art results in the BLURB benchmark.
8. Similarly, RoBERTa-base-PM-M3-Voc is trained from scratch over PubMed and PMC and MIMIC-III clinical notes [43]. is based on the hypothesis that pretraining over taskrelated unlabelled text allows the model to learn both domain and task-specific knowledge [44] (refer Figure  11).
9. In TAPT, task-related unlabelled sentences are gathered, and then the model is further pretrained.
10. TAPT is less expensive compared to other pretraining methods as it involves pretraining the model over a relatively small corpus of task-related unlabelled sentences.","What are the limitations of using general domain vocabularies in biomedical pretrained language models?
How does Domain-Specific Pretraining (DSPT) address the issue of inadequate representation of biomedical vocabulary in language models?
What is the impact of using PubMed abstracts and PMC full-text articles on the performance of PubMedBERT in the BLURB benchmark?
How does the length of the input sequence change when using general domain vocabularies versus domain-specific vocabularies in biomedical language models?
What are the advantages of Task-Adaptive PreTraining (TAPT) over other pretraining methods in the context of biomedical language models?","1. What are the limitations of using general domain vocabularies in biomedical pretrained language models?
2. How does Domain-Specific Pretraining (DSPT) address the issue of inadequate representation of biomedical vocabulary in language models?
3. What is the impact of using PubMed abstracts and PMC full-text articles on the performance of PubMedBERT in the BLURB benchmark?
4. How does the length of the input sequence change when using general domain vocabularies versus domain-specific vocabularies in biomedical language models?
5. What are the advantages of Task-Adaptive PreTraining (TAPT) over other pretraining methods in the context of biomedical language models?",None,,"Questions:

1. What are the limitations of using general domain vocabularies in biomedical pretrained language models?
2. How does Domain-Specific Pretraining (DSPT) address the issue of inadequate representation of biomedical vocabulary in language models?
3. What is the impact of using PubMed abstracts and PMC full-text articles on the performance of PubMedBERT in the BLURB benchmark?
4. How does the length of the input sequence change when using general domain vocabularies versus domain-specific vocabularies in biomedical language models?
5. What are the advantages of Task-Adaptive PreTraining (TAPT) over other pretraining methods in the context of biomedical language models?

Answer:

(p14.0) The main drawback in continual pretraining is the general domain vocabulary. For example, the WordPiece vocabulary in BERT is learned over English Wikipedia and Books Corpus [2]. As a result, the vocabulary does not represent the biomedical domain and hence many of the biomedical words are split into several subwords which hinders the model learning during pretraining and fine-tuning. Moreover, the length of the input sequence also increases as many of the in-domain words are split into several subwords. DSPT over in-domain text allows the model to have the in-domain vocabulary (refer Figure 10). For example, PubMedBERT is trained from scratch using PubMed abstracts and PMC full-text articles [20]. PubMed achieved state-of-the-art results in the BLURB benchmark. Similarly, RoBERTa-base-PM-M3-Voc is trained from scratch over PubMed and PMC and MIMIC-III clinical notes [43]. is based on the hypothesis that pretraining over taskrelated unlabelled text allows the model to learn both domain and task-specific knowledge [44] (refer Figure  11). In TAPT, task-related unlabelled sentences are gathered, and then the model is further pretrained. TAPT is less expensive compared to other pretraining methods as it involves pretraining the model over a relatively small corpus of task-related unlabelled sentences.","Questions:



Answer:

(p14.0) The main drawback in continual pretraining is the general domain vocabulary. For example, the WordPiece vocabulary in BERT is learned over English Wikipedia and Books Corpus [2]. As a result, the vocabulary does not represent the biomedical domain and hence many of the biomedical words are split into several subwords which hinders the model learning during pretraining and fine-tuning. Moreover, the length of the input sequence also increases as many of the in-domain words are split into several subwords. DSPT over in-domain text allows the model to have the in-domain vocabulary (refer Figure 10). For example, PubMedBERT is trained from scratch using PubMed abstracts and PMC full-text articles [20]. PubMed achieved state-of-the-art results in the BLURB benchmark. Similarly, RoBERTa-base-PM-M3-Voc is trained from scratch over PubMed and PMC and MIMIC-III clinical notes [43]. is based on the hypothesis that pretraining over taskrelated unlabelled text allows the model to learn both domain and task-specific knowledge [44] (refer Figure  11). In TAPT, task-related unlabelled sentences are gathered, and then the model is further pretrained. TAPT is less expensive compared to other pretraining methods as it involves pretraining the model over a relatively small corpus of task-related unlabelled sentences."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s17,Main Pretraining Tasks,"['p17.0', 'p17.1']","['The main pretraining tasks allow the model to learn language representations. Some of the commonly used main pretraining tasks are masked language modelling (MLM) [2], replaced token detection (RTD) [50], sentence boundary objective (SBO) [49], next sentence prediction (NSP) [2] and sentence order prediction (SOP) [15].', 'Masked Language Modeling (MLM). It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens [2]. The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens. However, the meaning of a word depends on both the left and right contexts. Devlin et al. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model. Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI. Less challenging as it involves topic prediction which is a relatively easy task.']","The main pretraining tasks allow the model to learn language representations. Some of the commonly used main pretraining tasks are masked language modelling (MLM) [2], replaced token detection (RTD) [50], sentence boundary objective (SBO) [49], next sentence prediction (NSP) [2] and sentence order prediction (SOP) [15].

Masked Language Modeling (MLM). It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens [2]. The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens. However, the meaning of a word depends on both the left and right contexts. Devlin et al. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model. Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI. Less challenging as it involves topic prediction which is a relatively easy task.","(p17.0) The main pretraining tasks allow the model to learn language representations. Some of the commonly used main pretraining tasks are masked language modelling (MLM) [2], replaced token detection (RTD) [50], sentence boundary objective (SBO) [49], next sentence prediction (NSP) [2] and sentence order prediction (SOP) [15].

(p17.1) Masked Language Modeling (MLM). It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens [2]. The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens. However, the meaning of a word depends on both the left and right contexts. Devlin et al. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model. Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI. Less challenging as it involves topic prediction which is a relatively easy task.","[['b48', 'b14', 'b1', 'b49'], ['b1']]","[['b48', 'b14', 'b1', 'b49'], ['b1']]",5,"1. The main pretraining tasks allow the model to learn language representations.
2. Some of the commonly used main pretraining tasks are masked language modelling (MLM) [2], replaced token detection (RTD)
3. [50], sentence boundary objective (SBO) [49], next sentence prediction (NSP) [2] and sentence order prediction (SOP) [15].Masked Language Modeling (MLM).
4. It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens [2].
5. The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens.
6. However, the meaning of a word depends on both the left and right contexts.
7. Devlin et al. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model.
8. Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI.
9. Less challenging as it involves topic prediction which is a relatively easy task.","What are the main pretraining tasks used in transformer-based biomedical pretrained language models?
How does masked language modeling (MLM) differ from unidirectional language modeling in terms of context utilization?
What specific advantage does MLM provide in learning language representations for transformer-based models?
Why is next sentence prediction (NSP) considered less challenging as a pretraining task in transformer-based models?
Can you explain the role of sentence order prediction (SOP) in pretraining transformer-based language models and its impact on model performance?","1. What are the main pretraining tasks used in transformer-based biomedical pretrained language models?
2. How does masked language modeling (MLM) differ from unidirectional language modeling in terms of context utilization?
3. What specific advantage does MLM provide in learning language representations for transformer-based models?
4. Why is next sentence prediction (NSP) considered less challenging as a pretraining task in transformer-based models?
5. Can you explain the role of sentence order prediction (SOP) in pretraining transformer-based language models and its impact on model performance?",None,,"Questions:

1. What are the main pretraining tasks used in transformer-based biomedical pretrained language models?
2. How does masked language modeling (MLM) differ from unidirectional language modeling in terms of context utilization?
3. What specific advantage does MLM provide in learning language representations for transformer-based models?
4. Why is next sentence prediction (NSP) considered less challenging as a pretraining task in transformer-based models?
5. Can you explain the role of sentence order prediction (SOP) in pretraining transformer-based language models and its impact on model performance?

Answer:

(p17.0) The main pretraining tasks allow the model to learn language representations. Some of the commonly used main pretraining tasks are masked language modelling (MLM) [2], replaced token detection (RTD) [50], sentence boundary objective (SBO) [49], next sentence prediction (NSP) [2] and sentence order prediction (SOP) [15].

(p17.1) Masked Language Modeling (MLM). It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens [2]. The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens. However, the meaning of a word depends on both the left and right contexts. Devlin et al. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model. Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI. Less challenging as it involves topic prediction which is a relatively easy task.","Questions:



Answer:

(p17.0) The main pretraining tasks allow the model to learn language representations. Some of the commonly used main pretraining tasks are masked language modelling (MLM) [2], replaced token detection (RTD) [50], sentence boundary objective (SBO) [49], next sentence prediction (NSP) [2] and sentence order prediction (SOP) [15].

(p17.1) Masked Language Modeling (MLM). It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens [2]. The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens. However, the meaning of a word depends on both the left and right contexts. Devlin et al. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model. Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI. Less challenging as it involves topic prediction which is a relatively easy task."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s18,BERT [2],"['p18.0', 'p18.1', 'p18.2', 'p18.3', 'p18.4', 'p18.5', 'p18.6', 'p18.7', 'p18.8', 'p18.9', 'p18.10']","['SOP sentence-level Allows the model to learn sentence-level reasoning skills by modeling intersentence coherence. More challenging compared to NSP as SOP involves only sentence coherence.', 'ALBERT [15] SBO phrase-level Model predicts the masked tokens in a span based on boundary token representations and position embeddings.', 'SpanBERT [49] RTD word-level Model checks every token whether it is replaced or not. More efficient compared to MLM as it involves all the tokens in the input.', ""ELECTRA [50]  {x 1 , x 2 , . . . , x m }, a subset of tokens is randomly chosen and these tokens are replaced. The authors replaced tokens, 80% of the time with a special token '[MASK]', 10% of the time with a random token, and 10% of the time with the same token. This is done to handle the mismatch between pretraining and fine-tuning phases. Formally,"", 'wherex is the masked version of x and m(x) represents the set of masked token positions. Some of the improvements like dynamic masking [3], whole word masking [2], [20], [51], whole entity masking [48], [52], and whole span masking [48] are introduced in MLM to further improve its efficiency as a pretraining task. Delvin et al. [2] used static masking to replace the tokens i.e., the input sentences are masked once during pre-processing and the model predicts the same masked tokens in the input sentences for every epoch during pretraining. In the case of dynamic masking [3], different tokens are masked in the input sentence for different epochs which prevents the model from predicting the same masked tokens in every epoch and hence it learns more. Whole word masking is much more challenging as the model has to predict the entire word rather than part of a word. In the case of the whole entity and span maskings, in-domain entities and phrases in the input sentences are identified and then masked rather than masking the randomly chosen tokens. As a result, the model learns entity-centric and in-domain linguistic knowledge during pretraining which enhances the performance of the model in downstream tasks [48], [52]. For example, Zhang et al. [48] trained MC-BERT using NSP and MLM with whole entity and span maskings. Michalopoulos et al. [46] used novel multi-label loss-based MLM along with NSP to further pretrain ClinicalBERT on MIMIC-III clinical notes to get UmlsBERT. Novel multilabel lossbased MLM allows the model to connect all the words under the same concept. Sequence-to-Sequence MLM (Seq2SeqLM) is an extension of MLM to models based on encoder-decoder architecture. Models like T5 [4] are pretrained using Seq2SeqLM pretraining task.', ""Replaced Token Detection (RTD) [50]. It is a novel pretraining task that involves verifying whether each token in the input is replaced or not. Initially, some of the tokens in the input sentences are replaced with words predicted by a small generator network, and then the model (discriminator) is asked to predict the status of each word as replaced or not. The two advantages of RTD over MLM are a) RTD provides more training signal compared to MLM as RTD involves checking the status of every token in the input rather than a subset of randomly chosen tokens like MLM. and b) Unlike MLM, RTD does not use any special tokens like '[MASK]' to corrupt the input. So, it avoids the mismatch problem that the special token '[MASK]' is seen only during pretraining but not during fine-tuning. Formally,"", 'wherex is the corrupted version of x and t = 1 when the token is not a replaced one. Span Boundary Objective (SBO) [49]. It is a novel pretraining task that involves predicting the entire masked span based on the context. Initially, a contiguous span of tokens is randomly chosen and masked and then the model is asked to predict the masked tokens in the span based on the token representations at the boundary. In the case of MLM, the model predicts the masked token based on the final hidden vector of the masked token. However, in the case of SBO, the model predicts the masked token in the span based on the final hidden vectors of the boundary tokens and the position embedding of the masked token. SBO is more challenging as it is difficult to predict the entire span ""frequent bathroom runs"" than predicting ""frequent"" when the model already sees ""bathroom runs"". SBO helps the model to achieve better results in span extraction-based tasks like entity extraction and question answering [49], [53]. Let s and e represent the start and end indices of the span in the input sequence. Then, each token x i in the span is predicted based on the final hidden vectors of the boundary tokens x s−1 , x e+1 and its position embedding p i−s+1 . Then', 'where y i = g(x s−1 , x e+1 , p i−s+1 ), g() represents feedforward network of two layers and S represents the positions of tokens in contiguous span.', 'Next Sentence Prediction (NSP) [2]. NSP is a sentence-level pretraining task that involves predicting whether given two sentences appear consecutively or not . It is basically a two-way sentence pair classification task. Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsN ext, IsN otN ext} depending on whether the two sentences are consecutive or not. NSP helps the model to learn sentence-level reasoning skills which are useful in downstream tasks involving sentence pairs like natural language inference, text similarity, and question answering [2]. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are positive and the rest negative. Let z represents aggregate vector representation of the sentence pair (x, y). Then,', 'where t = 1 when the two sentences x and y are consecutive. Sentence Order Prediction (SOP) [15]. SOP is a novel sentence-level pretraining task which models intersentence coherence. Like NSP, SOP is a two-way sentence pair classification. Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsSwapped, IsN otSwapped} depending on whether the sentences are swapped or not. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are swapped and the rest are not swapped. Unlike NSP which involves the prediction of both topic and coherence, SOP involves only sentence coherence prediction [15]. Topic prediction is comparatively easier which questions the effectiveness of NSP as a pretraining task [3], [15], [49]. Let z represent aggregate vector representation of the sentence pair (x, y). Then,', 'where t = 1 when the two sentences x and y are not swapped.']","SOP sentence-level Allows the model to learn sentence-level reasoning skills by modeling intersentence coherence. More challenging compared to NSP as SOP involves only sentence coherence.

ALBERT [15] SBO phrase-level Model predicts the masked tokens in a span based on boundary token representations and position embeddings.

SpanBERT [49] RTD word-level Model checks every token whether it is replaced or not. More efficient compared to MLM as it involves all the tokens in the input.

ELECTRA [50]  {x 1 , x 2 , . . . , x m }, a subset of tokens is randomly chosen and these tokens are replaced. The authors replaced tokens, 80% of the time with a special token '[MASK]', 10% of the time with a random token, and 10% of the time with the same token. This is done to handle the mismatch between pretraining and fine-tuning phases. Formally,

wherex is the masked version of x and m(x) represents the set of masked token positions. Some of the improvements like dynamic masking [3], whole word masking [2], [20], [51], whole entity masking [48], [52], and whole span masking [48] are introduced in MLM to further improve its efficiency as a pretraining task. Delvin et al. [2] used static masking to replace the tokens i.e., the input sentences are masked once during pre-processing and the model predicts the same masked tokens in the input sentences for every epoch during pretraining. In the case of dynamic masking [3], different tokens are masked in the input sentence for different epochs which prevents the model from predicting the same masked tokens in every epoch and hence it learns more. Whole word masking is much more challenging as the model has to predict the entire word rather than part of a word. In the case of the whole entity and span maskings, in-domain entities and phrases in the input sentences are identified and then masked rather than masking the randomly chosen tokens. As a result, the model learns entity-centric and in-domain linguistic knowledge during pretraining which enhances the performance of the model in downstream tasks [48], [52]. For example, Zhang et al. [48] trained MC-BERT using NSP and MLM with whole entity and span maskings. Michalopoulos et al. [46] used novel multi-label loss-based MLM along with NSP to further pretrain ClinicalBERT on MIMIC-III clinical notes to get UmlsBERT. Novel multilabel lossbased MLM allows the model to connect all the words under the same concept. Sequence-to-Sequence MLM (Seq2SeqLM) is an extension of MLM to models based on encoder-decoder architecture. Models like T5 [4] are pretrained using Seq2SeqLM pretraining task.

Replaced Token Detection (RTD) [50]. It is a novel pretraining task that involves verifying whether each token in the input is replaced or not. Initially, some of the tokens in the input sentences are replaced with words predicted by a small generator network, and then the model (discriminator) is asked to predict the status of each word as replaced or not. The two advantages of RTD over MLM are a) RTD provides more training signal compared to MLM as RTD involves checking the status of every token in the input rather than a subset of randomly chosen tokens like MLM. and b) Unlike MLM, RTD does not use any special tokens like '[MASK]' to corrupt the input. So, it avoids the mismatch problem that the special token '[MASK]' is seen only during pretraining but not during fine-tuning. Formally,

wherex is the corrupted version of x and t = 1 when the token is not a replaced one. Span Boundary Objective (SBO) [49]. It is a novel pretraining task that involves predicting the entire masked span based on the context. Initially, a contiguous span of tokens is randomly chosen and masked and then the model is asked to predict the masked tokens in the span based on the token representations at the boundary. In the case of MLM, the model predicts the masked token based on the final hidden vector of the masked token. However, in the case of SBO, the model predicts the masked token in the span based on the final hidden vectors of the boundary tokens and the position embedding of the masked token. SBO is more challenging as it is difficult to predict the entire span ""frequent bathroom runs"" than predicting ""frequent"" when the model already sees ""bathroom runs"". SBO helps the model to achieve better results in span extraction-based tasks like entity extraction and question answering [49], [53]. Let s and e represent the start and end indices of the span in the input sequence. Then, each token x i in the span is predicted based on the final hidden vectors of the boundary tokens x s−1 , x e+1 and its position embedding p i−s+1 . Then

where y i = g(x s−1 , x e+1 , p i−s+1 ), g() represents feedforward network of two layers and S represents the positions of tokens in contiguous span.

Next Sentence Prediction (NSP) [2]. NSP is a sentence-level pretraining task that involves predicting whether given two sentences appear consecutively or not . It is basically a two-way sentence pair classification task. Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsN ext, IsN otN ext} depending on whether the two sentences are consecutive or not. NSP helps the model to learn sentence-level reasoning skills which are useful in downstream tasks involving sentence pairs like natural language inference, text similarity, and question answering [2]. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are positive and the rest negative. Let z represents aggregate vector representation of the sentence pair (x, y). Then,

where t = 1 when the two sentences x and y are consecutive. Sentence Order Prediction (SOP) [15]. SOP is a novel sentence-level pretraining task which models intersentence coherence. Like NSP, SOP is a two-way sentence pair classification. Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsSwapped, IsN otSwapped} depending on whether the sentences are swapped or not. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are swapped and the rest are not swapped. Unlike NSP which involves the prediction of both topic and coherence, SOP involves only sentence coherence prediction [15]. Topic prediction is comparatively easier which questions the effectiveness of NSP as a pretraining task [3], [15], [49]. Let z represent aggregate vector representation of the sentence pair (x, y). Then,

where t = 1 when the two sentences x and y are not swapped.","(p18.0) SOP sentence-level Allows the model to learn sentence-level reasoning skills by modeling intersentence coherence. More challenging compared to NSP as SOP involves only sentence coherence.

(p18.1) ALBERT [15] SBO phrase-level Model predicts the masked tokens in a span based on boundary token representations and position embeddings.

(p18.2) SpanBERT [49] RTD word-level Model checks every token whether it is replaced or not. More efficient compared to MLM as it involves all the tokens in the input.

(p18.3) ELECTRA [50]  {x 1 , x 2 , . . . , x m }, a subset of tokens is randomly chosen and these tokens are replaced. The authors replaced tokens, 80% of the time with a special token '[MASK]', 10% of the time with a random token, and 10% of the time with the same token. This is done to handle the mismatch between pretraining and fine-tuning phases. Formally,

(p18.4) wherex is the masked version of x and m(x) represents the set of masked token positions. Some of the improvements like dynamic masking [3], whole word masking [2], [20], [51], whole entity masking [48], [52], and whole span masking [48] are introduced in MLM to further improve its efficiency as a pretraining task. Delvin et al. [2] used static masking to replace the tokens i.e., the input sentences are masked once during pre-processing and the model predicts the same masked tokens in the input sentences for every epoch during pretraining. In the case of dynamic masking [3], different tokens are masked in the input sentence for different epochs which prevents the model from predicting the same masked tokens in every epoch and hence it learns more. Whole word masking is much more challenging as the model has to predict the entire word rather than part of a word. In the case of the whole entity and span maskings, in-domain entities and phrases in the input sentences are identified and then masked rather than masking the randomly chosen tokens. As a result, the model learns entity-centric and in-domain linguistic knowledge during pretraining which enhances the performance of the model in downstream tasks [48], [52]. For example, Zhang et al. [48] trained MC-BERT using NSP and MLM with whole entity and span maskings. Michalopoulos et al. [46] used novel multi-label loss-based MLM along with NSP to further pretrain ClinicalBERT on MIMIC-III clinical notes to get UmlsBERT. Novel multilabel lossbased MLM allows the model to connect all the words under the same concept. Sequence-to-Sequence MLM (Seq2SeqLM) is an extension of MLM to models based on encoder-decoder architecture. Models like T5 [4] are pretrained using Seq2SeqLM pretraining task.

(p18.5) Replaced Token Detection (RTD) [50]. It is a novel pretraining task that involves verifying whether each token in the input is replaced or not. Initially, some of the tokens in the input sentences are replaced with words predicted by a small generator network, and then the model (discriminator) is asked to predict the status of each word as replaced or not. The two advantages of RTD over MLM are a) RTD provides more training signal compared to MLM as RTD involves checking the status of every token in the input rather than a subset of randomly chosen tokens like MLM. and b) Unlike MLM, RTD does not use any special tokens like '[MASK]' to corrupt the input. So, it avoids the mismatch problem that the special token '[MASK]' is seen only during pretraining but not during fine-tuning. Formally,

(p18.6) wherex is the corrupted version of x and t = 1 when the token is not a replaced one. Span Boundary Objective (SBO) [49]. It is a novel pretraining task that involves predicting the entire masked span based on the context. Initially, a contiguous span of tokens is randomly chosen and masked and then the model is asked to predict the masked tokens in the span based on the token representations at the boundary. In the case of MLM, the model predicts the masked token based on the final hidden vector of the masked token. However, in the case of SBO, the model predicts the masked token in the span based on the final hidden vectors of the boundary tokens and the position embedding of the masked token. SBO is more challenging as it is difficult to predict the entire span ""frequent bathroom runs"" than predicting ""frequent"" when the model already sees ""bathroom runs"". SBO helps the model to achieve better results in span extraction-based tasks like entity extraction and question answering [49], [53]. Let s and e represent the start and end indices of the span in the input sequence. Then, each token x i in the span is predicted based on the final hidden vectors of the boundary tokens x s−1 , x e+1 and its position embedding p i−s+1 . Then

(p18.7) where y i = g(x s−1 , x e+1 , p i−s+1 ), g() represents feedforward network of two layers and S represents the positions of tokens in contiguous span.

(p18.8) Next Sentence Prediction (NSP) [2]. NSP is a sentence-level pretraining task that involves predicting whether given two sentences appear consecutively or not . It is basically a two-way sentence pair classification task. Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsN ext, IsN otN ext} depending on whether the two sentences are consecutive or not. NSP helps the model to learn sentence-level reasoning skills which are useful in downstream tasks involving sentence pairs like natural language inference, text similarity, and question answering [2]. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are positive and the rest negative. Let z represents aggregate vector representation of the sentence pair (x, y). Then,

(p18.9) where t = 1 when the two sentences x and y are consecutive. Sentence Order Prediction (SOP) [15]. SOP is a novel sentence-level pretraining task which models intersentence coherence. Like NSP, SOP is a two-way sentence pair classification. Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsSwapped, IsN otSwapped} depending on whether the sentences are swapped or not. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are swapped and the rest are not swapped. Unlike NSP which involves the prediction of both topic and coherence, SOP involves only sentence coherence prediction [15]. Topic prediction is comparatively easier which questions the effectiveness of NSP as a pretraining task [3], [15], [49]. Let z represent aggregate vector representation of the sentence pair (x, y). Then,

(p18.10) where t = 1 when the two sentences x and y are not swapped.","[[], ['b14'], ['b48'], ['b49'], ['b47', 'b1', 'b2', 'b19', 'b51', 'b3', 'b50', 'b45'], ['b49'], ['b52', 'b48'], [], ['b1'], ['b14', 'b48', 'b2'], []]","[[], ['b14'], ['b48'], ['b49'], ['b47', 'b1', 'b2', 'b19', 'b51', 'b3', 'b50', 'b45'], ['b49'], ['b52', 'b48'], [], ['b1'], ['b14', 'b48', 'b2'], []]",18,"1. SOP sentence-level Allows the model to learn sentence-level reasoning skills by modeling intersentence coherence.
2. More challenging compared to NSP as SOP involves only sentence coherence.
3. ALBERT [15] SBO phrase-level Model predicts the masked tokens in a span based on boundary token representations and position embeddings.
4. SpanBERT [49] RTD word-level Model checks every token whether it is replaced or not.
5. More efficient compared to MLM as it involves all the tokens in the input.
6. ELECTRA [50]  {x 1 , x 2 , . . . , x m }, a subset of tokens is randomly chosen and these tokens are replaced.
7. The authors replaced tokens, 80% of the time with a special token '[MASK]', 10% of the time with a random token, and 10% of the time with the same token.
8. This is done to handle the mismatch between pretraining and fine-tuning phases.
9. Formally,wherex is the masked version of x and m(x) represents the set of masked token positions.
10. Some of the improvements like dynamic masking (x)0, whole word masking [2], [20], [51], whole entity masking [48], [52], and whole span masking [48] are introduced in MLM to further improve its efficiency as a pretraining task.
11. Delvin et al. [2] used static masking to replace the tokens i.e., the input sentences are masked once during pre-processing and the model predicts the same masked tokens in the input sentences for every epoch during pretraining.
12. In the case of dynamic masking (x)0, different tokens are masked in the input sentence for different epochs which prevents the model from predicting the same masked tokens in every epoch and hence it learns more.
13. Whole word masking is much more challenging as the model has to predict the entire word rather than part of a word.
14. In the case of the whole entity and span maskings, in-domain entities and phrases in the input sentences are identified and then masked rather than masking the randomly chosen tokens.
15. As a result, the model learns entity-centric and in-domain linguistic knowledge during pretraining which enhances the performance of the model in downstream tasks [48], [52].
16. For example, Zhang et al. [48] trained MC-BERT using NSP and MLM with whole entity and span maskings.
17. Michalopoulos et al. [46] used novel multi-label loss-based MLM along with NSP to further pretrain ClinicalBERT on MIMIC-III clinical notes to get UmlsBERT.
18. Novel multilabel lossbased MLM allows the model to connect all the words under the same concept.
19. Sequence-to-Sequence MLM (Seq2SeqLM) is an extension of MLM to models based on encoder-decoder architecture.
20. Models like T5 [4] are pretrained using Seq2SeqLM pretraining task.
21. Replaced Token Detection (RTD) [50].
22. It is a novel pretraining task that involves verifying whether each token in the input is replaced or not.
23. Initially, some of the tokens in the input sentences are replaced with words predicted by a small generator network, and then the model (discriminator) is asked to predict the status of each word as replaced or not.
24. The two advantages of RTD over MLM are a) RTD provides more training signal compared to MLM as RTD involves checking the status of every token in the input rather than a subset of randomly chosen tokens like MLM.
25. and b) Unlike MLM, RTD does not use any special tokens like '[MASK]' to corrupt the input.
26. So, it avoids the mismatch problem that the special token '[MASK]' is seen only during pretraining but not during fine-tuning.
27. Formally,wherex is the corrupted version of x and t = 1 when the token is not a replaced one.
28. Span Boundary Objective (SBO) [49].
29. It is a novel pretraining task that involves predicting the entire masked span based on the context.
30. Initially, a contiguous span of tokens is randomly chosen and masked and then the model is asked to predict the masked tokens in the span based on the token representations at the boundary.
31. In the case of MLM, the model predicts the masked token based on the final hidden vector of the masked token.
32. However, in the case of SBO, the model predicts the masked token in the span based on the final hidden vectors of the boundary tokens and the position embedding of the masked token.
33. SBO is more challenging as it is difficult to predict the entire span ""frequent bathroom runs"" than predicting ""frequent"" when the model already sees ""bathroom runs"".
34. SBO helps the model to achieve better results in span extraction-based tasks like entity extraction and question answering [49], [53].
35. Let s and e represent the start and end indices of the span in the input sequence.
36. Then, each token x i in the span is predicted based on the final hidden vectors of the boundary tokens x s−1 , x e+1 and its position embedding p i−s+1 .
37. Thenwhere y i = g(x s−1 , x e+1 , p i−s+1 ), g() represents feedforward network of two layers and S represents the positions of tokens in contiguous span.
38. Next Sentence Prediction (NSP) [2].
39. NSP is a sentence-level pretraining task that involves predicting whether given two sentences appear consecutively or not .
40. It is basically a two-way sentence pair classification task.
41. Formally, for a given sentence pair (x)3, the model has to predict one of the two labels {IsN ext, IsN otN
42. ext} depending on whether the two sentences are consecutive or not.
43. NSP helps the model to learn sentence-level reasoning skills which are useful in downstream tasks involving sentence pairs like natural language inference, text similarity, and question answering [2].
44. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are positive and the rest negative.
45. Let z represents aggregate vector representation of the sentence pair (x)3.
46. Then,where t = 1 when the two sentences x and y are consecutive.
47. Sentence Order Prediction (SOP) [15].
48. SOP is a novel sentence-level pretraining task which models intersentence coherence.
49. Like NSP, SOP is a two-way sentence pair classification.
50. Formally, for a given sentence pair (x)3, the model has to predict one of the two labels {IsSwapped, IsN otSwapped} depending on whether the sentences are swapped or not.
51. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are swapped and the rest are not swapped.
52. Unlike NSP which involves the prediction of both topic and coherence, SOP involves only sentence coherence prediction [15].
53. Topic prediction is comparatively easier which questions the effectiveness of NSP as a pretraining task (x)0, [15], [49].
54. Let z represent aggregate vector representation of the sentence pair (x)3.
55. Then,where t = 1 when the two sentences x and y are not swapped.","How does the Replaced Token Detection (RTD) pretraining task differ from the Masked Language Model (MLM) in terms of training signal and input corruption?
What are the advantages of dynamic masking over static masking in the context of Masked Language Model (MLM) pretraining tasks?
How does the Span Boundary Objective (SBO) enhance model performance in span extraction-based tasks compared to traditional MLM?
In what ways does the Sentence Order Prediction (SOP) task focus differently from the Next Sentence Prediction (NSP) task during pretraining?
What specific improvements do whole word, entity, and span masking introduce to the efficiency of MLM pretraining tasks?","1. How does the Replaced Token Detection (RTD) pretraining task differ from the Masked Language Model (MLM) in terms of training signal and input corruption?
2. What are the advantages of dynamic masking over static masking in the context of Masked Language Model (MLM) pretraining tasks?
3. How does the Span Boundary Objective (SBO) enhance model performance in span extraction-based tasks compared to traditional MLM?
4. In what ways does the Sentence Order Prediction (SOP) task focus differently from the Next Sentence Prediction (NSP) task during pretraining?
5. What specific improvements do whole word, entity, and span masking introduce to the efficiency of MLM pretraining tasks?",None,,"Questions:

1. How does the Replaced Token Detection (RTD) pretraining task differ from the Masked Language Model (MLM) in terms of training signal and input corruption?
2. What are the advantages of dynamic masking over static masking in the context of Masked Language Model (MLM) pretraining tasks?
3. How does the Span Boundary Objective (SBO) enhance model performance in span extraction-based tasks compared to traditional MLM?
4. In what ways does the Sentence Order Prediction (SOP) task focus differently from the Next Sentence Prediction (NSP) task during pretraining?
5. What specific improvements do whole word, entity, and span masking introduce to the efficiency of MLM pretraining tasks?

Answer:

(p18.0) SOP sentence-level Allows the model to learn sentence-level reasoning skills by modeling intersentence coherence. More challenging compared to NSP as SOP involves only sentence coherence.

(p18.1) ALBERT [15] SBO phrase-level Model predicts the masked tokens in a span based on boundary token representations and position embeddings.

(p18.2) SpanBERT [49] RTD word-level Model checks every token whether it is replaced or not. More efficient compared to MLM as it involves all the tokens in the input.

(p18.3) ELECTRA [50]  {x 1 , x 2 , . . . , x m }, a subset of tokens is randomly chosen and these tokens are replaced. The authors replaced tokens, 80% of the time with a special token '[MASK]', 10% of the time with a random token, and 10% of the time with the same token. This is done to handle the mismatch between pretraining and fine-tuning phases. Formally,

(p18.4) wherex is the masked version of x and m(x) represents the set of masked token positions. Some of the improvements like dynamic masking [3], whole word masking [2], [20], [51], whole entity masking [48], [52], and whole span masking [48] are introduced in MLM to further improve its efficiency as a pretraining task. Delvin et al. [2] used static masking to replace the tokens i.e., the input sentences are masked once during pre-processing and the model predicts the same masked tokens in the input sentences for every epoch during pretraining. In the case of dynamic masking [3], different tokens are masked in the input sentence for different epochs which prevents the model from predicting the same masked tokens in every epoch and hence it learns more. Whole word masking is much more challenging as the model has to predict the entire word rather than part of a word. In the case of the whole entity and span maskings, in-domain entities and phrases in the input sentences are identified and then masked rather than masking the randomly chosen tokens. As a result, the model learns entity-centric and in-domain linguistic knowledge during pretraining which enhances the performance of the model in downstream tasks [48], [52]. For example, Zhang et al. [48] trained MC-BERT using NSP and MLM with whole entity and span maskings. Michalopoulos et al. [46] used novel multi-label loss-based MLM along with NSP to further pretrain ClinicalBERT on MIMIC-III clinical notes to get UmlsBERT. Novel multilabel lossbased MLM allows the model to connect all the words under the same concept. Sequence-to-Sequence MLM (Seq2SeqLM) is an extension of MLM to models based on encoder-decoder architecture. Models like T5 [4] are pretrained using Seq2SeqLM pretraining task.

(p18.5) Replaced Token Detection (RTD) [50]. It is a novel pretraining task that involves verifying whether each token in the input is replaced or not. Initially, some of the tokens in the input sentences are replaced with words predicted by a small generator network, and then the model (discriminator) is asked to predict the status of each word as replaced or not. The two advantages of RTD over MLM are a) RTD provides more training signal compared to MLM as RTD involves checking the status of every token in the input rather than a subset of randomly chosen tokens like MLM. and b) Unlike MLM, RTD does not use any special tokens like '[MASK]' to corrupt the input. So, it avoids the mismatch problem that the special token '[MASK]' is seen only during pretraining but not during fine-tuning. Formally,

(p18.6) wherex is the corrupted version of x and t = 1 when the token is not a replaced one. Span Boundary Objective (SBO) [49]. It is a novel pretraining task that involves predicting the entire masked span based on the context. Initially, a contiguous span of tokens is randomly chosen and masked and then the model is asked to predict the masked tokens in the span based on the token representations at the boundary. In the case of MLM, the model predicts the masked token based on the final hidden vector of the masked token. However, in the case of SBO, the model predicts the masked token in the span based on the final hidden vectors of the boundary tokens and the position embedding of the masked token. SBO is more challenging as it is difficult to predict the entire span ""frequent bathroom runs"" than predicting ""frequent"" when the model already sees ""bathroom runs"". SBO helps the model to achieve better results in span extraction-based tasks like entity extraction and question answering [49], [53]. Let s and e represent the start and end indices of the span in the input sequence. Then, each token x i in the span is predicted based on the final hidden vectors of the boundary tokens x s−1 , x e+1 and its position embedding p i−s+1 . Then

(p18.7) where y i = g(x s−1 , x e+1 , p i−s+1 ), g() represents feedforward network of two layers and S represents the positions of tokens in contiguous span.

(p18.8) Next Sentence Prediction (NSP) [2]. NSP is a sentence-level pretraining task that involves predicting whether given two sentences appear consecutively or not . It is basically a two-way sentence pair classification task. Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsN ext, IsN otN ext} depending on whether the two sentences are consecutive or not. NSP helps the model to learn sentence-level reasoning skills which are useful in downstream tasks involving sentence pairs like natural language inference, text similarity, and question answering [2]. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are positive and the rest negative. Let z represents aggregate vector representation of the sentence pair (x, y). Then,

(p18.9) where t = 1 when the two sentences x and y are consecutive. Sentence Order Prediction (SOP) [15]. SOP is a novel sentence-level pretraining task which models intersentence coherence. Like NSP, SOP is a two-way sentence pair classification. Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsSwapped, IsN otSwapped} depending on whether the sentences are swapped or not. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are swapped and the rest are not swapped. Unlike NSP which involves the prediction of both topic and coherence, SOP involves only sentence coherence prediction [15]. Topic prediction is comparatively easier which questions the effectiveness of NSP as a pretraining task [3], [15], [49]. Let z represent aggregate vector representation of the sentence pair (x, y). Then,

(p18.10) where t = 1 when the two sentences x and y are not swapped.","Questions:



Answer:

(p18.0) SOP sentence-level Allows the model to learn sentence-level reasoning skills by modeling intersentence coherence. More challenging compared to NSP as SOP involves only sentence coherence.

(p18.1) ALBERT [15] SBO phrase-level Model predicts the masked tokens in a span based on boundary token representations and position embeddings.

(p18.2) SpanBERT [49] RTD word-level Model checks every token whether it is replaced or not. More efficient compared to MLM as it involves all the tokens in the input.

(p18.3) ELECTRA [50]  {x 1 , x 2 , . . . , x m }, a subset of tokens is randomly chosen and these tokens are replaced. The authors replaced tokens, 80% of the time with a special token '[MASK]', 10% of the time with a random token, and 10% of the time with the same token. This is done to handle the mismatch between pretraining and fine-tuning phases. Formally,

(p18.4) wherex is the masked version of x and m(x) represents the set of masked token positions. Some of the improvements like dynamic masking [3], whole word masking [2], [20], [51], whole entity masking [48], [52], and whole span masking [48] are introduced in MLM to further improve its efficiency as a pretraining task. Delvin et al. [2] used static masking to replace the tokens i.e., the input sentences are masked once during pre-processing and the model predicts the same masked tokens in the input sentences for every epoch during pretraining. In the case of dynamic masking [3], different tokens are masked in the input sentence for different epochs which prevents the model from predicting the same masked tokens in every epoch and hence it learns more. Whole word masking is much more challenging as the model has to predict the entire word rather than part of a word. In the case of the whole entity and span maskings, in-domain entities and phrases in the input sentences are identified and then masked rather than masking the randomly chosen tokens. As a result, the model learns entity-centric and in-domain linguistic knowledge during pretraining which enhances the performance of the model in downstream tasks [48], [52]. For example, Zhang et al. [48] trained MC-BERT using NSP and MLM with whole entity and span maskings. Michalopoulos et al. [46] used novel multi-label loss-based MLM along with NSP to further pretrain ClinicalBERT on MIMIC-III clinical notes to get UmlsBERT. Novel multilabel lossbased MLM allows the model to connect all the words under the same concept. Sequence-to-Sequence MLM (Seq2SeqLM) is an extension of MLM to models based on encoder-decoder architecture. Models like T5 [4] are pretrained using Seq2SeqLM pretraining task.

(p18.5) Replaced Token Detection (RTD) [50]. It is a novel pretraining task that involves verifying whether each token in the input is replaced or not. Initially, some of the tokens in the input sentences are replaced with words predicted by a small generator network, and then the model (discriminator) is asked to predict the status of each word as replaced or not. The two advantages of RTD over MLM are a) RTD provides more training signal compared to MLM as RTD involves checking the status of every token in the input rather than a subset of randomly chosen tokens like MLM. and b) Unlike MLM, RTD does not use any special tokens like '[MASK]' to corrupt the input. So, it avoids the mismatch problem that the special token '[MASK]' is seen only during pretraining but not during fine-tuning. Formally,

(p18.6) wherex is the corrupted version of x and t = 1 when the token is not a replaced one. Span Boundary Objective (SBO) [49]. It is a novel pretraining task that involves predicting the entire masked span based on the context. Initially, a contiguous span of tokens is randomly chosen and masked and then the model is asked to predict the masked tokens in the span based on the token representations at the boundary. In the case of MLM, the model predicts the masked token based on the final hidden vector of the masked token. However, in the case of SBO, the model predicts the masked token in the span based on the final hidden vectors of the boundary tokens and the position embedding of the masked token. SBO is more challenging as it is difficult to predict the entire span ""frequent bathroom runs"" than predicting ""frequent"" when the model already sees ""bathroom runs"". SBO helps the model to achieve better results in span extraction-based tasks like entity extraction and question answering [49], [53]. Let s and e represent the start and end indices of the span in the input sequence. Then, each token x i in the span is predicted based on the final hidden vectors of the boundary tokens x s−1 , x e+1 and its position embedding p i−s+1 . Then

(p18.7) where y i = g(x s−1 , x e+1 , p i−s+1 ), g() represents feedforward network of two layers and S represents the positions of tokens in contiguous span.

(p18.8) Next Sentence Prediction (NSP) [2]. NSP is a sentence-level pretraining task that involves predicting whether given two sentences appear consecutively or not . It is basically a two-way sentence pair classification task. Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsN ext, IsN otN ext} depending on whether the two sentences are consecutive or not. NSP helps the model to learn sentence-level reasoning skills which are useful in downstream tasks involving sentence pairs like natural language inference, text similarity, and question answering [2]. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are positive and the rest negative. Let z represents aggregate vector representation of the sentence pair (x, y). Then,

(p18.9) where t = 1 when the two sentences x and y are consecutive. Sentence Order Prediction (SOP) [15]. SOP is a novel sentence-level pretraining task which models intersentence coherence. Like NSP, SOP is a two-way sentence pair classification. Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsSwapped, IsN otSwapped} depending on whether the sentences are swapped or not. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are swapped and the rest are not swapped. Unlike NSP which involves the prediction of both topic and coherence, SOP involves only sentence coherence prediction [15]. Topic prediction is comparatively easier which questions the effectiveness of NSP as a pretraining task [3], [15], [49]. Let z represent aggregate vector representation of the sentence pair (x, y). Then,

(p18.10) where t = 1 when the two sentences x and y are not swapped."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s19,Auxiliary Pretraining Tasks,"['p19.0', 'p19.1']","['Auxiliary pretraining tasks help to inject knowledge from human-curated sources like UMLS [54] into indomain models to further enhance them. For example, the triple classification pretraining task involves identifying whether two concepts are connected by the relation or not [45]. This auxiliary task is used by Hao et al. [45] to inject UMLS relation knowledge into in-domain models. Yuan et al. [47] used two auxiliary pretraining tasks based on multi-similarity Loss and Knowledge embedding loss to further pretrain BioBERT on UMLS.', 'Similarly, Liu et al. [34] used multi-similarity loss-based pretraining task to inject UMLS synonym knowledge into PubMedBERT.']","Auxiliary pretraining tasks help to inject knowledge from human-curated sources like UMLS [54] into indomain models to further enhance them. For example, the triple classification pretraining task involves identifying whether two concepts are connected by the relation or not [45]. This auxiliary task is used by Hao et al. [45] to inject UMLS relation knowledge into in-domain models. Yuan et al. [47] used two auxiliary pretraining tasks based on multi-similarity Loss and Knowledge embedding loss to further pretrain BioBERT on UMLS.

Similarly, Liu et al. [34] used multi-similarity loss-based pretraining task to inject UMLS synonym knowledge into PubMedBERT.","(p19.0) Auxiliary pretraining tasks help to inject knowledge from human-curated sources like UMLS [54] into indomain models to further enhance them. For example, the triple classification pretraining task involves identifying whether two concepts are connected by the relation or not [45]. This auxiliary task is used by Hao et al. [45] to inject UMLS relation knowledge into in-domain models. Yuan et al. [47] used two auxiliary pretraining tasks based on multi-similarity Loss and Knowledge embedding loss to further pretrain BioBERT on UMLS.

(p19.1) Similarly, Liu et al. [34] used multi-similarity loss-based pretraining task to inject UMLS synonym knowledge into PubMedBERT.","[['b46', 'b44', 'b53'], ['b33']]","[['b46', 'b44', 'b53'], ['b33']]",4,"1. Auxiliary pretraining tasks help to inject knowledge from human-curated sources like UMLS [54] into indomain models to further enhance them.
2. For example, the triple classification pretraining task involves identifying whether two concepts are connected by the relation or not [45].
3. This auxiliary task is used by Hao et al. [45] to inject UMLS relation knowledge into in-domain models.
4. Yuan et al. [47] used two auxiliary pretraining tasks based on multi-similarity Loss and Knowledge embedding loss to further pretrain BioBERT on UMLS.
5. Similarly, Liu et al. [34] used multi-similarity loss-based pretraining task to inject UMLS synonym knowledge into PubMedBERT.","What is the role of auxiliary pretraining tasks in enhancing transformer-based biomedical pretrained language models?
How does the triple classification pretraining task assist in the incorporation of UMLS relation knowledge into in-domain models?
In what ways do auxiliary pretraining tasks based on multi-similarity loss and knowledge embedding loss contribute to the pretraining of BioBERT?
How is UMLS synonym knowledge integrated into PubMedBERT through the use of a multi-similarity loss-based pretraining task?
What are the benefits of incorporating human-curated sources like UMLS into in-domain models through auxiliary pretraining tasks?","1. What is the role of auxiliary pretraining tasks in enhancing transformer-based biomedical pretrained language models?
2. How does the triple classification pretraining task assist in the incorporation of UMLS relation knowledge into in-domain models?
3. In what ways do auxiliary pretraining tasks based on multi-similarity loss and knowledge embedding loss contribute to the pretraining of BioBERT?
4. How is UMLS synonym knowledge integrated into PubMedBERT through the use of a multi-similarity loss-based pretraining task?
5. What are the benefits of incorporating human-curated sources like UMLS into in-domain models through auxiliary pretraining tasks?",None,,"Questions:

1. What is the role of auxiliary pretraining tasks in enhancing transformer-based biomedical pretrained language models?
2. How does the triple classification pretraining task assist in the incorporation of UMLS relation knowledge into in-domain models?
3. In what ways do auxiliary pretraining tasks based on multi-similarity loss and knowledge embedding loss contribute to the pretraining of BioBERT?
4. How is UMLS synonym knowledge integrated into PubMedBERT through the use of a multi-similarity loss-based pretraining task?
5. What are the benefits of incorporating human-curated sources like UMLS into in-domain models through auxiliary pretraining tasks?

Answer:

(p19.0) Auxiliary pretraining tasks help to inject knowledge from human-curated sources like UMLS [54] into indomain models to further enhance them. For example, the triple classification pretraining task involves identifying whether two concepts are connected by the relation or not [45]. This auxiliary task is used by Hao et al. [45] to inject UMLS relation knowledge into in-domain models. Yuan et al. [47] used two auxiliary pretraining tasks based on multi-similarity Loss and Knowledge embedding loss to further pretrain BioBERT on UMLS.

(p19.1) Similarly, Liu et al. [34] used multi-similarity loss-based pretraining task to inject UMLS synonym knowledge into PubMedBERT.","Questions:



Answer:

(p19.0) Auxiliary pretraining tasks help to inject knowledge from human-curated sources like UMLS [54] into indomain models to further enhance them. For example, the triple classification pretraining task involves identifying whether two concepts are connected by the relation or not [45]. This auxiliary task is used by Hao et al. [45] to inject UMLS relation knowledge into in-domain models. Yuan et al. [47] used two auxiliary pretraining tasks based on multi-similarity Loss and Knowledge embedding loss to further pretrain BioBERT on UMLS.

(p19.1) Similarly, Liu et al. [34] used multi-similarity loss-based pretraining task to inject UMLS synonym knowledge into PubMedBERT."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s21,Intermediate Fine-Tuning (IFT),"['p21.0', 'p21.1', 'p21.2', 'p21.3']","['IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets. IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]- [58]. For example, Cengiz et al. [55] fine-tuned indomain model on general NLI datasets like SNLI [59] and MNLI [60] before fine-tuning on MedNLI [61].', 'Same Task Same Domain -Here, the source and target datasets are from the same task and domain. But the source dataset is a more generic one while the target dataset is more specific [62], [63]. For example, Gao et al. [63] fine-tuned BlueBERT on a large general biomedical NER corpus like MedMentions [64] or Semantic Medline before fine-tuning on the small target NER corpus.', 'Different Task Same Domain -Here, the source and target datasets are from different tasks but the same domain. Fine-tuning on source dataset which is from the same domain allows the model to gain more domainspecific knowledge which improves the performance of the model on the same domain target task [65]. McCreery et al. [65] fine-tuned the model on the medical questionanswer pairs dataset to enhance its performance on the medical question similarity dataset.', 'Different Task Different Domain -Here, the source and target datasets are from different tasks and different domains. For example, Jeong et al. [66] fine-tuned BioBERT on a general MultiNLI dataset to improve the performance of the model in biomedical QA. Here the model learns sentence level reasoning skills which are useful in biomedical QA.']","IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets. IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]- [58]. For example, Cengiz et al. [55] fine-tuned indomain model on general NLI datasets like SNLI [59] and MNLI [60] before fine-tuning on MedNLI [61].

Same Task Same Domain -Here, the source and target datasets are from the same task and domain. But the source dataset is a more generic one while the target dataset is more specific [62], [63]. For example, Gao et al. [63] fine-tuned BlueBERT on a large general biomedical NER corpus like MedMentions [64] or Semantic Medline before fine-tuning on the small target NER corpus.

Different Task Same Domain -Here, the source and target datasets are from different tasks but the same domain. Fine-tuning on source dataset which is from the same domain allows the model to gain more domainspecific knowledge which improves the performance of the model on the same domain target task [65]. McCreery et al. [65] fine-tuned the model on the medical questionanswer pairs dataset to enhance its performance on the medical question similarity dataset.

Different Task Different Domain -Here, the source and target datasets are from different tasks and different domains. For example, Jeong et al. [66] fine-tuned BioBERT on a general MultiNLI dataset to improve the performance of the model in biomedical QA. Here the model learns sentence level reasoning skills which are useful in biomedical QA.","(p21.0) IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets. IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]- [58]. For example, Cengiz et al. [55] fine-tuned indomain model on general NLI datasets like SNLI [59] and MNLI [60] before fine-tuning on MedNLI [61].

(p21.1) Same Task Same Domain -Here, the source and target datasets are from the same task and domain. But the source dataset is a more generic one while the target dataset is more specific [62], [63]. For example, Gao et al. [63] fine-tuned BlueBERT on a large general biomedical NER corpus like MedMentions [64] or Semantic Medline before fine-tuning on the small target NER corpus.

(p21.2) Different Task Same Domain -Here, the source and target datasets are from different tasks but the same domain. Fine-tuning on source dataset which is from the same domain allows the model to gain more domainspecific knowledge which improves the performance of the model on the same domain target task [65]. McCreery et al. [65] fine-tuned the model on the medical questionanswer pairs dataset to enhance its performance on the medical question similarity dataset.

(p21.3) Different Task Different Domain -Here, the source and target datasets are from different tasks and different domains. For example, Jeong et al. [66] fine-tuned BioBERT on a general MultiNLI dataset to improve the performance of the model in biomedical QA. Here the model learns sentence level reasoning skills which are useful in biomedical QA.","[['b54', 'b57', 'b60', 'b58', 'b59'], ['b62', 'b64', 'b63'], ['b65'], ['b66']]","[['b54', 'b57', 'b60', 'b58', 'b59'], ['b62', 'b64', 'b63'], ['b65'], ['b66']]",10,"1. IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets.
2. IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains.
3. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]- [58].
4. For example, Cengiz et al. [55] fine-tuned indomain model on general NLI datasets like SNLI [59] and MNLI [60] before fine-tuning on MedNLI [61].
5. Same Task Same Domain -Here, the source and target datasets are from the same task and domain.
6. But the source dataset is a more generic one while the target dataset is more specific [62], [63].
7. For example, Gao et al. [63] fine-tuned BlueBERT on a large general biomedical NER corpus like MedMentions [64] or Semantic Medline before fine-tuning on the small target NER corpus.
8. Different Task Same Domain -Here, the source and target datasets are from different tasks but the same domain.
9. Fine-tuning on source dataset which is from the same domain allows the model to gain more domainspecific knowledge which improves the performance of the model on the same domain target task [58]1.
10. McCreery et al. [58]1 fine-tuned the model on the medical questionanswer pairs dataset to enhance its performance on the medical question similarity dataset.
11. Different Task Different Domain -Here, the source and target datasets are from different tasks and different domains.
12. For example, Jeong et al. [58]2 fine-tuned BioBERT on a general MultiNLI dataset to improve the performance of the model in biomedical QA.
13. Here the model learns sentence level reasoning skills which are useful in biomedical QA.","How does Intermediate Fine-Tuning (IFT) enhance the performance of biomedical pretrained language models on small target datasets?
What are the benefits of fine-tuning a model on general domain datasets before applying it to small in-domain datasets?
How does fine-tuning on a large general dataset within the same domain affect the performance on a more specific target dataset?
In what way does fine-tuning a model on different tasks within the same domain contribute to its domain-specific knowledge and performance improvement?
Can fine-tuning a biomedical pretrained language model on datasets from different tasks and domains improve its sentence-level reasoning skills in biomedical question answering?","1. How does Intermediate Fine-Tuning (IFT) enhance the performance of biomedical pretrained language models on small target datasets?
2. What are the benefits of fine-tuning a model on general domain datasets before applying it to small in-domain datasets?
3. How does fine-tuning on a large general dataset within the same domain affect the performance on a more specific target dataset?
4. In what way does fine-tuning a model on different tasks within the same domain contribute to its domain-specific knowledge and performance improvement?
5. Can fine-tuning a biomedical pretrained language model on datasets from different tasks and domains improve its sentence-level reasoning skills in biomedical question answering?",None,,"Questions:

1. How does Intermediate Fine-Tuning (IFT) enhance the performance of biomedical pretrained language models on small target datasets?
2. What are the benefits of fine-tuning a model on general domain datasets before applying it to small in-domain datasets?
3. How does fine-tuning on a large general dataset within the same domain affect the performance on a more specific target dataset?
4. In what way does fine-tuning a model on different tasks within the same domain contribute to its domain-specific knowledge and performance improvement?
5. Can fine-tuning a biomedical pretrained language model on datasets from different tasks and domains improve its sentence-level reasoning skills in biomedical question answering?

Answer:

(p21.0) IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets. IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]- [58]. For example, Cengiz et al. [55] fine-tuned indomain model on general NLI datasets like SNLI [59] and MNLI [60] before fine-tuning on MedNLI [61].

(p21.1) Same Task Same Domain -Here, the source and target datasets are from the same task and domain. But the source dataset is a more generic one while the target dataset is more specific [62], [63]. For example, Gao et al. [63] fine-tuned BlueBERT on a large general biomedical NER corpus like MedMentions [64] or Semantic Medline before fine-tuning on the small target NER corpus.

(p21.2) Different Task Same Domain -Here, the source and target datasets are from different tasks but the same domain. Fine-tuning on source dataset which is from the same domain allows the model to gain more domainspecific knowledge which improves the performance of the model on the same domain target task [65]. McCreery et al. [65] fine-tuned the model on the medical questionanswer pairs dataset to enhance its performance on the medical question similarity dataset.

(p21.3) Different Task Different Domain -Here, the source and target datasets are from different tasks and different domains. For example, Jeong et al. [66] fine-tuned BioBERT on a general MultiNLI dataset to improve the performance of the model in biomedical QA. Here the model learns sentence level reasoning skills which are useful in biomedical QA.","Questions:



Answer:

(p21.0) IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets. IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]- [58]. For example, Cengiz et al. [55] fine-tuned indomain model on general NLI datasets like SNLI [59] and MNLI [60] before fine-tuning on MedNLI [61].

(p21.1) Same Task Same Domain -Here, the source and target datasets are from the same task and domain. But the source dataset is a more generic one while the target dataset is more specific [62], [63]. For example, Gao et al. [63] fine-tuned BlueBERT on a large general biomedical NER corpus like MedMentions [64] or Semantic Medline before fine-tuning on the small target NER corpus.

(p21.2) Different Task Same Domain -Here, the source and target datasets are from different tasks but the same domain. Fine-tuning on source dataset which is from the same domain allows the model to gain more domainspecific knowledge which improves the performance of the model on the same domain target task [65]. McCreery et al. [65] fine-tuned the model on the medical questionanswer pairs dataset to enhance its performance on the medical question similarity dataset.

(p21.3) Different Task Different Domain -Here, the source and target datasets are from different tasks and different domains. For example, Jeong et al. [66] fine-tuned BioBERT on a general MultiNLI dataset to improve the performance of the model in biomedical QA. Here the model learns sentence level reasoning skills which are useful in biomedical QA."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s22,Multi-Task Fine-Tuning,['p22.0'],"['Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. At the same time, due to the increase in training set size, the model is less prone to over-fitting. Multi-task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]. Moreover, having a single model for multitasks eliminates the need of deploying separate models for each task which saves computational resources, time, and deployment costs [70]. Multi-task fine-tuning may not provide the best results all the time [70]. In such cases, multi-task fine tuning can be applied iteratively to identify the best possible subset of tasks [71]. For example, Mahanjan et al. [71] applied multi-task finetuning iteratively to choose the best subset of related datasets. Finally, the authors fine-tuned the model on best subset of related datasets and achieved the best results on the Clinical STS [72] dataset. After multi-task fine-tuning, the model can be further fine-tuned on the target specific dataset separately to further enhance the performance of the model [73].']","Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. At the same time, due to the increase in training set size, the model is less prone to over-fitting. Multi-task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]. Moreover, having a single model for multitasks eliminates the need of deploying separate models for each task which saves computational resources, time, and deployment costs [70]. Multi-task fine-tuning may not provide the best results all the time [70]. In such cases, multi-task fine tuning can be applied iteratively to identify the best possible subset of tasks [71]. For example, Mahanjan et al. [71] applied multi-task finetuning iteratively to choose the best subset of related datasets. Finally, the authors fine-tuned the model on best subset of related datasets and achieved the best results on the Clinical STS [72] dataset. After multi-task fine-tuning, the model can be further fine-tuned on the target specific dataset separately to further enhance the performance of the model [73].","(p22.0) Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. At the same time, due to the increase in training set size, the model is less prone to over-fitting. Multi-task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]. Moreover, having a single model for multitasks eliminates the need of deploying separate models for each task which saves computational resources, time, and deployment costs [70]. Multi-task fine-tuning may not provide the best results all the time [70]. In such cases, multi-task fine tuning can be applied iteratively to identify the best possible subset of tasks [71]. For example, Mahanjan et al. [71] applied multi-task finetuning iteratively to choose the best subset of related datasets. Finally, the authors fine-tuned the model on best subset of related datasets and achieved the best results on the Clinical STS [72] dataset. After multi-task fine-tuning, the model can be further fine-tuned on the target specific dataset separately to further enhance the performance of the model [73].","[['b72', 'b67', 'b73', 'b69', 'b71', 'b70']]","[['b72', 'b67', 'b73', 'b69', 'b71', 'b70']]",6,"1. Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]-[69].
2. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer.
3. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks.
4. At the same time, due to the increase in training set size, the model is less prone to over-fitting.
5. Multi-task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69].
6. Moreover, having a single model for multitasks eliminates the need of deploying separate models for each task which saves computational resources, time, and deployment costs [70].
7. Multi-task fine-tuning may not provide the best results all the time [70].
8. In such cases, multi-task fine tuning can be applied iteratively to identify the best possible subset of tasks [71].
9. For example, Mahanjan et al. [71] applied multi-task finetuning iteratively to choose the best subset of related datasets.
10. Finally, the authors fine-tuned the model on best subset of related datasets and achieved the best results on the Clinical STS [72] dataset.
11. After multi-task fine-tuning, the model can be further fine-tuned on the target specific dataset separately to further enhance the performance of the model [73].","How does multi-task fine-tuning enhance the performance of transformer-based biomedical pretrained language models?
What are the benefits of using a single model for multiple tasks in the context of computational resources, time, and deployment costs?
Why is multi-task fine-tuning particularly beneficial in low resource scenarios within the biomedical domain?
How can multi-task fine-tuning be optimized to achieve the best results on specific datasets?
What is the process and advantage of further fine-tuning a model on a target-specific dataset after multi-task fine-tuning?","1. How does multi-task fine-tuning enhance the performance of transformer-based biomedical pretrained language models?
2. What are the benefits of using a single model for multiple tasks in the context of computational resources, time, and deployment costs?
3. Why is multi-task fine-tuning particularly beneficial in low resource scenarios within the biomedical domain?
4. How can multi-task fine-tuning be optimized to achieve the best results on specific datasets?
5. What is the process and advantage of further fine-tuning a model on a target-specific dataset after multi-task fine-tuning?",None,,"Questions:

1. How does multi-task fine-tuning enhance the performance of transformer-based biomedical pretrained language models?
2. What are the benefits of using a single model for multiple tasks in the context of computational resources, time, and deployment costs?
3. Why is multi-task fine-tuning particularly beneficial in low resource scenarios within the biomedical domain?
4. How can multi-task fine-tuning be optimized to achieve the best results on specific datasets?
5. What is the process and advantage of further fine-tuning a model on a target-specific dataset after multi-task fine-tuning?

Answer:

(p22.0) Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. At the same time, due to the increase in training set size, the model is less prone to over-fitting. Multi-task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]. Moreover, having a single model for multitasks eliminates the need of deploying separate models for each task which saves computational resources, time, and deployment costs [70]. Multi-task fine-tuning may not provide the best results all the time [70]. In such cases, multi-task fine tuning can be applied iteratively to identify the best possible subset of tasks [71]. For example, Mahanjan et al. [71] applied multi-task finetuning iteratively to choose the best subset of related datasets. Finally, the authors fine-tuned the model on best subset of related datasets and achieved the best results on the Clinical STS [72] dataset. After multi-task fine-tuning, the model can be further fine-tuned on the target specific dataset separately to further enhance the performance of the model [73].","Questions:



Answer:

(p22.0) Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. At the same time, due to the increase in training set size, the model is less prone to over-fitting. Multi-task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]. Moreover, having a single model for multitasks eliminates the need of deploying separate models for each task which saves computational resources, time, and deployment costs [70]. Multi-task fine-tuning may not provide the best results all the time [70]. In such cases, multi-task fine tuning can be applied iteratively to identify the best possible subset of tasks [71]. For example, Mahanjan et al. [71] applied multi-task finetuning iteratively to choose the best subset of related datasets. Finally, the authors fine-tuned the model on best subset of related datasets and achieved the best results on the Clinical STS [72] dataset. After multi-task fine-tuning, the model can be further fine-tuned on the target specific dataset separately to further enhance the performance of the model [73]."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s24,Main Embeddings,"['p24.0', 'p24.1', 'p24.2', 'p24.3', 'p24.4', 'p24.5', 'p24.6', 'p24.7']","['Text embeddings map the given sequence of words into a sequence of vectors. Text embeddings can be char, subword or code-based.', 'Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only. Each character is represented using an embedding. These embeddings are initialized randomly and learned during model pretraining. ELMo embedding model uses CharCNN to generate word representations from character embeddings [74]. Inspired by ELMo, BioCharBERT also uses CharCNN on the top of character embeddings to generate word representations [28]. AlphaBERT [75] also uses character embeddings. Unlike CharacterBERT, AlphaBERT directly combines character embeddings with position embeddings and then applies a stack of transformer encoders. The main advantage with character embeddings is the small size of vocabulary as it includes only characters. The disadvantage is longer pretraining times [28]. As the sequence length increases with character level embeddings, models are slow to pre-train.', 'Subword Embeddings-In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words. The main principle driving the subword embedding vocabulary construction is that frequent words should be represented as a single word and rare words should be represented in terms of meaningful subwords. Subword embedding vocabularies are always moderate in size as they use sub-words to represent rare and misspelled words. Some of the popular algorithms to generate vocabulary for sub-word embeddings are Byte-Pair Encoding (BPE) [76], Byte-Level BPE [77], Word-Piece [30], Unigram [78], and Sentencepiece [79].', 'Byte-Pair Encoding (BPE) [76] -It starts with a base vocabulary having all the unique characters in the training corpus. It augments the base vocabulary with the most frequent pairs until the desired vocabulary size is achieved. Byte-Pair Encoding algorithm can be summarized as 1) Prepare a large training corpus and fix the vocabulary size. 2) Generate a base vocabulary having all the unique characters in the training corpus. 3) Calculate the frequency of all the words in the corpus. 4) Augment the vocabulary with the most frequently occurring pair. 5) Until the desired vocabulary size is achieved, repeat step 4. Byte-Level BPE [77] -In Byte-Level BPE, each character is represented as a byte, and the rest of the procedure is the same as in BPE. Text is converted into a sequence of bytes and the most frequent byte pair is added into the base vocabulary until the desired size is achieved. Byte-Level BPE is extremely beneficial in the multilingual scenario. GPT-2 [77] and RoBERTa [3] use Byte-Level BPE embeddings.', 'WordPiece [30] -The working of WordPiece is almost the same as BPE. Word-Piece and BPE differ in the strategy used in selecting the symbol pair to augment the base vocabulary. BPE chooses the most frequent symbol pair while Word-Piece uses a language model to choose the symbol pair. BERT [2], DistilBERT [80], and ELECTRA model use WordPiece embeddings.', 'SentencePiece [79] -A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space. However, this assumption is not applicable in all languages. To overcome this, SentencePiece treats space as a character and includes it in the base vocabulary. The final vocabulary is generated iteratively using BPE or Unigram. XLNet [81], ALBERT [15], and T5 [4] models use SentencePiece embeddings.', 'Unigram [78] -Unigram is similar to BPE and Word-Piece in fixing the vocabulary size at the beginning itself. BPE and Word-piece start with a small base vocabulary and then augments the base vocabulary for a certain number of iterations. Unlike BPE and Word-Piece, Unigram starts with a large base vocabulary and then iteratively trims the symbols to arrive at a small final vocabulary. It is not used directly in any of the models. SentencePiece uses the Unigram algorithm to generate the final vocabulary.', 'Code embeddings -Code embeddings map the given sequence of codes into a sequence of vectors. For example, in the case of models like BERT-EHR [32], MedBERT [31], and BEHRT [27], the input is not a sequence of words. Instead, input is patient visits. Each patient visit is represented as a sequence of codes. The number of code embeddings varies from model to model. For example, MedBERT and BEHRT include embeddings only for disease codes while BERT-EHR includes embeddings for disease, medication, procedure, and clinical notes.']","Text embeddings map the given sequence of words into a sequence of vectors. Text embeddings can be char, subword or code-based.

Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only. Each character is represented using an embedding. These embeddings are initialized randomly and learned during model pretraining. ELMo embedding model uses CharCNN to generate word representations from character embeddings [74]. Inspired by ELMo, BioCharBERT also uses CharCNN on the top of character embeddings to generate word representations [28]. AlphaBERT [75] also uses character embeddings. Unlike CharacterBERT, AlphaBERT directly combines character embeddings with position embeddings and then applies a stack of transformer encoders. The main advantage with character embeddings is the small size of vocabulary as it includes only characters. The disadvantage is longer pretraining times [28]. As the sequence length increases with character level embeddings, models are slow to pre-train.

Subword Embeddings-In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words. The main principle driving the subword embedding vocabulary construction is that frequent words should be represented as a single word and rare words should be represented in terms of meaningful subwords. Subword embedding vocabularies are always moderate in size as they use sub-words to represent rare and misspelled words. Some of the popular algorithms to generate vocabulary for sub-word embeddings are Byte-Pair Encoding (BPE) [76], Byte-Level BPE [77], Word-Piece [30], Unigram [78], and Sentencepiece [79].

Byte-Pair Encoding (BPE) [76] -It starts with a base vocabulary having all the unique characters in the training corpus. It augments the base vocabulary with the most frequent pairs until the desired vocabulary size is achieved. Byte-Pair Encoding algorithm can be summarized as 1) Prepare a large training corpus and fix the vocabulary size. 2) Generate a base vocabulary having all the unique characters in the training corpus. 3) Calculate the frequency of all the words in the corpus. 4) Augment the vocabulary with the most frequently occurring pair. 5) Until the desired vocabulary size is achieved, repeat step 4. Byte-Level BPE [77] -In Byte-Level BPE, each character is represented as a byte, and the rest of the procedure is the same as in BPE. Text is converted into a sequence of bytes and the most frequent byte pair is added into the base vocabulary until the desired size is achieved. Byte-Level BPE is extremely beneficial in the multilingual scenario. GPT-2 [77] and RoBERTa [3] use Byte-Level BPE embeddings.

WordPiece [30] -The working of WordPiece is almost the same as BPE. Word-Piece and BPE differ in the strategy used in selecting the symbol pair to augment the base vocabulary. BPE chooses the most frequent symbol pair while Word-Piece uses a language model to choose the symbol pair. BERT [2], DistilBERT [80], and ELECTRA model use WordPiece embeddings.

SentencePiece [79] -A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space. However, this assumption is not applicable in all languages. To overcome this, SentencePiece treats space as a character and includes it in the base vocabulary. The final vocabulary is generated iteratively using BPE or Unigram. XLNet [81], ALBERT [15], and T5 [4] models use SentencePiece embeddings.

Unigram [78] -Unigram is similar to BPE and Word-Piece in fixing the vocabulary size at the beginning itself. BPE and Word-piece start with a small base vocabulary and then augments the base vocabulary for a certain number of iterations. Unlike BPE and Word-Piece, Unigram starts with a large base vocabulary and then iteratively trims the symbols to arrive at a small final vocabulary. It is not used directly in any of the models. SentencePiece uses the Unigram algorithm to generate the final vocabulary.

Code embeddings -Code embeddings map the given sequence of codes into a sequence of vectors. For example, in the case of models like BERT-EHR [32], MedBERT [31], and BEHRT [27], the input is not a sequence of words. Instead, input is patient visits. Each patient visit is represented as a sequence of codes. The number of code embeddings varies from model to model. For example, MedBERT and BEHRT include embeddings only for disease codes while BERT-EHR includes embeddings for disease, medication, procedure, and clinical notes.","(p24.0) Text embeddings map the given sequence of words into a sequence of vectors. Text embeddings can be char, subword or code-based.

(p24.1) Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only. Each character is represented using an embedding. These embeddings are initialized randomly and learned during model pretraining. ELMo embedding model uses CharCNN to generate word representations from character embeddings [74]. Inspired by ELMo, BioCharBERT also uses CharCNN on the top of character embeddings to generate word representations [28]. AlphaBERT [75] also uses character embeddings. Unlike CharacterBERT, AlphaBERT directly combines character embeddings with position embeddings and then applies a stack of transformer encoders. The main advantage with character embeddings is the small size of vocabulary as it includes only characters. The disadvantage is longer pretraining times [28]. As the sequence length increases with character level embeddings, models are slow to pre-train.

(p24.2) Subword Embeddings-In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words. The main principle driving the subword embedding vocabulary construction is that frequent words should be represented as a single word and rare words should be represented in terms of meaningful subwords. Subword embedding vocabularies are always moderate in size as they use sub-words to represent rare and misspelled words. Some of the popular algorithms to generate vocabulary for sub-word embeddings are Byte-Pair Encoding (BPE) [76], Byte-Level BPE [77], Word-Piece [30], Unigram [78], and Sentencepiece [79].

(p24.3) Byte-Pair Encoding (BPE) [76] -It starts with a base vocabulary having all the unique characters in the training corpus. It augments the base vocabulary with the most frequent pairs until the desired vocabulary size is achieved. Byte-Pair Encoding algorithm can be summarized as 1) Prepare a large training corpus and fix the vocabulary size. 2) Generate a base vocabulary having all the unique characters in the training corpus. 3) Calculate the frequency of all the words in the corpus. 4) Augment the vocabulary with the most frequently occurring pair. 5) Until the desired vocabulary size is achieved, repeat step 4. Byte-Level BPE [77] -In Byte-Level BPE, each character is represented as a byte, and the rest of the procedure is the same as in BPE. Text is converted into a sequence of bytes and the most frequent byte pair is added into the base vocabulary until the desired size is achieved. Byte-Level BPE is extremely beneficial in the multilingual scenario. GPT-2 [77] and RoBERTa [3] use Byte-Level BPE embeddings.

(p24.4) WordPiece [30] -The working of WordPiece is almost the same as BPE. Word-Piece and BPE differ in the strategy used in selecting the symbol pair to augment the base vocabulary. BPE chooses the most frequent symbol pair while Word-Piece uses a language model to choose the symbol pair. BERT [2], DistilBERT [80], and ELECTRA model use WordPiece embeddings.

(p24.5) SentencePiece [79] -A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space. However, this assumption is not applicable in all languages. To overcome this, SentencePiece treats space as a character and includes it in the base vocabulary. The final vocabulary is generated iteratively using BPE or Unigram. XLNet [81], ALBERT [15], and T5 [4] models use SentencePiece embeddings.

(p24.6) Unigram [78] -Unigram is similar to BPE and Word-Piece in fixing the vocabulary size at the beginning itself. BPE and Word-piece start with a small base vocabulary and then augments the base vocabulary for a certain number of iterations. Unlike BPE and Word-Piece, Unigram starts with a large base vocabulary and then iteratively trims the symbols to arrive at a small final vocabulary. It is not used directly in any of the models. SentencePiece uses the Unigram algorithm to generate the final vocabulary.

(p24.7) Code embeddings -Code embeddings map the given sequence of codes into a sequence of vectors. For example, in the case of models like BERT-EHR [32], MedBERT [31], and BEHRT [27], the input is not a sequence of words. Instead, input is patient visits. Each patient visit is represented as a sequence of codes. The number of code embeddings varies from model to model. For example, MedBERT and BEHRT include embeddings only for disease codes while BERT-EHR includes embeddings for disease, medication, procedure, and clinical notes.","[[], ['b74', 'b75', 'b27'], ['b78', 'b77', 'b29', 'b79', 'b76'], ['b77', 'b76', 'b2'], ['b80', 'b1', 'b29'], ['b79', 'b14', 'b81'], ['b78'], ['b30', 'b26', 'b31']]","[[], ['b74', 'b75', 'b27'], ['b78', 'b77', 'b29', 'b79', 'b76'], ['b77', 'b76', 'b2'], ['b80', 'b1', 'b29'], ['b79', 'b14', 'b81'], ['b78'], ['b30', 'b26', 'b31']]",21,"1. Text embeddings map the given sequence of words into a sequence of vectors.
2. Text embeddings can be char, subword or code-based.
3. Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only.
4. Each character is represented using an embedding.
5. These embeddings are initialized randomly and learned during model pretraining.
6. ELMo embedding model uses CharCNN to generate word representations from character embeddings [74].
7. Inspired by ELMo, BioCharBERT also uses CharCNN on the top of character embeddings to generate word representations [28]. AlphaBERT [75] also uses character embeddings.
8. Unlike CharacterBERT, AlphaBERT directly combines character embeddings with position embeddings and then applies a stack of transformer encoders.
9. The main advantage with character embeddings is the small size of vocabulary as it includes only characters.
10. The disadvantage is longer pretraining times [28].
11. As the sequence length increases with character level embeddings, models are slow to pre-train.
12. Subword Embeddings-In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words.
13. The main principle driving the subword embedding vocabulary construction is that frequent words should be represented as a single word and rare words should be represented in terms of meaningful subwords.
14. Subword embedding vocabularies are always moderate in size as they use sub-words to represent rare and misspelled words.
15. Some of the popular algorithms to generate vocabulary for sub-word embeddings are Byte-Pair Encoding (BPE) [76], Byte-Level BPE [77], Word-Piece [30], Unigram [75]2, and Sentencepiece [79].
16. Byte-Pair Encoding (BPE) [76] -It starts with a base vocabulary having all the unique characters in the training corpus.
17. It augments the base vocabulary with the most frequent pairs until the desired vocabulary size is achieved.
18. Byte-Pair Encoding algorithm can be summarized as 1)
19. Prepare a large training corpus and fix the vocabulary size.
20. 2) Generate a base vocabulary having all the unique characters in the training corpus.
21. 3) Calculate the frequency of all the words in the corpus.
22. 4) Augment the vocabulary with the most frequently occurring pair.
23. 5) Until the desired vocabulary size is achieved, repeat step 4.
24. Byte-Level BPE [77] -In Byte-Level BPE, each character is represented as a byte, and the rest of the procedure is the same as in BPE.
25. Text is converted into a sequence of bytes and the most frequent byte pair is added into the base vocabulary until the desired size is achieved.
26. Byte-Level BPE is extremely beneficial in the multilingual scenario.
27. GPT-2 [77] and RoBERTa [3] use Byte-Level BPE embeddings.
28. WordPiece [30] -The working of WordPiece is almost the same as BPE.
29. Word-Piece and BPE differ in the strategy used in selecting the symbol pair to augment the base vocabulary.
30. BPE chooses the most frequent symbol pair while Word-Piece uses a language model to choose the symbol pair.
31. BERT [2], DistilBERT [80], and ELECTRA model use WordPiece embeddings.
32. SentencePiece [79] -A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space.
33. However, this assumption is not applicable in all languages.
34. To overcome this, SentencePiece treats space as a character and includes it in the base vocabulary.
35. The final vocabulary is generated iteratively using BPE or Unigram.
36. XLNet [81], ALBERT [75]0, and T5 [75]1 models use SentencePiece embeddings.
37. Unigram [75]2 -Unigram is similar to BPE and Word-Piece in fixing the vocabulary size at the beginning itself.
38. BPE and Word-piece start with a small base vocabulary and then augments the base vocabulary for a certain number of iterations.
39. Unlike BPE and Word-Piece, Unigram starts with a large base vocabulary and then iteratively trims the symbols to arrive at a small final vocabulary.
40. It is not used directly in any of the models.
41. SentencePiece uses the Unigram algorithm to generate the final vocabulary.
42. Code embeddings -Code embeddings map the given sequence of codes into a sequence of vectors.
43. For example, in the case of models like BERT-EHR [75]3, MedBERT [75]4, and BEHRT [75]5, the input is not a sequence of words.
44. Instead, input is patient visits.
45. Each patient visit is represented as a sequence of codes.
46. The number of code embeddings varies from model to model.
47. For example, MedBERT and BEHRT include embeddings only for disease codes while BERT-EHR includes embeddings for disease, medication, procedure, and clinical notes.","What are the main types of text embeddings used in transformer-based biomedical pretrained language models?
How do character embeddings differ from subword and code embeddings in terms of vocabulary construction and pretraining efficiency?
What is the principle behind constructing subword embedding vocabularies in transformer-based models?
How does Byte-Pair Encoding (BPE) algorithm work in the context of generating subword embeddings for language models?
What are the unique features and applications of code embeddings in biomedical pretrained language models like BERT-EHR and MedBERT?","1. What are the main types of text embeddings used in transformer-based biomedical pretrained language models?
2. How do character embeddings differ from subword and code embeddings in terms of vocabulary construction and pretraining efficiency?
3. What is the principle behind constructing subword embedding vocabularies in transformer-based models?
4. How does Byte-Pair Encoding (BPE) algorithm work in the context of generating subword embeddings for language models?
5. What are the unique features and applications of code embeddings in biomedical pretrained language models like BERT-EHR and MedBERT?",None,,"Questions:

1. What are the main types of text embeddings used in transformer-based biomedical pretrained language models?
2. How do character embeddings differ from subword and code embeddings in terms of vocabulary construction and pretraining efficiency?
3. What is the principle behind constructing subword embedding vocabularies in transformer-based models?
4. How does Byte-Pair Encoding (BPE) algorithm work in the context of generating subword embeddings for language models?
5. What are the unique features and applications of code embeddings in biomedical pretrained language models like BERT-EHR and MedBERT?

Answer:

(p24.0) Text embeddings map the given sequence of words into a sequence of vectors. Text embeddings can be char, subword or code-based.

(p24.1) Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only. Each character is represented using an embedding. These embeddings are initialized randomly and learned during model pretraining. ELMo embedding model uses CharCNN to generate word representations from character embeddings [74]. Inspired by ELMo, BioCharBERT also uses CharCNN on the top of character embeddings to generate word representations [28]. AlphaBERT [75] also uses character embeddings. Unlike CharacterBERT, AlphaBERT directly combines character embeddings with position embeddings and then applies a stack of transformer encoders. The main advantage with character embeddings is the small size of vocabulary as it includes only characters. The disadvantage is longer pretraining times [28]. As the sequence length increases with character level embeddings, models are slow to pre-train.

(p24.2) Subword Embeddings-In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words. The main principle driving the subword embedding vocabulary construction is that frequent words should be represented as a single word and rare words should be represented in terms of meaningful subwords. Subword embedding vocabularies are always moderate in size as they use sub-words to represent rare and misspelled words. Some of the popular algorithms to generate vocabulary for sub-word embeddings are Byte-Pair Encoding (BPE) [76], Byte-Level BPE [77], Word-Piece [30], Unigram [78], and Sentencepiece [79].

(p24.3) Byte-Pair Encoding (BPE) [76] -It starts with a base vocabulary having all the unique characters in the training corpus. It augments the base vocabulary with the most frequent pairs until the desired vocabulary size is achieved. Byte-Pair Encoding algorithm can be summarized as 1) Prepare a large training corpus and fix the vocabulary size. 2) Generate a base vocabulary having all the unique characters in the training corpus. 3) Calculate the frequency of all the words in the corpus. 4) Augment the vocabulary with the most frequently occurring pair. 5) Until the desired vocabulary size is achieved, repeat step 4. Byte-Level BPE [77] -In Byte-Level BPE, each character is represented as a byte, and the rest of the procedure is the same as in BPE. Text is converted into a sequence of bytes and the most frequent byte pair is added into the base vocabulary until the desired size is achieved. Byte-Level BPE is extremely beneficial in the multilingual scenario. GPT-2 [77] and RoBERTa [3] use Byte-Level BPE embeddings.

(p24.4) WordPiece [30] -The working of WordPiece is almost the same as BPE. Word-Piece and BPE differ in the strategy used in selecting the symbol pair to augment the base vocabulary. BPE chooses the most frequent symbol pair while Word-Piece uses a language model to choose the symbol pair. BERT [2], DistilBERT [80], and ELECTRA model use WordPiece embeddings.

(p24.5) SentencePiece [79] -A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space. However, this assumption is not applicable in all languages. To overcome this, SentencePiece treats space as a character and includes it in the base vocabulary. The final vocabulary is generated iteratively using BPE or Unigram. XLNet [81], ALBERT [15], and T5 [4] models use SentencePiece embeddings.

(p24.6) Unigram [78] -Unigram is similar to BPE and Word-Piece in fixing the vocabulary size at the beginning itself. BPE and Word-piece start with a small base vocabulary and then augments the base vocabulary for a certain number of iterations. Unlike BPE and Word-Piece, Unigram starts with a large base vocabulary and then iteratively trims the symbols to arrive at a small final vocabulary. It is not used directly in any of the models. SentencePiece uses the Unigram algorithm to generate the final vocabulary.

(p24.7) Code embeddings -Code embeddings map the given sequence of codes into a sequence of vectors. For example, in the case of models like BERT-EHR [32], MedBERT [31], and BEHRT [27], the input is not a sequence of words. Instead, input is patient visits. Each patient visit is represented as a sequence of codes. The number of code embeddings varies from model to model. For example, MedBERT and BEHRT include embeddings only for disease codes while BERT-EHR includes embeddings for disease, medication, procedure, and clinical notes.","Questions:



Answer:

(p24.0) Text embeddings map the given sequence of words into a sequence of vectors. Text embeddings can be char, subword or code-based.

(p24.1) Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only. Each character is represented using an embedding. These embeddings are initialized randomly and learned during model pretraining. ELMo embedding model uses CharCNN to generate word representations from character embeddings [74]. Inspired by ELMo, BioCharBERT also uses CharCNN on the top of character embeddings to generate word representations [28]. AlphaBERT [75] also uses character embeddings. Unlike CharacterBERT, AlphaBERT directly combines character embeddings with position embeddings and then applies a stack of transformer encoders. The main advantage with character embeddings is the small size of vocabulary as it includes only characters. The disadvantage is longer pretraining times [28]. As the sequence length increases with character level embeddings, models are slow to pre-train.

(p24.2) Subword Embeddings-In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words. The main principle driving the subword embedding vocabulary construction is that frequent words should be represented as a single word and rare words should be represented in terms of meaningful subwords. Subword embedding vocabularies are always moderate in size as they use sub-words to represent rare and misspelled words. Some of the popular algorithms to generate vocabulary for sub-word embeddings are Byte-Pair Encoding (BPE) [76], Byte-Level BPE [77], Word-Piece [30], Unigram [78], and Sentencepiece [79].

(p24.3) Byte-Pair Encoding (BPE) [76] -It starts with a base vocabulary having all the unique characters in the training corpus. It augments the base vocabulary with the most frequent pairs until the desired vocabulary size is achieved. Byte-Pair Encoding algorithm can be summarized as 1) Prepare a large training corpus and fix the vocabulary size. 2) Generate a base vocabulary having all the unique characters in the training corpus. 3) Calculate the frequency of all the words in the corpus. 4) Augment the vocabulary with the most frequently occurring pair. 5) Until the desired vocabulary size is achieved, repeat step 4. Byte-Level BPE [77] -In Byte-Level BPE, each character is represented as a byte, and the rest of the procedure is the same as in BPE. Text is converted into a sequence of bytes and the most frequent byte pair is added into the base vocabulary until the desired size is achieved. Byte-Level BPE is extremely beneficial in the multilingual scenario. GPT-2 [77] and RoBERTa [3] use Byte-Level BPE embeddings.

(p24.4) WordPiece [30] -The working of WordPiece is almost the same as BPE. Word-Piece and BPE differ in the strategy used in selecting the symbol pair to augment the base vocabulary. BPE chooses the most frequent symbol pair while Word-Piece uses a language model to choose the symbol pair. BERT [2], DistilBERT [80], and ELECTRA model use WordPiece embeddings.

(p24.5) SentencePiece [79] -A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space. However, this assumption is not applicable in all languages. To overcome this, SentencePiece treats space as a character and includes it in the base vocabulary. The final vocabulary is generated iteratively using BPE or Unigram. XLNet [81], ALBERT [15], and T5 [4] models use SentencePiece embeddings.

(p24.6) Unigram [78] -Unigram is similar to BPE and Word-Piece in fixing the vocabulary size at the beginning itself. BPE and Word-piece start with a small base vocabulary and then augments the base vocabulary for a certain number of iterations. Unlike BPE and Word-Piece, Unigram starts with a large base vocabulary and then iteratively trims the symbols to arrive at a small final vocabulary. It is not used directly in any of the models. SentencePiece uses the Unigram algorithm to generate the final vocabulary.

(p24.7) Code embeddings -Code embeddings map the given sequence of codes into a sequence of vectors. For example, in the case of models like BERT-EHR [32], MedBERT [31], and BEHRT [27], the input is not a sequence of words. Instead, input is patient visits. Each patient visit is represented as a sequence of codes. The number of code embeddings varies from model to model. For example, MedBERT and BEHRT include embeddings only for disease codes while BERT-EHR includes embeddings for disease, medication, procedure, and clinical notes."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s25,Auxiliary Embeddings,"['p25.0', 'p25.1', 'p25.2', 'p25.3']","['Main embeddings represent the given input sequence in low dimensional space. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings. The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings. Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].', 'Segment Embeddings -Segment embeddings help to Age Embeddings -In models like BEHRT [27] and BERT-EHR [32], age embeddings are used in addition to other embeddings. Age embeddings provide the age of the patient and help the model to leverage temporal information. Age embedding is the same for all the codes in a single patient visit.', 'Gender Embeddings -In models like BEHRT [27] and BERT-EHR [32], gender embeddings are used in addition to other embeddings. Gender embeddings provide the gender information of the patient to the model. Gender embedding is the same for all the codes in all the patient visits.', 'Semantic Group Embeddings -Semantic group embeddings are used in UmlsBERT [46] to explicitly inform the model to learn similar representations for words from the same semantic group i.e., semantic group embedding is same for all the words which fall into the same semantic group. Besides, it also helps to provide better representations for rare words. Figure 14 shows transformer-based BPLMs taxonomy.']","Main embeddings represent the given input sequence in low dimensional space. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings. The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings. Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].

Segment Embeddings -Segment embeddings help to Age Embeddings -In models like BEHRT [27] and BERT-EHR [32], age embeddings are used in addition to other embeddings. Age embeddings provide the age of the patient and help the model to leverage temporal information. Age embedding is the same for all the codes in a single patient visit.

Gender Embeddings -In models like BEHRT [27] and BERT-EHR [32], gender embeddings are used in addition to other embeddings. Gender embeddings provide the gender information of the patient to the model. Gender embedding is the same for all the codes in all the patient visits.

Semantic Group Embeddings -Semantic group embeddings are used in UmlsBERT [46] to explicitly inform the model to learn similar representations for words from the same semantic group i.e., semantic group embedding is same for all the words which fall into the same semantic group. Besides, it also helps to provide better representations for rare words. Figure 14 shows transformer-based BPLMs taxonomy.","(p25.0) Main embeddings represent the given input sequence in low dimensional space. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings. The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings. Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].

(p25.1) Segment Embeddings -Segment embeddings help to Age Embeddings -In models like BEHRT [27] and BERT-EHR [32], age embeddings are used in addition to other embeddings. Age embeddings provide the age of the patient and help the model to leverage temporal information. Age embedding is the same for all the codes in a single patient visit.

(p25.2) Gender Embeddings -In models like BEHRT [27] and BERT-EHR [32], gender embeddings are used in addition to other embeddings. Gender embeddings provide the gender information of the patient to the model. Gender embedding is the same for all the codes in all the patient visits.

(p25.3) Semantic Group Embeddings -Semantic group embeddings are used in UmlsBERT [46] to explicitly inform the model to learn similar representations for words from the same semantic group i.e., semantic group embedding is same for all the words which fall into the same semantic group. Besides, it also helps to provide better representations for rare words. Figure 14 shows transformer-based BPLMs taxonomy.","[['b26', 'b1', 'b31'], ['b26', 'b31'], ['b26', 'b31'], ['b45']]","[['b26', 'b1', 'b31'], ['b26', 'b31'], ['b26', 'b31'], ['b45']]",8,"1. Main embeddings represent the given input sequence in low dimensional space.
2. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better.
3. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings.
4. The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence.
5. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings.
6. Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].
7. Segment Embeddings -Segment embeddings help to Age Embeddings -In models like BEHRT [27] and BERT-EHR [32], age embeddings are used in addition to other embeddings.
8. Age embeddings provide the age of the patient and help the model to leverage temporal information.
9. Age embedding is the same for all the codes in a single patient visit.
10. Gender Embeddings -In models like BEHRT [27] and BERT-EHR [32], gender embeddings are used in addition to other embeddings.
11. Gender embeddings provide the gender information of the patient to the model.
12. Gender embedding is the same for all the codes in all the patient visits.
13. Semantic Group Embeddings -Semantic group embeddings are used in UmlsBERT [46] to explicitly inform the model to learn similar representations for words from the same semantic group i.e., semantic group embedding is same for all the words which fall into the same semantic group.
14. Besides, it also helps to provide better representations for rare words.
15. Figure 14 shows transformer-based BPLMs taxonomy.","What is the role of auxiliary embeddings in enhancing the performance of transformer-based biomedical pretrained language models?
How do position embeddings contribute to the understanding of token order in the absence of convolution or recurrence layers in transformer models?
In what way do age embeddings improve the model's ability to leverage temporal information in patient data?
How do gender embeddings influence the model's interpretation of patient-specific information in biomedical pretrained language models?
What is the significance of semantic group embeddings in improving representations for words within the same semantic group in transformer-based models?","1. What is the role of auxiliary embeddings in enhancing the performance of transformer-based biomedical pretrained language models?
2. How do position embeddings contribute to the understanding of token order in the absence of convolution or recurrence layers in transformer models?
3. In what way do age embeddings improve the model's ability to leverage temporal information in patient data?
4. How do gender embeddings influence the model's interpretation of patient-specific information in biomedical pretrained language models?
5. What is the significance of semantic group embeddings in improving representations for words within the same semantic group in transformer-based models?",None,,"Questions:

1. What is the role of auxiliary embeddings in enhancing the performance of transformer-based biomedical pretrained language models?
2. How do position embeddings contribute to the understanding of token order in the absence of convolution or recurrence layers in transformer models?
3. In what way do age embeddings improve the model's ability to leverage temporal information in patient data?
4. How do gender embeddings influence the model's interpretation of patient-specific information in biomedical pretrained language models?
5. What is the significance of semantic group embeddings in improving representations for words within the same semantic group in transformer-based models?

Answer:

(p25.0) Main embeddings represent the given input sequence in low dimensional space. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings. The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings. Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].

(p25.1) Segment Embeddings -Segment embeddings help to Age Embeddings -In models like BEHRT [27] and BERT-EHR [32], age embeddings are used in addition to other embeddings. Age embeddings provide the age of the patient and help the model to leverage temporal information. Age embedding is the same for all the codes in a single patient visit.

(p25.2) Gender Embeddings -In models like BEHRT [27] and BERT-EHR [32], gender embeddings are used in addition to other embeddings. Gender embeddings provide the gender information of the patient to the model. Gender embedding is the same for all the codes in all the patient visits.

(p25.3) Semantic Group Embeddings -Semantic group embeddings are used in UmlsBERT [46] to explicitly inform the model to learn similar representations for words from the same semantic group i.e., semantic group embedding is same for all the words which fall into the same semantic group. Besides, it also helps to provide better representations for rare words. Figure 14 shows transformer-based BPLMs taxonomy.","Questions:



Answer:

(p25.0) Main embeddings represent the given input sequence in low dimensional space. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings. The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings. Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].

(p25.1) Segment Embeddings -Segment embeddings help to Age Embeddings -In models like BEHRT [27] and BERT-EHR [32], age embeddings are used in addition to other embeddings. Age embeddings provide the age of the patient and help the model to leverage temporal information. Age embedding is the same for all the codes in a single patient visit.

(p25.2) Gender Embeddings -In models like BEHRT [27] and BERT-EHR [32], gender embeddings are used in addition to other embeddings. Gender embeddings provide the gender information of the patient to the model. Gender embedding is the same for all the codes in all the patient visits.

(p25.3) Semantic Group Embeddings -Semantic group embeddings are used in UmlsBERT [46] to explicitly inform the model to learn similar representations for words from the same semantic group i.e., semantic group embedding is same for all the words which fall into the same semantic group. Besides, it also helps to provide better representations for rare words. Figure 14 shows transformer-based BPLMs taxonomy."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s28,Electronic Health Records,['p28.0'],"['In the last decade, most hospitals have been using Electronic Health Records (EHRs) to record patient as well as treatment details right from admission to discharge [82]. EHRs contain a vast amount of medical data which can be used to provide better patient care by knowledge discovery and the development of better algorithms. As EHR contains sensitive information related to patients, medical data must be de-identified before sharing. EHRs include both structured and unstructured data [83], [84]. Structured data includes laboratory test results, various medical codes, etc. Unstructured data include clinical notes like medication instructions, progress notes, discharge summaries, etc. Clinical notes include the most valuable patient information which is difficult and expensive to extract manually. So, there is a need for automatic information extraction methods to utilize the abundant medical data from EHRs in research as well as applications [84]- [86]. Following the success of transformer-based PLMs in the general domain, researchers in biomedical NLP also developed EHR-based T-BPLMs by pretraining over clinical notes or medical codes or both. MIMIC [87], [88] [19] further pre-trained XLNetbase on MIMIC-III Clinical Notes. The authors released two models namely a) pretrained model on nursing notes and b) pretrained model on discharge summaries. Yang et al. [39] further pre-trained general models like BERT, ELECTRA, RoBERTa, XLNet, and ALBERT on MIMIC-III and released in-domain PLMs. It is the first work to release in-domain models based on all the popular transformer-based PLMs. Unlike the above pretrained models which are pretrained on clinical text, recent works [27], [31], [39] released models which are pre-trained on disease codes or multi-modal EHR data. BEHRT [27] is trained from scratch using 1.6 million patient EHR data with MLM as pretraining task. The authors used code, position, age, and segment embeddings. Med-BERT [31] is trained from scratch using 28,490,650 patient EHR data with MLM and LOS (Length of Stay) as pretraining tasks. The authors used code, serialization and visit embeddings. BERT-EHR [32] is trained from scratch using multi-modal data from 43967 patient records with MLM as a pretraining task. Table 2 contains summary of various EHR based BPLMs.']","In the last decade, most hospitals have been using Electronic Health Records (EHRs) to record patient as well as treatment details right from admission to discharge [82]. EHRs contain a vast amount of medical data which can be used to provide better patient care by knowledge discovery and the development of better algorithms. As EHR contains sensitive information related to patients, medical data must be de-identified before sharing. EHRs include both structured and unstructured data [83], [84]. Structured data includes laboratory test results, various medical codes, etc. Unstructured data include clinical notes like medication instructions, progress notes, discharge summaries, etc. Clinical notes include the most valuable patient information which is difficult and expensive to extract manually. So, there is a need for automatic information extraction methods to utilize the abundant medical data from EHRs in research as well as applications [84]- [86]. Following the success of transformer-based PLMs in the general domain, researchers in biomedical NLP also developed EHR-based T-BPLMs by pretraining over clinical notes or medical codes or both. MIMIC [87], [88] [19] further pre-trained XLNetbase on MIMIC-III Clinical Notes. The authors released two models namely a) pretrained model on nursing notes and b) pretrained model on discharge summaries. Yang et al. [39] further pre-trained general models like BERT, ELECTRA, RoBERTa, XLNet, and ALBERT on MIMIC-III and released in-domain PLMs. It is the first work to release in-domain models based on all the popular transformer-based PLMs. Unlike the above pretrained models which are pretrained on clinical text, recent works [27], [31], [39] released models which are pre-trained on disease codes or multi-modal EHR data. BEHRT [27] is trained from scratch using 1.6 million patient EHR data with MLM as pretraining task. The authors used code, position, age, and segment embeddings. Med-BERT [31] is trained from scratch using 28,490,650 patient EHR data with MLM and LOS (Length of Stay) as pretraining tasks. The authors used code, serialization and visit embeddings. BERT-EHR [32] is trained from scratch using multi-modal data from 43967 patient records with MLM as a pretraining task. Table 2 contains summary of various EHR based BPLMs.","(p28.0) In the last decade, most hospitals have been using Electronic Health Records (EHRs) to record patient as well as treatment details right from admission to discharge [82]. EHRs contain a vast amount of medical data which can be used to provide better patient care by knowledge discovery and the development of better algorithms. As EHR contains sensitive information related to patients, medical data must be de-identified before sharing. EHRs include both structured and unstructured data [83], [84]. Structured data includes laboratory test results, various medical codes, etc. Unstructured data include clinical notes like medication instructions, progress notes, discharge summaries, etc. Clinical notes include the most valuable patient information which is difficult and expensive to extract manually. So, there is a need for automatic information extraction methods to utilize the abundant medical data from EHRs in research as well as applications [84]- [86]. Following the success of transformer-based PLMs in the general domain, researchers in biomedical NLP also developed EHR-based T-BPLMs by pretraining over clinical notes or medical codes or both. MIMIC [87], [88] [19] further pre-trained XLNetbase on MIMIC-III Clinical Notes. The authors released two models namely a) pretrained model on nursing notes and b) pretrained model on discharge summaries. Yang et al. [39] further pre-trained general models like BERT, ELECTRA, RoBERTa, XLNet, and ALBERT on MIMIC-III and released in-domain PLMs. It is the first work to release in-domain models based on all the popular transformer-based PLMs. Unlike the above pretrained models which are pretrained on clinical text, recent works [27], [31], [39] released models which are pre-trained on disease codes or multi-modal EHR data. BEHRT [27] is trained from scratch using 1.6 million patient EHR data with MLM as pretraining task. The authors used code, position, age, and segment embeddings. Med-BERT [31] is trained from scratch using 28,490,650 patient EHR data with MLM and LOS (Length of Stay) as pretraining tasks. The authors used code, serialization and visit embeddings. BERT-EHR [32] is trained from scratch using multi-modal data from 43967 patient records with MLM as a pretraining task. Table 2 contains summary of various EHR based BPLMs.","[['b82', 'b84', 'b87', 'b18', 'b38', 'b83', 'b31', 'b30', 'b86', 'b88', 'b26']]","[['b82', 'b84', 'b87', 'b18', 'b38', 'b83', 'b31', 'b30', 'b86', 'b88', 'b26']]",11,"1. In the last decade, most hospitals have been using Electronic Health Records (EHRs) to record patient as well as treatment details right from admission to discharge [82].
2. EHRs contain a vast amount of medical data which can be used to provide better patient care by knowledge discovery and the development of better algorithms.
3. As EHR contains sensitive information related to patients, medical data must be de-identified before sharing.
4. EHRs include both structured and unstructured data [83], [84].
5. Structured data includes laboratory test results, various medical codes, etc. Unstructured data include clinical notes like medication instructions, progress notes, discharge summaries, etc. Clinical notes include the most valuable patient information which is difficult and expensive to extract manually.
6. So, there is a need for automatic information extraction methods to utilize the abundant medical data from EHRs in research as well as applications [84]- [86].
7. Following the success of transformer-based PLMs in the general domain, researchers in biomedical NLP also developed EHR-based T-BPLMs by pretraining over clinical notes or medical codes or both.
8. MIMIC [87], [88] [19] further pre-trained XLNetbase on MIMIC-III Clinical Notes.
9. The authors released two models namely a) pretrained model on nursing notes and b) pretrained model on discharge summaries.
10. Yang et al. [82]2 further pre-trained general models like BERT, ELECTRA, RoBERTa, XLNet, and ALBERT on MIMIC-III and released in-domain PLMs.
11. It is the first work to release in-domain models based on all the popular transformer-based PLMs.
12. Unlike the above pretrained models which are pretrained on clinical text, recent works [82]3, [82]4, [82]2 released models which are pre-trained on disease codes or multi-modal EHR data.
13. BEHRT [82]3 is trained from scratch using 1.6 million patient EHR data with MLM as pretraining task.
14. The authors used code, position, age, and segment embeddings.
15. Med-BERT [82]4 is trained from scratch using 28,490,650 patient EHR data with MLM and LOS [82]5 as pretraining tasks.
16. The authors used code, serialization and visit embeddings.
17. BERT-EHR [82]6 is trained from scratch using multi-modal data from 43967 patient records with MLM as a pretraining task.
18. Table 2 contains summary of various EHR based BPLMs.","What are the primary types of data contained within Electronic Health Records (EHRs) and how do they differ?
How do transformer-based Pretrained Language Models (PLMs) contribute to the utilization of Electronic Health Records (EHRs) in biomedical research?
What are the challenges associated with extracting valuable patient information from clinical notes in EHRs?
Can you describe the significance of de-identifying medical data in EHRs before it is shared for research purposes?
What methodologies have been developed to pre-train language models on Electronic Health Records, and what unique data do they utilize?","1. What are the primary types of data contained within Electronic Health Records (EHRs) and how do they differ?
2. How do transformer-based Pretrained Language Models (PLMs) contribute to the utilization of Electronic Health Records (EHRs) in biomedical research?
3. What are the challenges associated with extracting valuable patient information from clinical notes in EHRs?
4. Can you describe the significance of de-identifying medical data in EHRs before it is shared for research purposes?
5. What methodologies have been developed to pre-train language models on Electronic Health Records, and what unique data do they utilize?",None,,"Questions:

1. What are the primary types of data contained within Electronic Health Records (EHRs) and how do they differ?
2. How do transformer-based Pretrained Language Models (PLMs) contribute to the utilization of Electronic Health Records (EHRs) in biomedical research?
3. What are the challenges associated with extracting valuable patient information from clinical notes in EHRs?
4. Can you describe the significance of de-identifying medical data in EHRs before it is shared for research purposes?
5. What methodologies have been developed to pre-train language models on Electronic Health Records, and what unique data do they utilize?

Answer:

(p28.0) In the last decade, most hospitals have been using Electronic Health Records (EHRs) to record patient as well as treatment details right from admission to discharge [82]. EHRs contain a vast amount of medical data which can be used to provide better patient care by knowledge discovery and the development of better algorithms. As EHR contains sensitive information related to patients, medical data must be de-identified before sharing. EHRs include both structured and unstructured data [83], [84]. Structured data includes laboratory test results, various medical codes, etc. Unstructured data include clinical notes like medication instructions, progress notes, discharge summaries, etc. Clinical notes include the most valuable patient information which is difficult and expensive to extract manually. So, there is a need for automatic information extraction methods to utilize the abundant medical data from EHRs in research as well as applications [84]- [86]. Following the success of transformer-based PLMs in the general domain, researchers in biomedical NLP also developed EHR-based T-BPLMs by pretraining over clinical notes or medical codes or both. MIMIC [87], [88] [19] further pre-trained XLNetbase on MIMIC-III Clinical Notes. The authors released two models namely a) pretrained model on nursing notes and b) pretrained model on discharge summaries. Yang et al. [39] further pre-trained general models like BERT, ELECTRA, RoBERTa, XLNet, and ALBERT on MIMIC-III and released in-domain PLMs. It is the first work to release in-domain models based on all the popular transformer-based PLMs. Unlike the above pretrained models which are pretrained on clinical text, recent works [27], [31], [39] released models which are pre-trained on disease codes or multi-modal EHR data. BEHRT [27] is trained from scratch using 1.6 million patient EHR data with MLM as pretraining task. The authors used code, position, age, and segment embeddings. Med-BERT [31] is trained from scratch using 28,490,650 patient EHR data with MLM and LOS (Length of Stay) as pretraining tasks. The authors used code, serialization and visit embeddings. BERT-EHR [32] is trained from scratch using multi-modal data from 43967 patient records with MLM as a pretraining task. Table 2 contains summary of various EHR based BPLMs.","Questions:



Answer:

(p28.0) In the last decade, most hospitals have been using Electronic Health Records (EHRs) to record patient as well as treatment details right from admission to discharge [82]. EHRs contain a vast amount of medical data which can be used to provide better patient care by knowledge discovery and the development of better algorithms. As EHR contains sensitive information related to patients, medical data must be de-identified before sharing. EHRs include both structured and unstructured data [83], [84]. Structured data includes laboratory test results, various medical codes, etc. Unstructured data include clinical notes like medication instructions, progress notes, discharge summaries, etc. Clinical notes include the most valuable patient information which is difficult and expensive to extract manually. So, there is a need for automatic information extraction methods to utilize the abundant medical data from EHRs in research as well as applications [84]- [86]. Following the success of transformer-based PLMs in the general domain, researchers in biomedical NLP also developed EHR-based T-BPLMs by pretraining over clinical notes or medical codes or both. MIMIC [87], [88] [19] further pre-trained XLNetbase on MIMIC-III Clinical Notes. The authors released two models namely a) pretrained model on nursing notes and b) pretrained model on discharge summaries. Yang et al. [39] further pre-trained general models like BERT, ELECTRA, RoBERTa, XLNet, and ALBERT on MIMIC-III and released in-domain PLMs. It is the first work to release in-domain models based on all the popular transformer-based PLMs. Unlike the above pretrained models which are pretrained on clinical text, recent works [27], [31], [39] released models which are pre-trained on disease codes or multi-modal EHR data. BEHRT [27] is trained from scratch using 1.6 million patient EHR data with MLM as pretraining task. The authors used code, position, age, and segment embeddings. Med-BERT [31] is trained from scratch using 28,490,650 patient EHR data with MLM and LOS (Length of Stay) as pretraining tasks. The authors used code, serialization and visit embeddings. BERT-EHR [32] is trained from scratch using multi-modal data from 43967 patient records with MLM as a pretraining task. Table 2 contains summary of various EHR based BPLMs."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s30,Social Media,['p30.0'],"['In the last decade, social media has become the first choice for internet users to express their thoughts. Apart from views about general topics, various social media platforms like Twitter, Reddit, AskAPatient, WebMD are used to share health-related experiences in the form of tweets, reviews, questions, and answers [107], [108]. Recent works have shown that health-related social media data is useful in many applications to provide better health-related services [109], [110]. The performance of general T-PLMs on Wikipedia and Books Corpus is limited on health-related social media datasets [92]. This is because social media text is highly informal with a lot of nonstandard abbreviations, irregular grammar, and typos. Researchers working at the intersection of social media and health, trained social media text-based T-BPLMs to handle social media texts. CT-BERT [92] is initialized from BERT-large and further pretrained on a corpus of 22.5M covid related tweets and this model showed up to 30% improvement compared to BERT-large, on five different classification datasets. BioRedditBERT [94] is initialized from BioBERT and further pretrained on healthrelated Reddit posts. The authors showed that BioRed-ditBERT outperforms in-domains models like BioBERT, BlueBERT, PubMedBERT, ClinicalBERT by up to 1.8% in normalizing health-related entity mentions. RuDR-BERT [95] is initialized from Multilingual BERT and pretrained on the raw part of the RuDReC corpus (1.4M reviews). The authors showed that RuDR-BERT outperforms multilingual BERT and Russian BERT on Russian sentence classification and clinical entity extraction datasets by large margins. EnRuDR-BERT [95] and EnDR-BERT [95] are obtained by further pretraining multilingual BERT on Russian and English health reviews and English health reviews respectively. Table 4 contains summary of social media text-based BPLMs.']","In the last decade, social media has become the first choice for internet users to express their thoughts. Apart from views about general topics, various social media platforms like Twitter, Reddit, AskAPatient, WebMD are used to share health-related experiences in the form of tweets, reviews, questions, and answers [107], [108]. Recent works have shown that health-related social media data is useful in many applications to provide better health-related services [109], [110]. The performance of general T-PLMs on Wikipedia and Books Corpus is limited on health-related social media datasets [92]. This is because social media text is highly informal with a lot of nonstandard abbreviations, irregular grammar, and typos. Researchers working at the intersection of social media and health, trained social media text-based T-BPLMs to handle social media texts. CT-BERT [92] is initialized from BERT-large and further pretrained on a corpus of 22.5M covid related tweets and this model showed up to 30% improvement compared to BERT-large, on five different classification datasets. BioRedditBERT [94] is initialized from BioBERT and further pretrained on healthrelated Reddit posts. The authors showed that BioRed-ditBERT outperforms in-domains models like BioBERT, BlueBERT, PubMedBERT, ClinicalBERT by up to 1.8% in normalizing health-related entity mentions. RuDR-BERT [95] is initialized from Multilingual BERT and pretrained on the raw part of the RuDReC corpus (1.4M reviews). The authors showed that RuDR-BERT outperforms multilingual BERT and Russian BERT on Russian sentence classification and clinical entity extraction datasets by large margins. EnRuDR-BERT [95] and EnDR-BERT [95] are obtained by further pretraining multilingual BERT on Russian and English health reviews and English health reviews respectively. Table 4 contains summary of social media text-based BPLMs.","(p30.0) In the last decade, social media has become the first choice for internet users to express their thoughts. Apart from views about general topics, various social media platforms like Twitter, Reddit, AskAPatient, WebMD are used to share health-related experiences in the form of tweets, reviews, questions, and answers [107], [108]. Recent works have shown that health-related social media data is useful in many applications to provide better health-related services [109], [110]. The performance of general T-PLMs on Wikipedia and Books Corpus is limited on health-related social media datasets [92]. This is because social media text is highly informal with a lot of nonstandard abbreviations, irregular grammar, and typos. Researchers working at the intersection of social media and health, trained social media text-based T-BPLMs to handle social media texts. CT-BERT [92] is initialized from BERT-large and further pretrained on a corpus of 22.5M covid related tweets and this model showed up to 30% improvement compared to BERT-large, on five different classification datasets. BioRedditBERT [94] is initialized from BioBERT and further pretrained on healthrelated Reddit posts. The authors showed that BioRed-ditBERT outperforms in-domains models like BioBERT, BlueBERT, PubMedBERT, ClinicalBERT by up to 1.8% in normalizing health-related entity mentions. RuDR-BERT [95] is initialized from Multilingual BERT and pretrained on the raw part of the RuDReC corpus (1.4M reviews). The authors showed that RuDR-BERT outperforms multilingual BERT and Russian BERT on Russian sentence classification and clinical entity extraction datasets by large margins. EnRuDR-BERT [95] and EnDR-BERT [95] are obtained by further pretraining multilingual BERT on Russian and English health reviews and English health reviews respectively. Table 4 contains summary of social media text-based BPLMs.","[['b110', 'b109', 'b108', 'b94', 'b107', 'b95', 'b92']]","[['b110', 'b109', 'b108', 'b94', 'b107', 'b95', 'b92']]",7,"1. In the last decade, social media has become the first choice for internet users to express their thoughts.
2. Apart from views about general topics, various social media platforms like Twitter, Reddit, AskAPatient, WebMD are used to share health-related experiences in the form of tweets, reviews, questions, and answers [107], [108].
3. Recent works have shown that health-related social media data is useful in many applications to provide better health-related services [109], [110].
4. The performance of general T-PLMs on Wikipedia and Books Corpus is limited on health-related social media datasets [92].
5. This is because social media text is highly informal with a lot of nonstandard abbreviations, irregular grammar, and typos.
6. Researchers working at the intersection of social media and health, trained social media text-based T-BPLMs to handle social media texts.
7. CT-BERT [92] is initialized from BERT-large and further pretrained on a corpus of 22.5M covid related tweets and this model showed up to 30% improvement compared to BERT-large, on five different classification datasets.
8. BioRedditBERT [94] is initialized from BioBERT and further pretrained on healthrelated Reddit posts.
9. The authors showed that BioRed-ditBERT outperforms in-domains models like BioBERT, BlueBERT, PubMedBERT, ClinicalBERT by up to 1.8% in normalizing health-related entity mentions.
10. RuDR-BERT [108]0 is initialized from Multilingual BERT and pretrained on the raw part of the RuDReC corpus (1.4M reviews).
11. The authors showed that RuDR-BERT outperforms multilingual BERT and Russian BERT on Russian sentence classification and clinical entity extraction datasets by large margins.
12. EnRuDR-BERT [108]0 and EnDR-BERT [108]0 are obtained by further pretraining multilingual BERT on Russian and English health reviews and English health reviews respectively.
13. Table 4 contains summary of social media text-based BPLMs.","How does the informality and irregularity of social media text impact the performance of general Transformer-based Pretrained Language Models (T-PLMs) on health-related datasets?
What specific improvements does CT-BERT show over BERT-large when pre-trained on COVID-related tweets for classification tasks?
In what ways does BioRedditBERT outperform other in-domain models like BioBERT and PubMedBERT when normalizing health-related entity mentions?
How does RuDR-BERT compare to multilingual BERT and Russian BERT in terms of performance on Russian sentence classification and clinical entity extraction datasets?
What are the benefits of further pretraining multilingual BERT on health-related reviews in different languages, as demonstrated by EnRuDR-BERT and EnDR-BERT?","1. How does the informality and irregularity of social media text impact the performance of general Transformer-based Pretrained Language Models (T-PLMs) on health-related datasets?
2. What specific improvements does CT-BERT show over BERT-large when pre-trained on COVID-related tweets for classification tasks?
3. In what ways does BioRedditBERT outperform other in-domain models like BioBERT and PubMedBERT when normalizing health-related entity mentions?
4. How does RuDR-BERT compare to multilingual BERT and Russian BERT in terms of performance on Russian sentence classification and clinical entity extraction datasets?
5. What are the benefits of further pretraining multilingual BERT on health-related reviews in different languages, as demonstrated by EnRuDR-BERT and EnDR-BERT?",None,,"Questions:

1. How does the informality and irregularity of social media text impact the performance of general Transformer-based Pretrained Language Models (T-PLMs) on health-related datasets?
2. What specific improvements does CT-BERT show over BERT-large when pre-trained on COVID-related tweets for classification tasks?
3. In what ways does BioRedditBERT outperform other in-domain models like BioBERT and PubMedBERT when normalizing health-related entity mentions?
4. How does RuDR-BERT compare to multilingual BERT and Russian BERT in terms of performance on Russian sentence classification and clinical entity extraction datasets?
5. What are the benefits of further pretraining multilingual BERT on health-related reviews in different languages, as demonstrated by EnRuDR-BERT and EnDR-BERT?

Answer:

(p30.0) In the last decade, social media has become the first choice for internet users to express their thoughts. Apart from views about general topics, various social media platforms like Twitter, Reddit, AskAPatient, WebMD are used to share health-related experiences in the form of tweets, reviews, questions, and answers [107], [108]. Recent works have shown that health-related social media data is useful in many applications to provide better health-related services [109], [110]. The performance of general T-PLMs on Wikipedia and Books Corpus is limited on health-related social media datasets [92]. This is because social media text is highly informal with a lot of nonstandard abbreviations, irregular grammar, and typos. Researchers working at the intersection of social media and health, trained social media text-based T-BPLMs to handle social media texts. CT-BERT [92] is initialized from BERT-large and further pretrained on a corpus of 22.5M covid related tweets and this model showed up to 30% improvement compared to BERT-large, on five different classification datasets. BioRedditBERT [94] is initialized from BioBERT and further pretrained on healthrelated Reddit posts. The authors showed that BioRed-ditBERT outperforms in-domains models like BioBERT, BlueBERT, PubMedBERT, ClinicalBERT by up to 1.8% in normalizing health-related entity mentions. RuDR-BERT [95] is initialized from Multilingual BERT and pretrained on the raw part of the RuDReC corpus (1.4M reviews). The authors showed that RuDR-BERT outperforms multilingual BERT and Russian BERT on Russian sentence classification and clinical entity extraction datasets by large margins. EnRuDR-BERT [95] and EnDR-BERT [95] are obtained by further pretraining multilingual BERT on Russian and English health reviews and English health reviews respectively. Table 4 contains summary of social media text-based BPLMs.","Questions:



Answer:

(p30.0) In the last decade, social media has become the first choice for internet users to express their thoughts. Apart from views about general topics, various social media platforms like Twitter, Reddit, AskAPatient, WebMD are used to share health-related experiences in the form of tweets, reviews, questions, and answers [107], [108]. Recent works have shown that health-related social media data is useful in many applications to provide better health-related services [109], [110]. The performance of general T-PLMs on Wikipedia and Books Corpus is limited on health-related social media datasets [92]. This is because social media text is highly informal with a lot of nonstandard abbreviations, irregular grammar, and typos. Researchers working at the intersection of social media and health, trained social media text-based T-BPLMs to handle social media texts. CT-BERT [92] is initialized from BERT-large and further pretrained on a corpus of 22.5M covid related tweets and this model showed up to 30% improvement compared to BERT-large, on five different classification datasets. BioRedditBERT [94] is initialized from BioBERT and further pretrained on healthrelated Reddit posts. The authors showed that BioRed-ditBERT outperforms in-domains models like BioBERT, BlueBERT, PubMedBERT, ClinicalBERT by up to 1.8% in normalizing health-related entity mentions. RuDR-BERT [95] is initialized from Multilingual BERT and pretrained on the raw part of the RuDReC corpus (1.4M reviews). The authors showed that RuDR-BERT outperforms multilingual BERT and Russian BERT on Russian sentence classification and clinical entity extraction datasets by large margins. EnRuDR-BERT [95] and EnDR-BERT [95] are obtained by further pretraining multilingual BERT on Russian and English health reviews and English health reviews respectively. Table 4 contains summary of social media text-based BPLMs."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s31,Scientific Literature,"['p31.0', 'p31.1', 'p31.2']","['In the last few decades, the amount of biomedical literature is growing at a rapid scale. As knowledge discovery from biomedical literature is useful in many applications, biomedical text mining is gaining popularity in the research community [16]. However, biomedical text significantly differs from the general text with a lot of domain-specific words. As a result, the performance of general T-PLMs is limited in many of the tasks. So, biomedical researchers focused on developing in-domain T-PLMs to handle biomedical text. PubMed and PMC are the two popular sources of biomedical text. PubMed con-  tains only biomedical literature citations and abstracts only while PMC contains full-text biomedical articles. As of March 2020, PubMed includes 30M citations and abstracts while PMC contains 7.5M full-text articles. Due to the large collection and broad coverage, these two are the first choice to pretrain T-BPLMs [16], [43], [96].', 'As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text. BioBERT [16] is the first biomedical pre-trained language model which is obtained by further pretraining general BERT on biomedical literature. BioBERTpt-bio [97] is obtained by further pretraining BERT Multilingual (base) on Brazilian biomedical corpus -scientific papers from PubMed (0.8M-only literature titles) + Scielo (health -12.4M + biological-3.2M: both titles and abstracts). BioMedBERT [100] is obtained by further pretraining BERT-large on BREATHE 1.0 corpus. BioMedBERT outperformed BioBERT on biomedical question answering.', 'The key reason for the better performance of BioMed-BERT is the diversity of biomedical text in the BREATHE corpus. The main drawback in developing biomedical models by further pretraining general models is the general vocabulary. To overcome this, researchers started to develop biomedical models by using DSPT. Microsoft researchers developed PubMed [20] model by DSPT with in-domain vocabulary and whole word masking strategy. OuBioBERT [21] is trained from scratch using focused PubMed abstracts (280M words) as core corpora and PubMed abstracts (2800M words) as satellite corpora. It outperforms BioBERT and BlueBERT in many of the tasks in the BLUE benchmark. Table 5 contains summary of scientific literature-based T-BPLMs.']","In the last few decades, the amount of biomedical literature is growing at a rapid scale. As knowledge discovery from biomedical literature is useful in many applications, biomedical text mining is gaining popularity in the research community [16]. However, biomedical text significantly differs from the general text with a lot of domain-specific words. As a result, the performance of general T-PLMs is limited in many of the tasks. So, biomedical researchers focused on developing in-domain T-PLMs to handle biomedical text. PubMed and PMC are the two popular sources of biomedical text. PubMed con-  tains only biomedical literature citations and abstracts only while PMC contains full-text biomedical articles. As of March 2020, PubMed includes 30M citations and abstracts while PMC contains 7.5M full-text articles. Due to the large collection and broad coverage, these two are the first choice to pretrain T-BPLMs [16], [43], [96].

As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text. BioBERT [16] is the first biomedical pre-trained language model which is obtained by further pretraining general BERT on biomedical literature. BioBERTpt-bio [97] is obtained by further pretraining BERT Multilingual (base) on Brazilian biomedical corpus -scientific papers from PubMed (0.8M-only literature titles) + Scielo (health -12.4M + biological-3.2M: both titles and abstracts). BioMedBERT [100] is obtained by further pretraining BERT-large on BREATHE 1.0 corpus. BioMedBERT outperformed BioBERT on biomedical question answering.

The key reason for the better performance of BioMed-BERT is the diversity of biomedical text in the BREATHE corpus. The main drawback in developing biomedical models by further pretraining general models is the general vocabulary. To overcome this, researchers started to develop biomedical models by using DSPT. Microsoft researchers developed PubMed [20] model by DSPT with in-domain vocabulary and whole word masking strategy. OuBioBERT [21] is trained from scratch using focused PubMed abstracts (280M words) as core corpora and PubMed abstracts (2800M words) as satellite corpora. It outperforms BioBERT and BlueBERT in many of the tasks in the BLUE benchmark. Table 5 contains summary of scientific literature-based T-BPLMs.","(p31.0) In the last few decades, the amount of biomedical literature is growing at a rapid scale. As knowledge discovery from biomedical literature is useful in many applications, biomedical text mining is gaining popularity in the research community [16]. However, biomedical text significantly differs from the general text with a lot of domain-specific words. As a result, the performance of general T-PLMs is limited in many of the tasks. So, biomedical researchers focused on developing in-domain T-PLMs to handle biomedical text. PubMed and PMC are the two popular sources of biomedical text. PubMed con-  tains only biomedical literature citations and abstracts only while PMC contains full-text biomedical articles. As of March 2020, PubMed includes 30M citations and abstracts while PMC contains 7.5M full-text articles. Due to the large collection and broad coverage, these two are the first choice to pretrain T-BPLMs [16], [43], [96].

(p31.1) As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text. BioBERT [16] is the first biomedical pre-trained language model which is obtained by further pretraining general BERT on biomedical literature. BioBERTpt-bio [97] is obtained by further pretraining BERT Multilingual (base) on Brazilian biomedical corpus -scientific papers from PubMed (0.8M-only literature titles) + Scielo (health -12.4M + biological-3.2M: both titles and abstracts). BioMedBERT [100] is obtained by further pretraining BERT-large on BREATHE 1.0 corpus. BioMedBERT outperformed BioBERT on biomedical question answering.

(p31.2) The key reason for the better performance of BioMed-BERT is the diversity of biomedical text in the BREATHE corpus. The main drawback in developing biomedical models by further pretraining general models is the general vocabulary. To overcome this, researchers started to develop biomedical models by using DSPT. Microsoft researchers developed PubMed [20] model by DSPT with in-domain vocabulary and whole word masking strategy. OuBioBERT [21] is trained from scratch using focused PubMed abstracts (280M words) as core corpora and PubMed abstracts (2800M words) as satellite corpora. It outperforms BioBERT and BlueBERT in many of the tasks in the BLUE benchmark. Table 5 contains summary of scientific literature-based T-BPLMs.","[['b42', 'b15', 'b96'], ['b15', 'b100', 'b97'], ['b20', 'b19']]","[['b42', 'b15', 'b96'], ['b15', 'b100', 'b97'], ['b20', 'b19']]",8,"1. In the last few decades, the amount of biomedical literature is growing at a rapid scale.
2. As knowledge discovery from biomedical literature is useful in many applications, biomedical text mining is gaining popularity in the research community [16].
3. However, biomedical text significantly differs from the general text with a lot of domain-specific words.
4. As a result, the performance of general T-PLMs is limited in many of the tasks.
5. So, biomedical researchers focused on developing in-domain T-PLMs to handle biomedical text.
6. PubMed and PMC are the two popular sources of biomedical text.
7. PubMed con-  tains only biomedical literature citations and abstracts only while PMC contains full-text biomedical articles.
8. As of March 2020, PubMed includes 30M citations and abstracts while PMC contains 7.5M full-text articles.
9. Due to the large collection and broad coverage, these two are the first choice to pretrain T-BPLMs [16], [43], [96].
10. As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text.
11. BioBERT [16] is the first biomedical pre-trained language model which is obtained by further pretraining general BERT on biomedical literature.
12. BioBERTpt-bio [97] is obtained by further pretraining BERT Multilingual (base) on Brazilian biomedical corpus -scientific papers from PubMed (0.8M-only literature titles) + Scielo (health -12.4M + biological-3.2M: both titles and abstracts).
13. BioMedBERT [100] is obtained by further pretraining BERT-large on BREATHE 1.0 corpus.
14. BioMedBERT outperformed BioBERT on biomedical question answering.
15. The key reason for the better performance of BioMed-BERT is the diversity of biomedical text in the BREATHE corpus.
16. The main drawback in developing biomedical models by further pretraining general models is the general vocabulary.
17. To overcome this, researchers started to develop biomedical models by using DSPT.
18. Microsoft researchers developed PubMed [20] model by DSPT with in-domain vocabulary and whole word masking strategy.
19. OuBioBERT [21] is trained from scratch using focused PubMed abstracts (280M words) as core corpora and PubMed abstracts (2800M words) as satellite corpora.
20. It outperforms BioBERT and BlueBERT in many of the tasks in the BLUE benchmark.
21. Table 5 contains summary of scientific literature-based T-BPLMs.","What are the primary sources of biomedical text used for pretraining Transformer-based Biomedical Pretrained Language Models (T-BPLMs)?
How does the performance of general Transformer-based Pretrained Language Models (T-PLMs) compare when applied to biomedical text mining tasks?
What was the first biomedical pre-trained language model, and how was it developed?
Why does BioMedBERT outperform BioBERT in biomedical question answering tasks?
How do models like OuBioBERT improve upon earlier T-BPLMs in terms of performance on the BLUE benchmark?","1. What are the primary sources of biomedical text used for pretraining Transformer-based Biomedical Pretrained Language Models (T-BPLMs)?
2. How does the performance of general Transformer-based Pretrained Language Models (T-PLMs) compare when applied to biomedical text mining tasks?
3. What was the first biomedical pre-trained language model, and how was it developed?
4. Why does BioMedBERT outperform BioBERT in biomedical question answering tasks?
5. How do models like OuBioBERT improve upon earlier T-BPLMs in terms of performance on the BLUE benchmark?",None,,"Questions:

1. What are the primary sources of biomedical text used for pretraining Transformer-based Biomedical Pretrained Language Models (T-BPLMs)?
2. How does the performance of general Transformer-based Pretrained Language Models (T-PLMs) compare when applied to biomedical text mining tasks?
3. What was the first biomedical pre-trained language model, and how was it developed?
4. Why does BioMedBERT outperform BioBERT in biomedical question answering tasks?
5. How do models like OuBioBERT improve upon earlier T-BPLMs in terms of performance on the BLUE benchmark?

Answer:

(p31.0) In the last few decades, the amount of biomedical literature is growing at a rapid scale. As knowledge discovery from biomedical literature is useful in many applications, biomedical text mining is gaining popularity in the research community [16]. However, biomedical text significantly differs from the general text with a lot of domain-specific words. As a result, the performance of general T-PLMs is limited in many of the tasks. So, biomedical researchers focused on developing in-domain T-PLMs to handle biomedical text. PubMed and PMC are the two popular sources of biomedical text. PubMed con-  tains only biomedical literature citations and abstracts only while PMC contains full-text biomedical articles. As of March 2020, PubMed includes 30M citations and abstracts while PMC contains 7.5M full-text articles. Due to the large collection and broad coverage, these two are the first choice to pretrain T-BPLMs [16], [43], [96].

(p31.1) As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text. BioBERT [16] is the first biomedical pre-trained language model which is obtained by further pretraining general BERT on biomedical literature. BioBERTpt-bio [97] is obtained by further pretraining BERT Multilingual (base) on Brazilian biomedical corpus -scientific papers from PubMed (0.8M-only literature titles) + Scielo (health -12.4M + biological-3.2M: both titles and abstracts). BioMedBERT [100] is obtained by further pretraining BERT-large on BREATHE 1.0 corpus. BioMedBERT outperformed BioBERT on biomedical question answering.

(p31.2) The key reason for the better performance of BioMed-BERT is the diversity of biomedical text in the BREATHE corpus. The main drawback in developing biomedical models by further pretraining general models is the general vocabulary. To overcome this, researchers started to develop biomedical models by using DSPT. Microsoft researchers developed PubMed [20] model by DSPT with in-domain vocabulary and whole word masking strategy. OuBioBERT [21] is trained from scratch using focused PubMed abstracts (280M words) as core corpora and PubMed abstracts (2800M words) as satellite corpora. It outperforms BioBERT and BlueBERT in many of the tasks in the BLUE benchmark. Table 5 contains summary of scientific literature-based T-BPLMs.","Questions:



Answer:

(p31.0) In the last few decades, the amount of biomedical literature is growing at a rapid scale. As knowledge discovery from biomedical literature is useful in many applications, biomedical text mining is gaining popularity in the research community [16]. However, biomedical text significantly differs from the general text with a lot of domain-specific words. As a result, the performance of general T-PLMs is limited in many of the tasks. So, biomedical researchers focused on developing in-domain T-PLMs to handle biomedical text. PubMed and PMC are the two popular sources of biomedical text. PubMed con-  tains only biomedical literature citations and abstracts only while PMC contains full-text biomedical articles. As of March 2020, PubMed includes 30M citations and abstracts while PMC contains 7.5M full-text articles. Due to the large collection and broad coverage, these two are the first choice to pretrain T-BPLMs [16], [43], [96].

(p31.1) As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text. BioBERT [16] is the first biomedical pre-trained language model which is obtained by further pretraining general BERT on biomedical literature. BioBERTpt-bio [97] is obtained by further pretraining BERT Multilingual (base) on Brazilian biomedical corpus -scientific papers from PubMed (0.8M-only literature titles) + Scielo (health -12.4M + biological-3.2M: both titles and abstracts). BioMedBERT [100] is obtained by further pretraining BERT-large on BREATHE 1.0 corpus. BioMedBERT outperformed BioBERT on biomedical question answering.

(p31.2) The key reason for the better performance of BioMed-BERT is the diversity of biomedical text in the BREATHE corpus. The main drawback in developing biomedical models by further pretraining general models is the general vocabulary. To overcome this, researchers started to develop biomedical models by using DSPT. Microsoft researchers developed PubMed [20] model by DSPT with in-domain vocabulary and whole word masking strategy. OuBioBERT [21] is trained from scratch using focused PubMed abstracts (280M words) as core corpora and PubMed abstracts (2800M words) as satellite corpora. It outperforms BioBERT and BlueBERT in many of the tasks in the BLUE benchmark. Table 5 contains summary of scientific literature-based T-BPLMs."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s32,Hybrid Corpora,['p32.0'],"['It is difficult to obtain a large amount of in-domain text in some cases. For example, MIMIC [87], [88] is the largest publicly available dataset of medical records.  The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC). However, to pretrain a transformer-based PLM from scratch, we require large volumes of text. To overcome this, some of the models are pretrained on general + in-domain text [41], [104] or, in-domain + related domain text [18], [28], [43], [97]. For example, BERT (jpCR+jpW) [ Table 6 contains hybrid corpora-based T-BPLMs.']","It is difficult to obtain a large amount of in-domain text in some cases. For example, MIMIC [87], [88] is the largest publicly available dataset of medical records.  The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC). However, to pretrain a transformer-based PLM from scratch, we require large volumes of text. To overcome this, some of the models are pretrained on general + in-domain text [41], [104] or, in-domain + related domain text [18], [28], [43], [97]. For example, BERT (jpCR+jpW) [ Table 6 contains hybrid corpora-based T-BPLMs.","(p32.0) It is difficult to obtain a large amount of in-domain text in some cases. For example, MIMIC [87], [88] is the largest publicly available dataset of medical records.  The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC). However, to pretrain a transformer-based PLM from scratch, we require large volumes of text. To overcome this, some of the models are pretrained on general + in-domain text [41], [104] or, in-domain + related domain text [18], [28], [43], [97]. For example, BERT (jpCR+jpW) [ Table 6 contains hybrid corpora-based T-BPLMs.","[['b17', 'b87', 'b40', 'b97', 'b42', None, 'b88', 'b27', 'b104']]","[['b17', 'b87', 'b40', 'b97', 'b42', None, 'b88', 'b27', 'b104']]",9,"1. It is difficult to obtain a large amount of in-domain text in some cases.
2. For example, MIMIC [87], [88] is the largest publicly available dataset of medical records.
3. The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC).
4. However, to pretrain a transformer-based PLM from scratch, we require large volumes of text.
5. To overcome this, some of the models are pretrained on general + in-domain text [41], [104] or, in-domain + related domain text [18], [28], [43], [97].
6. For example, BERT (jpCR+jpW) [ Table 6 contains hybrid corpora-based T-BPLMs.","What are the challenges in obtaining large volumes of in-domain text for pretraining transformer-based biomedical language models?
How does the size of the MIMIC dataset compare to the general Wikipedia corpus and biomedical scientific literature from PubMed + PMC?
Why is it necessary to have large volumes of text to pretrain a transformer-based pretrained language model (PLM)?
What strategies are employed to overcome the limitation of small in-domain datasets in pretraining transformer-based biomedical language models?
Can you provide examples of hybrid corpora used in pretraining transformer-based biomedical pretrained language models (T-BPLMs)?","1. What are the challenges in obtaining large volumes of in-domain text for pretraining transformer-based biomedical language models?
2. How does the size of the MIMIC dataset compare to the general Wikipedia corpus and biomedical scientific literature from PubMed + PMC?
3. Why is it necessary to have large volumes of text to pretrain a transformer-based pretrained language model (PLM)?
4. What strategies are employed to overcome the limitation of small in-domain datasets in pretraining transformer-based biomedical language models?
5. Can you provide examples of hybrid corpora used in pretraining transformer-based biomedical pretrained language models (T-BPLMs)?",None,,"Questions:

1. What are the challenges in obtaining large volumes of in-domain text for pretraining transformer-based biomedical language models?
2. How does the size of the MIMIC dataset compare to the general Wikipedia corpus and biomedical scientific literature from PubMed + PMC?
3. Why is it necessary to have large volumes of text to pretrain a transformer-based pretrained language model (PLM)?
4. What strategies are employed to overcome the limitation of small in-domain datasets in pretraining transformer-based biomedical language models?
5. Can you provide examples of hybrid corpora used in pretraining transformer-based biomedical pretrained language models (T-BPLMs)?

Answer:

(p32.0) It is difficult to obtain a large amount of in-domain text in some cases. For example, MIMIC [87], [88] is the largest publicly available dataset of medical records.  The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC). However, to pretrain a transformer-based PLM from scratch, we require large volumes of text. To overcome this, some of the models are pretrained on general + in-domain text [41], [104] or, in-domain + related domain text [18], [28], [43], [97]. For example, BERT (jpCR+jpW) [ Table 6 contains hybrid corpora-based T-BPLMs.","Questions:



Answer:

(p32.0) It is difficult to obtain a large amount of in-domain text in some cases. For example, MIMIC [87], [88] is the largest publicly available dataset of medical records.  The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC). However, to pretrain a transformer-based PLM from scratch, we require large volumes of text. To overcome this, some of the models are pretrained on general + in-domain text [41], [104] or, in-domain + related domain text [18], [28], [43], [97]. For example, BERT (jpCR+jpW) [ Table 6 contains hybrid corpora-based T-BPLMs."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s34,Language-Specific,['p34.0'],"['Following the success of BioBERT, ClinicalBERT, Pub-MedBERT in English biomedical tasks, researchers focused on developing T-BPLMs for other languages also by pretraining from scratch [41], [91], [111] or pretraining from Multilingual BERT [95], [97] or pretraining from monolingual BERT [48], [91], [112] models. For example, CHMBERT [112] is the first Chinese medical BERT model which is initialized from the general Chinese BERT model and further pretrained on a huge (185GB) corpus of Chinese medical text gathered from more than 100 hospitals. MC-BERT [48] is also initialized from general Chinese BERT and further pre-trained on hybrid corpora which includes general, biomedical, and medical texts. Table 7 contains a summary of language-specific T-BPLMs.']","Following the success of BioBERT, ClinicalBERT, Pub-MedBERT in English biomedical tasks, researchers focused on developing T-BPLMs for other languages also by pretraining from scratch [41], [91], [111] or pretraining from Multilingual BERT [95], [97] or pretraining from monolingual BERT [48], [91], [112] models. For example, CHMBERT [112] is the first Chinese medical BERT model which is initialized from the general Chinese BERT model and further pretrained on a huge (185GB) corpus of Chinese medical text gathered from more than 100 hospitals. MC-BERT [48] is also initialized from general Chinese BERT and further pre-trained on hybrid corpora which includes general, biomedical, and medical texts. Table 7 contains a summary of language-specific T-BPLMs.","(p34.0) Following the success of BioBERT, ClinicalBERT, Pub-MedBERT in English biomedical tasks, researchers focused on developing T-BPLMs for other languages also by pretraining from scratch [41], [91], [111] or pretraining from Multilingual BERT [95], [97] or pretraining from monolingual BERT [48], [91], [112] models. For example, CHMBERT [112] is the first Chinese medical BERT model which is initialized from the general Chinese BERT model and further pretrained on a huge (185GB) corpus of Chinese medical text gathered from more than 100 hospitals. MC-BERT [48] is also initialized from general Chinese BERT and further pre-trained on hybrid corpora which includes general, biomedical, and medical texts. Table 7 contains a summary of language-specific T-BPLMs.","[['b47', 'b111', 'b40', 'b97', 'b112', 'b95', 'b91']]","[['b47', 'b111', 'b40', 'b97', 'b112', 'b95', 'b91']]",7,"1. Following the success of BioBERT, ClinicalBERT, Pub-MedBERT in English biomedical tasks, researchers focused on developing T-BPLMs for other languages also by pretraining from scratch [41], [91], [111] or pretraining from Multilingual BERT [95], [97] or pretraining from monolingual BERT [48], [91], [112] models.
2. For example, CHMBERT [112] is the first Chinese medical BERT model which is initialized from the general Chinese BERT model and further pretrained on a huge (185GB) corpus of Chinese medical text gathered from more than 100 hospitals.
3. MC-BERT [48] is also initialized from general Chinese BERT and further pre-trained on hybrid corpora which includes general, biomedical, and medical texts.
4. Table 7 contains a summary of language-specific T-BPLMs.","What strategies have researchers employed to develop Transformer-based Biomedical Pretrained Language Models (T-BPLMs) for languages other than English?
How does CHMBERT differ in its initialization and pretraining corpus compared to general Chinese BERT models?
What is the significance of using a large corpus of medical text from over 100 hospitals in the pretraining of CHMBERT?
In what way does MC-BERT's pretraining corpus differ from that of CHMBERT, and how does it impact its application?
Can you describe the role of multilingual and monolingual BERT models in the development of language-specific T-BPLMs in the biomedical field?","1. What strategies have researchers employed to develop Transformer-based Biomedical Pretrained Language Models (T-BPLMs) for languages other than English?
2. How does CHMBERT differ in its initialization and pretraining corpus compared to general Chinese BERT models?
3. What is the significance of using a large corpus of medical text from over 100 hospitals in the pretraining of CHMBERT?
4. In what way does MC-BERT's pretraining corpus differ from that of CHMBERT, and how does it impact its application?
5. Can you describe the role of multilingual and monolingual BERT models in the development of language-specific T-BPLMs in the biomedical field?",None,,"Questions:

1. What strategies have researchers employed to develop Transformer-based Biomedical Pretrained Language Models (T-BPLMs) for languages other than English?
2. How does CHMBERT differ in its initialization and pretraining corpus compared to general Chinese BERT models?
3. What is the significance of using a large corpus of medical text from over 100 hospitals in the pretraining of CHMBERT?
4. In what way does MC-BERT's pretraining corpus differ from that of CHMBERT, and how does it impact its application?
5. Can you describe the role of multilingual and monolingual BERT models in the development of language-specific T-BPLMs in the biomedical field?

Answer:

(p34.0) Following the success of BioBERT, ClinicalBERT, Pub-MedBERT in English biomedical tasks, researchers focused on developing T-BPLMs for other languages also by pretraining from scratch [41], [91], [111] or pretraining from Multilingual BERT [95], [97] or pretraining from monolingual BERT [48], [91], [112] models. For example, CHMBERT [112] is the first Chinese medical BERT model which is initialized from the general Chinese BERT model and further pretrained on a huge (185GB) corpus of Chinese medical text gathered from more than 100 hospitals. MC-BERT [48] is also initialized from general Chinese BERT and further pre-trained on hybrid corpora which includes general, biomedical, and medical texts. Table 7 contains a summary of language-specific T-BPLMs.","Questions:



Answer:

(p34.0) Following the success of BioBERT, ClinicalBERT, Pub-MedBERT in English biomedical tasks, researchers focused on developing T-BPLMs for other languages also by pretraining from scratch [41], [91], [111] or pretraining from Multilingual BERT [95], [97] or pretraining from monolingual BERT [48], [91], [112] models. For example, CHMBERT [112] is the first Chinese medical BERT model which is initialized from the general Chinese BERT model and further pretrained on a huge (185GB) corpus of Chinese medical text gathered from more than 100 hospitals. MC-BERT [48] is also initialized from general Chinese BERT and further pre-trained on hybrid corpora which includes general, biomedical, and medical texts. Table 7 contains a summary of language-specific T-BPLMs."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s35,Ontology Enriched,['p35.0'],"['T-BPLMs like BioBERT, BlueBERT and PubMedBERT have achieved good results in many biomedical NLP tasks. These models acquire domain-specific knowledge by pretraining on large volumes of biomedical text. Recent works [34], [45]- [47] showed that these models acquire only the knowledge available in pretraining corpora and the performance of these models can be   [34], [45]- [47] . Clinical Kb-BERT and Clinical Kb-ALBERT [45] are obtained by further pretraining BioBERT and ALBERT models on MIMIC-III clinical notes and UMLS relation triplets. Here, pretraining involves three loss functions namely MLM, NSP, and triple classification. Triple classification involves identifying whether two concepts are connected by the relation or not and helps to inject UMLS relationship knowledge into the model. Umls-BERT [46] is initialized from ClinicalBERT and further pretrained on MIMIC-III clinical notes using novel multilabel loss-based MLM and NSP. The novel multi-label loss function allows the model to connect all the words under the same CUI. CoderBERT [47] is initialized from BioBERT and further pre-trained on UMLS concepts and relations using multi-similarity loss and knowledge embedding loss. Multi-similarity loss helps to learn close embeddings for entities under the same CUI and Knowledge embedding loss helps to inject relationship knowledge. SapBERT [120] is initialized from PubMedBERT and further pre-trained on UMLS synonyms using multisimilarity loss.  ']","T-BPLMs like BioBERT, BlueBERT and PubMedBERT have achieved good results in many biomedical NLP tasks. These models acquire domain-specific knowledge by pretraining on large volumes of biomedical text. Recent works [34], [45]- [47] showed that these models acquire only the knowledge available in pretraining corpora and the performance of these models can be   [34], [45]- [47] . Clinical Kb-BERT and Clinical Kb-ALBERT [45] are obtained by further pretraining BioBERT and ALBERT models on MIMIC-III clinical notes and UMLS relation triplets. Here, pretraining involves three loss functions namely MLM, NSP, and triple classification. Triple classification involves identifying whether two concepts are connected by the relation or not and helps to inject UMLS relationship knowledge into the model. Umls-BERT [46] is initialized from ClinicalBERT and further pretrained on MIMIC-III clinical notes using novel multilabel loss-based MLM and NSP. The novel multi-label loss function allows the model to connect all the words under the same CUI. CoderBERT [47] is initialized from BioBERT and further pre-trained on UMLS concepts and relations using multi-similarity loss and knowledge embedding loss. Multi-similarity loss helps to learn close embeddings for entities under the same CUI and Knowledge embedding loss helps to inject relationship knowledge. SapBERT [120] is initialized from PubMedBERT and further pre-trained on UMLS synonyms using multisimilarity loss.  ","(p35.0) T-BPLMs like BioBERT, BlueBERT and PubMedBERT have achieved good results in many biomedical NLP tasks. These models acquire domain-specific knowledge by pretraining on large volumes of biomedical text. Recent works [34], [45]- [47] showed that these models acquire only the knowledge available in pretraining corpora and the performance of these models can be   [34], [45]- [47] . Clinical Kb-BERT and Clinical Kb-ALBERT [45] are obtained by further pretraining BioBERT and ALBERT models on MIMIC-III clinical notes and UMLS relation triplets. Here, pretraining involves three loss functions namely MLM, NSP, and triple classification. Triple classification involves identifying whether two concepts are connected by the relation or not and helps to inject UMLS relationship knowledge into the model. Umls-BERT [46] is initialized from ClinicalBERT and further pretrained on MIMIC-III clinical notes using novel multilabel loss-based MLM and NSP. The novel multi-label loss function allows the model to connect all the words under the same CUI. CoderBERT [47] is initialized from BioBERT and further pre-trained on UMLS concepts and relations using multi-similarity loss and knowledge embedding loss. Multi-similarity loss helps to learn close embeddings for entities under the same CUI and Knowledge embedding loss helps to inject relationship knowledge. SapBERT [120] is initialized from PubMedBERT and further pre-trained on UMLS synonyms using multisimilarity loss.  ","[['b46', 'b44', 'b45', 'b33', 'b120']]","[['b46', 'b44', 'b45', 'b33', 'b120']]",5,"1. T-BPLMs like BioBERT, BlueBERT and PubMedBERT have achieved good results in many biomedical NLP tasks.
2. These models acquire domain-specific knowledge by pretraining on large volumes of biomedical text.
3. Recent works [34], [45]- [47] showed that these models acquire only the knowledge available in pretraining corpora and the performance of these models can be   [34], [45]- [47] .
4. Clinical Kb-BERT and Clinical Kb-ALBERT [45] are obtained by further pretraining BioBERT and ALBERT models on MIMIC-III clinical notes and UMLS relation triplets.
5. Here, pretraining involves three loss functions namely MLM, NSP, and triple classification.
6. Triple classification involves identifying whether two concepts are connected by the relation or not and helps to inject UMLS relationship knowledge into the model.
7. Umls-BERT [46] is initialized from ClinicalBERT and further pretrained on MIMIC-III clinical notes using novel multilabel loss-based MLM and NSP.
8. The novel multi-label loss function allows the model to connect all the words under the same CUI.
9. CoderBERT [47] is initialized from BioBERT and further pre-trained on UMLS concepts and relations using multi-similarity loss and knowledge embedding loss.
10. Multi-similarity loss helps to learn close embeddings for entities under the same CUI and Knowledge embedding loss helps to inject relationship knowledge.
11. SapBERT [120] is initialized from PubMedBERT and further pre-trained on UMLS synonyms using multisimilarity loss.","What are the primary methods used to enhance the domain-specific knowledge of Transformer-based Biomedical Pretrained Language Models (T-BPLMs) like BioBERT and PubMedBERT?
How does the triple classification loss function contribute to the performance of models like Clinical Kb-BERT and Clinical Kb-ALBERT in understanding biomedical relationships?
In what way does the novel multilabel loss-based MLM and NSP pretraining approach of Umls-BERT differ from traditional methods, and how does it benefit the model's understanding of clinical notes?
How does multi-similarity loss function in models like CoderBERT and SapBERT improve the representation of biomedical entities?
What role does knowledge embedding loss play in enhancing the relationship knowledge of T-BPLMs, and which models utilize this approach?","1. What are the primary methods used to enhance the domain-specific knowledge of Transformer-based Biomedical Pretrained Language Models (T-BPLMs) like BioBERT and PubMedBERT?
2. How does the triple classification loss function contribute to the performance of models like Clinical Kb-BERT and Clinical Kb-ALBERT in understanding biomedical relationships?
3. In what way does the novel multilabel loss-based MLM and NSP pretraining approach of Umls-BERT differ from traditional methods, and how does it benefit the model's understanding of clinical notes?
4. How does multi-similarity loss function in models like CoderBERT and SapBERT improve the representation of biomedical entities?
5. What role does knowledge embedding loss play in enhancing the relationship knowledge of T-BPLMs, and which models utilize this approach?",None,,"Questions:

1. What are the primary methods used to enhance the domain-specific knowledge of Transformer-based Biomedical Pretrained Language Models (T-BPLMs) like BioBERT and PubMedBERT?
2. How does the triple classification loss function contribute to the performance of models like Clinical Kb-BERT and Clinical Kb-ALBERT in understanding biomedical relationships?
3. In what way does the novel multilabel loss-based MLM and NSP pretraining approach of Umls-BERT differ from traditional methods, and how does it benefit the model's understanding of clinical notes?
4. How does multi-similarity loss function in models like CoderBERT and SapBERT improve the representation of biomedical entities?
5. What role does knowledge embedding loss play in enhancing the relationship knowledge of T-BPLMs, and which models utilize this approach?

Answer:

(p35.0) T-BPLMs like BioBERT, BlueBERT and PubMedBERT have achieved good results in many biomedical NLP tasks. These models acquire domain-specific knowledge by pretraining on large volumes of biomedical text. Recent works [34], [45]- [47] showed that these models acquire only the knowledge available in pretraining corpora and the performance of these models can be   [34], [45]- [47] . Clinical Kb-BERT and Clinical Kb-ALBERT [45] are obtained by further pretraining BioBERT and ALBERT models on MIMIC-III clinical notes and UMLS relation triplets. Here, pretraining involves three loss functions namely MLM, NSP, and triple classification. Triple classification involves identifying whether two concepts are connected by the relation or not and helps to inject UMLS relationship knowledge into the model. Umls-BERT [46] is initialized from ClinicalBERT and further pretrained on MIMIC-III clinical notes using novel multilabel loss-based MLM and NSP. The novel multi-label loss function allows the model to connect all the words under the same CUI. CoderBERT [47] is initialized from BioBERT and further pre-trained on UMLS concepts and relations using multi-similarity loss and knowledge embedding loss. Multi-similarity loss helps to learn close embeddings for entities under the same CUI and Knowledge embedding loss helps to inject relationship knowledge. SapBERT [120] is initialized from PubMedBERT and further pre-trained on UMLS synonyms using multisimilarity loss.  ","Questions:



Answer:

(p35.0) T-BPLMs like BioBERT, BlueBERT and PubMedBERT have achieved good results in many biomedical NLP tasks. These models acquire domain-specific knowledge by pretraining on large volumes of biomedical text. Recent works [34], [45]- [47] showed that these models acquire only the knowledge available in pretraining corpora and the performance of these models can be   [34], [45]- [47] . Clinical Kb-BERT and Clinical Kb-ALBERT [45] are obtained by further pretraining BioBERT and ALBERT models on MIMIC-III clinical notes and UMLS relation triplets. Here, pretraining involves three loss functions namely MLM, NSP, and triple classification. Triple classification involves identifying whether two concepts are connected by the relation or not and helps to inject UMLS relationship knowledge into the model. Umls-BERT [46] is initialized from ClinicalBERT and further pretrained on MIMIC-III clinical notes using novel multilabel loss-based MLM and NSP. The novel multi-label loss function allows the model to connect all the words under the same CUI. CoderBERT [47] is initialized from BioBERT and further pre-trained on UMLS concepts and relations using multi-similarity loss and knowledge embedding loss. Multi-similarity loss helps to learn close embeddings for entities under the same CUI and Knowledge embedding loss helps to inject relationship knowledge. SapBERT [120] is initialized from PubMedBERT and further pre-trained on UMLS synonyms using multisimilarity loss.  "
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s36,Green Models,"['p36.0', 'p36.1']","['CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords. This kind of representation increases the overall length of the input as well as hinders the model learning. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text. However, both these approaches involve learning the model parameters from scratch which is highly expensive. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].', 'Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123]. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction. This approach is completely inexpensive as it requires only CPU. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive. Table 9 contains summary of Green T-BPLMs.']","CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords. This kind of representation increases the overall length of the input as well as hinders the model learning. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text. However, both these approaches involve learning the model parameters from scratch which is highly expensive. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].

Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123]. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction. This approach is completely inexpensive as it requires only CPU. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive. Table 9 contains summary of Green T-BPLMs.","(p36.0) CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords. This kind of representation increases the overall length of the input as well as hinders the model learning. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text. However, both these approaches involve learning the model parameters from scratch which is highly expensive. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].

(p36.1) Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123]. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction. This approach is completely inexpensive as it requires only CPU. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive. Table 9 contains summary of Green T-BPLMs.","[['b122', 'b123'], ['b122', 'b123']]","[['b122', 'b123'], ['b122', 'b123']]",4,"1. CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus.
2. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords.
3. This kind of representation increases the overall length of the input as well as hinders the model learning.
4. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text.
5. However, both these approaches involve learning the model parameters from scratch which is highly expensive.
6. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].
7. Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123].
8. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach.
9. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors.
10. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors.
11. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge.
12. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction.
13. This approach is completely inexpensive as it requires only CPU.
14. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module.
15. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text.
16. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive.
17. Table 9 contains summary of Green T-BPLMs.","What are the primary challenges associated with adapting general Transformer-based Pretrained Language Models (T-PLMs) for use in the biomedical domain?
How do Domain-Specific Pretraining (DSPT) and Specialized Pretraining (SPT) approaches address the issue of in-domain vocabulary for Transformer-based models, and what are their limitations?
What defines Green Models in the context of Transformer-based Biomedical Pretrained Language Models, and why are they considered environmentally friendly?
How does GreenBioBERT extend general BERT for the biomedical domain, and what makes its approach cost-effective?
In what ways does exBERT differ from traditional BERT models when adapted for the biomedical domain, and how does it maintain cost efficiency?","1. What are the primary challenges associated with adapting general Transformer-based Pretrained Language Models (T-PLMs) for use in the biomedical domain?
2. How do Domain-Specific Pretraining (DSPT) and Specialized Pretraining (SPT) approaches address the issue of in-domain vocabulary for Transformer-based models, and what are their limitations?
3. What defines Green Models in the context of Transformer-based Biomedical Pretrained Language Models, and why are they considered environmentally friendly?
4. How does GreenBioBERT extend general BERT for the biomedical domain, and what makes its approach cost-effective?
5. In what ways does exBERT differ from traditional BERT models when adapted for the biomedical domain, and how does it maintain cost efficiency?",None,,"Questions:

1. What are the primary challenges associated with adapting general Transformer-based Pretrained Language Models (T-PLMs) for use in the biomedical domain?
2. How do Domain-Specific Pretraining (DSPT) and Specialized Pretraining (SPT) approaches address the issue of in-domain vocabulary for Transformer-based models, and what are their limitations?
3. What defines Green Models in the context of Transformer-based Biomedical Pretrained Language Models, and why are they considered environmentally friendly?
4. How does GreenBioBERT extend general BERT for the biomedical domain, and what makes its approach cost-effective?
5. In what ways does exBERT differ from traditional BERT models when adapted for the biomedical domain, and how does it maintain cost efficiency?

Answer:

(p36.0) CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords. This kind of representation increases the overall length of the input as well as hinders the model learning. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text. However, both these approaches involve learning the model parameters from scratch which is highly expensive. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].

(p36.1) Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123]. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction. This approach is completely inexpensive as it requires only CPU. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive. Table 9 contains summary of Green T-BPLMs.","Questions:



Answer:

(p36.0) CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords. This kind of representation increases the overall length of the input as well as hinders the model learning. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text. However, both these approaches involve learning the model parameters from scratch which is highly expensive. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].

(p36.1) Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123]. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction. This approach is completely inexpensive as it requires only CPU. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive. Table 9 contains summary of Green T-BPLMs."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s37,Debiased Models,['p37.0'],"['T-PLMs are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. Zhang et al. [127] further pretrained SciBERT [105] on MIMIC-III clinical notes and showed that the performance of the model is different for different groups. The authors applied adversarial pretraining debiasing to reduce the gender bias in the model. The authors released both the models publicly to encourage further research in debiasing T-BPLMs.']","T-PLMs are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. Zhang et al. [127] further pretrained SciBERT [105] on MIMIC-III clinical notes and showed that the performance of the model is different for different groups. The authors applied adversarial pretraining debiasing to reduce the gender bias in the model. The authors released both the models publicly to encourage further research in debiasing T-BPLMs.","(p37.0) T-PLMs are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. Zhang et al. [127] further pretrained SciBERT [105] on MIMIC-III clinical notes and showed that the performance of the model is different for different groups. The authors applied adversarial pretraining debiasing to reduce the gender bias in the model. The authors released both the models publicly to encourage further research in debiasing T-BPLMs.","[['b127', 'b126', 'b124', 'b105']]","[['b127', 'b126', 'b124', 'b105']]",4,"1. T-PLMs are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others.
2. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126].
3. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group.
4. Zhang et al. [127] further pretrained SciBERT [105] on MIMIC-III clinical notes and showed that the performance of the model is different for different groups.
5. The authors applied adversarial pretraining debiasing to reduce the gender bias in the model.
6. The authors released both the models publicly to encourage further research in debiasing T-BPLMs.","What are the primary causes of bias in Transformer-based Pretrained Language Models (T-PLMs) used in the biomedical field?
How does bias in the datasets influence the decision-making of Transformer-based Pretrained Language Models in biomedical applications?
What strategies have been proposed or implemented to mitigate bias in Transformer-based Biomedical Pretrained Language Models (T-BPLMs)?
How did Zhang et al. approach the reduction of gender bias in SciBERT through adversarial pretraining?
What is the significance of releasing debiased models for the research community focused on Transformer-based Biomedical Pretrained Language Models?","1. What are the primary causes of bias in Transformer-based Pretrained Language Models (T-PLMs) used in the biomedical field?
2. How does bias in the datasets influence the decision-making of Transformer-based Pretrained Language Models in biomedical applications?
3. What strategies have been proposed or implemented to mitigate bias in Transformer-based Biomedical Pretrained Language Models (T-BPLMs)?
4. How did Zhang et al. approach the reduction of gender bias in SciBERT through adversarial pretraining?
5. What is the significance of releasing debiased models for the research community focused on Transformer-based Biomedical Pretrained Language Models?",None,,"Questions:

1. What are the primary causes of bias in Transformer-based Pretrained Language Models (T-PLMs) used in the biomedical field?
2. How does bias in the datasets influence the decision-making of Transformer-based Pretrained Language Models in biomedical applications?
3. What strategies have been proposed or implemented to mitigate bias in Transformer-based Biomedical Pretrained Language Models (T-BPLMs)?
4. How did Zhang et al. approach the reduction of gender bias in SciBERT through adversarial pretraining?
5. What is the significance of releasing debiased models for the research community focused on Transformer-based Biomedical Pretrained Language Models?

Answer:

(p37.0) T-PLMs are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. Zhang et al. [127] further pretrained SciBERT [105] on MIMIC-III clinical notes and showed that the performance of the model is different for different groups. The authors applied adversarial pretraining debiasing to reduce the gender bias in the model. The authors released both the models publicly to encourage further research in debiasing T-BPLMs.","Questions:



Answer:

(p37.0) T-PLMs are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. Zhang et al. [127] further pretrained SciBERT [105] on MIMIC-III clinical notes and showed that the performance of the model is different for different groups. The authors applied adversarial pretraining debiasing to reduce the gender bias in the model. The authors released both the models publicly to encourage further research in debiasing T-BPLMs."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s38,Multi-Modal Models,['p38.0'],"['T-PLMs have achieved success in many of the NLP tasks including in specific domains like Biomedical. Recent research works have focused on developing pretrained models that can handle multi-modal data i.e., video + text [128], [129], image + text [130]- [134] etc. In Biomedical domain, models like BERTHop [134] and Medical-VLBERT [133] have been proposed recently to handle image + text data. BERTHop [134] is a multi-modal T-BPLM developed for Chest X-ray disease diagnosis. Like ViLBERT [131] and LXMERT [132], BERTHop uses separate encoder to encoder image and text inputs. BERTHop uses PixelHop++ [135] to encode image data and BlueBERT as text encoder. Medical-VLBERT is developed for automatic report generation from COVID-19 scans. Unlike BERTHop, Medical-VLBERT [133] uses shared encoder based on VL-BERT [130] to encode image and text data.  ']","T-PLMs have achieved success in many of the NLP tasks including in specific domains like Biomedical. Recent research works have focused on developing pretrained models that can handle multi-modal data i.e., video + text [128], [129], image + text [130]- [134] etc. In Biomedical domain, models like BERTHop [134] and Medical-VLBERT [133] have been proposed recently to handle image + text data. BERTHop [134] is a multi-modal T-BPLM developed for Chest X-ray disease diagnosis. Like ViLBERT [131] and LXMERT [132], BERTHop uses separate encoder to encoder image and text inputs. BERTHop uses PixelHop++ [135] to encode image data and BlueBERT as text encoder. Medical-VLBERT is developed for automatic report generation from COVID-19 scans. Unlike BERTHop, Medical-VLBERT [133] uses shared encoder based on VL-BERT [130] to encode image and text data.  ","(p38.0) T-PLMs have achieved success in many of the NLP tasks including in specific domains like Biomedical. Recent research works have focused on developing pretrained models that can handle multi-modal data i.e., video + text [128], [129], image + text [130]- [134] etc. In Biomedical domain, models like BERTHop [134] and Medical-VLBERT [133] have been proposed recently to handle image + text data. BERTHop [134] is a multi-modal T-BPLM developed for Chest X-ray disease diagnosis. Like ViLBERT [131] and LXMERT [132], BERTHop uses separate encoder to encoder image and text inputs. BERTHop uses PixelHop++ [135] to encode image data and BlueBERT as text encoder. Medical-VLBERT is developed for automatic report generation from COVID-19 scans. Unlike BERTHop, Medical-VLBERT [133] uses shared encoder based on VL-BERT [130] to encode image and text data.  ","[['b133', 'b129', 'b131', 'b134', 'b128', 'b132', 'b130', 'b135']]","[['b133', 'b129', 'b131', 'b134', 'b128', 'b132', 'b130', 'b135']]",8,"1. T-PLMs have achieved success in many of the NLP tasks including in specific domains like Biomedical.
2. Recent research works have focused on developing pretrained models that can handle multi-modal data i.e., video + text [128], [129], image + text [129]1- [134] etc.
3. In Biomedical domain, models like BERTHop [134] and Medical-VLBERT [129]0 have been proposed recently to handle image + text data.
4. BERTHop [134] is a multi-modal T-BPLM developed for Chest X-ray disease diagnosis.
5. Like ViLBERT [131] and LXMERT [132], BERTHop uses separate encoder to encoder image and text inputs.
6. BERTHop uses PixelHopetc.0 [135] to encode image data and BlueBERT as text encoder.
7. Medical-VLBERT is developed for automatic report generation from COVID-19 scans.
8. Unlike BERTHop, Medical-VLBERT [129]0 uses shared encoder based on VL-BERT [129]1 to encode image and text data.","What are the primary applications of multi-modal Transformer-based Biomedical Pretrained Language Models (T-BPLMs) in the biomedical domain?
How does BERTHop encode image and text data for Chest X-ray disease diagnosis?
What distinguishes Medical-VLBERT's approach to automatic report generation from COVID-19 scans from that of BERTHop?
In the context of multi-modal data processing, what are the advantages of using separate encoders for image and text inputs as seen in models like BERTHop?
What is the significance of shared encoder architectures in multi-modal T-BPLMs, as exemplified by Medical-VLBERT?","1. What are the primary applications of multi-modal Transformer-based Biomedical Pretrained Language Models (T-BPLMs) in the biomedical domain?
2. How does BERTHop encode image and text data for Chest X-ray disease diagnosis?
3. What distinguishes Medical-VLBERT's approach to automatic report generation from COVID-19 scans from that of BERTHop?
4. In the context of multi-modal data processing, what are the advantages of using separate encoders for image and text inputs as seen in models like BERTHop?
5. What is the significance of shared encoder architectures in multi-modal T-BPLMs, as exemplified by Medical-VLBERT?",None,,"Questions:

1. What are the primary applications of multi-modal Transformer-based Biomedical Pretrained Language Models (T-BPLMs) in the biomedical domain?
2. How does BERTHop encode image and text data for Chest X-ray disease diagnosis?
3. What distinguishes Medical-VLBERT's approach to automatic report generation from COVID-19 scans from that of BERTHop?
4. In the context of multi-modal data processing, what are the advantages of using separate encoders for image and text inputs as seen in models like BERTHop?
5. What is the significance of shared encoder architectures in multi-modal T-BPLMs, as exemplified by Medical-VLBERT?

Answer:

(p38.0) T-PLMs have achieved success in many of the NLP tasks including in specific domains like Biomedical. Recent research works have focused on developing pretrained models that can handle multi-modal data i.e., video + text [128], [129], image + text [130]- [134] etc. In Biomedical domain, models like BERTHop [134] and Medical-VLBERT [133] have been proposed recently to handle image + text data. BERTHop [134] is a multi-modal T-BPLM developed for Chest X-ray disease diagnosis. Like ViLBERT [131] and LXMERT [132], BERTHop uses separate encoder to encoder image and text inputs. BERTHop uses PixelHop++ [135] to encode image data and BlueBERT as text encoder. Medical-VLBERT is developed for automatic report generation from COVID-19 scans. Unlike BERTHop, Medical-VLBERT [133] uses shared encoder based on VL-BERT [130] to encode image and text data.  ","Questions:



Answer:

(p38.0) T-PLMs have achieved success in many of the NLP tasks including in specific domains like Biomedical. Recent research works have focused on developing pretrained models that can handle multi-modal data i.e., video + text [128], [129], image + text [130]- [134] etc. In Biomedical domain, models like BERTHop [134] and Medical-VLBERT [133] have been proposed recently to handle image + text data. BERTHop [134] is a multi-modal T-BPLM developed for Chest X-ray disease diagnosis. Like ViLBERT [131] and LXMERT [132], BERTHop uses separate encoder to encoder image and text inputs. BERTHop uses PixelHop++ [135] to encode image data and BlueBERT as text encoder. Medical-VLBERT is developed for automatic report generation from COVID-19 scans. Unlike BERTHop, Medical-VLBERT [133] uses shared encoder based on VL-BERT [130] to encode image and text data.  "
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s40,Natural Language Inference,['p40.0'],"['Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics. It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence. Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval [136] in the general domain and medical concept normalization [137], [138], semantic relatedness [139], question answering [66] in the biomedical domain. NLI is framed as a three-way sentence pair classification problem. Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair. MedNLI [61] is the in-domain NLI dataset with around 14k instances generated from MIMIC-III [88] clinical notes. As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al. [140] further pretrained BioBERT [16] on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45%. Cengiz et al. [55] applied an ensemble of two BioBERT models and achieved an accuracy of 84.7%. The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI [59] and MultiNLI [60] and then finetuned them on MedNLI.']","Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics. It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence. Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval [136] in the general domain and medical concept normalization [137], [138], semantic relatedness [139], question answering [66] in the biomedical domain. NLI is framed as a three-way sentence pair classification problem. Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair. MedNLI [61] is the in-domain NLI dataset with around 14k instances generated from MIMIC-III [88] clinical notes. As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al. [140] further pretrained BioBERT [16] on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45%. Cengiz et al. [55] applied an ensemble of two BioBERT models and achieved an accuracy of 84.7%. The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI [59] and MultiNLI [60] and then finetuned them on MedNLI.","(p40.0) Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics. It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence. Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval [136] in the general domain and medical concept normalization [137], [138], semantic relatedness [139], question answering [66] in the biomedical domain. NLI is framed as a three-way sentence pair classification problem. Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair. MedNLI [61] is the in-domain NLI dataset with around 14k instances generated from MIMIC-III [88] clinical notes. As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al. [140] further pretrained BioBERT [16] on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45%. Cengiz et al. [55] applied an ensemble of two BioBERT models and achieved an accuracy of 84.7%. The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI [59] and MultiNLI [60] and then finetuned them on MedNLI.","[['b136', 'b54', 'b59', 'b138', 'b140', 'b15', 'b60', 'b66', 'b88', 'b58', 'b139', 'b137']]","[['b136', 'b54', 'b59', 'b138', 'b140', 'b15', 'b60', 'b66', 'b88', 'b58', 'b139', 'b137']]",12,"1. Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics.
2. It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence.
3. Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval [136] in the general domain and medical concept normalization [137], [138], semantic relatedness [139], question answering [66] in the biomedical domain.
4. NLI is framed as a three-way sentence pair classification problem.
5. Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair.
6. MedNLI [61] is the in-domain NLI dataset with around 14k instances generated from MIMIC-III [88] clinical notes.
7. As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al. [140] further pretrained BioBERT [16] on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45%.
8. Cengiz et al. [136]0 applied an ensemble of two BioBERT models and achieved an accuracy of 84.7%.
9. The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI [136]1 and MultiNLI [136]2 and then finetuned them on MedNLI.","How does training on NLI datasets enhance models' abilities in understanding sentence-level semantics?
What role does NLI play in improving information retrieval and question answering in the biomedical domain?
In what way does the three-way sentence pair classification problem function within NLI tasks?
How did pretraining BioBERT on MIMIC-III clinical notes specifically impact its performance on the MedNLI dataset?
What approaches were taken to achieve higher accuracy rates on the MedNLI dataset by leveraging BioBERT models?","1. How does training on NLI datasets enhance models' abilities in understanding sentence-level semantics?
2. What role does NLI play in improving information retrieval and question answering in the biomedical domain?
3. In what way does the three-way sentence pair classification problem function within NLI tasks?
4. How did pretraining BioBERT on MIMIC-III clinical notes specifically impact its performance on the MedNLI dataset?
5. What approaches were taken to achieve higher accuracy rates on the MedNLI dataset by leveraging BioBERT models?",None,,"Questions:

1. How does training on NLI datasets enhance models' abilities in understanding sentence-level semantics?
2. What role does NLI play in improving information retrieval and question answering in the biomedical domain?
3. In what way does the three-way sentence pair classification problem function within NLI tasks?
4. How did pretraining BioBERT on MIMIC-III clinical notes specifically impact its performance on the MedNLI dataset?
5. What approaches were taken to achieve higher accuracy rates on the MedNLI dataset by leveraging BioBERT models?

Answer:

(p40.0) Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics. It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence. Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval [136] in the general domain and medical concept normalization [137], [138], semantic relatedness [139], question answering [66] in the biomedical domain. NLI is framed as a three-way sentence pair classification problem. Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair. MedNLI [61] is the in-domain NLI dataset with around 14k instances generated from MIMIC-III [88] clinical notes. As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al. [140] further pretrained BioBERT [16] on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45%. Cengiz et al. [55] applied an ensemble of two BioBERT models and achieved an accuracy of 84.7%. The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI [59] and MultiNLI [60] and then finetuned them on MedNLI.","Questions:



Answer:

(p40.0) Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics. It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence. Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval [136] in the general domain and medical concept normalization [137], [138], semantic relatedness [139], question answering [66] in the biomedical domain. NLI is framed as a three-way sentence pair classification problem. Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair. MedNLI [61] is the in-domain NLI dataset with around 14k instances generated from MIMIC-III [88] clinical notes. As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al. [140] further pretrained BioBERT [16] on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45%. Cengiz et al. [55] applied an ensemble of two BioBERT models and achieved an accuracy of 84.7%. The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI [59] and MultiNLI [60] and then finetuned them on MedNLI."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s41,Entity Extraction,"['p41.0', 'p41.1']","['Entity Extraction is the first step in unlocking valuable information in unstructured text data. Entity Extraction is useful in many tasks like entity linking, relation extraction, knowledge graph construction, etc. Extracting clinical entities like drug and adverse drug reactions is useful in pharmacovigilance and biomedical entities like proteins, chemicals and drugs is useful to discover knowledge in scientific literature. Some of the popular entity extraction datasets are I2B2 2010 [141], VAERS [142], CADEC [143], N2C2 2018 [144] , BC4CHEMD [145], B5CDR-Chem [146], JNLPBA [147] and NCBI-Disease [148].', 'Most of the existing work approaches entity extraction as sequence labeling or machine reading comprehension. In case of sequence labelling approach, BERT based models generate contextual representations for each token and then softmax layer [62], [149], [150], BiLSTM+Softmax [150], BiLSTM+CRF [62], [151]- [153] or CRF [53], [62], [151] is applied. Recent works showed adding BiLSTM on the top of the BERT model does not show much difference in performance [151], [152]. This is because transformer encoder layers in BERT based models do the same job of encoding contextual in token representations like BiLSTM. Some of the works experimented with general BERT for extracting clinical and biomedical entities. For example, Portelli et al. [53] showed that SpanBERT+CRF outperformed in-domain BERT models also in extracting clinical entities in social media text. Boudjellal et al. [104] developed ABioNER by further pretraining AraBERT [41] on general Arabic corpus + biomedical Arabic text and showed that ABioNER outperformed both multilingual BERT [2] and AraBERT on Arabic biomedical entity extraction. This shows that further pretraining general AraBERT on small in-domain text corpus improves the performance of the model. As in-domain datasets are comparatively small, some of the recent works [62], [63], [154] initially fine-tuned the models on similar datasets before fine-tuning on small target datasets. This intermediate fine-tuning allows the model to learn more task-specific knowledge which improves the performance of the model on small target datasets. For example, Gao et al. [63] proposed a novel approach entity extraction approach based on intermediate fine-tuning and semi-supervised learning. Here intermediate fine-tuning allows the model to learn more task-specific knowledge while semi-supervised learning allows to leverage task-related unlabelled data by assigning pseudo labels. Recently Sun et al. [62] formulated biomedical entity extraction as question answering and showed that BioBERT+QA outperformed BioBERT+ (Softmax / CRF / BiLSTM-CRF) on six datasets.']","Entity Extraction is the first step in unlocking valuable information in unstructured text data. Entity Extraction is useful in many tasks like entity linking, relation extraction, knowledge graph construction, etc. Extracting clinical entities like drug and adverse drug reactions is useful in pharmacovigilance and biomedical entities like proteins, chemicals and drugs is useful to discover knowledge in scientific literature. Some of the popular entity extraction datasets are I2B2 2010 [141], VAERS [142], CADEC [143], N2C2 2018 [144] , BC4CHEMD [145], B5CDR-Chem [146], JNLPBA [147] and NCBI-Disease [148].

Most of the existing work approaches entity extraction as sequence labeling or machine reading comprehension. In case of sequence labelling approach, BERT based models generate contextual representations for each token and then softmax layer [62], [149], [150], BiLSTM+Softmax [150], BiLSTM+CRF [62], [151]- [153] or CRF [53], [62], [151] is applied. Recent works showed adding BiLSTM on the top of the BERT model does not show much difference in performance [151], [152]. This is because transformer encoder layers in BERT based models do the same job of encoding contextual in token representations like BiLSTM. Some of the works experimented with general BERT for extracting clinical and biomedical entities. For example, Portelli et al. [53] showed that SpanBERT+CRF outperformed in-domain BERT models also in extracting clinical entities in social media text. Boudjellal et al. [104] developed ABioNER by further pretraining AraBERT [41] on general Arabic corpus + biomedical Arabic text and showed that ABioNER outperformed both multilingual BERT [2] and AraBERT on Arabic biomedical entity extraction. This shows that further pretraining general AraBERT on small in-domain text corpus improves the performance of the model. As in-domain datasets are comparatively small, some of the recent works [62], [63], [154] initially fine-tuned the models on similar datasets before fine-tuning on small target datasets. This intermediate fine-tuning allows the model to learn more task-specific knowledge which improves the performance of the model on small target datasets. For example, Gao et al. [63] proposed a novel approach entity extraction approach based on intermediate fine-tuning and semi-supervised learning. Here intermediate fine-tuning allows the model to learn more task-specific knowledge while semi-supervised learning allows to leverage task-related unlabelled data by assigning pseudo labels. Recently Sun et al. [62] formulated biomedical entity extraction as question answering and showed that BioBERT+QA outperformed BioBERT+ (Softmax / CRF / BiLSTM-CRF) on six datasets.","(p41.0) Entity Extraction is the first step in unlocking valuable information in unstructured text data. Entity Extraction is useful in many tasks like entity linking, relation extraction, knowledge graph construction, etc. Extracting clinical entities like drug and adverse drug reactions is useful in pharmacovigilance and biomedical entities like proteins, chemicals and drugs is useful to discover knowledge in scientific literature. Some of the popular entity extraction datasets are I2B2 2010 [141], VAERS [142], CADEC [143], N2C2 2018 [144] , BC4CHEMD [145], B5CDR-Chem [146], JNLPBA [147] and NCBI-Disease [148].

(p41.1) Most of the existing work approaches entity extraction as sequence labeling or machine reading comprehension. In case of sequence labelling approach, BERT based models generate contextual representations for each token and then softmax layer [62], [149], [150], BiLSTM+Softmax [150], BiLSTM+CRF [62], [151]- [153] or CRF [53], [62], [151] is applied. Recent works showed adding BiLSTM on the top of the BERT model does not show much difference in performance [151], [152]. This is because transformer encoder layers in BERT based models do the same job of encoding contextual in token representations like BiLSTM. Some of the works experimented with general BERT for extracting clinical and biomedical entities. For example, Portelli et al. [53] showed that SpanBERT+CRF outperformed in-domain BERT models also in extracting clinical entities in social media text. Boudjellal et al. [104] developed ABioNER by further pretraining AraBERT [41] on general Arabic corpus + biomedical Arabic text and showed that ABioNER outperformed both multilingual BERT [2] and AraBERT on Arabic biomedical entity extraction. This shows that further pretraining general AraBERT on small in-domain text corpus improves the performance of the model. As in-domain datasets are comparatively small, some of the recent works [62], [63], [154] initially fine-tuned the models on similar datasets before fine-tuning on small target datasets. This intermediate fine-tuning allows the model to learn more task-specific knowledge which improves the performance of the model on small target datasets. For example, Gao et al. [63] proposed a novel approach entity extraction approach based on intermediate fine-tuning and semi-supervised learning. Here intermediate fine-tuning allows the model to learn more task-specific knowledge while semi-supervised learning allows to leverage task-related unlabelled data by assigning pseudo labels. Recently Sun et al. [62] formulated biomedical entity extraction as question answering and showed that BioBERT+QA outperformed BioBERT+ (Softmax / CRF / BiLSTM-CRF) on six datasets.","[['b146', 'b147', 'b141', 'b143', 'b145', 'b144', 'b142', 'b148'], ['b151', 'b1', 'b152', 'b62', 'b63', 'b40', 'b150', 'b153', 'b154', 'b52', 'b149', 'b104']]","[['b146', 'b147', 'b141', 'b143', 'b145', 'b144', 'b142', 'b148'], ['b151', 'b1', 'b152', 'b62', 'b63', 'b40', 'b150', 'b153', 'b154', 'b52', 'b149', 'b104']]",20,"1. Entity Extraction is the first step in unlocking valuable information in unstructured text data.
2. Entity Extraction is useful in many tasks like entity linking, relation extraction, knowledge graph construction, etc. Extracting clinical entities like drug and adverse drug reactions is useful in pharmacovigilance and biomedical entities like proteins, chemicals and drugs is useful to discover knowledge in scientific literature.
3. Some of the popular entity extraction datasets are I2B2 2010 [141], VAERS [142], CADEC [143], N2C2 2018 [144] , BC4CHEMD [145], B5CDR-Chem [146], JNLPBA [147] and NCBI-Disease [148].
4. Most of the existing work approaches entity extraction as sequence labeling or machine reading comprehension.
5. In case of sequence labelling approach, BERT based models generate contextual representations for each token and then softmax layer [143]8, [149], [142]1, BiLSTM+Softmax [142]1, BiLSTM+CRF [143]8, [142]8- [142]4 or CRF [143]0, [143]8, [142]8 is applied.
6. Recent works showed adding BiLSTM on the top of the BERT model does not show much difference in performance [142]8, [142]9.
7. This is because transformer encoder layers in BERT based models do the same job of encoding contextual in token representations like BiLSTM.
8. Some of the works experimented with general BERT for extracting clinical and biomedical entities.
9. For example, Portelli et al. [143]0 showed that SpanBERT+CRF outperformed in-domain BERT models also in extracting clinical entities in social media text.
10. Boudjellal et al. [143]1 developed ABioNER by further pretraining AraBERT [143]2 on general Arabic corpus + biomedical Arabic text and showed that ABioNER outperformed both multilingual BERT [143]3 and AraBERT on Arabic biomedical entity extraction.
11. This shows that further pretraining general AraBERT on small in-domain text corpus improves the performance of the model.
12. As in-domain datasets are comparatively small, some of the recent works [143]8, [143]7, [143]6 initially fine-tuned the models on similar datasets before fine-tuning on small target datasets.
13. This intermediate fine-tuning allows the model to learn more task-specific knowledge which improves the performance of the model on small target datasets.
14. For example, Gao et al. [143]7 proposed a novel approach entity extraction approach based on intermediate fine-tuning and semi-supervised learning.
15. Here intermediate fine-tuning allows the model to learn more task-specific knowledge while semi-supervised learning allows to leverage task-related unlabelled data by assigning pseudo labels.
16. Recently Sun et al. [143]8 formulated biomedical entity extraction as question answering and showed that BioBERT+QA outperformed BioBERT+ [143]9 on six datasets.","What are the advantages of using transformer-based models like BERT for entity extraction in biomedical texts?
How does the performance of SpanBERT+CRF compare to in-domain BERT models in extracting clinical entities from social media text?
In what ways does intermediate fine-tuning enhance the performance of models on small target datasets for entity extraction tasks?
How does the approach of formulating biomedical entity extraction as question answering improve model performance on datasets?
What role does semi-supervised learning play in improving entity extraction models by leveraging unlabeled data?","1. What are the advantages of using transformer-based models like BERT for entity extraction in biomedical texts?
2. How does the performance of SpanBERT+CRF compare to in-domain BERT models in extracting clinical entities from social media text?
3. In what ways does intermediate fine-tuning enhance the performance of models on small target datasets for entity extraction tasks?
4. How does the approach of formulating biomedical entity extraction as question answering improve model performance on datasets?
5. What role does semi-supervised learning play in improving entity extraction models by leveraging unlabeled data?",None,,"Questions:

1. What are the advantages of using transformer-based models like BERT for entity extraction in biomedical texts?
2. How does the performance of SpanBERT+CRF compare to in-domain BERT models in extracting clinical entities from social media text?
3. In what ways does intermediate fine-tuning enhance the performance of models on small target datasets for entity extraction tasks?
4. How does the approach of formulating biomedical entity extraction as question answering improve model performance on datasets?
5. What role does semi-supervised learning play in improving entity extraction models by leveraging unlabeled data?

Answer:

(p41.0) Entity Extraction is the first step in unlocking valuable information in unstructured text data. Entity Extraction is useful in many tasks like entity linking, relation extraction, knowledge graph construction, etc. Extracting clinical entities like drug and adverse drug reactions is useful in pharmacovigilance and biomedical entities like proteins, chemicals and drugs is useful to discover knowledge in scientific literature. Some of the popular entity extraction datasets are I2B2 2010 [141], VAERS [142], CADEC [143], N2C2 2018 [144] , BC4CHEMD [145], B5CDR-Chem [146], JNLPBA [147] and NCBI-Disease [148].

(p41.1) Most of the existing work approaches entity extraction as sequence labeling or machine reading comprehension. In case of sequence labelling approach, BERT based models generate contextual representations for each token and then softmax layer [62], [149], [150], BiLSTM+Softmax [150], BiLSTM+CRF [62], [151]- [153] or CRF [53], [62], [151] is applied. Recent works showed adding BiLSTM on the top of the BERT model does not show much difference in performance [151], [152]. This is because transformer encoder layers in BERT based models do the same job of encoding contextual in token representations like BiLSTM. Some of the works experimented with general BERT for extracting clinical and biomedical entities. For example, Portelli et al. [53] showed that SpanBERT+CRF outperformed in-domain BERT models also in extracting clinical entities in social media text. Boudjellal et al. [104] developed ABioNER by further pretraining AraBERT [41] on general Arabic corpus + biomedical Arabic text and showed that ABioNER outperformed both multilingual BERT [2] and AraBERT on Arabic biomedical entity extraction. This shows that further pretraining general AraBERT on small in-domain text corpus improves the performance of the model. As in-domain datasets are comparatively small, some of the recent works [62], [63], [154] initially fine-tuned the models on similar datasets before fine-tuning on small target datasets. This intermediate fine-tuning allows the model to learn more task-specific knowledge which improves the performance of the model on small target datasets. For example, Gao et al. [63] proposed a novel approach entity extraction approach based on intermediate fine-tuning and semi-supervised learning. Here intermediate fine-tuning allows the model to learn more task-specific knowledge while semi-supervised learning allows to leverage task-related unlabelled data by assigning pseudo labels. Recently Sun et al. [62] formulated biomedical entity extraction as question answering and showed that BioBERT+QA outperformed BioBERT+ (Softmax / CRF / BiLSTM-CRF) on six datasets.","Questions:



Answer:

(p41.0) Entity Extraction is the first step in unlocking valuable information in unstructured text data. Entity Extraction is useful in many tasks like entity linking, relation extraction, knowledge graph construction, etc. Extracting clinical entities like drug and adverse drug reactions is useful in pharmacovigilance and biomedical entities like proteins, chemicals and drugs is useful to discover knowledge in scientific literature. Some of the popular entity extraction datasets are I2B2 2010 [141], VAERS [142], CADEC [143], N2C2 2018 [144] , BC4CHEMD [145], B5CDR-Chem [146], JNLPBA [147] and NCBI-Disease [148].

(p41.1) Most of the existing work approaches entity extraction as sequence labeling or machine reading comprehension. In case of sequence labelling approach, BERT based models generate contextual representations for each token and then softmax layer [62], [149], [150], BiLSTM+Softmax [150], BiLSTM+CRF [62], [151]- [153] or CRF [53], [62], [151] is applied. Recent works showed adding BiLSTM on the top of the BERT model does not show much difference in performance [151], [152]. This is because transformer encoder layers in BERT based models do the same job of encoding contextual in token representations like BiLSTM. Some of the works experimented with general BERT for extracting clinical and biomedical entities. For example, Portelli et al. [53] showed that SpanBERT+CRF outperformed in-domain BERT models also in extracting clinical entities in social media text. Boudjellal et al. [104] developed ABioNER by further pretraining AraBERT [41] on general Arabic corpus + biomedical Arabic text and showed that ABioNER outperformed both multilingual BERT [2] and AraBERT on Arabic biomedical entity extraction. This shows that further pretraining general AraBERT on small in-domain text corpus improves the performance of the model. As in-domain datasets are comparatively small, some of the recent works [62], [63], [154] initially fine-tuned the models on similar datasets before fine-tuning on small target datasets. This intermediate fine-tuning allows the model to learn more task-specific knowledge which improves the performance of the model on small target datasets. For example, Gao et al. [63] proposed a novel approach entity extraction approach based on intermediate fine-tuning and semi-supervised learning. Here intermediate fine-tuning allows the model to learn more task-specific knowledge while semi-supervised learning allows to leverage task-related unlabelled data by assigning pseudo labels. Recently Sun et al. [62] formulated biomedical entity extraction as question answering and showed that BioBERT+QA outperformed BioBERT+ (Softmax / CRF / BiLSTM-CRF) on six datasets."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s42,Semantic Textual Similarity,"['p42.0', 'p42.1']","['Semantic Textual Similarity quantifies the degree of semantic similarity between two phrases or sentences. Unlike NLI which classifies the given sentence pair into one of three classes, semantic textual similarity returns a value that indicates the degree of similarity. Both NLI and STS require sentence-level semantics. STS is useful in tasks like concept relatedness [139], medical concept normalization [137], [138] , duplicate text detection [155], question answering [65], [156] and text summarization [157]. Moreover, Reimers et al. [136] showed that training transformer-based PLMs on STS datasets allow the model to learn sentence-level semantics and hence better represent variable-length texts like phrases or sentences. Models like BERT learn the joint representation of given sentence pair and a task-specific sigmoid layer gives the similarity value. BIOSSES [158] and Clinical STS [72] are the commonly used datasets to train and evaluate indomain STS models.', ""Recent works exploited general models for clinical STS [56], [57], [65], [159]. For example, Yang et al. [56] achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model. As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets [56], [57], [65], [71]. Xiong et al. [160] enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE [161] based entity representations. Mutinda et al. [155] achieved a Pearson correlation score of 0.8320 by finetuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets. Mahajan et al. [71] proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset. Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning. The main drawback in the above existing works is giving '[CLS]' vector as sentence pair representation to the sigmoid layer. This is because the '[CLS]' vector contains only partial information. Unlike existing works, Wang et al. [159] applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results.""]","Semantic Textual Similarity quantifies the degree of semantic similarity between two phrases or sentences. Unlike NLI which classifies the given sentence pair into one of three classes, semantic textual similarity returns a value that indicates the degree of similarity. Both NLI and STS require sentence-level semantics. STS is useful in tasks like concept relatedness [139], medical concept normalization [137], [138] , duplicate text detection [155], question answering [65], [156] and text summarization [157]. Moreover, Reimers et al. [136] showed that training transformer-based PLMs on STS datasets allow the model to learn sentence-level semantics and hence better represent variable-length texts like phrases or sentences. Models like BERT learn the joint representation of given sentence pair and a task-specific sigmoid layer gives the similarity value. BIOSSES [158] and Clinical STS [72] are the commonly used datasets to train and evaluate indomain STS models.

Recent works exploited general models for clinical STS [56], [57], [65], [159]. For example, Yang et al. [56] achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model. As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets [56], [57], [65], [71]. Xiong et al. [160] enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE [161] based entity representations. Mutinda et al. [155] achieved a Pearson correlation score of 0.8320 by finetuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets. Mahajan et al. [71] proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset. Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning. The main drawback in the above existing works is giving '[CLS]' vector as sentence pair representation to the sigmoid layer. This is because the '[CLS]' vector contains only partial information. Unlike existing works, Wang et al. [159] applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results.","(p42.0) Semantic Textual Similarity quantifies the degree of semantic similarity between two phrases or sentences. Unlike NLI which classifies the given sentence pair into one of three classes, semantic textual similarity returns a value that indicates the degree of similarity. Both NLI and STS require sentence-level semantics. STS is useful in tasks like concept relatedness [139], medical concept normalization [137], [138] , duplicate text detection [155], question answering [65], [156] and text summarization [157]. Moreover, Reimers et al. [136] showed that training transformer-based PLMs on STS datasets allow the model to learn sentence-level semantics and hence better represent variable-length texts like phrases or sentences. Models like BERT learn the joint representation of given sentence pair and a task-specific sigmoid layer gives the similarity value. BIOSSES [158] and Clinical STS [72] are the commonly used datasets to train and evaluate indomain STS models.

(p42.1) Recent works exploited general models for clinical STS [56], [57], [65], [159]. For example, Yang et al. [56] achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model. As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets [56], [57], [65], [71]. Xiong et al. [160] enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE [161] based entity representations. Mutinda et al. [155] achieved a Pearson correlation score of 0.8320 by finetuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets. Mahajan et al. [71] proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset. Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning. The main drawback in the above existing works is giving '[CLS]' vector as sentence pair representation to the sigmoid layer. This is because the '[CLS]' vector contains only partial information. Unlike existing works, Wang et al. [159] applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results.","[['b136', 'b72', 'b138', 'b156', 'b158', 'b65', 'b155', 'b157', 'b139', 'b137'], ['b159', 'b55', 'b160', 'b65', 'b161', 'b56', 'b155', 'b71']]","[['b136', 'b72', 'b138', 'b156', 'b158', 'b65', 'b155', 'b157', 'b139', 'b137'], ['b159', 'b55', 'b160', 'b65', 'b161', 'b56', 'b155', 'b71']]",18,"1. Semantic Textual Similarity quantifies the degree of semantic similarity between two phrases or sentences.
2. Unlike NLI which classifies the given sentence pair into one of three classes, semantic textual similarity returns a value that indicates the degree of similarity.
3. Both NLI and STS require sentence-level semantics.
4. STS is useful in tasks like concept relatedness [139], medical concept normalization [137], [138] , duplicate text detection [138]1, question answering [137]7, [156] and text summarization [157].
5. Moreover, Reimers et al. [136] showed that training transformer-based PLMs on STS datasets allow the model to learn sentence-level semantics and hence better represent variable-length texts like phrases or sentences.
6. Models like BERT learn the joint representation of given sentence pair and a task-specific sigmoid layer gives the similarity value.
7. BIOSSES [158] and Clinical STS [72] are the commonly used datasets to train and evaluate indomain STS models.
8. Recent works exploited general models for clinical STS [137]5, [137]6, [137]7, [138]5.
9. For example, Yang et al. [137]5 achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model.
10. As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets [137]5, [137]6, [137]7, [138]2.
11. Xiong et al. [137]9 enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE [138]0 based entity representations.
12. Mutinda et al. [138]1 achieved a Pearson correlation score of 0.8320 by finetuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets.
13. Mahajan et al. [138]2 proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset.
14. Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning.
15. The main drawback in the above existing works is giving '[138]4' vector as sentence pair representation to the sigmoid layer.
16. This is because the '[138]4' vector contains only partial information.
17. Unlike existing works, Wang et al. [138]5 applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results.","How does Semantic Textual Similarity (STS) differ from Natural Language Inference (NLI) in evaluating sentence pairs?
What are some applications of Semantic Textual Similarity (STS) in the biomedical field?
How do transformer-based Pretrained Language Models (PLMs) learn sentence-level semantics through STS datasets?
What strategies have been employed to improve the performance of models on clinical STS datasets given their small size?
What is the significance of using hierarchical convolution and pooling methods over the '[CLS]' vector representation in achieving better results for sentence pair similarity tasks?","1. How does Semantic Textual Similarity (STS) differ from Natural Language Inference (NLI) in evaluating sentence pairs?
2. What are some applications of Semantic Textual Similarity (STS) in the biomedical field?
3. How do transformer-based Pretrained Language Models (PLMs) learn sentence-level semantics through STS datasets?
4. What strategies have been employed to improve the performance of models on clinical STS datasets given their small size?
5. What is the significance of using hierarchical convolution and pooling methods over the '[CLS]' vector representation in achieving better results for sentence pair similarity tasks?",None,,"Questions:

1. How does Semantic Textual Similarity (STS) differ from Natural Language Inference (NLI) in evaluating sentence pairs?
2. What are some applications of Semantic Textual Similarity (STS) in the biomedical field?
3. How do transformer-based Pretrained Language Models (PLMs) learn sentence-level semantics through STS datasets?
4. What strategies have been employed to improve the performance of models on clinical STS datasets given their small size?
5. What is the significance of using hierarchical convolution and pooling methods over the '[CLS]' vector representation in achieving better results for sentence pair similarity tasks?

Answer:

(p42.0) Semantic Textual Similarity quantifies the degree of semantic similarity between two phrases or sentences. Unlike NLI which classifies the given sentence pair into one of three classes, semantic textual similarity returns a value that indicates the degree of similarity. Both NLI and STS require sentence-level semantics. STS is useful in tasks like concept relatedness [139], medical concept normalization [137], [138] , duplicate text detection [155], question answering [65], [156] and text summarization [157]. Moreover, Reimers et al. [136] showed that training transformer-based PLMs on STS datasets allow the model to learn sentence-level semantics and hence better represent variable-length texts like phrases or sentences. Models like BERT learn the joint representation of given sentence pair and a task-specific sigmoid layer gives the similarity value. BIOSSES [158] and Clinical STS [72] are the commonly used datasets to train and evaluate indomain STS models.

(p42.1) Recent works exploited general models for clinical STS [56], [57], [65], [159]. For example, Yang et al. [56] achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model. As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets [56], [57], [65], [71]. Xiong et al. [160] enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE [161] based entity representations. Mutinda et al. [155] achieved a Pearson correlation score of 0.8320 by finetuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets. Mahajan et al. [71] proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset. Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning. The main drawback in the above existing works is giving '[CLS]' vector as sentence pair representation to the sigmoid layer. This is because the '[CLS]' vector contains only partial information. Unlike existing works, Wang et al. [159] applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results.","Questions:



Answer:

(p42.0) Semantic Textual Similarity quantifies the degree of semantic similarity between two phrases or sentences. Unlike NLI which classifies the given sentence pair into one of three classes, semantic textual similarity returns a value that indicates the degree of similarity. Both NLI and STS require sentence-level semantics. STS is useful in tasks like concept relatedness [139], medical concept normalization [137], [138] , duplicate text detection [155], question answering [65], [156] and text summarization [157]. Moreover, Reimers et al. [136] showed that training transformer-based PLMs on STS datasets allow the model to learn sentence-level semantics and hence better represent variable-length texts like phrases or sentences. Models like BERT learn the joint representation of given sentence pair and a task-specific sigmoid layer gives the similarity value. BIOSSES [158] and Clinical STS [72] are the commonly used datasets to train and evaluate indomain STS models.

(p42.1) Recent works exploited general models for clinical STS [56], [57], [65], [159]. For example, Yang et al. [56] achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model. As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets [56], [57], [65], [71]. Xiong et al. [160] enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE [161] based entity representations. Mutinda et al. [155] achieved a Pearson correlation score of 0.8320 by finetuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets. Mahajan et al. [71] proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset. Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning. The main drawback in the above existing works is giving '[CLS]' vector as sentence pair representation to the sigmoid layer. This is because the '[CLS]' vector contains only partial information. Unlike existing works, Wang et al. [159] applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s43,Relation Extraction,['p43.0'],"[""Relation extraction is a crucial task in information extraction which identifies the semantic relations between entities in text. Entity extraction followed by relation extraction helps to convert unstructured text into structured data. Extracting relations between entities is useful in many tasks like knowledge graph construction, text summarization, and question answering. Some of relation extraction datasets are I2B2 2012 [162], AIMED [163], ChemProt [164], DDI [165], I2B2 2010 [141] and EU-ADR [166]. Wei et al. [167] achieved the best results on two datasets using MIMIC-BERT [40] +Softmax. Thillaisundaram and Togia [168] applied SciBERT +Softmax to extract relations from biomedical abstracts as part of AGAC track of BioNLP-OST 2019 shared tasks [169]. Liu et al. [170] proposed SciBERT+Softmax for relation extraction in biomedical text. They showed SciBERT+Softmax outperforms BERT+Softmax on three biomedical relation extraction datasets. The main drawback in the above existing works is that they utilize only the partial knowledge from the last layer in the form of '[CLS]' vector. Su et al. [171] added attention on the top of BioBERT to fully utilize the information in the last layer and achieved the best results on three biomedical extraction datasets. The authors generated the final representation by concatenating '[CLS]' vector and weighted sum vector of final hidden state vectors. When compared to applying LSTM on the final hidden state vectors, the attention layer gives better results.""]","Relation extraction is a crucial task in information extraction which identifies the semantic relations between entities in text. Entity extraction followed by relation extraction helps to convert unstructured text into structured data. Extracting relations between entities is useful in many tasks like knowledge graph construction, text summarization, and question answering. Some of relation extraction datasets are I2B2 2012 [162], AIMED [163], ChemProt [164], DDI [165], I2B2 2010 [141] and EU-ADR [166]. Wei et al. [167] achieved the best results on two datasets using MIMIC-BERT [40] +Softmax. Thillaisundaram and Togia [168] applied SciBERT +Softmax to extract relations from biomedical abstracts as part of AGAC track of BioNLP-OST 2019 shared tasks [169]. Liu et al. [170] proposed SciBERT+Softmax for relation extraction in biomedical text. They showed SciBERT+Softmax outperforms BERT+Softmax on three biomedical relation extraction datasets. The main drawback in the above existing works is that they utilize only the partial knowledge from the last layer in the form of '[CLS]' vector. Su et al. [171] added attention on the top of BioBERT to fully utilize the information in the last layer and achieved the best results on three biomedical extraction datasets. The authors generated the final representation by concatenating '[CLS]' vector and weighted sum vector of final hidden state vectors. When compared to applying LSTM on the final hidden state vectors, the attention layer gives better results.","(p43.0) Relation extraction is a crucial task in information extraction which identifies the semantic relations between entities in text. Entity extraction followed by relation extraction helps to convert unstructured text into structured data. Extracting relations between entities is useful in many tasks like knowledge graph construction, text summarization, and question answering. Some of relation extraction datasets are I2B2 2012 [162], AIMED [163], ChemProt [164], DDI [165], I2B2 2010 [141] and EU-ADR [166]. Wei et al. [167] achieved the best results on two datasets using MIMIC-BERT [40] +Softmax. Thillaisundaram and Togia [168] applied SciBERT +Softmax to extract relations from biomedical abstracts as part of AGAC track of BioNLP-OST 2019 shared tasks [169]. Liu et al. [170] proposed SciBERT+Softmax for relation extraction in biomedical text. They showed SciBERT+Softmax outperforms BERT+Softmax on three biomedical relation extraction datasets. The main drawback in the above existing works is that they utilize only the partial knowledge from the last layer in the form of '[CLS]' vector. Su et al. [171] added attention on the top of BioBERT to fully utilize the information in the last layer and achieved the best results on three biomedical extraction datasets. The authors generated the final representation by concatenating '[CLS]' vector and weighted sum vector of final hidden state vectors. When compared to applying LSTM on the final hidden state vectors, the attention layer gives better results.","[['b171', 'b167', 'b164', 'b168', 'b170', 'b39', 'b165', 'b169', 'b141', 'b162', 'b163', 'b166']]","[['b171', 'b167', 'b164', 'b168', 'b170', 'b39', 'b165', 'b169', 'b141', 'b162', 'b163', 'b166']]",12,"1. Relation extraction is a crucial task in information extraction which identifies the semantic relations between entities in text.
2. Entity extraction followed by relation extraction helps to convert unstructured text into structured data.
3. Extracting relations between entities is useful in many tasks like knowledge graph construction, text summarization, and question answering.
4. Some of relation extraction datasets are I2B2 2012 [162], AIMED [163], ChemProt [164], DDI [165], I2B2 2010 [141] and EU-ADR [166].
5. Wei et al. [167] achieved the best results on two datasets using MIMIC-BERT [40] +Softmax.
6. Thillaisundaram and Togia [168] applied SciBERT +Softmax to extract relations from biomedical abstracts as part of AGAC track of BioNLP-OST 2019 shared tasks [169].
7. Liu et al. [163]0 proposed SciBERT+Softmax for relation extraction in biomedical text.
8. They showed SciBERT+Softmax outperforms BERT+Softmax on three biomedical relation extraction datasets.
9. The main drawback in the above existing works is that they utilize only the partial knowledge from the last layer in the form of '[163]3' vector.
10. Su et al. [163]2 added attention on the top of BioBERT to fully utilize the information in the last layer and achieved the best results on three biomedical extraction datasets.
11. The authors generated the final representation by concatenating '[163]3' vector and weighted sum vector of final hidden state vectors.
12. When compared to applying LSTM on the final hidden state vectors, the attention layer gives better results.","What are the primary applications of relation extraction in the context of biomedical text analysis?
How do MIMIC-BERT and SciBERT with Softmax layer perform in relation extraction tasks compared to traditional BERT models?
What are the limitations of using only the '[CLS]' vector for relation extraction in biomedical texts as identified in recent studies?
How does the addition of an attention layer improve the performance of BioBERT in biomedical relation extraction tasks?
Can you compare the effectiveness of LSTM and attention layers in processing the final hidden state vectors for relation extraction?","1. What are the primary applications of relation extraction in the context of biomedical text analysis?
2. How do MIMIC-BERT and SciBERT with Softmax layer perform in relation extraction tasks compared to traditional BERT models?
3. What are the limitations of using only the '[CLS]' vector for relation extraction in biomedical texts as identified in recent studies?
4. How does the addition of an attention layer improve the performance of BioBERT in biomedical relation extraction tasks?
5. Can you compare the effectiveness of LSTM and attention layers in processing the final hidden state vectors for relation extraction?","# What are the effective strategies for relation extraction in biomedical text using Transformer-based models?
  - Utilizing pretrained language models like MIMIC-BERT and SciBERT combined with Softmax for achieving significant results on biomedical datasets.
  - Applying attention mechanisms on top of models like BioBERT to fully leverage the information in the last layer, showing improvement over methods that only use the '[CLS]' vector.
  - Comparing the effectiveness of different approaches, such as LSTM versus attention layers on the final hidden state vectors, to optimize relation extraction performance.",1. What are the effective strategies for relation extraction in biomedical text using Transformer-based models?,"Questions:

1. What are the primary applications of relation extraction in the context of biomedical text analysis?
2. How do MIMIC-BERT and SciBERT with Softmax layer perform in relation extraction tasks compared to traditional BERT models?
3. What are the limitations of using only the '[CLS]' vector for relation extraction in biomedical texts as identified in recent studies?
4. How does the addition of an attention layer improve the performance of BioBERT in biomedical relation extraction tasks?
5. Can you compare the effectiveness of LSTM and attention layers in processing the final hidden state vectors for relation extraction?

Answer:

(p43.0) Relation extraction is a crucial task in information extraction which identifies the semantic relations between entities in text. Entity extraction followed by relation extraction helps to convert unstructured text into structured data. Extracting relations between entities is useful in many tasks like knowledge graph construction, text summarization, and question answering. Some of relation extraction datasets are I2B2 2012 [162], AIMED [163], ChemProt [164], DDI [165], I2B2 2010 [141] and EU-ADR [166]. Wei et al. [167] achieved the best results on two datasets using MIMIC-BERT [40] +Softmax. Thillaisundaram and Togia [168] applied SciBERT +Softmax to extract relations from biomedical abstracts as part of AGAC track of BioNLP-OST 2019 shared tasks [169]. Liu et al. [170] proposed SciBERT+Softmax for relation extraction in biomedical text. They showed SciBERT+Softmax outperforms BERT+Softmax on three biomedical relation extraction datasets. The main drawback in the above existing works is that they utilize only the partial knowledge from the last layer in the form of '[CLS]' vector. Su et al. [171] added attention on the top of BioBERT to fully utilize the information in the last layer and achieved the best results on three biomedical extraction datasets. The authors generated the final representation by concatenating '[CLS]' vector and weighted sum vector of final hidden state vectors. When compared to applying LSTM on the final hidden state vectors, the attention layer gives better results.","Questions:

1. What are the effective strategies for relation extraction in biomedical text using Transformer-based models?

Answer:

(p43.0) Relation extraction is a crucial task in information extraction which identifies the semantic relations between entities in text. Entity extraction followed by relation extraction helps to convert unstructured text into structured data. Extracting relations between entities is useful in many tasks like knowledge graph construction, text summarization, and question answering. Some of relation extraction datasets are I2B2 2012 [162], AIMED [163], ChemProt [164], DDI [165], I2B2 2010 [141] and EU-ADR [166]. Wei et al. [167] achieved the best results on two datasets using MIMIC-BERT [40] +Softmax. Thillaisundaram and Togia [168] applied SciBERT +Softmax to extract relations from biomedical abstracts as part of AGAC track of BioNLP-OST 2019 shared tasks [169]. Liu et al. [170] proposed SciBERT+Softmax for relation extraction in biomedical text. They showed SciBERT+Softmax outperforms BERT+Softmax on three biomedical relation extraction datasets. The main drawback in the above existing works is that they utilize only the partial knowledge from the last layer in the form of '[CLS]' vector. Su et al. [171] added attention on the top of BioBERT to fully utilize the information in the last layer and achieved the best results on three biomedical extraction datasets. The authors generated the final representation by concatenating '[CLS]' vector and weighted sum vector of final hidden state vectors. When compared to applying LSTM on the final hidden state vectors, the attention layer gives better results."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s44,Text Classification,"['p44.0', 'p44.1']","[""Text classification involves assigning one of the predefined labels to variable-length texts like phrases, sentences, paragraphs, or documents. Text classification involves an encoder which is usually a transformerbased PLM and a task-specific softmax classifier. '[CLS]' vector or weighted sum of final hidden state vectors is treated as an aggregate representation of given text. The fully connected dense layer in the classifier projects text representation vector into n-dimensional vector space where 'n' represents the number of predefined labels. Finally, the softmax function is applied to get the probabilities of all the labels. Garadi et al. [172] formulated prescription medication (PM) identification from tweets as four-way text classification. They achieved good results using models like BERT, RoBERTa, XLNet, ALBERT, and DistillBERT. They trained different ML-based meta classifiers with predictions from pre-trained models as inputs and further improved the results."", 'Shen et al. [173] applied various in-domain BERT for Alzheimer disease clinical notes classification and achieved the best results using PubMedBERT and BioBERT. They generated labels for the training instances using a rule-based NLP algorithm. Chen et al. [174] further pretrained the general BERT model on 1.5 drugrelated tweets and showed that further pretraining improves the performance of the model on ADR tweets classification. Recent works [175], [176] showed that adding attention on the top of the BERT model improves the performance of the model in clinical text classification. They introduced a custom attention model to aggregate the encoder output and showed that this leads to improved performance and interpretability.']","Text classification involves assigning one of the predefined labels to variable-length texts like phrases, sentences, paragraphs, or documents. Text classification involves an encoder which is usually a transformerbased PLM and a task-specific softmax classifier. '[CLS]' vector or weighted sum of final hidden state vectors is treated as an aggregate representation of given text. The fully connected dense layer in the classifier projects text representation vector into n-dimensional vector space where 'n' represents the number of predefined labels. Finally, the softmax function is applied to get the probabilities of all the labels. Garadi et al. [172] formulated prescription medication (PM) identification from tweets as four-way text classification. They achieved good results using models like BERT, RoBERTa, XLNet, ALBERT, and DistillBERT. They trained different ML-based meta classifiers with predictions from pre-trained models as inputs and further improved the results.

Shen et al. [173] applied various in-domain BERT for Alzheimer disease clinical notes classification and achieved the best results using PubMedBERT and BioBERT. They generated labels for the training instances using a rule-based NLP algorithm. Chen et al. [174] further pretrained the general BERT model on 1.5 drugrelated tweets and showed that further pretraining improves the performance of the model on ADR tweets classification. Recent works [175], [176] showed that adding attention on the top of the BERT model improves the performance of the model in clinical text classification. They introduced a custom attention model to aggregate the encoder output and showed that this leads to improved performance and interpretability.","(p44.0) Text classification involves assigning one of the predefined labels to variable-length texts like phrases, sentences, paragraphs, or documents. Text classification involves an encoder which is usually a transformerbased PLM and a task-specific softmax classifier. '[CLS]' vector or weighted sum of final hidden state vectors is treated as an aggregate representation of given text. The fully connected dense layer in the classifier projects text representation vector into n-dimensional vector space where 'n' represents the number of predefined labels. Finally, the softmax function is applied to get the probabilities of all the labels. Garadi et al. [172] formulated prescription medication (PM) identification from tweets as four-way text classification. They achieved good results using models like BERT, RoBERTa, XLNet, ALBERT, and DistillBERT. They trained different ML-based meta classifiers with predictions from pre-trained models as inputs and further improved the results.

(p44.1) Shen et al. [173] applied various in-domain BERT for Alzheimer disease clinical notes classification and achieved the best results using PubMedBERT and BioBERT. They generated labels for the training instances using a rule-based NLP algorithm. Chen et al. [174] further pretrained the general BERT model on 1.5 drugrelated tweets and showed that further pretraining improves the performance of the model on ADR tweets classification. Recent works [175], [176] showed that adding attention on the top of the BERT model improves the performance of the model in clinical text classification. They introduced a custom attention model to aggregate the encoder output and showed that this leads to improved performance and interpretability.","[['b172'], ['b177', 'b176', 'b175', 'b174']]","[['b172'], ['b177', 'b176', 'b175', 'b174']]",5,"1. Text classification involves assigning one of the predefined labels to variable-length texts like phrases, sentences, paragraphs, or documents.
2. Text classification involves an encoder which is usually a transformerbased PLM and a task-specific softmax classifier.
3. '[CLS]' vector or weighted sum of final hidden state vectors is treated as an aggregate representation of given text.
4. The fully connected dense layer in the classifier projects text representation vector into n-dimensional vector space where 'n' represents the number of predefined labels.
5. Finally, the softmax function is applied to get the probabilities of all the labels.
6. Garadi et al. [172] formulated prescription medication (PM) identification from tweets as four-way text classification.
7. They achieved good results using models like BERT, RoBERTa, XLNet, ALBERT, and DistillBERT.
8. They trained different ML-based meta classifiers with predictions from pre-trained models as inputs and further improved the results.
9. Shen et al. [173] applied various in-domain BERT for Alzheimer disease clinical notes classification and achieved the best results using PubMedBERT and BioBERT.
10. They generated labels for the training instances using a rule-based NLP algorithm.
11. Chen et al. [174] further pretrained the general BERT model on 1.5 drugrelated tweets and showed that further pretraining improves the performance of the model on ADR tweets classification.
12. Recent works [175], [176] showed that adding attention on the top of the BERT model improves the performance of the model in clinical text classification.
13. They introduced a custom attention model to aggregate the encoder output and showed that this leads to improved performance and interpretability.","How does the use of a '[CLS]' vector or weighted sum of final hidden state vectors enhance text classification in transformer-based PLMs?
What strategies did Garadi et al. employ to achieve effective prescription medication identification from tweets using transformer-based models?
In what ways does further pretraining on drug-related tweets enhance the performance of BERT models for ADR tweet classification?
How does the addition of custom attention models on top of BERT affect its performance and interpretability in clinical text classification tasks?
What advantages do domain-specific BERT models like PubMedBERT and BioBERT offer in the classification of Alzheimer disease clinical notes over general BERT models?","1. How does the use of a '[CLS]' vector or weighted sum of final hidden state vectors enhance text classification in transformer-based PLMs?
2. What strategies did Garadi et al. employ to achieve effective prescription medication identification from tweets using transformer-based models?
3. In what ways does further pretraining on drug-related tweets enhance the performance of BERT models for ADR tweet classification?
4. How does the addition of custom attention models on top of BERT affect its performance and interpretability in clinical text classification tasks?
5. What advantages do domain-specific BERT models like PubMedBERT and BioBERT offer in the classification of Alzheimer disease clinical notes over general BERT models?",None,,"Questions:

1. How does the use of a '[CLS]' vector or weighted sum of final hidden state vectors enhance text classification in transformer-based PLMs?
2. What strategies did Garadi et al. employ to achieve effective prescription medication identification from tweets using transformer-based models?
3. In what ways does further pretraining on drug-related tweets enhance the performance of BERT models for ADR tweet classification?
4. How does the addition of custom attention models on top of BERT affect its performance and interpretability in clinical text classification tasks?
5. What advantages do domain-specific BERT models like PubMedBERT and BioBERT offer in the classification of Alzheimer disease clinical notes over general BERT models?

Answer:

(p44.0) Text classification involves assigning one of the predefined labels to variable-length texts like phrases, sentences, paragraphs, or documents. Text classification involves an encoder which is usually a transformerbased PLM and a task-specific softmax classifier. '[CLS]' vector or weighted sum of final hidden state vectors is treated as an aggregate representation of given text. The fully connected dense layer in the classifier projects text representation vector into n-dimensional vector space where 'n' represents the number of predefined labels. Finally, the softmax function is applied to get the probabilities of all the labels. Garadi et al. [172] formulated prescription medication (PM) identification from tweets as four-way text classification. They achieved good results using models like BERT, RoBERTa, XLNet, ALBERT, and DistillBERT. They trained different ML-based meta classifiers with predictions from pre-trained models as inputs and further improved the results.

(p44.1) Shen et al. [173] applied various in-domain BERT for Alzheimer disease clinical notes classification and achieved the best results using PubMedBERT and BioBERT. They generated labels for the training instances using a rule-based NLP algorithm. Chen et al. [174] further pretrained the general BERT model on 1.5 drugrelated tweets and showed that further pretraining improves the performance of the model on ADR tweets classification. Recent works [175], [176] showed that adding attention on the top of the BERT model improves the performance of the model in clinical text classification. They introduced a custom attention model to aggregate the encoder output and showed that this leads to improved performance and interpretability.","Questions:



Answer:

(p44.0) Text classification involves assigning one of the predefined labels to variable-length texts like phrases, sentences, paragraphs, or documents. Text classification involves an encoder which is usually a transformerbased PLM and a task-specific softmax classifier. '[CLS]' vector or weighted sum of final hidden state vectors is treated as an aggregate representation of given text. The fully connected dense layer in the classifier projects text representation vector into n-dimensional vector space where 'n' represents the number of predefined labels. Finally, the softmax function is applied to get the probabilities of all the labels. Garadi et al. [172] formulated prescription medication (PM) identification from tweets as four-way text classification. They achieved good results using models like BERT, RoBERTa, XLNet, ALBERT, and DistillBERT. They trained different ML-based meta classifiers with predictions from pre-trained models as inputs and further improved the results.

(p44.1) Shen et al. [173] applied various in-domain BERT for Alzheimer disease clinical notes classification and achieved the best results using PubMedBERT and BioBERT. They generated labels for the training instances using a rule-based NLP algorithm. Chen et al. [174] further pretrained the general BERT model on 1.5 drugrelated tweets and showed that further pretraining improves the performance of the model on ADR tweets classification. Recent works [175], [176] showed that adding attention on the top of the BERT model improves the performance of the model in clinical text classification. They introduced a custom attention model to aggregate the encoder output and showed that this leads to improved performance and interpretability."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s45,Question Answering,['p45.0'],"['Question Answering (QA) aims to extract answers for the given queries. QA helps to quickly find information in clinical notes or biomedical literature and thus saves a lot of time. The main challenge in developing automated QA systems for the clinical or biomedical domain is the small size of datasets. Developing large QA datasets in the clinical or biomedical domain is quite expensive and requires a lot of time also. Some of the popular in-domain QA datasets are emrQA [177], CliCR [178], PubMedQA [179] COVID-QA [180], MASH-QA [181] and Health-QA [182]. Chakraborty et al. [183] showed BioMedBERT obtained by further pretraining BERT-Large on BREATHE 1.0 corpus outperformed BioBERT on biomedical question answering. The main reason for this is the diversity of text in BREATHE 1.0 corpus. Pergola et al. [52] introduced Biomedical Entity Masking (BEM) which allows the model to learn entity-centric knowledge during further pretraining. They showed that BEM improved the performance of both general and indomain models on two in-domain QA datasets. Recent works used intermediate fine-tuning on general QA [58], [183] or NLI [66] datasets or multi-tasking [184] to improve the performance of in-domain QA models. For example, Soni et al. [183] achieved the best results on a) CliCR by intermediate fine-tuning on SQuaD using BioBERT and b) emrQA by intermediate fine-tuning on SQuaD and CliCR using Clinical BERT. Yoon et al. [58] showed that intermediate fine-tuning on general domain Squad datasets improves the performance on biomedical question answering datasets. Akdemir et al. [184] proposed a novel multi-task model based on BioBERT for biomedical question answering. They used biomedical NER as an auxiliary task and showed that transfer learning from the bioNER task improves performance on question answering tasks.']","Question Answering (QA) aims to extract answers for the given queries. QA helps to quickly find information in clinical notes or biomedical literature and thus saves a lot of time. The main challenge in developing automated QA systems for the clinical or biomedical domain is the small size of datasets. Developing large QA datasets in the clinical or biomedical domain is quite expensive and requires a lot of time also. Some of the popular in-domain QA datasets are emrQA [177], CliCR [178], PubMedQA [179] COVID-QA [180], MASH-QA [181] and Health-QA [182]. Chakraborty et al. [183] showed BioMedBERT obtained by further pretraining BERT-Large on BREATHE 1.0 corpus outperformed BioBERT on biomedical question answering. The main reason for this is the diversity of text in BREATHE 1.0 corpus. Pergola et al. [52] introduced Biomedical Entity Masking (BEM) which allows the model to learn entity-centric knowledge during further pretraining. They showed that BEM improved the performance of both general and indomain models on two in-domain QA datasets. Recent works used intermediate fine-tuning on general QA [58], [183] or NLI [66] datasets or multi-tasking [184] to improve the performance of in-domain QA models. For example, Soni et al. [183] achieved the best results on a) CliCR by intermediate fine-tuning on SQuaD using BioBERT and b) emrQA by intermediate fine-tuning on SQuaD and CliCR using Clinical BERT. Yoon et al. [58] showed that intermediate fine-tuning on general domain Squad datasets improves the performance on biomedical question answering datasets. Akdemir et al. [184] proposed a novel multi-task model based on BioBERT for biomedical question answering. They used biomedical NER as an auxiliary task and showed that transfer learning from the bioNER task improves performance on question answering tasks.","(p45.0) Question Answering (QA) aims to extract answers for the given queries. QA helps to quickly find information in clinical notes or biomedical literature and thus saves a lot of time. The main challenge in developing automated QA systems for the clinical or biomedical domain is the small size of datasets. Developing large QA datasets in the clinical or biomedical domain is quite expensive and requires a lot of time also. Some of the popular in-domain QA datasets are emrQA [177], CliCR [178], PubMedQA [179] COVID-QA [180], MASH-QA [181] and Health-QA [182]. Chakraborty et al. [183] showed BioMedBERT obtained by further pretraining BERT-Large on BREATHE 1.0 corpus outperformed BioBERT on biomedical question answering. The main reason for this is the diversity of text in BREATHE 1.0 corpus. Pergola et al. [52] introduced Biomedical Entity Masking (BEM) which allows the model to learn entity-centric knowledge during further pretraining. They showed that BEM improved the performance of both general and indomain models on two in-domain QA datasets. Recent works used intermediate fine-tuning on general QA [58], [183] or NLI [66] datasets or multi-tasking [184] to improve the performance of in-domain QA models. For example, Soni et al. [183] achieved the best results on a) CliCR by intermediate fine-tuning on SQuaD using BioBERT and b) emrQA by intermediate fine-tuning on SQuaD and CliCR using Clinical BERT. Yoon et al. [58] showed that intermediate fine-tuning on general domain Squad datasets improves the performance on biomedical question answering datasets. Akdemir et al. [184] proposed a novel multi-task model based on BioBERT for biomedical question answering. They used biomedical NER as an auxiliary task and showed that transfer learning from the bioNER task improves performance on question answering tasks.","[['b57', 'b184', 'b181', 'b182', 'b66', 'b185', 'b179', 'b51', 'b183', 'b180', 'b178']]","[['b57', 'b184', 'b181', 'b182', 'b66', 'b185', 'b179', 'b51', 'b183', 'b180', 'b178']]",11,"1. Question Answering (QA) aims to extract answers for the given queries.
2. QA helps to quickly find information in clinical notes or biomedical literature and thus saves a lot of time.
3. The main challenge in developing automated QA systems for the clinical or biomedical domain is the small size of datasets.
4. Developing large QA datasets in the clinical or biomedical domain is quite expensive and requires a lot of time also.
5. Some of the popular in-domain QA datasets are emrQA [177], CliCR [178], PubMedQA [179] COVID-QA [180], MASH-QA [181] and Health-QA [182].
6. Chakraborty et al. [177]4 showed BioMedBERT obtained by further pretraining BERT-Large on BREATHE 1.0 corpus outperformed BioBERT on biomedical question answering.
7. The main reason for this is the diversity of text in BREATHE 1.0 corpus.
8. Pergola et al. [52] introduced Biomedical Entity Masking (BEM) which allows the model to learn entity-centric knowledge during further pretraining.
9. They showed that BEM improved the performance of both general and indomain models on two in-domain QA datasets.
10. Recent works used intermediate fine-tuning on general QA [177]5, [177]4 or NLI [177]2 datasets or multi-tasking [177]6 to improve the performance of in-domain QA models.
11. For example, Soni et al. [177]4 achieved the best results on a) CliCR by intermediate fine-tuning on SQuaD using BioBERT and b) emrQA by intermediate fine-tuning on SQuaD and CliCR using Clinical BERT.
12. Yoon et al. [177]5 showed that intermediate fine-tuning on general domain Squad datasets improves the performance on biomedical question answering datasets.
13. Akdemir et al. [177]6 proposed a novel multi-task model based on BioBERT for biomedical question answering.
14. They used biomedical NER as an auxiliary task and showed that transfer learning from the bioNER task improves performance on question answering tasks.","What are the main challenges in developing automated QA systems for the clinical or biomedical domain?
How does the diversity of text in the BREATHE 1.0 corpus contribute to the performance of BioMedBERT in biomedical question answering?
What role does Biomedical Entity Masking (BEM) play in improving the performance of in-domain QA models?
How does intermediate fine-tuning on general QA datasets affect the performance of biomedical question answering models?
In what way does incorporating biomedical Named Entity Recognition (NER) as an auxiliary task in a multi-task model enhance question answering performance in the biomedical domain?","1. What are the main challenges in developing automated QA systems for the clinical or biomedical domain?
2. How does the diversity of text in the BREATHE 1.0 corpus contribute to the performance of BioMedBERT in biomedical question answering?
3. What role does Biomedical Entity Masking (BEM) play in improving the performance of in-domain QA models?
4. How does intermediate fine-tuning on general QA datasets affect the performance of biomedical question answering models?
5. In what way does incorporating biomedical Named Entity Recognition (NER) as an auxiliary task in a multi-task model enhance question answering performance in the biomedical domain?",None,,"Questions:

1. What are the main challenges in developing automated QA systems for the clinical or biomedical domain?
2. How does the diversity of text in the BREATHE 1.0 corpus contribute to the performance of BioMedBERT in biomedical question answering?
3. What role does Biomedical Entity Masking (BEM) play in improving the performance of in-domain QA models?
4. How does intermediate fine-tuning on general QA datasets affect the performance of biomedical question answering models?
5. In what way does incorporating biomedical Named Entity Recognition (NER) as an auxiliary task in a multi-task model enhance question answering performance in the biomedical domain?

Answer:

(p45.0) Question Answering (QA) aims to extract answers for the given queries. QA helps to quickly find information in clinical notes or biomedical literature and thus saves a lot of time. The main challenge in developing automated QA systems for the clinical or biomedical domain is the small size of datasets. Developing large QA datasets in the clinical or biomedical domain is quite expensive and requires a lot of time also. Some of the popular in-domain QA datasets are emrQA [177], CliCR [178], PubMedQA [179] COVID-QA [180], MASH-QA [181] and Health-QA [182]. Chakraborty et al. [183] showed BioMedBERT obtained by further pretraining BERT-Large on BREATHE 1.0 corpus outperformed BioBERT on biomedical question answering. The main reason for this is the diversity of text in BREATHE 1.0 corpus. Pergola et al. [52] introduced Biomedical Entity Masking (BEM) which allows the model to learn entity-centric knowledge during further pretraining. They showed that BEM improved the performance of both general and indomain models on two in-domain QA datasets. Recent works used intermediate fine-tuning on general QA [58], [183] or NLI [66] datasets or multi-tasking [184] to improve the performance of in-domain QA models. For example, Soni et al. [183] achieved the best results on a) CliCR by intermediate fine-tuning on SQuaD using BioBERT and b) emrQA by intermediate fine-tuning on SQuaD and CliCR using Clinical BERT. Yoon et al. [58] showed that intermediate fine-tuning on general domain Squad datasets improves the performance on biomedical question answering datasets. Akdemir et al. [184] proposed a novel multi-task model based on BioBERT for biomedical question answering. They used biomedical NER as an auxiliary task and showed that transfer learning from the bioNER task improves performance on question answering tasks.","Questions:



Answer:

(p45.0) Question Answering (QA) aims to extract answers for the given queries. QA helps to quickly find information in clinical notes or biomedical literature and thus saves a lot of time. The main challenge in developing automated QA systems for the clinical or biomedical domain is the small size of datasets. Developing large QA datasets in the clinical or biomedical domain is quite expensive and requires a lot of time also. Some of the popular in-domain QA datasets are emrQA [177], CliCR [178], PubMedQA [179] COVID-QA [180], MASH-QA [181] and Health-QA [182]. Chakraborty et al. [183] showed BioMedBERT obtained by further pretraining BERT-Large on BREATHE 1.0 corpus outperformed BioBERT on biomedical question answering. The main reason for this is the diversity of text in BREATHE 1.0 corpus. Pergola et al. [52] introduced Biomedical Entity Masking (BEM) which allows the model to learn entity-centric knowledge during further pretraining. They showed that BEM improved the performance of both general and indomain models on two in-domain QA datasets. Recent works used intermediate fine-tuning on general QA [58], [183] or NLI [66] datasets or multi-tasking [184] to improve the performance of in-domain QA models. For example, Soni et al. [183] achieved the best results on a) CliCR by intermediate fine-tuning on SQuaD using BioBERT and b) emrQA by intermediate fine-tuning on SQuaD and CliCR using Clinical BERT. Yoon et al. [58] showed that intermediate fine-tuning on general domain Squad datasets improves the performance on biomedical question answering datasets. Akdemir et al. [184] proposed a novel multi-task model based on BioBERT for biomedical question answering. They used biomedical NER as an auxiliary task and showed that transfer learning from the bioNER task improves performance on question answering tasks."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s46,Text Summarization,"['p46.0', 'p46.1', 'p46.2']","['In general, sources of biomedical information like clinical notes, scientific papers, radiology reports are length in nature. Researchers and domain experts need to go through a number of biomedical documents. As biomedical documents are length in nature, there is a need for automatic biomedical text summarization which reduces the effort and time for researchers and domain experts [185], [186]. Text summarization falls into two broad categories namely extractive summarization which identifies the most relevant sentences in the document while abstractive summarization generates new text which represents the summary of the document [187]. There are no standard datasets for biomedical text summarization. Researchers usually treat scientific papers as documents and their abstracts as summaries [188], [189].', 'Moradi et al. [188] proposed a novel approach to summarize biomedical scientific articles. They embedded sentences, generated clusters, and then extracted the most informative sentences from each of the clusters. They showed that BERT-large outperformed other models including the in-domain BERT models like BioBERT.', 'In the case of small models, BioBERT outperformed others. Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text. They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences. Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text. BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence. The sentences with the highest score are considered as the summary. Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT.']","In general, sources of biomedical information like clinical notes, scientific papers, radiology reports are length in nature. Researchers and domain experts need to go through a number of biomedical documents. As biomedical documents are length in nature, there is a need for automatic biomedical text summarization which reduces the effort and time for researchers and domain experts [185], [186]. Text summarization falls into two broad categories namely extractive summarization which identifies the most relevant sentences in the document while abstractive summarization generates new text which represents the summary of the document [187]. There are no standard datasets for biomedical text summarization. Researchers usually treat scientific papers as documents and their abstracts as summaries [188], [189].

Moradi et al. [188] proposed a novel approach to summarize biomedical scientific articles. They embedded sentences, generated clusters, and then extracted the most informative sentences from each of the clusters. They showed that BERT-large outperformed other models including the in-domain BERT models like BioBERT.

In the case of small models, BioBERT outperformed others. Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text. They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences. Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text. BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence. The sentences with the highest score are considered as the summary. Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT.","(p46.0) In general, sources of biomedical information like clinical notes, scientific papers, radiology reports are length in nature. Researchers and domain experts need to go through a number of biomedical documents. As biomedical documents are length in nature, there is a need for automatic biomedical text summarization which reduces the effort and time for researchers and domain experts [185], [186]. Text summarization falls into two broad categories namely extractive summarization which identifies the most relevant sentences in the document while abstractive summarization generates new text which represents the summary of the document [187]. There are no standard datasets for biomedical text summarization. Researchers usually treat scientific papers as documents and their abstracts as summaries [188], [189].

(p46.1) Moradi et al. [188] proposed a novel approach to summarize biomedical scientific articles. They embedded sentences, generated clusters, and then extracted the most informative sentences from each of the clusters. They showed that BERT-large outperformed other models including the in-domain BERT models like BioBERT.

(p46.2) In the case of small models, BioBERT outperformed others. Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text. They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences. Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text. BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence. The sentences with the highest score are considered as the summary. Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT.","[['b186', 'b190', 'b188', 'b187', 'b189'], ['b189'], ['b191', 'b75', 'b190']]","[['b186', 'b190', 'b188', 'b187', 'b189'], ['b189'], ['b191', 'b75', 'b190']]",9,"1. In general, sources of biomedical information like clinical notes, scientific papers, radiology reports are length in nature.
2. Researchers and domain experts need to go through a number of biomedical documents.
3. As biomedical documents are length in nature, there is a need for automatic biomedical text summarization which reduces the effort and time for researchers and domain experts [185], [186].
4. Text summarization falls into two broad categories namely extractive summarization which identifies the most relevant sentences in the document while abstractive summarization generates new text which represents the summary of the document [187].
5. There are no standard datasets for biomedical text summarization.
6. Researchers usually treat scientific papers as documents and their abstracts as summaries [188], [189].Moradi et al. [188] proposed a novel approach to summarize biomedical scientific articles.
7. They embedded sentences, generated clusters, and then extracted the most informative sentences from each of the clusters.
8. They showed that BERT-large outperformed other models including the in-domain BERT models like BioBERT.
9. In the case of small models, BioBERT outperformed others.
10. Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text.
11. They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences.
12. Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text.
13. BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence.
14. The sentences with the highest score are considered as the summary.
15. Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT.","What are the two main types of text summarization techniques in the context of biomedical document analysis?
How do researchers typically create datasets for training and evaluating biomedical text summarization models?
What unique strategy did Moradi et al. employ to summarize biomedical scientific articles effectively?
In the comparison of BERT-large and BioBERT for biomedical text summarization, which model demonstrated superior performance?
How does the BioBERTSum method for biomedical text summarization utilize BioBERT and transformer decoders differently from other approaches?","1. What are the two main types of text summarization techniques in the context of biomedical document analysis?
2. How do researchers typically create datasets for training and evaluating biomedical text summarization models?
3. What unique strategy did Moradi et al. employ to summarize biomedical scientific articles effectively?
4. In the comparison of BERT-large and BioBERT for biomedical text summarization, which model demonstrated superior performance?
5. How does the BioBERTSum method for biomedical text summarization utilize BioBERT and transformer decoders differently from other approaches?",None,,"Questions:

1. What are the two main types of text summarization techniques in the context of biomedical document analysis?
2. How do researchers typically create datasets for training and evaluating biomedical text summarization models?
3. What unique strategy did Moradi et al. employ to summarize biomedical scientific articles effectively?
4. In the comparison of BERT-large and BioBERT for biomedical text summarization, which model demonstrated superior performance?
5. How does the BioBERTSum method for biomedical text summarization utilize BioBERT and transformer decoders differently from other approaches?

Answer:

(p46.0) In general, sources of biomedical information like clinical notes, scientific papers, radiology reports are length in nature. Researchers and domain experts need to go through a number of biomedical documents. As biomedical documents are length in nature, there is a need for automatic biomedical text summarization which reduces the effort and time for researchers and domain experts [185], [186]. Text summarization falls into two broad categories namely extractive summarization which identifies the most relevant sentences in the document while abstractive summarization generates new text which represents the summary of the document [187]. There are no standard datasets for biomedical text summarization. Researchers usually treat scientific papers as documents and their abstracts as summaries [188], [189].

(p46.1) Moradi et al. [188] proposed a novel approach to summarize biomedical scientific articles. They embedded sentences, generated clusters, and then extracted the most informative sentences from each of the clusters. They showed that BERT-large outperformed other models including the in-domain BERT models like BioBERT.

(p46.2) In the case of small models, BioBERT outperformed others. Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text. They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences. Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text. BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence. The sentences with the highest score are considered as the summary. Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT.","Questions:



Answer:

(p46.0) In general, sources of biomedical information like clinical notes, scientific papers, radiology reports are length in nature. Researchers and domain experts need to go through a number of biomedical documents. As biomedical documents are length in nature, there is a need for automatic biomedical text summarization which reduces the effort and time for researchers and domain experts [185], [186]. Text summarization falls into two broad categories namely extractive summarization which identifies the most relevant sentences in the document while abstractive summarization generates new text which represents the summary of the document [187]. There are no standard datasets for biomedical text summarization. Researchers usually treat scientific papers as documents and their abstracts as summaries [188], [189].

(p46.1) Moradi et al. [188] proposed a novel approach to summarize biomedical scientific articles. They embedded sentences, generated clusters, and then extracted the most informative sentences from each of the clusters. They showed that BERT-large outperformed other models including the in-domain BERT models like BioBERT.

(p46.2) In the case of small models, BioBERT outperformed others. Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text. They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences. Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text. BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence. The sentences with the highest score are considered as the summary. Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s52,Semi-Supervised Learning,"['p52.0', 'p52.1', 'p52.2']","['Fine-tunes the model on training instances along with pseudo labeled instances', 'Allows the model to leverage task-related unlabelled instances.', 'Fine-tuning must be done iteratively to reduce the noisy labeled instances. BERT models to the biomedical domain. Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44]. TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191]. b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123]. The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123].']","Fine-tunes the model on training instances along with pseudo labeled instances

Allows the model to leverage task-related unlabelled instances.

Fine-tuning must be done iteratively to reduce the noisy labeled instances. BERT models to the biomedical domain. Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44]. TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191]. b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123]. The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123].","(p52.0) Fine-tunes the model on training instances along with pseudo labeled instances

(p52.1) Allows the model to leverage task-related unlabelled instances.

(p52.2) Fine-tuning must be done iteratively to reduce the noisy labeled instances. BERT models to the biomedical domain. Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44]. TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191]. b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123]. The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123].","[[], [], ['b43', 'b122', 'b192', 'b123']]","[[], [], ['b43', 'b122', 'b192', 'b123']]",4,"1. Fine-tunes the model on training instances along with pseudo labeled instancesAllows the model to leverage task-related unlabelled instances.
2. Fine-tuning must be done iteratively to reduce the noisy labeled instances.
3. BERT models to the biomedical domain.
4. Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44].
5. TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances.
6. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191].
7. b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123].
8. The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123].","How does semi-supervised learning enhance the performance of transformer-based biomedical pretrained language models?
What are the key strategies for adapting general transformer-based pretrained language models to the biomedical domain?
How does Task Adaptative Pretraining (TAPT) contribute to domain and task-specific knowledge acquisition in biomedical pretrained language models?
In what ways can the embedding layer of transformer-based pretrained language models be refined for better adaptation to the biomedical domain?
What role does pseudo labeling play in the iterative fine-tuning process of biomedical pretrained language models?","1. How does semi-supervised learning enhance the performance of transformer-based biomedical pretrained language models?
2. What are the key strategies for adapting general transformer-based pretrained language models to the biomedical domain?
3. How does Task Adaptative Pretraining (TAPT) contribute to domain and task-specific knowledge acquisition in biomedical pretrained language models?
4. In what ways can the embedding layer of transformer-based pretrained language models be refined for better adaptation to the biomedical domain?
5. What role does pseudo labeling play in the iterative fine-tuning process of biomedical pretrained language models?",None,,"Questions:

1. How does semi-supervised learning enhance the performance of transformer-based biomedical pretrained language models?
2. What are the key strategies for adapting general transformer-based pretrained language models to the biomedical domain?
3. How does Task Adaptative Pretraining (TAPT) contribute to domain and task-specific knowledge acquisition in biomedical pretrained language models?
4. In what ways can the embedding layer of transformer-based pretrained language models be refined for better adaptation to the biomedical domain?
5. What role does pseudo labeling play in the iterative fine-tuning process of biomedical pretrained language models?

Answer:

(p52.0) Fine-tunes the model on training instances along with pseudo labeled instances

(p52.1) Allows the model to leverage task-related unlabelled instances.

(p52.2) Fine-tuning must be done iteratively to reduce the noisy labeled instances. BERT models to the biomedical domain. Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44]. TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191]. b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123]. The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123].","Questions:



Answer:

(p52.0) Fine-tunes the model on training instances along with pseudo labeled instances

(p52.1) Allows the model to leverage task-related unlabelled instances.

(p52.2) Fine-tuning must be done iteratively to reduce the noisy labeled instances. BERT models to the biomedical domain. Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44]. TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191]. b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123]. The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123]."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s53,Ontology Knowledge Injection,['p53.0'],"['Models like BioBERT, PubMedBERT have achieved good results in many of the tasks. However, these models lack knowledge from human-curated knowledge sources. These models can be further enhanced by ontology knowledge injection. Ontology knowledge injection can be done in many ways namely a) continual pretraining the models on UMLS synonyms [34], [46] or relations [45] or both [47] b) continual pretraining the models on UMLS concept definitions [192] and c) feature vector constructed using ontology is added to the sequence vector learned by the models [174].']","Models like BioBERT, PubMedBERT have achieved good results in many of the tasks. However, these models lack knowledge from human-curated knowledge sources. These models can be further enhanced by ontology knowledge injection. Ontology knowledge injection can be done in many ways namely a) continual pretraining the models on UMLS synonyms [34], [46] or relations [45] or both [47] b) continual pretraining the models on UMLS concept definitions [192] and c) feature vector constructed using ontology is added to the sequence vector learned by the models [174].","(p53.0) Models like BioBERT, PubMedBERT have achieved good results in many of the tasks. However, these models lack knowledge from human-curated knowledge sources. These models can be further enhanced by ontology knowledge injection. Ontology knowledge injection can be done in many ways namely a) continual pretraining the models on UMLS synonyms [34], [46] or relations [45] or both [47] b) continual pretraining the models on UMLS concept definitions [192] and c) feature vector constructed using ontology is added to the sequence vector learned by the models [174].","[['b46', 'b175', 'b193', 'b44', 'b45', 'b33']]","[['b46', 'b175', 'b193', 'b44', 'b45', 'b33']]",6,"1. Models like BioBERT, PubMedBERT have achieved good results in many of the tasks.
2. However, these models lack knowledge from human-curated knowledge sources.
3. These models can be further enhanced by ontology knowledge injection.
4. Ontology knowledge injection can be done in many ways namely a) continual pretraining the models on UMLS synonyms [34], [46] or relations [45] or both [47] b) continual pretraining the models on UMLS concept definitions [192] and c) feature vector constructed using ontology is added to the sequence vector learned by the models [174].","What are the main strategies for enhancing transformer-based biomedical pretrained language models with ontology knowledge?
How does continual pretraining with UMLS synonyms and relations improve the performance of models like BioBERT and PubMedBERT?
In what ways can ontology knowledge injection benefit transformer-based biomedical pretrained language models?
What role does the feature vector constructed using ontology play when added to the sequence vector in biomedical pretrained language models?
How does the inclusion of UMLS concept definitions during continual pretraining affect the capabilities of biomedical pretrained language models?","1. What are the main strategies for enhancing transformer-based biomedical pretrained language models with ontology knowledge?
2. How does continual pretraining with UMLS synonyms and relations improve the performance of models like BioBERT and PubMedBERT?
3. In what ways can ontology knowledge injection benefit transformer-based biomedical pretrained language models?
4. What role does the feature vector constructed using ontology play when added to the sequence vector in biomedical pretrained language models?
5. How does the inclusion of UMLS concept definitions during continual pretraining affect the capabilities of biomedical pretrained language models?",None,,"Questions:

1. What are the main strategies for enhancing transformer-based biomedical pretrained language models with ontology knowledge?
2. How does continual pretraining with UMLS synonyms and relations improve the performance of models like BioBERT and PubMedBERT?
3. In what ways can ontology knowledge injection benefit transformer-based biomedical pretrained language models?
4. What role does the feature vector constructed using ontology play when added to the sequence vector in biomedical pretrained language models?
5. How does the inclusion of UMLS concept definitions during continual pretraining affect the capabilities of biomedical pretrained language models?

Answer:

(p53.0) Models like BioBERT, PubMedBERT have achieved good results in many of the tasks. However, these models lack knowledge from human-curated knowledge sources. These models can be further enhanced by ontology knowledge injection. Ontology knowledge injection can be done in many ways namely a) continual pretraining the models on UMLS synonyms [34], [46] or relations [45] or both [47] b) continual pretraining the models on UMLS concept definitions [192] and c) feature vector constructed using ontology is added to the sequence vector learned by the models [174].","Questions:



Answer:

(p53.0) Models like BioBERT, PubMedBERT have achieved good results in many of the tasks. However, these models lack knowledge from human-curated knowledge sources. These models can be further enhanced by ontology knowledge injection. Ontology knowledge injection can be done in many ways namely a) continual pretraining the models on UMLS synonyms [34], [46] or relations [45] or both [47] b) continual pretraining the models on UMLS concept definitions [192] and c) feature vector constructed using ontology is added to the sequence vector learned by the models [174]."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s54,Small Datasets,"['p54.0', 'p54.1', 'p54.2', 'p54.3']","['Pretraining on large volumes of in-domain text allows the model language representations while fine-tuning allows the model to learn task-specific knowledge. During fine-tuning the model must learn sufficient task-specific knowledge to achieve good results on the task where the input distribution, as well as label space, is different from pretraining [193], [194]. With small target datasets, the models are not able to learn enough task-specific which limits the performance. To over the small size of target datasets, the possible solutions are Intermediate Fine-Tuning -Intermediate fine-tuning on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets [55]- [58], [62], [63], [65], [66] and', 'Multi-Task Fine-Tuning -Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. Multi-Task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]- [71], [73].', 'Data Augmentation -Data augmentation helps us to create new training instances from existing instances. These newly creating training instances are close to original training data and helpful in low resource scenarios. Back translation and EDA [195] are the top popular techniques for data augmentation. For example, Wang et al. [159] used back translation to augment the training instances to train the clinical text similarity model. The domain-specific ontologies like UMLS can also be used to augment the training instances [153].', 'Semi-Supervised Learning -Semi-supervised learning augments the training set with pseudo-labeled instances. The model which is fine-tuned on the original training set is used to label the task-related unlabelled instances. The model is again fine-tuned on the augmented training set and this process is repeated until the model converges [63], [196] . Table 11 contains a brief summary of these approaches.']","Pretraining on large volumes of in-domain text allows the model language representations while fine-tuning allows the model to learn task-specific knowledge. During fine-tuning the model must learn sufficient task-specific knowledge to achieve good results on the task where the input distribution, as well as label space, is different from pretraining [193], [194]. With small target datasets, the models are not able to learn enough task-specific which limits the performance. To over the small size of target datasets, the possible solutions are Intermediate Fine-Tuning -Intermediate fine-tuning on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets [55]- [58], [62], [63], [65], [66] and

Multi-Task Fine-Tuning -Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. Multi-Task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]- [71], [73].

Data Augmentation -Data augmentation helps us to create new training instances from existing instances. These newly creating training instances are close to original training data and helpful in low resource scenarios. Back translation and EDA [195] are the top popular techniques for data augmentation. For example, Wang et al. [159] used back translation to augment the training instances to train the clinical text similarity model. The domain-specific ontologies like UMLS can also be used to augment the training instances [153].

Semi-Supervised Learning -Semi-supervised learning augments the training set with pseudo-labeled instances. The model which is fine-tuned on the original training set is used to label the task-related unlabelled instances. The model is again fine-tuned on the augmented training set and this process is repeated until the model converges [63], [196] . Table 11 contains a brief summary of these approaches.","(p54.0) Pretraining on large volumes of in-domain text allows the model language representations while fine-tuning allows the model to learn task-specific knowledge. During fine-tuning the model must learn sufficient task-specific knowledge to achieve good results on the task where the input distribution, as well as label space, is different from pretraining [193], [194]. With small target datasets, the models are not able to learn enough task-specific which limits the performance. To over the small size of target datasets, the possible solutions are Intermediate Fine-Tuning -Intermediate fine-tuning on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets [55]- [58], [62], [63], [65], [66] and

(p54.1) Multi-Task Fine-Tuning -Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. Multi-Task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]- [71], [73].

(p54.2) Data Augmentation -Data augmentation helps us to create new training instances from existing instances. These newly creating training instances are close to original training data and helpful in low resource scenarios. Back translation and EDA [195] are the top popular techniques for data augmentation. For example, Wang et al. [159] used back translation to augment the training instances to train the clinical text similarity model. The domain-specific ontologies like UMLS can also be used to augment the training instances [153].

(p54.3) Semi-Supervised Learning -Semi-supervised learning augments the training set with pseudo-labeled instances. The model which is fine-tuned on the original training set is used to label the task-related unlabelled instances. The model is again fine-tuned on the augmented training set and this process is repeated until the model converges [63], [196] . Table 11 contains a brief summary of these approaches.","[['b54', 'b57', 'b62', 'b63', 'b65', 'b66', 'b195', 'b194'], ['b69', 'b73', 'b67', 'b71'], ['b153', 'b196', 'b159'], ['b63', 'b197']]","[['b54', 'b57', 'b62', 'b63', 'b65', 'b66', 'b195', 'b194'], ['b69', 'b73', 'b67', 'b71'], ['b153', 'b196', 'b159'], ['b63', 'b197']]",17,"1. Pretraining on large volumes of in-domain text allows the model language representations while fine-tuning allows the model to learn task-specific knowledge.
2. During fine-tuning the model must learn sufficient task-specific knowledge to achieve good results on the task where the input distribution, as well as label space, is different from pretraining [193], [194].
3. With small target datasets, the models are not able to learn enough task-specific which limits the performance.
4. To over the small size of target datasets, the possible solutions are Intermediate Fine-Tuning -Intermediate fine-tuning on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets [55]- [58], [62], [194]6, [65], [66] andMulti-Task Fine-Tuning -Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [194]0.
5. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer.
6. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks.
7. Multi-Task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [194]0- [194]1, [194]2.Data Augmentation -Data augmentation helps us to create new training instances from existing instances.
8. These newly creating training instances are close to original training data and helpful in low resource scenarios.
9. Back translation and EDA [194]3 are the top popular techniques for data augmentation.
10. For example, Wang et al. [194]4 used back translation to augment the training instances to train the clinical text similarity model.
11. The domain-specific ontologies like UMLS can also be used to augment the training instances [194]5.
12. Semi-Supervised Learning -Semi-supervised learning augments the training set with pseudo-labeled instances.
13. The model which is fine-tuned on the original training set is used to label the task-related unlabelled instances.
14. The model is again fine-tuned on the augmented training set and this process is repeated until the model converges [194]6, [194]7 .
15. Table 11 contains a brief summary of these approaches.","How does Intermediate Fine-Tuning improve model performance on small target datasets in the biomedical domain?
What are the benefits of Multi-Task Fine-Tuning in low resource scenarios for biomedical pretrained language models?
How does Data Augmentation contribute to training models in low resource scenarios, and what are some popular techniques?
In what way does Semi-Supervised Learning utilize pseudo-labeled instances to enhance model training?
Can domain-specific ontologies like UMLS be used for Data Augmentation, and if so, how do they contribute to model performance?","1. How does Intermediate Fine-Tuning improve model performance on small target datasets in the biomedical domain?
2. What are the benefits of Multi-Task Fine-Tuning in low resource scenarios for biomedical pretrained language models?
3. How does Data Augmentation contribute to training models in low resource scenarios, and what are some popular techniques?
4. In what way does Semi-Supervised Learning utilize pseudo-labeled instances to enhance model training?
5. Can domain-specific ontologies like UMLS be used for Data Augmentation, and if so, how do they contribute to model performance?",None,,"Questions:

1. How does Intermediate Fine-Tuning improve model performance on small target datasets in the biomedical domain?
2. What are the benefits of Multi-Task Fine-Tuning in low resource scenarios for biomedical pretrained language models?
3. How does Data Augmentation contribute to training models in low resource scenarios, and what are some popular techniques?
4. In what way does Semi-Supervised Learning utilize pseudo-labeled instances to enhance model training?
5. Can domain-specific ontologies like UMLS be used for Data Augmentation, and if so, how do they contribute to model performance?

Answer:

(p54.0) Pretraining on large volumes of in-domain text allows the model language representations while fine-tuning allows the model to learn task-specific knowledge. During fine-tuning the model must learn sufficient task-specific knowledge to achieve good results on the task where the input distribution, as well as label space, is different from pretraining [193], [194]. With small target datasets, the models are not able to learn enough task-specific which limits the performance. To over the small size of target datasets, the possible solutions are Intermediate Fine-Tuning -Intermediate fine-tuning on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets [55]- [58], [62], [63], [65], [66] and

(p54.1) Multi-Task Fine-Tuning -Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. Multi-Task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]- [71], [73].

(p54.2) Data Augmentation -Data augmentation helps us to create new training instances from existing instances. These newly creating training instances are close to original training data and helpful in low resource scenarios. Back translation and EDA [195] are the top popular techniques for data augmentation. For example, Wang et al. [159] used back translation to augment the training instances to train the clinical text similarity model. The domain-specific ontologies like UMLS can also be used to augment the training instances [153].

(p54.3) Semi-Supervised Learning -Semi-supervised learning augments the training set with pseudo-labeled instances. The model which is fine-tuned on the original training set is used to label the task-related unlabelled instances. The model is again fine-tuned on the augmented training set and this process is repeated until the model converges [63], [196] . Table 11 contains a brief summary of these approaches.","Questions:



Answer:

(p54.0) Pretraining on large volumes of in-domain text allows the model language representations while fine-tuning allows the model to learn task-specific knowledge. During fine-tuning the model must learn sufficient task-specific knowledge to achieve good results on the task where the input distribution, as well as label space, is different from pretraining [193], [194]. With small target datasets, the models are not able to learn enough task-specific which limits the performance. To over the small size of target datasets, the possible solutions are Intermediate Fine-Tuning -Intermediate fine-tuning on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets [55]- [58], [62], [63], [65], [66] and

(p54.1) Multi-Task Fine-Tuning -Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. Multi-Task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]- [71], [73].

(p54.2) Data Augmentation -Data augmentation helps us to create new training instances from existing instances. These newly creating training instances are close to original training data and helpful in low resource scenarios. Back translation and EDA [195] are the top popular techniques for data augmentation. For example, Wang et al. [159] used back translation to augment the training instances to train the clinical text similarity model. The domain-specific ontologies like UMLS can also be used to augment the training instances [153].

(p54.3) Semi-Supervised Learning -Semi-supervised learning augments the training set with pseudo-labeled instances. The model which is fine-tuned on the original training set is used to label the task-related unlabelled instances. The model is again fine-tuned on the augmented training set and this process is repeated until the model converges [63], [196] . Table 11 contains a brief summary of these approaches."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s55,Robustness to Noise,['p55.0'],"['Transformed based PLMs have achieved the best results in many of the tasks. However, the performance of these models on noisy test instances is limited [197]- [200]. This is because the model is mostly trained on less noisy instances. As these models mostly never encounter noisy instances during fine-tuning, the performance is significantly reduced on noisy instances. Apart from this, the noisy words in the instances are split into several subtokens which affect the model learning. The robustness of models is crucial particularly insensitive domains like biomedical [199], [200]. Two possible solutions are a) CharBERT [28] -replaced the WordPiece based embedding layer with CharCNN based embedding layer. Here word representation is generated from character embeddings using CharCNN. b) Adversarial Training [200] -Here, the training set is augmented with the noisy instances. Training the model on an augmented training set exposes the model to noisy instances and hence the model performs better on noisy instances.']","Transformed based PLMs have achieved the best results in many of the tasks. However, the performance of these models on noisy test instances is limited [197]- [200]. This is because the model is mostly trained on less noisy instances. As these models mostly never encounter noisy instances during fine-tuning, the performance is significantly reduced on noisy instances. Apart from this, the noisy words in the instances are split into several subtokens which affect the model learning. The robustness of models is crucial particularly insensitive domains like biomedical [199], [200]. Two possible solutions are a) CharBERT [28] -replaced the WordPiece based embedding layer with CharCNN based embedding layer. Here word representation is generated from character embeddings using CharCNN. b) Adversarial Training [200] -Here, the training set is augmented with the noisy instances. Training the model on an augmented training set exposes the model to noisy instances and hence the model performs better on noisy instances.","(p55.0) Transformed based PLMs have achieved the best results in many of the tasks. However, the performance of these models on noisy test instances is limited [197]- [200]. This is because the model is mostly trained on less noisy instances. As these models mostly never encounter noisy instances during fine-tuning, the performance is significantly reduced on noisy instances. Apart from this, the noisy words in the instances are split into several subtokens which affect the model learning. The robustness of models is crucial particularly insensitive domains like biomedical [199], [200]. Two possible solutions are a) CharBERT [28] -replaced the WordPiece based embedding layer with CharCNN based embedding layer. Here word representation is generated from character embeddings using CharCNN. b) Adversarial Training [200] -Here, the training set is augmented with the noisy instances. Training the model on an augmented training set exposes the model to noisy instances and hence the model performs better on noisy instances.","[['b201', 'b200', 'b27', 'b198']]","[['b201', 'b200', 'b27', 'b198']]",4,"1. Transformed based PLMs have achieved the best results in many of the tasks.
2. However, the performance of these models on noisy test instances is limited [197]- [200].
3. This is because the model is mostly trained on less noisy instances.
4. As these models mostly never encounter noisy instances during fine-tuning, the performance is significantly reduced on noisy instances.
5. Apart from this, the noisy words in the instances are split into several subtokens which affect the model learning.
6. The robustness of models is crucial particularly insensitive domains like biomedical [199], [200].
7. Two possible solutions are a) CharBERT [28] -replaced the WordPiece based embedding layer with CharCNN based embedding layer.
8. Here word representation is generated from character embeddings using CharCNN.
9. b) Adversarial Training [200] -Here, the training set is augmented with the noisy instances.
10. Training the model on an augmented training set exposes the model to noisy instances and hence the model performs better on noisy instances.","What are the main challenges transformer-based PLMs face when dealing with noisy test instances?
How do noisy words in test instances affect the learning process of transformer-based PLMs?
Why is robustness particularly crucial for transformer-based PLMs in sensitive domains such as biomedical?
How does CharBERT improve the robustness of transformer-based PLMs to noise compared to traditional WordPiece based embedding?
In what way does adversarial training enhance the performance of transformer-based PLMs on noisy instances?","1. What are the main challenges transformer-based PLMs face when dealing with noisy test instances?
2. How do noisy words in test instances affect the learning process of transformer-based PLMs?
3. Why is robustness particularly crucial for transformer-based PLMs in sensitive domains such as biomedical?
4. How does CharBERT improve the robustness of transformer-based PLMs to noise compared to traditional WordPiece based embedding?
5. In what way does adversarial training enhance the performance of transformer-based PLMs on noisy instances?",None,,"Questions:

1. What are the main challenges transformer-based PLMs face when dealing with noisy test instances?
2. How do noisy words in test instances affect the learning process of transformer-based PLMs?
3. Why is robustness particularly crucial for transformer-based PLMs in sensitive domains such as biomedical?
4. How does CharBERT improve the robustness of transformer-based PLMs to noise compared to traditional WordPiece based embedding?
5. In what way does adversarial training enhance the performance of transformer-based PLMs on noisy instances?

Answer:

(p55.0) Transformed based PLMs have achieved the best results in many of the tasks. However, the performance of these models on noisy test instances is limited [197]- [200]. This is because the model is mostly trained on less noisy instances. As these models mostly never encounter noisy instances during fine-tuning, the performance is significantly reduced on noisy instances. Apart from this, the noisy words in the instances are split into several subtokens which affect the model learning. The robustness of models is crucial particularly insensitive domains like biomedical [199], [200]. Two possible solutions are a) CharBERT [28] -replaced the WordPiece based embedding layer with CharCNN based embedding layer. Here word representation is generated from character embeddings using CharCNN. b) Adversarial Training [200] -Here, the training set is augmented with the noisy instances. Training the model on an augmented training set exposes the model to noisy instances and hence the model performs better on noisy instances.","Questions:



Answer:

(p55.0) Transformed based PLMs have achieved the best results in many of the tasks. However, the performance of these models on noisy test instances is limited [197]- [200]. This is because the model is mostly trained on less noisy instances. As these models mostly never encounter noisy instances during fine-tuning, the performance is significantly reduced on noisy instances. Apart from this, the noisy words in the instances are split into several subtokens which affect the model learning. The robustness of models is crucial particularly insensitive domains like biomedical [199], [200]. Two possible solutions are a) CharBERT [28] -replaced the WordPiece based embedding layer with CharCNN based embedding layer. Here word representation is generated from character embeddings using CharCNN. b) Adversarial Training [200] -Here, the training set is augmented with the noisy instances. Training the model on an augmented training set exposes the model to noisy instances and hence the model performs better on noisy instances."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s58,Quality Sequence Representation,['p58.0'],"['For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation. According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information. The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space. Finally, a softmax is applied to convert it into a vector of probabilities. However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector.']","For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation. According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information. The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space. Finally, a softmax is applied to convert it into a vector of probabilities. However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector.","(p58.0) For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation. According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information. The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space. Finally, a softmax is applied to convert it into a vector of probabilities. However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector.","[['b171', 'b136', 'b176', 'b1', 'b159', 'b177', 'b56']]","[['b171', 'b136', 'b176', 'b1', 'b159', 'b177', 'b56']]",7,"1. For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation.
2. According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information.
3. The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space.
4. Finally, a softmax is applied to convert it into a vector of probabilities.
5. However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector.","How does the final hidden vector of the special token contribute to sequence representation in transformer-based models?
What alternative methods have recent studies shown to improve final sequence representation beyond the special token vector approach?
How does the application of a softmax function to the final hidden vector influence the output in text classification models?
What are the advantages of using max-pooling, attention, or hierarchical convolution layers over the special token vector for sequence representation?
In what way does the size of the label space impact the projection of the final hidden vector in transformer-based text classification tasks?","1. How does the final hidden vector of the special token contribute to sequence representation in transformer-based models?
2. What alternative methods have recent studies shown to improve final sequence representation beyond the special token vector approach?
3. How does the application of a softmax function to the final hidden vector influence the output in text classification models?
4. What are the advantages of using max-pooling, attention, or hierarchical convolution layers over the special token vector for sequence representation?
5. In what way does the size of the label space impact the projection of the final hidden vector in transformer-based text classification tasks?",None,,"Questions:

1. How does the final hidden vector of the special token contribute to sequence representation in transformer-based models?
2. What alternative methods have recent studies shown to improve final sequence representation beyond the special token vector approach?
3. How does the application of a softmax function to the final hidden vector influence the output in text classification models?
4. What are the advantages of using max-pooling, attention, or hierarchical convolution layers over the special token vector for sequence representation?
5. In what way does the size of the label space impact the projection of the final hidden vector in transformer-based text classification tasks?

Answer:

(p58.0) For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation. According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information. The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space. Finally, a softmax is applied to convert it into a vector of probabilities. However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector.","Questions:



Answer:

(p58.0) For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation. According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information. The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space. Finally, a softmax is applied to convert it into a vector of probabilities. However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s60,Mitigating Bias,['p60.0'],"['With the success of deep learning models in various tasks, deep learning-based systems are used to automate the decisions like department recommendation [112], disease prediction [31], [32] etc. However, these models are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. Real-world datasets have a bias in many forms. It can be based on various attributes like gender, age, ethnicity, and marital status. These attributes are considered as protected or sensitive [201]. For example, in the MIMIC-III dataset [88] a) heart disease is more common in males compared to females-an example of gender bias b) there are fewer clinical studies involving black patients compared to other groups -an example of ethnicity bias. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. There are few works that identified and addressed bias in transformer-based biomedical language models. Zhang et al. [127] using a simple word competition task showed that SciBERT [105] exhibits ethnicity bias. Moreover, the authors showed SciBERT model when further-pretrained on clinical notes exhibits performance differences for different protected attributes. They further showed that adversarial pretraining debiasing has little impact in reducing bias. Minot et al. [202] proposed an approach based on data augmentation to identify and reduce gender bias in patient notes. This is an area that needs to be explored further to improve reduce bias and improve the fairness in model decisions.']","With the success of deep learning models in various tasks, deep learning-based systems are used to automate the decisions like department recommendation [112], disease prediction [31], [32] etc. However, these models are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. Real-world datasets have a bias in many forms. It can be based on various attributes like gender, age, ethnicity, and marital status. These attributes are considered as protected or sensitive [201]. For example, in the MIMIC-III dataset [88] a) heart disease is more common in males compared to females-an example of gender bias b) there are fewer clinical studies involving black patients compared to other groups -an example of ethnicity bias. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. There are few works that identified and addressed bias in transformer-based biomedical language models. Zhang et al. [127] using a simple word competition task showed that SciBERT [105] exhibits ethnicity bias. Moreover, the authors showed SciBERT model when further-pretrained on clinical notes exhibits performance differences for different protected attributes. They further showed that adversarial pretraining debiasing has little impact in reducing bias. Minot et al. [202] proposed an approach based on data augmentation to identify and reduce gender bias in patient notes. This is an area that needs to be explored further to improve reduce bias and improve the fairness in model decisions.","(p60.0) With the success of deep learning models in various tasks, deep learning-based systems are used to automate the decisions like department recommendation [112], disease prediction [31], [32] etc. However, these models are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. Real-world datasets have a bias in many forms. It can be based on various attributes like gender, age, ethnicity, and marital status. These attributes are considered as protected or sensitive [201]. For example, in the MIMIC-III dataset [88] a) heart disease is more common in males compared to females-an example of gender bias b) there are fewer clinical studies involving black patients compared to other groups -an example of ethnicity bias. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. There are few works that identified and addressed bias in transformer-based biomedical language models. Zhang et al. [127] using a simple word competition task showed that SciBERT [105] exhibits ethnicity bias. Moreover, the authors showed SciBERT model when further-pretrained on clinical notes exhibits performance differences for different protected attributes. They further showed that adversarial pretraining debiasing has little impact in reducing bias. Minot et al. [202] proposed an approach based on data augmentation to identify and reduce gender bias in patient notes. This is an area that needs to be explored further to improve reduce bias and improve the fairness in model decisions.","[['b124', 'b203', 'b202', 'b31', 'b112', 'b30', 'b127', 'b105', 'b88', 'b126']]","[['b124', 'b203', 'b202', 'b31', 'b112', 'b30', 'b127', 'b105', 'b88', 'b126']]",10,"1. With the success of deep learning models in various tasks, deep learning-based systems are used to automate the decisions like department recommendation [112], disease prediction [31], [32] etc. However, these models are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others.
2. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126].
3. Real-world datasets have a bias in many forms.
4. It can be based on various attributes like gender, age, ethnicity, and marital status.
5. These attributes are considered as protected or sensitive [201].
6. For example, in the MIMIC-III dataset [88] a) heart disease is more common in males compared to females-an example of gender bias b) there are fewer clinical studies involving black patients compared to other groups -an example of ethnicity bias.
7. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group.
8. There are few works that identified and addressed bias in transformer-based biomedical language models.
9. Zhang et al. [127] using a simple word competition task showed that SciBERT [105] exhibits ethnicity bias.
10. Moreover, the authors showed SciBERT model when further-pretrained on clinical notes exhibits performance differences for different protected attributes.
11. They further showed that adversarial pretraining debiasing has little impact in reducing bias.
12. Minot et al. [202] proposed an approach based on data augmentation to identify and reduce gender bias in patient notes.
13. This is an area that needs to be explored further to improve reduce bias and improve the fairness in model decisions.","How does gender bias specifically manifest in the MIMIC-III dataset?
What are the primary reasons for unfair decisions in deep learning-based systems according to the section?
What strategies have been proposed to mitigate bias in transformer-based biomedical language models?
How effective is adversarial pretraining in reducing bias in transformer-based models?
What approach has been suggested for reducing gender bias in patient notes, and what implications does this have for future research in bias mitigation?","1. How does gender bias specifically manifest in the MIMIC-III dataset?
2. What are the primary reasons for unfair decisions in deep learning-based systems according to the section?
3. What strategies have been proposed to mitigate bias in transformer-based biomedical language models?
4. How effective is adversarial pretraining in reducing bias in transformer-based models?
5. What approach has been suggested for reducing gender bias in patient notes, and what implications does this have for future research in bias mitigation?",None,,"Questions:

1. How does gender bias specifically manifest in the MIMIC-III dataset?
2. What are the primary reasons for unfair decisions in deep learning-based systems according to the section?
3. What strategies have been proposed to mitigate bias in transformer-based biomedical language models?
4. How effective is adversarial pretraining in reducing bias in transformer-based models?
5. What approach has been suggested for reducing gender bias in patient notes, and what implications does this have for future research in bias mitigation?

Answer:

(p60.0) With the success of deep learning models in various tasks, deep learning-based systems are used to automate the decisions like department recommendation [112], disease prediction [31], [32] etc. However, these models are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. Real-world datasets have a bias in many forms. It can be based on various attributes like gender, age, ethnicity, and marital status. These attributes are considered as protected or sensitive [201]. For example, in the MIMIC-III dataset [88] a) heart disease is more common in males compared to females-an example of gender bias b) there are fewer clinical studies involving black patients compared to other groups -an example of ethnicity bias. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. There are few works that identified and addressed bias in transformer-based biomedical language models. Zhang et al. [127] using a simple word competition task showed that SciBERT [105] exhibits ethnicity bias. Moreover, the authors showed SciBERT model when further-pretrained on clinical notes exhibits performance differences for different protected attributes. They further showed that adversarial pretraining debiasing has little impact in reducing bias. Minot et al. [202] proposed an approach based on data augmentation to identify and reduce gender bias in patient notes. This is an area that needs to be explored further to improve reduce bias and improve the fairness in model decisions.","Questions:



Answer:

(p60.0) With the success of deep learning models in various tasks, deep learning-based systems are used to automate the decisions like department recommendation [112], disease prediction [31], [32] etc. However, these models are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. Real-world datasets have a bias in many forms. It can be based on various attributes like gender, age, ethnicity, and marital status. These attributes are considered as protected or sensitive [201]. For example, in the MIMIC-III dataset [88] a) heart disease is more common in males compared to females-an example of gender bias b) there are fewer clinical studies involving black patients compared to other groups -an example of ethnicity bias. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. There are few works that identified and addressed bias in transformer-based biomedical language models. Zhang et al. [127] using a simple word competition task showed that SciBERT [105] exhibits ethnicity bias. Moreover, the authors showed SciBERT model when further-pretrained on clinical notes exhibits performance differences for different protected attributes. They further showed that adversarial pretraining debiasing has little impact in reducing bias. Minot et al. [202] proposed an approach based on data augmentation to identify and reduce gender bias in patient notes. This is an area that needs to be explored further to improve reduce bias and improve the fairness in model decisions."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s64,Benchmarks,['p64.0'],"['In general, a benchmark is a tool to evaluate the performance of models across different NLP tasks. A benchmark is required because we expect the pre-trained language models to be general and robust i.e., the models perform well across tasks rather than on one or two specific tasks. A benchmark with one or more datasets for multiple NLP tasks helps to assess the general ability and robustness of models. In general domain, we have a number of benchmarks like GLUE [207] and Super-GLUE [208] (general language understanding), XGLUE [209] (cross lingual language understanding) and LinCE [210] (code switching). In biomedical domain there are three benchmarks namely BLUE [18], BLURB [20] and ChineseBLUE [48]. BLUE introduced by Peng et al. [18] contains ten datasets for five biomedical NLP tasks, while BLURB contains thirteen datasets for six tasks and ChineseBLUE contains eight tasks with nine datasets. BLUE and ChineseBLUE include both EHR and scientific literature-based datasets, while BLURB includes only biomedical scientific literature-based datasets. The semantics of EHR and medical social media texts are different from biomedical scientific literature. So, there is a need of exclusive benchmarks for EHR and medical social media-based datasets.']","In general, a benchmark is a tool to evaluate the performance of models across different NLP tasks. A benchmark is required because we expect the pre-trained language models to be general and robust i.e., the models perform well across tasks rather than on one or two specific tasks. A benchmark with one or more datasets for multiple NLP tasks helps to assess the general ability and robustness of models. In general domain, we have a number of benchmarks like GLUE [207] and Super-GLUE [208] (general language understanding), XGLUE [209] (cross lingual language understanding) and LinCE [210] (code switching). In biomedical domain there are three benchmarks namely BLUE [18], BLURB [20] and ChineseBLUE [48]. BLUE introduced by Peng et al. [18] contains ten datasets for five biomedical NLP tasks, while BLURB contains thirteen datasets for six tasks and ChineseBLUE contains eight tasks with nine datasets. BLUE and ChineseBLUE include both EHR and scientific literature-based datasets, while BLURB includes only biomedical scientific literature-based datasets. The semantics of EHR and medical social media texts are different from biomedical scientific literature. So, there is a need of exclusive benchmarks for EHR and medical social media-based datasets.","(p64.0) In general, a benchmark is a tool to evaluate the performance of models across different NLP tasks. A benchmark is required because we expect the pre-trained language models to be general and robust i.e., the models perform well across tasks rather than on one or two specific tasks. A benchmark with one or more datasets for multiple NLP tasks helps to assess the general ability and robustness of models. In general domain, we have a number of benchmarks like GLUE [207] and Super-GLUE [208] (general language understanding), XGLUE [209] (cross lingual language understanding) and LinCE [210] (code switching). In biomedical domain there are three benchmarks namely BLUE [18], BLURB [20] and ChineseBLUE [48]. BLUE introduced by Peng et al. [18] contains ten datasets for five biomedical NLP tasks, while BLURB contains thirteen datasets for six tasks and ChineseBLUE contains eight tasks with nine datasets. BLUE and ChineseBLUE include both EHR and scientific literature-based datasets, while BLURB includes only biomedical scientific literature-based datasets. The semantics of EHR and medical social media texts are different from biomedical scientific literature. So, there is a need of exclusive benchmarks for EHR and medical social media-based datasets.","[['b210', 'b17', 'b47', 'b19', 'b208', 'b209', 'b211']]","[['b210', 'b17', 'b47', 'b19', 'b208', 'b209', 'b211']]",7,"1. In general, a benchmark is a tool to evaluate the performance of models across different NLP tasks.
2. A benchmark is required because we expect the pre-trained language models to be general and robust i.e., the models perform well across tasks rather than on one or two specific tasks.
3. A benchmark with one or more datasets for multiple NLP tasks helps to assess the general ability and robustness of models.
4. In general domain, we have a number of benchmarks like GLUE [207] and Super-GLUE [208] (general language understanding), XGLUE [209] (cross lingual language understanding) and LinCE [210] (code switching).
5. In biomedical domain there are three benchmarks namely BLUE [208]0, BLURB [20] and ChineseBLUE [48].
6. BLUE introduced by Peng et al. [208]0 contains ten datasets for five biomedical NLP tasks, while BLURB contains thirteen datasets for six tasks and ChineseBLUE contains eight tasks with nine datasets.
7. BLUE and ChineseBLUE include both EHR and scientific literature-based datasets, while BLURB includes only biomedical scientific literature-based datasets.
8. The semantics of EHR and medical social media texts are different from biomedical scientific literature.
9. So, there is a need of exclusive benchmarks for EHR and medical social media-based datasets.","What is the purpose of benchmarks in evaluating the performance of NLP models in the biomedical domain?
How do benchmarks contribute to assessing the general ability and robustness of pre-trained language models across different NLP tasks?
What distinguishes the benchmarks GLUE, Super-GLUE, XGLUE, and LinCE in the general domain from BLUE, BLURB, and ChineseBLUE in the biomedical domain?
Why is there a need for exclusive benchmarks for EHR and medical social media-based datasets in the biomedical field?
How do the datasets included in BLUE, BLURB, and ChineseBLUE benchmarks differ in terms of their focus on EHR, scientific literature, and medical social media texts?","1. What is the purpose of benchmarks in evaluating the performance of NLP models in the biomedical domain?
2. How do benchmarks contribute to assessing the general ability and robustness of pre-trained language models across different NLP tasks?
3. What distinguishes the benchmarks GLUE, Super-GLUE, XGLUE, and LinCE in the general domain from BLUE, BLURB, and ChineseBLUE in the biomedical domain?
4. Why is there a need for exclusive benchmarks for EHR and medical social media-based datasets in the biomedical field?
5. How do the datasets included in BLUE, BLURB, and ChineseBLUE benchmarks differ in terms of their focus on EHR, scientific literature, and medical social media texts?",None,,"Questions:

1. What is the purpose of benchmarks in evaluating the performance of NLP models in the biomedical domain?
2. How do benchmarks contribute to assessing the general ability and robustness of pre-trained language models across different NLP tasks?
3. What distinguishes the benchmarks GLUE, Super-GLUE, XGLUE, and LinCE in the general domain from BLUE, BLURB, and ChineseBLUE in the biomedical domain?
4. Why is there a need for exclusive benchmarks for EHR and medical social media-based datasets in the biomedical field?
5. How do the datasets included in BLUE, BLURB, and ChineseBLUE benchmarks differ in terms of their focus on EHR, scientific literature, and medical social media texts?

Answer:

(p64.0) In general, a benchmark is a tool to evaluate the performance of models across different NLP tasks. A benchmark is required because we expect the pre-trained language models to be general and robust i.e., the models perform well across tasks rather than on one or two specific tasks. A benchmark with one or more datasets for multiple NLP tasks helps to assess the general ability and robustness of models. In general domain, we have a number of benchmarks like GLUE [207] and Super-GLUE [208] (general language understanding), XGLUE [209] (cross lingual language understanding) and LinCE [210] (code switching). In biomedical domain there are three benchmarks namely BLUE [18], BLURB [20] and ChineseBLUE [48]. BLUE introduced by Peng et al. [18] contains ten datasets for five biomedical NLP tasks, while BLURB contains thirteen datasets for six tasks and ChineseBLUE contains eight tasks with nine datasets. BLUE and ChineseBLUE include both EHR and scientific literature-based datasets, while BLURB includes only biomedical scientific literature-based datasets. The semantics of EHR and medical social media texts are different from biomedical scientific literature. So, there is a need of exclusive benchmarks for EHR and medical social media-based datasets.","Questions:



Answer:

(p64.0) In general, a benchmark is a tool to evaluate the performance of models across different NLP tasks. A benchmark is required because we expect the pre-trained language models to be general and robust i.e., the models perform well across tasks rather than on one or two specific tasks. A benchmark with one or more datasets for multiple NLP tasks helps to assess the general ability and robustness of models. In general domain, we have a number of benchmarks like GLUE [207] and Super-GLUE [208] (general language understanding), XGLUE [209] (cross lingual language understanding) and LinCE [210] (code switching). In biomedical domain there are three benchmarks namely BLUE [18], BLURB [20] and ChineseBLUE [48]. BLUE introduced by Peng et al. [18] contains ten datasets for five biomedical NLP tasks, while BLURB contains thirteen datasets for six tasks and ChineseBLUE contains eight tasks with nine datasets. BLUE and ChineseBLUE include both EHR and scientific literature-based datasets, while BLURB includes only biomedical scientific literature-based datasets. The semantics of EHR and medical social media texts are different from biomedical scientific literature. So, there is a need of exclusive benchmarks for EHR and medical social media-based datasets."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Linguistics, Medicine, Computer Science",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s65,Intrinsic Probes,['p65.0'],"['During pretraining, PLMs learn syntactic, semantic knowledge along with factual and common-sense knowledge available in the pretraining corpus [14]. Intrinsic probes through light on the knowledge learned by PLMs during pretraining. In general NLP, researchers proposed several intrinsic probes like LAMA, Negated and Misprimed LAMA [211], XLAMA [212], X-FACTR [213], MickeyProbe [214] to understand the knowledge encoded in pretrained models. For example, LAMA [211] probes the factual and common-sense knowledge of English pretrained models, while X-FACTR [213] probes the factual knowledge of multi-lingual pretrained models. However, there is no such intrinsic probes in Biomedical domain to through light on the knowledge learned by BPLMs during pretraining. This is an area which requires much attention from Biomedical NLP community.']","During pretraining, PLMs learn syntactic, semantic knowledge along with factual and common-sense knowledge available in the pretraining corpus [14]. Intrinsic probes through light on the knowledge learned by PLMs during pretraining. In general NLP, researchers proposed several intrinsic probes like LAMA, Negated and Misprimed LAMA [211], XLAMA [212], X-FACTR [213], MickeyProbe [214] to understand the knowledge encoded in pretrained models. For example, LAMA [211] probes the factual and common-sense knowledge of English pretrained models, while X-FACTR [213] probes the factual knowledge of multi-lingual pretrained models. However, there is no such intrinsic probes in Biomedical domain to through light on the knowledge learned by BPLMs during pretraining. This is an area which requires much attention from Biomedical NLP community.","(p65.0) During pretraining, PLMs learn syntactic, semantic knowledge along with factual and common-sense knowledge available in the pretraining corpus [14]. Intrinsic probes through light on the knowledge learned by PLMs during pretraining. In general NLP, researchers proposed several intrinsic probes like LAMA, Negated and Misprimed LAMA [211], XLAMA [212], X-FACTR [213], MickeyProbe [214] to understand the knowledge encoded in pretrained models. For example, LAMA [211] probes the factual and common-sense knowledge of English pretrained models, while X-FACTR [213] probes the factual knowledge of multi-lingual pretrained models. However, there is no such intrinsic probes in Biomedical domain to through light on the knowledge learned by BPLMs during pretraining. This is an area which requires much attention from Biomedical NLP community.","[['b213', 'b214', 'b13', 'b215', 'b212']]","[['b213', 'b214', 'b13', 'b215', 'b212']]",5,"1. During pretraining, PLMs learn syntactic, semantic knowledge along with factual and common-sense knowledge available in the pretraining corpus [14].
2. Intrinsic probes through light on the knowledge learned by PLMs during pretraining.
3. In general NLP, researchers proposed several intrinsic probes like LAMA, Negated and Misprimed LAMA [211], XLAMA [212], X-FACTR [213], MickeyProbe [214] to understand the knowledge encoded in pretrained models.
4. For example, LAMA [211] probes the factual and common-sense knowledge of English pretrained models, while X-FACTR [213] probes the factual knowledge of multi-lingual pretrained models.
5. However, there is no such intrinsic probes in Biomedical domain to through light on the knowledge learned by BPLMs during pretraining.
6. This is an area which requires much attention from Biomedical NLP community.","What are intrinsic probes and how do they help in understanding the knowledge encoded in pretrained language models?
How do LAMA and X-FACTR intrinsic probes differ in their approach to probing pretrained models?
Why is there a lack of intrinsic probes specifically designed for the Biomedical domain in evaluating BPLMs?
What types of knowledge do PLMs learn during pretraining according to the section?
What has been the role of the general NLP research community in developing intrinsic probes for understanding pretrained models?","1. What are intrinsic probes and how do they help in understanding the knowledge encoded in pretrained language models?
2. How do LAMA and X-FACTR intrinsic probes differ in their approach to probing pretrained models?
3. Why is there a lack of intrinsic probes specifically designed for the Biomedical domain in evaluating BPLMs?
4. What types of knowledge do PLMs learn during pretraining according to the section?
5. What has been the role of the general NLP research community in developing intrinsic probes for understanding pretrained models?",None,,"Questions:

1. What are intrinsic probes and how do they help in understanding the knowledge encoded in pretrained language models?
2. How do LAMA and X-FACTR intrinsic probes differ in their approach to probing pretrained models?
3. Why is there a lack of intrinsic probes specifically designed for the Biomedical domain in evaluating BPLMs?
4. What types of knowledge do PLMs learn during pretraining according to the section?
5. What has been the role of the general NLP research community in developing intrinsic probes for understanding pretrained models?

Answer:

(p65.0) During pretraining, PLMs learn syntactic, semantic knowledge along with factual and common-sense knowledge available in the pretraining corpus [14]. Intrinsic probes through light on the knowledge learned by PLMs during pretraining. In general NLP, researchers proposed several intrinsic probes like LAMA, Negated and Misprimed LAMA [211], XLAMA [212], X-FACTR [213], MickeyProbe [214] to understand the knowledge encoded in pretrained models. For example, LAMA [211] probes the factual and common-sense knowledge of English pretrained models, while X-FACTR [213] probes the factual knowledge of multi-lingual pretrained models. However, there is no such intrinsic probes in Biomedical domain to through light on the knowledge learned by BPLMs during pretraining. This is an area which requires much attention from Biomedical NLP community.","Questions:



Answer:

(p65.0) During pretraining, PLMs learn syntactic, semantic knowledge along with factual and common-sense knowledge available in the pretraining corpus [14]. Intrinsic probes through light on the knowledge learned by PLMs during pretraining. In general NLP, researchers proposed several intrinsic probes like LAMA, Negated and Misprimed LAMA [211], XLAMA [212], X-FACTR [213], MickeyProbe [214] to understand the knowledge encoded in pretrained models. For example, LAMA [211] probes the factual and common-sense knowledge of English pretrained models, while X-FACTR [213] probes the factual knowledge of multi-lingual pretrained models. However, there is no such intrinsic probes in Biomedical domain to through light on the knowledge learned by BPLMs during pretraining. This is an area which requires much attention from Biomedical NLP community."
