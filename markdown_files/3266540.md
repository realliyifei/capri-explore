# Probabilistic Inductive Inference: a Survey *

CorpusID: 3266540
 
tags: #Mathematics, #Computer_Science

URL: [https://www.semanticscholar.org/paper/8d19716f9dd0c05b762030c2a87ca2da48574a53](https://www.semanticscholar.org/paper/8d19716f9dd0c05b762030c2a87ca2da48574a53)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Probabilistic Inductive Inference: a Survey *
15 Feb 1999

Andris Ambainis ambainis@cs.berkeley.edu 
Computer Science Division
University of California Berkeley
94720-1776CAU.S.A

Probabilistic Inductive Inference: a Survey *
15 Feb 1999
Inductive inference is a recursion-theoretic theory of learning, first developed by E. M.Gold (1967). This paper surveys developments in probabilistic inductive inference. We mainly focus on finite inference of recursive functions, since this simple paradigm has produced the most interesting (and most complex) results.

# Introduction

Understanding the process of learning has always fascinated scientists. There are several computational theories of learning. One of the oldest theories is inductive inference established by Gold [15]. This theory considers the process of learning from a viewpoint of the computability theory. Unlike other theories of learning (for example, PAC-learning [37,20]), inductive inference does not make probabilistic assumptions about the world. However, probabilistic algorithms appear in inductive inference and the study of probabilistic inductive inference creates a lot of interesting problems with elements of both computability theory and combinatorics. In this paper, we survey some of these problems.

We start with a general introduction to inductive inference. Learning can be considered as a process of gathering information about an unknown object, processing this information and obtaining a description of the unknown object. Ideally, we would like to obtain a complete description of the object.

In the theory of inductive inference, objects are arbitrary recursive (computable) functions (or recursively enumerable languages). The reason is that any algorithmic behavior can be represented as a recursive (computable) function and, hence, we obtain a model that includes any learning situation. Throughout this paper, we only consider learning of total recursive functions (except section 8 where we consider recursively enumerable languages).

The natural data about a function f are its values f (0), f (1), f (2), . . . and the natural representation of these data is the sequence 0, f (0) , 1, f (1) , 2, f (2) , . . .. The most general type of description for a computable function is a program in a universal programming language. Any other description can be converted to this form.

This gives us the following learning model. A learning algorithm receives the values of an unknown function f in the natural order: 0, f (0) , 1, f (1) , 2, f (2) , . . . and produces a program h. The algorithm succeeds on f if the program h computes f .

We will compare the classes of functions identifiable by probabilistic algorithms with different probabilities of correct answer.


# Definitions

Next, we introduce the formal notation and definitions used in this paper. For more background information, see [33] for recursive function (computability) theory, [34,25] for set theory and [5,30] for inductive inference.

A learning machine is an algorithmic device that reads values of a function f : f (0), f (1), . . .. Having seen finitely many values of the function it can output a conjecture. A conjecture is a program in some fixed acceptable programming system [26,33].

It makes sense to allow the learning machine to revise its conjecture. In this case, the last conjecture output by the algorithm should be correct but intermediate conjectures may be wrong. This increases the power of the learning algorithm. Also, this can be motivated by the fact that humans learning a complex behavior (for example, foreign language or driving), do not learn it completely at once. Rather, they first learn a part of it, then extend it by learning more. This model where an unlimited number of conjectures is allowed is called learning in the limit [15].
Definition 1 [15] (a) A deterministic learning machine M EX-identifies (identifies in the limit) a function f if, given f (0), f (1), . . . it outputs a sequence of programs h 0 , h 1 , . . . such that, for some i, h i = h i+1 = h i+2 = . . . and h i is a program computing f . (b) M EX-identifies a set of functions U if it EX-identifies all f ∈ U.
(c) EX denotes the set of all sets U that are EX-identifiable.

A model where only one conjecture is allowed and it must be correct has been studied as well. This model is called finite learning. It is more restricted and (in most contexts) simpler. The problems that we consider are fairly simple for probabilistic learning in the limit (cf. [31,32,35]) but are much more complicated (and more interesting) for finite learning. Therefore, in this survey, we focus on finite learning. References to work on other models of inductive inference are given in sections 8 and 9. Next, we define identification by probabilistic machines. We define it for FIN but the definition carries over to EX and other paradigms as well.  It is easy to see that [r, s]FIN ⊆ r s FIN. (Just choose one of the machines in the team uniformly at random and simulate.) In some cases, the opposite is also true and every probabilistic machine can be simulated by a team.

The main goal of research in probabilistic inductive inference is determining how p FIN depends on the accepting probability p. Formally, it means describing the probability hierarchy.


## Definition 5 The probability hierarchy for FIN is the set of all points p such that there is
U ∈ p FIN but U / ∈ p + ǫ FIN for ǫ > 0.

# Explicit results for FIN

Probabilistic FIN-identification was first studied by Freivalds [14]. He showed that any probabilistic learning machine with the probability of correct answer above 2/3 can be replaced by an equivalent deterministic machine. He also characterized the probability hierarchy for FIN between 1/2 and 2/3.


## Theorem 1 [14]

(a) If p > 2/3, then p FIN = FIN.
(b) 2/3 F IN = F IN. (c) If n/(2n − 1) ≥ p > (n + 1)/(2n + 1), then p FIN = n/(2n − 1) FIN = [n, 2n − 1]FIN. (d) (n + 1)/(2n + 1) FIN = n/(2n − 1) FIN.
It also makes sense to consider probabilistic algorithms with the probability of correct answer 1/2 and below because there are infinitely many outputs and, hence, even designing an algorithm that gives the correct answer with probability ǫ (for an arbitrarily small fixed ǫ > 0) may be nontrivial. Here, the first result was a surprising discovery that a "2 out of 4" team is more powerful than a "1 out of 2" team. We see that the power of a team depends not only on the ratio of machines that must succeed but also on the number of machines in the team.

The next step was moving below probability 1/2.
Theorem 3 [12] Let p 0 = 1 2 , p 1 = 24 49 , p 2 = 20 41 , p 3 = 18 37 , p 4 = 17 35 . Then, for all i ∈ {0, 1, 2, 3} (a) For all x ∈]p i+1 , p i ], x FIN = p i FIN, and (b) p i FIN = p i+1 FIN.
Each of these cutpoints was proven separately and there seemed to be no formula or unifying proof argument connecting them. It took several years to obtain a more general result. 


## Definition 7 (a) A set of functions U is PFIN-identifiable if there is a Popperian machine

M that FIN-identifies U.


## (b) PFIN denotes the collection of all PFIN-identifiable sets.

Probabilistic and team PFIN-identification are introduced similarly. It is important that the requirement about learners outputting only programs computing total recursive functions is absolute, i.e.

1. All conjectures of all machines in a PFIN-team must be programs computing total recursive functions.


## 2.

A probabilistic PFIN-machine is not allowed to output a program which does not compute a total recursive function even with a very small probability.

Daley, Kalyanasundaram and Velauthapillai [10,8] proved counterparts of Theorems 1, 2 and 3 for PFIN. The situation for probabilities greater than or equal to 1/2 was precisely the same as for FIN, only the proofs became simpler. For probabilities smaller than 1/2, two sequences of points where the power of probabilistic machines changed were discovered. The first, 4n 9n−2 , started at 12 25 and converged to 4 9 .

Theorem 5 [10] (a) The probability hierarchy for PFIN in the interval
[ 1 2 , 1] is { n 2n−1 |n ≥ 1}. (b) The probability hierarchy for PFIN in the interval [ 4 9 , 1 2 ] is { 4n 9n−2 |n ≥ 2}.
The second sequence was more complicated. It was actually a union of three simpler sequences corresponding to three different ways how machines in a team can behave.


## Theorem 6 [8] The probability hierarchy for PFIN in the interval
[ 3 7 , 4 9 ] is { 6n 14n−3 |n ≥ 6} ∪ { 3n 7n−1 |n ≥ 12} ∪ { 8n 19n−4 |4 ≤ n ≤ 11}.
However, even for Popperian learning, things were getting more complicated as the probabilities decreased (this can be observed both by just comparing the sequences of probabilities in Theorems 5 and 6 and by looking at the arguments that were used to prove these theorems). As result of that, the authors of [8] wrote that the prospects of determining all cutpoints are bleak even for the interval [2/5, 1/2].


# General results for PFIN

An alternative approach was proposed in [1]. Instead of trying to find all cutpoints explicitly, [1] focused on studying the general properties of the whole probability structure.

The first step was describing existing diagonalization constructions (i.e. constructions proving that there is U ∈ p PFIN such that U / ∈ p + ǫ PFIN for ǫ > 0) in a general form. This led to a conjecture that P P F IN is equal to the set A defined as follows.  . . . ? 4 9 . . . 3 7 . . . Indeed, A = P P F IN and the first step in proving that was observing some structural properties of this set. Figure 1 shows the known parts of probability hierarchies for EX, FIN and PFIN. It is easy to see that all are well-ordered in decreasing order. The set A defined above is well-ordered as well.


## Definition 8 [34, 25] A set A is well-ordered if there is no infinite strictly increasing sequence of elements of A. A set A is well-ordered in decreasing order if there is no infinite strictly increasing sequence of elements of A.


## Theorem 8 [1] The set A is well-ordered and has a system of notations.

A system of notations is an algorithmic description for a well-ordered set. It allows to find preceding elements, given one element. This notion was introduced by Kleene for constructive ordinals [21] and extended to sets of reals (like A) in [1]. Well-orderedness is crucial because it allows to use induction over elements of the set A. Having the system of notations is important to make this induction algorithmic. Using well-orderedness and system of notations, [1] showed the following result.

Theorem 9 [1] Let p ∈ A and p ′ < p be such that there is no p ′′ ∈ A with p ′ ≤ p ′′ < p. Then, p PFIN = p ′ PFIN.
Corollary 1 [1] A = P P F IN .
This approach gives two other interesting results.

Theorem 10 [1] The probability structure of PFIN is decidable, i.e. there is an algorithm that receives two probabilities p 1 and p 2 and answers whether p 1 PFIN = p 2 PFIN.
Theorem 11 [1] Let p ∈ P P F IN . Then, there is a k such that [pk, k]PFIN = p PFIN.
Thus, teams of different size can have different learning power (the counterpart of Theorem 2 in [8]) but we always have the "best" team size such that team of this size can simulate any probabilistic machine (and hence, team of any other size with the same success ratio).

Finally, it is also possible to determine the precise ordering type of the probability hierarchy. The table below shows how the complexity of the ordering increases when probabilities decrease.

Interval Ordering type of the probability hierarchy
[ 1 2 , 1] ω [ 4 9 , 1] 2ω [ 3 7 , 1] 3ω [ 2 5 , 1] ω 2 [ 3 8 , 1] ω 3 [ 1 3 , 1] ω ω [ 1 4 , 1] ω ω ω [0, 1] ǫ 0
ω is the ordering type corresponding to a single infinite sequence (2/3, 3/5, 4/7, . . .), kω is the ordering type of a set consisting of k infinite sequences. ω 2 is the ordering type of a set consisting of infinite sequence of sequences and ω 3 is the ordering type of an infinite sequence of ω 2 -type sets. ω ω is the limit of ω, ω 2 , ω 3 , . . .. Further ordering types can be defined similarly [34,25]. The last one, ǫ 0 , is the limit of ω, ω ω , ω ω ω , . . . and is considered to be so big that it is hard to find any intuitive description for it 1 . This shows that the explored part of PFIN-hierarchy (the interval [ 3 7 , 1], the ordering type 3ω) is very simple compared to the entire hierarchy. This result can be also considered as a partial explanation why it is unrealistic to find explicit values for all points in the probability hierarchy.


# General results for FIN?

An easy corollary of results in [1] is
Theorem 12 If p 1 PFIN = p 2 PFIN, then p 1 FIN = p 2 FIN.
Thus, any diagonalization argument that works for PFIN will work for FIN as well. This means that the probability hierarchy for FIN is at least as complicated as for PFIN. (In fact, it is more complicated because there are points (like 24/49) that are not contained in the PFIN hierarchy but appear in the FIN hierarchy.)

There have been several attempts to move beyond explicit probabilities and to find general proof methods for FIN. Daley and Kalyanasundaram [9,11] have developed a set of reduction arguments (techniques to reduce the problems about inclusions for smaller probabilities to already solved problems for inclusion at bigger probabilities). These arguments were essential to proving Theorem 4. They also were able to explain "the strange probabilities" of Theorem 3. Yet, the number of cases they had to handle is huge and, for further progress, even more general techniques are necessary.

Similar reduction arguments for PFIN [8] were the foundation for general results of [1]. So, we may expect that methods of [11] could serve as a foundation for similar results for FIN.

Another approach was taken by [6,7,4] who defined asymmetric teams, a generalization of usual teams. [3] showed that a more general result about asymmetric teams (well-quasiorderedness) would imply well-orderedness and decidability of the FIN-hierarchy. They also claimed a proof of well-quasi-orderedness for asymmetric FIN teams. However, a bug was discovered in this proof and it turned out that asymmetric FIN-teams are not well-quasiordered [6]. This suggests that, if it is possible to prove a counterpart of results in section 5 for FIN, then the proof should use properties that are specific to traditional teams.


# Other problems about FIN and PFIN

Besides finding the cutpoints, there are other problems about probabilistic and team learning that are worth studying. One of them relates probabilistic and team learning to oracle computation.

Assume that we have two teams (or probabilistic machines) and one of them is weaker than the other. If we allow the weaker team (probabilistic machine) to access some oracle (for example, K, the oracle for the halting problem), we increase the power of this team and it may be able to learn everything that the stronger team can learn. Kummer [24]  for any A such that K ≤ T A (i.e., the halting oracle K is Turing-reducible to A).

The second theorem considers the question whether oracles weaker than K can be used as A in some cases. For this result, we need some extra definitions.

Let M 1 , M 2 , . . . be an enumeration of all Turing machines and ϕ i be the partial function computed by M i . Definition 9 P A denotes the set of all oracles A such that given A, there is a function f (x, y) that is computable with the oracle A and:

1. If ϕ x (y) = 0 or ϕ x (y) = 1, then f (x, y) = ϕ x (y).


## Otherwise, f (x, y) can be anything but it must be defined (even if ϕ x (y) is undefined).

In other words, an oracle in P A can be used to extend any partial recursive function to a total recursive function that is consistent with the original function. Equivalently, P A can be defined as the set of all oracles A that are Turing-equivalent to a complete and consistent extension of Peano arithmetic [24,19,29]. If K reduces to A, then A ∈ P A. However, the converse is not true [29]. Thus, we see that a weaker oracle may suffice because there are A ∈ P A such that K is not Turing-reducible to A [29]. Kummer [24] asked whether these two possibilities (we need an oracle A such that K ≤ T A or any A ∈ P A suffices) are the only ones. We have a partial answer [2].

Definition 10 P A ′ denotes the set of all oracles A such that given A, there is a function f (x, y) that is computable with an oracle A with the following properties:

1. If ϕ x (y) = 0 or ϕ x (y) = 1, then f (x, y) = ϕ x (y).

2. Otherwise, f (x, y) can be anything but it must be defined (even if ϕ x (y) is undefined).

3. If, for some x, there is at most one y such that ϕ x (y) = 1, then f (x, y) = 1 for at most one y. 


## P A ′ (see definition 10).

It is easy to see that P A ⊆ P A ′ ⊆ {A : K ≤ T A}. However, we do not know whether P A ′ coincides with P A, {A : K ≤ T A} or is different from both of them.

Other properties of the probability hierarchy deserve studying as well. For example, [1] asked how close are points of the PFIN-hierarchy one to another. 


# Other paradigms of inductive inference

Similar problems can be studied for other paradigms of inductive inference (besides FIN). One of the most interesting open cases is probabilistic language learning in the limit. In language learning, the object to be learned is a recursively enumerable language L. The standard presentation for the language is a text.

Definition 11 [15] (a) A text T for a language L is an enumeration (in any order) of all words in L.

(b) A learning machine M TxtEx-identifies a language L, if given any text T for L as an input, it outputs a sequence of grammars g 1 , g 2 , . . . such that g i = g i+1 = g i+2 = . . . and g i recognizes L, for some i ∈ IN.

(c) A set U of languages is called TxtEx-identifiable (identifiable in the limit) if there is a machine M that TxtEx-identifies every L ∈ U. TxtEx denotes the collection of all TxtEx-learnable sets.

Note that M does not get any information about the words not in L. This is the biggest difference between inductive inference of functions and languages. If f (x) = y, M knows that after receiving f (x). If a word x is not in L, M never knows it because this may be the case that x ∈ L but it has not appeared in the input yet.

The probability hierarchy for TxtEx has been studied by Jain and Sharma [18]. Below, we summarize their main results. The first theorem concerns the probability at which a probabilistic machine becomes stronger than a deterministic one. Similarly to PFIN or FIN, it is 2/3. However, the similarities end once the next point in the probability hierarchy is revealed. It is 5/8 (instead of 3/5). It remains open what are the next points below 5/8. Theorem 16 [18] (a) If p > 2/3, then p TxtEx = TxtEx.   The second theorem concerns relationships between teams of different size at probability 1/2. Teams of different size may have different learning power (similarly to FIN or PFIN). Also similarly to PFIN and FIN, all [n, 2n]-team sizes for odd n were equivalent. However, for even n results were no longer the same as for FIN or PFIN.

Theorem 17 [18] 


) A deterministic learning machine M finitely identifies (FIN-identifies) a function f if, receiving f (0), f (1), . . . as the input, it produces a program computing function f . (b) M FIN-identifies a set of functions U if it FIN-identifies any function f ∈ U. (c) A set of functions U is called FIN-identifiable if there exists a deterministic learning machine that identifies U. The collection of all FIN-identifiable sets is denoted by FIN.

## Definition 3
3(a) A probabilistic learning machine M p FIN-identifies (FIN-identifies with probability p) the set of functions U if, for any function f ∈ U the probability that M F IN-identifies f is at least p.(b) The collection of all p FIN-identifiable sets is denoted by p FIN.Team identification is another idea closely related to probabilistic identification. A team is just a finite set of learning machines {M 1 , M 2 , . . . , M s }.

## Definition 4
4(a) A team M [r, s]FIN-identifies the function f if at least r of learning machines M 1 , . . ., M s FIN-identify f . (b) The collection of all [r, s]FIN-identifiable sets is denoted by [r, s]FIN.


) There is a set of functions U such that U ∈ [2, 4]FIN but U / ∈ [1, 2]FIN. (b) [1, 2]FIN = [3, 6]FIN = [5, 10]FIN = . . . and [2, 4]FIN = [4, 8]FIN = [6, 12]FIN = . . ..(c) 1/2 FIN = [2, 4]FIN.

## Figure 1 :
1The probability hierarchies for EX, FIN and PFIN 2. If p 1 , p 2 , . . . , p s ∈ A and p ∈ [0, 1] is such that there exist q 1 , . . . , q s ∈ [0, 1] satisfying (a) q 1 + q 2 + . . . + q s = p; +1−p = p i for i = 1, . . . , s, then p ∈ A.

## ( c )
cIf 5/8 < p ≤ 2/3, then p TxtEx = [2, 3]TxtEx.


Theorem 4[11] The probability hierarchy for FIN in the interval[ 12 Thus, it appeared that there was a formula ( 12m−16 25m−34 ) and a general argument for this interval. It only was obscured by exceptions from this formula at the beginning. With probabilities getting smaller, progress became more and more difficult. The full proof of Theorem 4 was more than 100 pages long. On the other hand, it only described the situation for the interval [ 12One of the approaches to this situation was considering Popperian FINite identification(PFIN), a restricted version of FIN. FIN allows two types of errors on functions that are not identified by a machine. These are 1. Errors of commission. The program output by a machine M produces a value different from the value of the input function. 2. Errors of omission. The program output by M does not halt on some input.Errors of omission are ones that cause most trouble. The reason is that, given a program h output by a machine M, we cannot tell whether h halts on input x. If we eliminate them, the model becomes simpler and still remains interesting.Definition 6 [28] A learning machine M is Popperian if it does not make errors of omission (i.e., if all conjectures on all inputs are programs computing total functions).25 , 1 
2 ] is { 12m−16 
25m−34 |m ≥ 
2} ∪ { 24 
49 , 20 
41 , 17 
35 , 15 
31 , 27 
56 }. 

25 , 1 
2 ]. 

4 Explicit results for PFIN 




Theorem 7[1,24] Let P PFIN be the probability hierarchy for PFIN and p 1 , . . . , p s ∈ P P F IN . +1−p = p i for i = 1, . . . , s, then p ∈ P P F IN .Let p ∈ [0, 1]. If there are q 1 ≥ 0, . . . , q s ≥ 0 such that 

1. q 1 + q 2 + . . . + q s = p; 

2. 

p 

q i 


2 studied the following problem: given a, b, c, d such that [a, b]FIN ⊆ [c, d]FIN, what is the class of oracles A such that [a, b]FIN ⊆ [c, d]FIN[A]? ([c, d]FIN[A] denotes the collection of sets of functions that are identifiable by a [c, d]FIN-team with access to oracle A) We summarize his results in two theorems below. The first theorem partitions a, b, c, d into such that [a, b]FIN ⊆ [c, d]FIN[A] for some A and such that [a, b]FIN ⊆ [c, d]FIN[A] for all A. It also shows that, whenever A exists, the halting oracle K can be used as A.Theorem 13 [24] 

1. If there is k ∈ IN such that m 
n ≤ 1 
k < m ′ 
n ′ , then [m, n]FIN ⊆ [m ′ , n ′ ]FIN[A] for any 
oracle A. 

2. If 1 
k+1 < m 
n and m ′ 
n ′ ≤ 1 
k for some k, then [m, n]FIN ⊆ [m ′ , n ′ ]FIN[A] 


Theorem 14 [24] 1. Let m, n be such that [m, n]PFIN ⊆ [m ′ , n ′ ]PFIN. Then, [m, n]PFIN ⊆ [m ′ , n ′ ]PFIN[A] if and only if [m, n]FIN ⊆ [m ′ , n ′ ]FIN[A] if and only if K ≤ T A. 2. [24, 49]FIN ⊆ [1, 2]FIN[A] if and only if A ∈ P A.


Theorem 15 [2] For any a, b, c, d such that [a, b]FIN ⊆ [c, d]FIN, the set of oracles A such that [a, b]FIN ⊆ [c, d]FIN[A] is one of the following: 1. The empty set. 2. The set of all A such that K ≤ T A.3. P A (see definition 9).


Question 1 [1] Is it true that there is a constant c > 1 such that every interval [x, y] ⊆ [ 1 n , 1 n−1 ] with y − x ≥ ( 1 c ) n contains at least one point from the PFIN-hierarchy? A similar question can be asked about FIN.
It is also known[34] that ǫ 0 is the ordering type of the set of all expressions possible in first-order arithmetic but this does not look very relevant to our inductive inference result.
Related problems about oracles have been also studied in[13,23] 
Conclusions and related workWe surveyed the work in probabilistic inductive inference, with an emphasis on recent work for FIN and PFIN. For good surveys about earlier results, see[5,30]for inductive inference in general and[36,16]for probabilistic inductive inference.The biggest challenge in the area remains obtaining general results about the probability hierarchy for unrestricted FIN. In section 6, we mentioned several approaches to this problem. None of them has been completely successful but there is a chance that these ideas can be extended, giving more insight about unrestricted FIN. There are other interesting problems about FIN that deserve studying as well (like FIN with oracles, section 7).Besides FIN, probability hierarchies for other learning models can be studied. Probabilistic language learning[18]in the limit is the most interesting among those. Other recently studied models are probabilistic language learning with monotonicity restrictions[27]and probabilistic learning up to a small set of errors[22].
. = . Txtex, 1, 2]TxtEx = [3, 6]TxtEx = [5, 10[1, 2]TxtEx = [3, 6]TxtEx = [5, 10]TxtEx = . . ..

For all i ≥ 0. 2 i , 2 i+1 ]TxtEx = [2 i+1 , 2 i+2 ]TxtEx3. For all i ≥ 0, [2 i , 2 i+1 ]TxtEx = [2 i+1 , 2 i+2 ]TxtEx.

In both cases, we see that there both similarities with FIN and differences. We think that it can be very interesting to study this hierarchy. However, before general results are proved, it may be necessary to get a better knowledge of explicit probabilities and to accumulate more proof techniques. In both cases, we see that there both similarities with FIN and differences. We think that it can be very interesting to study this hierarchy. However, before general results are proved, it may be necessary to get a better knowledge of explicit probabilities and to accumulate more proof techniques.

Probabilistic and team PFIN-type learning: General properties. A Ambainis, Proceedings of the Ninth Annual Conference on Computational Learning Theory. the Ninth Annual Conference on Computational Learning TheoryACM PressA. Ambainis. Probabilistic and team PFIN-type learning: General properties. In Pro- ceedings of the Ninth Annual Conference on Computational Learning Theory, pages 157-168. ACM Press, 1996.

General oracle results for finite learning. A Ambainis, A. Ambainis. General oracle results for finite learning. Unpublished, 1998.

Team learning as a game. A Ambainis, K Apsītis, R Freivalds, W Gasarch, C H Smith, Proceedings of the 8th International Workshop on Algorithmic Learning Theory (ALT-97), volume 1316 of LNAI. Ming Li and Akira Maruokathe 8th International Workshop on Algorithmic Learning Theory (ALT-97), volume 1316 of LNAIBerlinSpringerA. Ambainis, K. Apsītis, R. Freivalds, W. Gasarch, and C. H. Smith. Team learning as a game. In Ming Li and Akira Maruoka, editors, Proceedings of the 8th International Workshop on Algorithmic Learning Theory (ALT-97), volume 1316 of LNAI, pages 2-17, Berlin, October 6-8 1997. Springer.

Hierarchies of probabilistic and team fin-learning. A Ambainis, K Apsītis, R Freivalds, C H Smith, Theoretical Computer Science. to appearA. Ambainis, K. Apsītis, R. Freivalds, and C. H. Smith. Hierarchies of probabilistic and team fin-learning. Theoretical Computer Science, 1999. to appear.

A survey of inductive inference: Theory and methods. D Angluin, C Smith, Computing Surveys. 15D. Angluin and C. Smith. A survey of inductive inference: Theory and methods. Computing Surveys, 15:237-289, 1983.

Hierarchies of Probabilistic and Team Learning. K Apsītis, College ParkUniversity of MarylandPhD thesisK. Apsītis. Hierarchies of Probabilistic and Team Learning. PhD thesis, University of Maryland, College Park, 1998.

Smith Asymmetric team learning. K Apsītis, R Freivalds, C H , Proceedings of the Tenth Annual Conference on Computational Learning Theory. the Tenth Annual Conference on Computational Learning TheoryACM PressK. Apsītis, R. Freivalds, and C. H. Smith Asymmetric team learning. In Proceedings of the Tenth Annual Conference on Computational Learning Theory, pages 90-95. ACM Press, 1997.

Use of reduction arguments in determining Popperian FIN-type learning capabilities. R Daley, B Kalyanasundaram, Algorithmic Learning Theory: Fourth International Workshop (ALT '93). K. Jantke, S. Kobayashi, E. Tomita, and T. YokomoriSpringer-Verlag744R. Daley and B. Kalyanasundaram. Use of reduction arguments in determining Poppe- rian FIN-type learning capabilities. In K. Jantke, S. Kobayashi, E. Tomita, and T. Yoko- mori, editors, Algorithmic Learning Theory: Fourth International Workshop (ALT '93), volume 744 of Lecture Notes in Artificial Intelligence, pages 173-186. Springer-Verlag, 1993.

Towards reduction arguments for FINite learning. R Daley, B Kalyanasundaram, Algorithmic Learning for Knowledge-Based Systems. K. Jantke and S. LangeSpringer-Verlag961R. Daley and B. Kalyanasundaram. Towards reduction arguments for FINite learning. In K. Jantke and S. Lange, editors, Algorithmic Learning for Knowledge-Based Systems, volume 961 of Lecture Notes in Artificial Intelligence, pages 63-74. Springer-Verlag, 1995.

The power of probabilism in Popperian finite learning. R Daley, B Kalyanasundaram, M Velauthapillai, Analogical and Inductive Inference, Proceedings of the Third International Workshop. Springer-Verlag642R. Daley, B. Kalyanasundaram, and M. Velauthapillai. The power of probabilism in Popperian finite learning. In Analogical and Inductive Inference, Proceedings of the Third International Workshop, volume 642 of Lecture Notes in Artificial Intelligence, pages 151-169. Springer-Verlag, 1992.

FINite learning capabilities and their limits. Robert Daley, Bala Kalyanasundaram, Proceedings of the 10th Annual Conference on Computational Learning Theory. the 10th Annual Conference on Computational Learning TheoryNew YorkACM PresRobert Daley and Bala Kalyanasundaram. FINite learning capabilities and their limits. In Proceedings of the 10th Annual Conference on Computational Learning Theory, pages 81-89, New York, July6-9 1997. ACM Pres.

Breaking the probability 1 2 barrier in FIN-type learning. Robert Daley, Bala Kalyanasundaram, Mahendran Velauthapillai, Journal of Computer and System Sciences. 503Robert Daley, Bala Kalyanasundaram, and Mahendran Velauthapillai. Breaking the probability 1 2 barrier in FIN-type learning. Journal of Computer and System Sciences, 50(3):574-599, June 1995.

Extremes in the degrees of inferability. L Fortnow, W Gasarch, S Jain, E Kinber, M Kummer, S Kurtz, M Pleszkoch, T Slaman, R Solovay, F Stephan, Annals of Pure and Applied Logic. 66L. Fortnow, W. Gasarch, S. Jain, E. Kinber, M. Kummer, S. Kurtz, M. Pleszkoch, T. Slaman, R. Solovay, and F. Stephan. Extremes in the degrees of inferability. Annals of Pure and Applied Logic, 66:231-276, 1994.

Finite identification of general recursive functions by probabilistic strategies. R Freivalds, Proceedings of the Conference on Fundamentals of Computation Theory. the Conference on Fundamentals of Computation TheoryBerlinAkademie-VerlagR. Freivalds. Finite identification of general recursive functions by probabilistic strate- gies. In Proceedings of the Conference on Fundamentals of Computation Theory, pages 138-145. Akademie-Verlag, Berlin, 1979.

Language identification in the limit. E M Gold, Information and Control. 10E. M. Gold. Language identification in the limit. Information and Control, 10:447-474, 1967.

On identification by teams and probabilistic machines. S Jain, A Sharma, Algorithmic Learning for Knowledge-Based Systems. K. Jantke and S. LangeSpringer-Verlag961S. Jain and A. Sharma. On identification by teams and probabilistic machines. In K. Jantke and S. Lange, editors, Algorithmic Learning for Knowledge-Based Systems, volume 961 of Lecture Notes in Artificial Intelligence, pages 109-146. Springer-Verlag, 1995.

Finite identification of function by teams with success ratio 1/2 and above. Information and Computation. S Jain, A Sharma, M Velauthapillai, 121S. Jain, A. Sharma, and M. Velauthapillai. Finite identification of function by teams with success ratio 1/2 and above. Information and Computation, 121:201-213, 1995.

Computational limits on team identification of languages. Information and Computation. Sanjay Jain, Arun Sharma, 130Sanjay Jain and Arun Sharma. Computational limits on team identification of lan- guages. Information and Computation, 130(1):19-60, 10 October 1996.

Degrees of functions with no fixed points. C Jockusch, Logic, Methodology and Philosophy of Science VIII. AmsterdamElsevierC. Jockusch. Degrees of functions with no fixed points. In Logic, Methodology and Philosophy of Science VIII, pages 191-201. Elsevier, Amsterdam, 1989.

An Introduction to Computational Learning Theory. M Kearns, U Vazirani, MIT PressM. Kearns and U. Vazirani. An Introduction to Computational Learning Theory. MIT Press, 1994.

Notations for ordinal numbers. S Kleene, Journal of Symbolic Logic. 3S. Kleene. Notations for ordinal numbers. Journal of Symbolic Logic, 3:150-155, 1938.

Probabilistic limit identification up to "small" sets. J Vīksna, Algorithmic Learning Theory: Seventh International Workshop (ALT '96). 1160J. Vīksna. Probabilistic limit identification up to "small" sets. In Algorithmic Learning Theory: Seventh International Workshop (ALT '96), volume 1160 of Lecture Notes in Artificial Intelligence, pages 312-324, 1996.

On the structure of degrees of inferability. M Kummer, F Stephan, Journal of Computer and System Sciences. 522M. Kummer and F. Stephan. On the structure of degrees of inferability. Journal of Computer and System Sciences, 52(2):214-238, 1996.

The strength of noninclusions for teams of finite learners (extended abstract). Martin Kummer, Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory. the Seventh Annual ACM Conference on Computational Learning TheoryNew Brunswick, New JerseyACM PressMartin Kummer. The strength of noninclusions for teams of finite learners (extended abstract). In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, pages 268-277, New Brunswick, New Jersey, 12-15 July 1994. ACM Press.

Set Theory. K Kuratowski, A Mostovski, North HollandK. Kuratowski and A. Mostovski. Set Theory. North Holland, 1967.

An Introduction to the General Theory of Algorithms. M Machtey, P Young, North HollandNew YorkM. Machtey and P. Young. An Introduction to the General Theory of Algorithms. North Holland, New York, 1978.

Probabilistic language learning under monotonicity constraints. Léa Meyer, Theoretical Computer Science. 1851Léa Meyer. Probabilistic language learning under monotonicity constraints. Theoretical Computer Science, 185(1):81-128, 10 October 1997.

Some natural properties of strong identification in inductive inference. E Minicozzi, Theoretical Computer Science. E. Minicozzi. Some natural properties of strong identification in inductive inference. Theoretical Computer Science, pages 345-360, 1976.

Classical Recursion Theory. P Odifreddi, North-HollandAmsterdamP. Odifreddi. Classical Recursion Theory. North-Holland, Amsterdam, 1989.

Systems that Learn: An Introduction to Learning Theory for Cognitive and Computer Scientists. D Osherson, M Stob, S Weinstein, MIT PressD. Osherson, M. Stob, and S. Weinstein. Systems that Learn: An Introduction to Learning Theory for Cognitive and Computer Scientists. MIT Press, 1986.

Probabilistic inductive inference. L Pitt, Journal of the ACM. 36L. Pitt. Probabilistic inductive inference. Journal of the ACM, 36:383-433, 1989.

Probability and plurality for aggregations of learning machines. L Pitt, C Smith, Information and Computation. 77L. Pitt and C. Smith. Probability and plurality for aggregations of learning machines. Information and Computation, 77:77-92, 1988.

H Rogers, Theory of Recursive Functions and Effective Computability. MIT PressH. Rogers. Theory of Recursive Functions and Effective Computability. McGraw-Hill, 1967. Reprinted by MIT Press in 1987.

Cardinal and ordinal numbers. W Sierpinski, PWN -Polish Scientific PublishersSecond revised editionW. Sierpinski. Cardinal and ordinal numbers. PWN -Polish Scientific Publishers, 1965. Second revised edition.

The power of pluralism for automatic program synthesis. C Smith, Journal of the ACM. 29C. Smith. The power of pluralism for automatic program synthesis. Journal of the ACM, 29:1144-1165, 1982.

Three decades of team learning. C Smith, Algorithmic Learning Theory: Fourth International Workshop (ALT '93). S. Arikawa and K. JantkeSpringer-Verlag872C. Smith. Three decades of team learning. In S. Arikawa and K. Jantke, editors, Algorithmic Learning Theory: Fourth International Workshop (ALT '93), volume 872 of Lecture Notes in Artificial Intelligence, pages 211-228. Springer-Verlag, 1994.

A theory of the learnable. L Valiant, Communications of the ACM. 27L. Valiant. A theory of the learnable. Communications of the ACM, 27:1134-1142, 1984.