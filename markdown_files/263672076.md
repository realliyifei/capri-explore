# Towards Unified Deep Image Deraining: A Survey and A New Benchmark

CorpusID: 263672076
 
tags: #Computer_Science

URL: [https://www.semanticscholar.org/paper/1a36e49fa5b01bfa2b67ed85beb040027d3b7965](https://www.semanticscholar.org/paper/1a36e49fa5b01bfa2b67ed85beb040027d3b7965)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Towards Unified Deep Image Deraining: A Survey and A New Benchmark
5 Oct 2023

Xiang Chen 
Syn RS LSM CVPR
Rain12 [31] 0/122016

Jinshan Pan 
Syn RS LSM CVPR
Rain12 [31] 0/122016

Jiangxin Dong 
Syn RS LSM CVPR
Rain12 [31] 0/122016

Jinhui Tang 
Syn RS LSM CVPR
Rain12 [31] 0/122016

Towards Unified Deep Image Deraining: A Survey and A New Benchmark
5 Oct 202395548BF8AFB30241C82E0AD9730D85FCarXiv:2310.03535v1[cs.CV]Single image derainingrain streaksimage restorationbenchmark datasetdeep learning
Recent years have witnessed significant advances in image deraining due to the kinds of effective image priors and deep learning models.As each deraining approach has individual settings (e.g., training and test datasets, evaluation criteria), how to fairly evaluate existing approaches comprehensively is not a trivial task.Although existing surveys aim to review of image deraining approaches comprehensively, few of them focus on providing unify evaluation settings to examine the deraining capability and practicality evaluation.In this paper, we provide a comprehensive review of existing image deraining method and provide a unify evaluation setting to evaluate the performance of image deraining methods.We construct a new high-quality benchmark named HQ-RAIN to further conduct extensive evaluation, consisting of 5,000 paired high-resolution synthetic images with higher harmony and realism.We also discuss the existing challenges and highlight several future research opportunities worth exploring.To facilitate the reproduction and tracking of the latest deraining technologies for general users, we build an online platform to provide the off-the-shelf toolkit, involving the large-scale performance evaluation.This online platform and the proposed new benchmark are publicly available and will be regularly updated at http://www.deraining.tech/.

# INTRODUCTION

R AIN is a challenging adverse weather condition which always obstructs or deforms the image background.It consequently degrades the visual quality of images and impairs the performance of outdoor vision-based systems, including video surveillance [1], autonomous driving [2] and person identification [3], etc. Single image deraining, also known as rain removal, aims to recover the rain-free image from the observed rainy one.A large number of deraining methods and benchmark datasets have been proposed in recent years with demonstrated success.Since this topic booms with the deep learning techniques [4], [5], [6], [7], it is significant to timely and comprehensively evaluate the state-of-the-art approaches to demonstrate their strength and weakness, thus facilitating the future development of this research field for more robust algorithms.

To facilitate the performance comparison and analysis of existing methods, it is indispensable to collect a representative benchmark dataset.There exist several classical synthetic datasets for single image deraining task, such as the Rain100L [8], Rain100H [8], and Rain800 [5].Although these synthetic datasets are prevalent in current studies [9], [10], [11], [12], [13], [14], most approaches randomly add rain effects to any clear backgrounds to build the synthetic paired data, which inevitably leads to inadequate realism and harmony, especially in the sky region (see Figure 1(a)).As a result, there is a large domain gap between synthetic and real data, which limits the ability of training better deep deraining models.Recently, significant efforts have been • X. Chen, J. Pan, J. Dong, and J. Tang are with the School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, 210094, China.E-mail: chenxiang@njust.edu.cn,sdluran@gmail.com,dongjxjx@gmail.com,jinhuitang@njust.edu.cn.

• Corresponding author: Jinshan Pan.

made to solve real-world image deraining, where the largescale datasets are proposed, e.g, SPA-Data [15] and GT-RAIN [16].Unfortunately, these real datasets still have several shortcomings, including low resolution, spatial misalignment, limited rain appearances and background variation [17].In fact, obtaining strictly aligned rainy and clear image pairs in complex rainy environments is time-consuming and laborious.Returning to the problem itself, it is urgent and necessary to establish a high-quality benchmark and pave the way for future research in this field.Despite lots of efforts have been made on existing benchmarks [4], [5], [6], [7], [17], there still exist several potential challenges.According to statistics in Figure 1(b), existing methods usually use individual evaluation settings to evaluate the deraining performance.The evaluations based on different settings do not fully reflect performance to some extent.On one side, multiple training sets are trained in a mixed way, and a pretrained model is obtained to evaluate different testing sets.The typical mixed training based methods include MSPFN [18], MPRNet [19], Restormer [20].On the other side, each training set is trained independently, and multiple pretrained models are obtained to evaluate the corresponding testing sets.The typical independent training based methods include SPDNet [12], IDT [11], DRSformer [9].Under the setting of independent training, the usage relationships between training data and testing data are also quite diverse, especially for Rain100L/H and its new version, Rain200L/H [8].As a result, this makes it difficult to make comparisons among the reported quantitative results.In addition, the validation metrics of the evaluated deraining algorithms are not the same, such as calculating PSNR and SSIM metric [21] in different channels (RGB or Y), and converting YCbCr space by MATLAB or OpenCV tool in different ways.This naturally leads to incomparable and sometimes contradictory quantitative results reported in their literature.Hence, it is of importance to integrate them in a unified track for evaluation on a common criteria.

Towards these goals, we first establish the latest development milestone of single image deraining field, especially with the increasing number of solutions in recent years.Note that this paper only focuses on image deraining, excluding video deraining.Based on this survey, we intend to summarize two tracks (i.e., mixed training and independent training) to report the performance of existing representative methods within the same evaluation criteria.It can provide a guideline on fair and reasonable comparisons of different methods in future work for researchers, especially for beginners who are new to this filed.We suggest that researchers can first try to categorize their methods into the corresponding track, and then pay special attention to evaluating and comparing previous methods that belong to the same track.Furthermore, we also construct a new high-quality image deraining benchmark, called HQ-RAIN, to further facilitate the performance comparison.To the best of our knowledge, this is the first attempt to improve the realism and harmony of synthetic rainy images, with the aim of reducing the domain gap between the synthetic and real data.Inspired by image harmonization [22], we can treat a real rainy image as harmonized image, and further generate a synthetic rainy dataset by background collection, rain streaks synthesis and image blending, making it compatible with the background area in the synthetic image.In addition to reviewing and analyzing the existing methods, we also develop a unified online platform to facilitate the performance evaluation.To keep pace with the fast development, this platform releases a public repository to provide an up-to-date record of recent advancements, which can be served as a reference point for new researchers.The online platform and the proposed new benchmark can be found at http://www.deraining.tech.


## Our Contributions

The contribution of this paper are mainly three-fold.(1) We present a systematic and comprehensive review of recent advancements in single image deraining field.(2) We construct a new high-quality image deraining banchmark (called HQ-RAIN) to inspire further researches for more robust algorithms.(3) We provide an online platform to offer a fair and unified performance evaluation for deep image deraining methods.


## Relations with Other Surveys

Despite the fact that deep learning has dominated image deraining research, a timely and thorough survey on the latest technologies is lacking.There are two early surveys of image deraining [4], [6] that mainly review prior-based and CNN-based methods.In comparison to recent survey [7] that reviews current methods from different perspectives, this paper seeks to provide much more than a summary of recent research progress.Although these surveys have provided comprehensive analyses of existing datasets, they still overlook two key factors when evaluting existing algorithms: synthetic dataset quality and common evaluation criteria.To the best of our knowledge, these two issues have not been well addressed in the literatures of image deraining.Our survey will fill the gap in this research.


## Organization

The rest of this paper is structured as follows.In Section 2, we present the problem formulations of some common rainy image models.Section 3 provides a systematic review of representative image deraining methods.Section 4 introduces our proposed new benchmark.In Section 5, we conduct extensive performance evaluations and analyses for representative methods.Section 6 discusses the open challenges and new perspectives for future studies.Finally, the concluding remarks will be drawn in Section 7.


# PROBLEM FORMULATION

In this section, we review several representative rain models proposed in the existing methods.These different rain models can be used to simulate various types of rain degradation.It should be noted that all these models are heuristic, as they are only rough approximations of the real world.


## Linear Superposition Model (LSM).

The most of the existing methods adpot simple and common rain model, i.e., linear superposition model (LSM), to achieve the separation of undesired rain streaks and latent clear background.The rainy image R degraded by rain streaks is formulated as:
R = B + S,(1)
where B and S represent the rain-free background layer and the rain streak layer, respectively.For convenience, we use the uniform mathematical symbols in the following.


## Screen Blend Model (SBM).

Different from the LSM, Luo et al. [23] formulate a non-linear composite model, i.e., screen blend model (SBM), for modeling rainy images.It considers that the rain streak layer and background layer may affect the apperance of each other, which can be expressed as:
R = B + S − B ⊙ S,(2)
where ⊙ denotes the element-wise multiplication operator.

According to [23], these rainy images rendered by the SBM tend to look more realistic than those by the LSM.


## Heavy Rain Model (HRM).

In the case of heavy rain, the accumulation of distant rain streaks and water particles in the atmosphere leads to the rain veiling effect.This visual effect of rain accumulation produces a mist (or haze)-like phenomenon in the image background [8].Based on the joint observation of the rain streak model and the atmospheric scattering model [24], the formulation of this heavy rain model (HRM) is defined as:
R = B + n i=1 S i ⊙ T + (1 − T) ⊙ A,(3)
where T and A refers to the transmisson map and the global atmospheric light, respectively.Each S i is termed as the rain streak layer with the same direction.Here, i is the number of the rain streak layers, and its maximum value is n.


## Depth-guided Rain Model (DRM).
R = (1 − S − F) ⊙ B + S + F ⊙ A,(4)
where F ∈ [0, 1] represents the fog layer.F is further written as F = 1 − e −βd(x) , where β determines the thickness of fog, and d(x) denotes the scene depth.

Transmission Medium Model (TMM).Based on the observation that rain streaks and vapors are entangled with each other, Wang et al. [26] remodel rain imaging by formulating both rain streaks and vapors as transmission medium, which is formulated as:
R = (T s + T v ) ⊙ B + [1 − (T s + T v )] ⊙ A,(5)
where T s and T v represent the transmission map of rain streaks and vapors, respectively.


## Raindrop Mask Model (RMM).

The clear background image may be obscured or blurred by persistent raindrops that cling to camera lenses or window glasses as they fall and flow [27].Qian et al. [28] formulate a raindrop degraded image as the result of combining the raindrop effect and the background image:
R = (1 − M) ⊙ B + D,(6)
where D denotes the obstruction or blurry effect brought by the raindrops.M is the binary mask.If M(x) = 1, the pixel x in the mask belongs to the raindrop region, otherwise it is a part of the background image.


## Mixture of Rain Model (MRM).

During outdoor image capturing, rain streaks and raindrops may co-occur.The lighting conditions that have a significant impact on the transparency of the raindrops during image capture may alter as a result of rain streaks.As a result, removing rain streaks and raindrops cannot simply be construed as a rain streak removal and a raindrop removal combined.The mixture of rain model (MRM) [29] can be modeled as:
R = (1 − M) ⊙ (B + S) + αD,(7)
where α indicates the atmospheric lighting coefficient.


# TECHNICAL DEVELOPMENT REVIEW

In this section, we first provide a latest milestone of single image deraining in Table 1.Then, we analyze the technical characteristics of the deep learning-based methods from five main perspectives of the learning strategies, network structures, loss functions, experimental datasets, and evaluation metrics.In what follows, we select representative methods based on the following principles: publishing in well-known journals (e.g., TPAMI, IJCV) and conferences (e.g., CVPR, ICCV), providing the official codes to ensure credibility, preferring works with more citations and Github stars.


## Method Categories

Based on the problem formulations, image deraining methods can be categorized into model-based methods and datadriven methods [4], [6].Early model-based approaches [91], [92], [93], [94], [95], [96] usually formulate the image deraining as a filtering problem and restore the rain-free image by kinds of filters.Although rain streaks can be removed, some important strictures are also removed during the filtering process.As image deraining is ill-posed, lots of model-based approaches develops kinds of image priors based on the statistical properties of rain streaks and clear images, including image decomposition [30], low-rank representation [97], discriminative sparse coding [23], and Gaussian mixture model [31], etc.Although better results have been achieved, these approaches cannot handle complex and varying rainy TABLE 1: Summarization of representative single image deraining methods."SL", "SSL" and "UL" stand for supervised/semi-supervised/unsupervised learning-based methods."CE", "Adv", "KL", "TV", "Contra" and "QS" denote cross entropy loss, adversarial loss, Kullback Leibler loss, total variation loss, contrastive loss and quasi-sparsity loss, respectively."self-selected" denotes images selected by the author rather than common datasets."P", "R", "F" and "U" represent parameter, run-time, FLOPs and user study, respectively."-" means this item is not available or not indicated in their paper.scenes because the hand-crafted rain models used these approaches do not hold.In addition, most algorithms [32], [33] still need time-consuming iterative computations due to the complexity of the optimization procedure.

Data-driven methods are able to learn models to better describe the properties of rain streaks and clear images.As one of the representative data-driven approach, image deraining based on deep learning approaches have achieved significant performance.Since the representative methods [8], [34], [35] have shown that using deep learning help rain removal, lot of approaches based on the convolutional neural networks (CNNs) [6], [18], [34], [35], [36], [53] have been developed to solve image deraining and achieve decent performance in kinds of image deraining benchmarks [4], [5], [6], [17].To restore realistic images, lots of approaches employ generative adversarial networks (GANs) [98] to solve image deraining.With guidance of the adversarial learning strategy, the GAN-based methods [28], [43], [48], [61] are able to restore clear images with better perception quality.

However, as the basic operation in CNNs or GANs, the convolution operation is spatial invariant and has limited receptive field, which cannot model the spatial variant property and global structures of clear images well.To solve this problem, Transformers [99] have been applied to image deraining and achieve significant advancements as they can model the non-local information for better image reconstruction.Motivated by the success of recent diffusion model [100] in generating high-quality images, diffusionbased approaches [89] achieve promising results in kinds of image restoration problems, which may facilitate the image deraining problem.

In the following, we will briefly review the CNN-based, GAN-based, and Transformer-based representative deraining methods in details.


## Learning Strategies

Along this research line, we categorize existing deep image deraining methods into supervised learning, unsupervised learning and semi-supervised learning.Based on the statistic analysis in Table 1, supervised learning (86.2%) is the most widely used approach.There are emerging approaches starting in 2019 that make use of unsupervised learning (8.3%) and semi-supervised learning (5.5%).Next, we review some representative methods of each strategy.Supervised Learning.The classic deep learning-based image deraining baseline DerainNet [34] adopts a simple CNN to learn the nonlinear mapping function from paired clear and rainy images.Later, diverse network structures and designs have been extensively utilized to further enhance the ability of end-to-end learning, such as multi-scale fusion [64], [65] and multi-stage progressive [18], [19].To better represent the rain distribution, several studies take rain characteristics such as rain density [36], location [8], [53], depth [25], sparsity [73], non-local [39], veiling effect [26], [43] into account.Recently, Transformers-based models have achieved significant performance in single image deraining due to their ability to model non-local information required for accurate image reconstruction.Xiao et al. [11] designed an image deraining Transformer (IDT) with spatial-based and complementary window-based Transformer modules.Very recently, Chen et al. [9] developed a Sparse Transformer (DRSformer) to achieve state-of-the-art (SOTA) performance in the category of supervised learning methods.Although impressive performance has been attained, these fully-supervised approaches require paired synthetic data, which poorly mimics the degradation that occurs in the real world.To this end, the semi-supervised and unsupervised learning have been proposed for image deraining [79], [101].

Unsupervised Learning.To alleviate the pressure of collecting large-scale paired training data, unsupervised learning has been proposed for image deraining.In fact, learning a deep deraining network from unpaired data is more challenging compared to fully-supervised models.Zhu et al. [54] first proposed an end-to-end CycleGAN-based deraining model, RainRemoval-GAN (RR-GAN) without using paired training data.Jin et al. [55] presented an unsupervised deraining generative adversarial network (UD-GAN) by introducing self-supervised constraints from the intrinsic statistics of unpaired rainy and clear images.Another CycleGANbased deraining network, termed as DerainCycleGAN [66], was designed using an unsupervised rain attentive detector.By modelling of the structure discrepancy between the clear image and rain distribution, Yu et al. [76] developed an unsupervised directional gradient based optimization model (UDGNet) for real rain streaks removal.Recently, Chen et al. [79] leveraged dual contrastive learning to encourage the model to explore the mutual features of rainy images and clean ones in the deep feature space, where the features from the unpaired clear exemplars can facilitate rain removal.Likewise, Ye et al. [80] incorporated non-local self-similarity into contrastive learning to further improve unsupervised image deraining performance.

Semi-supervised Learning.Semi-supervised learning has recently been offered as a way to combine the advantages of supervised learning and unsupervised learning.Wei et al. [45] proposed the first semi-supervised image deraining (SSIR) framework, treating rain removal as domain adaption problem.The work of Syn2Real [58] was a Gaussian process based semi-supervised image deraining method by jointly modeling the labeled and unlabeled latent space.Huang et al. [68] presented a memory-oriented semi-supervised (MOSS) framework to adaptively record various appearances of rain degradations.In [74], the semi-supervised learning and knowledge distillation components were combined to formulate a rain direction regularizer, which can preserve the sharpness and structures of the rain maps.For unsupervised/semi-supervised rain removal, continued research in this direction is highly encouraged.


## Network Architectures

Significant progress has been made due to the development of kinds of network architectures in image deraining.Next, we review the representative network designs in this field.

CNN-based Network Design.Eigen et al. [102] designed the first CNN-based method by constructing a three-layer CNN that learns the mapping between corrupted image patches and their corresponding clean counterparts.Inspired by the deep residual network (ResNet) [103], Fu et al. [35] utilized the ResNet structure to integrate the high-frequency detail layer content of an image and perform regression on the negative residual rain information.Yang et al. [8] proposed a multi-task deep learning architecture that initially detects rain regions, which inherently provides supplementary information for the task of rain removal.To better characterize rain streaks of varying scales and shapes, a multi-stream densely connected framework was designed by [36], consisting of two main branches: rain-density classification and rain streak removal.Due to the overlapping nature of rain streak layers, removing rain in a single stage is challenging.Thus, Li et al. [38] decomposed the rain removal into multiple stages by introducing the recurrent neural network to retain useful information from previous stages and facilitate rain removal in subsequent stages.By formulating a rain imaging model based on the visual effects of rain in relation to scene depth information, Hu et al. further [25] proposed the depth-guided attention network to remove rain streaks and fog.Instead of deeper and complex networks, Ren et al. [42] discovered that a simple combination of ResNet with multi-stage recursion yields favorable deraining performance.Considering the role of spatial attention is to selectively focus on specific regions or locations in an image, Wang et al. [15] presented a spatial attention network designed to learn discriminative rain information in a local-toglobal manner.Yasarla et al. [44] incorporated uncertaintyguided learning into the network to learn the residual maps and corresponding confidence maps at various scales.These maps are subsequently propagated back to guide the net-work in subsequent layers.By incorporating the Gaussian-Laplacian image pyramid decomposition technique, Fu et al. [52] proposed a lightweight pyramid network to simplify the learning problem.Inspired by the success of multi-scale strategies in vision tasks, Jiang et al. [18] proposed a multiscale progressive fusion network that better characterizes rain streaks across multiple scales through the use of image pyramid representation.Wang et al. [57] proposed a rain convolutional dictionary network that leverages the inherent dictionary learning mechanism to accurately encode rain shapes.Deng et al. [14] constructed a two-branch parallel network consisting of a detail repair sub-network and a rain residual sub-network based on squeeze-and-excitation mechanism.To better compute channel-wise correlation and spatial-wise coherence, Fu et al. [13], [104] introduced a dual graph convolutional architecture to help image restoration and facilitate rain removal.Under the iterative guidance of the residue channel prior, Yi et al. [12] proposed a structure preserving deraining architecture to reconstruct highquality rain-free images.Drawing on the concept of closedloop control, Li et al. [69] designed a robust representation learning network structure by incorporating feedback mechanism into the CNN.Recently, Fu et al. [10] developed a patch-wise hypergraph convolutional network architecture to help the model to explore non-local content of the images.


## GAN-based Network Design.

Driven by GAN in the image generation task [98], Qian et al. [28] incorporated a GANbased architecture, where the generative network employs an attentive-recurrent network to generate an attention map.This attention map, along with the input image, is then utilized in a contextual autoencoder to generate a rain-free result.Afterwards, Zhang et al. [48] proposed a conditional GAN-based architecture with a densely-connected generator and a multi-scale discriminator.To achieve heavy rain image restoration, the method of Li et al. [43] consists of a two-stage network architecture: an initial physics-based sub-network followed by a depth-guided GAN refinement sub-network.Based on the consistency between the estimated results of the physical model and the observed image, Pan et al. [61] proposed a GAN-based network constrained by a physics model to remove rain.Motivated by the image disentanglement strategy [105], Ye et al. [71] presented a CycleGAN-based joint rain generation and removal framework by performing the translations on the simpler rain space.Ni et al. [72] put forward a rain intensity controlling GAN, which comprises three sub-networks: a main controlling network, a high-frequency rain-streak elimination network, and a background extraction network, which enables seamless control over rain intensities by leveraging interpolation techniques within the deep feature space.With the popularity of generative models [106], Wang et al. [70] introduced a variational rain generation network to implicitly infer the underlying statistical distribution of rain.

Transformer-based Network Design.Inspired by the success of Transformer-based networks in the NLP and highlevel vision field [107], image processing Transformer (IPT) [67] was first designed with multi-heads and multi-tails shared transformer body to solve a host of low-level vision tasks, including image deraining.In order to consider the high compatibility of Transformers and CNNs in the image deraining task, Jiang et al. [87] integrated the benefits of selfattention and CNNs into an association learning-based network for efficient rain removal.Similarly, Chen et al. [88] formulated a hybrid stage-by-stage progressive network structure that combines the features by CNN and Transformer to help image restoration.UNet [108], as a classic network architecture design, excels in image restoration tasks due to its ability to capture both high-level and low-level features through the use of skip connections.Wang et al. [81] built a U-shaped hierarchical network architecture with locallyenhanced window Transformer blocks.To reduce computational complexity, Zamir et al. [20] proposed an efficient Transformer consisting of a multi-Dconv head transposed attention and a gated-Dconv feed-forward network.To compensate for the shortcomings of position encoding, Xiao et al. [11] developed a relative position enhanced multi-head self-attention to improve the representation ability.Instead of vanilla self-attention in Transformers [99], a top-k sparse attention [9] was introduced into the model to maintain the most relevant features for boosting deraining performance.


## Loss Functions

It is vital to choose suitable loss functions to constrain the model for better removing rain.The choice of loss functions is highly diverse among the existing approaches.We detail representative loss functions as follows.

Content-based Loss Function.The commonly used loss functions are based on the image content, which is usually defined as: L 1 (Mean Absolute Error, MAE) and L 2 (Mean Squared Error, MSE) losses can be written as:
L content = 1 HW C H i=1 W j=1 C c=1 ρ( B − B),(8)
where B and B denote the derained image and groundtruth; ρ(•) denotes the robust function.H, W and C represent the height, width, and number of channels of the image, respectively.The commonly used ones include:
ρ( B − B) = ∥ B − B∥ p , ρ( B − B) = B − B 2 + ϵ 2 ,(9)
where p and ϵ denote the positive scalars.

Structure-based Loss Function.The structural similarity index measure (SSIM) is used to measure the quality of restored images.It is defined as:
L SSIM = 1 − 2µ B µ B + C 1 µ 2 B + µ 2 B + C 1 • 2σ B B + C 2 σ 2 B + σ 2 B + C 2 , (10)
where µ B and µ B are the average of B and B over pixels, σ B and σ B are the variances of B and B, σ B B is the covariance between σ B and σ B .C 1 and C 2 are two fixed constants.

Adversarial Loss Function.The adversarial loss is derived from the minimax game between the generator G and discriminator D, which usually is formulated as:
min G max D E R∼S [log(1 − D(G(R))] + E B∼T [log(D(B))],(11)
where the rainy images R are sampled from the source manifold S while the clear images B are sampled from the target manifold T .Here, G minimizes the loss to produce clear images that resemble real samples.In contrast, D maximizes the loss to discriminate between generated clear images and real clear images.To optimize the generator, we can define it as:
L G = log(1 − D(G(R))).(12)
Meanwhile, the loss function for optimizing the discriminator is minimized by:
L D = − log(1 − D(G(R))) − log(D(B)). (13)
Cross Entropy Loss Function.The cross entropy loss for rain density classification [8], [36], [63] is designed to take advantage of the rain density information to guide the deraining process:
L CE = − n i=1 p (x i ) log (q (x i )) ,(14)
where x i denotes the data sample, p(x i ) and q(x i ) are two probability distributions on random variables.

Kullback-Leibler Loss Function.The Kullback-Leibler (KL) divergence is used to measure the similarity of different distributions.recent studies [60], [70] employ this metric to measure the distribution similarity between the groundtruth and derained image:
L KL = n i=1 p(x i ) log p(x i ) q(x i ) .(15)
Total Variation Loss Function.The original total variation is mainly regarded as an effective image prior to for image restoration [109].Given its decent performance in image restoration, several approaches [45], [68], [74], [76] use it to constrain the network training for better performance.This constraint is defined as:
L TV = ∇ h B 1 + ∇ v B 1 ,(16)
where ∇ h and ∇ v represent the horizontal and vertical gradient operators, respectively.

Contrastive Loss Function.Contrastive loss function is mainly used to improve the perceptual quality of derained images by positive and negative examples [79], [80].This constraint is usually defined as:
L Cont = − log exp (v • v + /τ ) n i=0 exp v • v − i /τ ,(17)
where v denotes a query representation, v + and v − i are corresponding positive and negative samples.n is the number of negatives, τ denotes a temperature parameter, and • denotes the inner product operation.

Quasi Sparsity Loss Function.The quasi sparsity loss function is mainly used to intrinsic sparsity prior of clear images [73]:
L QS = N t=1 i,k ω i,k * B 1 + ω i,k * R − B 1 , (18)
where * is the convolution operation.ω i,k denotes the k th filter centered at the i th pixel.


## Deraining Datasets

To better evaluate the image deraining methods, lots of image deraining datasets have been proposed.Table 2 presents an overview of the existing datasets for single image deraining, including synthetic and real-world datasets.

Rain12 [31] is only a test dataset that contains 12 synthesized images with a single sort of rain streak.[8] contain 1,800 synthetic image pairs for training and 100 ones for test, where the clear images are selected from the BSD dataset [110].These rain streaks are created either by adding simulated line streaks or by using photorealistic rendering techniques [111].


## Rain100L and Rain100H

Rain200L and Rain200H [8] are created based on the original Rain100L and Rain100H datasets by filtering out duplicate background images.Among them, there are 1,800 synthetic training pairs and 200 test images.[35], also known as Rain1400, contains 12,600 image pairs for training and 1,400 ones for test, where the clear images are selected from the BSD dataset [110], UCID dataset [112] and Google image search.Each clear image is used to generate 14 synthetic images with different rain directions and density levels.[36], also known as Rain1200, consists of 12,000 synthetic training pairs and 1,200 test pairs with three rain density levels (i.e., light, medium, and heavy).


## DDN-Data


## DID-Data

RainDrop [28] is the first raindrop removal dataset, which conains 1,119 pairs of raindrop images with varied backgrounds using a camera with two aligned pieces of glass (one sprayed with water, and the other is left clean).Rain800 [5] includes 700 image pairs for training and 100 for test, where the clear images are randomly chosen from the BSD dataset [110] and UCID dataset [112].[15] is the first paired real-world dataset which utilizes the human-supervised percentile video filtering to obtain the ground turths.It contains 638,492 rainy/clear image patches for training and 1,000 testing ones.MPID [5] serves both machine and human vision by incorporating a considerably wider variety of rain models, including both synthetic and real-world images.There are three different forms of rain in it: rain streak, raindrops, and rain mist.The training set contains 2,400, 861, and 700 image pairs, whereas the test set has 200, 149, and 70 image pairs.RainCityscapes [25] is made up of 262 training images and 33 test images from Cityscape's training and validation sets [113], which are chosen as clear background images.The authors simulate rain and fog on the photographs using the camera settings and scene depth information.[43] contains 9,000 training samples and 1,500 test samples, where the clear backgrounds are obtained from [28].It also considers depth information to synthesize rain accumulation by using [114].


## SPA-Data


## Outdoor-Rain

Rain13K [18] is the mixed datasets collected from multiple previous datasets, which consists of 13,712 image pairs for training and 4,298 test images.There are five test sets, i.e., TABLE 2: Summarization of public datasets for the single image deraining task."Syn" and "Real" denote the synthetic and real-world rainy datasets."RS", "RD", and "RA" represent the rain streak, raindrop and rain accumulation effect, respectively.Note that DDN-Data [35] and DID-Data [36] are also termed as Rain1400 and Rain1200 in some papers.

Rain100H [8], Rain100L [8], Test100 [5], Test2800 [35], and Test1200 [35].

RainKITTI2012 and RainKITTI2015 [63]  RainDS [29] is divided into two subsets: RainDS-Syn and RainDS-Real.Based on first-person view driving scenarios from autonomous driving datasets, RainDS-Syn is a synthetic dataset made up of 3,600 image pairs corrupted by raindrops and rain streaks.By manually mimicking rainfall with a sprinkler, RainDS-Real is a collection of 750 realworld images degraded by raindrops and rain streaks.

RainDirection [74] contains 2,920 high-resolution synthetic image for training and 430 ones for test, where the clear images are selected from the Flick2K and DIV2K dataset [116].Each rainy image is assigned with a direction label.[16] is the large-scale dataset with real paired data captured diverse rain effects.It contains 31,524 rainy and clear frame pairings, which are divided into 26,124 training frames, 3,300 validation frames, and 2,100 test frames.


## GT-RAIN


## Evaluation Metrics

There are several ways to evaluate the performance of image deraining models.One common way is to use some evaluation metrics including full and/or non-reference image quality assessment (IQA) and human-based evaluation (e.g., user study) to evaluate the quality of the derained images.In addition to the quality of the restoration results, the model complexity is also an important factor.Moreover, given the image deraining can be regarded as a pre-processing step, whether the derained images facilitate the following tasks is another commonly used evaluation metric.In the following, we provide details about these aforementioned evaluation metrics.

Full-Reference Metrics.The commonly used full-reference IQA metrics include Peak Signal-to-Noise Ratio (PSNR) [21], Structure Similarity Index (SSIM) [117], Feature Similarity (FSIM) [118], and Learned Perceptual Image Patch Similarity (LPIPS) [119].The PSNR assesses the pixel-level similarity between two images, whereas the SSIM measures similarity according to structure information.The FSIM and LPIPS measure similarity at the feature level between image pairs for quality evaluation.Note that, higher PSNR, SSIM, FSIM, and lower LPIPS indicate better image visual quality.

Non-Reference Metrics.In terms of the rainy images without ground truth images, the non-reference IQA indicators are used for quantitatively evaluate the restoration performance, including Natural Image Quality Evaluator (NIQE) [120], Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) [121], and Spatial-Spectral Entropy-based Quality (SSEQ) [122].The smaller scores of NIQE, BRISQUE and SSEQ indicate clearer contents and better perceptual quality.

User Study.User study is a representative subjective evaluation method.Users can choose the image with the best deraining performance from a group of images.The premise is to anonymize the method and randomly sort the images in each group to ensure fairness.In general, the user study score is the mean opinion score (MOS) from a group of participants.A high MOS indicates superior perceptual quality from human perspectives.


## Model Efficiency. Model efficiency is critical driver for the practical application of deep image deraining algorithms.

The evaluation metrics for model efficiency typically include the numbers of parameters (#Params), inference running time, and Floating Point Operations (FLOPs).It is widely known that a shorter running time and a smaller #Params and FLOPs means better model efficiency.

Application-based Evaluations.One of the goals of image deraining, in addition to enhancing the visual quality, is to serve high-level vision tasks, such as object recognition [123] and segmentation [124].Thus, to verify the performance of different methods, the effects of image deraining on outdoor vision based applications are further investigated.

Robust Analyses on Adversarial Attacks.Rain highly degrades in a variety of ways, and the deraining model may suffer from a performance loss due to scene inconsistencies.Adversarial attacks try to degrade the deraining algorithms' Fig. 2: Example images from previous representative datasets [5], [8], [29], [35], [36], [74] and our proposed HQ-RAIN.

output by adding a small quantity of visually undetectable disturbances to the input rainy images.Recent studies [125], [126] have investigated various types of adversarial attacks focusing on rain degradation problems and their impact on both human and machine vision tasks.


# HQ-RAIN: A NEW BENCHMARK

Although lots of single image deraining datasets have been proposed, the quality of most synthetic rain dataset is suboptimal in terms of realism and harmony.As a result, a large domain gap exists between synthetic and real data, thereby limiting the capacity to train better deep deraining models.In the following, we first describe the construction of the new benchmark and then compare it with other commonly used image deraining datasets.


## Dataset Construction

Existing approaches usually add rain streaks into the clear images to obtain rainy images.However, this will leads to unnatural results as shown in Figure 2, especially in the sky region.Instead of simply adding rain streaks into the clear images, we develop an effective image synthesization approach to obtain more realistic datasets.Our method include the background collection, rain streaks synthesis, and image blending, which will be presented in the follows.

Background Collection.The quality of clear backgrounds (i.e., ground-truth images) is equally significant for constructing paired datasets, which was not taken into account in previous studies.In other words, the existing synthetic datasets [5], [8], [18], [36], [74] only focus on the synthesized rain while ignoring the high-quality backgrounds that we also need.On the one hand, the ground-truths of these datasets have some unexpected problems about the images: low resolution, watermark, compression artifact, which may interfere with the quality of model learning and high-quality image reconstruction.On the other hand, they often overlook the basic fact that rainy weather mostly occurs under cloudy or low background brightness imaging conditions.When revisiting the rainy images in the existing datasets, we find that there are very obvious rain streaks in the clear blue sky region, which makes them look incompatible.

To ensure more realistic and harmonious synthetic rainy images for the next step, our selection of ground-truths is based on a strict set of collection criteria.Specifically, we first collect clear and rain-free scenes using a Python program based on Scrapy to download images from Google search.In addition, we also elaborately select several backgrounds covering abundant scenes from the DPED [127] and RainDS [29] datasets.Low quality images that contain poor resolution, website watermark, compression artifact and blur are filtered out.All clear blue sky regions are filtered out as well to ensure appropriate background composition.Here, we tend to choose suitable rain-free backgrounds based on human visual perception of real rainy days by considering sky, illumination, and ground conditions.Overall, our ground-truths covers a large variety of typical daylight and night scenes from urban locations (e.g., streets, buildings, cityscapes) to natural scenery (e.g., hills, lakes, vegetations).Similar to [16], despite the fact that our collection of groundtruths is reliant on streamers, Google Image's fair use clause permits for its distribution to the academic community.


## Rain Streaks Synthesis.

The fidelity and diversity of rain are two key factors in the rain streaks synthesis step.For convenience, most synthesis methods [5] adopt Photoshop software 1 to render the streaks.However, this manual synthesis based method is time-consuming and labor-intensive.Inspired by [128], [129], we model the generation of rain streak layers as the motion blur process, which naturally takes advantage of two crucial aspects of rain streaks: repeatability and directionality.Mathematically, it can be expressed as:
S = K(l, θ, w) * N(n),(19)
where N denotes the rain mask generated by random noise n.Here, we use uniform random numbers and thresholds to control the level of noise.l and θ are the length and angle of the motion blur kernel K ∈ R p×p .We further add the rotated diagonal kernel using Gaussian blur to make the rain thickness w.The noise quantity n, rain length l, rain angle θ, and rain thickness w are obtained by sampling 1.The PhotoShop implementation of the rain streaks synthesis method.Please refer to https://www.photoshopessentials.com/ photo-effects/photoshop-weather-effects-rain/.Although in different synthesis methods the rain streaks are simulated and visually similar to humans, our method exhibits more flexible and higher rain diversity, which have great effect on the coverage of real-world rain.

Image Blending.Most existing synthetic rainy images add rain streaks linearly to the rain-free backgrounds, which can easily make the composite image look unnatural, especially in the sky area.Our goal is to ensure the visual realism and harmony of synthesized rainy images, thereby reducing the domain gap between the synthetic and real images, which is never explored before.Thus, instead of directly copy-andpasting, we adopt image blending [130] technique to yield a synthesized image.Compared to image harmonization task [22], due to the similarity of rain streaks, we do not need to accurately depict objects for the blending mask [131].To this end, alpha blending is utilized to process the rain layer and background layer, where the alpha value of a pixel in a given layer indicates how much of the colors from lower levels may be visible through the color at that level.Formally, it can be defined as follows:
R r,g,b = α ⊙ S + (1 − α) ⊙ B, (20)
where α is the blending ratio.Here, we set α to [0.8, 0.9].⊙ denotes the element-wise multiplication operator.Noted that R, G, and B channels are processed separately.


## Benchmark Statistics.

As a result, we propose a new single image deraining benchmark with high-quality backgrounds, diverse rain streaks, and harmonious layer blending, called HQ-RAIN.In total, the training and testing set of the HQ-RAIN contains 4,500 and 500 synthetic images, respectively.The average resolution of all images is 1367 × 931.See Figure 3 for several image pairs in HQ-RAIN.We also propose another new realistic dataset named RE-RAIN, to uniformly evaluate generalization performance of deraining models.Although several unlabeled real datasets have been collected [45], [132], there are some drawbacks that have the negative effects on the evaluation of generalization performance.For one thing, these real datasets contain some low-resolution images with watermarks, as well as unusual images from other bad weather conditions, which is beyond the scope of research on removing rain.For another, some rainy images have too light rain streaks, making it difficult to recognize their rain regions and thus unable to effectively evaluate deraining performance.To this end, we create a high-quality real benchmark RE-RAIN for evaluating real-world image deraining, containing 300 real rainy images without ground truths which are elaborately selected from the Internet and related works [74].


## Comparisons with Previous Datasets

Subjective Assessment.We conduct an online user study to evaluate the quality (i.e., how realistic) of the synthesis rainy images.Following the [70], we prepare for 70 rainy images, randomly chosen from 7 datasets (i.e., Rain100L/H [8], Rain800 [5], DID-Data [36], DDN-Data [35], RainDS-Syn [29], RainDirection [74] and HQ-RAIN) with 10 samples from each dataset.We recruit 30 participants with 15 males and 15 females.For each participant, we randomly show them 70 rainy images.Then, using a 5-point Likert scale (i.e., strongly agree, agree, borderline, disagree, and strongly disagree), all participants are asked to judge how realistic each image looks.Finally, 300 ratings are received for each category.Figure 4 shows the user study results.Our HQ-RAIN consistently outperforms other benchmarks, which also reveals that our synthetic rain is evaluated to be substantially more realistic than previous datasets.

Objective Assessment.In addition to subjective assessment, we also conduct objective comparisons to verify the high quality of our proposed dataset.Here, we adopt the Kullback-Leibler Divergence (KLD) [133], also known as relative entropy, to measure the difference between two probability distributions (i.e., the synthetic image and real- The vertical axis represents the value of KLD, and the horizontal axis represents the number of samples.Obviously, our proposed HQ-RAIN obtains a lowest KLD score, indicating that our dataset has a smaller domain gap between the synthetic and real-world images compared to others.

world image).Figure 5 presents the comparison results of the representative synthetic benchmarks [5], [8], [35], [36] and our benchmark, showing that our HQ-RAIN are close to the distribution of real-world rainy images.The reason behind this is that HQ-RAIN fully considers the harmony of the synthesized rainy images, thereby narrowing the domain gap between synthetic and real images.We hope our benchmark can provide new impetus for future research.


# COMPREHENSIVE EVALUATIONS AND ANALYSIS

In this section, we provide empirical analysis and conduct extensive evaluations of representative methods on several datasets and our proposed benchmark.In addition, we develop an online platform to provide the off-the-shelf toolkit, involving the large-scale performance evaluation.


## Track Establishment and Evaluated Methods

For a comprehensive performance evaluation, we first select previously representative benchmarks to participate in our survey.Faced with dozens of existing benchmarks, it is not rare that we feel confused about which dataset to choose for the experiment at hand.In fact, our goal is to enable models to learn better generalization from representative datasets.To this end, we perform cross-domain generalizability validation on the commonly used datasets using  [20].Table 4 shows that methods trained on the Rain13K [18] generalizes to an unseen samples well, due to the data diversity.In addition, a single dataset can also achieve the best results in certain specific scenarios, such as DDN-Data → RainDS-Real-RS.In other words, mixed datasets have advantages in comprehensive deraining performance, while single dataset has advantages in image-specific (e.g., heavy rain) deraining performance.

To intuitively compare the deraining results under these two different training modes, we create two main tracks, i.e., mixed training track and independent training track.For mixed training track, we adopt Rain13K benchmark [18] as the track participant.For independent training track, we find that Rain200L/H can help method generalize well than Rain100L/H, because Rain200L/H avoids the problem of duplicate image backgrounds in the training and testing sets in the old version.According to the generalization gain in Table 4, we finally adopt Rain200L/H [8], DID-Data [35], DDN-Data [35] and SPA-Data [15] as this track participants.Note that currently these two tracks only consider general rain removal, and do not include the datasets created for new tasks, such as RainKITTI2012/2015 [63] in stereo image deraining.In each track, the usage setting of training and testing datasets, as well as the measurement criteria, are the same.The detailed usage descriptions are tabulated in Table 3.In what follows, we will report the benchmarking results of representative methods on these two tracks.


## Evaluation on Mixed Training Track

In this track, a pretrained model is obtained to evaluate multiple testing sets, including Test100 [5], Rain100H [8], Rain100L [8], Test2800 [35], and Test1200 [35].From statistics in Table 1, we find that this track is highly favored by image TABLE 5: Quantitative comparisons on the Rain13K mixed benchmark dataset.Top 1 st and 2 nd results are marked in red and blue respectively.restoration task, thanks to the convenience of training on this track, which facilitates rapid evaluation of deraining performance.Here, we report 15 representative methods in Table 5, i.e., DerainNet [34], SIRR [45], DIDMDN [36], UMRL [44], RESCAN [38], PReNet [42], MSPFN [18], MPRNet [19], HINet [134], SPAIR [75], KiT [135], DGUNet [82], DGUNet+ [82], MAXIM-2S [83], and Restormer [20].Each method is evaluated under the same implementation using two wellknown used indicators: PSNR [21] and SSIM [117].So far, Restormer [20] has achieved the state-of-the-art quantitative average results in the mixed training track.In addition, DGUNet+ [82] obtained the second best average PSNR value, while MAXIM-2S [83] achieved the second best average SSIM score.Furthermore, we also present the visual comparison and the examples are shown in Figure 6.It can be seen that Transformers-based methods (e.g., MAXIM-2S [83] and Restormer [20]) can achieve significant improvement compared to the previous CNN models, as they can model the non-local information which is vital for high-quality image reconstruction.However, all approaches still need to break through in terms of image detail recovery.
Methods Test100 [5] Rain100H [8] Rain100L [8] Test2800 [35] Test1200 [35] Average PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM

## Evaluation on Independent Training Track

In this track, multiple pretrained models are obtained to evaluate the corresponding testing sets, including Rain200L [8], Rain200H [8], DID-Data [36], DDN-Data [35], and SPA-Data [15].Here, we provide 22 representative methods in Table 6, i.e., DDN [35], DID-MDN [36], RESCAN [38], NLEDN [39], JORDER-E [53], ID-CGAN [48], SIRR [45], PReNet [42], SPANet [15], FBL [62], MSPFN [18], RCDNet [57], Syn2Real [58], SGCN [104], MPRNet [19], DualGCN [13], SPDNet [12], Uformer [81], Restormer [20], IDT [11], HCN [10], and DRSformer [9].The quantitative results are quoted from previous works [9], [10].Each method is evaluated under the same implementation using PSNR [21] and SSIM [117].

So far, DRSformer [9] achieves the state-of-the-art quantitative average results in the independent training track.We can note that the best performance of different methods gradually converges.The performance gaps between DRSformer [9], Restormer [20] and IDT [11] are considerably close.We further present qualitative comparisons of deraining results in Figure 7, where the "spar" shares similar directions and distributions to rain streaks.Notably, none of the methods can preserve the original details of the image content area while removing the rain streaks.Overall, there is still a certain gap between these results and ground truths.


## Evaluation on the HQ-RAIN Dataset

We further conduct experiments on the proposed new benchmark to evaluate the performance of existing meth-ods.Specifically, we select 10 representative methods including 6 CNN-based approaches (i.e., LPNet [52], PReNet [42], JORDER-E [53], RCDNet [57], HINet [134] and SPDNet [12]) and 4 Transformer-based methods (i.e., Uformer [81],

Restormer [20], IDT [11] and DRSformer [9]) for evaluation.

For fair comparisons, we use the official released codes of these approaches.All methods are retrained on the HQ-RAIN benchmark.We choose their best pre-trained models, which achieve the highest PSNR value on the HQ-RAIN testing set.Meanwhile, these pre-trained models also are used to validate the RE-RAIN benchmark.All experiments are implemented on the servers equipped with NVIDIA GeForce RTX 3090 GPUs.For the HQ-RAIN benchmark PReNet [42] JORDER-E [53] RCDNet [57] HINet [134] SPDNet [12] Uformer [81] Restormer [20] IDT [11] DRSformer  with ground truth images, we use PSNR [21], SSIM [117],

and LPIPS [119] to evaluate the quality of each restored images.For the RE-RAIN benchmark without ground truth images, we use the non-reference metrics including NIQE [120], PIQE [136], and BRISQUE [121].Higher PSNR and SSIM scores indicate higher restoration quality, while lower LPIPS, NIQE, PIQE and BRISQUE values indicate better perceptual quality.The above-mentioned metrics are calculated based on the Y channel in YCbCr space of derained images for fair comparisons.

Table 7 demonstrates quantitative results of all methods on the HQ-RAIN and RE-RAIN benchmarks.We note that DRSformer [9] and Restormer [20] obtain the top two performance on the synthetic HQ-RAIN benchmark.However, they do not perform well on the real-world RE-RAIN benchmark, which means that these two methods do generalize well on real-world applications.In addition, among the CNN-based methods, HINet [134] achieves the competitive results on the HQ-RAIN.As some specific domain knowledge is beneficial for real-world rain removal [57], intergrading some specific domain knowledge into the model designs improves the generalization ability of deep learning.The results on the RE-RAIN demonstrate that most CNN-based methods have better generalization performance than Transformer-based methods as most CNNbased approaches integrate domain knowledge (e.g., rain structure and kernel) to better guide the model designs.Figures 8 and 9 show several visual comparisons.The DRSformer [9] performs better than the evaluated methods, which keeps consistent with the above quantitative results.Figure 9 show that LPNet [52] generates better derained images.Based on above evaluations, deraining real-world images is challenging and worthy further studied.


## Computational Complexity

The computational complexity is also one of the important factor for image deaining methods.Table 7 shows the computational complexity of different methods, including trainable model parameters, FLOPs and running time on a 256 × 256 image.LPNet [52] requires lower computational complexity as it develops a lightweight pyramid network using domain-specific knowledge to simplify the learning process.In contrast, the model size of HINet [134] is extremely large, reaching 88.67M, which limits its usage  in practical applications.The FLOPs of DRSformer [9] is relatively higher as it involves the computation of the topk self-attention.For inference time, most existing methods, especially Transformer-based approaches, are not efficient.Thus, how to develop efficient yet effective method is still worthy investigation.


## Application-Based Evaluation

To investigate whether the image deraining process benefits downstream vision-based applications, e.g., object detection, we apply the popular object detection pre-trained model (YOLOv3 [138]) to evaluate the deraining results.Here, we create a high-quality version of BDD350 (joint image deraining and object detection) [87].Following [139], we calculate the mean Average Precision (mAP) and mean Average Recall (mAR) under different Intersection of Union (IoU) thresholds using the available evaluation tool 2 .In addition, visual comparisons are presented in Figure 10.As one can see, the detection precision of the deraining results by all algorithms shows different degrees of improvement over that of original rainy inputs.From Table 8, RCDNet 2. https://github.com/PaddlePaddle/PaddleDetection[57] surprisingly achieves relatively robust performance on the object detection.This may be because this method can better protect the image semantic information by using an interpretable rain dictionary model.However, some recent methods (e.g., IDT [11] and DRSformer [9]) exhibit rapid performance degradation under higher threshold setting, suggesting that there are still rooms for improvement.


## Online Evaluation Platform

To facilitate the large-scale evaluation and tracking of the latest deraining technologies for general users, we develop an online platform 3 for current research development of image deraining.Figure 11 shows some screenshots of the proposed online evaluation platform.The proposed evaluation platform provides a comprehensive repository, including direct links and source codes for 100 representative methods, to continuously track recent developments in this fast-advancing field.It also provides evaluations of several popular and representative image deraining methods to better facilitate the following researches.In addition, the evaluation platform allows users submit the derained


# FUTURE RESEARCH PROSPECTS

Single image deraining has undergone rapid development in recent years, thanks to the incorporation of deep learning techniques.Researchers have made significant contributions by proposing effective models, promoting the optimization process, and establishing abundant datasets to advance the field.Despite achieving promising results, there are still several opportunities to explore in this field including how to model the degradation process, effective deraining models, and its potential applications for the following image analysis tasks.


## Degradation Progress

The degradation progress models the generation of the rainy images.Thus, how to better model the degradation progress is critical for image deraining in the deep learning era.As the real degradation is complex, the image synthesization based on the linear composition model, e.g., (1), may not be able to model the real degradation.The methods trained on the datasets based on the linear composition model may not generalize well on real-world applications due to the domain gap of the data distributions.

Although several approaches design kinds of hardwares to capture real-world rainy images, the diversity of images and complexity of rainy environments (e.g., low-light environment, hazy environment, and so on) are not sufficient to cover the realistic scenarios.Therefore, how to describe the degradation of clear image in rainy settings including the rainy streak distributions, rainy image properties, the laws of physics of rain and so on is a great of need but still challenging.


## Model Development

High-quality Image Deraining Models.As image deraining aims to recover realistic clear images, how to develop an effective model that describes the properties of clear image is important.Below we will present some promising directions for network improvements.(1) Integrating domain knowledge into model development.For instance, by leveraging semantic segmentation, we can identify specific regions of interest in the image such as sky or roads [140].This may allow us to use unequal information from different regions to guide the model in removing rain and avoiding unnecessary information loss.(2) Modeling the distributions of clear images.We can use image generation approaches (e.g., VQGAN [141] or diffusion models [100]) to facilitate the high-quality image restoration.(3) Exploring external information of clear images or distilling the knowledge from large pre-trained models to facilitate the rain removal.By prompt learning or knowledge distillation, the model can benefit from the learned representations and generalization capabilities.This allows the model to effectively utilize the specific prompts or knowledge to better understand and remove rain from images.

Efficient Image Deraining Models.Developing efficient image deraining models is important due to resource-limited devices in most real-world applications.Facing this challenge, we can adopt pruning [142], dimension reduction and feature reuse [143] to remove unnecessary connections or parameters from the network.In addition, knowledge distillation [144] can also be used to facilitate the transfer of knowledge from a complex model to a compact one, effectively compressing the model while maintaining performance.We can also explore neural architecture search [145] to learn compact models by introducing various search strategies to sample architectures from the defined search space.


## Potential Applications

Relations of Image Deraining and Other Tasks.Most existing methods mainly focuses on the individual image deraining task and usually use the image deraining task as a pre-or post-processing step for other tasks.Although the development of these fields are independent, different tasks can have a constructive influence in promoting each other, such as Deblur-YOLO [146] and YOLO-in-the-Dark [147].We call for closer collaboration across low-level and highlevel communities.


# CONCLUDING REMARKS

We have provided comprehensive overview of the existing image deraining methods.We first analyze the research progress, discuss the limitations, and provide empirical evaluations of image deraining methods.To better evaluate existing methods, we propose develop a rainy image generation approach based on an image harmonization approach and develop a realistic image deraining dataset.We have established two parallel tracks (i.e., mixed training track and independent training track) to facilitate effective and fair performance comparison among different methods.We carry out qualitative and quantitative analyses to investigate the open challenges and provide the future prospects.To provide fair evaluation protocols, we build an online platform to better evaluate the deraining methods and facilitate the following researches.The online evaluation platform is publicly available and will be regularly updated.

## Fig. 1 :
1
Fig. 1: Analysis of commonly used synthetic benchmark characteristic and usage based on over a hundred literatures about image deraining.(a) Our proposed new benchmark narrows the domain gap between real rainy images, due to its properties of high-quality rain with higher realism and diversity, and high-quality GT with higher harmony and resolution.(b) Statistics on the usage relationship between training and test datasets in their experiments with existing methods.The upper semicircle means training datasets, and the lower semicircle means testing datasets.


## Fig. 3 :
3
Fig. 3: More example image pairs sampled from the proposed HQ-RAIN.Best viewed by zooming in the figures.


## Fig. 4 :
4
Fig.4: User study results.The ratings given by all participants on different synthesis datasets.


## Fig. 6 :
6
Fig. 6: Visual quality comparison of mixed training track on the Rain100H dataset.Please zoom in the figures for better view of the rain removal and detail recovery.


## Fig. 7 :
7
Fig. 7: Visual quality comparison of independent training track on the DDN-Data dataset.Please zoom in the figures for better view of the rain removal and detail recovery.


## Fig. 8 :
8
Fig. 8: Visual quality comparison of new benchmark track on the HQ-RAIN dataset.Please zoom in the figures for better view of the rain removal and detail recovery.


## Fig. 9 :
9
Fig. 9: Visual quality comparison of new benchmark track on the RE-RAIN dataset.Please zoom in the figures for better view of the rain removal and detail recovery.


## Fig. 10 :
10
Fig. 10: Visual comparison of joint image deraining and object detection.Please zoom in the figures for better view of the rain removal and object detection.


## [ 22  22 Fig. 11 :
222211
Fig.11: Screenshots of the developed online evaluation platform.Some design styles refer to the SIDD website[137].


## TABLE 3 :
3
Dataset descriptions of mixed training track and independent training track.
TracksMixed Training TrackIndependent Training TrackSource DatasetsRain800 [5]Rain100H [8]Rain100L [8]Rain14000 [35]Rain1200 [35]Rain12 [31]TotalRain200L [8]Rain200H [8]DID-Data [35]DDN-Data [35]SPA-Data [15]Train Samples7001800011,200012137121800180012,00012,600638,492Test Samples10010010028001200043002002001,2001,4001,000NameTest100Rain100H Rain100LTest2800Test1200--Rain200L Rain200H DID-Data DDN-Data SPA-Data1.4 1.6 1.8Rain100L Rain100H Rain800 DID-Data DDN-Data HQ-RAINKullback-Leibler Divergence1.2value1.00.80.60.40.216number 11 16 21 26 31 36 41 46
Fig. 5: Comparison of Kullback CLeibler divergence (KLD) between different synthetic datasets and one real dataset.


## TABLE 4 :
4
Cross-domain generalizability analysis.Training Set → Testing Set means training on the dataset X and testing on the dataset Y.
Training Set → Testing SetPSNRSSIMAvg.PSNR Avg.SSIM-→ RainDS-Syn-RS -→ RainDS-Real-RS22.47 23.580.6765 0.651523.020.6640Rain100L → RainDS-Syn-RS Rain100L → RainDS-Real-RS23.41 23.250.7026 0.648323.330.6754Rain100H → RainDS-Syn-RS Rain100H → RainDS-Real-RS26.19 23.620.8502 0.658724.900.7544Rain200L → RainDS-Syn-RS Rain200L → RainDS-Real-RS23.48 23.320.7085 0.650723.400.6796Rain200H → RainDS-Syn-RS Rain200H → RainDS-Real-RS26.33 23.680.8599 0.660125.000.7600DID-Data → RainDS-Syn-RS DID-Data → RainDS-Real-RS26.93 24.190.7699 0.665225.560.7175DDN-Data → RainDS-Syn-RS DDN-Data → RainDS-Real-RS29.02 24.740.8193 0.689326.880.7543SPA-Data → RainDS-Syn-RS SPA-Data → RainDS-Real-RS22.65 23.930.7226 0.662323.290.6924GT-RAIN → RainDS-Syn-RS GT-RAIN → RainDS-Real-RS22.33 23.280.7088 0.675022.800.6919Rain13K → RainDS-Syn-RS Rain13K → RainDS-Real-RS30.83 24.730.8890 0.688927.780.7889same deep model Restormer

## TABLE 6 :
6
Results of independent training track.Quantitative comparisons on five previous commonly used benchmark datasets.Top 1 st and 2 nd results are marked in red and blue respectively.Average PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑
MethodsRain200L [8]Rain200H [8]DID-Data [36]DDN-Data [35]SPA-Data [15]Inputs26.710.843813.080.373423.640.779425.240.809834.140.941524.560.7496DDN [35]34.680.967126.050.805630.970.911630.000.904136.160.945731.570.9068DID-MDN [36]35.400.961826.610.824231.300.920731.490.914638.160.976332.590.9195RESCAN [38]36.090.969726.750.835333.380.941731.940.934538.110.970733.250.9304NLEDN [39]39.130.982129.790.900534.680.958332.150.939842.970.983535.740.9528JORDER-E [53]37.250.975229.350.890533.980.950232.010.932140.780.980134.670.9456ID-CGAN [48]35.190.969425.020.843030.250.921729.060.916238.470.962431.590.9225SIRR [45]34.750.969026.550.819030.570.910430.010.907835.310.941131.430.9095PReNet [42]37.800.981429.040.899133.170.948132.600.945940.160.981634.550.9512SPANet [15]35.790.965326.270.866633.040.948929.850.911740.240.981133.030.9347FBL [62]39.020.982730.070.902134.260.932033.050.933442.800.982435.840.9465MSPFN [18]38.580.982729.360.903433.720.955032.990.933343.430.984335.610.9517RCDNet [57]39.170.988530.240.904834.080.953233.040.947243.360.983135.970.9554Syn2Real [58]35.920.970927.900.860532.390.939029.290.905735.280.974532.150.9301SGCN [104]37.950.982229.350.905533.580.947732.140.936443.840.986235.370.9516MPRNet [19]39.470.982530.670.911033.990.959033.100.934743.640.984436.170.9543DualGCN [13]40.730.988631.150.912534.370.962033.010.948944.180.990236.680.9604SPDNet [12]40.500.987531.280.920734.570.956033.150.945743.200.987136.540.9594Uformer [81]40.200.986030.800.910535.020.962133.950.954546.130.991337.220.9609Restormer [20]40.990.989032.000.932935.290.964134.200.957147.980.992138.090.9670IDT [11]40.740.988432.100.934434.890.962333.840.954947.350.993037.780.9666HCN [10]41.310.989231.340.924834.700.961333.420.951245.030.990737.160.9634DRSformer [9]41.230.989432.170.932635.350.964634.350.958848.540.992438.320.9676

## TABLE 7 :
7
Quantitative comparisons on the HQ-RAIN and RE-RAIN benchmark datasets.Top 1 st and 2 nd results are marked in red and blue respectively."#FLOPs", "#Params" and "#Runtime" represent FLOPs (in G), the number of trainable parameters (in M) and inference time (in second), respectively.
MethodsLPNet [52]

## TABLE 8 :
8
Quantitative comparisons of mAP and mAR under different IoU thresholds of downstream object detection task.Top 1 st and 2 nd results are marked in red and blue respectively.
MethodsLPNet [52]PReNet [42]JORDER-E [53]RCDNet [57]HINet [134]SPDNet [12]Uformer [81]Restormer [20]IDT [11]DRSformer [9]IoU threshold=0.5mAP ↑0.4800.5070.5420.5560.5660.4690.5730.4990.5410.556mAR ↑0.7040.6990.6980.7390.7320.6570.7360.6870.7360.735IoU threshold=0.6mAP ↑0.3330.3750.4290.4300.3790.3810.4250.4260.3880.397mAR ↑0.5600.4690.5750.6170.5070.4710.6060.6410.4740.482IoU threshold=0.7mAP ↑0.2490.2910.2850.3060.2990.2910.3000.3050.3010.301mAR ↑0.3630.3640.3570.3740.3680.3590.3750.3670.372
http://yu-li.github.io/paper/licvpr16 rain.zipRain100L [8] 1,800/100 Syn RS LSM CVPR 2017 https://www.icst.pku.edu.cn/struct/Projects/jointrain removal.htmlRain100H [8] 1,800/100 Syn RS LSM CVPR 2017 https://www.icst.pku.edu.cn/struct/Projects/jointrain removal.htmlRain200L [8] 1,800/200 Syn RS LSM CVPR 2017 https://www.icst.pku.edu.cn/struct/Projects/jointrain removal.htmlRain200H [8] 1,800/200 Syn RS LSM CVPR 2017 https://www.icst.pku.edu.cn/struct/Projects/jointrain removal.htmlDDN-Data [35] 12,600/1,400 Syn RS LSM CVPR 2017 https://xueyangfu.github.io/projects/cvpr2017.htmlDID-Data [36] 12,000/1,200 Syn RS LSM CVPR 2018 https://github.com/hezhangsprinter/DID-MDNRainDrop [28] 861/249 Syn RD RMM CVPR 2018 https://github.com/rui1996/DeRaindropRain800 [5] 700/100 Syn RS LSM TCSVT 2019 https://github.com/hezhangsprinter/ID-CGANSPA-Data [15] 638,492/1,000 Real RS LSM CVPR 2019 https://github.com/stevewongv/SPANetMPID [5] 3,961/419 Syn+Real RS+RD+RA LSM+HRM CVPR 2019 https://github.com/panda-lab/Single-Image-DerainingRainCityscapes [25] 9,432/1,188 Syn RS+RA DRM CVPR 2019 https://github.com/xw-hu/DAF-NetOutdoor-Rain [43] 9,000/1,500 Syn RS+RA HRM CVPR 2019 https://github.com/liruoteng/HeavyRainRemovalRain13K [18] 13,712/4,298 Syn RS LSM CVPR 2020 https://github.com/kuijiang94/MSPFNRainKITTI2012 [63] 4,062/4,085 Syn RS LSM ECCV 2020 https://github.com/HDCVLab/Stereo-Image-DerainingRainKITTI2015 [63] 4,200/4,189 Syn RS LSM ECCV 2020 https://github.com/HDCVLab/Stereo-Image-DerainingRainDS [29] 3,450/900 Syn+Real RS+RD LSM+RMM+MRM CVPR 2021 https://github.com/Songforrr/RainDSCCN RainDirection [74] 2,920/430 Syn RS LSM ICCV 2021 https://github.com/Yueziyu/RainDirection-and-Real3000-DatasetGT-RAIN [16] 28,217/2,100 Real RS LSM ECCV 2022 https://github.com/UCLA-VMG/GT-RAIN
Real-world anomaly detection in surveillance videos. W Sultani, C Chen, M Shah, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018

Deep reinforcement learning for autonomous driving: A survey. B R Kiran, I Sobh, V Talpaert, P Mannion, A A Al Sallab, S Yogamani, P Pérez, IEEE Transactions on Intelligent Transportation Systems. 12021

Deep learning for person re-identification: A survey and outlook. M Ye, J Shen, G Lin, T Xiang, L Shao, S C Hoi, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4462021

A survey on rain removal from video and single image. H Wang, Y Wu, M Li, Q Zhao, D Meng, arXiv:1909.083262019. 1, 2, 3, 4arXiv preprint

Single image deraining: A comprehensive benchmark analysis. S Li, I B Araujo, W Ren, Z Wang, E K Tokuda, R H Junior, R Cesar-Junior, J Zhang, X Guo, X Cao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition201910121, 4, 7, 8, 9

Single image deraining: From model-based to data-driven and beyond. W Yang, R T Tan, S Wang, Y Fang, J Liu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 43112020. 1, 2, 3, 4

Datadriven single image deraining: a comprehensive review and new perspectives. Z Zhang, Y Wei, H Zhang, Y Yang, S Yan, M Wang, Pattern Recognition. 121097402023

Deep joint rain detection and removal from a single image. W Yang, R T Tan, J Feng, J Liu, Z Guo, S Yan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017. 1, 3, 4, 5, 7, 8, 91013

Learning a sparse transformer network for effective image deraining. X Chen, H Li, M Li, J Pan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2023. 1, 4, 5, 6, 121315

Continual image deraining with hypergraph convolutional networks. X Fu, J Xiao, Y Zhu, A Liu, F Wu, Z.-J Zha, IEEE Transactions on Pattern Analysis and Machine Intelligence. 12132023. 1, 4, 6

Image de-raining transformer. J Xiao, X Fu, A Liu, F Wu, Z.-J Zha, IEEE Transactions on Pattern Analysis and Machine Intelligence. 13152022. 1, 4, 5, 6, 12

Structurepreserving deraining with residue channel prior guidance. Q Yi, J Li, Q Dai, F Fang, G Zhang, T Zeng, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2021. 1, 4, 6, 121315

Rain streak removal via dual graph convolutional network. X Fu, Q Qi, Z.-J Zha, Y Zhu, X Ding, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2021113

Detail-recovery image deraining via context aggregation networks. S Deng, M Wei, J Wang, Y Feng, L Liang, H Xie, F L Wang, M Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition202014

Spatial attentive single-image deraining with a high quality real rain dataset. T Wang, X Yang, K Xu, S Chen, Q Zhang, R W Lau, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2019. 279. 1, 4, 5, 7, 8, 111213

Not just streaks: Towards ground truth for single image deraining. Y Ba, H Zhang, E Yang, A Suzuki, A Pfahnl, C C Chandrappa, C M De Melo, S You, S Soatto, A Wong, Proceedings of the European Conference on Computer Vision. the European Conference on Computer Vision202219

Toward real-world single image deraining: A new benchmark and beyond. W Li, Q Zhang, J Zhang, Z Huang, X Tian, D Tao, arXiv:2206.05514202214arXiv preprint

Multi-scale progressive fusion network for single image deraining. K Jiang, Z Wang, P Yi, C Chen, B Huang, Y Luo, J Ma, J Jiang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition1, 4, 5, 6, 7, 8, 920201213

Multi-stage progressive image restoration. S W Zamir, A Arora, S Khan, M Hayat, F S Khan, M.-H Yang, L Shao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition202114131, 4, 5

Restormer: Efficient transformer for high-resolution image restoration. S W Zamir, A Arora, S Khan, M Hayat, F S Khan, M.-H Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2022. 1, 4, 6, 111215

Scope of validity of psnr in image/video quality assessment. Q Huynh-Thu, M Ghanbari, Electronics Letters. 4413142008. 1, 8

Dovenet: Deep image harmonization via domain verification. W Cong, J Zhang, L Niu, L Liu, Z Ling, W Li, L Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2020210

Removing rain from a single image via discriminative sparse coding. Y Luo, Y Xu, H Ji, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision201534

Optics of the atmosphere: scattering by molecules and particles. E J Mccartney, 1976New York

Depth-attentional features for single-image rain removal. X Hu, C.-W Fu, L Zhu, P.-A Heng, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition201973, 4, 5

Rethinking image deraining via rain streaks and vapors. Y Wang, Y Song, C Ma, B Zeng, Proceedings of the European Conference on Computer Vision. the European Conference on Computer Vision202035

Adherent raindrop modeling, detection and removal in video. S You, R T Tan, R Kawakami, Y Mukaigawa, K Ikeuchi, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3892015

Attentive generative adversarial network for raindrop removal from a single image. R Qian, R T Tan, W Yang, J Su, J Liu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition201873, 4, 6

Removing raindrops and rain streaks in one go. R Quan, X Yu, Y Liang, Y Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2021103, 4, 8, 9

Automatic single-imagebased rain streaks removal via image decomposition. L.-W Kang, C.-W Lin, Y.-H Fu, IEEE Transactions on Image Processing. 2142011. 3, 4

Rain streak removal using layer priors. Y Li, R T Tan, X Guo, J Lu, M S Brown, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition20168113, 4, 7

Joint bi-layer optimization for single-image rain streak removal. L Zhu, C.-W Fu, D Lischinski, P.-A Heng, Proceedings of the IEEE. the IEEE20174

Joint convolutional analysis and synthesis sparse representation for single image layer separation. S Gu, D Meng, W Zuo, L Zhang, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision20174

Clearing the skies: A deep network architecture for single-image rain removal. X Fu, J Huang, X Ding, Y Liao, J Paisley, IEEE Transactions on Image Processing. 266122017

Removing rain from single images via a deep detail network. X Fu, J Huang, D Zeng, Y Huang, X Ding, J Paisley, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017. 4, 5, 7, 8, 91013

Density-aware single image deraining using a multi-stream dense network. H Zhang, V M Patel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018. 4, 5, 7, 8, 91013

Learning dual convolutional neural networks for low-level vision. J Pan, S Liu, D Sun, J Zhang, Y Liu, J Ren, Z Li, J Tang, H Lu, Y.-W Tai, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition20184

Recurrent squeeze-andexcitation context aggregation net for single image deraining. X Li, J Wu, Z Lin, H Liu, H Zha, Proceedings of the European Conference on Computer Vision. the European Conference on Computer Vision2018413

Nonlocally enhanced encoder-decoder network for single image deraining. G Li, X He, W Zhang, H Chang, L Dong, L Lin, Proceedings of the 26th ACM International Conference on Multimedia. the 26th ACM International Conference on Multimedia2018413

Residual-guide network for single image deraining. Z Fan, H Wu, X Fu, Y Huang, X Ding, Proceedings of the 26th ACM International Conference on Multimedia. the 26th ACM International Conference on Multimedia2018

Gated context aggregation network for image dehazing and deraining. D Chen, M He, Q Fan, J Liao, L Zhang, D Hou, L Yuan, G Hua, Proceedings of the IEEE Winter Conference on Applications of Computer Vision. the IEEE Winter Conference on Applications of Computer Vision20194

Progressive image deraining networks: A better and simpler baseline. D Ren, W Zuo, Q Hu, P Zhu, D Meng, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2019415

Heavy rain image restoration: Integrating physics model and conditional adversarial learning. R Li, L.-F Cheong, R T Tan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2019

Uncertainty guided multi-scale residual learning-using a cycle spinning cnn for single image deraining. R Yasarla, V M Patel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2019412

Semi-supervised transfer learning for image rain removal. W Wei, D Meng, Q Zhao, Z Xu, Y Wu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2019413

Deep learning for seeing through window with raindrops. Y Quan, S Deng, Y Chen, H Ji, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition20194

Erl-net: Entangled representation learning for single image de-raining. G Wang, C Sun, A Sowmya, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision20194

Image de-raining using a conditional generative adversarial network. H Zhang, V Sindagi, V M Patel, IEEE Transactions on Circuits and Systems for Video Technology. 20193013

Dtdn: Dual-task de-raining network. Z Wang, J Li, G Song, Proceedings of the 27th ACM International Conference on Multimedia. the 27th ACM International Conference on Multimedia20194

Gradual network for single image de-raining. W Yu, Z Huang, W Zhang, L Feng, N Xiao, Proceedings of the 27th ACM International Conference on Multimedia. the 27th ACM International Conference on Multimedia2019

Single image deraining via recurrent hierarchy enhancement network. Y Yang, H Lu, Proceedings of the 27th ACM International Conference on Multimedia. the 27th ACM International Conference on Multimedia20194

Lightweight pyramid networks for image deraining. X Fu, B Liang, Y Huang, X Ding, J Paisley, IEEE Transactions on Neural Networks and Learning Systems. 316152019. 4, 6, 13

Joint rain detection and removal from a single image with contextualized deep networks. W Yang, R T Tan, J Feng, Z Guo, S Yan, J Liu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 426152019. 4, 5, 12, 13

Singe image rain removal with unpaired information: A differentiable programming perspective. H Zhu, X Peng, J T Zhou, S Yang, V Chanderasekh, L Li, J.-H Lim, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201945

Unsupervised single image deraining with self-supervised constraints. X Jin, Z Chen, J Lin, Z Chen, W Zhou, Proceedings of the IEEE International Conference on Image Processing. the IEEE International Conference on Image Processing201945

Dual residual networks leveraging the potential of paired operations for image restoration. X Liu, M Suganuma, Z Sun, T Okatani, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition20194

A model-driven deep neural network for single image rain removal. H Wang, Q Xie, Q Zhao, D Meng, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2020415

Syn2real transfer learning for image deraining using gaussian processes. R Yasarla, V A Sindagi, V M Patel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2020413

All in one bad weather removal using architectural search. R Li, R T Tan, L.-F Cheong, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition20204

Conditional variational image deraining. Y Du, J Xu, X Zhen, M.-M Cheng, L Shao, IEEE Transactions on Image Processing. 2972020

Physics-based generative adversarial models for image restoration and beyond. J Pan, J Dong, Y Liu, J Zhang, J Ren, J Tang, Y.-W Tai, M.-H Yang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 43762020

Towards scale-free rain streak removal via self-supervised fractal band learning. W Yang, S Wang, D Xu, X Wang, J Liu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence20201213

Beyond monocular deraining: Stereo image deraining via semantic understanding. K Zhang, W Luo, W Ren, J Wang, F Zhao, L Ma, H Li, Proceedings of the European Conference on Computer Vision. the European Conference on Computer Vision2020411

Dcsfn: Deep cross-scale fusion network for single image rain removal. C Wang, X Xing, Y Wu, Z Su, J Chen, Proceedings of the 28th ACM International Conference on Multimedia. the 28th ACM International Conference on Multimedia202045

Joint self-attention and scaleaggregation for self-calibrated deraining network. C Wang, Y Wu, Z Su, J Chen, Proceedings of the 28th ACM International Conference on Multimedia. the 28th ACM International Conference on Multimedia202045

Deraincyclegan: Rain attentive cyclegan for single image deraining and rainmaking. Y Wei, Z Zhang, Y Wang, M Xu, Y Yang, S Yan, M Wang, IEEE Transactions on Image Processing. 3052021

Pre-trained image processing transformer. H Chen, Y Wang, T Guo, C Xu, Y Deng, Z Liu, S Ma, C Xu, C Xu, W Gao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2021126

Memory oriented transfer learning for semi-supervised image deraining. H Huang, A Yu, R He, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition202147

Robust representation learning with feedback for single image deraining. C Chen, H Li, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition202146

From rain generation to rain removal. H Wang, Z Yue, Q Xie, Q Zhao, Y Zheng, D Meng, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition20211410

Closing the loop: Joint rain generation and removal via disentangled image translation. Y Ye, Y Chang, H Zhou, L Yan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition202146

Controlling the rain: From removal to rendering. S Ni, X Cao, T Yue, X Hu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition202146

Multi-decoding deraining network and quasi-sparsity based training. Y Wang, C Ma, B Zeng, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2021137

Unpaired learning for deep image deraining with rain direction regularizer. Y Liu, Z Yue, J Pan, Z Su, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2021104, 5, 7, 8, 9

Spatially-adaptive image restoration using distortion-guided networks. K Purohit, M Suin, A Rajagopalan, V N Boddeti, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2021412

Unsupervised image deraining: Optimization model driven deep cnn. C Yu, Y Chang, Y Li, X Zhao, L Yan, Proceedings of the 29th ACM International Conference on Multimedia. the 29th ACM International Conference on Multimedia202147

Efficientderain: Learning pixel-wise dilation filtering for high-efficiency single-image deraining. Q Guo, J Sun, F Juefei-Xu, L Ma, X Xie, W Feng, Y Liu, J Zhao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence20214

Selective wavelet attention learning for single image deraining. H Huang, A Yu, Z Chai, R He, T Tan, International Journal of Computer Vision. 2021129

Unpaired deep image deraining using dual contrastive learning. X Chen, J Pan, K Jiang, Y Li, Y Huang, C Kong, L Dai, Z Fan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition202247

Unsupervised deraining: Where contrastive learning meets selfsimilarity. Y Ye, C Yu, Y Chang, L Zhu, X.-L Zhao, L Yan, Y Tian, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition202247

Uformer: A general u-shaped transformer for image restoration. Z Wang, X Cun, J Bao, W Zhou, J Liu, H Li, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition20221715

Deep generalized unfolding networks for image restoration. C Mou, Q Wang, J Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition20221712

Maxim: Multi-axis mlp for image processing. Z Tu, H Talebi, H Zhang, F Yang, P Milanfar, A Bovik, Y Li, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2022412

All-in-one image restoration for unknown corruption. B Li, X Liu, P Hu, Z Wu, J Lv, X Peng, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition202217

Transweather: Transformer-based restoration of images degraded by adverse weather conditions. J M J Valanarasu, R Yasarla, V M Patel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition20224

Generative status estimation and information decoupling for image rain removal. D Lin, W Xin, J Shen, R Zhang, R Liu, M Wang, W Xie, Q Guo, P Li, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing Systems2022

Magic elf: Image deraining meets association learning and transformer. K Jiang, Z Wang, C Chen, Z Wang, L Cui, C.-W Lin, 615

Hybrid cnn-transformer feature fusion for single image deraining. X Chen, J Pan, J Lu, Z Fan, H Li, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202346

Restoring vision in adverse weather conditions with patch-based denoising diffusion models. O Özdenizci, R Legenstein, IEEE Transactions on Pattern Analysis and Machine Intelligence. 42023

Smartassign: Learning a smart knowledge assignment strategy for deraining and desnowing. Y Wang, C Ma, J Liu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition20234

Guided image filtering. K He, J Sun, X Tang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3562012

Removing rain and snow in a single image using guided filter. J Xu, W Zhao, P Liu, X Tang, Proceedings of the IEEE International Conference on Computer Science and Automation Engineering. the IEEE International Conference on Computer Science and Automation Engineering20122

An improved guidance image based method to remove rain and snow in a single image. Computer and Information Science. 53492012

Single-image deraining using an adaptive nonlocal means filter. J.-H Kim, C Lee, J.-Y Sim, C.-S Kim, Proceedings of the IEEE International Conference on Image Processing. the IEEE International Conference on Image Processing2013

Single-imagebased rain and snow removal using multi-guided filter. X Zheng, Y Liao, W Guo, X Fu, X Ding, Proceedings of the International Conference on Neural Information Processing. the International Conference on Neural Information Processing2013

Single image rain and snow removal via guided l0 smoothing filter. X Ding, L Chen, X Zheng, Y Huang, D Zeng, Multimedia Tools and Applications. 201675

A generalized low-rank appearance model for spatio-temporally correlated rain streaks. Y.-L Chen, C.-T Hsu, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2013

Generative adversarial networks. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Communications of the ACM. 631162020

Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in Neural Information Processing Systems. 2017306

Denoising diffusion probabilistic models. J Ho, A Jain, P Abbeel, Advances in Neural Information Processing Systems. 20203316

An unsupervised attentive-adversarial learning framework for single image deraining. W Liu, R Jiang, C Chen, T Lu, Z Xiong, arXiv:2202.096352022arXiv preprint

Restoring an image taken through a window covered with dirt or rain. D Eigen, D Krishnan, R Fergus, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision20135

Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition20165

Successive graph convolutional network for image de-raining. X Fu, Q Qi, Z.-J Zha, X Ding, F Wu, J Paisley, International Journal of Computer Vision. 1296132021

Unpaired deep image dehazing using contrastive disentanglement learning. X Chen, Z Fan, P Li, L Dai, C Kong, Z Zheng, Y Huang, Y Li, European Conference on Computer Vision. 2022

Singan: Learning a generative model from a single natural image. T R Shaham, T Dekel, T Michaeli, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2019

A survey on vision transformer. K Han, Y Wang, H Chen, X Chen, J Guo, Z Liu, Y Tang, A Xiao, C Xu, Y Xu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4512022

U-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, Proceedings of the Medical Image Computing and Computer-Assisted Intervention. the Medical Image Computing and Computer-Assisted Intervention2015

Recent developments in total variation image restoration. T Chan, S Esedoglu, F Park, A Yip, Mathematical Models of Computer Vision. 1722005

Contour detection and hierarchical image segmentation. P Arbelaez, M Maire, C Fowlkes, J Malik, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3352010

Photorealistic rendering of rain streaks. K Garg, S K Nayar, ACM Transactions on Graphics. 2532006

Ucid: An uncompressed color image database. G Schaefer, M Stich, Proceedings of the Storage and Retrieval Methods and Applications for Multimedia. the Storage and Retrieval Methods and Applications for Multimedia20035307

The cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition20167

Unsupervised monocular depth estimation with left-right consistency. C Godard, O Mac Aodha, G J Brostow, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition20177

Vision meets robotics: The kitti dataset. A Geiger, P Lenz, C Stiller, R Urtasun, The International Journal of Robotics Research. 32112013

Ntire 2017 challenge on single image super-resolution: Methods and results. R Timofte, E Agustsson, L Van Gool, M.-H Yang, L Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition Workshops20178

Image quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE Transactions on Image Processing. 134142004

Fsim: A feature similarity index for image quality assessment. L Zhang, L Zhang, X Mou, D Zhang, IEEE Transactions on Image Processing. 2082011

The unreasonable effectiveness of deep features as a perceptual metric. R Zhang, P Isola, A A Efros, E Shechtman, O Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018814

No-reference image quality assessment in the spatial domain. A Mittal, A K Moorthy, A C Bovik, IEEE Transactions on Image Processing. 2112142012

Making a "completely blind" image quality analyzer. A Mittal, R Soundararajan, A C Bovik, IEEE Signal Processing Letters. 203142012

No-reference image quality assessment based on spatial and spectral entropies. L Liu, B Liu, H Huang, A C Bovik, Signal Processing: Image Communication. 2982014

Deep learning for generic object detection: A survey. L Liu, W Ouyang, X Wang, P Fieguth, J Chen, X Liu, M Pietikäinen, International Journal of Computer Vision. 12882020

The devil is in the decoder: Classification, regression and gans. Z Wojna, V Ferrari, S Guadarrama, N Silberman, L.-C Chen, A Fathi, J Uijlings, International Journal of Computer Vision. 12782019

Towards robust rain removal against adversarial attacks: A comprehensive benchmark analysis and beyond. Y Yu, W Yang, Y.-P Tan, A C Kot, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition20229

Adversarial attack with raindrops. J Liu, B Lu, M Xiong, T Zhang, H Xiong, arXiv:2302.142672023arXiv preprint

Dslr-quality photos on mobile devices with deep convolutional networks. A Ignatov, N Kobyshev, R Timofte, K Vanhoey, L Van Gool, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision20179

Vision and rain. K Garg, S K Nayar, International Journal of Computer Vision. 7592007

Rain streaks removal for single image via kernel-guided convolutional neural network. Y.-T Wang, X.-L Zhao, T.-X Jiang, L.-J Deng, Y Chang, T.-Z Huang, IEEE Transactions on Neural Networks and Learning Systems. 3282020

Compositing digital images. T Porter, T Duff, Proceedings of the 11th Annual Conference on Computer Graphics and Interactive Techniques. the 11th Annual Conference on Computer Graphics and Interactive Techniques1984

Deep image blending. L Zhang, T Wen, J Shi, Proceedings of the IEEE Winter Conference on Applications of Computer Vision. the IEEE Winter Conference on Applications of Computer Vision2020

Semi-deraingan: A new semi-supervised single image deraining. Y Wei, Z Zhang, Y Wang, H Zhang, M Zhao, M Xu, M Wang, Proceedings of the IEEE International Conference on Multimedia and Expo. the IEEE International Conference on Multimedia and Expo2021

Kullback-leibler divergence. J M Joyce, International Encyclopedia of Statistical Science. 2011

Hinet: Half instance normalization network for image restoration. L Chen, X Lu, J Zhang, X Chu, C Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition20211215

Knn local attention for image restoration. H Lee, H Choi, K Sohn, D Min, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition202212

Blind image quality evaluation using perception based features. N Venkatanath, D Praneeth, M C Bh, S S Channappayya, S S Medasani, Proceedings of the National Conference on Communications. the National Conference on Communications2015

A high-quality denoising dataset for smartphone cameras. A Abdelhamed, S Lin, M S Brown, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition201816

Yolov3: An incremental improvement. J Redmon, A Farhadi, arXiv:1804.02767201815arXiv preprint

Low-light image and video enhancement using deep learning: A survey. C Li, C Guo, L Han, J Jiang, M.-M Cheng, J Gu, C C Loy, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4412152021

A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, arXiv:2304.02643Segment anything. 202316arXiv preprint

Taming transformers for high-resolution image synthesis. P Esser, R Rombach, B Ommer, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition20211216

Learned token pruning for transformers. S Kim, S Shen, D Thorsley, A Gholami, W Kwon, J Hassoun, K Keutzer, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining202217

Deep image matting: A comprehensive survey. J Li, J Zhang, D Tao, arXiv:2304.04672202317arXiv preprint

Knowledge distillation: A survey. J Gou, B Yu, S J Maybank, D Tao, International Journal of Computer Vision. 129172021

Neural architecture search: A survey. T Elsken, J H Metzen, F Hutter, The Journal of Machine Learning Research. 201171997-2017, 2019

Deblur-yolo: Real-time object detection with efficient blind motion deblurring. S Zheng, Y Wu, S Jiang, C Lu, G Gupta, Proceedings of the International Joint Conference on Neural Networks. the International Joint Conference on Neural Networks202117

Yolo in the dark-domain adaptation method for merging multiple models. Y Sasagawa, H Nagahara, Proceedings of the European Conference on Computer Vision. the European Conference on Computer Vision202017